improving generalization active learning active learning differs learning examples learning algorithm assumes least control part input domain receives information situations active learning provably powerful learning examples alone giving better generalization fixed number training examplesin article consider problem learning binary concept absence noise describe formalism active concept learning called selective sampling show may approximately implemented neural network selective sampling learner receives distribution information environment queries oracle parts domain considers useful test implementation called sgnetwork three domains observe significant improvement generalization b introduction vs active learning neural network generalization problems studied respect random sampling training examples chosen random network simply passive learner approach generally referred learning examples baum haussler 1989 examine problem analytically neural networks cohn tesauro 1992 provide empirical study neural network generalization learning examples also number empirical efforts le cun et al 1990 aimed improving neural network generalization learning examples learning examples however universally applicable paradigm many natural learning systems simply passive instead make use least form active learning examine problem domain active learning mean form learning learning program control inputs trains natural systems humans phenomenon exhibited high levels eg active examination objects low subconscious levels eg fernald kuhls 1987 work infant reactions motherese speech within broad definition active learning restrict attention simple intuitive form concept learning via membership queries membership query learner queries point input domain oracle returns classification point much work formal learning theory directed study queries see eg angluin 1986 valiant 1984 recently queries examined respect role improving generalization behavior many formal problems active learning provably powerful passively learning randomly given examples simple example locating boundary unit line interval order achieve expected position error less ffl one would need draw 1 training examples published machine learning 152201221 1994 preliminary version paper appears cohn et al 1990 one allowed sequentially make membership queries binary search possible assuming uniform distribution position error ffl may reached oln 1 queries one imagine number algorithms employing membership queries active learning studying problem learning binary concepts errorfree environment problems learner may proceed examining information already given determining region uncertainty area domain believes misclassification still possible learner asks examples exclusively region paper discusses formalization simple approach call selective sampling section 2 describe concept learning problem detail give formal definition selective sampling describing conditions necessary approach useful section 3 describe sgnetwork neural network implementation technique inspired versionspace search mitchell 1982 section 4 contains results testing implementation several different problem domains section 5 discusses limitations selective sampling approach sections 6 7 contain reference related work field concluding discussion paper concept learning selective sampling given arbitrary domain x define concept c subset points domain example x might twodimensional space c might set points lying inside fixed rectangle plane classify point x 2 x membership concept c write otherwise popular use artificial neural networks concept classifiers x presented input appropriately trained network activates designated output node threshold x 2 c x instance concept c formally concept class c set concepts usually described description language example class c may set twodimensional axisparallel rectangles see figure 1 case neural networks concept class usually set concepts network may trained classify10000011 figure 1 concept class defined set axisparallel rectangles two dimensions several positive negative examples depicted several consistent concepts class 21 generalization target concept training example pair x tx consisting point x usually drawn distribution p points classification tx x 2 say x tx positive example otherwise negative example concept c consistent example x tx cx tx concept produces classification point x target error c respect distribution p probability c disagree random example drawn p write randomly according p generalization problem posed follows given concept class c unknown target arbitrary error rate ffl confidence ffi many examples draw classify arbitrary distribution p order find concept c 2 c consistent examples fflc p ffl confidence least problem formalized valiant 1984 studied neural networks baum haussler 1989 haussler 19891100000011 figure 2 region uncertainty rs set points x domain two concepts consistent training examples yet disagree classification x 22 region uncertainty consider concept class c set examples classification regions domain may implicitly determined figure 2 concepts c consistent instances may agree parts interested areas determined available information define region uncertainty consistent arbitrary distribution p define size region incremental learning procedure classify train examples ff monotonically nonincreasing point falls outside rs leave unchanged point inside restrict region thus ff probability new random point p reduce uncertainty rs serves envelope consistent concepts disagreement concepts must lie within rs rs also bounds potential error consistent hypothesis choose error current hypothesis ffl ffl ff since basis changing current hypothesis without contradicting point ff also bound probability additional point reducing error 23 selective sampling active learning let us consider learning sequential process drawing examples one another determine much information successive example gives us draw random whole domain probability individual sample reduce error ff defined decreases zero draw examples means efficiency learning process also approaches zero eventually examples draw provide us information concept trying learn consider happens recalculate rs region uncertainty new example draw examples within rs example reduce rs reduce uncertainty decrease efficiency draw examples call process selective sampling distribution p known eg p uniform perform selective sampling directly randomly querying points according p lie strictly inside rs frequently however sample distribution well target concept unknown case cannot choose points domain impugnity risk assuming distribution differs greatly actual underlying p many problems though still make use distribution information without pay full cost drawing classifying example rather assuming drawing classified example atomic operation valiant 1984 blumer et al 1988 may divide operation two steps first drawing unclassified example distribution second querying classification point cost drawing point distribution small compared cost finding points proper classification filter points drawn distribution drawing random selecting classifying training fall rs approach well suited problems speech recognition unlabeled speech data plentiful classifying labeling speech segments laborious process training set size 50 100 150 200 random sampling pass sampling 3 pass sampling 4 pass sampling pass sampling pass sampling figure 3 batch size selective sampling approaches one process yields diminishing improvements added computational costs figure plots error vs training set size selective sampling using different batch sizes learning axisparallel rectangle two dimensions since calculating rs may computationally expensive may want perform selective sampling batches first pass draw initial batch training examples 0 p train determine initial rs define new distribution p 0 sample zero outside maintains relative distribution p inside rs make second pass drawing second batch training examples adding first determining new smaller rs smaller batch size passes made efficiently algorithm draw training examples see figure 3 however since rs recalculated pass advantage must weighed added computational cost incurred calculation 24 approximations selective sampling even simple concept classes set axisparallel rectangles two dimensions may difficult computationally expensive exactly represent region uncertainty class rectangles negative examples lie along corners region add complexity causing nicks outer corners rs figure 2 realistic complicated classes representing exactly easily become difficult impossible task using good approximation rs may however sufficient allow selective sampling practical implementations selective sampling possible number approximations process including maintaining close superset subset rs assume able maintain superset r point rs also superset selectively sample inside r assured exclude part domain interest penalty pay efficiency may also train points interest efficiency approach compared pure selective sampling measured ratio p rx able maintain subset r sampling training algorithm must take additional precautions given iteration part rs excluded sampling need ensure successive iterations choose subsets cover entire region uncertainty example technique discussed next section also need keep number examples iteration small prevent oversampling one part domain remainder paper denote arbitrary algorithms approximation true region uncertainty r 3 neural networks selective sampling selective sampling approach holds promise improved generalization many trainable classifiers remainder paper concerned demonstrating approximation selective sampling may implemented using feedforward neural network trained error backpropagation backpropagation algorithm rumelhart et al 1986 supervised neural network learning technique network presented training set inputoutput pairs x tx learns output tx given input x train neural network using standard backpropagation take training example x tx copy x input nodes network figure 4 1 calculate individual neuron outputs layer layer beginning first hidden layer proceeding output layer output neuron j computed w ji connection weight neuron j neuron squashing function produces neuron outputs range 0 1 define error output node n error value propagated back network see rumelhart et al 1986 details neuron j error term x connection weights w ji adjusted adding deltaw ji j constant learning rate adjustment incrementally decreases error network example x tx presenting training example turn sufficiently large network generally converge set weights assume inputs normalized range 0 1 first hidden layer second hidden layer output layer input layer network output network input connection weights figure 4 simple feedforward neural network node computes weighted sum inputs passes sum sigmoidal squashing function passes result output network acceptably small error training example concept learning model target values training examples 1 0 depending whether input instance concept learned patterns trained error less threshold point need draw attention distinction neural networks architecture configuration 2 architecture neural network refers parameters network change training case networks topology transfer functions configuration network refers network parameters change training case weights given connections neurons although network training algorithms involve changing networks topology training eg ash 1989 consider fixed topologies train weight adjustment theory methods described modification equally applicable trainable classifiers neural network architecture single output node concept class c specified set configurations network take configurations implements mapping input x output 0 1 many configurations may implement mapping set threshold 05 output may say particular configuration c represents concept c x 2 c cx 05 see figure 5 trained training set say network configuration c implements concept c consistent training set use c denote concept c network c implements consider naive algorithm selective sampling neural networks examine short comings describe sgnet based versionspace paradigm mitchell 1982 overcomes difficulties 31 naive neural network querying algorithm observation neural network implementation concept learner may produce realvalued output thresholded suggests naive algorithm defining region uncertainty network trained tolerances divide points domain one three classifications 1 09 greater 0 01 less uncertain 01 09 may say last category corresponds region network uncertain may thus define r approximation region uncertainty figure 6 problem applying approach measures uncertainty particular configuration uncertainty among configurations possible given architecture fact 2 terminology judd 1988 figure 5 thresholded output trained neural network c serves classifier representing concept c hopefully similar unknown target concept part rs full region comprised differences possible consistent network configurations limitation exacerbated inductive bias learning algorithms including backpropa gation backpropagation algorithm attempting classify set points tends draw sharp distinctions become overly confident regions still unknown result r chosen method general small subset true region uncertainty pathological example behavior exhibited figures 7a 7b figure 7a initial random sampling failed yield positive examples triangle right training backpropagation examples yields region uncertainty two contours concentrates left half domain completely exclusion right final result 10 iterations querying learning shown figure 7b strategy related ones prone failure form whenever regions detail target concept discovered initial random sampling stage 32 version space mitchell 1982 describes learning procedure based partial ordering generality concepts learned concept c 1 general another concept c 2 c 2 ae c 1 c 1 6ae c 2 c 2 6ae c 1 two concepts incomparable concept class c set examples version space subset consistent g bound concepts version space maintain two subsets set specific consistent concepts cg similarly set general concepts consistent concept c must case c g 2 g 2 g one may active learning version space examining instances fall difference g region delta symmetric difference operator instance region proves positive generalize accommodate new information proves negative g g modified exclude either case version space space plausible hypotheses reduced every query 33 implementing active versionspace search since entire neural network configuration represents single concept complete version space cannot directly represented single neural network fact haussler 1987 pointed size figure naive approach representing region uncertainty use networks transition area 0 1 represent part domain network uncertain g sets could grow exponentially size training set representing sets completely would require keeping track manipulating exponential number network configurations however modify versionspace search make problem tractable done impose according distribution p strict index ordering concepts class define concept c 1 general concept c 2 random point x drawn definition generality concepts class comparable makes sense speak ordering represent single general concept g single specific concept may still many concepts generality impediment need know concepts general case greater generality concept g chosen maintaining two concepts window version space r sdeltag subset deltag thus point x guaranteed reduce size version space positive invalidate leave us another either general one equally specific one includes new point similarly new point classified negative invalidate g proceeding fashion approximate stepbystep traversal g sets using fixed representation size 34 sgnet neural network versionspace search algorithm since interested selecting examples improve generalization behavior given neural network architecture n define concept class question set concepts learnable n learning algorithm manage obtain network configurations represent g concepts described simple matter implement modified versionspace search following two subsections first describe one may learn specific general concept associated network describe two networks may used selectively sample r defined regions disagree 341 implementing specificgeneral network describe one may learn specific concept consistent given data case learning g general concept analogous specific network set examples according distribution p one classifies positive example points fact positive classifies negative much possible rest domain requirement amounts choosing c consistent minimizes p rx 2 c figure 7 pathological example naive network querying left initial random sample failed detect second disjoint region target concept b right 10 successive iterations naive querying algorithm ignored region concentrated region seen examples dotted line denotes true boundary unknown target concept network may arrived employing inductive bias inductive bias predisposition learning algorithm solutions others learning algorithms inherently least form inductive bias whether preference simple solutions complex ones tendency choose solutions absolute values parameters remain small 3 explicitly add new inductive bias backpropagation algorithm penalizing network part domain classifies positive add bias prefers specific concepts general ones weight penalty must carefully adjusted large enough outweigh training examples network converge training data must however large enough outweigh inductive bias learning algorithm force find specific configuration consistent negative bias may implemented drawing unclassified points p creating case p known arbitrarily labeling negative examples add background examples training set figure 8 creates background bias domain weighted input distribution p networks least error background patterns ones specific according p order allow network converge actual training examples spite background examples must balance influence background examples training data network learns training example x error term equation 1 approach zero error term arbitrary background example may remain constant unless push random background example exerts network weights deltaw ji decreased match normal training examples deltaw ji x background examples dominate network converge solution achieve balance using different learning rates training examples background examples dynamically decrease background learning rate function networks error training set time present training example x calculate new background learning rate error network x constant train single background 3 inductive biases inherent backpropagation well studied appears tendency fit data using smallest number units possible figure 8 training large number background points addition regular training data forces network specific configuration example using value j 0 repeat formally algorithm follows 1 initialize network random configuration c 2 actual training examples x 3 otherwise select next actual training example x tx 4 calculate output error network c input x backpropagate network adjusting weights according deltaw ji 5 calculate new background learning rate 6 draw point p create background example 0 7 calculate output error backpropagate network adjusting weights according modified equation deltaw ji 7 go step 2 optimally fl set weight update background patterns always infinitesimally smaller weight update actual training patterns allowing network anneal specific configuration however requires prohibitive amount training time empirically found setting provides adequate bias still allows convergence reasonable number iterations similar procedure used produce general network adding positive inductive bias classifying background points drawn p positive 342 implementing active learning sgnet represent concepts g simple matter test point x membership r determining sx 6 gx selective sampling may implemented follows point drawn distribution sdeltag two networks agree classification point discarded point sdeltag true classification queried added training set practice merge inputs g networks illustrated figure 9 train together important note technique somewhat robust failure modes degrade efficiency single sampling iteration rather causing overall failure learning process either typical network architecture split separate g networks inputs merged g g figure 9 construction sgnetwork equivalent original g networks fail converge training data points failed converge contained sdeltag region eligible additional sampling next iteration cases found additional examples suffice push network local minimum network converge training set settles solutions near specificgeneral networks consistent data examples gleaned next iteration still useful since chosen virtue lying areas two networks disagreed points settle discrepancies two may lead oversampling region cause technique fail effects two failure modes minimized keeping number examples taken iteration small increases efficiency learning process terms number examples classified observed tradeoff computational resources required time new data added training set network may completely readjust incorporate new information found practice large training set sizes often efficient simply retrain entire network scratch new examples added recent work pratt offers hope retraining may made efficient use information transfer strategies iterations 4 experimental results experiments using selective sampling run three types problems solving simple boundary recognition problem two dimensions learning 25input realvalued threshold function recognizing secure region small power system 41 triangle learner twoinput network two hidden layers 8 3 units single output trained uniform distribution examples positive inside pair triangles negative elsewhere task chosen intuitive visual appeal requires learning nonconnected concept task demands training algorithm sample selection scheme simple convex shape baseline case consisted 12 networks trained randomly drawn examples training set sizes 10 150 points increments 10 examples eight test cases run architecture data selected four runs sgnetwork using 15 selective sampling iterations 10 examples figures 10a 10b additionally 12 runs naive querying algorithm described section 31 run comparison networks trained selectively sampled data showed marked consistent improvement randomly sampled networks ones trained naive querying figure 11 naive querying algorithm displayed much erratic performance two algorithms possibly due pathological nature failure modes figure 10 triangle learner problem learned 150 random examples left learned 150 examples drawn 15 passes selective sampling b right dotted line denotes true boundary unknown target concept 42 realvalued threshold function used 25bit realvalued threshold problem quantitative measure network performance simple higherdimensional problem six runs selective sampling using iterations 10 examples per iteration trained problem compared 12 identical networks trained randomly sampled data results figure 12 indicate much steeper learning curve selective sampling plotting generalization error number training examples networks trained randomly sampled data exhibited roughly polynomial curve would expected following blumer et al 1988 using simple linear regression 1 ffl error data fit coefficient determination r 2 0987 networks trained selectively sampled data comparison fit indicating fit polynomial good visually selectively sampled networks exhibited steeper drop generalization error would expected active learning method using linear regression natural logarithm errors selectively sampled networks exhibited decrease generalization error matching error drops 15 indicating good fit exponential curve comparison randomly sampled networks fit domain sgnetwork appears provide almost exponential improvement generalization increasing training set size much one would expect good active learning algorithm suggests sgnetwork represents good approximation region uncertainty domain thus implements good approximation selective sampling additional experiments run using 2 3 4 20 iterations indicate error decreases sampling process broken smaller frequent iterations observation consistent increased efficiency sampling new information incorporated earlier sampling process 43 power system security analysis various load parameters electrical power system within certain range system secure otherwise risks thermal overload brownout previous research aggoune et al 1989 determined problem amenable neural network learning random sampling problem domain inefficient terms examples needed range parameters system run known distribution information readily available set parameters point domain one analytically determine whether system secure must done solving training set size 50 100 150 03 random sampling naive querying selective sampling figure generalization error vs training set size random sampling naive querying selective sampling irregularity naive querying algorithms error may due intermittent failure find triangles intial random sample timeconsuming system equations thus since classification point much expensive determination input distribution problem amenable solution selective sampling baseline case random sampling four dimensions studied hwang et al 1990 used comparison experiments ran six sets networks initial random training sets 500 data points added single iteration selective sampling networks trained small second iteration 300 points total 800 well large second iteration 2000 total 2500 points results compared baseline cases 800 2500 points randomly sampled data estimated network errors testing 14979 randomly drawn test points improvement single extra iteration selective sampling yielded small set 107 total error 517 instead 547 large set resulted improvement 126 total 421 instead 482 difference significant greater 90 confidence 5 limitations selective sampling approach number limitations selective sampling approach practical mentioned previous section discussing implementations technique others theoretical 51 practical limitations discussed earlier paper exact implementation selective sampling practical relatively simple concept classes class becomes complex becomes difficult compute maintain accurate approximation rs case maintaining superset increased concept complexity seems lead cases r effectively contains entire domain reducing efficiency selective sampling random sam pling example section 24 illustrates nicely bounding box suffices approximation training set size sampling selective sampling polynomial exponential figure 12 generalization error vs training set size random sampling selective sampling standard deviation error averages 000265 random case 000116 selectively sampled case rectangles two dimensions nicks box bounding 20dimensional figure could conceivably require approximation contain domain space case maintaining subset increased concept complexity leads extreme r contains small subset rs cases oversampling regions becomes critical problem due inductive bias training algorithm even training set size one may omit large regions domain 52 theoretical limitations selective sampling draws power ability differentiate region uncertainty bulk domain cases representational complexity concept large neural network many hidden units however rs extend whole domain concept already welllearned even though maximum error may small due number places error may arise total uncertainty may remain large thus depending desired final error rate selective sampling may come effect longer needed similarly input dimension large bulk domain may uncertain even simple concepts one method avoiding problem use bayesian probabilities measure degree utility querying various parts region uncertainty approach recently studied david mackay 1991 discussed briefly following section 6 related work work described paper extension results published cohn et al 1990 prior work since many related results active learning large body work studying effects queries strict learning theory viewpoint primarily respect learning formal concepts boolean expressions finite state automata angluin 1986 showed minimal finite state automata polynomially learnable valiant sense examples alone could learned using polynomial number queries oracle provides counterexamples valiant 1984 considers various classes learnable using variety forms active learning work eisenberg rivest 1990 puts bounds degree membership queries examples help generalization underlying distribution unknown additionally given certain smoothness constraints distribution describe queries may used learn class initial segments unit line actual implementations querying systems learning recently explored work done hwang et al 1990 implements querying neural networks means inverting activation trained network determine uncertain approach shows promise concept learning cases relatively compact connected concepts already produced impressive results power system static security problem however susceptible pathology discussed section 31 algorithm due baum lang 1991 uses queries reduce computational costs training single hiddenlayer neural network algorithm makes queries allow network efficiently determine connection weights input layer hidden layer seung et al 1992 independently proposed similar scheme selecting queries basing lack consensus committee learners freund et al 1993 showed size committee increases beyond two learners used selective sampling accuracy ones utility estimate increases sharply work david mackay 1992 pursues related approach data selection using bayesian analysis assigning prior probabilities concept network configuration one determine utility querying various parts rs fact point lies within rs means consistent configurations disagree classification point point edge rs though may configurations disagree querying point decrease size rs infinitesimally small amount using bayesian analysis one may effect determine number configurations disagree given point thus determine parts rs uncertain 7 conclusion paper presented theory selective sampling described neural network implementation theory examined performance resulting system several domains selective sampling rudimentary form active learning benefit formal grounding learning theory neural network implementation tested demonstrates significant improvement passive random sampling techniques number simple problems paradigm suited concept learning problems relevant input distribution known cost obtaining unlabeled example input distribution small compared cost labeling example limitations selective sampling become apparent complex problem domains approach opens door study sophisticated techniques querying learning natural intuitive means active learning acknowledgements work supported national science foundation grant number ccr9108314 washington technology center ibm corporation majority work done david cohn dept computer science engineering university washington remainder done david cohn ibm j watson research center yorktown heights ny 10598 would like thank jai choi siri weerasooriya work running simulation data power system problem would also like thank two anonymous referees suggestions earlier version paper r artificial neural networks power system static security assessment learning regular sets queries counterexamples dynamic node creation backpropagation networks size net gives valid generalization constructing hidden units using examples queries learnability vapnikchervonenkis dimension training 3node neural network npcomplete training connectionist networks queries selective sampling tight vapnikchervonenkis bounds neural computation volume4volumeissue2issuepages249269pages sample complexity paclearning using random chosen examples acoustic determinants infant preference motherese speech learning conjunctive concepts structural domains generalizing pac model neural nets learning applications query learning based boundary search gradient computation trained multilayer perceptrons complexity loading shallow neural networks optimal brain damage generalization search learning internal representations error propagation theory learnable tr ctr gary weiss ye tian maximizing classifier utility training data costly acm sigkdd explorations newsletter v8 n2 p3138 december 2006 patricia g foschi huan liu active learning detecting spectrally variable subject color infrared imagery pattern recognition letters v25 n13 p15091517 1 october 2004 tracy hammond randall davis interactive learning structural shape descriptions automatically generated nearmiss examples proceedings 11th international conference intelligent user interfaces january 29february 01 2006 sydney australia geoff hulten pedro domingos mining complex models arbitrarily large databases constant time proceedings eighth acm sigkdd international conference knowledge discovery data mining july 2326 2002 edmonton alberta canada p engelbrecht r brits supervised training using unsupervised approach active learning neural processing letters v15 n3 p247260 june 2002 prem melville foster provost maytal saartsechansky raymond mooney economical active featurevalue acquisition expected utility estimation proceedings 1st international workshop utilitybased data mining p1016 august 2121 2005 chicago illinois rebecca hwa minimizing training corpus parser acquisition proceedings 2001 workshop computational natural language learning p16 july 0607 2001 toulouse france brigham anderson andrew moore active learning hidden markov models objective functions algorithms proceedings 22nd international conference machine learning p916 august 0711 2005 bonn germany rebecca hwa sample selection statistical grammar induction proceedings 2000 joint sigdat conference empirical methods natural language processing large corpora held conjunction 38th annual meeting association computational linguistics p4552 october 0708 2000 hong kong sean p engelson ido dagan minimizing manual annotation cost supervised training corpora proceedings 34th annual meeting association computational linguistics p319326 june 2427 1996 santa cruz california kiyonori ohtake analysis selective strategies build dependencyanalyzed corpus proceedings colingacl main conference poster sessions p635642 july 1718 2006 sydney australia george k baah alexander gray mary jean harrold online anomaly detection deployed software statistical machine learning approach proceedings 3rd international workshop software quality assurance november 0606 2006 portland oregon jason baldridge miles osborne active learning hpsg parse selection proceedings seventh conference natural language learning hltnaacl 2003 p1724 may 31 2003 edmonton canada prem melville raymond j mooney diverse ensembles active learning proceedings twentyfirst international conference machine learning p74 july 0408 2004 banff alberta canada mark steedman rebecca hwa stephen clark miles osborne anoop sarkar julia hockenmaier paul ruhlen steven baker jeremiah crim example selection bootstrapping statistical parsers proceedings conference north american chapter association computational linguistics human language technology p157164 may 27june 01 2003 edmonton canada hinrich schtze emre velipasaoglu jan pedersen performance thresholding practical text classification proceedings 15th acm international conference information knowledge management november 0611 2006 arlington virginia usa rebecca hwa sample selection statistical parsing computational linguistics v30 n3 p253276 september 2004 james f bowring james rehg mary jean harrold active learning automatic classification software behavior acm sigsoft software engineering notes v29 n4 july 2004 p engelbrecht sensitivity analysis decision boundaries neural processing letters v10 n3 p253266 dec 1999 stephen soderland learning information extraction rules semistructured free text machine learning v34 n13 p233272 feb 1999 michael lindenbaum shaul markovitch dmitry rusakov selective sampling nearest neighbor classifiers machine learning v54 n2 p125152 february 2004 steven wolfman tessa lau pedro domingos daniel weld mixed initiative interfaces learning tasks smartedit talks back proceedings 6th international conference intelligent user interfaces p167174 january 1417 2001 santa fe new mexico united states gaurav pandey himanshu gupta pabitra mitra stochastic scheduling active support vector learning algorithms proceedings 2005 acm symposium applied computing march 1317 2005 santa fe new mexico qi su dmitry pavlov jyhherng chow wendell c baker internetscale collection humanreviewed data proceedings 16th international conference world wide web may 0812 2007 banff alberta canada atsushi fujii takenobu tokunaga kentaro inui hozumi tanaka selective sampling examplebased word sense disambiguation computational linguistics v24 n4 p573597 december 1998 sunita sarawagi anuradha bhamidipaty interactive deduplication using active learning proceedings eighth acm sigkdd international conference knowledge discovery data mining july 2326 2002 edmonton alberta canada leonardo franco sergio cannas generalization selection examples feedforward neural networks neural computation v12 n10 p24052426 october 2000 aleksander kocz joshua alspector asymmetric missingdata problems overcoming lack negative data preference ranking information retrieval v5 n1 p540 january 2002 jianqiang shen thomas g dietterich active em reduce noise activity recognition proceedings 12th international conference intelligent user interfaces january 2831 2007 honolulu hawaii usa francois barbanon daniel p miranker sphinx schema integration example journal intelligent information systems v29 n2 p145184 october 2007 yevgeniy vorobeychik michael p wellman satinder singh learning payoff functions infinite games machine learning v67 n12 p145168 may 2007 kinh tieu paul viola boosting image retrieval international journal computer vision v56 n12 p1736 januaryfebruary 2004 dilek hakkanitr giuseppe riccardi gokhan tur active approach spoken language processing acm transactions speech language processing tslp v3 n3 p131 october 2006 zhihua zhou ming li tritraining exploiting unlabeled data using three classifiers ieee transactions knowledge data engineering v17 n11 p15291541 november 2005 pabitra mitra b uma shankar sankar k pal segmentation multispectral remote sensing images using active support vector machines pattern recognition letters v25 n9 p10671074 2 july 2004 joel ratsaby learning multicategory classification sample queries information computation v185 n2 p298327 september 15 yoram baram ran elyaniv kobi luz online choice active learning algorithms journal machine learning research 5 p255291 1212004 mariaflorina balcan alina beygelzimer john langford agnostic active learning proceedings 23rd international conference machine learning p6572 june 2529 2006 pittsburgh pennsylvania maytal saartsechansky foster provost active sampling class probability estimation ranking machine learning v54 n2 p153178 february 2004 david lewis william gale sequential algorithm training text classifiers proceedings 17th annual international acm sigir conference research development information retrieval p312 july 0306 1994 dublin ireland huan liu hiroshi motoda lei yu selective sampling approach active feature selection artificial intelligence v159 n12 p4974 november 2004 vijay iyengar chidanand apte tong zhang active learning using adaptive resampling proceedings sixth acm sigkdd international conference knowledge discovery data mining p9198 august 2023 2000 boston massachusetts united states hema raghavan omid madani rosie jones active learning feedback features instances journal machine learning research 7 p16551686 1212006 russell greiner adam j grove dan roth learning costsensitive active classifiers artificial intelligence v139 n2 p137174 august 2002 raymond j mooney loriene roy contentbased book recommending using learning text categorization proceedings fifth acm conference digital libraries p195204 june 0207 2000 san antonio texas united states henrik jacobsson crystallizing substochastic sequential machine extractor cryssmex neural computation v18 n9 p22112255 september 2006 huan liu hiroshi motoda issues instance selection data mining knowledge discovery v6 n2 p115130 april 2002 xingquan zhu xindong wu costconstrained data acquisition intelligent data preparation ieee transactions knowledge data engineering v17 n11 p15421556 november 2005 gediminas adomavicius alexander tuzhilin toward next generation recommender systems survey stateoftheart possible extensions ieee transactions knowledge data engineering v17 n6 p734749 june 2005 p engelbrecht sensitivity analysis selective learning feedforward neural networks fundamenta informaticae v46 n3 p219252 august 2001 andries p engelbrecht sensitivity analysis selective learning feedforward neural networks fundamenta informaticae v45 n4 p295328 december 2001 hasenjger h ritter active learning neural networks new learning paradigms soft computing physicaverlag gmbh heidelberg germany 2002