inference modelbased cluster analysis new approach cluster analysis introduced based parsimonious geometric modelling withingroup covariance matrices mixture multivariate normal distributions using hierarchical agglomeration iterative relocation works well widely used via mclust software available splus statlib however several limitations assessment uncertainty classification partition suboptimal parameter estimates biased shape matrix specified user prior group probabilities assumed equal method choosing number groups based crude approximation formal way choosing various possible models included propose new approach overcomes difficulties consists exact bayesian inference via gibbs sampling calculation bayes factors choosing model number groups output using laplacemetropolis estimator works well several real simulated examples b introduction banfield raftery 1993 hereafter br building work murtagh raftery 1984 introduced new approach cluster analysis based mixture multivariate normal distributions covariance matrices modeled parsimoniously geometrically interpretable way general finite normal mixture distribution n data points dimensions k groups oedeltaj sigma multivariate normal density mean covariance matrix sigma vector group mixing proportions k 0 br approach based variant standard spectral decomposition sigma k namely k scalar orthogonal matrix factor equation 2 geometric controls volume kth group k shape k orientation imposing constraints ie group volume shape different orientations one obtains different models lead turn different clustering algorithms models considered including parsimonious spherically shaped ones listed table 1 br developed algorithms aimed maximizing classification likelihood function vector group memberships namely belongs kth group algorithms use hierarchical agglomeration iterative relocation worked well several real simulated data sets fairly widely used implemented software mclust splus function fortran program available statlib 1 however br algorithms limitations several common agglomerative hierarchical clustering methods obtain fortran version send email message send mclust general address statlibstatcmuedu table 1 clustering models entries indicate whether feature interest shape orientation volume group model sigma k shape orientation volume 1 spherical none 2 k spherical none different 3 sigma 4 k sigma different 5 k ad k different k different different 7 k da k different different 8 sigma k different different different give point classifications individual produce assessment associated uncertainty b tend yield partitions suboptimal even often good due use hierarchical agglomeration c estimates model parameters based estimated partition tend biased marriott 1975 assume mixing proportions k equation 1 equal e algorithms based models 5 6 table 1 require shape matrix specified advance user drawback general although sometimes useful f choose k number groups br proposed approximation posterior probabilities based quantity called awe approximate weight evidence worked fairly well practice quite crude g br proposed formal way choosing among possible models must done user know way fully overcoming limitations f g fully bayesian analysis develop possible ways overcoming discussed section 4 present new approach clustering based models table 1 consists fully bayesian inference models via gibbs sampling overcomes limitations mentioned recently proposed way calculating bayes factors posterior simulation output laplacemetropolis estimator raftery 1995 lewis raftery 1994 used choose model determine number groups one step section 21 describe bayesian estimation models table 1 using markov chain monte carlo methods section 22 outline bayes factors calculated mcmc output used determine appropriate model number groups section 3 show methods work real simulated data sets bayesian inference banfieldraftery clustering models using gibbs sampler 21 estimation assume data classified x arise random vector density 1 corresponding classification variables unobserved concerned bayesian inference model parameters classification indicators mcmc methods provide general recipe bayesian analysis mixtures instance lavine west 1992 soubiran celeux diebolt robert 1991 used gibbs sampler estimating parameters multivariate gaussian mixture without assuming specific characteristics component variance matrices diebolt robert 1994 considered gibbs sampler data augmentation method tanner wong 1987 general univariate gaussian mixtures proved algorithms converge distribution true posterior distribution mixture parameters like authors use conjugate priors parameters mixture model prior distribution mixing proportions dirichlet distribution prior distributions means k mixture components conditionally variance matrices sigma k gaussian k jsigma k n prior distribution variance matrices depends model given model turn estimate eight models table 1 simulating joint posterior distribution using gibbs sampler smith roberts 1993 case consists following steps 1 simulate classification variables according posterior probabilities namely 2 simulate vector mixing proportions according posterior distribution conditional 3 simulate parameters model according posterior distributions conditional validity procedure namely fact markov chain associated algorithm converges distribution true posterior distribution shown diebolt robert 1990 context onedimensional normal mixtures proof based duality principle uses finite space nature chain associated chain ergodic state space kg thus geometrically convergent even mixing properties transfer automatically sequence values important properties central limit theorem law iterated logarithm satisfied diebolt robert 1994 robert 1993 results also apply difference complex simulation structure imposed variance assumptions steps 1 2 depend considered model step 1 straightforward step 2 consists simulating conditional posterior distribution namely ff 1 3 different models table 1 described appendix model turn 22 choosing number groups model bayes factors br left choice model user noting model 6 table 1 corresponding criterion gives good results many situations based choice number clusters awe criterion crude approximation twice log bayes factor number clusters versus one cluster develop way choosing model number groups time using accurate approximation bayes factor br compute approximate bayes factors gibbs sampler output using laplace metropolis estimator raftery 1995 shown give accurate results lewis raftery 1994 follows word model refers combination one models table 1 specified number clusters bayes another model 0 given data ratio posterior prior odds namely equation 4 z vector parameters k pr k jm k prior density called integrated likelihood model k review bayes factors calculation interpretation see kass raftery 1995 bayesian model selection based bayes factors whose key ingredient integrated likelihood model main computational challenge thus approximate integrated likelihood using gibbs sampler output using laplacemetropolis estimator integrated likelihood raftery 1995 lewis raftery 1994 laplace method integrals based taylor series expansion realvalued function fu ddimensional vector u yields approximation z u value u f attains maximum minus inverse hessian f evaluated u applied equation 5 yields dimension posterior mode psi minus inverse hessian logfprdjprg evaluated arguments similar appendix tierney kadane 1986 show regular statistical models relative error equation 7 hence resulting approximation sample size laplace method often accurate directly applicable derivatives requires easily available idea laplacemetropolis estimator get around limitations laplace method using posterior simulation estimate quantities needs laplace method requires posterior mode jpsij laplacemetropolis estimator estimates gibbs sampler output using robust location scale estimators likelihood approximate posterior mode prdj quantities substituted equation 7 obtain integrated likelihood bayes factors computed taking ratios integrated likelihoods equation 4 3 examples present three examples illustrate ability methods overcome limitations ag methods described section 1 first example uses simulated data second third examples based real data sets example consider models k sigma k sigma models probably used gaussian mixture models clustering data eg mclachlan basford 1988 generalizations k ands k sigma allow different volumes proved powerful several practical situations celeux govaert 1994 priors chosen among conjugate priors section 21 fairly flat region likelihood substantial much greater elsewhere thus satisfy principle stable estimation edwards lindman savage 1963 could expected results would relatively insensitive reasonable changes prior also checked empirically example used x empirical mean vector variance matrix whole data set oe 2 greatest eigenvalue notation used defined appendix amount information contained prior similar contained typical single observation thus prior may viewed comparable true prior person rather little information similar priors used generalized linear models raftery 1993 linear regression models raftery madigan hoeting 1994 example assessed sensivity results changes prior found small sensitivity results first example included gauss figure example 1 simulated data 31 example 1 simulated data simulated 200 points bivariate twocomponent gaussian mixture equal pro portions mean vectors data shown figure 1 first 600 iterations gibbs sampler output model k two groups shown figure 2 convergence almost immediate successive draws almost independent similar results obtained starting values used 1500 iterations estimated gibbsit program enough estimate cumulative distribution function 025 975 quantiles within sigma01 parameters raftery lewis 1993 1995 first 10 iterations discarded model comparison results shown table 2 correct model k correct number groups 2 strongly favored posterior means parameters preferred model close true values marginal posterior distribution summarized figure 3 shows posterior distribution principal circles two groups sensitivity prior distribution investigated table 3 new gibbs sampler figure 2 example 1 time series plot first 600 gibbs sampler iterations volume parameters b mean group 1 c mean group 2 table 2 example 1 approximate log integrated likelihoods groups k sigma k sigma figure 3 example 1 posterior distribution principal circles k model two groups one circle gibbs sampler iteration group center table 3 example 1 sensitivity selected results changes prior hyperparameters 1500 simulations selected model prior parameters log b 23 pr log bayes factor two groups three groups denotes data x overall mean data denotes means two optimal partitions namely run done choice prior parameters differences table 3 due true sensitivity monte carlo variation true sensitivity thus likely smaller estimation results quite insensitive testing results somewhat sensitive expected kass raftery 1995 overall conclusions remain combinations prior parameters considered perhaps greatest advantage present approach fully assesses uncertainty group membership rather merely giving single best partition figure 4 summarized showing uncertainty point measured clear x belongs kth group small u also small data u large one point 55 one lies boundary two groups 32 example 2 butterfly classification figure 32 shows four wing measurements butterfly analyze data two measurements z 3 z 4 23 butterflies shown figure 6 celeux robert 1993 aim decide many species represented group insects classify table 4 shows model k sigma four groups favored quite strongly alternatives posterior means parameters figure 4 example 1 uncertainty plot point vertical line length proportional u plotted longest line length 05 figure 5 example 2 butterfly measurements z3 z4 20 22 24 26 282030246810121315171921 22 23 figure example 2 butterfly data values z 3 z 4 23 butterflies table 4 example 2 approximate log integrated likelihoods groups k sigma k sigma 013 likely group memberships posteriori shown figure 7 along associated uncertainties butterflies classified confidence except numbers 4 15 close boundary groups 1 3 group 4 consists one butterfly clearly correct classification known indeed four groups addition correct classification exactly equal optimal classification found methods celeux robert 1993 figure 7 example 2 estimated group memberships uncertainty plot butterfly 33 example 3 kinematic stellar data fairly recently believed galaxy consists two stellar populations disk halo recently hypothesized fact three stellar populations old thin disk thick disk halo distinguished spatial distributions velocities metallicities hypotheses different implications theories formation galaxy evidence deciding whether two three populations shown figure 8 shows radial rotational velocities stars soubiran 1993 table 5 shows model k sigma preferred strong evidence three groups two balance astronomical opinion also tilted towards conclusion based much information velocities used including star positions metallicities soubiran 1993 impressive strong conclusion reached present methods using relatively small part total available information posterior means parameters preferred model 552 corresponding partition shown figure 9 u figure 8 example 3 kinematic stellar data radial u rotational v velocities 2370 stars galaxy source soubiran 1993 table 5 example 3 approximate log integrated likelihoods groups k sigma k sigma astr figure 9 example 3 optimal partition model k sigma figure 10 example 3 uncertainty plot uncertainty plot shown figure 10 areas high uncertainty boundaries two three groups greatest uncertainty two small areas three groups intersect presented fully bayesian analysis modelbased clustering methodolology banfield raftery 1993 overcomes many limitations approach appears work well several examples alternative frequentist approaches might easier implement consist maximizing likelihood using em algorithm maximizing classification likelihood using classification em cem algorithm celeux govaert 1995 considered approaches full range clustering models derived eigenvalue decomposition group variance matrices including considered shown particular possible find maximum likelihood estimate shape matrix approaches could overcome limitations b e section 1 maximum likelihood approach could also overcome limitation c would ovecome difficulty partly provide estimate uncertainty group membership assessment incomplete take account uncertainty overcome limitations f g examples explicitly considered models 14 table 1 sufficient data considered however generally would useful consider eight models proceeding way using results section 21 appendix gibbs sampling clustering models describe step 3 gibbs sampling eight clustering models given classification vector use notation componentwise statistics location scale follows denote model eigenvalue decomposition variance matrix written brackets instance k ad denotes mixture model equal volumes equal shapes different orientations model scale parameter common components mixture assume prior distribution parameters conjugate namely 2 ae means inverted gamma distribution gamma 1rae gammar2 posterior distribution therefore convolution normal distributions k inverse gamma distribution gibbs components step 3 31 p 32 simulate b model k variance scales different prior distributions similar components recover case treated diebolt robert1994 namely groups generated separately 31 p 32 simulate c model sigma need consider eigenvalue decomposition covariance matrix sigma prior distribution given means sigma inverse wishart distribution prsigma jsigmaj gammamp12 expftrpsisigma gamma1 2g step 3 gibbs sampling decomposed follows 31 32 simulate ae model k sigma 0 prior distribution three components make model identifiable setting gibbs sampling decomposed follows 31 32 simulate 33 simulate ae model k ad distinction entirely geometric volume versus shape therefore consider single parameter first term diagonal 1 longer constrained equal 1 prior distribution parameters k model distribution orthogonal matrix k delicate specify use marginal inverse wishart distribution w gamma1 analogy previous cases marginal distribution derived explicitly choice p scale matrix z k k k denotes tth element diagonal matrix k k posterior distribution whole set parameters fa gamman k 2 gammar k leads following gibbs steps 31 32 simulate jd 33 use approximation distribution k 14 ie assume k direction shape components inverse wishart random variable independent true asymptotically ie number degrees freedom goes infinity anderson 1984 theorem 1351 considerably simplifies simulation moderate effects resulting posterior distribution thus assume couples distributed w gamma1 derive posterior distribution gamma2 tr theta exp condensed way saying diagonal elements distributed according inverted gamma distributions j ig n k distributed posteriori principal direction vectors following inverse wishart distribution f model k k ad k gibbs simulations k k similar model replacing 15 simulation also quite close previous version since exp gammatr assume k igl k 2 ae k 2 posterior distribution k gibbs sampler g model k da k need isolate volume k consider parameters k first term diagonal 1k constrained equal 1 prior distribution parameters k k model tk igr tk 2 ae tk 2 z posterior distribution whole set parameters gamman k 2 gammar tk tk ae tk 2 leads followings gibbs steps 31 32 simulate tk jd ig 33 simulate use metropolis step recover exp gamma2 tr model sigma k standard gaussian mixture model considered lavine west 1992 soubiran et al 1991 case need make use eigenvalue decomposition sigma k prior distribution k sigma k corresponding gibbs step r introduction multivariate statistical analysis modelbased gaussian non gaussian clustering gaussian parsimonious clustering models bayesian estimation finite mixture distributions bayesian estimation finite mixture distributions part ii sampling implementation bayesian statistical inference psychological research bayes factors bayesian method classification discrimination estimating bayes factors via posterior simulation laplacemetropolis estimator separating mixtures normal distributions mixture models fitting straight lines point patterns approximate bayes factors accounting model uncertainty generalized linear models many iterations gibbs sampler number iterations convergence diagnostics generic metropolis algorithms acounting model uncertainty linear regression bayesian computation via gibbs sampler related markov chain monte carlo methods kinematics galaxys stellar population proper motion survey analyse de melanges gaussiens pour de petits echantillons application la cinematique stellaire tr ctr mario figueiredo anil k jain unsupervised learning finite mixture models ieee transactions pattern analysis machine intelligence v24 n3 p381396 march 2002 zhihua zhang kap luk chan yiming wu chibiao chen learning multivariate gaussian mixture model reversible jump mcmc algorithm statistics computing v14 n4 p343355 october 2004