collective mining bayesian networks distributed heterogeneous data present collective approach learning bayesian network distributed heterogeneous data approach first learn local bayesian network site using local data site identifies observations likely evidence coupling local nonlocal variables transmits subset observations central site another bayesian network learnt central site using data transmitted local site local central bayesian networks combined obtain collective bayesian network models entire data experimental results theoretical justification demonstrate feasibility approach presented b table 1 homogeneous case site table credit card transaction records account amount location previous unusual number record transaction table 2 homogeneous case site b table credit card transaction records account amount location previous unusual number record transaction commercial domains eg internet corporate intranets introduces new dimension process large number distributed sources data used discovering knowledge cost data communication distributed databases signicant factor increasingly mobile connected world large number distributed data sources cost consists several components like limited network bandwidth b data security c existing organizational structure applications environment eld distributed knowledge discovery data mining ddm studies algo rithms systems humancomputer interaction issues knowledge discovery applications distributed environments minimizing cost paper consider bayesian network bn model represent uncertain knowledge specically address problem learning bn heterogenous distributed data uses collective data mining cdm approach introduced earlier kargupta et al kargupta huang sivakumar johnson 2001 kargupta park hershberger johnson 2000 hershberger kargupta 2001 park kargupta 2002 section 2 provides background reviews existing literature area section 3 presents collective bayesian learning technique experimental results three datasets one simulated two real world presented section 4 preliminary version results presented chen sivakumar kargupta 2001a chen sivakumar kargupta 2001b finally section 5 provides concluding remarks directions future work 2 background motivation related work section provide background motivation problem means example review existing literature area distributed data mining ddm must deal dierent possibilities data distribution dierent sites may contain data common set features collective mining bayesian networks distributed heterogeneous data 3 problem domain case relational data would mean consistent database schema across sites homogeneous case tables 1 2 illustrate case using example hypothetical credit card transaction domain1 two data sites b connected network ddmobjective domain may nd patterns fraudulent transactions note tables schema underlying distribution data may may identical across dierent data sites general case data sites may heterogeneous words sites may contain tables dierent schemata dierent features observed dierent sites let us illustrate case relational data table 3 shows two datatables site x upper table contains weatherrelated data lower one contains demographic data table 4 shows content site contains holiday toy sales data objective ddm process may detecting relations toy sales demographic weather related features general heterogeneous case tables may related dierent sets key indices example tables 3 upper lower related key feature city hand table 3 lower table 4 related key feature state consider heterogenous data scenario paper would like mention heterogenous databases general could complicated scenario example maybe set overlapping features observed one site moreover existence key used link together observations across sites crucial approach example web log mining application key used link together observations across sites could produced using either cookie user ip address combination log data like time access however assumptions overly restrictive required reasonable solution distributed bayesian learning problem 21 motivation bayesian networks oer useful information mutual dependencies among features application domain information used gaining better understanding dynamics process obser vation financial data analysis manufacturing process monitoring sensor data analysis web mining examples mining bayesian networks quite useful bayesian techniques also useful mining distributed data section discuss examples scenario explain proposed collective bayesian learning algorithm useful practice advances computing communication wired wireless networks resulted many pervasive distributed computing environments ternet intranets local area networks ad hoc wireless networks examples many environments dierent distributed sources voluminous data multiple compute nodes since data mining oers capability sifting data search useful information nds many applications distributed systems like centralized monolithic counterparts please note credit card domain may always consistent schema domain used illustration 4 r chen et al table 3 heterogeneous case site x two tables one weather demography city temp humidity wind chill seattle city state size average proportion earning small seattle table action figure 4799 23 id power toads 2350 2 many domains distributed processing data natural scalable solution example consider ad hoc wireless sensor network dierent sensor nodes monitoring timecritical events central collection data every sensor node may create heavy trac limited bandwidth wireless channels may also drain lot power devices distributed architecture data mining likely reduce communication load also reduce battery power evenly across dierent nodes sensor network one easily imagine similar needs distributed computation data mining primitives ad hoc wireless networks mobile devices like pdas cellphones wearable computers potential applications include personalization collaborative process monitoring intrusion detection ad hoc wireless networks need data mining architectures pay careful attention distributed resources data computing communication order consume near optimal fashion distributed data mining ddm considers data mining broader context objective ddm perform data mining operations based type availability distributed resources may choose download data sets single site perform data mining operations central location however decision ddm based properties computing storage communication capabilities contrast traditional central collective mining bayesian networks distributed heterogeneous data 5 ized data mining methodology collection data single location prior analysis invariant characteristic wireless domain example fact applications deal timecritical distributed data likely benet paying careful attention distributed resources computation storage cost communication world wide web good example contains distributed data computing resources increasing number databases eg weather databases oceanographic data wwwnoaagov data streams eg nancial data wwwnasdaqcom emerging disease information wwwcdcgov coming online many change frequently easy think many applications require regular monitoring diverse distributed sources data distributed approach analyze data likely scalable practical particularly application involves large number data sites distributed approach may also nd applications mining remote sensing astronomy data example nasa earth observing system eos data collector number satellites holds 1450 data sets stored managed distributed dierent eos data information system eosdis sites geographically located usa pair terra spacecraft landsat 7 alone produces 350 gb eosdis data per day online mining system eos data streams may scale use centralized data mining architecture mining distributed eos repositories associating information existing environmental databases may benet ddm astronomy size telescope image archives already reached terabyte range continue increase fast information collected new allsky surveyors gscii mclean hawkins spagna lattanzi lasker jenkner white 1998 sloan digital survey szalay 1998 ddm may oer practical scalable solution mining large distributed astronomy data repositories ddm may also useful environments multiple compute nodes connected high speed networks even data quickly centralized using relatively fast network proper balancing computational load among cluster nodes may require distributed approach 22 related work review important literature bn learning bn probabilistic graphical model represents uncertain knowledge jensen 1996 spiegelhalter lauritzen 1990 buntine 1991 discuss parameter learning bn complete data whereas binder koller russel kanazawa 1997 thiesson 1995 discuss parameter learning incomplete data using gradient method lauritzen 1995 proposed em algorithm learn bayesian network parameters whereas bauer koller singer 1997 describe methods accelerating convergence em algorithm learning using gibbs sampling proposed thomas spiegelhalter gilks 1992 gilks richardson spiegelhalter 1996 bayesian score learn structure bn discussed cooper herskovits 1992 buntine 1991 heck erman geiger chickering 1995 learning structure bn based minimal description length mdl principle presented bouck aert 1994 lam bacchus 1994 suzuki 1993 learning bn structure 6 r chen et al using greedy hillclimbing variants introduced heckerman gieger 1995 whereas chickering 1996 introduced method based search equivalence network classes methods approximating full bayesian model averaging presented buntine 1991 heckerman gieger 1995 madigan raftery 1994 learning structure bn incomplete data considered chickering heckerman 1997 cheeseman stutz 1996 friedman 1998 meila jordan 1998 singh 1997 relationship causality bayesian networks discussed heckerman gieger 1995 pearl 1993 spirtes glymour scheines 1993 buntine 1991 friedman goldszmidt 1997 lam bacchus 1994 discuss sequentially update structure bn based additional data applications bayesian network clustering autoclass classication presented cheeseman stutz 1996 ezawa 1995 friedman geiger goldszmidt 1997 singh provan 1995 zweig russel 1998 used bns speech recognition whereas breese heckerman kadie 1998 discussed collaborative ltering methods use bn learning algorithms applications causal learning social sciences presented spirtes et al 1993 important problem learn bayesian network data distributed sites centralized solution problem download datasets distributed sites kenji 1997 worked homogeneous distributed learning scenario case every distributed site feature dierent observations paper address heterogenous case site data subset features knowledge signicant work addresses heterogenous case 3 collective bayesian learning following briey review bayesian networks discuss collective approach learning bayesian network specically designed distributed data scenario 31 bayesian networks review bayesian network bn probabilistic graph model dened g p e directed acyclic graph dag jensen 1996 heckerman 1998 v vertex set represents variables problem e edge set denotes probabilistic relationships among variables variable x 2 v parent x node directed link x let pax denote set parents x conditional independence property represented follows property simplify computations bayesian network model example joint distribution set variables v written product conditional probabilities follows collective mining bayesian networks distributed heterogeneous data 7 fig 1 asia model conditional independence variables either obtained priori expert knowledge discerned data combination jensen 1996 set conditional distributions called parameters bayesian network note variable x parents marginal distribution x figure 1 bayesian network called asia model adapted lauritzen spiegelhalter 1988 variables dyspnoea tuberculosis lung cancer bronchitis asia xray either smoking binary vari ables joint distribution variables ordering variables constitutes constraint structure bayesian network variable x appears variable parent x use ordering prior knowledge example two important issues using bayesian network learning bayesian network b probabilistic inferencing learning bn involves learning structure network directed graph obtaining conditional probabilities parameters associated network bayesian network constructed usually need determine various probabilities interest model process referred probabilistic inference example asia model diagnosis application would require nding probability pb j bronchitis given observed symptom dyspnoea probability usually called posterior probability computed using bayes rule 32 collective bayesian network learning strategy present collective strategy learn bayesian network structure parameters data distributed among dierent sites centralized solution problem download datasets distributed sites central site many applications would feasible size data available communication bandwidth due security considerations learning bn homogeneous case studied kenji 1997 case every distributed site set features dierent set 8 r chen et al observations address heterogenous case distributed site observations subset features primary steps approach compute local bns local model involving variables observed site local variables based local data site based local bn identify observations likely evidence coupling local nonlocal variables transmit subset observations central site central site limited number observations variables available using compute nonlocal bn consisting links variables across two sites combine local models links discovered central site obtain collective bn nonlocal bn thus constructed would eective identifying associations variables across sites whereas local bns would detect associations among local variables site conditional probabilities also estimated similar manner probabilities involve variables single site estimated locally whereas ones involve variables dierent sites estimated central site methodology could used update network based new data first new data tested well ts local model acceptable statistical observation used update local conditional probability estimates otherwise also transmitted central site update appropriate conditional probabilities cross terms finally collective bn obtained taking union nodes edges local bns nonlocal bn along conditional probabilities appropriate bns probabilistic inference performed based collective bn note transmitting local bns central site would involve signicantly lower communication compared transmitting local data quite evident learning probabilistic relationships variables belong single local site straightforward pose additional diculty compared centralized approach2 important objective correctly identify coupling variables belong two sites correspond edges graph connect variables two sites conditional probabilityies associated nodes following describe approach selecting observations local sites likely evidence strong coupling variables two dierent sites 33 selection samples transmission global site simplicity assume data distributed two sites illustrate approach using bn figure 1 extension approach two sites straightforward let us denote b variables left right groups respectively figure 1 assume may true arbitrary bayesian network structure discuss issue last section collective mining bayesian networks distributed heterogeneous data 9 observations available site whereas observations b available dierent site b furthermore assume common feature key index used associate given observation site corresponding observation site b naturally local site local bayesian network learned using samples site would give bn structure involving local variables site associated conditional probabilities let pa pb denote estimated probability function involving local variables product conditional probabilities indicated 2 since pax pbx denote probability likelihood obtaining observation x sites b would call probability functions likelihood functions la lb local model obtained sites b respectively observations site ranked based well ts local model using local likelihood functions observations site large likelihood la evidence local relationships site variables whereas low likelihoods la possible evidence cross relationships variables across sites let sa denote set keys associated latter observations low likelihood la practice step implemented dierent ways example set threshold lax x 2 sa sites b transmit set keys sa sb respectively central site intersection observations corresponding set keys obtained local sites central site following argument justies selection strategy using rules probability assumed conditional independence bn figure 1 easy show set variables b link connecting variable particular note rst three terms righthand side 5 involve variables local site whereas last two terms socalled cross terms involving variables sites b similarly shown pb therefore observation low likelihood sites b ie pa pb small indication large observation since observations small pv less likely occur notice 5 7 terms common pb j nbb precisely conditional probabilities involve variables sites b words observation indicates coupling variables sites b hence transmitted central site identify specic coupling links associated conditional probabilities sense approach learning cross terms bn involves selective sampling given dataset relevant identication coupling sites type importance sampling select observations high conditional probabilities corresponding terms involving variables sites naturally values dierent variables features dierent sites corresponding selected observations pooled together central site learn coupling links well estimate associated conditional distributions selected observations design useful identify links bn local individual sites veried experiments see section 4 34 performance analysis following present brief theoretical analysis performance proposed collective learning method compare performance collective bn bayesian network learned using centralized approach referred centralized bn sequel two types errors involved learning bn error bn structure b error parameters probabilities bn structure error dened sum number correct edges missed number incorrect edges detected parameter error need quantify dis tance two probability distributions consider learning error parameters assuming structure bn correctly determined given widely used metric kullbackleibler kl distance crossentropy measure dklp q two discrete probabilities fpig fqig n number possible outcomes indeed p empirically observed distribution data samples fsi 1 mg h hypothesis candidate probability distribution underlying true distribution abe takeuchi warmuth 1991 therefore minimizing kl distance respect empirically observed distribution equivalent nding maximum likelihood solution h lnhsi since bn provides natural factorization joint probability terms conditional probabilities node see 2 convenient express kl distance two joint distributions terms corresponding conditional distributions let h c two possible joint distributions variables bn corresponding conditional distribution node xi variable collective mining bayesian networks distributed heterogeneous data 11 node set parents node following dasgupta 1997 dene distance hi ci respect true distribution p easy show equations 10 11 provide useful decomposition kl distance true distribution p two dierent hypotheses c h useful analysis sample complexity following subsection 35 sample complexity derive relationship accuracy collective bn number samples transmitted central site consider unrestricted multinomial class bn node variables boolean hypothesis class h determined set possible conditional distributions dierent nodes given bn n variables hypothesis class h need choose hypothesis h 2 h close unknown distribution p given error threshold condence threshold interested constructing function n number samples larger n hopt 2 h hypothesis minimizes dklp h smallest value n satises requirement called sample complexity usually referred probably approximately correct pac framework friedman yakhini 1996 examined sample complexity maximum description length principle mdl based learning procedure bns dasgupta 1997 gave thorough analysis multinomial model boolean variables suppose bn n nodes node k parents given upper bound sample complexity equation 13 gives relation sample size bound conditional probability see use ideas compare performance collective learning method centralized method x condence suppose cen found centralized method given sample size using 13 following analysis dasgupta 1997 hcoepnt optimal hypothesis hcen hypothesis obtained based centralized approach 11 collective bn learning method set nodes split two parts let vl set nodes parent nodes local site vc set nodes least one parent node belonging site dierent node asia model dg use nl nc denote cardinality sets vl vc node x 2 vl collective method learn conditional probability px j pax using data depends local variables therefore x 2 vl local terms c1ol cen nodes vc data transmitted central site used learn conditional probability suppose mc data samples transmitted central site error threshold c2ol satises 13 xed condence 1 therefore x 2 vc 14 dcp p hcooplt hcol c2nol c2ol cen general since collective learning method mc samples available central site 11 17 comparing 16 18 easy see error threshold collective method col dierence error threshold collective centralized method equation 19 shows two important properties collective method first dierence performance independent variables vl means performance collective method parameters local variables centralized method second collective method tradeo accuracy communication overhead data communicate closely c2ol cen mc collective mining bayesian networks distributed heterogeneous data 13 server client selecting samples transmission structure learning local term detecting crossterms parameter learning local term parameter learning crossterms likelihood computation collective bn assembling data transmission control table 5 functionality clientserver distrbn 4 experimental results tested approach three dierent datasets asia model real web log data alarm network present results three cases following subsections software package called distrbn developed implement proposed algorithm software based bn c library called smile3 clientserver architecture client program runs local site server central site one local sites functionality clientserver given table 5 start distributed bn learning task using distrbn clients local sites take charge local bn learning likelihood computing based indices low likelihood samples site server determines indices samples needed detecting crosslinks samples corresponding indices transmitted local sites server along local bn model server learns crossterms incorporates local models along crossterms obtain collective bn 41 asia model experiment illustrates ability proposed collective learning approach correctly obtain structure bn including crosslinks well parameters bn experiments performed dataset generated bn depicted figure 1 asia model conditional probability variable multidimensional array dimensions arranged order ordering variables viz dg table 6 top depicts conditional probability node e laid rst dimension toggles fastest table 6 write conditional probability node e single vector follows 09 01 01 001 01 09 09 099 conditional probabilities parameters asia model given table 6 bottom following ordering scheme generated observations model split two sites illustrated figure 1 site variables e x site b variables b split variables two sites arbitrary done fashion yield two crossterms practice control distribution variables among dierent sites dictated variables observed stored site note two edges connect variables site site b rest six edges local 3 homepage httpwww2sispittedugenie 14 r chen et al table 6 top conditional probability node e bottom conditional probabilities asia model table 7 conditional probabilities local site local site b local bayesian networks constructed using conditional independence test based algorithm cheng bell liu 1997 learning bn structure maximum likelihood based method estimating conditional proba bilities local networks exact far edges involving local variables tested ability collective approach detect two nonlocal edges estimated parameters two local bayesian network depicted table 7 clearly estimated probabilities nodes except nodes e close true probabilities given table 6 words parameters involve local variables successfully learnt local sites fraction samples whose likelihood smaller selected threshold identied site experiments set collective mining bayesian networks distributed heterogeneous data 15 errors bn structure kl distance joint probabilities kl fraction observations communicated fraction observations communicated fig 2 performance collective bn left structure learning error right parameter learning error constant empirical mean local likelihood values empirical standard deviation local likelihood values samples likelihood less threshold ta site tb site b sites sent central site central site learns global bn based samples finally collective bn formed taking union edges detected locally detected central site error structure learning collective bayesian network dened sum number correct edges missed number incorrect edges detected done dierent values figure 2 left depicts error function number samples communicated determined clear exact structure obtained transmitting 5 total samples next assessed accuracy estimated conditional probabilities collective bn used conditional probabilities local bn local terms ones estimated global site cross terms compared performance bn learnt using centralized approach aggregating data single site figure 2 right depicts kl distance dpcntrv pcollv joint probabilities computed using collective approach one computed using centralized approach clearly even small communication overhead estimated conditional probabilities based collective approach quite close obtained centralized approach important test approach error estimating conditional probabilities cross terms estimated global site based selective subset data paper metric called conditional kl ckl distance dened follows distance two conditional probability tables node xi note row cpt distribution xed parent conguration picjoll kl distance variable xi fraction observations commuicated fraction observations commuicated fraction observations commuicated fraction observations commuicated fig 3 kl distance conditional probabilities specic parent conguration j ckl distance derived equation 10 true distribution p equation 10 replaced pcntr since get p real world application kl distance conditional probabilities computed based collective bn bn obtained using centralized approach transmitting data one site cross terms pd j e b figure 3 top left depicts ckl distance node e gure 3 top right depicts ckl distance node clearly even small data communication estimates conditional probabilities crossterms based collective approach quite close obtained centralized approach verify validity approach transmitted data central site used estimate two local terms node node b corresponding ckl distances depicted bottom row figure 3 left node right node b clear estimates probabilities quite poor clearly demonstrates technique used perform biased sampling discovering relationships variables across sites 42 webserver log data second set experiments used data real world domain web server log data experiment illustrates ability proposed collective learning approach learn parameters bn real world web log data web server log contains records user interactions collective mining bayesian networks distributed heterogeneous data 17 request resources servers received web log mining provide useful information dierent user proles turn used oer personalized services well better design organize web resources based usage history application raw web log le obtained web server school eecs washington state university httpwwweecswsuedu three steps processing first preprocess raw web log le transform session form useful application transform clean web log le many redundant records web log le example page may contain several pictures someone accesses page web log le records accessing pictures record accessing page enough remove records step reduce web log le size dramatically transform web log le session form involves identifying sequence logs single session based ip address cookies available time access session corresponds logs single user single web session consider session data sample categorize resource html video audio etc requested server eight categories eadmission fcourse geecs home hresearch eee faculty ccs faculty llab facilities tcontact information aadmission information ucourse information heecs home rresearch categories features general would several tens perhaps couple hundred categories depending webserver categorization done carefully would automated large web server finally feature value session set one zero depending whether user requested resources corresponding category 8feature binary dataset thus obtained used learn bn figure 4 illustrates process schematically central bn rst obtained using whole dataset figure 5 depicts structure centralized bn split features two sets corresponding scenario resources split two dierent web servers site features e c u site b features l h r assumed bn structure known estimated parameters probability distribution bn using collective bn learning approach figure 6 shows kl distance central bn collective bn function fraction observations communicated clearly parameters collective bn close central bn even small fraction data communication 43 alarm network experiment illustrates scalability approach respect number sites b features c observations test scalability bn parameter learning assuming network structure given used real world bn application called alarm network 37 nodes 46 edges successful application bn medical diagnosis area alarm network widely used benchmark network evaluate algorithm alarm network developed online monitoring patients intensive care units generously contributed community bein lich suermondt chavez cooper 1989 structure alarm network collective bn fig 4 schematic illustrating preprocessing mining web log data fig 5 web log mining bn structure collective mining bayesian networks distributed heterogeneous data 19 kl distance joint fraction observations communicated fig 6 kl distance joint probabilities shown gure 7 nodes takes discrete values necessarily bi nary node variables alarm network follows 1anaphylaxis 2 intubation 3kinkedtube 4disconnect 5minvolset 6ventmach7venttube 8ventlung 9ventalv 10artco2 11tpr 12hypovolemia 13lvfailure 14 33history 34minvol 35pap 36pcwp 37press order test scalability approach respect number nodes observations dataset 15000 samples generated 37 nodes split 4 sites follows site 1 f3 4 5 6 7 8 15g site2 f2 site4 f1 11 12 13 14 23 24 25 26 28 33 36g note 13 cross edges assumed structure bayesian network given tested approach estimating conditional probabilities kl distance conditional probabilities see equation 21 estimated based collective bn bn obtained using centralized approach computed particular illustrate results conditional probabilities two dierent nodes 20 21 cross terms figure 8 left depicts ckl distance node 20 two estimates figure 8 right depicts similar distance node 21 clearly even small data communication estimates conditional probabilities crossterms based collective approach quite close obtained centralized approach note node 21 4 parents one local site node 21 three dierent sites also conditional probability table node 21 54 parameters corresponding possible congurations 20 r chen et al fig 7 alarm network node 21 four parents consequently learning parameters node nontrivial task experiments clearly demonstrates technique used perform biased sampling discovering relationships variables across sites simulation also illustrates fact proposed approach scales well respect number nodes samples next test scalability approach respect number sites number sites increases cross edges means nodes whose parameters learnt central site using limited portion data moreover distribution variables among sites would aect local models likelihood computation local sites consequently samples selected transmission successively split variables across two three four sites case illustrate performance using conditional probability two xed nodes 20 21 table 8 depicts location parents nodes 20 21 case nodes 20 21 always assigned site 1 increasing number sites distributing parents node 21 among sites learning problem deliberately made dicult figure 9 depicts ckl distance nodes 20 21 dierent cases clearly increasing number sites make collective learning collective mining bayesian networks distributed heterogeneous data 21 fraction observations commuicated fraction observations commuicated fig 8 kl distance conditional probabilities alarm network parents n20 parents n21 table 8 location parents nodes 20 21 dierent cases dicult performance smaller number sites better larger number sites however ckl distance decreases rapidly even large number sites moreover performance approach similar increasing number sites relative small portion samples transmitted gure 9 35 samples transmitted clearly illustrates approach scales well respect number sites 5 discussions conclusions presented approach learning bns distributed heterogenous data based collective learning strategy local model obtained site global associations determined selective transmission data central site experiments performance collective bn quite comparable obtained centralized approach even small data communication knowledge rst approach learning bns distributed heterogenous data experiments suggest collective learning scales well respect number sites samples features many interesting applications possible bn model web log data example specic structures overall bn would indicate special user patterns could used identify new user patterns accordingly personalize oers services pro 22 r chen et al fraction observations commuicated fraction observations commuicated fig 9 kl distance conditional probabilities alarm network dierent number sites vided users another interesting application classify users dierent groups based usage patterns thought decomposing overall bn obtained log data collective learning number subbns subbn representing specic group similar preferences actively pursuing ideas would report results future publication discuss limitations proposed approach suggest possible directions future work hidden node local sites certain network structures may possible obtain correct local links based local data site example consider asia model shown figure 1 observations corresponding variables e x available site corresponding variables l b available site b case learn local bn site b would expect false edge node l node edges overall bn fact node e hidden unobserved site b veried experimentally well however crosslinks detected correctly central site using selectively sampled data therefore necessary reexamine local links discovering crosslinks words postprocessing resulting overall bn required eliminate false local edges done evaluating appropriate score metric bn congurations without suspect collective mining bayesian networks distributed heterogeneous data 23 local links currently pursuing issue note however encounter problem examples presented section 4 assumptions data mentioned earlier assume existence key links observations across sites moreover consider simple heterogenous partition data variable set dierent sites nonoverlapping also assume data stationary data points come distribution free outliers simplifying assumptions derive reasonable algorithm distributed bayesian learning suitable learning strategies would allow us relax assumptions would important area research structure learning even data centralized learning structure bn considerably involved estimating parameters probabilities associated network distributed data scenario problem obtaining correct network structure even pronounced hidden node problem discussed earlier one example centralized case prior domain knowledge local site form probabilistic independence direct causation would helpful experiments asia model demonstrate proposed collective bn learning approach obtain network structure reasonable least simple cases however beginning deserves careful investigation parameter learning proposed collective method designed structure parameter learning likelihood computation local site trivial job introduces computational overhead may acceptable realtime applications online monitoring stock market data real time online learning applications need considerably reduce amount computation local sites towards end proposed new collective learning method chen sivakumar 2002 learning parameters bn assumes structure bn known dramatically reduces local computation time performance bounds approach selective sampling data maybe evidence crossterms reasonable based discussion section 3 see eq 47 veried experimentally three examples section 4 currently working towards obtaining bounds performance collective bn compared obtained centralized approach function data communication involved acknowledgements work partially supported nasa cooperative agreement ncc 21252 would like thank anonymous reviewers valuable comments suggestions r polynomial learnability probabilistic concepts respect kullbackleibler divergence update rules parameter estimation bayesian networks alarm monitoring system case study two probabilistic inference techniques belief networks properties bayesian network learning algorithms empirical analysis predictive algorithms collaborativeltering theory renement bayesian networks bayesian classication autoclass theory results new algorithm learning parameters bayesian network distributed data learning belief networks data information theory based approach learning equivalence classes bayesian network structure frauduncollectable debt detection using bayesian network based learning system rare binary outcome mixed data structures bayesian structural em algorithm sequential update bayesian network structure sample complexity learning bayesian networks markov chain monte carlo practice tutorial learning bayesian networks learning bayesian networks unication discrete introduction bayesian networks collective data mining new perspective toward distributed data mining distributed cooperative bayesian learning strategies model selection accounting model uncertainty graphical models using occams window estimating dependency structure hidden variable distributed data mining algorithms learning bayesian networks incomplete data comparison induction algorithms selective nonselective bayesian classiers construction bayesian networks databases based mdl scheme accelerated quantication bayesian networks incomplete data bugs program perform bayesian inference using gibbs sampling speech recognition dynamic bayesian networks author biographies insert photo krishnamoorthy sivakumar assistant professor school electrical engineering computer science advances distributed parallel knowledge discovery correspondence oprint requests k tr ctr yutao guo jorg p muller multiagent collaborative learning distributed business systems proceedings third international joint conference autonomous agents multiagent systems p11561163 july 1923 2004 new york new york satoshi morinaga kenji yamanishi junichi takeuchi distributed cooperative mining information consortia proceedings ninth acm sigkdd international conference knowledge discovery data mining august 2427 2003 washington dc