practical approach dynamic load balancing abstractthis paper presents cohesive practical load balancing framework improves upon existing strategies techniques portable broad range prevalent architectures including massively parallel machines cray t3de intel paragon shared memory systems silicon graphics powerchallenge networks workstations part work adaptive heat diffusion scheme presented well task selection mechanism preserve improve communication locality unlike many previous efforts arena techniques applied two largescale industrial applications variety multicomputers process work exposes serious deficiency current load balancing strategies motivating work area b introduction number trends computational science engineering increased need effective dynamic load balancing techniques particular particleplasma simulations recently become common generally less favorable load distribution characteristics continuum calculations navierstokes flow solvers even continuum problems use dynamically adapted grids moving boundaries solution resolution necessitates runtime load balancing maintain efficiency past ten years researchers proposed research sponsored advanced research projects agency contract number dabt6395 c0116 number strategies dynamic load balancing 2 3 4 5 6 7 9 10 11 17 19 20 22 23 24 goal work build upon best methods develop new algorithms remedy shortcomings previous efforts techniques designed scalable portable easytouse improvements existing algorithms include derivation faster diffusive scheme transfers less work achieve balanced state algorithms mechanisms selecting transferring tasks also introduced techniques attempt maintain improve communication locality underlying application parametric studies illustrate benefits offered faster diffusion algorithm well efficacy locality preservation techniques finally framework applied two largescale applications running hundreds processors success methods one case demonstrates utility techniques failure second application motivates research area revealing limitations current approaches abstract goal load balancing stated follows given collection tasks comprising computation set computers tasks may executed find mapping tasks computers results computer approximately equal amount work mapping balances workload processors typically increase overall efficiency computation increasing overall efficiency typically reduce run time computation considering load balancing problem important distinguish problem decomposition task mapping problem decomposition involves exploitation concurrency control data access algorithm result decomposition set communicating tasks solve problem parallel tasks mapped computers way best fits problem one concern task mapping computer roughly equal workload load balancing problem stated cases computation time associated given task determined priori circumstances one perform task mapping beginning computation called static load balancing important increasingly common class applications workload particular task may change course computation cannot estimated beforehand applications mapping tasks computers must change dynamically computation practical approach dynamic load balancing divide problem following five phases estimate computers load must provided first determine load imbalance exists estimates workloads associated individual tasks must also maintained determine tasks transferred best balance computation profitability determination loads computers calculated presence load imbalance detected cost imbalance exceeds cost load balancing load balancing initiated calculation based measurements taken first phase ideal work transfers necessary balance computation calculated tasks selected transfer exchange best fulfill vectors provided previous step task selection typically constrained communication locality task size considerations selected tasks transferred one computer another state communication integrity must maintained ensure algorithmic correctness decomposing load balancing process distinct phases one experiment plug andplay fashion different strategies steps allowing space techniques fully readily explored also possible customize load balancing algorithm particular application replacing general methods specifically designed certain class computations 3 assumptions notation following assumptions made regard scenario techniques herein applied first computers assumed homogeneous processing capac ity sources external load underlying software system must provide basic messagepassing library simple pointtopoint communication send receive op erations basic global operations global sum example finally access accurate milli microsecond level system clock must provided following variables notations used denote various quantities system ffl p computers system ffl network connecting computers ddimensional mesh torus sizes dimensions q ffl diameter network denoted length longest path two computers network according routing algorithm used eg ddimensional mesh would assuming messages routed fully dimension proceeding next ffl mapping function tasks respective computers called thus mi computer task mapped set tasks mapped computer ffl set neighbors either computer task denoted n appropriate context used neighbors computer adjacent physical network neighbors task tasks communicates 4 algorithms implementation section presents design choices implementation methodology outlined section 2 choices among algorithms techniques motivated necessary 41 load evaluation usefulness load balancing scheme directly dependent quality load measurement prediction accurate load evaluation necessary determine load imbalance exists calculate much work transferred alleviate imbalance ultimately determine tasks best fit work transfer vectors load evaluation performed either completely application completely load balancing system mixture application system facilities primary advantage applicationbased approach predictive power application developer direct knowledge algorithms inputs best chance determining future workload task finiteelement solver example load may function number grid cells number cells changes due grid adap tation news change immediately propagated load balancing system complex applications disadvantage approach determining abstract workload task translates actual cpu cycles system dependent factors cache size virtual memory paging easily skew execution time task large factor one way overcome performance peculiarities particular architecture measure load task directly timing one use timing facilities profile task providing accurate measurements categories execution time communication overhead timings easily provided library runtime system systems label execution time communication operations runtime execution time actually sending receiving data communication time systemonly approach may fall short comes load prediction however past behavior may poor predictor future performance applications load evolves relatively smooth fashion data modeling techniques data modeling statistics robust curve fitting used robust techniques tolerance noise example discarding spurious values rigorous way however load evolves highly unpredictable manner given system knowledge quantities affecting load additional information may required robust flexible approach perhaps hybrid application system methods combining applicationspecific information system timing facilities much practical predict performance complex application particle simulation example time required one iteration partition problem may function number grid cells well number particles contained grid cells using timing routines application determine weight predicting execution time next iteration given limitations applicationonly systemonly approaches general purpose load balancing framework must allow use applicationspecific load prediction model provide profiling routines necessary make model accurate system provide set generic models adequate broad classes applications experience simple techniques keeping enough load history predict load linear quadratic function often sufficient case system provide feedback quality load prediction model used load predictions inaccurate relative actual run times system generate appropriate warnings whatever load prediction model used output load evaluation following given task j workload task determined l j load computer therefore l j 42 load balance initiation load balancing useful one must first determine load balance comprised two phases detecting load imbalance exists determining cost load balancing exceeds possible benefits load balance efficiency computation ratio average computer load maximum computer load load balancing framework might therefore consider initiating load balancing whenever efficiency computation userspecified threshold eff min applications total load expected remain fairly con stant load balancing would undertaken cases load computer exceeds lavg eff min l avg calculated initially provided application similar approach described 10 11 22 load balancing initiated whenever com puters load falls outside specified upper lower limits method poorly suited situations total load changing example system initially balanced load every computer doubles system still balanced method would load cause load balancing initiated eff min less 50 percent another method suggested load balance difference computers load local load average ie average load computer neighbors exceeds threshold 22 problem technique may fail guarantee global load balance consider example case linear array computer load il const local load average nonextremal computers would igamma1ii1 const load balancing would initiated even small threshold despite fact global efficiency 50 percent ie l const const load balancing would initiated extremal computers relative threshold would unreasonably small even moderate values eff min large arrays analysis applies case computer would initiate load balancing whenever relative difference load one neighbors exceeded threshold guarantee efficiency eff min relative difference must general less problem tight bound many cases violated load balancing may actually unnecessary reason adhoc methods suggested inexpensive completely local also introduce synchronization point otherwise asynchronous ap plication certainly qualities strive given increasing availability threads asynchronous communication facilities global load imbalance detection may less costly previously perceived using separate load balancing thread computer load imbalance detection phase overlapped application load balancing threads synchronize would affect application thus simplest way determine load balance may calculate maximum average computer loads using global maximum sum operations complete olog 2 p steps architec tures using quantities one calculate efficiency directly even load imbalance exists may better load balance simply cost load balancing would exceed benefits better work distribution time required load balance measured directly using available facilities expected reduction run time due load balancing estimated loosely assuming efficiency increased eff min precisely maintaining history improvement past load balancing steps expected improvement exceeds cost load balancing next stage load balancing process begin 22 43 work transfer vector calculation determining advantageous load balance one must calculate much work ideally transferred one computer another interest preserving communication locality transfers undertaken neighboring computers transfer vector algorithms presented literature three particular stand hierarchical balancing method generalized dimensional exchange diffusive techniques hierarchical balancing hb method global recursive approach load balancing problem 7 22 algorithm set computers divided roughly half total load calculated partition work transfer vector partitions required make load per computer equal ie one partition p 1 computers total workload l 1 another partition p 2 computers aggregate workload transfer first partition second given deltal 1 transfer vector calculated partition divided balanced recur sively taking account transfers calculated higher levels hb algorithm calculates transfer vectors required achieve perfect load balance olog 2 p steps one disadvantage hb method data transfer two partitions occurs single point may acceptable linear array tree networks fail fully utilize bandwidth highly connected networks simple generalization hb method meshes tori perform algorithm separately dimension example 2d mesh computers column could perform hb method sulting row total load row resulting computer total load general ddimensional meshes tori algorithm requires steps note case hypercubes dimensional hierarchical balancing dhb method reduces dimensional exchange de method presented 2 22 de method computers hypercube pair neighbors dimension exchange half difference respective workloads results balance log steps authors 24 present generalization technique arbitrary connected graphs call generalized dimensional exchange gde network maximum degree jn max j links neighboring computers minimally colored computer two links color edge color computer exchanges neighbor across link times load difference process repeated balanced state reached deltal k1 deltal 0 particular case 05 gde algorithm called averaging gde method agde 24 agde method also presented 7 judged inferior hb method latters lower time complexity authors 24 also present method determining value algorithm converges rapidly call gde method using parameter optimal gde method ogde methods diffusionlike described diffusive literature 7 based diffusion authors 24 rightly point diffusive methods based solution diffusion equation l first presented method load balancing 2 diffusion also explored 22 found superior load balancing strategies terms perfor mance robustness scalability general diffusive strategy given 5 unlike previous work method uses fully implicit differencing scheme solve heat equation multidimensional torus specified accuracy advantage implicit scheme timestep size diffusion iteration limited dimension network explicit schemes timestep size limited 2 gammad ddimensional mesh torus algorithm 5 quickly decimates large load imbalances converges slowly smooth lowfrequency state reached one way overcome difficulty increase timestep size load imbalance becomes less severe rigorous technique borrowed work integrating ordinary differential equations odes 13 particular view problem system odes l apply two methods calculate ffil particular ffit recognize although matrix results spatial discretization curvature operator partial differential equation pde load balancing problem actually spatially discrete begin thus system odes instead pde analogy diffusion pde guides construction first method calculating ffil firstorder accurate implicit technique described 5 method produces local error offit 2 ffit timestep size secondorder accurate method 18 produces local error offit 3 thus take timestep methods difference values produced gives us estimate error ffit taking maximum difference computer denoted err max take relative error err rel errmax using error estimate proportional ffit 2 adjust ffit large possible achieve desired error ff err rel desired accuracy safety factor avoid readjust timestep size previous adjustment large resulting adaptive timestepping diffusion algorithm given figure 1 44 task selection work transfer vectors computers calculated necessary determine tasks moved meet vectors quality task selection directly impacts ultimate quality load balancing two options satisfying transfer vector two computers one attempt move tasks unidirectionally one computer another one exchange tasks two computers resulting net transfer work tasks average workload high relative magnitude transfer vectors may difficult find tasks fit vectors hand exchanging tasks one potentially satisfy small transfer vectors swapping two sets tasks roughly total load cases enough tasks oneway transfers adequate cost metric described used eliminate unnecessary exchanges problem selecting tasks move satisfy particular transfer vector weakly npcomplete since simply subset sum problem fortunately approximation algorithms exist allow subset sum problem solved specified nonzero accuracy polynomial time 12 considering algorithm important note concerns may constrain task transfer options particular one would like avoid costly trans diffuse deltal ij 0 neighbor j 2 n send l neighbors j 2 n receive l j neighbors j 2 n lavg neighbors k 1 send hl kgamma1 neighbors j 2 n receive hl kgamma1 j neighbors j 2 n end err errmax errmax send l neighbors j 2 n receive l j neighbors j 2 n deltal ij deltal ij neighbor j 2 n errmax diffuse figure 1 adaptive timestepping diffusion algorithm executed computer fers either large numbers tasks large quantities data unless absolutely necessary one would also like guide task selection preserve best possible existing communication locality application general one would like associate cost transfer given set tasks find lowest cost set particular desired transfer problem attacked considering problem related subset sum problem namely 01 knapsack problem latter problem one knapsack maximum weight capacity w set n items weights w values v respectively one seeks find maximumvalue subset items whose total weight exceed w context task selection one set tasks loads l transfer costs c important note l negative task transferred onto given computer c also negative actually advantageous transfer task computer given transfer deltal one wishes find minimumcost set tasks whose exchange achieves transfer one specify cost function cl minimum cost subset tasks 0 achieves net transfer l letting c0 0 zero cl 0 1 l 6 0 one find values cl computing order increasing following end lowest cost transfer l given cl n problem algorithm runtime 2 l max l max largest absolute value l algorithm therefore pseudopolynomial 12 one overcome difficulty approximating values l truncate lower b bits l l log ffll max relative deviation optimal load transfer l proof follows manner proof given 01 knapsack approximation algorithm 12 sake space produce run time thereby reduced n 2 l max positive nonzero ffl find lowestcost transfers polynomial time function cl n calculated question becomes transfer use value cl n finite l closest deltal without exceeding lowest cost transfer within ffl transfer actually closest deltal ie transfer would found exact search one might tempted take subset yielded value however using subset somewhat away deltal one potentially achieve much lower cost rigorous approach follows given target accuracy ffl define ffl ffl perform approximation algorithm accuracy ffl 0 lowest cost transfer closest within accuracy ffl 0 deltal take subset lowest cost within ffl 0 closest transfer lowest cost subset within ffl transfer actually nearest deltal next question determine target accuracy ffl general may unnecessary computer fully satisfy transfer vectors work transfer vectors given algorithms previous section eager algorithms specify transfer work instances may unnecessary case large point disturbance ex ample failure two computers far disturbance satisfy transfer vectors may little effect global load balance one way determining extent computer must satisfy transfer vectors following general computer set outgoing positive transfer vectors set incoming negative transfer vectors particular computer denote sum former deltal sum latter deltal gamma order achieve desired efficiency computer must guarantee new load less lavg eff min assuming incoming transfer vectors satisfied either necessity chance new load least l thus order guarantee new load less lavg eff min computer must leave fraction ffl outgoing transfer vectors unsatisfied according eff min solving maximum ffl gives eff min practice ffl max lower limit 10 gamma2 10 gamma3 since value zero possible especially case computer maximum load also note using ffl max approximation algorithm guarantee satisfactory exchange tasks found accuracy guarantee since exchange may impossible given set tasks instead merely provides guidance hard approximation algorithm try find best solution degree tradeoffs cheaper exchanges acceptable since selection algorithm cannot general satisfy transfer vector single attempt necessary make multiple attempts example worstcase scenario tasks one computer computers neighbors overloaded computer hope incoming transfer vectors satisfied first round exchanges case one would expect least od exchange rounds would necessary algorithm propose task selection thus follows transfer vectors colored manner described gde algorithm color every computer attempts satisfy transfer vector color adjusting ffl max account degree transfer vectors thus far fulfilled algorithm repeated colors exhausted termination occurs progress made reducing transfer vectors termination occur earlier computers satisfied minimum requirement outgoing transfer vectors ie ffl max one every computer first termination condition guaranteed met given configuration tasks minimum nonzero exchange total outstanding transfer vectors reduced least amount step since transfer vectors finite size algorithm terminate admittedly weak bound typical situations never seen task selection require iterationsat required od steps case severe load imbalance safe approach would bound number steps multiple selection algorithm suggests task may move multiple hops process satisfying transfer vectors movement may discouraged appropriate cost functions since data structures task may large storeandforward style remapping may prove costly better method instead transfer token contains information task load current location data structures task selection complete tokens arrived final destinations computers send tasks states directly final locations note cost function used encourages locality token may moved back computer corresponding task actually resides eliminating need data transfer 45 task migration addition selecting tasks move load balancing framework must also provide mechanisms actually moving tasks fromone computer another task movement must preserve integrity tasks state pending communication transport tasks state typically requires assistance application especially complex data structures linked lists hash tables involved example user may required write routines pack unpack free state task 5 parametric experiments section presents results various parametric experiments exposing tradeoffs different load balancing mechanisms particular comparisons drawn various transfer vector algorithms influence cost metrics task migration application locality demonstrated 51 comparison transfer vector algorithms transfer vector algorithms presented section 43 previously compared terms execution times 2 7 22 24 poorly studied exception experiments 22 amount work transfer algorithms require achieve load balance algorithms section 43 implemented using message passing interface 16 run 256 processors cray t3d hb algorithm mapped threedimensional torus architecture t3d partitioning network along largest dimension stage transferring work processors center plane division gde diffusion algorithms took advantage wraparound connections figure 2 compares total work transfer execution times transfer vector algorithms varying numbers processors case randomly chosen computer contained work system transfer vector algorithms improved efficiency least percent scenario intended illustrate worstcase behavior algorithms case much analysis algorithms done figure 3 quantities compared except load continuous random variable distributed uniformly 08 12 goal illustrate algorithms performance characteristics realistic situationin particular balance maintenance figure shows exception hb method algorithms transferred fairly judicious amount work diffusion agde algorithms transferred least work dhb ogde algorithms transferring 30 12 percent work respec total work transfer number processors ogde diff1 diff201003005007009050 100 150 200 250 300 time sec number processors ogde diff1 diff2 figure 2 worstcase total work transfer left execution times right various transfer vector algorithms varying numbers processors tively diff1 denotes diffusion algorithm presented 5 diff2 diffusion algorithm presented case agde algorithm seems best bet transferring amount work diffusion algorithms least ten times faster basis gde algorithm considered superior diffusion 24 however typical case figure 3 tells somewhat different story case see diffusion algorithms transferred least work specifically algorithms transferred 127 percent work case hb method 80 percent dhb technique percent agde 60 percent ogde number processors grew however speed advantage nondiffusive algorithms much less apparent point disturbance scenario given transfer tasks quite costly applications involving gigabytes data small performance advantage 14 milliseconds case offered nondiffusive algorithms questionable value important points note although ogde algorithm somewhat faster agde algorithm proponents 24 shown transferred around 20 percent work test cases also despite speed hb algorithm primary consideration 7 algorithm transfers extraordinary amount work order achieve load balance also illustrated 22 thus appears little recommend except perhaps case tree linear array networks total work transfer number processors ogde diff1 diff200200601001450 100 150 200 250 300 time sec number processors ogde diff1 diff2 figure 3 averagecase total work transfer left execution times right various transfer vector algorithms varying numbers processors 52 task movement reduction locality preservation cost used constrain task movement prodigious number tasks often trans ferred transfer tasks negatively impact communication locality following experiments demonstrate providing appropriate cost function task move ment one drastically reduce impact load balancing application task movement deemed free large number tasks often transferred order achieve load balance example 100 trials artificial computation 256 nodes intel paragon 10 tasks per node mean efficiency 70 percent average 638 tasks transferred achieve efficiency least 90 percent certainly one would expect 25 percent tasks needed transferred improvement setting transfer cost task one instead zero average number tasks transferred reduced factor four 160 approximately six percent tasks system reducing size tasks transferred may prove important reducing number tasks transferred example may less expensive transfer two small tasks single much larger one experiment size tasks data structures uniformly distributed interval 128 512 kilobytes taking tasks transfer cost size data structures reduced average time migrate tasks 84 38 seconds similar results obtained simulation silicon wafer manufacturing reactor running network 20 workstations application briefly described section 62 case using unit task transfer cost reduced transfer time 50 percent zero cost using tasks sizes transfer cost reduced transfer time 61 percent another concern transfer tasks transfers disrupt communication locality application communication costs application significantly increased relocating tasks far tasks communicate may better load balance random load conditions several localitypreserving cost metrics compared first case tasks transfer cost taken change distance actual location data structures proposed new location ie transfer task old dist function gives network distance two computers old cur new original task mapping current proposed task remapping new proposed task remapping respectively short cost transfer positive increases distance proposed new location task old location cost negative distance decreases cost takes nothing account regarding location tasks communicants task moved away neighbors encouragement move back thus one would expect metric retard locality degradation prevent another metric considered cost change tasks distance original location computation first started case task encouraged move back began locality good begin one would expect metric preserve locality one would expect improve locality poor initially final cost metric used based idea center communication words task ideal computer relocate determined finding center minimized old j v ij cost communication tasks j twodimensional mesh example one would calculate weighted average rowcolumn locations tasks neighbors cost moving task change distance ideal location center course tasks neighbors moving time ideal location changing somewhat selection process cases however one would expect ideal location task change greatly even neighbors move somewhat one would expect metric would improve poor locality well maintain existing locality three metrics described compared zerocost metric synthetic computation similar described computation begun 16 mesh paragon nodes 10 tasks tasks connected threedimensional grid task average two neighbors local computer one neighbor four adjacent computers thus initial locality high load balancing brought efficiency 90 percent task loads changed efficiency reduced around 70 percent task would calculate average distance neighbors figure 4 shows average distance function number load balancing steps one see locality decays rapidly attempt made maintain first cost metric slows decay prevent second third metrics limit increase average distance metric factors 21 26 respectively case also presented figure 4 locality poor initiallytasks assigned random computers third metric used improve locality ultimately reduced average distance communicating tasks 79 percent within 23 percent locality obtained problem started high locality figures show cost metric tremendous impact locality application metrics used fairly simple complex metrics might yield even better results 6 applications experiments load balancing algorithm presented section 4 applied two largescale applications running scalable concurrent programming library formerly called 20601000 200 400 600 800 1000 average distance number load balancing steps zerocost distcur distorig average distance number load balancing steps distcenter figure 4 average distance communicating tasks function load balancing steps various locality metrics left improvement initially poor locality right concurrent graph library 18 chapter gives brief overview programming library well applications including algorithms specific problems applied also provides performance numbers load balancing demonstrating practical efficacy load balancing framework one application exposing interesting problem second case 61 scalable concurrent programming library scalable concurrent programming library scplib provides basic programming technology support irregular applications scalable concurrent hardware scplib tasks communicate one another unidirectional channels mapping tasks computers controlled library hidden user communication channels since mapping work computers explicit possible dynamically change mapping long user provides mechanism sending receiving context task ie tasks state scplib uses general abstraction user reuse existing checkpointing routines readwrite data fromto communication port instead file port figure 5 shows example computational graph mapping set computers well schematic representation software structure individual task functionality layered top systemspecific messagepassing io thread synchronization routines result small implementation interface porting li routines routines physics state user comm figure 5 computational graph nine tasks represented shaded discs mapped onto four user portion comprised tasks state routines act upon library portion comprised communication list auxiliary routines load balancing granularity control visualization functions braries new architecture typically requires days fact libraries used wide range distributedmemory multicomputers cray t3d t3e intel paragon avalon a12 sharedmemory systems silicon graphics powerchallenge origin 2000 networked workstations pcs latter running windows nt 62 plasma reactor simulations direct simulation monte carlo dsmc technique simulation collisional plasmas rarefied gases dsmc method solves boltzmann equation simulating individual particles since impossible simulate actual number particles realistic system small number macroparticles used representing large number real particles simulation millions macroparticles made practical decoupling interactions first space particles move divided grid collisions considered particles within grid cell furthermore collisions detected path intersections rather approximated stochastic model parameters relative velocities particles question statistical methods used recover macroscopic quantities temperature pressure limiting simplifying dsmc compute move particles send away particles exit current partition receive particles neighboring partitions collide particles gatherscatter obtain global statistics calculate termination condition based global statistics converged figure concurrent dsmc algorithm interactions fashion order computation drastically reduced hawk threedimensional concurrent dsmc application used model neutral flow plasma reactors used vlsi manufacturing 14 18 dsmc algorithm executes partition problem given figure 6 task concurrent graph represents partition physical space executes algorithm state task collection grid cells particles contained region physics routines incorporate associated collision chemistry surface models communication list used implement interpartition transfers resulting particle motion gaseous electronics conference gec reactor standard reactor design studied extensively early version hawk used regular hexahedral grids simulation gec reactor conducted 580000cell grid cells 330000 cells represented regions reactor particles may move remaining dead particleless cells comprise regions outside reactor simulations 28 million particles conducted using grid description details 57 percent grid cells actually contained particles even cells contain particles density varied order magnitude consequently one would expect standard spatial decomposition mapping grid would result inefficient computation indeed case gec grid divided 2560 partitions mapped onto 256 processors intel paragon wide variance particle density partition overall efficiency computation quite low approximately 11 percent efficiency improved 86 percent load balancing including cost load balancing resulted 87 percent reduction run time figure 7 shows corresponding improvement workload distribution recent version hawk code uses irregular tetrahedral grids simulation conducted 124000cell grid gec reactor problem run 128 processors intel paragon processor approximately five partitions mapped although load changed rapidly early timesteps number particles increased zero 12 million load balancing able maintain efficiency 82 percent reducing runtime factor 26 load balancing problem required average 12 seconds per attempt hawk also used simulation proprietary reactor designs intel cor poration simulations conducted networks 10 25 ibm rs6000 workstations without load balancing efficiency computations typically percent load balancing able maintain efficiency 80 percent increasing throughput much factor two many load balancing techniques described paper also incorporated another dsmc code developed researchers russian institute theoretical applied mechanics 8 case however work transfer vectors satisfied transferring entire partitions fromone computer another rather exchanging small groups cells along partition interfaces adjacent computers feature approach locality naturally maintained since one effect adjusting partition bound aries problem space capsule reentry running 256 processors intel paragon percent linear speedup obtained dynamic load balancing versus 55 percent ideal speedup random static mapping 10 percent ideal speedup load bal ancing interesting note random static mapping actually achieved fairly good load balance communication widely distributed cells costly reducing scalability 63 ion thruster simulations particleincell pic computational technique used simulating highly rarefied particle flows presence electromagnetic field fundamental feature pic order 0number processors percent utilization figure 7 utilization distributions 100 time steps dsmc code load balancing reducing method calculating interaction particles field grid superimposed computational domain electromagnetic effect particle respect vertices grid cell containing calculated governing field equations solved grid points typically using iterative solver field solver converged effects new field propagated back particles adjusting trajectories ac cordingly reciprocal interaction calculated repeatedly throughout computation termination criteria particle concentration reached scalable concurrent programming laboratory collaboration space power propulsion laboratory mit department aeronautics astronautics developed 3d concurrent simulation capability called plumepic 15 pic algorithm partition problem presented figure 8 state associated task comprised portion grid particles contained within corresponding physical space communication list associated task graph describes possible destinations particles move outside partition data dependencies required implement field solver physics routines used figure 8 describe dynamics particle movement solution field phenomenon ion thruster backflow studied simulation esexargos satellite configuration using parameters hughes thruster grid used contained 94 million axially aligned hexahedra partitioned 1575 blocks mapped onto pic compute time exhausted move particles send away particles exit current partition receive particles neighboring partitions update charge density based new particle positions gatherscatter obtain global norm calculate termination condition based global norm global norm termination condition send boundary potentials neighbors receive boundary potentials neighbors compute single iteration field solver gatherscatter obtain new global norm pic compute figure 8 concurrent pic algorithm 256processor cray t3d course simulation 34 million particles moving domain distribution particles highly irregular moreover distribution changed dramatically time result static mapping grid partitions computers would result large inefficiencies point computation fact illustrated figure 9 shows computer spends large percentage time idle even load balancing idle time computer often better still high certainly load balancing algorithm improve work distribution extent dsmc code closer examination reveals shortcoming due twophase nature pic code dsmc application singlephase com putation load balancing fairly straightforward pic code two phases particle transport field solution different load distribution characteristics result balancing total load two phases given computer balance individual phases computation fact graphically illustrated figure 10 one see load distribution total load computer improved dramatically least sense variance greatly reduced load distributions two component phases remained poor consequently overall efficiency low see effect smaller scale consider case two computers shown figure 11 one 50 units number time sec idle communication particle push field solve figure 9 run time breakdowns 100 time steps pic code starting several different time steps pair adjacent bars show average time components computer load balancing respectively decrease field solve time last time step due fact field reaching steady state hence iterative solver converges quickly50150250 number processors percent utilization total field solve particle push50150250 number processors percent utilization total field solve particle push figure 10 preload balancing left postload balancing right utilization distributions computers based total work field solver work particle push work phase one work 100 units phase two work computer 100 phase one 50 phase two units obviously computers total amount work however synchronization completion phase one computers phase two begin computation inefficient first computer must wait second start phase two second computer must wait first computation complete examples suggest one needs load balancing strategy jointly balances phase computation impractical alternate two distributions example phases may finely interleaved making cost frequent redistribution work prohibitive one way consider load vector instead scalar vector component load phase compu tation component balanced separately problems encountered would circumvented computer would roughly equal amount work phase implying total amount work also equal hence little idle time would occur synchronization points phases notice characteristics pic code also imply general one must assign multiple partitions computer regions grid high particletocell ratio partition region must paired partition low particletocell ratio achieve effective load balancing phases similar situation illustrated figure 12 shows simple domain cannot divided two contiguous pieces balance loads two computers mapped 7 related work presented summary work related methodology techniques used paper gradient load balancing methods explored extensively literature 10 11 22 pointed 11 22 basic gradient model may result undertransfers work lightly loaded processors authors 11 present workaround computers check underloaded processor still underloaded committing transfer conducted directly overloaded underloaded processor method scalability diffusive gde strategies shown inferior per computer 1 computer 2 phase 2 phase 2 phase 1 computer 1 computer 2 phase 1 add synchronization figure 11 demonstration low efficiency balanced system example computers total amount work ie load balanced sense ever synchronization interposed unbalanced phases idle time introduced formance 22 recursive bisection methods operate partitioning problem domain achieve load balance reduce communication costs presentations techniques appear context static load balancing 1 23 although formulations appropriate dynamic domain repartitioning exist 19 20 many methods exist repartitioning computation including various geometrically based techniques interesting methods utilize spectral properties matrix encapsulating adjacency computation unfortunately methods fairly high computational cost also blur distinct phases load balancing presented section 1 combination limitations makes techniques unsuitable use general purpose load balancing framework heuristics load balancing particle simulations relevant two applications targeted section presented 4 9 interesting note authors 4 observed phenomenon applying method pic application seen section 63 namely methods worked well particle push phase substantially dominated field solve phase due fact imbalance field phase completely neglected inherently scalar approach authors suggested remedy situation however taskbased approaches load balancing include scalable task pool 6 heuristic phase 2 imbalanced balanced phase 1 computer 1 computer 2 phase 2 phase 1 figure 12 bars represent onedimensional space phase one dominates one end phase two dominates domain cannot divided evenly two computers single cut cut middle would balance total load neither component phases would balanced cut anywhere else might either balance first second phase way achieve balance assign multiple partitions computer transferring tasks computers based probability vectors 3 scalable iterative bidding model 17 techniques make assumptions complete task independence task load uniformity applicable context work 8 conclusion future work paper describes practical comprehensive approach load balancing applied nontrivial applications incorporated approach new diffusion algorithm offers good tradeoff total work transfer run time task selection mechanism allows task size communication costs guide task movement work remains done however following three areas improvement could dramatically increase effectiveness utility strategy presented consider load vector rather scalar quantity experiments pic code section 6 clearly demonstrate limitations scalar view load load balancing algorithm clearly achieved good balance total load computer failed balance components load result overall efficiency low jointly balancing phases comprising computation one hope achieve good overall load balance viewing load vector one way accomplish 21 load balancing heterogeneous case case computers heterogeneous processing capacity relative capabilities computers must taken account work movement decisions load diffusion algorithm situation analogous heat diffusion heterogeneous media task selection must also modified account change tasks runtime migrates one computer another use dynamic granularity control taskbased load balancing strategies fail whenever load single task exceeds average load computers matter task moved computer mapped overloaded dividing task smaller subtasks one alleviate problem providing viable work movement options general task division conglomeration used dynamically manage granularity computation maintain best number tasks increasing decreasing available options necessary incorporating changes load balancing framework could applied greater variety situations meantime methods described useful fine medium grain singlephase applications running homogeneous computing resources acknowledgements access intel paragon provided california institute technology center advanced computing research access cray t3d provided national aeronat ics space administration jet propulsion laboratory facilitated california institute technology r fast multilevel implementation recursive spectral bisection partitioning unstructured problems dynamic load balancing distributed memory multiprocessors dynamic load balancing using tasktransfer probabilities dynamic load balancing 2d concurrent plasma pic code parabolic load balancing algorithm distributed implementation task pool multilevel diffusion method dynamic load balancing parallel dsmc strategies 3d com putations dynamic load balancing parallelized particle simulations mimd com puters gradient model load balancing method parallel loadbalancing extension gradient model computational complexity numerical recipes c concurrent simulation plasma reac tors threedimensional plasma paricleincell calculations ion thruster backflow contamination mpi complete ref erence partially asynchronous iterative algorithm distributed load balancing concurrent improved spectral bisection algorithm application dynamic load balancing dynamic loadbalancing pde solvers adaptive unstructured meshes load balancing technique multiphase computa tions strategies dynamic load balancing highly parallel computers performance dynamic load balancing algorithms unstructured mesh calculations load balancing parallel computers tr ctr j ray optimization distributed objectoriented systems addendum 2000 proceedings conference objectoriented programming systems languages applications addendum p153154 january 2000 minneapolis minnesota united states javier roca j carlos ortega j antonio lvarez julia mateo data neighboring local load balancing operations proceedings 9th wseas international conference computers p16 july 1416 2005 athens greece k hering j lser j markwardt dibsim parallel functional logic simulator allowing dynamic load balancing proceedings conference design automation test europe p472478 march 2001 munich germany bin fu zahir tari dynamic load distribution strategy systems high task variation heavy traffic proceedings acm symposium applied computing march 0912 2003 melbourne florida corts ripoll f ced senar e luque asynchronous iterative load balancing algorithm discrete load model journal parallel distributed computing v62 n12 p17291746 december 2002 marco conti enrico gregori fabio panzieri qosbased architectures geographically replicated web servers cluster computing v4 n2 p109120 april 2001 k antonis j garofalakis mourtos p spirakis hierarchical adaptive distributed algorithm load balancing journal parallel distributed computing v64 n1 p151162 january 2004 g george yin chengzhong xu le yi wang optimal remapping dynamic bulk synchronous computations via stochastic control approach ieee transactions parallel distributed systems v14 n1 p5162 january fong chengzhong xu le yi wang optimal periodic remapping dynamic bulk synchronous computations journal parallel distributed computing v63 n11 p10361049 november hansheinrich ngeli dynamic load balancing diffusion heterogeneous systems journal parallel distributed computing v64 n4 p481497 april 2004 lapsun cheung yukwok kwok load balancing approaches distributed object computing systems journal supercomputing v27 n2 p149175 february 2004 alexander e kostin isik aybay gurcu oz randomized contentionbased loadbalancing protocol distributed multiserver queuing system ieee transactions parallel distributed systems v11 n12 p12521273 december 2000 yukwong kwok lapsun cheung new fuzzydecision based load balancing system distributed object computing journal parallel distributed computing v64 n2 p238253 february 2004 saeed iqbal graham f carey performance analysis dynamic load balancing algorithms variable number processors journal parallel distributed computing v65 n8 p934948 august 2005 arnaud legrand hlne renard yves robert frdric vivien mapping loadbalancing iterative computations ieee transactions parallel distributed systems v15 n6 p546558 june 2004 changxun wu randal burns handling heterogeneity shareddisk file systems proceedings acmieee conference supercomputing p7 november 1521 faouzi kamoun toward best maintenance practices communications network management international journal network management v15 n5 p321334 september 2005 kirk schloegel george karypis vipin kumar wavefront diffusion lmsr algorithms dynamic repartitioning adaptive meshes ieee transactions parallel distributed systems v12 n5 p451466 may 2001 changxun wu randal burns tunable randomization load management shareddisk clusters acm transactions storage tos v1 n1 p108131 february 2005 jack dongarra ian foster geoffrey fox william gropp ken kennedy linda torczon andy white references sourcebook parallel computing morgan kaufmann publishers inc san francisco ca