efficient solution cache thrashing problem caused true data sharing abstractwhen parallel programs executed multiprocessors private caches set data may repeatedly used modified different threads data sharing often result cache thrashing degrades memory performance paper presents evaluates loop restructuring method reduce even eliminate cache thrashing caused true data sharing nested parallel loops method uses compiler analysis applies linear algebra theory numbers subscript expressions array references due methods simplicity efficiently implemented parallel compiler experimental results show quite significant performance improvements existing static dynamic scheduling methods b introduction parallel processing systems memory hierarchies become quite common today commonly multiprocessor systems local cache processor bridge speed gap processor main memory systems use multilevel caches 5 14 often copyback snoopy cache protocol employed maintain cache coherence multiprocessor systems certain supercomputers also use local memory viewed programcontrolled cache programs nested parallel loops executed parallel processing systems important assign parallel loop iterations processors way unnecessary data movement different caches minimized convenience paper call iteration parallel loop thread following loop nest example i1100 k1100 enddo enddo example loop j parallel loop value fixed statement loopcarried dependences j varies loop j executed 100 times loop nest creating 10000 iterations 10000 threads total thread addresses 100 elements array many array elements repeatedly accessed threads shown table 1 figure 1 ij denotes thread corresponding loop index values denotes set threads created index value shown figure 2 exist lists threads 11 12 21 thread modifies reuses array elements accessed neighboring threads list threads list assigned different processors data array unnecessarily move back forth different caches system causing cache thrashing problem due true data sharing 12 nested loop construct shown example quite common parallel code used scientific computation j fang lu studied large number programs including linpack benchmarks perfect club benchmarks programs mechanical cae computational chemistry image signal processing petroleum applications 11 reported almost timeconsuming loop nests contain least three loop levels 60 contain least one parallel loop even using loop interchange move parallel loops outwards legal still found 94 parallel loops enclosed sequential loops loop nests include cases parallel loop appears outermost loop level subroutine subroutine called callstatement contained sequential loop loop nests perfectly nested ie exist statements right inner loop fang lu proposed thread alignment algorithm solve cache thrashing problem may created multinested loops algorithm however assigns threads processors either solving linear equations run time storing precomputed numerical solutions equations processors memory since storing numerical solutions requires quite large memory space since exact number threads often cannot determined statically due unknown loop bounds favor online computation approach paper present method reduce runtime overhead fang lus algorithm using thorough compiler analysis array references derive closedform formula solves thread assignment equations thread assignment becomes highly efficient run time previously presented preliminary algorithms 22 23 deal simple case datadependent array references use linear function subscripts experimental data given paper extend work covering multiple linear functions clarifying underlying theory report experimental results using silicon graphics sgi multiprocessor table 1 elements accessed thread method compiler analyzes data dependences threads uses information restructure nested loop perfectly nested otherwise order reduce even eliminate true data sharing causes cache thrashing method efficiently implemented parallel compiler experimental results show quite significant improvement existing static dynamic scheduling methods follows first address related work introduce basic concepts assumptions present solutions cache thrashing problem due true data sharing lastly show experimental results conducted sgi multiprocessor system related work extensive research regarding efficient memory hierarchies reported literature abusufah kuck lawrie use loop blocking improve paging performance improving locality references 2 wolfe proposes iteration space tiling way improve data reuse cache local memory 35 gallivan jalby gannon define reference window dependence variables referenced source sink dependence 15 16 executing source dependence save associated reference window cache sink also t13 figure 1 elements accessed different threads executed may increase number cache hits carr callahan kennedy 7 8 discuss options compiler control uniprocessors memory hierarchy wolf lam develop algorithm estimates temporal spatial reuse given loop permutation 34 optimizations attempt maximize reuse cached data single processor also secondary effect improving multiprocessor performance reducing bandwidth requirement processor thereby reducing contention memory system contrast work considers multiprocessor environment processor local cache local memory different processors may share data work peir cytron 28 shang fortes 30 dhollander 9 share common goal partitioning index set independent execution subsets corresponding loop iterations execute different processors without interprocessor communication methods apply specific type loop nest called uniform recurrence uniform dependence algorithm loops perfectly nested loop bounds constant loopcarried dependences constant distances array subscripts form loop index c integer constant hudak abraham 1 18 develop static partitioning approach called adaptive data partitioning adp reduce interprocessor communication iterative dataparallel loops also assume perfectly nested loops loop body restricted update single data point ai twodimensional global matrix subscript expressions righthand side array references restricted sum parallel loop index small constant subscript expressions lefthand array references restricted contain parallel loop indices tomko abraham t13 t1003 figure 2 lists threads accessing similar array elements 32 develop iteration partitioning techniques dataparallel application programs assume one pair data access functions loop index variable appear one dimension array subscript expression agarwal kranz natarajan 3 propose framework automatically partitioning parallel loops minimize cache coherence traffic sharedmemory multiprocessors restrict discussion perfectly nested doall loops assume rectangular iteration spaces unlike previous works work considers nested loops necessarily perfectly nested loop bounds variables array subscript expressions much general many researchers studied cache false sharing problem 10 17 19 33 cache thrashing occurs different processors share cache line multiple words although processors share word many algorithms proposed reduce false sharing better memory allocation better thread scheduling program transformations work considers cache thrashing due true sharing data words work closely related research done fang lu 11 12 26 13 work iteration space partitioned set equivalence classes processor uses formula determine iterations belong equivalence class execution time processor executes corresponding iterations reduce eliminate cache thrashing iterations solution vectors linear integer system fang lus work vectors may either computed run time may precomputed later retrieved run time loop bounds known execution approaches require additional execution time processor fetches next iteration unlike fang lus approaches solve thrashing problem compile time reduce runtime overhead achieve effect reducing cache thrashing new method restructures loops compile time based thorough analysis relationship array element accesses loop indices nested loop performed experiments commercial multiprocessor namely silicon graphics challenge cluster thereby obtaining real data regarding cache thrashing reduction contrast previous data mainly simulations 11 12 26 3 basic concepts assumptions data dependences statements defined 6 24 4 25 statement 1 uses result another statement 2 1 flowdependent 2 1 safely store result fetches old data stored location 1 antidependent 2 1 overwrites result 2 1 outputdependent 2 dependence within iteration loop called loopindependent dependence dependence across iterations loop called loopcarried dependence cache thrashing due true data sharing outermost loop parallel data dependences cross threads therefore paper consider loop nests whose outermost loops sequential simplify discussion make following assumptions program pattern 1 functions representing array subscript expressions linear 2 loop construct considered consists sequential loop embraces one several singlelevel parallel loops exist multilevel parallel loops one level parallelized commercial sharedmemory multiprocessor systems hence shown loop nest model three levels parallel loop immediately enclosed sequential loop immediately enclosing sequential loop enddo enddo figure 3 loop nest model linear mappings iteration space n 1 theta n 2 theta n 3 domain space 1 theta 2 expressed array k loops example necessarily perfectly nested restructuring techniques presented later assume arbitrary loop bounds although showing lower bounds 1 simplicity notation multiple array variables multiple linear subscript functions may exist nested loop since considering cache thrashing due true data sharing ie due data dependences threads also write loop nest figure 3 enddo enddo fl array name appearing loop body h linear mappings iteration space n 1 theta n 2 theta n 3 domain space 1 fl theta 2 fl fl potentially dependent reference pairs number pairs fang lu 11 reported arrays involved nested loops usually twodimensional threedimensional smallsized third dimension latter treated small number twodimensional arrays nested loops parallel loop innermost level degenerate cases loop nest figure 3 therefore loop nest model seems quite general method also applied loop nest contains several separate parallel loops middle level parallel loops may restructured according reference patterns threads different instances parallel loop aligned currently align threads created different parallel inner loops programs complicated loop nests patternmatching techniques used identify loop subnest matches nest shown figure 3 outer inner loops part subnest ignored long loop indices appear array subscripts compiler analysis based simple multiprocessor model cache memory following characteristics 1 local processor 2 uses copyback snoopy coherence strategy line size one word transformed code however execute correctly machines multiword cache line multilevel caches furthermore experimental results show performance transformed code quite good realistic machines analysis also extended incorporate machine parameters cache line size solutions section analyze relationship linear functions array subscripts based analysis restructure given loop nest reduce eliminate cache thrashing due true data sharing consider nested loops necessarily perfectly nested may variable loop bounds clarity presentation section 41 first discuss deal dependent reference pairs subscript function used references pair different pairs may use different subscript functions later section 42 discuss deal general cases using simple affine transforms fit model 41 basic model subsection assume pair dependent references subscript function used references assumption extract subscript function h fl j k pair dependent references model nested loop pairs dependent references illustrated following code segment enddo enddo without loss generality suppose linear subscript functions different assume function h rank 2 form h fl j take following example enddo enddo figure 4 nested loop multiple linear subscript functions example data dependences exist within loop j however two data dependences exist whole loop nest one references b two linear functions consider one dependence iteration subspace n 1 theta n 2 called reduced iteration space omits k loop order find iterations reduced iteration space may access common memory locations within corresponding threads define set elements array fl accessed within thread using subscript function h fl definition 1 given iteration reduced iteration space elements fl f fl accessed within thread denoted 0 j 0 g definition 2 suppose ij 0 j 0 two threads corresponding ji reduced iteration space given loop nest ij say dependence h fl denoted ij definition 3 exists fl 1 fl ij since f fl g fl linear terms j k following lemma obvious lemma 1 program pattern described exist fl 1 fl two iterations iteration space n 1 theta n 2 theta n 3 j constant n 0 series iterations space satisfy following equations following lemma two theorems establish relationship loop indexes corresponding two interdependent threads use index relationship stagger loop iteration space interdependent threads assigned processors lemma 2 let ij 0 j 0 two threads ij exist k k 0 theorem 1 let b fl1 exist k k 0 b fl1 fl2 gamma fl1 b fl2 fl2 c fl1 gamma fl1 c fl2 b fl1 fl2 gamma fl1 b fl2 proofs lemma 2 theorem 1 obvious definition 3 consider case b fl1 fl2 gamma fl1 b assuming loop bounds n 2 n 3 large enough satisfy c fl1 assumptions almost always true practice 31 true parallel loops small important assumptions following theorem theorem 2 21 let b fl1 fl2 gamma fl1 b fact j loop middle level loop guarantees ij 1 fl1 0 2 fl2 0 order find threads data dependence relations thread ij make following definition definition 4 given iteration j reduced iteration space let ij denote following set iterations space r r l fl1 l fl1 6 0 l fl2 1 fl defined gcd fl equal gcdb fl1 c fl2 gammac fl1 b fl2 fl2 c fl1 gammaa fl1 c fl2 b fl1 fl2 gammaa fl1 b fl2 equal gammagcd b fl1 c fl2 c fl1 b fl2 fl2 c fl1 fl1 c fl2 b fl1 fl2 fl1 b fl2 guarantee l fl1 0 2 l gcd fl equal gcda fl1 b fl1 equal gammagcda fl1 b fl1 guarantee l fl1 0 l gcd fl equal gcda fl2 b fl2 equal gammagcda fl2 b fl2 guarantee l fl1 0 called staggering parameter corresponding linear function h fl exist data dependences given pair references define staggering parameter l fl1 l fl2 0 0 staggering parameters example figure 4 calculated l 11 l 12 l 21 l 22 according definition 41 following theorem derived theorems 1 2 definition 4 states use staggering parameters uniquely partition threads independent sets theorem 3 ij defined satisfies 1 theorem indicates ij includes iterations whose corresponding threads data dependence relation ij call ij equivalence class reduced iteration space order eliminate true data sharing threads equivalence class assigned processor want restructure reduced iteration space threads equivalence class appear column staggering parameter l computed dependent reference pair tells us stagger row reduced iteration space columns right l 2 0 left relative ith threads involved dependence pair aligned column different staggering parameters may require staggering iteration space different ways however staggering parameters proportion staggering unified staggering parameter defined satisfy requirements simultaneously definition 5 given staggering parameters l call g l12 unified staggering parameter lemma 3 21 condition l k1 definition 5 true iterations belong two different equivalent classes b iterations belong two different equivalence classes theorem 4 21 condition l k1 definition 5 true reduced iteration space must staggered according unified staggering parameter g l12 l11 g order reduce eliminate data sharing among threads ie gth row reduced iteration space must staggered j l12 l11 gj columns right l 12 0 left l 12 0 relative ith row given loop nest satisfies condition l k1 definition 5 according theorem 4 reduced iteration space transformed staggered reduced iteration space sris leaving first g rows unchanged staggering remaining rows using unified staggering parameter data dependences different columns sris however staggering parameters proportion ie exist j k longer obtain unique unified staggering parameter moreover staggering alone longer sufficient eliminating data dependences different columns restructured iteration space threads equivalence class still different columns perform procedure called compacting stacks columns onto discuss staggering first definition given staggering parameters l l 11 l 21 l m1 suppose exists j k 1 according theory numbers 27 exist integers 1 2 satisfy fl l fl2 call g g 0 unified staggering parameter note since mtuple 1 2 necessarily unique g g 0 may unique either definition 6 unified staggering parameter g g 0 example figure 4 found staggering using unified staggering parameter g g 0 resulting sris four possible shapes shown figure 5b gamma e figure 5a shows details one shapes figure 6a 6b show reduced iteration space example figure 4 staggering 33 unified staggering parameter next compute compacting parameter using algorithm 1 2 presented shortly partition sris n chunks total number columns sris devided compacting parameter figure 5d gamma e dwide chunks stacked onto form compacted iteration space width shown figure 7 explain later threads different columns compacting sris independent moreover product g equals number equivalence classes sris shown figure 6b example transformed compacted form shown figure 8 following algorithm computes compacting parameter algorithm 1 input set staggering parameters l output compacting parameter step 1 2element subset fl i1 l j1 g fl 11 l 21 l m1 g compute 2 hl i1 l j1 step 2 jelement subset fl pick element say g0 g1 g0 g1 e g0 g1 figure 5 sris outlines using euclidean algorithm compute integers b apply algorithm 2 find nonzero integers r 2 r j r original reduced iteration space 11 figure reduced iteration space rearrangement let r step 3 j 3 compute step 4 established later unique regardless choice l 1 1 step 2 calculate compacting parameter nonzero integers r need found algorithm 1 integer coefficients b computed euclidean algorithm algorithm 2 therefore invoked derive group nonzero integer coefficients group integer coefficients linear expression algorithm 2 input nonzero positive integers output nonzero integers 1 even number zero coefficients 0 2k p among figure 7 compacted sris step 2 odd number zero coefficients 1 0 obviously nonzero integers computed algorithm 2 satisfy example figure 4 since two linear functions loop nest step 1 step 4 algorithm 1 used calculate compacting parameter next need establish two important facts first compacting threads different columns independent second compacting parameter computed algorithm 1 figure 8 reduced iteration space compacting largest number independent columns possible result compacting sris constant value first fact established theorem 5 theorem 6 corollary 1 introduce following definition definition 7 given iteration j reduced iteration space staggering parameters unified staggering parameter g g 0 integers satisfy set iterations 0 ij constructed follows 1 integer r iteration space belongs 0 2 exist integers r zero integer r iterations 0 space r r following three lemmas theorem 5 show 0 ij equivalence class ij process constructing 0 ij immediately following lemma lemma 4 given iterations reduced iteration space unified staggering parameter g g 0 exists integer r lemma 5 21 given iterations 0 reduced iteration space exist integers r zero r fl l fl2 r lemma 6 21 given iterations reduced iteration space 0 theorem 5 21 given staggering parameters l iteration j reduced iteration space next establish 0 ij result staggering g g 0 followed compacting stated corollary 1 lemma 7 21 given staggering parameters l integers satisfy 1 r 2 exist integers r 0 satisfying 1 integers r 00 satisfying r 00 exists integer k 1 r 00 theorem 6 21 compacting parameter determined algorithm 1 0 r fl l fl2 integers zero satisfy r exists integer k 0 corollary 1 set 0 ij definition 7 satisfies k r integers g g 0 unified staggering parameter definition 6 compacting parameter computed algorithm 1 result threads different columns compacting sris inde pendent next establish theorem 7 two columns columns apart computed algorithm 1 dependent therefore largest possible number independent columns result compacting sris constant number theorem 7 given j ij proof according computed algorithm 1 exist integers r definition 0 simplify process staggering compacting reduced iteration space following theorem used replace multiple staggering parameters proportion single staggering parameter theorem 8 21 given staggering parameters l staggering parameters satisfying exists integer r satisfying estimate time needed compiler compute staggering parameters unified staggering parameter compacting parameter suppose reference pairs complexity determining staggering parameters om unified staggering parameter staggering parameters determined om euclidean algorithm let number groups staggering parameters parameters group proportion 0 small practice according theorem 8 need consider one representative group complexity algorithm 1 2 computing compacting parameter c 2 lastly show result restructuring original loop nest based staggering compacting note staggering parameters proportion compacting unnecessary data dependence elimination however improve load balance compact sris compacting factor equals number available processors restructured code parameterized loop bounds number available processors obtained system call runtime need recompile different number available processors given loop nest perfectly nested resulting code staggering compacting shown code segment 1 listed code segment 1 result restructuring perfectly nested loop multiple linear enddo enddo enddo given loop nest perfectly nested resulting code two variants one g 0 0 show code g 0 listed code 0 similar 21 code segment lb 1 lb 2 lower bounds j ub 1 ub 2 upper bounds two loops g g 0 unified staggering parameter compacting parameter respectively determined psi psj local variables processor uses determine first iteration j 0 loop j executed variables j 0 offset also local processor psi psj processor modified every g iterations loop according staggering compacting parameters values psi psj initialized different processors lb respectively define function mod x mod code segment 2 restructured code case g 0 6 0 endif endif enddo enddo enddo 42 extended model theory developed previous subsection extended general cases subscript functions pair references necessary suppose following two linear functions belong pair references order determine iterations reduced iteration space dependent due reference pair consider affine transformation linear function h 2 expressed 21 22 denote order use previous results section 41 let identical implies 11 21 12 22 c 11 c 21 c 22 21 22 apply algorithms section 41 2 yield staggering parameter say given iteration 0 iteration must dependence 00 affine transformation iteration 0 dependence 00 transformation denote distance l 0 2 calculated l 0 constant meaning iterations cannot aligned constant staggering parameter common practice since loop j doall loop nest model two linear functions cofficients loop index variables j implies ff 1 paper consider case ff define l 0 2 two constants given staggering parameter case equations 1 2 unique solution unique staggering parameter l 0 hand exist multiple solutions following theorem shows certain conditions l 0 determined different proportion theorem 9 assume ff 1 staggering parameter l subscript function affine transformation solution equations 1 2 l 0 equal fi l proportion solution fi equations 1 2 proportion prove proportion l supposing every solution equations 1 2 written 0 solution homogeneous system associated equations 1 2 12 b 11 gamma b 12 11 6 0 22 b 12 b 11 gamma b 12 11 22 b c 12 12 b 11 gamma b 12 11 suppose c 12 according definition 4 c 12 case 12 b theorem 2 1 11 2 12 therefore according definition 4 also proportion proportion 2 proportion condition theorem 9 met choose l 0 staggering parameter reference pair table shows examples staggering parameters different subscript functions appearing dependent reference pair loop index variables listed order outermost loop level innermost simultaneously consider two reference pairs ai j bi j thread ij share array element ai thread i3j1 array element bi j thread i1j3 using theorem 9 staggering parameters l 0 two pairs 31 13 respectively unified staggering parameter compacting parameter calculated g g 0 8 table 2 examples different functions dependent reference pair loop nest dependent reference pair 5 experimental results thread alignment techniques described paper implemented backend optimizations kdparpro 20 knowledgebased parallelizing tool perform intra interprocedural data dependence analysis large number parallelizing transformations including loop interchange loop distribution loop skewing strip mining fortran programs evaluate effect thread alignment techniques performance multiprocessors memory hierarchies experimented three programs linpack benchmarks sgi challenge cluster configured contain twenty mips 4400 processors first programs parallelized optimized using kdparpro reduce eliminate cache thrashing due true data sharing tool recognized nested loops may cause thrashing applied techniques described previous section analyze restructure loop nests parallelized programs compiled using sgis f77 compiler optimization option o2 sequential versions programs compiled machine using optimization option f77 output binary codes executed various configurations different number processors dedicated time mips 4400 processor 16kbyte primary data cache 4mbyte secondary cache cache block size 32 bytes primary data cache 128 bytes secondary cache fast wide split transaction bus powerpath2 used coherent interconnect cache coherence maintained snoopy writeinvalidate strategy compared results obtained using algorithm align threads obtained using four different loop scheduling strategies provided sgi system software namely simple interleave dynamic gss simple method divides iterations number processors assigns chunk consecutive iterations one processor interleave scheduling method divides iterations chunks size specified chunk option execution chunks statically interleaved among processes dynamic scheduling iterations also divided chunksized chunks process finishes chunk however enters critical section grab next available chunk gss scheduling 29 chunk size varied depending number iterations remaining none sgiprovided methods consider task alignment speedup parallel execution shared memory machines like sgi cluster affected many factors including program parallelism data locality scheduling overhead load balance usually gss dynamic interleave schedulings small chunk size supposed show better load balance simple scheduling hand tend incur scheduling overhead simple furthermore simple captures data locality cases schedulings programs selected linpack sgefa spodi ssifa sgefa factors double precision matrix gaussian elimination main loop structure program consists three imperfectly nested loops innermost loop inside subroutine saxpy multiplies vector constant adds result another vector order show array access pattern inside loop body inlined saxpy code section given however kept subroutine call applied techniques program enddo enddo sris staggering iterations reduced iteration space k j shown figure 9 program consider linear function j staggering parameter 10 according definition 44 number processors used determine compacting factor figure 9 sris sgefa spodi computes determinant inverse certain double precision symmetric positivedefinite matrix two main loop nests program shown restructured loop nests sgefa innermost loop contained subroutine saxpy first loop nest enddo enddo second loop nest enddo enddo sriss staggering iterations reduced iteration spaces k j j k two loop nests respectively shown figure 10ab linear functions considered j k respectively staggering parameters 10 according definition 44 loop nest 1 b loop nest 2 figure 10 sriss spodi ssifa factors double precision symmetric matrix elimination main loop nest program shown view backward goto loop outermost sequential loop within value kstep may change 1 2 different iterations based input matrix depending value kstep one two parallel loop nests inside outermost sequential loop executed iteration outermost loop index step outermost loop equals ie gamma1 gamma2 array access patterns two kstep values slightly different innermost loop inside saxpy subroutine 10 continue k eq enddo enddo enddo 20 continue sriss staggering iterations reduced iteration space k jj two different ksteps shown figure 11 clarity use c kstep denote value kstep current k iteration use p kstep value previous k iteration threads aligned well properly align threads created current k iteration previous k iteration need consider four possible cases dependences one two references another two references ai third ai k gamma jj ai last ai k gamma cases staggering parameter l figure 11 sriss ssifa problem sizes used experiments n100 1000 performance parallel codes transformed techniques compared performance achieved scheduling methods provided sgi shown figures 1215 sgefa ssifa may require pivoting nonpositivedefinite symmetric matrices positivedefinite symmetric matrices show data sgefa pivoting show data without pivoting ssifa pivoting may potentially destroy task alignment1352 4 8 number processors interleave chunk interleave chunk dynamic gss simple method number processors interleave chunk interleave chunk dynamic gss simple method b n1000 figure 12 gaussian elimination sgefa12345 number processors interleave chunk interleave chunk dynamic gss simple method n10026101418 number processors interleave chunk interleave chunk dynamic gss simple method b n1000 figure 13 determinant inverse symmetric positive matrix spodi figures show method always outperforms sgis scheduling methods exception program sgefa program methods performance almost simple although method outperforms simple 14 16 processors attributed reduction cache thrashing due true data sharing problem tends severe processors running simple scheduling method tends get better performance dynamic gss interleave methods results better locality less cache thrashing cases also incurs less scheduling overhead programs number processors interleave chunk interleave chunk dynamic gss simple method n100246810 number processors interleave chunk interleave chunk dynamic gss simple method b n1000 figure 14 factorization symmetric matrix ssifa exhibit good load balance like spodi ssifa programs linpack deal symmetric matrices simples performance results degrade substantially method outperforms simple quite significantly cases especially spodi figure 13 well ssifa without pivoting figure 15 method beats simple much 105 able get improvement simple program sgefa pivoting much likely destroy locality try keep rest programs attribute performance gain simple reduction cache thrashing due true data sharing better load balance although ssifa pivoting believe method benefits load balancing note sgi system software cannot pick right scheduling method automatically fit particular program hand method seems capable delivering good performance different loop shapes thrashing problem becomes serious parallel systems processors greater communication overhead method likely even effective 6 conclusions paper presents method reduced iteration space rearranged according staggering compacting parameters nested loop either perfectly nested imperfectly nested restructured reduce even eliminate cache thrashing due true data sharing method efficiently implemented parallel compiler although analysis per se based simple machine model resulting code executes correctly complex models experimental results show transformed code perform quite well real machine extend techniques proposed paper incorporate additional machine parameters interesting future work r performance enhancement paging systems program analysis transformations automatic partitioning parallel loops data arrays distributed sharedmemory multiprocessors automatic loop interchange multilevel cache hierarchies organizations dependence analysis supercomputing improving register allocation subscripted variables compiling scientific code complex memory hierarchies partitioning labeling loops unimodular transformations eliminating false sharing solution cache pingpong problem risc based parallel processing systems cache local memory thrashing compiler strategy parallel processing systems iteration partition approach cache local memory thrashing parallel processing performance optimizations problem optimizing data transfers complex memory systems strategies cache local memory management global program transformation effects program parallelization stripmining transformation cache performance multiprocessor compiler techniques data partitioning sequentially iterated parallel loops reducing false sharing shared memory multiprocessors compiletime data transformations design implementation knowledgebased parallelizing tool efficient solution cache thrashing problem extended version loop restructuring techniques thrashing problem loop staggering structure computers computations dependence graphs compiler op timizations solution cache pingpong problem multiprocessor systems introduction theory numbers minimum distance method partitioning recurrences multiproces sors guided selfscheduling practical scheduling scheme parallel supercomputers time optimal linear schedules algorithms uniform dependencies empirical study fortran programs parallelizing compilers iteration partitioning resolving stride conflicts cachecoherent multiprocessors false sharing spatial locality multiprocessor caches data locality optimizing algorithm iteration space tiling tr