simulation study decoupled vector architectures decoupling techniques applied vector processor resulting large increase performance vectorizable programs simulate selection perfect club specfp92 benchmark suites compare execution time conventional single port vector architecture decoupled vector architecture decoupling increases performance factor greater 14 realistic memory latencies ideal memory system zero latency still speedup much 13 significant portion paper devoted studying tradeoffs involved choosing suitable size queues decoupled architecture hardware cost queues need large achieve performance advantages decoupling b introduction recent years witnessed increasing gap processor speed memory speed due two main reasons first technological improvements cpu speed matched similar improvements memory chips second instruction level parallelism available recent processors increased since several instructions issued processor cycle total amount data requested per cycle memory system much higher two factors led situation memory chips order 10 100 times slower cpus total execution time program greatly dominated average memory access time current superscalar processors attacking memory latency problem basically three main types techniques caching multithreading decoupling sometimes may appear together cachebased superscalar processors reduce average memory access time placing working set program faster level memory hierarchy software hardware techniques 5 23 devised prefetch data high levels memory hierarchy lower levels closer cpu data actually needed top program transformations loop blocking 16 proven useful fit working set program cache recently address data prediction receive much attention potential solution indirectly masking memory latency 21 multithreaded processors 1 30 attack memory latency problem switching threads computations amount parallelism exploitable aug ments probability halting cpu due hazard decreases occupation functional units increases total throughput system improved single thread still pays latency delays cpu presumably never idle thanks mixing different threads computation decoupled scalar processors 27 25 18 focused numerical computation attack memory latency problem making observation execution program split two different tasks moving data processor executing arithmetic instructions perform program com putations decoupled processor typically two independent processors address processor computation processor perform two tasks asynchronously communicate architectural queues latency hidden fact usually address processor able slip ahead computation processor start loading data needed soon computation processor excess data produced address processor stored queues stays retrieved computation processor vector machines traditionally tackled latency problem use long vectors memory vector operation started pays initial po tentially long latency works long stream elements effectively amortizes latency across elements although vector machines successful many years certain types numerical calculations still much room improvement several studies recent years 24 8 show performance achieved vector architectures real programs far theoretical peak performance machine 8 shown memory port singleport vector computer heavily underutilized even programs memory bound also shows vector processor could spend 50 execution cycles waiting data come memory despite need improve memory response time vector architectures possible apply hardware software techniques used scalar processors techniques either expensive exhibit poor performance vector context example caches software pipelining two techniques studied 17 19 28 22 context vector processors proved useful enough widespread use current vector machines conclusion order obtain full performance vector processor additional mechanism used reduce memory delays com ing lack bandwidth long latencies experienced programs many techniques borrowed superscalar microprocessor world paper focus decoupling also explored alternatives multithreading 12 outoforder execution 13 purpose paper show using decoupling techniques vector processor 11 performance vector programs greatly improved show even ideal memory system zero latency decoupling provides significant advantage standard mode operation also present data showing realistic latencies decoupled vector architectures perform substantially better nondecoupled vector architectures another benefit journal supercomputing 3 decoupling also allows tolerate latencies inside processor functional unit register crossbar latencies paper organized follows section 2 describes baseline decoupled architectures studied throughout paper section 3 discuss simulation environment benchmark programs used experiments presented section 4 provides background analysis performance traditional vector machine section 5 detail performance decoupled vector proposal finally section 6 presents conclusions future lines work 2 vector architectures implementations study based traditional vector processor numerical applications primarily maturity compilers availability benchmarks simulation tools feel general conclusions extend vector applications however decoupled vector architecture propose modeled convex c3400 section describe base c3400 architecture implementation henceforth reference architecture decoupled vector architecture generically referred dva main implication election c3400 study restricted class vector computers one memory port two functional units also important point used output convex compilers evaluate decoupled architecture means proposal studied paper able execute fully transparent manner already existing instruction set 21 reference architecture convex c3400 7 consists scalar unit independent vector unit see fig 1 scalar unit executes instructions involve scalar registers registers issues maximum one instruction per cycle vector unit consists two computation units fu1 fu2 one memory accessing unit mem fu2 unit general purpose arithmetic unit capable executing vector instructions fu1 unit restricted functional unit executes vector instructions except multiplication division square root functional units fully pipelined vector unit 8 vector registers hold 128 elements 64 bits eight vector registers connected functional units restricted crossbar pairs vector registers grouped register bank share two read ports one write port links functional units compiler responsible scheduling vector instructions allocating vector registers port conflicts arise fetch decode unit sregs aregs rxbar wxbar figure 1 reference vector architecture modeled convex c3400 22 decoupled vector architecture decoupled vector architecture propose uses fetch processor split incoming nondecoupled instruction stream three different decoupled streams see fig 2 three streams goes different processor address processor ap performs memory accesses behalf two processors scalar processor sp performs scalar computations vector processor vp performs vector computations three processors communicate set implementational queues proceed independently set queues akin implementational queues found floating point part r8000 microprocessor15 main difference decoupled architecture previous scalar decoupled architectures zs 1 26 map200 6 pipe 14 fom 4 two computational processors instead one two computation processors sp vp split due different nature operands work scalars vectors respectively fetch processor fetches instructions sequential nondecoupled instruction stream translates decoupled version translation processor proceed independently yet synchronizes communication queues needed example memory instruction loads register v5 fetched fp translated two pseudoinstructions load instruction sent ap load data vector load data queue vldq queue 1 fig 2 qmov instruction sent vp dictates move operation vldq final destination register v5 important note qmovs generated fp journal supercomputing 5 2 1 figure 2 decoupled vector architecture studied paper queue names 1 vector load data queue vldq 2 vector store data queue vsdq 3 address load queue alq 4 address store queue asq 5 scalar load data queue sldq 6 scalar store data queue ssdq scalaraddress control queues vectoraddress control queue 1011 scalarvector control queues instructions real sense ie belong programmer visible instruction set qmov opcodes hidden inside implementation note total hardware added original reference architecture shown figure 1 consists communication queues private decode unit one three processors resources inside processor decoupled vector architecture reference architecture worth noting though queues added scalar queues therefore require small amount extra area vldq vsdq hold full vector registers queues 2 3 fig 2 slot queues equivalent normal vector register 128 elements thus requiring 1kb storage space one key points architecture achieve good performance relatively slots two queues address processor performs memory accesses scalar vector well address computations scalar memory accesses go first scalar cache holds scalar data vector accesses go cache access main memory directly one pipelined port access memory shared memory accesses address processor inserts load instructions address load queue alq store instructions address store queue asq stores stay queue associated data 6 roger espasa mateo valero shows either output queue vp vector store data queue vsdq output queue sp scalar store data queue ssdq either load store becomes ready ie dependencies associated data necessary present sent address bus soon becomes available case load store ready ap always gives priority loads preserve sequential semantics program address processor needs ensure safe ordering memory instructions held alq asq memory accesses processed two steps first associated memory region computed second region used disambiguate memory instruction previous memory instructions still held address queues ap using disambiguation information dependency scoreboard maintained scoreboard ensures 1 loads executed inorder 2 stores executed inorder 3 loads execute older stores associated memory regions overlap dependences found scoreboard guarantees loads stores performed original program order correctness guaranteed memory region defined 5tuple h start end addresses resp consecutive region bytes memory vl vs sz vector length vector stride access granularity needed vector memory operations end address 2 computed 1 sz scalar memory accesses vl set 1 vs 0 special case gathers scatters properly characterized memory region 1 set 0 2 set 2 scoreboard find dependence gatherscatter previous future memory instructions vector processor performs vector computations main difference vp reference architecture vp two functional units dedicated move data processor two units qmov units able move data vldq data queue filled ap vector registers move data registers vadq drained ap sending contents memory included two qmov units instead one otherwise vp would paying high overhead common sequences code compared reference architecture set control queues connecting three processors queues 711 fig 2 needed instructions mixed operands common case vector instructions scalar register source operand ie mul v0s3 v5 cases include mixed register instructions vector gathers require address vector sent ap vector reductions produce scalar register result journal supercomputing 7 3 methodology 31 simulation environment asses performance benefits decoupled vector architectures taken trace driven approach perfect club specfp92 programs chosen benchmarks 3 tracing procedure follows perfect club programs compiled convex c3480 7 machine using fortran compiler version 80 optimization level o2 implies scalar optimizations plus vectorization executables processed using dixie 9 tool decomposes executables basic blocks instruments basic blocks produce four types traces basic block trace trace values set vector length register trace values set vector stride register trace memory references actually trace base address memory references dixie instruments basic blocks program including library code especially important since number fortran intrinsic routines sin cos exp etc translated compiler library calls library routines highly vectorized tuned underlying architecture represent high fraction vector operations executed program thus essential capture behavior order accurately model execution time programs executables processed dixie modified executables run convex machine runs produce desired set traces accurately represent execution programs trace fed two different simulators developed first simulator model convex c34 architecture representative single memory port vector computers second simulator extension first introduce decoupling using two cyclebycycle simulators gather data necessary discuss performance benefits decoupling 32 benchmark programs interested benefits decoupling vector architectures selected benchmark programs highly vectorizable 70 programs perfect specfp92 benchmarks chose 10 programs achieve least 70 vectorization table 1 presents statistics selected perfect club specfp92 programs column number 2 indicates suite program belongs column 3 presents total number memory accesses including vector scalar load store accesses next column total number operations performed vector mode column 5 number scalar instructions executed sixth column percentage vectorization program define percentage vectorization ratio number vector operations total number operations performed program finally column seven presents average vector length used vector instructions ratio vector operations vector instructions table 1 basic operation counts perfect club programs columns 35 millions mem vect scal avg program suite ops ops ins vect vl hydro2d spec 1785 2203 23 990 101 arc2d perf 1959 2157 flo52 perf 706 551 su2cor spec 1561 1862 66 957 125 bdna perf 795 889 128 869 81 trfd perf 826 438 156 757 22 dyfesm perf 502 298 108 747 21 important thing remark table 1 programs memory bound run reference machine take column labeled vect ops divide 2 get minimum number cycles required execute vector computations two vector functional units available comparing column mem ops result division see bottleneck programs always memory port absolute minimum execution time programs determined total amount memory accesses performs remark worth keeping mind since following sections show even memory port bottleneck programs usage always good one would intuitively expect 4 bottlenecks reference architecture first present analysis execution ten benchmark programs run reference architecture simulator consider three vector functional units reference architecture fu2 fu1 mem machine state represented 3tuple represents individual state one three units given point time example 3tuple hfu2 fu1mem represents state units working represents state vector units idle execution time program thus split eight possible states figure 3 presents splitting execution time states ten benchmark programs plotted time spent state memory latencies 1 20 70 100 cycles figure see number cycles programs proceed peak floating point speed states low number cycles states changes relatively little memory latency increases fraction journal supercomputing 9 swm25620006000 execution cycles hydro2d10003000 arc2d10003000 nasa710003000 execution cycles dyfesm5001500 figure 3 functional unit usage reference architecture bar represents total execution time program given latency values xaxis represent memory latencies cycles fully used cycles decreases memory latency high impact total execution time programs dyfesm trfd flo52 relatively small vector lengths effect memory latency seen noting increase cycles spent state h sum cycles corresponding states mem unit idle quite high programs four states correspond cycles memory port could potentially used fetch data memory future computations figure 4 presents percentage cycles total execution time latency 70 port idle time ranges 30 65 total execution time 10 benchmark programs memory bound run single port vector machine two functional units therefore unused memory cycles result lack loadstore work done 5 performance dva section present performance decoupled vector architecture versus reference architecture ref first start ignoring latencies functional units inside processor concentrate study effects main memory latency sections 5155 study determine swm256 hydro2d arc2d flo52 nasa7 su2cor tomcatv bdna trfd dyfesm2060 idle memory port 170 figure 4 percentage cycles memory port idle 4 different memory latencies costeffective parameters achieve highest performance proceed consider effect arithmetic functional units register crossbar latencies execution time section 56 first show decoupling tolerates well memory latencies also useful tolerating smaller latencies inside processor start defining dva architecture infinite queues latency delays unbounded dva udva short compare reference architecture introduce limitations udva branch misprediction penalties limited queue sizes real functional unit laten cies step step see individual effect restriction steps reach realistic version dva rdva compared ref udva machines 51 udva versus ref unbounded dva architecture udva version decoupled architecture queues set large value 128 slots latency delays moreover perfect branch prediction model assumed icache modeled following experiments since previous data indicates low pressure icache 10 arithmetic functional units scalar vector 1 cycle latency vector register file read write crossbars latency startup penalty vector instructions benefits decoupling seen fig 5 program plot total execution time udva ref architectures memory latency varied 1 100 cycles graph also show minimum absolute execution time theoretically achieved curve ideal along bottom graph compute ideal execution time program use total number cycles con journal supercomputing 11 swm2565060cycles x flo5210cycles cycles dyfesm10cycles ref udva ideal figure 5 udva versus reference architecture benchmark programs sumed heavily used vector unit fu1 fu2 mem thus ideal essentially eliminate data memory dependences program consider performance limited saturated resource across entire execution overall results suggest two important points first dva architecture shows clear speedup ref architecture even memory latency 1 cycle even latency memory system decoupling produces similar effect prefetching technique advantage ap knows data loaded incorrect prefetches second important point slopes execution time curves reference decoupled architectures substantially different implies decoupling tolerates long memory delay much better current vector architectures memory latency cycles1216 hydro2d arc2d su2cor bdna trfd dyfesm figure 6 speedup dva reference architecture benchmark programs overall decoupling helping minimize number cycles machine halted waiting memory recall section 4 execution time program could partitioned eight different states decoupling greatly reduces cycles spent state h summarize speedups obtained fig 6 presents speedup dva ref architecture particular value memory latency speedups latency 100 range 132 tomcatv 170 dyfesm 52 reducing iq length first limitation introduce udva reduction instruction queues feed three computational processors ap sp vp section look slowdown experienced udva size apiq spiq vpiq queues reduced 128 instructions 32 16 8 4 instructions order reduce amount simulation required chosen fix value memory latency parameter 50 cycles seen previous section udva tolerates well wide range memory latencies thus expect value 50 cycles quite representative full 1100 latency range size instruction queues important since gives upper bound occupation queues system example determines maximum number entries waiting load address queues figure 7 presents slowdown respect udva ten benchmarks three instruction queues reduced 32 16 8 4 slots fig 7 see performance 128 32 16entries instruction queues virtually benchmarks numbers decided set iq length 16 entries rest experiments presented paper size line typical instruction queues found current microprocessors 31 journal supercomputing 13 swm256 hydro2d arc2d flo52 nasa7 su2cor tomcatv bdna trfd dyfesm102106 figure 7 slowdown experienced udva reducing iq size swm256 hydro2d arc2d flo52 nasa7 su2cor tomcatv bdna trfd dyfesm101103 slowdown figure 8 slowdown due branch mispredictions three models speculation 53 effects branch prediction section look negative effects introduced branch mispredictions branch prediction mechanism evaluated directmapped btb holding entry branch target address 2bit predictor predictor found 20 augmented basic btb mechanism 8deep return stack akin one found 2 evaluated accuracy branch predictor 64 entries btb accuracy varies lot across set benchmarks programs flo52 nasa7 come worst misprediction rate around 30 tomcatv less 04 mispredicted branches nonetheless misprediction rate rather high set programs considered easy jumping pattern numerical codes tend dominated doloops due combination two facts first vectorization reduced absolute total number branches performed programs unbalanced way number easily predictable loop branches diminished factor proportional vector length could high 128 difficult branches found remaining scalar portion code essentially second factor using small btb compared found current superscalar microprocessors typical btb could 4096 entries 29 although prediction accuracy good impact mispredicted branches total execution time small figure 8 presents slowdown 14 roger espasa mateo valero due mispredicted branches relative performance architecture section 52 since prediction accuracy high tested benefit could obtained able speculate across several branches fig 8 bars labeled u1 correspond architecture allows one unresolved branch bars labeled u2 u3 correspond able speculate across 2 3 branches respectively first observation impact mispredicted branches rather low see flo52 30 misprediction rate total impact mispredicted branches 05 second observation speculating across multiple branches provides benefits specially dyfesm cost certainly justified simplicity one outstanding branch resolved plus vector architectures simulations following sections performed using 64entry btb allowing 1 unresolved branch 54 reducing vector queues length 541 vector load data queue section look usage vector load data queue goal determine queue size achieves almost performance 128slot queue used previous sections yet minimizes much possible hardware costs figure 9 presents distribution busy slots vldq benchmark programs program plot three distributions corresponding three different memory latency values bar graphs represents total number cycles vldq certain number busy slots example trfd latency 1 vldq completely empty zero busy slots around 500 millions cycles fig 9 see common use 6 slots except swm256 tomcatv 6 slots enough cover around 8590 cycles latency increased 1 cycle 50 100 cycles graphs show shift occupation towards higher number slots example consider programs arc2d nasa7 su2cor 1 cycle memory latency programs typically 23 busy slots latency increases three programs show increase total usage vldq typically use around 45 slots expected longer memory latency higher number busy slots since memory system outstanding requests therefore needs slots queue execution impact reducing vldq size seen fig 10 expected data seen fig 9 reducing queue size 16 8 slots noticeable programs going 4 slots affects mostly nasa7 bdna impact less 1 reducing vldq 2 slots would start hurt performance although much worst case would nasa7 around 4 impact already discussed 2 slots clearly lower bound size vldq accommodate memory journal supercomputing 15 swm256500cycles arc2d100300500 flo52100200cycles tomcatv100cycles dyfesm100300cycles figure 9 busy slots vldq benchmark programs three different memory latency values bound loops reducing queue 1 slot would stop decoupling effect present architecture looking data presented section decided pick 4 slots vldq following sections use size vldq 542 vector store data queue usage vector store data queue presents different pattern vldq recall ap always tries give priority load operations front stores effect putting much pressure vsdq points become full even 128 slots situation unusual may seem long ap encounters dependencies load store long swm256 hydro2d arc2d flo52 nasa7 su2cor tomcatv bdna trfd dyfesm102106 figure 10 slowdown due reducing vldq size relative section 53 loads dispatch stores retrieved vsdq sent memory thus occupation vsdq much higher vldq figure 11 presents distribution busy slots vsdq benchmark programs load queue plot three distributions corresponding three different memory latency values bar graphs represents total number cycles vsdq certain number busy slots make plots clear pruned graphs next name program indicate quartile amount shown example full set data shown nasa7 q100 bars hydro2d graph present 95 available data rest data set small seen plot compensate loss information graph also includes maximum value x axis took particular program hydro2d graph shows maximum occupation vsdq reached 118 slots although xaxis plot goes 50 note 6 programs point queue completely filled 128 full slots although common occupation ranges slots 4 programs occupation queue bounded bdna maximum occupation 34 slots queue su2cor maximum 23 bounding mostly due high percentage spill code time vector load tries recover vector stack previously spilled store ap detects dependency needs update contents memory draining queue heavily limits amount old stores kept vsdq programs trfd dyfesm qualitatively different two program simply dont decouple well program dyfesm recurrence forces three main processors ap sp vp work lock step thus typically allowing maximum 1 full slot program trfd core triangular matrix decomposition order matrix accessed makes iteration main loop dependent previous iterations causes lot loadstore dependencies queues dependencies resolved journal supercomputing 17 q999400800cycles hydro2d q95400800 arc2d q9421000 cycles 28 78 128 su2cor q999400800 tomcatv q10050cycles trfd q1005000 1 dyfesm q999200600cycles figure 11 busy slots vsdq benchmarks three different memory latency values case spill code draining queue updating memory thus queue never reach large occupation execution impact reducing vsdq size seen fig 12 bars show amount storage vsdq important performance mostly due fact singlememory port environment matter reorder loads stores every single store performed anyway thus sending store memory point data ready later change much overall computation rate data presented section selected 4slot vsdq following experiments swm256 hydro2d arc2d flo52 nasa7 su2cor tomcatv bdna trfd dyfesm100102 figure 12 slowdown due reducing vsdq queues size relative section 541 swm256 hydro2d arc2d flo52 nasa7 su2cor tomcatv bdna trfd dyfesm100slowdown figure 13 slowdown due reducing scalar queues size relative section 542 55 reducing scalar queues length section look impact reducing size various scalar queues system looking back fig 2 reducing queues numbered 38 1011 128 slots 16 slots queue number 9 vacq vp toap control queue reduced 128 slots 1 slot note queue holds one full vector register used gatherscatter operations size chosen reasonably close modern outoforder superscalar processors queues 31 impact reductions seen fig 13 overall using 8 entry queue scalar queues enough sustain performance 128 entry queues even small 2 entry queues slowdown around 101 3 programs dyfesm bdna nasa7 nonetheless beard mind programs heavily vectorized small degradation performance scalar side tempered small percentage scalar code present benchmarks order make safe decision took 16 entries queue scalar queues present architecture journal supercomputing 19 table 2 latency parameters vector scalar functional units parameters latency scal vect intfp vector startup 1 read xbar 2 add 12 6 mul 52 7 logicshift 12 4 div 349 20 sqrt 349 20 56 effects functional unit latencies section look effects latencies inside computation processors architecture far models simulated functional units using 1 cycle latency vector registers readwrite crossbars modeled free go section proceed three steps first add architecture latencies vector functional units table 2 shows values chosen second step add penalty 1 cycle vector startup vector operation third step add 2 cycles vector read crossbar latency add 2 cycles vector write crossbar latencies last step set latencies scalar units also shown table 2 figure 14 shows set stacked bars degradation performance aforementioned effects added bar bottom labeled vect lat represents slowdown relative section 55 following bar labeled startup slowdown respect performance vect lat bar similarly following bars thus total height bar combined slowdown effects figure 14 shows two different behaviors seven ten programs latencies small impact 5 due fact decoupling good tolerating memory latencies general helps covering latencies inside processor hand two programs trfd dyfesm show slowdowns bad 111 115 respectively behavior two programs surprising given already saw section 541 trfd dyfesm difficulties decoupling interiteration dependences trfd recurrences dyfesm saw achieve small occupation vector load data queue indicates bad degree decoupling couple fact swm256 hydro2d arc2d flo52 nasa7 su2cor tomcatv bdna trfd dyfesm105slowdown scal lat r xbar startup vect lat figure 14 slowdown due modeling arithmetic unit latencies vector pipeline crossbars relative section 55 relatively low vector lengths trfd dyfesm see cycle added vector dependency graph typically enlarging critical path program going detailed breakdown fig 14 shows vector functional unit latencies highest impact latencies added section worth noting though order latencies added might impact relative importance individual category nonetheless since vector latencies units largest latencies added since programs highly vectorized group likely impact performance fig 14 confirms startup penalty seen programs trfd dyfesm impact less 1 vector register file readwrite crossbar latencies impact programs except swm256 tomcatv typically latencies amount impact 05 1 percentage points vectorized codes around 23 percentage points less vectorized trfd dyfesm scalar latencies low impact programs partly due shorter vector ones partly due small fraction scalar code partly due scalar latencies masked vector latencies decided compare impact functional unit latencies reference machine dva machine simulate reference machine latencies reference machine standard latencies compute resulting slowdowns compare slowdowns slowdowns fig 14 presented result comparison seen fig 15 results show cases effect functional unit latencies much worse inorder reference machine decoupled machine since decoupling introduces form dynamic scheduling hide latencies previously critical path performing memory loads advance journal supercomputing 21 swm256 hydro2d arc2d flo52 nasa7 su2cor tomcatv bdna trfd dyfesm105115 slowdown udva ref figure 15 comparison functional unit latency impact udva ref machines 57 rdva versus ref data presented last section reached realistic implementation originally proposed udva realistic version referred rdva main parameters follows instruction queues scalar queues 16 entries long address queues ap also entries long latencies used functional units readwrite crossbars register file shown table 2 vldq vsdq 4 slots control queue connecting vp ap single slot branch prediction mechanism 64 entry btb 1 unresolved branch supported section replot fullscale comparison udva rdva ref several latencies figure 16 presents data three architectures memory latency varied 1 cycle 100 cycles almost programs difference udva rdva rather small slopes relatively parallel swm256 difference almost 0 programs hydro2d tomcatv arc2d su2cor slowdown rdva udva less respectively 1029 1031 1037 1044 programs flo52 bdna nasa7 higher slowdown moreover slope curve rdva performance starts diverging udva high values latency finally dyfesm trfd seen previous sections take significant performance hit going udva rdva 6 summary future work paper described basic decoupled vector architecture dva uses principles decoupling hide memory latency seen vector processors 22 roger espasa mateo valero swm2565060cycles x flo5210cycles x cycles x dyfesm10cycles x ref rdva udva ideal figure 16 comparison ref udva rdva execution times several latencies dva architecture shows clear speedup ref architecture even memory latency 1 cycle speedup due fact ap slips ahead vp loads data advance vp needs input operand almost always ready queues even latency memory system slipping produces similar effect prefetching technique advantage ap knows data loaded incorrect prefetches thus partitioning program separate tasks helps exploiting parallelism ap vp translates increase performance even absence memory latency moreover increase latency see slopes curves execution time benchmarks remain fairly stable whereas ref architecture much sensitive increase memory latency memory latency set journal supercomputing 23 50 cycles example speedups rdva ref machine range 118140 latency increased 100 cycles speedups go high 15 seen speed improvements implemented reasonable costperformance tradeoff section 54 shown length queues need large allow decoupling take place vector load queue four slots enough achieve high fraction maximumperformance obtainable infinite queue side vector store queue need large experiments varying store queue length indicate store queue two elements achieves almost performance one sixteen slots ability tolerate large memory latencies critical future high performance computers order reduce costs high performance sram vector memory systems turned sdram based memory sys tems change unfortunately significantly increase memory latency point decoupling come rescue shown 100 cycle latencies gracefully tolerated performance increase respect traditional inorder machine moreover although paper look single processor case decoupling technique would also effective vector multiprocessors help reducing negative effect conflicts interconnection network memory modules simulation results presented paper indicate vector architectures benefit many techniques currently found superscalar processors applied decoupling alternatives applying multithreaded techniques improve memory port usage 12 outoforder execution together register renaming 13 currently pursuing latter approach r performance tradeoffs multithreaded processors perfect club benchmarks effective performance evaluation supercomput ers organization architecture tradeoffs fom performance study software hardware data prefetching strategies functionally parallel architectures array processors convex architecture reference manual c series quantitative analysis vector code dixie trace generation system c3480 instruction level characterization perfect club programs vector computer decoupled vector architectures multithreaded vector architectures pipe vlsi decoupled architecture optimizing parallelism data locality cache performance vector supercomputers memory latency effects decoupled architectures software pipelining effective scheduling technique vliw machines branch prediction strategies branch target buffer design value locality load value prediction vector register design polycyclic vector scheduling design evaluation compiler algorithm prefetching explaining gap theoretical peak performance real performance supercomputer architectures decoupled accessexecute computer architectures simulation study decoupled architecture computers polycyclic vector scheduling vs chaining 1port vector supercomputers design microarchitecture ultrasparci exploiting choice instruction fetch issue implementable simultaneous multithreading processor mips r10000 superscalar microprocessor tr simulation study decoupled architecture computers zs1 central processor software pipelining effective scheduling technique vliw machines polycyclic vector scheduling vs chaining 1port vector supercomputers optimizing parallelism data locality design evaluation compiler algorithm prefetching designing tfp microprocessor performance study software hardware data prefetching schemes cache performance vector supercomputers explaining gap theoretical peak performance real performance supercomputer architectures outoforder vector architectures vector register design polycyclic vector scheduling decoupled accessexecute computer architectures mips r10000 superscalar microprocessor memory latency effects decoupled architectures performance tradeoffs multithreaded processors decoupled vector architectures multithreaded vector architectures quantitative analysis vector code ctr mostafa soliman stanislav g sedukhin matrix bidiagonalization implementation evaluation trident processor neural parallel scientific computations v11 n4 p395422 december mostafa soliman stanislav g sedukhin trident scalable architecture scalar vector matrix operations australian computer science communications v24 n3 p9199 januaryfebruary 2002