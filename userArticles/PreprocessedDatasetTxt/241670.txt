parallelized direct execution simulation messagepassing parallel programs abstractas massively parallel computers proliferate growing interest finding ways performance massively parallel codes efficiently predicted problem arises diverse contexts parallelizing compilers parallel performance monitoring parallel algorithm development paper describe one solution one directly executes application code uses discreteevent simulator model details presumed parallel machine operating system communication network behavior approach computationally expensive interested parallelization specifically parallelization discreteevent simulator describe methods suitable parallelized direct execution simulation messagepassing parallel programs report performance system lapse large application parallel simulation environment built intel paragon codes measured date lapse predicts performance well typically within 10 relative error depending nature application code observed low slowdowns relative natively executing code high relative speedups using 64 processors b introduction performance prediction andor analysis parallel programs currently important area research especially parallel computers coming dominate high performance computing arena writers parallel compilers would like able predict performance aid towards generating efficient highly parallel code users performance instrumentation tuning tools interested predicting observing parallel performance must deal fact instrumentation code may perturb measurements developers new parallel algorithms interested predicting well performance new algorithm scales increasing problem size machine architecture general users may interested performance tuning codes large numbers processors infrequently available using fewer readily available resources designers new communication networks interested evaluating designs realistic workloads method direct execution simulation4 5 9 11 16 application codes offers solution problems direct execution simulation application code directly executed obtain information applications execution behavior references application code simulated virtual machine trapped simulator point view application code running virtual machine thus application executes temporal calls wallclock time message type available response depends state virtual machine simulator simulated time calls placement point view simulator application code driver describing activity simulated detailed direct execution simulator parallel programs offers potential accurate prediction parallel program performance large possibly asyetunbuilt systems approach require great deal computation good candidate parallelization execution application processes clear source parallelism one easily envisions system discreteevent virtual machine simulator resides one processor pool processors host directly executing application processes solution perform poorly though situations either communication path simulator becomes bottleneck simulator execution bottleneck important consider problem parallelizing virtual machine simulator paper considers problem parallelizing virtual machine simulator messagepassing parallel programs methods describe implemented tool named lapse large application parallel simulation environment implemented intel paragon paragon codes pieces lapse specific paragon synchronization algorithm describe paper applicable general messagepassing systems lapse accepts input makefile describing build application application source code lapse initialization file describing size characteristics virtual machine mapping virtual machine onto physical machine lapse automatically builds loads executes direct execution simulation application code lapse supports applications written c fortran mixture two describe lapses performance four parallel applications two linear system solvers one indirect one direct continuous time markov chain simulator graphics library driver measuring speedups relative simulations using one processor slowdowns relative natively executing code observe wide range performance characteristics depending large part nature code simu lated lapse proven accurately predict performance typically within 10 simulated many 512 virtual processors using 64 actual processors main limitation weve encountered limited physical memory problem specific paragon issue parallel architectures better support virtual memory several projects use direct execution simulation multiprocessor systems among find two pertinent characteristics type network simulated ii whether simulation parallelized table 1 uses attributes categorize relevant existing work lapse tool communication simulator messagepassing network parallel messagepassing network parallel maxpar3 shared memory cacheing serial cachecoherent shared memory serial messagepassing network serial messagepassing network serial cachecoherent shared memory serial cachecoherent shared memory parallel table 1 direct execution simulation tools among current simulators simulation cachecoherency protocols important concern lapse implemented intel paragon13 support virtual shared memory coherency protocols complicate simulation problem considerably facet lapse need deal however existing work identified contextswitching overhead key performance consideration one directly affects us much order magnitude improvement observed directexecution simulator uses lightweight thread constructs accelerate contextswitching small grain sizes thread packages available us support appearance independent virtual address spaces necessary approach able context shared machine government lab modify operating system kernel support lapse processes necessity unix threads subject operating systems mechanisms scheduling subject costs context switching wisconsin wind tunnel wwt knowledge working multiprocessor simulator uses multiprocessor cm5 execute simulation hase operational parallel time 12 published intent hase use commercially available simulator based optimistic time warp synchronization protocol worthwhile note differences lapse wwt first matter purpose lapses primary goal support scalability performance analysis paragon codes wwt tool cachecoherency protocol researchers designed simulate different type machine host second difference matter lookahead ability conservative parallel simulation predict future behavior cachecoherent system processor may interact communication network cache miss may cache affected write time another processor lookahead apparently poor wwt deals keeping things close synchrony wwt exploits assumption communication processors requires least b 100 number cycles application object code altered cause wwt application processes synchronize every b cycles communication deferred next barrier assurance barrier occurs communication affected recipient method synchronization special case yawns 19 22 protocol wwt ignores network contention assuming latency every message fixed known contrast paragon processors less tightly coupled cachecoherent setting memory reference might generate network event messagepassing setting explicit call messagepassing subroutine influence network behavior fact allows less rigid approach synchronization particular many large numerical programs alternate long computation phase communication occurs communicationintensive phase better lookahead possibilities lapse avoid synchronization long periods network idleness whereas wwt cannot lookahead available lapse comes observation many applications long portions execution path independent time cases application code executed well advance actually simulating timing execution path independent time lookahead still obtained provided lower bound operating system overhead required send receive message finally wwt uses customized operating system cleverly exploits cm5 idiosyncrasies recognize misses simulated cache lapse runs purely application individual elements lapse proposed eg parallelized direct execution support different network simulators contributions show effectively synchronize parallelized directexecution simulations distributed memory programs demonstrate feasibility approach actual implementation testing nontrivial codes observe sometimes excellent performance combination features makes lapse unique among peers section 2 gives overview lapse system section 3 describes lapse transforms massively parallel code direct execution simulation section 4 details synchronization strategy section 5 describes experiments results respect validation slowdown speedups section 6 presents conclusions overview parallel program distributed memory machine comprised n application processes distributed among n n processors parallel programs constructed equivalence presently assume assume mapping static application processes communicate message passing using explicit calls system library routines example csend call intel nx library sends message calling application passes arguments defining message type userdefined integer message base address length process id processor id recipient control returned application soon memory area occupied message available reuse crecv called receive message arguments message type base address place message maximum message length control returned application process message received irecv asynchronous version crecv receive routine called anticipated message arrives incoming message transfer directly network location specified user otherwise message transfers network communications buffer subsequent receive call msgdone copies message users buffer figure 1a illustrates application process views time runs period calls system messagepassing routine send receive message message transaction complete upon return control application time system spends handling messages invisible application application process knows execution durations eg process 1 measure predict durations gamma 0 c gamma b e gamma assumption interrupted analogous service times queueing network assume durations independent network activity assuming lack interruptions facet deal durations give us information exploit parallel simulation synchronization protocol durations messagepassing overheads determined part network state illustrated figure 1b figure 1b message sent process 1 process 2 starting time time control passed operating system processor 1 operating system overhead required prepare message transmission network control returned application process 1 time b message begins coming network processor 2 time f thereby interrupting process 2 middle execution block point operating system processor 2 gains control handle interrupt message completely received operating system control returned application process 2 time g process 2 finally reaches code time explicitly receives message case copying system buffer user space additional overhead incurred message receipt overhead completed time point process 2 begins executing contrast process 1 reaches receive statement time c message process 2 arrives arriving message moves directly user buffer application process 1 continues execution receive completed time application processes unaware timing details falls simulator assign virtual times event times b c function execution durations network view b process 2 process 1 execute execute execute send0 receive b execute receive c send executeprocess 1 process 2 executeb send execute receive execute receive c execute execute application view figure 1 application network views parallel application timings reported application processes evaluation messagepassing delays assumed operating system overheads messagepassing interrupt handling model messagepassing activity affects execution application processes indeed application simulator lapse maintains application process data structure reflecting figure 1b called timeline records observed application events assigns simulation times timeline essentially futureevents list application process simulator timestamps futureevents modified simulation progresses depending simulation activity simulator conceptually separated application simulator network simulator although set future application networks events essentially single event list figure 2 gives overview lapses communication structure application submitted lapse recompiled lapse macros redirect application messagepassing calls corresponding lapse routines set routines known lapse interface interface code linked application becoming resident address space application calls interface routines typically trigger interaction application processes interface routines instance application call send message trapped interface routine sends messageto received interface routine corresponding matching message receive another application process interface routine also communicates appl interface interface appl network simulator application simulator processor appl interface appl interface communication network network simulator application simulator processor messages interface routines application routines messages network simulators messages interface application simulator figure 2 lapse communication structure application simulator notifying activity performed application wishes perform lapse application simulator responsible receiving descriptions application events application interface routines assigning simulation times events timelines like illustrated figure 1 application simulator interacts network simulator responsible simulating activity portion virtual communication network application simulator network simulator interacts form single process responsible simulating number application processes typically always resident physical processor collection application network simulators cooperatively synchronize parallel discreteevent simulation interface application simulator corresponding network simulator simple supporting integration different network simulators performance data paper taken puredelay simulator ignores contention seen nocontention assumption deleterious lapses ability predict performance 3 applicationlapse interaction section briefly describe application code transformed lapse simulation code lapse preprocessing invoked modifying applications makefile ie input file unix make command references compiler linker replaced references lapse scripts compilation script prepends source file list macro definitions remap every messagepassing routine call corresponding lapse routine way interactions application code virtual machine environment trapped excepting asynchronously triggered message handlers whose use discouraged paragon next source file compiled assembly code instrumented code increments basic block boundaries accumulating count number instructions executed far instrumented code compiled linker script causes additional linkage lapse interface code includes routines messagepassing calls remapped instrumented code attached lapse interface code become one osf1 unix process call application process represented timeline attendant data structures virtual processor vp simulation every call interface routine temporally sensitive insensitive insensitive call one whose result effect depend state virtual machine point call put another way applications execution path depend insensitive calls may depend results sensitive call calls temporally insensitive including corresponding transmission receipt messages eg csend crecv isend irecv extended versions paragon cause calling processor block condition satisfied msgwait gsync global reductions sensitive calls include realtime clocks dclock asynchronous probes existence particular messages iprobe iprobex queries status anticipated messages msgdone appreciate importance distinction compare two code fragments fragment 1 fragment 2 crecvmsgtype msgadrs msgsize fragments wait message type msgtype appear process send message fragment 1 simply blocks waits message processes message sends message 1 byte process 0 node 10 fragment 2 polls boolean valued function msgdone determine message present routine busywork called parameter changes every call number times busywork called parameters passed depend responses msgdone none calls fragment 1 temporally sensitive fragment 2s call msgdone temporally sensitive lapse answer msgdone query come simulator based whether simulator observes message interest simulated message arrives simulator simulator answers msgdone affirmatively tell lapse interface message available interface waits corresponding application message actually arrive another application process way application calls msgdone routine exactly many times would simulated virtual machine case close linkage simulator application execution fragment 1 needs interaction simulator execution therefore fragment 1 application code run well advance simulator eventually simulates timing events corresponding fragment 1 runtime lapse loads simulator application processes onto physical machine specified input file typically physical processor assigned one simulator process number application processes although arrangements separating simulation application processes also possible either case application process notified identity single simulator process interact likewise simulator process initialized identity application processes interact application processes permitted execute whenever lapse interface routine called recovers number application instructions executed since last call lapse routine count inflated input parameter effective clock ticks per instruction obtained input file produce estimate length time vp would run without interruption virtual machine last two interactions machine mechanism determining input parameter described section 5 next call temporally insensitive parameters transformed virtual machine coordinates physical machine coordinates requested operation performed case message sends receives take advantage address space coresidence interface routine application leave memory address parameters unaltered temporally sensitive calls involve actual execution request interface either case subject flowcontrol considerations interface routine next sends message assigned simulator reporting measured execution burst requested operation point interface routine temporally insensitive call completes operation eg blocks crecv call anticipated message arrives returns control calling application process temporally sensitive call waits response simulator upon receiving returns response application paragon library nx includes rich collection global reduction operations lapse maps lapse routines implement using primitive sendreceive calls instruments routines though part application account cost global synchronization instance original application call redirected lapse routine implements barrier standard treefashion virtual machine coordinates since code remapped instrumented exactly like application code cost synchronization obtained executing lapse version paragon library function lapse supports almost nx calls consistent single program multiple data spmd paradigm principle omissions calls asynchronous message handling hrecv message cancellation calls message calls involving type masks calls rarely used computing environment obtain accurate timing application operating system overhead sending receiving messages needs properly accounted lapse currently estimates operating system overheads example overhead clock ticks execute csend modeled b theta l startup cost l message length b cost per byte estimates b obtained measurements operating system principle would also possible instrument operating system instruction counting code run instrumented code part simulator would eliminate need using estimated path lengths however requires access operating system source code well authorization run modified kernel synchronization strictly serial simulation executes events monotone nondecreasing order event times view parallel simulator collection individual discreteevent simulators run separate processor ability time scheduling event different simulator desire events processor also executed monotone increasing order least compute state though monotonicity achieved earliest event processor timestamp processor executes event must either certain simulator still schedule event smaller timestamp must prepared rollback begin recomputing time event later scheduled rich literature solutions synchronization problem good introductory surveys found 10 25 survey stateofthe art found 20 conservative synchronization protocols prohibit simulator executing event possibility earlier event scheduled later optimistic protocols allow outofsequence event processing style strengths weaknesses shown perform well applications poorly others regardless method good performance always dependent models tendency towards relatively infrequent scheduling events one processor onto another conservative methods frequently work well slackness identified ahead time computation ie simulator able continuously compute distribute lower bounds future times may schedule events processors ability make predictions called lookahead model dependent optimistic methods basically work assuming slackness exists correcting errors made assumption proves false optimistic methods potential general carry overheads necessary recovering errors optimistic methods also much difficult implement correctly lapse uses new conservative synchronization protocol whoa windowbased halting appointments tailored characteristics simulation problem suppose simulation time previously chosen global synchronization point initially simulators simulated cooperatively establish simulation time wt simulate events timestamps wt within window simulators may still synchronize pairwise fashion global synchronization occurs boundaries windows every simulator reached time chosen window simulated process continues simulation terminates given lower window edge value wt computed ensure message send events whose timestamps eventually fall wt exist already applications timelines synchronization within window governed dynamic computation distribution lower bounds future times one simulator may affect another interaction simulators remote send events ie message send call whose destination vp managed simulator source lower bounds called appointments 21 appointment associated every remote send event every timeline initially computed event received updated simulation progresses appointment reflects best known lower bound associated message reaches network hardware application simulator passes appointments corresponding network simulator transforms appointment network simulator messages destination depending network simulated appointments network simulators may also order network simulator computes halting time minimum incoming appointment time neither application network simulator ever processes event whose timestamp larger simulators current halting time simulation progresses simulator either sends forewarned messages increases lowerbound network entry times modifications occur network appointment times either disappear increase simulators halting time must increase least large wt halting time increases application network simulators free execute events smaller timestamps protocol deadlockif h 0 wt least halting time among simulators simulator whose appointment defines h 0 must simulation clock value strictly less h 0 must event timestamp strictly less h 0 safely execute elaborate whoa two steps first discuss appointments computed used updated secondly discuss construction windows 41 appointments lapse exploits tendency messagepassing codes execution paths largely insensitive timing key observation one execute application processes largely online trace generators maintain timelines potentially long list futureevents whoa builds appointments windows around lists subsection describe two types appointments application appointments application simulators network simulators network appointments network simulators consider figure 3 illustrating vps timeline situation simulation time simulator whole advanced timeline records execution bursts lengths abc local csend local csend csend remote crecv crecv c figure 3 application appointment calculation vps timeline application events wish compute lower bound network entry time remote message events application appointment lower bound instant scheduled network simulator model network entry constructed sum residual time current execution startup block plus execution burst lengths plus lower bounds startup costs intervening application events plus lower bound startup cost event application events processed appointment may increase application appointment made remote send shortly event reported application simulator whoa strives keep appointments current simulator state evolves appointments change specifically two cases first may happen duration application event startup larger lower bound assumed usually arises include costlike copying messagethat always suffered hence included lower bound event executed additional cost included appointments remote send events affected vp increased amount additional cost appointments may change presence blocking example simulation crecv figure 3 fails find anticipated message vp becomes suspended simulation time advances without message appearing appointments computed suspended vp must increase likewise vp interrupted arrival message becomes suspended message completely received appointments must updated period suspension point vp becomes suspended say find vps minimum outgoing appointment time compute difference periodically schedule vp appointments update event every units time vp becomes unsuspended execution event advances vps appointments units update event removed event list vp released suspension point updated appointments passed individual simulators synchronize using network appointments network appointment time 0 simulator j message promise event reporting ms arrival subnetwork managed j scheduled j time 0 given ms application appointment simulator network simulator constructs network appointment adding network latency time depends distance message travels lapse currently sends network appointment every time network simulator receives new updated application appointment overkill developing version much reduces communication volume associated appointments lapse presently uses contention free network model updating network appointments needed however sophisticated updating network appointments required network models capture contention transmission simulated message one network simulator another removes associated appointment upon receipt target vp taken suspended long takes message completely arrive depends number packets decomposed target network simulator notes time message completely received data needed window computation described every network simulator maintains single halting time application network simulators instant halting time time current minimum incoming network appointment event timestamp less current halting time may safely exe cuted halting time increases appointments change messages received eventually halting time increases past upper edge current window remaining events window executed simulator engages computing upper edge next window 42 window construction suppose time upper edge window simulator engages window construction protocol halting time larger remaining events timestamps less upper edge wt next window computed ensure lower bound timestamp next unknown ie timeline remote send event simulator may execute wt thus measure far future globally application processes advanced ahead simulation processes desire many events found vps timeline times wt better amortize cost computing wt towards end simulators first action wait every one vps timelines many events apparently possible always wait least one event present reasons involving paragons management communication buffers user specified flowcontrol parameter f involved decision general rule simulator wait events either application processes reports blocked least f application events exist timeline already application process reports current blockedunblocked state every event reports state also reported separately changes blocked unblocked three ways application process may become blocked first blocked last call made interface routine temporally sensitive become unblocked must wait simulator executes temporally sensitive call responds secondly application process may blocked temporally insensitive call crecv msgwait wait application message received another application process time application process becomes runnable interface routine notifies simulator change finally application may become temporarily blocked purposes message flowcontrol timelines filled simulator enters global vector componentwise min reduction results permit simulator compute wt simulator offers 3tuple reduction first element l lower bound last timeline event unsuspended vp second element lower bound last timeline event suspended vp computed assumption released suspension immediately third component r lower bound time suspended vp released r minimum three valuesthe minimum completion time message midst received minimum incoming network appointment time simulator lower bound minimum time unsuspended vp next sends message vp includes local sends given global minima l min min r min simulator computes b lower bound startup cost send event construction assured remote send event whose timestamp ultimately falls wt resident timeline point window constructed property key whoa ensures appointment every remote send event window already established synchronization appointment management needed allows us use appointments wt define dynamically evolving pairwise synchronization schedule deadlock effect temporally sensitive calls window construction deserves remark presence temporally sensitive call timeline indicates application process blockedthe call always last one vps timeline means values l r offered simulator reduction small difference wt also small extreme cases calls temporally sensitive tend occur simultaneously vps net effect serialize simulation communication events 5 experiments section report experiments using lapse describing set four scientific applications used experimentation first address issue validation ie quantifying accuracy lapse timing predictions next quantify overheads captured application slowdown defined time takes lapse execute application n virtual processors n physical processors divided time takes application running natively execute n physical processors characterize simulation relative speedup defined time takes lapse execute application n virtual processors n physical processors divided time takes lapse execute application one physical processor relative speedup representative absolute speedup execution time hypothetical optimized serial simulator divided lapse n processors justification belief lapse avoids unnecessary overheads running serially essentially overhead lapse avoid maintenance appointment data structures however overhead small compared actual execution application attendant instruction counting would done serial simulator fact one applications sor high computation communication ratio lapse runs four vp problem four processors 18 times slower native application four processors see slowdowns speedups strongly affected amount lookahead present application well applications computation communication ratio context application lookahead refers far advance timing simulator application able allowed run 51 applications experimented four parallel applications representative variety scientific engineering workloads although claim made intended comprehensively span possible workloads applications arise physics computational fluid dynamics performance modeling computer communications systems graphics first application sor solves poissons boundary value equation two dimensions see chapter 17 23 partial differential equation pde two dimensions given set values boundary case rectangle pde discretized resulting large sparse system k 2 linear equations k 2 number discretized grid points equations solved using simultaneous relaxation sor oddeven ordering iteration solution interior grid point j xi j requires xi 1 n processors arranged square grid processor assigned subgrid size g theta g n computation thus results news north east west south communications pattern number instructions executed news exchange order g 2 size pairwise exchange order g bytes thus varying g adjust computation communications ratio application call ratio low application results highly regular communications pattern message passing calls synchronous csend crecv thus application execution path timing independent thereby good application lookahead second application bps domain decomposition solver finite difference finite element discretizations twodimensional elliptic partial differential equations 14 type problem arises frequently computational fluid dynamics algorithm bramblepasciak substructuring type consisting conjugate gradient method preconditioned approximation inverse matrix operator obtained discretizing pde 1 conjugate gradient method applied sparse matrix highly parallelizable requiring iteration nearest neighbor communication formation matrixvector products global reduction operation formation inner products domain pde partitioned nonoverlapping subdomains subdomains separated wire basket consisting edges vertices values specified edges vertices independent problems may solved subdomain interior done final iteration algorithm objective earlier iterations arrive sufficiently accurate values unknowns wirebasket process requires independent solutions edge iteration well small global solution problem defined vertices specific test case poissons equation unit square dirichlet boundary conditions specific advantage taken existence fast poisson solvers subdomain interiors code uses synchronous calls csend cprobe crecv global reductions thus exhibits good application lookahead code runs numbers processors powers 4 two levels computation communication ratio obtained varying size g theta g subdomain assigned processor high ratio low ratio code written combination c fortran provided us david keyes ion stoica icase old dominion university third application apucs continuous time markov chain discrete event simulator using parallel algorithm described 18 specific system simulated queueing network model large distributed computing system described 18 application highly irregular communications patterns involving anytoany pairwise messages application also uses synchronous message passing calls csend irecv msgwaits well global reductions parameter settings set 18 high computation communication ratio achieved setting low computation communication ratio achieved setting fourth application pgl parallel graphics library written support visualization scientific data 6 sample driver program library processor generates number randomly spaced colored triangles certain size processors rotate shade vertices triangles display scan lines distributed interleaved fashion processors endpoints scan lines triangle called span sent processor responsible scan line scan line pixels colored interpolating endpoints spans zbuffer algorithm used hidden surface removal process repeated f frames code uses anytoany pairwise communications highly asynchronous frequently using irecvs temporally sensitive msgdone call simulator little application lookahead code consists approximately 14000 lines c written provided us thomas crockett icase high lowcomputation communication ratios achieved assigning 1000 500 triangles per processor respectively however application harder adjust computation communication ratio designations high low used mainly distinguish two different workloads 52 validation section describe process validated lapse timing predictions describe results validation experiments order lapse make reasonable predic tions must know overhead system message passing calls eg csend interconnection network transit times well time application spends message passing call obtained estimates system overheads measuring system using test programs written specifically determine overheads overheads calls depend message length eg csend modeled l message length b constants estimated regression measurement data interconnection network transit times similarly measured found except long messages software overheads sending receiving messages far greater hardware transit times thus predictions using simple network simulator model link contention expected accurate obtain application time message passing calls proceeded follows first ran application natively ie without lapse small number nodes typically 2 theta 2 certain data size per node eg g sor number triangles per node pgl unix time command gives us estimate amount user time u consumed application next ran application data size lapse reports total number user instructions compute conversion factor average time per user instruction conversion factor incorporates average case sense instruction mix cache hit ratio etc application conversion factor used subsequent runs application lapse timing predictions example j application instructions executed two message passing calls lapse computes time calls j theta c given application data size per processor use conversion factor c measured small number nodes predict timings large number nodes example sor fixed value g number instructions instruction mix data access patterns executed message passing calls approximately 2 theta 2 grid processors 8 theta 8 grid processors thus fixed g expect conversion factor approximately independent number processors however different value g conversion factor may different separate conversion factors computed g computation communication ratios applications cases application execution path timing dependent several iterations computing conversion factors may required results presented two iterations used application run natively lapse variety numbers nodes timings compared table 2 presents percentage differences lapse predictions execution times actual native execution times seen table maximum error 6 observed larger prediction errors always occurred short runs initialization effects may present example pglhigh error 64 nodes gamma12 generating 10 frames 3 error reported table 2 20 frames runs span range application efficiencies example lapse estimates sor 16 processors spends 47 time executing user instructions opposed executing system instructions waiting messages spends 90 time executing user instructions results show lapse provide accurate timing estimates range scientific engineering applications 53 slowdowns next investigated amount overhead involved running lapse several types overheads first overhead involved counting application instructions second overhead actually timing simulation parallel third operating system overhead arises managing multiple processes per node spmd application running natively one process per node overheads captured single measurement called slowdown defined time takes lapse execute application n nodes divided time takes execute application natively n nodes measurements indicated instruction counting overhead generally ranges 30 80 overheads obtained direct comparison time natively execute application application augmented instruction counting timing simulation harder separate simulation overhead operating system overhead shall attempt however measurements presented 8 indicating osf1 paragon process management overheads increase superlinearly number processes per node increase note parallel simulation overheads aside lapse must send least twice many messages natively executing application message sent application results application application message well application simulator message although may processes node consider overall slowdown lapse begin demonstrating effect application lookahead simulation speed lapse attempts run application well advance simulator provided application able explained earlier results larger windows usersupplied flowcontrol parameter f determines maximum number message passing events application permitted run advance simulator running lapse application good lookahead ie one synchronous sends receives study effect lookahead controlled manner varying parameter f table 3 presents results experiment apucs running 8 processors one vpprocessor table reports slowdowns function f well average number application events executed per window per vp application event defined call lapse interface low computation communication ratio slowdown decreases 304 93 f increases 2 16 time increases 03 80 high ratio slowdown decreases 120 37 increases 03 46 f increases 2 16 demonstrates slowdown strongly dependent upon application lookahead table 3 also illustrates effect flowcontrol algorithm used lapse low ratio f increases past 16 slowdown increases contributing factor behavior follows remote send event associated appointment time simulation time advances appointments may updated resulting additional communication thus large number events timeline appointments overhead increases increasing f increases maximum allowable number events timeline also tends increase average number application events per window however always increase slowdown large f see apucshigh case levels f increases reason leveling entirely clear number factors interacting complex manner factors include operating system process scheduling algorithms computation communication ratios application simulator window construction algorithm etc next consider slowdowns set four applications described earlier compared lapse native application time codes running 4 16 32 64 processors lapse runs two processes per node one application process one simulator process f set moderate values obtain good lookahead whenever application permitted results experiments reported table 4 observe first slowdown increases number processors increases effect using global window cost computing new window increases logarithmically number processors increases importantly observed average size window decreases number vps increases occurs simply wt computed function minimum values taken vps increasing number vps increases likelihood low values submitted reduction observe next given application number processors slowdown typically increases applications computation communication ratio decreases high ratio time spent executing application code thus simulation overheads less important slowdowns sor bps apucs quite modest 18 280 considerably low end slowdowns reported executiondriven simulators 5 24 recall codes good application lookahead since make much use temporally sensitive message passing calls however pgl slowdowns significantly higher described earlier application little lookahead since executing many msgdone calls continually forces synchronization application simulation processes lapse deal type temporal question blocking application process simulator advanced simulation time time msgdone question asked severely limits size windows constructed windowing algorithm thereby slows simulation speed thus pgl application processes frequently blocked application events per simulation window one simulation process per node window construction algorithm becomes expensive relative amount simulation work done window factors counteracted extent placing multiple application processes per node simulating multiple vps per node separating application processing nodes simulation nodes example consider pglhigh slowdown 139 64 nodes configuring lapse run 48 nodes 32 nodes application processing different set 16 nodes simulation slowdown decreases 117 similar reductions slowdown obtained pgllow next measure relative speedups running lapse paragon using n virtual processors n physical processors n n ran applications variety combinations n n applications operating system physical memory constraints typically limit level multiprogramming 8 vpsprocessor case bpshigh 4 vpsnode could handled effectively limits size n especially lapse running one processor first consider case simulating small number vps small number nodes specifically simulating 8 vps 1 8 nodes bps number nodes must power 4 case simulate 4 vps 1 4 nodes table 5 shows relative speedups experiments relative speedup defined lapse time one node divided lapse time n nodes applications good lookahead sor bps apucs speedups increase monotonically applications relative speedups 4 nodes 24 37 relative efficiencies 060 092 8 nodes relative speedups range 35 47 relative efficiencies 043 060 pgl maximum relative speedup 17 obtained increasing 1 2 nodes next consider simulating 64 vps 8 64 nodes table 6 shows relative speedups experiments relative speedup defined lapse time 8 nodes 8 vps per node divided lapse time n nodes bpshigh maximum number vps per node 4 speedups stated relative lapse time 16 nodes 4 vps per node speedups higher applications good lookahead lapse runs 39 70 times faster 64 nodes 16 nodes sor bps apucs maximum pgl speedup 25 also able run 512 vp case sorvery low 64 nodes since access 512 nodes actual slowdown could computed however lapse ran 100 times slower time lapse predicted would take application run 512 processors remember lapse processor 8 vps hence least 8 times much work hypothetical larger system application compcomm number processors low 4 3 2 1 bps low 6 1 4 bps high 1 2 3 apucs low 1 2 3 1 apucs high 2 2 1 0 pgl low 1 2 1 5 pgl high 1 2 1 3 table 2 lapse validations percentage errors predicted execution times compcomm maximum slowdown avg application application events per lookahead 4 142 11 8 102 33 93 80 4 61 13 8 43 28 table 3 lapse slowdowns apucs function application lookahead lapse execution time divided native execution time using 8 processors application compcomm number processors low 175 284 240 280 sor high 18 40 41 61 bps low 30 75 119 bps high 19 24 30 apucs low 75 111 135 188 apucs high 30 59 76 129 pgl low 151 616 992 148 pgl high 174 702 116 139 table 4 lapse slowdowns lapse execution time divided native execution time using number processors application compcomm number processors low 10 18 31 47 bps low 10 17 24 bps high 10 20 37 apucs low 10 17 25 38 apucs high 10 16 24 35 pgl low 10 15 16 17 pgl high 10 14 14 16 table 5 lapse speedups 8 vp problem 4 vp problem bps application compcomm number vps low 10 20 35 59 sor high 10 17 42 70 bps low 10 19 29 44 apucs low 10 19 30 41 apucs high 10 18 28 39 pgl low 10 19 22 25 pgl high 10 18 21 23 table relative speedups 64 vp problem times relative 8 processors 8 virtual processors per processor bps high times relative 16 processors 4 virtual processors per processor 6 conclusions paper describes tool lapse supports parallelized direct execution simulation parallel message passing applications describe synchronization protocol whoa suitable directexecution simulation general messagepassing systems provide performance data lapses implementation intel paragon multicomputer synchronization protocol conservative exploits observation messagepassing codes frequently exhibit long periods execution paths insensitive temporal considerations using lapse timing predictions applications running large number processors made executing application smaller number processors simultaneously running timing simulation larger machine lapse timing predictions validated four scientific engineering applications typically predictions within 10 actual execution times often within 5 simulation speed shown depend several factors computation communication ratio application amount application lookahead applications good lookahead slowdowns modest good simulation speedups obtained applications without good lookahead namely whose execution paths depend answers temporally sensitive questions slowdowns quite high however number possibilities increasing lookahead potentially increasing simulation speed applications consider first application calls clock routines current approach lapse block application simulation time advanced point clock call made report simulation time however many applications clock values used well set eg timing intervals applications would possible continue running application beyond clock call accept simulation clock returns asyn chronously block application unreturned clock value actually gets used simulation time yet advanced calling time transparent support mechanism would costly since would involve checking certain storage locations change values namely clock values stored however simple set routines designed read subsequently use clock values would inserted application place clock calls however pose problem context automated instrumentation system would likely desire frequent clock calls second type temporally sensitive call handled query status messages example probe asks message certain type lapse currently blocks application simulation time advanced time probe call made answers probe however approach conservative certain cases suppose application probes yet potentially unknown time say 100 suppose simulator advanced time 50 time probe probed message already present time 50 messages cant canceled guaranteed time 100 simulator could inform application thereby unblocking application requires somewhat elaborate data structures applicationsimulation flowcontrol currently lapse since eg message types may reused simulator must keep track message destined probe however experience pgl code indicates optimization really sophisticated form lookahead well worth pursuing order accelerate simulation number worthwhile extensions lapse pursuing first lapse currently runs paragon provides paragon timing estimates porting lapse work conjunction software package nxlib 15 provides paragon messagepassing library networks workstations nxlib provides workstations paragon functionality port augment functionality timing second model paragons operating system hardware currently fairly crude example current network model pure delay network implemented packetbypacket parallel simulator paragons mesh interconnection network process integrating lapse model paragons operating system also simple include models paragons internal algorithms eg models way paragon manages communications buffers planning investigate incorporate features model especially lookahead calculations need changed resulting complex models addition plan investigate port lapse run model newly emerging mpi message passing interface 17 standard parallel platforms besides paragon acknowledgments grateful jeff earickson intel supercomputers generous assistance helping us better understand intel paragon system r construction preconditioners elliptic problems substructuring impact synchronization granularity parallel systems efficient simulation parallel computer systems rice parallel processing testbed parallel rendering algorithm mimd architectures multiprocessor simulation tracing using tango distributed memory lapse parallel simulation messagepassing programs simon simulator multicomputer networks parallel discrete event simulation efficient instruction level simulation computers hierarchical architecture design simulation environment paragon users guide comparison domain decomposition techniques elliptic partial differential equations parallel implementation paragon parallel programming environment sun workstations dynamictracedriven simulator evaluating parallelism message passing interface forum parallel simulation markovian queueing networks using adaptive uniformization efficient aggregaton multiple lps distributed memory parallel simulations parallel simulation today parallel discreteevent simulation fcfs stochastic queueing networks cost conservative synchronization parallel discreteevent simulations numerical recipes c art scientific computing wisconsin wind tun nel virtual prototyping parallel computers distributed simulation discrete event systems tr ctr phillip dickens workstationbased parallel directexecution simulator acm sigsim simulation digest v27 n1 p174177 july 1997 david nicol analysis composite synchronization proceedings sixteenth workshop parallel distributed simulation may 1215 2002 washington dc kalyan perumalla richard fujimoto prashant j thakare santosh pande homa karimabadi yuri omelchenko jonathan driscoll performance prediction largescale parallel discrete event models physical systems proceedings 37th conference winter simulation december 0407 2005 orlando florida xuhui li jiannong cao yanxiang direct execution approach simulating mobile agent algorithms journal supercomputing v29 n2 p171184 august 2004 ruoming jin gagan agrawal performance prediction random write reductions case study modeling shared memory programs acm sigmetrics performance evaluation review v30 n1 june 2002 david nicol jason liu composite synchronization parallel discreteevent simulation ieee transactions parallel distributed systems v13 n5 p433446 may 2002 thom mclean richard fujimoto repeatability realtime distributed simulation executions proceedings fourteenth workshop parallel distributed simulation p2332 may 2831 2000 bologna italy vikram adve rajive bagrodia ewa deelman thomas phan rizos sakellariou compilersupported simulation highly scalable parallel applications proceedings 1999 acmieee conference supercomputing cdrom p1es november 1419 1999 portland oregon united states sundeep prakash rajive l bagrodia mpisim using parallel simulation evaluate mpi programs proceedings 30th conference winter simulation p467474 december 1316 1998 washington dc united states thomas phan rajive bagrodia optimistic simulation parallel messagepassing applications proceedings fifteenth workshop parallel distributed simulation p173181 may 1518 2001 lake arrowhead california united states ruoming jin gagan agrawal methodology detailed performance modeling reduction computations smp machines performance evaluation v60 n14 p73105 may 2005 rajive bagrodia ewa deeljman steven docy thomas phan performance prediction large parallel applications using parallel simulations acm sigplan notices v34 n8 p151162 aug 1999 rajive bagrodia ewa deelman thomas phan parallel simulation largescale parallel applications international journal high performance computing applications v15 n1 p312 february 2001 ewa deelman rajive bagrodia rizos sakellariou vikram adve improving lookahead parallel discrete event simulations largescale applications using compiler analysis proceedings fifteenth workshop parallel distributed simulation p513 may 1518 2001 lake arrowhead california united states sundeep prakash ewa deelman rajive bagrodia asynchronous parallel simulation parallel programs ieee transactions software engineering v26 n5 p385400 may 2000 vikram adve rajive bagrodia ewa deelman rizos sakellariou compileroptimized simulation largescale applications high performance architectures journal parallel distributed computing v62 n3 p393426 march 2002 jason liu yougu yuan david nicol robert gray calvin c newport david kotz luiz felipe perrone empirical validation wireless models simulations ad hoc routing protocols simulation v81 n4 p307323 april 2005 rajive l bagrodia parallel languages discreteevent simulation models ieee computational science engineering v5 n2 p2738 april 1998