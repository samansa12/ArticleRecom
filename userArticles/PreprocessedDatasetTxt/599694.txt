reinforcement learning call admission control routing quality service constraints multimedia networks paper solve call admission control routing problem multimedia networks via reinforcement learning rl problem requires network revenue maximized simultaneously meeting quality service constraints forbid entry certain states use certain actions problem formulated constrained semimarkov decision process show rl provides solution problem able earn significantly higher revenues alternative heuristics b introduction number researchers recently explored application reinforcement learning rl resource allocation admission control problems telecommu nications eg channel allocation wireless systems network routing admission control telecommunication networks nie haykin 1998 singh 1997 boyan littman 1994 marbach et al 1998 paper focuses applications rl method call admission control cac routing broadband multimedia communication networks atm networks broadband networks carry heterogeneous traffic types simultaneously channels channels packetbased customers send varying rates time calls arrive depart time network choose accept reject connection requests new call accepted network choose appropriate route deliver call source node destination node network provides quality service qos guarantees packet level eg maximum probability congestion call level eg limits call blocking probabil ities return network collects revenue payoff customers calls accepts network network wants find cac routing policy maximizes long term revenueutility meets qos constraints maximizing revenue meeting qos constraints suggests constrained semimarkov decision process smdp mitra et al 1998 rapid growth number states problem complexity led rl approaches prob lem marbach tsitsiklis 1997 marbach et al 1998 however rl applications ignored qos criteria work draws closely related fundamental problem constrained optimization semimarkov decision processes studied researchers control theory operation search artificial intelligence communities see eg altman shwartz 1991 feinberg 1994 gabor et al 1998 unlike modelbased algorithms eg linear programming mitra et al 1998 rl algorithm used paper stochastic iterative algorithm require priori knowledge state transition probabilities associated underlying markov chain thus used solve real network problems large state spaces cannot handled modelbased algorithms automatically adapt real traffic conditions work builds earlier work authors brown et al 1999 provides general framework studying cac routing problem qos constraints also provides detailed information proofs rl algorithm used study contains results combined cac routing multimedia network reported tong brown 1999 section 2 describes problem model used study section 3 formulates cac problem smdp gives rl algorithm solves smdp section 4 considers qos constraints details simulations cac single link system presented section 5 combined cac network routing studied section 6 simulation results 4node 12link network section 7 concludes paper 2 problem description section describes cac problem singlelink communication system substantial literature cac one link multiservice networks eg marbach tsitsiklis 1997 mitra et al 1998 references dziong mason 1994 single link case significant since basic building block larger networks shown section 6 paper combined cac routing multilink network system decomposed single link processes thus first focus singlelink system users attempt access link time network immediately chooses accept reject call accepted call generates traffic terms bandwidth function time later time call terminates departs network call accepted network receives immediate revenue payment network measures qos metrics transmission delays packet loss ratios call rejection probabilities service class compares guarantees given calls problem described call arrival traffic departure processes revenue payments qos metrics qos constraints network model concrete describe choices used later examples calls divided discrete classes indexed calls generated via independent poisson arrival processes arrival rate exponential holding times mean holding time 1 within call bandwidth onoff process traffic either generating packets rate r rate zero mean holding times 1 1 class call admitted system collects reinforcement learning call admission control 3 fixed amount revenue interpreted average reward carrying igammath class call dziong mason 1994 network element connects network fixed bandwidth b total bandwidth used accepted calls varies time one important packetlevel qos metric fraction time total bandwidth exceeds network bandwidth causes packet losses ie congestion probability choose packetlevel qos guarantee upper limit congestion probability p denote capacity constraint previous works eg carlstrom nordstrom 1997 marbach et al 1998 call constant bandwidth time effect qos predictable variable rate traffic safely approximated assuming always transmits maximum peak rate peak rate allocation underutilizes network cases orders magnitude less possible network efficiency improved statistical multiplexing statistically bursty sources unlikely simultaneously communicate peak rates thus possible carry bursty variable rate traffic would possible allocating capacity according peak rate requirements maintaining service quality stochastic traffic rates real traffic desire high network utilizationrevenue resulting potential qos violations characterize problem study another important qos metric calllevel blocking probability offered traffic class must cut back meet capacity constraint important fairly denote fairness constraint fairness defined number different ways one intuitive notion calls every class entitled admission probability equivalently rejection probability dziong mason 1994 precisely defined section 4 ultimately goal find policy every system state chooses correct control action maximize revenue subject qos constraints formally consider following problem finding cac policy maximizes j 0 1 subject j j l fset policiesg 3 k number qos constraints l real numbers characterize qos constraints j 0 characterizes average network revenue policy j j characterize qos policy consider objectives form k action chosen state n according policy reward functions associated revenue assumed bounded n average sojourn times state n action n indexes ngammath decision epoch decisions made points time referred decision epochs 3 semimarkov decision processes reinforcement learning following sections develop components problem finish justifying particular method suitable cac problem 31 states actions section develops state action model reduced state space representation suitable cac problem cac problem formulated semimarkov decision process smdp state transitions control selections take place discrete times time one transition next continuous random variable given point time system particular configuration x defined number type ongoing calls number calls state type random times event e occur one event occur time instant e gammavector indicating either class call arrival call termination call turned call turned event configuration event together determine state system e iclass system 3i dimensional vector since number possible choices e general small compared x size state space dominated configuration part state shown using nearly complete decomposability approximation reduce state descriptor form stands call arrival departure event class let configuration e denote gammavector whose elements equal zero except ith element whose value unity states associated class call arrival states associated class call departure reduction ignoring number calls state events call turned gives us enough accuracy cac problem shown experimentally mitra et al 1998 give two reasons simplification first moment call turns decision point admission controller therefore action needs taken theorem 2 appendix shows ignoring events call turned valid section 34 also provides discussions similar simplifications second intuitively clear simplification good approximation process describing number calls state reaches equilibrium change number calls progress due call arrivaldeparture hence making call admission decision number calls class progress important number calls class state quantities oscillate rapidly relative call arrivals departures view ignorance state aggregation assume fixed x qvalues change much different reinforcement learning call admission control 5 discussions section 36 justify reduction dropping note affect congestion probability assuming process reaches equilibrium corresponds fixed x assuming source independence probability configuration x given binomial distribution b fraction time class call spends state average congestion probability class fixed x thus x 0 1fdeltag indicator function average congestion probability depends x capacity constraints associated 6 conservative set set x long run average packetlevel qos constraints always satisfied never go state period time capacity constraint violated stay forever set c c uniquely determines state space x e mitra et al 1998 considers aggressive approach packetlevel qos constraints averages across allowable configurations x let x2ca x total system time x portion system spends x c set allowable configurations x x2ca less equal target p obviously c unique c c possible c although general conservative occasions emphasis dependence c c c p also write c c p c p summary choose state descriptor number class calls progress e stands new class call arrival gammae class call departure 1 event occurs learner choose action feasible event action set asf0reject 1acceptg upon new call arrival call terminations decision points action needs taken symbolically 6 h tong tx brown states asfgamma1no action due call departuresg note actions available state general depend example adding new call state violate capacity constraint action set state constrained f0g subsequent random time another event occurs cycle repeats revenue structure cac task learner determine policy accepting calls given maximizes longrun average revenue infinite horizon meeting qos requirements cac system constitutes finite state space eg due capacity constraint finite action space afgamma101g semimarkov decision process 32 transition probabilities section considers probability model concludes large state spaces classical approaches based transition probability model fea sible theoretically state transition probability ps 0 probability going state action next state 0 derived mitra et al 1998 depends configuration x call arrival rates exact system models often infeasible several important reasons first call arrival rates may depend call class also configuration x dziong mason 1994 therefore call arrival rate class may constant general second network reasonable size state space extremely large example 4node 12link network 3 service types 10 states marbach et al 1998 even possible explicitly list states finally fixing model computing optimal policy means robust actual traffic condition departs assumed model reasons clear practical system large state space difficult impossible determine exact transition model markov chain performing modelbased algorithm compute optimal policy main motivation study apply modelfree rl algorithms solve cac problems although explicitly compute transition probabilities make following assumptions study let ss 0 continuous random inter transition time state state 0 action probability distribution f ss 0 ja assumption a1 assumption a2 expectation ss 0 2 reinforcement learning call admission control 7 z 1df ss 0 ja 10 particular exists assumption a3 unichain condition every stationary policy transition matrix ps 0 determines markov chain one ergodic class possibly empty set transient states assumption a1 guarantees transition probabilities well defined assumption a2 guarantees number transitions finite time interval almost surely finite a3 guarantees except initial transient states state reach state nonzero probability 33 qlearning section develops rl methodology used paper unconstrained maximization revenue qos constraints considered section 4 learn optimal policy using watkins qlearning algorithm watkins dayan 1992 given optimal qvalues q policy defined optimal particular 12 implies following procedures call arrives qvalue accepting call qvalue rejecting call determined rejection higher value drop call else acceptance higher value accept call one action qvalue exists call departure learn q update value function follows transition state 0 action time ss 0 stepsize learning rate k integer variable index successive updates ff 0 chosen sufficiently close 0 discounted problem equivalent average reward problem tauberian approximation gabor et al 1998 well known qlearning 13 robbinsmonro stochastic approximation method solves socalled bellman optimality equation associated decision process let z 1e gammaff df ss 0 ja 2 assumption a2 guarantees h contraction mapping contraction factor z 1e gammaff df ss 0 ja respect maximum norm theorem 1 suppose thatx 2 stateaction pair updated infinite number times q k converges probability 1 q every proof see bertsekas tsitsiklis 1996 34 simplified learning process practical issue concerning implementation qlearning 13 discussions qlearning needs executed every state transition including transition caused call departure feasible action set one action states associated call departures necessary learn optimal qvalues states induce optimal policy states possible avoid updates qvalues departure states still get optimal policy reduce amount computation storage qvalues significantly since state space almost halved dropping call departure states note interesting states decisions need made associated call arrivals g decision point jumps one arrival next arrival interarrival period may contain zero one departures given e j first arrival e cases n 0 departures two adjacent arrivals chapmankolmogorov equations bertsekas gallager 1992 transition probability actual decision process intermediate state corresponds call departure shown appendix optimal policy obtained qlearning states associated call arrivals result reinforcement learning call admission control 9 intuitive since call departures random disturbances affect state transitions even though 18 complicates already intractable transition model smdp since qlearning depend explicit model asymptotic convergence optimal policy follows 35 exploration order qlearning perform well potentially important stateaction pairs must explored specifically convergence theorem qlearning requires stateaction pairs tried infinitely often section develops exploration strategy suitable cac problem common way try stateaction pairs rl small probability ffl random action rather action recommended rl chosen decision point training socalled fflgammarandom exploration cac problem considered paper without exploration states visited probabilities several orders higher states experiments shown fflgammarandom exploration unlikely help situation therefore training states visited many times states visited times resulting qvalue functions far converging optimal policy cannot expected reasonable time see call arrival process modeled truncation independent mm1 queues system truncated system untruncated system except configurations capacity constraint violated eliminated stationary distribution system assuming greedy policy policy always accepts new call capacity constraint violated adding new call given bertsekas gallager 1992 g normalization constant allowed set configurations truncated system since state action deterministically define next configuration x 0 next state 0 event part 0 e 0 arrival action needs taken occurs independent x 0 probability determined due memoryless assumption stationary distribution states 0 depends 19 example consider experimental parameters shown table 1 section 5 except simplify calculation allowable configuration set c truncated system use peakrate allocation 1g using 19 20 visited state 02297 least visited state p ie five orders difference stationary distribution stateaction pairs small system shown szepesvari 1998 convergence rate qlearning approximated suitable constant b 0 k index 13 defined 15 overcome slow convergence caused small value p min pmax stationary distribution controlled exploration scheme derived based facts qlearning offpolicy learning method sutton barto 1998 section 76 smdp state transitions thus state distri bution controlled choosing appropriate actions state training one feasible actions probability ffl control action chosen leads least visited configuration fflgammadirected heuristic effectively reduces difference number visits states significantly speeds convergence value functions terms qlearning formula 13 action chosen according exploration scheme action b chosen according current qvalue 36 function approximation vs lookup tables qlearning deals effectively curse modeling explicit state transition model needed simulator used instead another major difficulty smdp problems curse dimensionality exponential state space explosion problem dimension treatment assumed problem state space kept small enough lookup table used clearly number stateaction pairs becomes large lookup table representation infeasible compact representation q represented function smaller set parameters using function approximator necessary paper choose approximation architecture correspond state aggregation consider partition state space disjoint subsets gammadimensional parameter vector oe whose mth component meant approximate qvalue function states 2 sm action words dealing piecewise constant approximation qs value small lookup table used aggregated problem case shown bertsekas tsitsiklis 1996 qlearning converges optimal policy aggregated problem function approximators used may perform well practice however convergence result state aggregation case wish avoid proposition 68 bertsekas tsitsiklis 1996 tauberian reinforcement learning call admission control 11 approximation easy show performance loss due state aggregation bounded j 0 0 optimal average revenue per unit time original aggregated problem respectively defined 15 cac state aggregation interpreted featurebased architecture whereby assign common value oem states given share common feature vector example feature vector may involve call class three value indicator specifies whether load call class high medium low system instead specifying precisely number ongoing calls class x since states similar numbers calls would expected similar qvalues expected small therefore state space greatly reduced lookup table used 37 summary section formulates cac problem smdp justify qlearning approach solving cac problem shows simplify problem ignoring details within call processes computing qvalues states decision standard fflgammarandom exploration policies significantly slow learning problem simple fflgammadirected exploration strategy introduced aggregation states shown simplifying heuristic follows readily problem structure next section develops method incorporating constraints framework 4 constraints restrict maximization policies never violate qos guarantees 1 3 general smdp problems constrained optimal policy randomized stationary policy randomizes k states problem k gammaconstraints feinberg 1994 however modelbased linear programming algorithms employed derive policy impractical cac number states large since randomizations needed k states usually much smaller total number states nonrandomized stationary policy learned rl often good approximation constrained optimal policy gabor et al 1998 general smdp due stochastic state transitions meeting constraints may possible eg state matter actions taken possibility entering restricted states admission control service quality depends number calls admitted system adding calls strictly controlled admission controller meeting qos constraints possible consider two important classes qos constraints cac integrated service network one statedependent constraints past dependent constraints conservative capacity constraint example statedependent constraints statedependent constraints qos intrinsic state congestion probability function solely number calls progress current state cf 6 pastdependent constraints depend statistics past history example fairness criterion fairness depends statistics rejection ratios past history address two constraints separately 41 capacity constraint simplicity consider total packet congestion probability upper bound p conservative approach means set c c p cf 6 7 x 0 stated conservative capacity constraint intrinsic property state depends current state allows us collect qos statistics state treat principled way eg computing confidence intervals estimates current state action n uniquely determine next configuration xn1 projected congestion probability next state n1 determined xn1 therefore forecast impact need evaluate pxn1 expected congestion probability greater less constraint p action cause pxn1 action eliminated feasible action set asn cac adding new call violate capacity constraint feasible action reject new call request considering aggressive capacity constraint need determine set c c p allowable configurations defined implicitly uniquely lim x2ca x total time system spends x x2ca x note distribution xt depends control policy generalization case different service types different packetlevel qos requirements easily made reinforcement learning call admission control 13 stated c c p serves possible c usually conservative construct aggressive set c p gradually decrease p c 1 find series sets c c p c corresponding changing p c clearly size c c p nonincreasing decrease p c however must always contain c c p practice value p c0 learned policy aggressive congestion probability sufficiently close still less constraint p search c p stop choose c p c0 aggressive capacity constraint essence try find corresponding value conservative threshold p c aggressive threshold p construct c conservative approach way aggressive capacity constraint remains statedependent constraint conservative capacity constraint implement constraint constraining action set state although c determined way may aggressive one term revenue maximization 1 3 loss optimality expected small 42 fairness constraint measured rejection ratio class upon nth call arrival nth decision made arbitrarily constraints r n may able find feasible policy fairness constraint involves comparisons rejection ratios types calls formulate fairness constraints 1ii 1ii l maximum allowed rejection ratio discrepancy feasible policy exists always rejecting call types aggressive fairness constraint formulated lim l 28 sn sn1 intertransition duration state n n1 action formulation constrained smdp problem 1 3 capacity constraint implemented constraining feasible action set state described preceding subsection deal fairness constraint use lagrange multiplier framework studied beutler ross 1986 since fairness constraint pastdependent constraint vector rsn1 depends rejection ratios past tory fit framework need include history information state descriptor new state descriptor form gammavector req resp rej denotes total number call requests resp rejections class current call arrival time interval last current call request original state 14 h tong tx brown descriptor obtain markov chain expansion however state space enlarged significantly specifically due inclusion req rej state space infinite must resort form function approximation solve smdp problem paper use state aggregation approximation architecture quantizing rejection ratios r terms lagrange multiplier consider unconstrained optimization parametrized reward original reward function associated cost function associated constraint numerator 28 exists nonrandomized policy solves bellman optimality equation associated reward function 30 mean time achieves equality 28 beutler ross 1986 shows constrained optimal policy case optimal policy exist shown constrained optimality achieved randomization one state 0 two nonrandomized policies 2 differ slightly undershooting resp overshooting l clearly case nonrandomized constrained optimal policy exist 1 next best nonrandomized policy loss optimality minimal reasons avoid complications randomized policies concentrate nonrandomized policies study 43 summary section shows constraints introduced problem either modulating action space modifying reward function optimality requires randomized policy since policy needs randomized two states many states greatly simplify search restricting deterministic policies 5 simulation results experiments use following model total bandwidth normalized 10 unit traffic per unit time target congestion probability p two source types considered properties shown table 1 fairness constraint average rejection ratio discrepancy two service types differ l noted holding times exponential first concentrate conservative approach capacity constraint since exploration employed ensure potentially important stateaction reinforcement learning call admission control 15 table 1 experimental parameters source type parameter ii rate r 008 02 mean period 1 5 5 mean period 1 15 45 call arrival rate 0067 02 call holding time 1 immediate payoff pairs tried naturally enables us collect statistics used estimate qos stateaction pairs emphasized single visit state sufficient determine long run qos metrics due variability within call process number times stateaction pair visited increases estimated service quality becomes accurate confidence gradually eliminate stateaction pairs violate qos requirements consequence value function updated gradually correct subset stateaction space sense qos requirements met action within subspace stated section 4 capacity constraint eliminates stateaction pairs violate congestion probability upper limit experiments use simple way eliminate stateaction pairs confidence since target congestion probability total number visits configuration x counted number time steps simulation wx number congestions x wx x 200 wx 20000 conclude acceptable thresholds provide close approximations confidence intervals brown 1997 sophisticated way estimate px proposed tong brown 1998 artificial neural networks nns trained based maximum likelihood principle nn estimates px extrapolate well p simulations discount factor ff chosen 10 gamma4 learning rate exploration initial qvalues rl artificially set qlearning started greedy policy training completed apply test data set compare policy obtained rl alternative heuristic policies final qos measurements obtained end rl training learning qos used testing different policies test rl policies new call arrival algorithm first determines accepting call violate qos call rejected else action chosen according arg max a2as qs 0rejectg qos constraint use three cases peak rate allocation statistical multiplexing function learned online denoted qos learned statistical multiplexing function given priori denoted qos given examine six different cases 1 rl qos given 2 rl qos learned 3 rl peak rate heuristic accepts calls valuable class ie type qos given 5 greedy qos given 6 greedy peak rate results shown fig 1 clear simultaneous qlearning qos learning converges correctly rl policy obtained giving qos priori standard qlearning see significant gains 15 due statistical multiplexing 6 vs 5 3 vs 1 gains due rl 25 6 vs 3 5 vs 2 together yield 45 increase revenue conservative peak rate allocation example also clear figure rl policies perform better heuristic policies fig 2 shows rejection ratios different policies consider aggressive approach capacity constraint simulation found value p corresponds aggressive capacity constraint p acceptance regions ie c c c aggressive conservative approaches shown fig 3 aggressive acceptance region much larger conservative one figure number type ii users starts two due insufficient measurement data confidence level region comparing figs 4 5 figs 1 2 see aggressive approach earns significantly revenue conservative approach greedy policy rl policy note peak rate allocation earns total amount rewards unnormalized approaches fig 4 qvalues initialize rl policy starts greedy policy examples performance improvement due rl significant improvement due statistical multiplexing fairness constraint imposed case rejection ratios two types calls differ significantly fairness constraint requires two rejection ratios cannot differ 5 average test rl fairness constraint set reward parameters type call 1 type ii call 10 keep parameters table 1 unchanged stated use featurebased state aggregation cope difficulty large state space caused fairness constraint specifically learn qhs instead qs feature quantization following experiment experienced rejection ratio discrepancy frs quantized 100 levels quantized 2 levels corresponding 4 approximate average interarrival time although aggregated experiment cases complicated also possible aggregate simpler feature found 800 simulation learned rl policy compared greedy policy fairness constraint accepts calls long fairness constraint met otherwise fairness constraint violated accepts calls class experiencing highest rejection ratio results shown figs 6 7 fairness strong constraint possible policies gain due rl reduces expected figs 1 4 6 see qlearning converges quickly fact rl curves figures show oscillations connected learning rates reinforcement learning call admission control 17 total reward comparison different policies exponential onoff246 1rl qos given 2rl qos learned 3rl peak rate 4greedy type 5greedy qos given 6greedy peak rate figure 1 comparison total rewards rl learning qos capacity constraint rl given qos measurements rl peak rate greedy policies peak rate allocation normalized greedy total reward rates 1greedy peak rate 2rl peak rate 3greedy qos given 4rl qos learned exponential onoff figure 2 comparison rejection ratios policies learned fig 1 13 specifically order qlearning converge fl k satisfy 16 17 simulations used small constant learning rate condition 17 met reason 17 adhered typically prior knowledge decreased learning rate becomes small algorithm may stop making noticeable progress training process could become long 6 combining cac network routing general issues cac routing closely related communication network combined cac routing also formulated smdp however exact characterization network state would require specification number calls progress class possible route network detailed specification state intractable computation assuming statistical independence links network dziong 1997 krishnan 1990 form decompositions network routing process single link processes usually employed dziong mason 1994 marbach et al 1998 based preceding results single link admission control link state independence approximation propose decomposition rule allows decentralized training decision making combined cac routing network also tries maximize network revenue 123579number users class number users class ii comparison accept regions aggressive conservative figure 3 comparison acceptance regions total reward comparison different policies exponential onoff rl qos learned greedy qos given greedy peak rate figure 4 comparison total rewards rl learning qos capacity constraint greedy policy peak rate alloca tion normalized greedy total reward aggressive 30103050709rejection rates 1greedy peak rate 2greedy qos given 3rl qos learned exponentialonoff figure 5 comparison rejection ratios policies learned fig 4 reinforcement learning call admission control 19 total reward comparison different policies exponential onoff rl qos learned greedy qos given figure 6 comparison total rewards obtained rl policy greedy policy capacity constraint fairness constraint imposed normalized greedy total reward rejection rates figure 7 comparison rejection ratios capacity constraint fairness constraint policies learned fig 6 let r denote predefined routes network action space system action due call departures route new call route r 2rg link j node node j keeps separate link state variable whenever new call type k routed route r contains link j immediate reward associated link j equal c ij satisfying ij2r example number links along route r qlearning performed link similarly single link case arrival update qvalue link j arrival associated link new type k call originated node destined node decision made node following way od set routes carry call without violating qos constraints ii define net gain g r accepting new call routing decision r ae p ij2r theta figure 8 network model admission routing decision r r2as od f0g r 34 decision making r reject call otherwise route call route r approach although network state simplified link state link action space link simplified acceptg dziong mason 1994 marbach et al 1998 important since link qfunctions distinguish singlelink calls multilink calls avoid accepting many multilink calls block singlelink calls may bring amount revenue using less network resources table 2 experimental parameters source type parameter ii iii rate r call arrival rate 01 01 0067 call holding time 1 200 180 120 immediate payoff present simulation results obtained case network consisting 4 nodes 12 unidirectional links two different classes links total bandwidth 15 2 units respectively indicated thick thin arrows reinforcement learning call admission control 21 fig 8 assume three different source types whose parameters given table 2 call arrivals node independent poisson processes mean destination node randomly selected among three nodes source destination node pair list possible routes consists three entries direct path two alternative 2hop routes emphasize effect rl consider capacity constraint assume peak rate allocation link simulations use featurebased state aggregation approximate qvalues link learn qhs r instead qs r ie numbers ongoing calls type aggregated eight levels policy obtained rl compared commonly used heuristic policy gives direct path priority direct path reaches capacity heuristic try 2hop routes find one violate capacity constraint route exists call rejected results given fig 9 total fig 10 call rejection ratios fig 11 routing behavior results show rl policy increases total revenue almost 30 compare commonly used heuristic routing policy 7 conclusion paper formulates cac routing problem constrained smdp provides rl algorithm computing optimal control policy incorporate two important classes qos constraints statedependent pastdependent con straints rl solution maximize networks revenue formulation quite general applied capacity fairness constraints approach experimented single link well network problem showed significant improvement even simple examples future work includes study combined cac routing studying function approximators neural networks approximate qvalue functions acknowledgments work funded nsf career award ncr9624791 appendix proof simplified learning process following theorem shows avoid learning qvalues state transitions corresponding calls turned section 31 call departures section 34 let j ff cs asg set intermediate states g 22 h tong tx brown total reward comparison different routing policies exponential onoff rl heuristic figure 9 comparison total rewards 4node network normalized heuristic total reward 20103050709rejection ratios 1heuristic 2rl exponential onoff rejection rates figure 10 comparison rejection ratios policies fig 9 routingheuristic portion calls routed direct paths direct 1st 2hop 2nd 2hop routingrl portion calls routed direct paths direct 1st 2hop 2nd 2hop figure 11 comparison routing behavior policies fig 9 reinforcement learning call admission control 23 theorem 2 assume 2 2 2 takes sa steps go state 0 states 0 1 1 optimal stationary policy modified decision process considering states 2 also optimal original decision process proof optimal policy original problem z 1e gammaff df ss 0 ja 2 optimal policy ff modified decision process since one feasible action 2 1 2 2 n 18 sa due assumption hi z 1e gammaff df ss 0 ja oe 0 first state 2 define z 1e gammaff df ss 0 ja since sa finite without loss generality assume values sa procedure similar summation term a3 becomes hi deltads second summation second term formula due condition ps combining a5 a6 a3 a4 a7 z 1e gammaff df ss 0 ja uniqueness optimal value function easy verify j ff therefore a1 a2 a8 proof eg a4 used memoryless property transition processes cac states serve 0 since 0 possible call departures due capacity constraint state action take finite number consecutive call departures reach state like 0 r adaptive control constrained markov chains data networks adaptive statistical multiplexing broadband communications optimizing admission control ensuring quality service multimedia networks via reinforcement learning control selfsimilar atm call traffic reinforcement learning call admission routing multiservice loss networks ieee trans atm network resource management constrained semimarkov decision processes average reward international conference machine learning markov decision algorithms dynamic routing neurodynamic approach admission control atm networks single single link case reinforcement learning call admission control routing integrated service networks robust dynamic admission control unified cell call qos statistical multiplexers reinforcement learning dynamic channel allocation cellular telephone systems reinforcement learning asymptotic convergencerate qlearning advances nips 10 estimating loss rates integrated services network neural networks adaptive call admission control quality service con straints reinforcement learning solution tr