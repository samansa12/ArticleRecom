efficient errorcorrecting viterbi parsing abstractthe problem errorcorrecting parsing ecp using insertiondeletionsubstitution error model finite state machine examined viterbi algorithm straightforwardly extended perform ecp though resulting computational complexity become prohibitive many applications propose three approaches order achieve efficient implementation viterbilike ecp compatible beam search acceleration techniques language processing shape recognition experiments assess performance proposed algorithms presented b introduction problem errorcorrecting parsing ecp fundamental syntactic pattern recognition spr 11 13 data generally distorted noisy also arises many areas language modeling 23 6 speech processing 7 22 ocr 18 grammatical inference 20 coding theory 8 15 sequence comparison 21 many problems arising several research areas ecp related finding best path trellis problem solved viterbi algorithm va 10 wellknown mceliece 19 makes good description operation complexity va application decoding linear block codes hand wiberg et al 24 use alternative decoding scheme based tanner graphs rather trellises nevertheless work described 8 15 seems specifically deal ecp problem stated sect 2 henceforth shall concerned application ecp spr assumed stochastic finite state model fsm stochas tic error model accounting insertions substitutions deletions symbols given 1 symbols belong alphabet sigma stands set primitives features characterise given pattern aim recognise therefore objects represented strings symbols belonging sigma hand fsm accounts generally infinite set different strings corresponding several ways given object represented error model accounts typical variations pattern strings tend exhibit regard standard forms represented fsm error model given recognition problem amounts simple problem finitestate parsing given input string symbols compute probability string belongs language generated fsm point view interested maximum likelihood derivation viterbilike string instead sum likelihoods every derivation forwardlike multiclass situation one fsm provided class probabilities used recognition using maximum likelihood classification rule string classified class represented fsm whose probability generating string maximum however many cases test strings cannot exactly parsed fsms leading zero probabilities classes often solved ecp fsm deterministic non errorcorrecting parsing trivial otherwise va used error model provided viterbi framework adopted ecp expense higher computational cost unfor 1 note work also related problem approximately matching regular expres sions since equivalent fsms see 2 introduction problem tunately higher cost become prohibitive many applications interest computational problem ecp outlined next section solutions problem proposed sects 3 4 5 sect 6 describes adaptation wellknown beam search technique 17 accelerate parsing process sect 7 presents experiments carried test performance distinct approaches 2 computational problem ecp general problem finitestate parsing error correction formulated search minimum cost path 2 trellis diagram associated fsm given input string x trellis directed acyclic multistage graph node q j k corresponds state q j stage k stage k associated symbol x k string parsed every edge trellis stands transition state q stage k state q j stage k thanks acyclic nature graph dynamic programming dp used solve search problem leading wellknown viterbi algorithm trellis diagram straightforwardly extended parse errors produced changing one symbol another symbol errors produced inserting symbol symbol original string way taking substitution insertion errors account efficient ecp implemented extended trellis diagram still shape directed acyclic multistage graph fig 1 b unfortunately extension trellis diagram also parse errors produced deletion one consecutive symbols original string results kind graph edges nodes belonging stage k fig 1 c nevertheless fsm cycles resulting graph still acyclic dp negative log probabilities sums rather products used avoid underflows c b k k k k figure 1 trellis substitution proper fsm transitions b insertion transitions c deletion transitions acyclic fsm deletion transitions cyclic fsm edge actually labelled symbol sigma still applied leading efficient algorithm implemented simple extension va 3 however cycles exist fsm dp longer directly used problem becomes one finding minimum cost path general directed cyclic graph fig 1 noted 15 still take advantage fact edges kind graphs lefttoright structure consider column separate stage like va 3 solving problem score ordering bouloutas et al 8 formulate interesting recurrence relation solve problem stated sect 2 notation follows l stage k 8l stage k 1 cost minimum cost path initial states state q stage k inverse transition function ffi fsm k1 cost minimum cost transition state q stage k state q l stage k k1 cost minimum cost path state q l state q j stage k 1 correctness lies since pair states fsm evaluation yields cost minimum cost deletion path fig 2 shows algorithm called ev1 developed 1 q set states fsm lines 13 referred initblock lines 1216 inssubsblock line retblock remainder paper 1 j 2 q 2 j initial state cq j initialcost else cq j 3 endfor 4 jxj 5 7 8 9 cq j 10 endfor 11 endwhile 12 2 q 13 l 2 ffiq 14 cq l cq l 15 endfor 16 endfor 17 endfor 18 return argmin final state figure 2 algorithmic scheme ev1 ev1pq given transitions negative cost dijkstras strategy followed order compute lines 611 fig 2 transitions state discarded perform computation state q l whose score given parsing stage minimum chosen score j 2 ffiq l updated minimumscore state chosen scores direct successors updated states whose score could updated input string x parsed thetajxj delta using ev1 care implementation q 0 taken algorithm improved using priority queues 1 implementation q 0 implementation scores therefore positions states heap need dynamically changed done simply storing pointer state heap order perform heapify operation position state whose score changed worstcase time complexity loop lines 611 fig 2 case ojqj given jqj delta b operations performed heap 1 b branching factor maximum number transitions associated state q since b jqj many cases worstcase time complexity version ev1 called ev1pq throughout paper parse input string x ojxj log jqj note fsm fullyconnected graph performance ev1pq worse ev1 4 solving problem iteratively another approach coping deletion problem consists performing consecutive iterations compute minimum cost path state parsing stage k line 9 fig 2 using deletion transitions fig 1 c iterative procedure performed score updating produced properness overall computation guaranteed idea independently proposed 20 15 though paper hart bouloutas comprehensive work deals many kinds error rules efficiently copes associated computational problems resulting algorithm called ev2 shown fig 3 let number iterations done lines 39 fig 3 time complexity lines ot delta jqj delta b would 1 deletion transition changed score state fsm 5 least one deletion transition per state changed score state consecutive iterations would jqj produced states traversed reverse topological score order 5 ev2 thus parse input string x ojxj delta jqj delta b 1 initblock 2 jxj 3 repeat 4 l 2 q 5 j 2 ffiq l 7 endfor 8 endfor 9 cq j k changed j 2 q 10 inssubsblock 11 endfor 12 retblock figure 3 algorithmic scheme ev2 ojxj delta jqj 2 delta b best worst cases respectively performance average case strongly depends following order states fsm closely possible parsing deletion transitions number effective deletion transitions 5 5 solving problem depthfirst ordering propose algorithm based recurrence relation extends previous ideas 20 ecp acyclic fsms general fsms l stage k 2 equation 1 generalisation function returns given state q set states topological predecessors q fsm initial state w q l k1 minimum cost path state q l state q j stage k 1 includes states topological predecessors state q j computation w performed following topological order states fsm parsing deletion transitions fsm cycles depthfirst topological sort states computed detecting backedges 3 ie transitions produce cycles fsm leads fixed order traversal list states fsm parsing process backtracking becomes necessary ensure correctness overall computation backedge coming state q another state q j updates cost q j solution problem stated sect 2 unfortunately directly compatible beam search bs techniques using bs list states traversed different almost every parsing stage therefore large computational overheads could introduced parsing process compute depthfirst sort list visited states parsing stage avoided depthfirst sorting states visited using bucketsort binsort techniques 1 need use adequate ordering key proposal compute store ordering key psi 8i 2 q shown fig 4 1 2 q 2 backedges coming 3 endfor 4 ae 5 edge q backedge 7 ae 8 endfor 9 endfor figure 4 computation psi 8i 2 q psi ae implemented arrays easily shown relation set psi 8i 2 q partial order 1 therefore pair states q q j psi exists transition path q q j vice versa permutation maps q nondecreasing sequence piq found using psi means bucketsort number buckets used max depthfirst traversal fsm along computation psi 8i 2 q performed preprocessing stage taking ojqj delta b computing steps 4 5 piq found thing worry given parsing stage find backedge parsed backedge parsed backtracking performed score q j changed two algorithms based equation 2 ev3 ev3v2 developed 5 algorithms piq implemented hash table fig 5 shows ev3 algorithm ev3v2 similar uses list piq performing parsing insertion substitution deletion errors ev3 uses unsorted list q bs purposes see next section parsing insertions substitutions list piq parsing deletions 1 initblock 2 3 jxj 4 l 2 piq 5 j 2 ffiq l 7 j 62 piq add j bucket psi j piq endif 8 cq j k changed psi 9 backtrack bucket psi j piq endif 10 endfor 11 endfor 12 2 q 13 l 2 14 cq l cq l 15 l 62 piq add l bucket psi l piq endif 16 endfor 17 endfor 18 endfor 19 retblock figure 5 algorithmic scheme ev3 time complexity lines 411 fig 5 time finding piq times maximum branching factor b piq found using bucketsort complexity bucketsort sort n elements number buckets case psi clearly bounded jqj see fig 4 best case backedge requires backtracking recomputation resulting time complexity therefore ojqj delta b worst case linearon jqj number backedges requiring backtracking recomputation paths already computed 3 20 leading ojqj 2 delta b time complexity ev3 ev3v2 parse input string x ojxj delta jqj delta b ojxj delta jqj 2 delta b best worst cases respectively performance average case depends structure fsms also number backedges require backtracking computation state reached theoretical formulation average cost difficult requires assumptions probabilistic distributions space possible fsms always feasible 6 beam search beam search 17 bs classical acceleration technique va search technique often yields approximately optimal even optimal solutions drastically cutting search space 22 5 respect bs comparable clever strategies based search proposed 14 idea keep beam promising paths stage trellis beam width constant value empirically tuned achieve adequate tradeoff efficiency accuracy search lower parameter lower accuracy lower computing time vice versa implementation bs consists keeping paths visited states score lower given bound sake efficiency bound implemented adding currently found lowest score beam width 22 q implemented double linked list leaving first place lowestscore state avoid overhead strategy generally results significant differences 3 upper bound quite pessimistic since assumes depthsearch ordering resulting number backedges change score state linear jqj regard strict implementation bs extension algorithms ev1 ev1pq ev2 perform bs straightforward 4 5 case ev3 ev3v2 extension easy thanks ordering key psi allows building list piq states visited see sect 5 ev1 ev1pq ev2 use linked list slight differences number visited states exist due fact ev1 ev1pq parse deletion transitions score order ev3 also uses linked list able tightly follow bs strategy slight differences exist due fact ev3 parses deletions topological order ev3v2 parses transitions following order problem computation first bound value used next parsing stage k since likely first state piq minimumscore one ev3v2 overcomes problem computing approximate bound value minimum cost edge lowestscore state found parsing stage k plus beam width practice differences number visited states among algorithms prove negligible 5 7 experiments results two series experiments carried order assess effectiveness parsing results ecp mainly efficiency speed algorithms previously discussed first case ecp used clean artificially distorted sentences language learning task called miniature language acquisition second case ecp applied recognise planar shapes hand written digits coded chaincoding contours corresponding images 18 cases required stochastic fsms automatically learned means ktsi grammatical inference algorithm proposed 12 algorithm infers stochastic fsm accepts smallest ktestable language strict sense kts language contains training sentences stochastic kts 4 consists pseudonatural language describing simple visual scenes vocabulary 26 words languages equivalent languages modeled wellknown ngrams increasing values k 2 10 2 7 used first second series experiments yielded increasing size fsms required studying computational behaviour different algorithms cases roughly handtuned errormodel parameters used experiments carried hp9000 unix workstation model 735 performing 121 mips 71 language processing experiments set nine stochastic fsms ranging 26 71 538 states automatically learned 50 000 clean sentences mla task 9 used experiments test set consisted 1 000 sentences different used training distorted using conventional distortion model 16 order simulate kinds errors typically faced speech processing tasks generally resulted sentences longer belonged languages learned fsms three different percentages global distortion evenly distributed among insertion deletion substitution parameters used 5 10 effectiveness ecp summarised table 1 quality parsing measured terms word error rate wer 5 original undis torted test sentences obtained ecp corresponding distorted sentences testset perplexity pp 6 results obtained without bs identical algorithms adequately learned kts models distortion wer test sentences reduced factor ranging 2 3 best results obtained 6ts model 3 231 states perplexity figures closely follow tendency wer values k greater 6 tended degrade results due lack generalisation usually exhibited 5 minimum number insertions substitutions deletions 6 2 power crossentropy sum maximum loglikelihood score input distorted test sentence divided overall number parsed words 23 kts models k increased beyond certain value 6 case explicitly assessed column labelled n table 1 shows number original undistorted test sentences would accepted corresponding kts models without ecp 7 table 1 parsing results terms word error rate wer testset perplexity pp fsm without bs lp experiments distortion 1 distortion 5 distortion 10 jqj value 26 2 0 043 436 219 555 480 728 71 538 10 475 373 348 487 415 632 529 table 2 wers pps beam width using fsm 3 231 states lp experiments distortion wer pp wer pp wer pp 5 346 423 170 387 170 387 10 608 571 359 517 332 515 table 2 shows effect using bs ecp process 6ts model 3 231 states using ev1 negligible differences observed algorithms due distinct ways bs implemented 5 four increasing values beam width ff tested 5 10 20 40 1 means bs setting ff 20 provided results identical achieved full search see also table 1 decreasing ff gracefully degrades results fig 6 shows relative efficiency different algorithms increasing fsm 7 notable even almost 50 undistorted test sentences rejected without ecp noticeable distortion reduction still achieved 10 5distorted sentences therefore proves useful dealing imperfect input also improving effectiveness imperfect fsms computation time centiseconds number states fsm figure average computing times centiseconds different algorithms measured without bs 10 distortion lp experiments sizes without bs results 10 distortion reported results similar 5 1 variance observed results negligible specifically standard deviation computing time per symbol parsed never greater 67 average computing time per symbol parsed means probability close 1 real computing times match corresponding expected values dramatically higher computational demand ev1 clear figure use priority queues ev1pq compute contributes alleviating computational cost though resulting jqj log jqj time complexity still exceedingly high two implementations ev3 show much better linearonjqj performance 8 ev2 also shows linear time complexity slope larger ev3 due number iterations required parsing deletion transitions 5 figs 7 8 show impact bs performance different algorithms fig 7 shows effect increasing distortion rate fixed ff 9 20 ie results 8 observed computing times preprocessing stage ev3 ev3v2 see sect 5 negligible ranging less centisecond smallest fsm 36 centiseconds largest fsm 9 differences scores consecutive parsing stages tend level larger errors produced therefore increase distortion produces increase rate visited states 1 dist02 04 06 computation time centiseconds number states fsm051525354555 computation time centiseconds number states fsm 5 dist0515253545550 10000 20000 30000 40000 50000 60000 70000 80000 computation time centiseconds number states fsm 10 dist figure 7 average computing times observed distortion rate lp experiments identical full search 1 distortion number visited states small algorithms perform fairly well 5 10 distortion ev1 highly costdemanding ev1pq also tends higher computational costs ev2 ev3 show best performance also using bs ev3 gets best results larger fsms finally fig 8 shows effect different ff highest distortion rate 10 significant differences exist ff 20 taken account ff smaller 20 suboptimal results produced see tables 1 2 best results systematically achieved ev3 ev2 although ev3 clearly outperforms ev2 number visited states higher certain bound see 5 details experimental study 72 ocr experiments experiments 2 400 images handwritten digits used strings representing images obtained following outer contour using chaincode eight directions 18 resulting string corpus randomly split given ff 04 06 computation time centiseconds number states fsm 04 06 computation time centiseconds number states fsm computation time centiseconds number states fsm computation time centiseconds number states fsm figure 8 average computing times centiseconds observed 10 distortion beam width values 5 10 20 40 lp experiments disjoint training test sets size training set used automatically learn six different stochastic kts fsms per digit using values k 2 7 value k 10 kts models learned 10 classes digits merged whole global model classdependent labelled final states resulted six stochastic fsms increasing size ranging 80 states testing carried using maximum likelihood classification rule mentioned sect 1 table 3 shows overall recognition rates achieved ecp without bs fsm 18 416 states gets best results decreasing value k tended degrade effectiveness informal tests show recognition rates without ecp drop 50 models table 3 recognition rate achieved ocr experiments fsm jqj value recog rate 758 908 951 972 972 975 computation time centiseconds number states fsm figure 9 average computing times centiseconds different algorithms measured without bs ocr experiments fig 9 shows observed average parsing times ecp algorithm without bs observed variance also negligible observed standard deviation computing time per symbol parsed never greater 72 average computing time per symbol parsed performance achieved much better algorithms performance ev2 case worse one observed previous set experiments even ev1pq outperformed ev2 ev2 significantly depends specific conditions data see 5 impact bs studied model supplying best recognition results namely 7ts model 18 416 states see table 3 results shown table 4 case none values ff tested yielded identical recognition rate achieved full search 975 however width 30 appears good tradeoff efficiency recognition rate fastest performance using bs achieved ev2 ev3 differences among different algorithms significant widest beam 30 case ev3 15 times faster ev2 17 times faster ev1pq 10 observed computing times preprocessing stage ev3 ev3v2 negligible ranging less centisecond 7 centiseconds table 4 impact using bs recognition rate computing time centiseconds ocr experiments ff recog rate ev3 ev3v2 ev2 ev1pq ev1 concluding remarks several techniques proposed costefficient implementation finitestate correcting viterbi parsing key process many applications areas syntactic pattern recognition language processing grammatical inference coding theory etc significant improvement parsing speed regard previous approaches achieved ev3 algorithm proposed furthermore dramatic acceleration achieved applying suboptimal beam search strategies proposed algorithms algorithms developed allow integration search strategy although minor differences rate visited states lead small differences performance case ev3 also exhibited better behaviour algorithms 9 acknowledgements authors wish thank anonymous reviewers careful reading valuable comments work partially funded european union spanish cicyt contracts itltros30268 tic970745c0102 r design analysis computer algorithms algorithms finding patterns strings fast viterbi decoding error correction two different approaches costefficient viterbi parsing error correction different approaches efficient errorcorrecting viterbi parsing experimental comparison simplifying language errorcorrecting decoding decoding channels insertions deletions substitutions applications speech recognition two extensions viterbi algorithm miniature language acquisition touchstone cognitive science viterbi algorithm inference ktestable languages strict sense application introduction efficient priorityfirst search maximumlikelihood softdecision decoding linear block codes correcting dependent errors sequences generated finitestate processes evaluating performance connectedword speech recognition systems harpy speech recognition system comparison syntactic statistical techniques offline ocr bcjr trellis linear block codes time warps fast accurate speaker independent speech recognition using structural models learnt ecgi algorithm codes iterative decoding general graphs tr ctr christoph ringlstetter klaus u schulz stoyan mihov orthographic errors web pages toward cleaner web corpora computational linguistics v32 n3 p295340 september 2006 juancarlos amengual alberto sanchis enrique vidal josmiguel bened language simplification errorcorrecting grammatical inference techniques machine learning v44 n12 p143159 julyaugust 2001 francisco casacuberta enrique vidal learning finitestate models machine translation machine learning v66 n1 p6991 january 2007