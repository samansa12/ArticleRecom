extracting hidden context concept drift due hidden changes context complicates learning many domains including financial prediction medical diagnosis communication network performance existing machine learning approaches problem use incremental learning online paradigm batch offline learners tend ineffective domains hidden changes context assume training set homogeneous offline metalearning approach identification hidden context presented new approach uses existing batch learner process contextual clustering identify stable hidden contexts associated context specific locally stable concepts approach broadly applicable extraction context reflected time spatial attributes several algorithms approach presented evaluated successful application approach complex flight simulator control task also presented b introduction real world machine learning problems important properties domain hidden view furthermore hidden properties may change time machine learning tools applied domains must able produce classifiers available data must able detect effect changes hidden properties example finance successful stock buying strategy change dramatically response interest rate changes world events season result concepts learnt one time subsequently become inaccurate concept drift occurs changes context surrounding observations hidden changes context cause problems machine learning approach assumes concept stability many domains hidden contexts expected recur domains include financial prediction dynamic control commercial data mining applications recurring contexts may due cyclic phenomena seasons year may associated irregular phenomena inflation rates market mood machine learning systems applied domains hidden changes context tended incremental online systems concept definition updated new labeled observations processed schlimmer granger 1986 adaptation new domains generally achieved decaying importance older instances widmer kubats online system flora3 widmer kubat 1993 exploits recurring hidden context system traverses sequence input data stores concepts appear stable interval time stable concepts retrieved allowing algorithm adapt quickly observed domain changes context previously encountered stable concepts also identified offline using batch learning algorithms financial institutions manufacturing facilities government departments etc store large amounts historical data data analyzed offline discover regularities patterns data however may affected context changes without records context maintained handle sit uations batch learner must augmented detect hidden changes context missing context often reflected temporal proximity events exam ple may days customers buy chocolates hidden context case might public holiday due following week hidden context also distributed nontemporal dimension making completely transparent online learners example remote sensing task learning classify trees species may affected surrounding forest type forest type available learning system forms hidden context distributed geographic region rather time offline methods finding stable concepts applied domains simplicity article retains convention organizing hidden context time methods presented generalize properties time stable concept associated one intervals time shift one stable concept another represents change context thus interval identified particular context presents opportunity build models hidden context model may desirable explanatory purposes understand domain may incorporated online predictive model model may also used identify new attribute correlates hidden context paper present splice offline metalearning system contextsensitive learning splice designed identify stable concepts supervised learning domains hidden changes context begin reviewing related work machine learning contextsensitive mains followed description splice methodology initial implementation splice splice1 previously shown improve standard induction method simple domains briefly discuss work presenting improved algorithm splice2 splice2 shown superior complex domains splice2 evaluation concludes application complex control task 2 background online learning methods domains hidden changes context adapt new contexts decaying importance older instances stagger schlimmer granger 1996 first reported machine learning system dealt hidden changes context system dealt changes context discarding concepts fell threshold accuracy splice related flora widmer kubat 1996 family online learners adapt hidden changes context updating current concept match window recent instances rapid adaptation changes context assured altering window size response shifts prediction accuracy concept complexity one version flora3 widmer kubat 1993 adapts domains recurring hidden context storing stable concepts reused whenever context change suspected concept reused first updated match examples current window allows flora3 deal discrepancies recalled concept actual situation rather adjunct online learning splice makes strategy storing stable concepts primary focus offline learning approach machine learning explicit window recent instances used flora first presented kubat 1989 used many online systems dealing hidden changes context approach used supervised learning kubat widmer 1995 unsupervised learning kilander jansson 1993 also used adapt batch learners online learning tasks repeatedly learning window recent instances harries horn 1995 1989 use window also made sensitive changes distribution instances salganicoff 1993 replaces first first updating method discarding older examples new item appears similar region attribute space batch machine learning methods assume training items independent unordered also assume available information directly represented attributes provided result batch learners generally treat hidden changes context noise example sammut hurst kedzier michie 1992 report learning pilot aircraft flight simulator note successful flight could achieved without explicitly dividing flight stages case known changes context learning could broken several subtasks within stage flight control strategy concept learnt stable splice applies assumption concepts likely stable period time problem detecting stable concepts extracting hidden context 3 splice splices input sequence training examples consisting feature vector known classification data ordered time may contain hidden 4changes context data splice attempts learn set stable concepts associated different hidden context since contexts recur several disjoint intervals data set may associated concept online learners domains hidden context assume concept stable interval time splice also uses assumption batch learning hence sequences examples data set combined intervals appear belong context splice attempts cluster similar intervals applying notion similarity context reflected degree intervals well classified concept called contextual clustering informally stable concept expression holds true period time difficulty finding stable concept determining long period clearly many concepts may true short periods splice uses heuristic divide data stream minimal number partitions contextual clusters may contain disjoint intervals dataset stable concept created one contextual cluster poorly classify examples contextual clusters sense stable concept specific concept describing contextual cluster rigorous method comparing different sets contextual clusters might use minimum description length mdl measure rissanen 1983 mdl principle states best theory given concept minimize amount information needs sent sender receiver receiver correctly classify items shared dataset case information sent would include stable concepts context switching method list exceptions good set contextual clusters result stable concepts give shorter description length describing data would single concept optimal set contextual clusters achieve minimum description length possible brute force approach finding set clusters satisfy mdl measure would consider possible combinations contextual clusters dataset select combination minimum description length clearly impractical manganaris 1996 applies minimum description length heuristic creation piecewise polynomial function series numbers method adapted pednault 1989 dynamic programming space possible partitions searched 2 time adapting method splice would give time complexity 4 therefore splice uses heuristic approach find stable concepts good enough splice algorithm metalearning algorithm concepts induced directly application existing batch leaner study use quinlans c45 quinlan 1993 splice methodology could implemented using propositional learning systems c45 used without modification furthermore since noise dealt c45 splice contains explicit noise handling mechanism unusual levels noise dealt altering c45 parameters main purpose paper present splice2 algorithm however first briefly describe predecessor splice1 harries horn press shortcomings motivate development splice2 algorithm 31 splice1 first uses heuristic identify likely context boundaries data partitioned boundaries partitions combined according similarity context stable concepts induced resulting contextual clusters details splice1 algorithm previously reported harries horn press give brief overview section begin example timestamped give position sequence training data thus time forms continuous attribute changes context expressed example hidden context interest rate might change time99 c45 used induce decision tree whole training set node tree contains test attribute test special attribute time interpreted indicating possible change context example table 1 shows simple decision tree might used stock market investment tree includes test time suggests change context may occurred time1995 splice1 uses 1995 boundary partition data set assume interval defined partitions identified stable concept table 1 sample decision tree domain hidden changes context attribute attribute attribute attribute time 1995 stable concept induced one interval accurately classifies examples another interval assume intervals similar contexts degree accuracy provides continuous measure degree similarity intervals grouped contextual similarity adjacent intervals combined larger context identified disjunctive subsets combined recurring context identified c45 applied resulting contextual clusters produce final stable concepts 32 splice1 prediction harries horn press shown splice1 build accurate classifiers standard induction algorithm sample domains hidden splice machine learner dynamic concept switching training stable concepts figure 1 splice online prediction changes context summarize results provide comparison online method flora3 task splice1 used produce set stable concepts applied online prediction task figure 1 shows process schematically online classification achieved switching stable concepts according current context 321 stagger data set data sets following experiments based used evaluate stagger schlimmer granger 1986 subsequently used evaluate flora widmer kubat 1996 approach substantially different use data set allows comparison results program used generate data sets allows us control recurrence contexts factors noise 1 duration contexts task four attributes time size color shape time treated continuous attribute size three possible values small medium large color three possible values red green blue shape also three possible values circular triangular square program randomly generates sequence examples attribute space example given unique time stamp boolean classification based upon one three target concepts target concepts 1 2 3 artificial contexts created fixing target concepts one stagger concepts preset intervals data series 322 online prediction experiment compares accuracy splice1 c45 trained data set containing changes hidden context order demonstrate splice approach valid online classification tasks show sample prediction problem splice1 used offline generate set stable concepts training data training data used generate single concept using c45 offline training resulting concepts applied simulated online prediction task c45 provides baseline performance task trained without time attribute c45 benefits omission attribute values time training set repeat test set even comparison altogether fair c45 designed use domains hidden changes context training set consisted concept 1 50 instances 2 50 instances 3 50 instances test set consisted concepts 1 50 instances 2 50 instances 3 50 instances repeated 1 50 instances 2 50 instances 3 50 instances apply stable concepts identified splice prediction necessary devise method selecting relevant stable concepts trivial problem hence purposes experiment chose simple voting method new example classification accuracy stable concept last five examples calculated accurate concept used classify new example ties accuracy resolved random selection first case classified randomly selected stable concept figure 2 shows number correct classifications item test set splice1 c45 100 randomly generated training test sets figure 2 shows splice1 successfully identified stable concepts training set correct concept successfully selected prediction better 95 cases extreme dips accuracy contexts change effect method used select stable concepts c45 performs relatively well concept 2 accuracy approximately 70 concepts 1 3 correctly classifies 50 60 cases noise increases performance splice1 gradually declines 30 noise harries horn press worst result achieved splice1 85 classification accuracy concept 2 c45 hand still classifies approximately accuracy achieved figure 2 task similar online learning task attempted using flora widmer kubat 1996 stagger schlimmer granger 1986 combination splice1 simple strategy selection current stable concept effective simple context sensitive prediction task selection mechanism assumes least one stable concepts correct splice1 almost immediately moves maximum accuracy new stable concept similar problem flora family widmer kubat 1996 particular flora3 learner designed exploit recurring context appear reach much level accuracy splice1 although online learning method flora requires time fully reflect changes context correct item number splice figure 2 online prediction task compares splice1 stable concepts single c45 concept comparison problematic number reasons splice1 advantage first seeing training set containing 50 instances context beginning classify furthermore assumption possible contexts seen training set correct task online learners advantage continuous feedback unconstrained updating concepts splice1 feedback constrained select stable concepts learnt training data splice1 learnt stable concept second chance complex domains could beneficial use combination splice1 adaptive online learner 4 splice2 uses assumption splits time resulting run c45 accurately reflect changes context always true splice2 devised reduce reliance initial partitioning like splice1 splice2 metalearner experiments use c45 quinlan 1993 induction tool splice2 algorithm detailed figure 2 two stages algorithm discussed turn 41 stage 1 partition dataset begins guessing initial partitioning data subsequent stages refine initial guess three methods may used initial guess ffl random partitioning randomly divide data set fixed number partitions ffl partitioning c45 used splice1 test time found c45 run entire data set used initial partition table 2 splice2 algorithm ordered data set window size parameter partition dataset partition dataset time using either preset number random splits c45 per splice1 prior domain knowledge identified partitions form initial contextual clusters c45 applied initial contextual clusters produce initial interim concepts ffl stage 2 contextual clustering combination interim concept item original data set allocated score based upon total accuracy concept items fixed size window time surrounding item cluster original data set items share maximum scores concept clusters form new set contextual clusters c45 used create new set interim concepts new contextual clusters stage 2 repeated interim concepts change fixed number iterations completed last iteration provides set stable concepts dataset order initial contextual clusters initial concepts figure 3 splice2 stage 1 ffl prior domain knowledge domains prior knowledge available likely stable concepts denote version splice2 using random partitioning splice2r version using c45 partitioning splice2c version using prior domain knowledge splice2p dataset partitioned interval dataset stored initial contextual cluster c45 applied cluster produce decision tree known interim concept see figure 3 42 stage 2 contextual clustering stage 2 iteratively refines contextual clusters generated stage 1 iteration new set contextual clusters created attempt better identify stable concepts data set stage proceeds testing interim concept classification accuracy items original data set score computed pair concept item number score based upon number correct classifications achieved window surrounding item see figure 4 window designed capture notion context likely stable period time online learning systems apply notion implicitly using window recent instances many systems use fixed window size generally chosen size minimum context duration expected window splice function online method namely identify single context ideally window size dynamic flora allowing window adjust figure 4 splice2 stage 2 using interim concept accuracy window capture context different concepts computational efficiency simplicity use fixed sized window context item represented concept correctly classifies items within window surrounding item present window size set 20 default window size chosen bias contexts identified 20 instances duration considered shortest context would valuable online classification window size altered different domains define w ij score concept j applied window centered example correct jm 1 correctly classifies example window size current contextual clusters stage one previous iteration stage two discarded new contextual clusters created interim concept j scores computed item allocated contextual cluster associated interim concept j maximizes w ij interim concepts interim concepts discarded c45 applied new contextual cluster learn new set interim concepts figure 5 contextual clustering process iterates either fixed number repetitions completed interim concepts change one iteration next last iteration stage provides set stable concepts final contextual clusters give intervals time different contexts active 421 alternate weight domains strong bias toward single class 90 class 10 class b well represented classes dominate contextual clustering process lead clusters formed combining l clusters concepts figure 5 splice2 stage 2 create interim concepts contexts similar classifications well represented classes quite dissimilar classification poorly represented classes problematic domains correct classification rare classes important learning fly see section 6 domains altered give equal importance accuracy classes window ignoring relative representations different classes given item interim concept j new sums possible classifications proportion items given class correctly classified 2 c number classes c class number example correctly classifies example window size 43 splice2 walk section present walk splice2r algorithm using simple dataset recurring context dataset consists randomly generated stagger instances classified according following pattern context 3 repetitions following structure concept 1 instances concept 2 instances concept 3 applied dataset begins randomly partitioning dataset four periods partition labeled contextual cluster figure 6 shows instances r item figure 6 initial contextual clusters created randomly partitioning dataset associated contextual cluster drawn original dataset c45 applied clusters cc induce interim concepts ic table 3 shows induced concepts ic 4 relates closely target concept table 3 initial interim concepts 3471 color redgreen yes 700273 ic3 color redblue size mediumlarge yes 14078 ic4 size mediumlarge shape squaretriangular color redblue 330165 calculated combinations item interim concept j table 4 shows calculated w ij scores fragment dataset figure table represents number items window surrounding item number classified correctly given interim concept ic j instance interim concept ic 3 classifies 12 items correctly window surrounding item number item allocated new contextual cluster based upon interim concept ic j yielded highest score w ij illustrated table 4 highest score column italicized base table note new contextual cluster item allocated new contextual cluster associated single interim concept example interim concept ic 1 gives highest value item 80 item allocated new contextual cluster item figure 7 contextual clusters0 created first iteration contextual clustering process24 contextual cluster item figure 8 contextual clusters000 created third final iteration contextual clustering contextual cluster cc 1 0 table contains items allocated one contextual cluster implies change context around item 182 table scores initial interim concepts items training set concepts item number allocate new contextual clusters cc 0 expected improve previous clusters better approximating hidden contexts figure 7 shows distribution new contextual clusters cc 0 clusters new set interim concepts ic 0 induced contextual clustering iterates either fixed number repetitions completed interim concepts change one iteration next figure 8 shows contextual clusters two iterations clustering stage remaining contextual cluster corresponds hidden context original training set point final interim concepts ic 000 renamed stable concepts sc stable concept one sc 1 000 first target concept stable concept three third target concept stable concept four sc 4 000 second target concept contextual cluster two cc 2 000 contain items used induce stable concept 5 experimental comparison splice1 splice2 section describes two experiments comparing performance splice2 splice1 first comparison clustering performance across range duration noise levels number hidden changes context fixed second compares performance systems across range noise hidden context changes duration fixed experiments performance determined checking concepts induced system agree original concepts used generating data experiments use stagger data described prior experiment however unlike prior experiment training data required assessing performance original concepts 51 contextual clustering splice1 vs splice2 experiment designed compare clustering performance two systems splice1 shown correctly induce three stagger concepts range noise context duration conditions harries horn press experiment complicates task identifying concepts including context changes order compare clustering stage alone splice2c used represent splice2 performance ensured initial partitioning used versions splice identical versions splice trained independently generated data sets data set consisted examples classified according following pattern con repetitions following structure concept 1 instances concept 2 instances concept 3 instances duration ranged 10 100 noise ranged 0 30 gave total 14 context changes training set splice2 run default window size 20 number iterations contextual clustering set three c45 run default parameters subsetting stable concepts learnt system assessed correctness original concepts results averaged 100 repetitions combination noise duration show proportion correct stable concept identifications found average number incorrect stable concepts identified figure 9 shows number concepts correctly identified splice1 range context durations noise levels accuracy versions converges maximum number concepts higher context durations levels noise versions show graceful degradation accuracy noise increased splice2 recognizes concepts almost levels noise concept duration concepts correctly identified context duration noise 10 noise 20 noise 30 noise05152510 20 concepts correctly identified context duration noise 10 noise 20 noise 30 noise figure 9 concepts correctly identified splice1 splice2 duration changed2610 incorrect concepts context duration 0 noise 10 noise 20 noise 30 noise2610 incorrect concepts context duration 0 noise 10 noise 20 noise 30 noise figure 10 incorrect concepts identified splice1 splice2 figure compares number incorrect concepts identified splice1 splice2 versions show relatively stable level performance level noise levels noise splice2 induces substantially fewer incorrect concepts splice1 versions splice used initial partitioning conclude iterative refinement contextual clusters used splice2 responsible improvement splice1 results suggests splice2 clustering mechanism better able overcome effects frequent context changes high levels noise next experiment investigate hypothesis fixing context duration testing different levels context change 52 effect context repetition previous experiment demonstrated splice2 performs better splice 1 fixed number context changes however experiment provided little insight effect different levels context repetition experiment investigates effect varying number hidden context changes data use splice2c ensure partitioning used splice1 also examine results achieved splice2r experiment system trained independent set data consisted following pattern contexts r repetitions structure con cept 1 50 instances concept 2 50 instances concept 3 50 instances r varies one five effects noise also evaluated range noise 0 40 splice2 run default window size 20 three iterations contextual clustering used partitions parameters c45 performance measure used used prior experiment figure 11 shows number concepts correctly induced splice1 splice2c combination context repetition noise results indicate systems achieve similar levels performance one context repetition splice2c performs far better splice1 almost levels repetition greater one comparing shapes performance graphs system interesting shows increasing level performance across almost levels noise increase repetition context change hand splice 1 show initial rise subsequent decline performance number repetitions increases exception 0 noise versions identify three concepts repetition levels three figure 12 shows number correct stagger concepts identified splice 2r results shows rise recognition accuracy repetitions increase maximum 3 concepts recognized noise levels number concepts recognized similar figure 11 splice2c concepts correctly identified context repetition 0 noise 10 noise 20 noise 30 noise concepts correctly identified context repetition 0 noise 10 noise 20 noise 30 noise 40 noise figure 11 concepts correctly identified splice1 splice2c across range context concepts correctly identified context repetition 0 noise 10 noise 20 noise 30 noise 40 noise figure 12 splice2r concept identification similarity results splice2c splice2r shows main c45 partitioning provides benefit use random partitioning results experiment indicate splice2c improvement improves concept recognition response increasing levels context repetition performance splice1 degrades increased levels context changes inability splice1 cope high levels context change probably due failure partitioning method number partitions required time increases task inducing correct global concept becomes difficult information gain available given partition time reduced likelihood erroneously selecting another possibly noisy attribute upon partition data set increased result context changes time liable missed splice2c affected poor initial partitioning rebuilds context boundaries iteration contextual clustering hence poor initial partition minimal effect system take advantage increases context examples splice1 still interesting substantially less work splice2 effective domains relatively context changes anticipate stronger partitioning method would make splice1 resilient frequent changes context results also indicate c45 partitioning method helpful domain 6 applying splice learning fly domain test splice2 methodology wished apply substantially complex domain artificial data described available data collected flight simulation experiments used behavioral cloning sammut et al 1992 previous work domain found necessary explicitly divide domain series individual learning tasks stages splice2 able induce effective pilot substantial proportion original flight plan explicitly provided stages following sections briefly describe problem domain application splice2 61 domain learning fly experiments sammut et al 1992 intended demonstrate possible build controllers complex dynamic systems recording actions skilled operator response current state system flight simulator chosen dynamic system complex system requiring high degree skill operate successfully yet well un derstood experimental setup collect data several human subjects flying predetermined flight plan data would input induction program c45 flight plan provided human subjects 1 take fly altitude 2000 feet 2 level fly distance 32000 feet starting point 3 turn right compass heading approximately 330 degrees 4 northsouth distance 42000 feet turn left head back towards runway turn considered complete azimuth 140 degrees 180 degrees 5 line runway 6 descend runway keeping line 7 land runway log includes 15 attributes showing position motion 4 control tributes position motion attributes record state plane whereas control attributes record actions pilot position motion attributes ground g limit wing stall twist elevation azimuth roll speed elevation speed azimuth speed airspeed climbspeed ew distance altitude ns distance fuel control attributes rollers elevator thrust flaps rudder used implementation unrealistic values control attributes provide target classes induction separate decision trees control attribute decision trees tested compiling trees autopilot code simulator flying simulator original experiments three subjects flew flight plan times data set 90000 records produced originally thought combined data could submitted learning program however proved complex task learning systems available problems largely due mixing data different contexts first critical type context pilot different pilots different flying styles responses situation may differ hence flights separated according pilot furthermore actions particular pilot differ according stage flight pilot adopts different strategies depending whether turning aircraft climbing landing etc succeed inducing competent control strategy learning algorithm would able distinguish different cases since methods available could manual separation data flight stages required since pilots given intermediate flight goals division stages onerous divisions immediately obvious exam ple initially lining descending separated two different stages however without separation decision trees generated c45 would miss runway lineup stage introduced successful behavioral clone could produced stages used behavioral cloning could found human intervention often included quite lot trialanderror experimen tation work described suggests flight stages treated different contexts splice2 approach automate separation flight data appropriate contexts learning 62 flying splice2 domain introduces additional difficulty splice previous behavioral cloning experiments built decisions trees four actions seven stages resulting 28 decision trees flying simulator decision trees switched depending current stage however splice2 applied four learning tasks viz building controller elevators another rollers thrust flaps guarantee exactly context divisions found causes problems two actions must coordinated example turn aircraft rollers elevators must used together contexts two actions coincide new roller action say may commenced corresponding elevator action may start time thus causing lack coordination failure execute correct manoeuvre problem avoided combining rollers elevators single attribute corresponding stick position since rollers take one 15 discrete values elevators take one 11 discrete values combined attribute 165 possible values represented training set problem know switch contexts original behavioral clones included handcrafted code accomplish however splice builds contexts automatic means switching necessary online prediction experiment reported section 322 context selected using voting mechanism mechanism relied upon immediate feedback classification accuracy feedback flight chose learn switch examples stable concept labelled identifier concept input c45 time predict context state flight belongs thus identifying appropriate stable concept controller associated context designing selection mechanism remained situationaction paradigm previous cloning experiments adopted comparisons meaningful found original uses classification accuracy perform well class frequencies wildly different due well represented classes dominating contextual clustering process leading clusters similar classification well represented classes dissimilar classification poorly represented classes problematic successful flights depend upon correct classification rare classes problem reduced adopting alternative scoring method defined equation 2 addition adjusted c45 parameters pruning level ensure rare classes treated noise also augmented recognize domain discontinuities end one flight beginning next altering w ij 0 predictions flight flight example incorporated w ij 0 45000 40000 35000 30000 25000 20000 15000 10000 5000 020004000600050015002500northsouth feet eastwest feet height feet figure 13 flight comparison able successfully fly first four stages flight training data extracted using data first four stages noted even changes domain combining rollers elevator c45 unable make first turn without explicit division domain stages figure 13 shows three flights ffl successful splice2 flight stages 1 4 ffl best c45 flight ffl sample complete flight settings used splice2p post pruning turned c 100 ffl three iterations clustering stage ffl window size 50 instances ffl initial partitioning set four equal divisions first flight 40000 35000 30000 25000 20000 15000 10000 5000 10001000300050000100020003000 northsouth feet eastwest feet height feet context 3 context 4 figure 14 distribution local concepts used successful flight chart shows one sixty time steps necessary clarity remove brief context changes initially investigated use larger window random partitioning successfully created two contexts one primarily concerning first five stages flight concerning last two number contexts successful pilot could induced reducing window size lead contexts less well defined also could fly plane solution bias clustering providing initial partitioning based first four stages flight research needed determine correspondence number initial partitions number flight stages accidental something fundamental involved distinguished four contextual clusters rough correlation flight plan stages contextual cluster contains items multiple stages training flights context 1 instances four stages better representation instances first half stage 1 context 2 roughly corresponds second half stage 1 stage 2 part 3 instances context 3 stage 2 onward primarily stage 4 context 4 particularly interesting also corresponds primarily stage 4 contains less items context 3 parts stage surprising correspondence context stage complete original division flight plan stages convenience description flight could well divided many different ways short splice contexts capturing something additional division flight stages figure 14 shows stable concept used instant splice2 flight make context changes clearly visible number points plotted figure 14 fewer actually recorded lower resolution details context changes visible chart take handled context 1 climb 2000 feet level handled context 2 context 3 also occasional instances level region flight context 1 took straight level flight right hand turn handled mix contexts 3 4 subsequently flying northwest handled contexts 2 3 context 4 initiating left hand turn done context 1 rest left hand turn handled combination contexts 3 3 sammut et al 1992 divided flight stages trial error partitions found successfully used build behavioral clones reason believe partition unique fact expect similar kinds behaviors used different stages example behavior straight level flight needed several points flight turning left turning right moreover control settings might used different maneouvres depending current state flight figure 14 shows splice intermixed behaviors much done manual division stages although needs investigation reasonable conjecture approach like splice find finely tuned control strategies achieved manually subsequent experiment attempted hand craft perfect switching strategy associating stable concepts stage flight plan switching method successfully fly plane present addition stages flight causes catastrophic interference first two stages last 3 stages splice2 yet unable completely distinguish parts flight however use splice2 synthesizing controllers stages 1 4 first time automated procedure successful identifying contexts complex domain use decision tree select current context reasonably effective decision tree uses attributes stable concepts way refer past effect flying short term memory issue work comparison original learning fly project sammut et al 1992 used situationaction control experiment serves demonstrate offline contextsensitive learning applied quite complex data sets promising results 7 related work substantial amount work dealing known changes context using batch learning methods much work directly relevant challenges faced using stable concepts online prediction known context dealt dividing domain context inducing different classifiers context classification time metaclassifier used switch classifiers according current context sammut et al 1992 katz gately collins 1990 application stable concepts online classification used paper sections 322 62 use similar switching approach unfortunately always possible guarantee hidden contexts known new contexts might dealt adapting existing stable concept kubat 1996 demonstrates knowledge embedded decision tree transfered new context augmenting decision tree second tier trained new context second tier provides soft matching weights leaf original decision tree use two tiered structure originally proposed michalski 1990 dealing flexible concepts pratt 1993 shows knowledge existing neural network reused significantly increase speed learning new context methods transfer knowledge known contexts could used online adapt stable concepts manner analogous used flora3 may possible improve accuracy stable concepts combining data range contexts turney 1993 turney halasz 1993 applies contextual normalization contextual expansion contextual weighting range domains demonstrates methods improve classification accuracy instance based learning aha kibler albert 1991 multivariate gression could particularly valuable version splice using instance based methods instead c45 somewhat different online method designed detect exploit contextual attributes metalb widmer 1996 case contextual attributes predict relevance attributes metalb uses contextual attributes detected trigger changes set features presented classifier approach definition context quite different used splice overall philosophy similar widmer concludes stating identification contextual features first step towards naming thus able reason contexts one main goals splice result reasoning would model hidden context models hidden context could used online classification systems augment existing reactive concept switching proactive component models hidden context might also applied improving domain understanding first steps toward building models hidden context taken article learning fly experiment section 62 used model hidden context based types instances expected context switch stable concepts summarize splice begins build bridge online methods dealing hidden changes context batch methods dealing known change context 8 conclusion article presented new offline paradigm recognizing dealing hidden changes context hidden changes context occur domain prediction task poorly understood context difficult isolate attribute previous work hidden changes context used online learning approach new approach splice uses offline batch metalearning extract hidden context induce associated stable concepts incorporates existing machine learning systems paper c45 quinlan 1993 initial implementation briefly reviewed new version splice2 presented full evaluation splice approach included online prediction task series hidden context recognition tasks complex control task initial implementation splice approach used c45 divide data series likely changes context process called contextual clustering grouped intervals appearing context process used semantics concepts induced interval measure similarity context resulting contextual clusters used create context specific concepts specify context boundaries limitations splice1 permitting refinement partition boundaries splice2 clusters basis individual members data series hence context boundaries restricted boundaries found partitioning stage context boundaries refined splice2 much robust quality initial partitioning successfully detected dealt hidden context complex control task learning fly behavioral cloning domain based upon learning autopilot given series sample flights fixed flight plan previous work domain required user specify stages flight splice2 able successfully fly substantial fragment initial flight plan without stages contexts specified first time automated procedure successful identifying context complex domain number improvements could made splice algorithms partitioning method used shown problematic splice1 high levels noise hidden changes context use existing machine learning system provide partitioning elegant better solution may implement specialized method designed deal additional complexity time one approach augment decision tree algorithm allow many splits fayyad irani 1993 selected attributes neither splice1 splice2 provide direct comparison relative advantage dividing domain one set contexts another one comparison method could used minimum description length mdl heuristic rissanen 1983 mdl principle best theory given concept minimize amount information need sent sender receiver receiver correctly classify items shared dataset case information sent must contain stable concepts context switching method list exceptions least would allow direct comparison given contextsensitive global concept using stable concepts context switching contextinsensitive global concept contextual clustering method could use mdl heuristic guide search possible context divisions approaches used selecting current context online voting method domains immediate feedback decision tree domain without immediate feedback sophisticated approaches would use model hidden context model could use knowledge expected context duration order stability could also incorporate existing attributes domain feedback decision tree used context switching learning fly task primitive implementation model using existing attributes select context exciting possibility use characteristics contexts identified splice guide search external world attribute similar characteristics attributes could incorporated current attribute set allowing bootstrapping domain representation could used within knowledge discovery databases kdd approach fayyad piatskyshapiro smyth 1996 includes notion analysts reiterate data selection learning data mining tasks perhaps method could provide way automated agent select potentially useful attributes outside world extend existing domain knowledge acknowledgments would like thank editors anonymous reviewers suggestions led much improved paper michael harries partially supported australian postgraduate award industrial sponsored rmb australia notes 1 following experiments n noise implies class randomly selected probability n method generating noise chosen consistent widmer kubat 1996 r incremental batch learning detecting concept drift financial time series prediction using symbolic machine learning neural computation floating approximation timevarying knowledge bases second tier decision trees adapting drift continuous domains classifying sensor data calchas learning flexible concepts fundimental ideas method based twotiered representation experiments applying inductive inference principles surface reconstruction universal prior integers estimation minimum description length annals statistics density adaptive learning forgetting learning fly robust classification contextsensitive features recognition exploitation contextual clues via incremental meta learning effective learning dynamic environments explicit concept tracking learning presence concept drift hidden contexts tr ctr francisco ferrertroyano jesus aguilarruiz jose c riquelme incremental rule learning based example nearness numerical data streams proceedings 2005 acm symposium applied computing march 1317 2005 santa fe new mexico francisco ferrertroyano jesus aguilarruiz jose c riquelme data streams classification incremental rule learning parameterized generalization proceedings 2006 acm symposium applied computing april 2327 2006 dijon france anand narasimhamurthy ludmila kuncheva framework generating data simulate changing environments proceedings 25th conference proceedings 25th iasted international multiconference artificial intelligence applications p384389 february 1214 2007 innsbruck austria chunsheng yang sylvain ltourneau learning predict train wheel failures proceeding eleventh acm sigkdd international conference knowledge discovery data mining august 2124 2005 chicago illinois usa dwi h widyantoro john yen relevant data expansion learning concept drift sparsely labeled data ieee transactions knowledge data engineering v17 n3 p401412 march 2005 antonin rozsypal miroslav kubat association mining timevarying domains intelligent data analysis v9 n3 p273288 may 2005 bruce edmonds learning exploiting context agents proceedings first international joint conference autonomous agents multiagent systems part 3 july 1519 2002 bologna italy