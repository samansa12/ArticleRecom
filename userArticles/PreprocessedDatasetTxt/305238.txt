adaptively preconditioned gmres algorithms restarted gmres algorithm proposed saad schultz siam j sci statist comput 7 1986 pp 856869 one popular iterative methods solution large linear systems equations axb nonsymmetric sparse matrix algorithm particularly attractive good preconditioner available present paper describes two new methods determining preconditioners spectral information gathered arnoldi process iterations restarted gmres algorithm methods seek determine invariant subspace matrix associated eigenvalues close origin move eigenvalues higher rate convergence iterative methods achieved b introduction many problems applied mathematics engineering give rise large linear systems equations sparse nonsymmetric nonsingular matrix often desirable sometimes necessary solve systems iterative method let x 0 initial approximate solution 11 let r associated residual vector introduce krylov subspaces associated matrix vector r 0 many popular iterative methods determine mth iterate xm xm gamma x refer methods krylov subspace iterative methods see eg freund et al 12 recent review let iterate xm generated krylov subspace iterative method residual error r associated xm satisfies residual polynomial pm determined iterative method satisfies denote euclidean norm r n well associated induced matrix norm r nthetan restarted generalized minimal residual algorithm saad schultz 22 described section 3 one popular krylov subspace iterative methods solution linear systems nonsymmetric matrix residual polynomial determined algorithm satisfies department mathematics computer science kent state university kent oh 44242 email jbaglamamcskentedu research supported part nsf grant f377 dmr8920147 alcom department mathematical sciences stevens institute technology hoboken nj 07030 email nacalvettinanetornlgov research supported part nsf grant dms9404692 z computer science department stanford university stanford ca 94305 x department mathematics computer science kent state university kent oh 44242 email reichelmcskentedu research supported part nsf grant dms9404706 j baglama et al denotes set polynomials p degree analysis implementation restarted gmresm algorithm modifications thereof continue receive considerable attention see eg 4 5 7 algorithms particularly attractive suitable preconditioner nthetan matrix available see eg 2 15 21 recent discussions preconditioners matrix gamma1 good preconditioner application iterative method interest preconditioned linear system equations gives higher rate convergence computed iterates application iterative method original linear system 11 moreover would like preconditioner gamma1 property w 2 r n vector gamma1 w rapidly evaluated matrix gamma1 15 sometimes referred left preconditioner present paper describes two new adaptive methods determining preconditioners iterations restarted gmresm algorithm standard implementation restarted gmresm algorithm 22 based arnoldi process 1 described section 2 allows spectral information gathered iterations use information determine approximation invariant subspace associated eigenvalues close origin preconditioner essentially removes influence eigenvalues rate convergence focus effect preconditioner spectrum however known rate convergence iterates computed gmresm algorithm also determined pseudospectra see nachtigal et al 19 ease presentation ignore effect preconditioner pseudospectra present paper preconditioners particularly effective cluster eigenvalues large influence rate convergence illustrations found section 4 determination well application preconditioners require evaluation matrixvector products matrix addition needed arnoldi process evaluation certain residual errors implementations use recurrence formulas implicitly restarted arnoldi ira method described sorensen 23 recently lehoucq 17 preconditioners combined preconditioners also applicable known efficient preconditioner available different method adaptively determine preconditioner iterations restarted gmresm algorithm recently described erhel et al 11 utilizing recurrence formulas ira method preconditioning scheme allows flexibility choice preconditioner requires less computer memory method described 11 another adaptive preconditioning method presented kharchenko yeremin 16 method differs schemes approximate invariant subspaces determined morgan 18 also uses approximate invariant subspaces improve rate convergence restarted gmresm algorithm instead constructing preconditioner appends approximate invariant subspace krylov subspaces generated arnoldi process feel new algorithms attractive simplicity ira method algorithms based typically determines adequate approximate invariant subspaces fairly rapidly adaptive preconditioners 3 let matrix spectral factorization 13 14 yield bound denotes spectrum note bound 17 would decrease able replace subset preconditioners roughly effect definiteness assume eigenvalues ordered according let scaled good approximation scaling determined iterations discussed arnoldi process determines decomposition form vm 2 r nthetam mthetam upper hessenberg matrix refer 110 arnoldi decomposition throughout paper e j denotes jth axis vector appropriate dimension j denotes identity matrix order j vm e columns vm span krylov subspace km r 0 defined 12 future reference define let matrix v k 2 r nthetak consist first k columns vm let columns matrix wngammak span r n nspanfv k g spanfv k g denotes span columns v k assume w ngammak thus columns matrix v k wngammak form orthogonal basis r n introduce matrix use inverse matrices form 112 k n left precondi tioners form inverse given proposition 11 let q 2 r nthetan orthogonal matrix partition according submatrix v consists k first columns q submatrix w consists remaining columns assume nonsingular matrix nonsingular inverse given 4 j baglama et al proof matrix 113 written theta ngammak therefore theta ngammak shows 114 columns matrix v proposition 11 span invariant subspace eigenvalues matrix gamma1 expressed terms eigenvalues corollary 12 let matrices v w h proposition 11 assume moreover columns matrix v span invariant subspace associated eigenvalues eigenvalue 1 multiplicity least k proof matrix similar theta 12 22 22 ng formula 116 representation 115 yield theta 12 22 thus spectrum gamma1 consists 22 eigenvalue 1 multiplicity latter eigenvalue least k result analogous corollary 12 right preconditioner shown erhel et al 11 remark application preconditioners form 114 simplified fact thus matrix w computed following example compares bounds rate convergence iterates determined gmresm algorithm applied original linear system 11 preconditioned linear system 15 preconditioner 114 assume conditions corollary 12 hold example 11 assume spectral factorization form 16 eigenvalues real positive let eigenvalues ordered according 18 17 yields lim sup min adaptive preconditioners 5 tm z chebyshev polynomial first kind degree interval equality 119 follows wellknown properties chebyshev polynomials see eg 13 section 1015 preconditioner 114 assume conditions corollary 12 hold preconditioner eliminates influence k smallest eigenvalues rate convergence gmresm algorithm specifically gmresm algorithm applied preconditioned linear system 15 yields sequence residual vectors bounded lim sup usual r bound 120 shown first noting lim sup applying bound 118 righthand side 121 2 actual computations determine preconditioner krylov subspace spanfv k g close invariant subspace computations example 11 suggest gmresm algorithm require fewer iterations determine accurate approximate solution 11 applied preconditioned linear system 15 preconditioner applied original unpreconditioned system 11 verified numerical experiments presented section 4 2 construction preconditioners section describe determine approximate invariant subspace associated eigenvalues using recursion formulas ira method sorensen 23 first present arnoldi process 1 algorithm 21 arnoldi process input k upper hessenberg matrix h output upper hessenberg matrix nthetam h j v endfor jwe may assume vectors f generated algorithm 21 nonvanishing f columns matrix v j generated span invariant subspace v j used construct preconditioner described example 11 input algorithm 21 initial vector f 0 6 j baglama et al provided note f 6 0 define matrix orthogonal columns sequel use matrix matrix hessenbergtype whose leading theta principal submatrix hm whose 1st row numerical difficulties arise computation vectors f j al gorithm computations done modified gramschmidt process one reorthogonalization neglecting enforce orthogonality vector f j vectors give rise spurious eigenvalues matrix hm ie eigenvalues cannot considered approximations eigenvalues given arnoldi decomposition 110 initial vector recursion formulas ira method used compute vector monic polynomial degree evaluating new matrixvector products matrix coefficient jmgammak scaling factor chosen kv mgammak 1 discuss selection zeros z j recursion formulas ira method truncated versions recursion formulas qr algorithm explicit shifts zeros z j chosen shifts see eg 13 chapter 7 description qr algorithm therefore sometimes refer zeros z j shifts thus let decomposition 110 given determine qr factorization r 1 upper triangular putting also hessenberg matrix multiplication equation 242 e 1 yields ae 1 equation 25 displays relationship initial arnoldi vector v 1 vector v 1 1 applying shifts obtain decomposition 2 adaptive preconditioners 7 denotes orthogonal matrix associated shift z j introduce partitioning hmgammak equate first k columns righthand side lefthand side 26 gives v mgammak mgammak follows 28 v mgammak thus 27 arnoldi decomposition construction vector v mgammak written 23 description ira method based recursion formulas qr algorithm explicit shifts implementation based qr algorithm implicit shifts reason numerical stability see 13 chapter 7 description qr algorithm use implicit shifts allows application complex conjugate shifts without using complex arithmetic first apply arnoldi process compute arnoldi decomposition 110 use recursion formulas ira method determine arnoldi decomposition 27 purpose computations determine accurate approximation invariant subspace associated eigenvalues would like choose zeros z z mgammak mgammak first column v mgammak k defined 23 close invariant subspace associated eigenvalues let f denote eigenvalues upper hessenberg matrix hm 110 order avm orthogonal projection consider j approximations eigenvalues order force vector v mgammak 1 invariant subspace associated k eigenvalues smallest magnitude choose zeros ie z j chosen available approximations eigenvalues largest magnitude selection zeros discussed sorensen 23 calvetti 8 j baglama et al et al 6 lehoucq 17 zeros referred exact shifts numerical experience indicates ordering shifts according 210 adequate sense computed matrices h mgammak close k eigenvalues hm smallest magnitude let f k j1 eigenvalueeigenvector pairs h mgammak k introduce vectors approximate eigenvalueeigenvector pairs residual errors accept spanfv k g approximate invariant subspace kf mgammak ffl subspace 0 parameter purpose matrix h mgammak k 211 make bound invariant scaling inequalities 211 satisfied apply algorithm 21 arnoldi decomposition 27 input order determine new arnoldi decomposition 110 theta upper hessenberg matrix hm apply recursion formulas ira method zeros chosen hm largest magnitude gives arnoldi decomposition form 27 check whether inequalities 211 satisfied computations repeated fashion 211 holds obtain way arnoldi decomposition form 27 matrices k h k general spanfv k g accurate approximation invariant subspace associated k eigenvalues smallest magnitude h k accurate approximation set f j g k accuracy approximations depends parameter ffl subspace 211 distribution eigenvalues departure normality matrices v k h k obtained used define first preconditioner used 117 describe section 3 combine ira process restarted gmres algorithm richardson iteration improve available approximate solution 11 determining preconditioner gamma1 1 computed preconditioner 1 apply method outlined preconditioned system 15 order determine approximation invariant subspace associated eigenvalues smallest magnitude matrix simultaneously improve available approximate solution 11 yields new preconditioner gamma1 2 system gamma1 equivalently new preconditioner 1 system 11 computations continued manner determined preconditioner form specified integer ff 0 1 form 213 preconditioner makes natural scale 19 holds approximation scaling achieved scaling linear system 11 factor 1j eigenvalue adaptive preconditioners 9 largest magnitude one matrices hm generated algorithm 21 computation preconditioner gamma1 1 remark certain matrices techniques achieving scaling may available instance one may able use gershgorin disks inequality matrix norm induced vector norm see 24 chapter 6 details latter topics 3 iterative methods section describes two algorithms adaptive preconditioning detail one algorithm 35 combines ira process richardson iteration gmres algorithm scheme algorithm 36 apply richardson iteration first recall restarted gmresm algorithm saad schultz 22 solution linear systems equations 11 algorithm 31 restarted gmresm algorithm input initial approximate solution x output approximate solution xm associated residual vector r krm kkr solution compute algorithm 21 input matrices vm1 hm defined 21 22 respectively also available compute solution ym hm yk endwhilewe describe improve available approximate solution richardson iteration applying recursion formulas ira method arnoldi decomposition iterations carried without evaluating matrixvector products matrix let x 0 available approximate solution 11 richardson iteration written relaxation parameters would like parameters ffi j approximate solutions x j converge rapidly solution 11 j increases future reference note residual vectors 32 written theorem 32 let x 0 approximate solution 11 let r consider arnoldi decomposition initial vector k apply recursion formulas ira method zeros z residual vectors 32 associated iterates 31 computed richardson iteration relaxation parameters j baglama et al given q denotes orthogonal matrix r upper triangular matrix associated zero z ira recursion formulas moreover v proof first show 34 1 substitution v representation 33 shows turn case 2 33 35 obtain replace vmq 1 equations 241244 multiply equation 242 obtained e 1 shows analogously 25 substitution 37 36 shows 34 2 continuing manner yields case treated separately arnoldi decomposition similarly 3 obtain v 1 choosing ae completes proof prior development gmresm algorithm saad 20 introduced full orthogonalization algorithm galerkin method solution 11 let x 0 approximate solution 11 let r 0 associated residual vector consider arnoldi decomposition 110 let v theorem 32 fom algorithm determines improved approximate solution xm 11 solving linear system letting following result shows approximate solution determined richardson iteration theorem 33 let vectors x 0 r 0 arnoldi decomposition 110 theorem 32 assume arnoldi decomposition exists adaptive preconditioners 11 kfm k 6 0 matrix hm arnoldi decomposition nonsingular let 34 let relaxation parameters richardson iteration reciprocal values eigenvalues hm exact arithmetic approximate solution xm determined richardson iteration 3132 equals approximate solution computed fom algorithm proof substitute use fact linear system 38 written hm introduce polynomials f g bilinear form construction g j polynomial degree j particular equations 310 311 yield shows pm residual polynomial degree fom algorithm therefore satisfies pm combining formulas 110 311 yields identity shows eigenvalues f hm zeros gm particular therefore pm written pm follows comparison 312 13 33 shows steps richardson iteration relaxation parameters j application fom algorithm correspond residual polynomial therefore equivalent implementation iterative method based following observation corollary 34 let x j gamma1 approximate solution 11 let r associated residual vector let arnoldi decomposition initial vector j1 eigenvalues h let x approximate solution j baglama et al obtained one step richardson iteration relaxation parameter q let application recursion formulas ira method 313 shift q yield arnoldi decomposition av 1 triangular matrix qr factorization h moreover h 1 proof corollary follows theorem 32 fact use exact shift eigenvalues reduced matrix h 1 eigenvalues original matrix h except shift latter result shown sorensen 23 lemma 310 corollary shows apply shifts one time determine required residual vectors first column matrices v available arnoldi decompositions analogous result established complex conjugate shifts latter case recursion formulas ira method implemented using qr algorithm implicit double shifts obviates need use complex arithmetic double step richardson iteration complex conjugate relaxation parameters also carried without using complex arithmetic notational simplicity algorithm iterative method use double shifts double steps however implementation algorithm used computed examples section 4 algorithm 35 adaptively preconditioned gmresm algorithm richardson iteration input tolerance computed approximate solution ffl solution tolerance approximate invariant subspace ffl subspace dimension largest krylov subspace determined dimension k approximate invariant subspace computed maximum number ff 0 preconditioners gamma1 j computed maximum number fi 0 arnoldi decompositions order determined preconditioner output computed approximate solution x associated residual vector r j preconditioner compute algorithm 21 initial vector compute eigenvalues f matrix hm arnoldi decomposition order according 29 scale matrix righthand side linear system factor 1j j equation 19 approximately satisfied apply shift m1gamma arnoldi decomposition compute residual vector gamma1 r j described corollary 34 gives mgamma adaptive preconditioners 13 endfor bound 211 satisfied goto use arnoldi decomposition gamma1 av mgammak k input algorithm 21 apply arnoldi process compute arnoldi decomposition endfor fi 1 improve approximate solution gmresk update preconditioner k well matrices 22 available compute solution k 2 r k min 2 gamma1 3 r jk solution done endfor ff kr j kkr solution apply algorithm 21 initial vector matrices vm1 hm defined 21 22 respectively compute solution ym 4 r jm endwhilein algorithm 35 compute matrixvector products matrix applying arnoldi process evaluating residual vectors r lines labeled 3 4 examine storage requirement algorithm 35 count number nvectors stored storage necessary represent matrix ignored since independent iterative method used preconditioner requires storage n theta k matrix v k limit number preconditioners ff 0 thus preconditioner gamma1 defined 213 requires storage ff 0 k nvectors particular matrix gamma1 actually formed line marked 2 algorithm 35 interpreted symbolically mean storage matrix gamma1 formula evaluating matrixvector products gamma1 updated gmresm algorithm whileloop algorithm 35 requires additional storage vectors x j r j matrix vm1 2 r nthetam1 equivalent storage 3 nvectors vector algorithm 35 scaling factor stored first column matrix vm1 last column vm1 contains vector fm scaling factor righthand side vector b also stored therefore total storage requirement algorithm 35 ff algorithm 36 obtained replacing richardson iteration algorithm 35 gmres algorithm replacement makes residual error decrease smoothly iterations proceed however iterates preconditioners generated algorithms 35 36 found 14 j baglama et al former algorithm seldom gives faster convergence illustrated section 4 therefore feel algorithms interest storage requirement algorithm 36 essentially algorithm 35 notational simplic ity algorithm 36 use double shifts however implementation algorithm used computed examples section 4 algorithm 36 adaptively preconditioned gmresm algorithm input tolerance computed approximate solution ffl solution tolerance approximate invariant subspace ffl subspace dimension largest krylov subspace determined dimension k approximate invariant subspace computed maximum number ff 0 preconditioners gamma1 j computed maximum number fi 0 arnoldi decompositions order determined preconditioner output computed approximate solution x associated residual vector r j preconditioner compute algorithm 21 initial vector apply gmresm determine matrices vm1 hm defined 21 22 respectively compute solution ym compute eigenvalues f matrix hm arnoldi decomposition order according 29 scale matrix righthand side linear system factor 1j j equation 19 approximately satisfied apply shift m1gamma arnoldi decomposition using ira formulas 2428 gives arnoldi decomposition mgamma endfor bound 211 satisfied goto use arnoldi decomposition gamma1 av mgammak k input algorithm 21 apply arnoldi process compute arnoldi decomposition apply gmresm determine matrices vm1 hm defined 22 respectively compute solution ym endfor fi 1 improve approximate solution gmresk update preconditioner k well adaptive preconditioners 15 trices v k1 22 available compute solution k 2 r k min 2 gamma1 3 r jk solution done endfor ff kr j kkr solution apply algorithm 21 initial vector matrices vm1 hm defined 21 22 respectively compute solution ym 4 r jm endwhilethe comments regarding lines labels 2 3 4 algorithm 35 also apply algorithm 36 4 numerical experiments numerical experiments presented section carried hp 9000735 computer using matlab examples chose initial approximate solution x b vector b randomly generated uniformly distributed entries open interval 0 1 purpose experiments compare algorithms 35 36 restarted parameter 0 chosen latter algorithm allowed least much computer storage former two algorithms also compare algorithms 35 36 gmres algorithm without restarts refer latter scheme full gmres terminated iterations iterative methods soon residual vector r j determined ffl solution algorithms 35 36 chose input parameter values ffl subspace 20 storage requirement algorithms 35 36 choice parameters 54 nvectors compare schemes restarted gmres60 algorithm requires storage 62 nvectors v 61 xm see algorithm 31 storage count assumes residual vector r algorithm 31 scaling factor stored first column matrix v 61 example 41 let matrix 2 r 200theta200 partitioned according 12 22 11 2 r 30theta30 circulant matrix first row gamma32 2 entries diagonal matrix 22 2 r 170theta170 uniformly distributed random numbers interval 1 10 matrix 12 zero matrix appropriate order thus j baglama et al matrix 30 eigenvalues circle center gamma32 radius 2 remaining eigenvalues uniformly distributed open interval 1 10 figure 41 shows denotes last preconditioner computed algorithm 35 shifts 210 eigenvalues shown unscaled matrix eigenvalues gamma1 also unscaled matrix associated preconditioner unscaled preconditioner maps eigenvalues smallest magnitude approximately signre n j n j illustrated figure 41 figure 42 shows iterates converge rapidly preconditioner removed many eigenvalues circle fz 2g remark plot determined algorithm 36 looks roughly plot eigenvalues preconditioner shown figure 41 graph algorithm 35 figure 42 continuous curve generated evaluating kr j k every value j residual vector r j defined ie every step richardson iteration every minimization residual error gmres algorithm graph algorithm 36 figure 42 dashed curve generated evaluating kr j k every minimization residual error gmres algorithm number matrixvector products matrix reported table 41 however number actually required algorithms 35 36 piecewise linear graph gmres60 figure 42 obtained linear interpolation nodes nodes marked circles column size krylov subspace table 41 displays parameter used algorithms 31 35 36 column preconditioners shows number preconditioners gamma1 used sufficiently accurate solution found number bounded ff 0 column vectors preconditioner parameter k algorithms 35 36 column labeled total vectors used counts number nvectors storage graph figure 42 dashdotted curve full gmres obtained applying gmresm solution 11 increasing values order improve initial approximate solution x 0 approximate solution xm sufficiently small residual error kr k determined figure 42 shows 10logarithm relative residual error kr k kkr 0 k example 42 consider 200 theta 200 block bidiagonal matrix gammay gammay gammay eigenvalues given 2j gamma1 figures 43 44 analogous figures 41 42 respectively table 42 analogous table 41 distribution eigenvalues gamma1 figure 43 indicates tolerance ffl subspace used computations large determine accurate approximate invariant subspace nevertheless adaptive preconditioners 17 eigenvalues closest origin removed algorithms 35 36 yield faster convergence restarted gmres60 algorithm see figure 44 2 example 43 let pores3 matrix harwellboeing matrix collection matrix 0 nonsymmetric order 3474 nonzero entries purpose shift obtain matrix positive eigenvalues figures 45 46 analogous figures 41 42 respectively table 43 analogous table 41 see eigenvalues matrix close origin others large magnitude figure 45 illustrates preconditioner moves eigenvalues away origin approximately signre n j n j negative figure 46 shows rate convergence 2 example 44 let diagonal matrix order 200 diagonal entries figures 47 49 analogous figures 41 42 respectively table 44 analogous table 41 figure 48 illustrates preconditioner moved smallest eigenvalues except one away origin figure 49 shows rate convergence 2 example 45 examples chose shifts according 210 ie determined approximations subspaces associated eigenvalues smallest magnitude present example illustrates algorithms 35 36 easily modified determine approximations invariant subspaces specifically used algorithm 36 solve linear system equations example 41 chose shifts eigenvalues largest real part matrices hm generated iterations thus sought determine invariant subspaces associated eigenvalues smallest real part figure 410 analogous figure 41 shows dots gamma1 eigenvalues circle removed number matrixvector products required stopping criterion satisfied 311 less numbers matrixvector products reported table 41 2 5 conclusion paper describes new preconditioning methods well suited use restarted gmresm algorithm numerous computed examples indicate iterates generated methods converge significantly faster iterates determined restarted gmres algorithm requires computer storage algorithms 35 36 describe versions preconditioning method eigenvalues j smallest magnitude matrix mapped approximately easy modify preconditioners eigenvalues mapped acknowledgements work paper carried last three authors visited computer science department ips eth would like thank walter gander martin gutknecht making visits possible would like thank marcus grote discussions code extracting matrices harwellboeing matrix collection richard lehoucq providing us reference 16 j baglama et al r principle minimized iterations solution matrix eigenvalue problem cambridge university press iterative methods computing eigenvalues large symmetric matrix newton basis gmres implementation parallel implementation gmres al gorithm implicitly restarted lanczos method large symmetric eigenvalue problems deflated augmented krylov subspace techniques parallel implementation restarted gmres iterative algorithm nonsymmetric systems linear equations numerical stability gmres parallel gmres version general sparse matrices restarted gmres preconditioned deflation iterative solution linear systems matrix computations gmrescr arnoldilanczos matrix approximation problems parallel preconditioning sparse approximate inverses eigenvalue translation based preconditioners gmresk method analysis implementation implicitly restarted arnoldi iteration restarted gmres method augmented eigenvectors hybrid gmres algorithm nonsymmetric linear systems krylov subspace methods solving large unsymmetric systems linear equations preconditioned krylov subspace methods cfd applications gmres generalized minimum residual algorithm solving nonsymmetric linear systems implicit application polynomial filters kstep arnoldi method introduction numerical analysis superlinear convergence behaviour gmres tr ctr loghin ruiz touhami adaptive preconditioners nonlinear systems equations journal computational applied mathematics v189 n1 p362374 1 may 2006 ronald b morgan restarted blockgmres deflation eigenvalues applied numerical mathematics v54 n2 p222236 july 2005 paul j harris ke chen efficient preconditioners iterative solution galerkin boundary element equation threedimensional exterior helmholtz problem journal computational applied mathematics v156 n2 p303318 15 july