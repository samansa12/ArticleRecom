choice wavelet smoothness primary resolution threshold wavelet shrinkage article introduces fast crossvalidation algorithm performs wavelet shrinkage data sets arbitrary size irregular design also simultaneously selects good values primary resolution number vanishing momentswe demonstrate utility method suggesting alternative estimates conditional mean wellknown ethanol data set alternative estimates outperform kovacsilverman method global variance estimate 25 careful selection number vanishing moments primary resolution alternative estimates simpler competitive results based kovacsilverman algorithm equipped local variance estimatewe include detailed simulation study illustrates crossvalidation method successfully picks good values primary resolution number vanishing moments unknown functions based walsh functions test response changing primary resolution piecewise polynomials zero one derivative test response function smoothness b introduction wavelet shrinkage technique estimating curves presence noise appealing nearly minimax wide range functions computationally practical spatially adaptive see seminal work donoho et al 1995 paper assumes familiarity wavelets wavelet shrinkage level nason department mathematics university bristol university walk bristol bs8 1tw uk silverman 1994 also rely heavily developments kovac silverman 2000 recent survey research area see vidakovic 1999 abramovich bailey sapatinas 2000 early wavelet shrinkage techniques relied mallats 1989 pyramid algorithm computing discrete wavelet transform dwt standard form requires data equally spaced contain 2 j values limited data situation wavelet shrinkage works taking dwt thresholding shrinking coefficients taking inverse transformation great deal research effort expended methods choose threshold value wavelet shrinkage see vidakovic 1999 excellent overview however threshold albeit important one parameter involved wavelet shrinkage successful shrinkage following criteria need also chosen well primary resolution thresholding coefficients applied coefficients whose resolution level equal finer primary resolution primary resolution parameter similar usual bandwidth parameter linear smoothing methods wavelet shrinkage choice primary resolution first investigated hall patil 1995 hall nason 1997 suggest actually choosing primary resolution continuous scale may advantageous however even primary resolution chosen discrete scale standard wavelet shrinkage critical use good values bandwidth critical linear smoothing analysing wavelet little detailed attention paid problem wavelet used wavelet shrinkage daubechies 1988 series compactly supported wavelets provide family mother wavelets varying smooth ness v v number vanishing moments v 2 c v r 02 c r space times continuously differentiable functions r consider daubechies extremal phase wavelets article ranging discontinuous haar wavelet smooth denote quadrature mirror filter associated daubechies extremal phase wavelet order v n hv length filter type wavelet transform also important example translationinvariant transform coifman donoho 1995 often gives better results using dwt threshold applied early work described donoho et al 1995 considered two methods applying threshold wavelet coefficients keeporkill hard thresholding similar model selection shrinkorkill soft threshold ing techniques suggested firm thresholding gao bruce 1997 recently bayesian wavelet shrinkage also thresholding interpretation see chipman et al 1997 abramovich et al 1998 article proposes fastupdate crossvalidation method aims find good combinations threshold number vanishing moments wavelet smoothness v primary resolution parameters often unique optimal combination parameters interact sometimes quite different combinations give similar results crossvalidation method described could extended incorporate different choices threshold application type wavelet transform clear choices could implemented fastupdate algorithm crossvalidation threshold selection wavelet shrinkage proposed especially functions sparsely represented wavelets nason 1996 generalized crossvalidation wavelet shrinkage proposed jansen malfait vial 1997 crossvalidation techniques proposed wang 1996 weyrich warhola 1998 course crossvalidation general technique around long time see stone 1974 details recently papers hall turlach 1997 sardy et al 1999 kovac silverman 2000 adapted wavelet shrinkage methodology data sets arbitrary size irregular design see also antoniadis gregoire vial 1997 antoniadis pham 1998 work fast linear wavelet methods random designs also mention recent algorithms based lifting transform show great promise curve surface estimation irregular data see example daubechies et al 1999 excellent review crossvalidatory method development kovac silverman 2000 works data sets irregular design arbitrary size additionally gives useful information wavelet primary resolution use well choosing threshold value 11 wavelet shrinkage kovacsilverman algorithm first establish notation describe data model specified kovac silverman 2000 suppose fx x 2 0 1 function wish estimate observe n data points gx according model i0 iid noise mean zero variance 2 fx i0 necessarily equally spaced kovac silverman 2000 propose choosing new equally spaced grid 0 1 interpolate observed data onto new grid propose choosing ng throughout article linearly interpolate original data new values k grid 2 although admit higher order interpolants reweighting schemes might also use writing original interpolated data vectors linear transform described 2 written matrix form interpolation matrix r depends x row r always contains either one two nonzero entries always sum one interpolation grid useful technique certainly new see example jones lotwick 1983 silverman 1986 kovac silverman 2000 apply wavelet shrinkage interpolated data first involves taking dwt n n orthogonal matrix associated daubechies extremal phase wavelet v vanishing moments practice mallats fast algorithm used matrix multiplication representation mathematically convenient given model 1 particular iid assumption noise covariance interpolated data given kovac silverman 2000 exploit fact linear interpolation scheme described actually band matrix applying dwt kovac silverman 2000 show variances individual wavelet coefficients computed exactly knowledge 2 estimated data using fast algorithm computational order ob2 j bandwidth matrix useful consequence kovac silvermans work variance wavelet coefficients computed effort computing wavelet coefficients hv 2 j also fast vannucci corradi 2000 also present fast algorithm compute variancecovariance matrix wavelet coefficients link twodimensional dwt kovac silverman 2000 show knowledge wavelet coefficient variances permits extension universal sure thresholds donoho johnstone 1994 1995 interpolated data situation also interested choosing threshold crossvalidation also simultaneously selecting good values number vanishing moments v primary resolution p next section shows possible apply full leaveoneout crossvalidation kovacsilverman setup still retain fast algorithm crossvalidation crossvalidation wellestablished technique assessing model prediction error situation selecting good choices threshold number vanishing moments primary resolution following sections describe obtain leaveone estimate prediction error wavelet shrinkage estimator f tvp x threshold number vanishing moments v primary resolution p aims minimize mean integrated squared error mise z 1n dx methodology could easily choose another form loss function par ticular wavelet shrinkage might interested better near known discontinuities inhomogeneities example however using mise estimate tvp estimate f constructed data except ith point find good values v p however believe first minimizer come across sense optimal unlike say crossvalidation score developed nason 1996 score multiple minima many possible investigated next sections describe construction efficient computation leaveoneout predictor using original data points apart key efficiency removing ith original data point changes grid points thus wavelet coefficients local x 21 leaveoneout interpolation removing ith original data point local effect interpolated data points linear interpolation grid points lie interval ith point one end points affected points interpolated point k either left right x removed removed new grid points left x 1 updated take value g 1 right xn 2 take value g n 2 assume x 1 k x compute value updated interpolated points k old ones k simple formula removed point l computed local ith original point recomputed n chosen well k need updated record indices k whose k value updated pass onto next stage 22 updating wavelet transform previous section tells changed mallat algorithm recursive algorithm takes fy k g n 1 k0 input computes coarser versions called father wavelet coefficients detail coefficients successive levels coarse coefficients father wavelet coefficients formula computing coarser approximation data c j 1 finer approximation c j given finest scale approximation c j initializes algorithm coarser ap proximations c j generated using successive applications 3 mother wavelet coefficients lost moving finer scale c j coarser scale c j 1 computed similar formula 3 except smoothing filter h v replaced detail extraction filter g v refer nason silverman 1994 vidakovic 1999 details key point efficiency changing single k affects wavelet coefficients derived k coefficients local k changed summary changing k changes wavelet coefficients specifically single father wavelet coefficient c j k changed coarser father wavelet coefficients c j 1 l k n hv need recomputed dxe smallest integer greater equal x bxc largest integer less equal x recall dwt recursive starting fy k g n 1 input formula 4 shows coefficients need recomputed coarser resolution level supplies indices changed recursively routine next coarsest level efficiency gains achieved noting range changed c j coefficients recomputing coarser c j 1 involved example c j k changed k min k k max need recompute c j 1 l k min n hv words 1k max k min n hv 1 coarser coefficients resolution level j 1 need computed k max k min 1 coefficients level j since coefficient computation takes hv recursive update wavelet coefficients effectively hv j hence extremely fast ie effectively o1 respect n n note similar algorithm developed dwt inversion 23 updating wavelet coefficient variance factors given covariance matrix wavelet coefficients covariance matrix given w v w removing ith point alters covariance matrix rr n n matrix r changes n n 1 matrix r let r k denote kth row r nonzero entry position ig rows r r r except r one entry missing zero r however rows k 2 r different r r therefore difference r r zero apart crossshaped region row column r general nonzero however since band matrices entries crossshaped region except close main diagonal less b away main diagonal zero summarizing compute covariance matrix interpolated data using almost zero apart rowscolumns near main diagonal r compute wavelet coefficient variance new interpolated data need consider w since already know w v w kovac silverman algorithm computation w v w easily performed using updating wavelet transform described previous section since multiplication w w simply application dwt since entries rows columns zero updating algorithm executed first zero transform nonzero entries rowcolumn application transform extremely fast hv j 24 thresholding inversion optimisation using information wavelet coefficients variance change one identify coefficient positions quantity jk jk changed jk updated variances wavelet coefficients jk variance factors kovac silverman 2000 2 estimate 2 jk updated empirical coefficients wavelet shrinkage estimate typically computed using robust estimator median absolute deviation mad wavelet coefficients finest level divided 06745 estimate updated quickly keeping track coefficients finest resolution level change thresholding point removal need note jk changed jk changed previously thresholded point removal subsequently smaller absolute size threshold estimate change means inversion performed prediction error simply taken nonremoved point estimate however jk changed status since last time estimate recomputed using efficient inverse algorithm described section 22 optimisation found grid search algorithm works extremely effectively finding minimal values also used golden section search method table 1 table showing values 1000 3 sf 311 fixed various values primary resolution p number vanishing moments v number vanishing moments v tends get stuck one multiple minima another strategy adopt condition universal threshold log n optimise find good values v p using good values v p optimise threshold strategy effective practice universal threshold makes useful starting value optimiser value independent v p 3 example ethanol data describe simulation study present applied example detail wellstudied ethanol data brinkman 1981 analysis cleveland et al 1993 hastie 1993 importantly purposes kovac silverman 2000 data consist measurements experiment ethanol burned single cylinder engine concentration total amount nitric oxide nitrogen dioxide engine exhaust normalized work done engine related equivalence ratio measure richness air ethanol mixture note range xaxis equivalence ratio variable 0535 1232 linearly shifted 0 1 ethanol data plotted top lefthand corner figure 1 purposes example fixed threshold value equal universal threshold value donoho johnstone 1994 default kovacsilverman method chooses equallyspaced grid points interpolate original data thus universal threshold computed 2 log n 311 threshold fixed table shows computed value values primary resolution ranging 0 6 numbers vanishing moments smoothness ranging 1 10 daubechies extremal phase series table clear lowest value occurs 4 might also interest one could continue higher values next conditioned three different pairs v p optimized equivalence ratio equivalence ratio equivalence ratio equivalence ratio figure 1 top left ethanol data nox emission versus equivalence ratio clockwise top right estimates f tvp x v p equal 311 8 2 313 10 5 288 2 3 threshold value minimize cases 8 2 10 5 2 3 minimizing thresholds 311 313 288 actually far universal threshold estimated curves f tvp x shown figure 1 topright plot figure shows best estimate underlying curve kovac silverman 2000 use daubechies extremal phase wavelet vanishing moments primary resolution 3 referring table 1 one see terms minimizing combination ranked 31st combinations tried therefore obtain approximate 25 improvement kovac silverman using best combination improvement using search method also demonstrated comparing plot best estimate two best bottom row figure 1 kovac silverman 2000 plots use combination 5 3 universal threshold bottomleft uses global estimate variance bottomright uses local estimate variance noticing variance nox variable decreases equivalence ratio use complex local variance estimate motivated fact estimate global variance contains small spike 08 like one bottomleft plot figure 1 however examining topright plot figure 1 tracks double bump spike 08 claim actually kovac silverman 2000 need go trouble forming local variance estimate merely need change number vanishing moments v 8 primary resolution 2 repeated kovacsilverman 2000 analysis new number vanishing moments primary resolution resulting estimate significantly better looks like topright plot figure 1 note exactly since use soft thresholding choose exactly universal threshold whereas use hard thresholding optimize value way trying denigrate kovac silverman 2000 indeed paper based extremely useful methodology however used example stress choice number vanishing moments primary resolution extremely important probably important choice threshold considerably neglected much literature available software 4 simulation study performed several simulations show number vanishing moments primary resolution correctly selected crossvalidation method produces broadly similar results globalsure type thresholding also universal thresholding although goal universal thresholding mise minimisation globalsure thresholding method introduced nason 1996 single threshold version leveldependent technique based steins unbiased risk estimation wavelet shrinkage introduced donoho johnstone 1995 however ethanol example demonstrates previous section important show methodology adapt features smoothness underlying function scale features require particular choices numbers vanishing moments primary resolution first simulation adapting primary scale concentrates choice primary resolution next three simulations adapting smoothness concentrate choice number vanishing moments adapting smoothness simulations fix threshold universal threshold using sample sizes 500 sample size perform 10 simulations find best combinations number vanishing moments primary resolution case use gaussian noise zero mean variance 2 specified section adapting smoothness simulations demonstrate primary resolution influenced discontinuities lowestorder derivatives number vanishing moments chosen crossvalidation algorithm influenced underlying smoothness true function however results hard fast occasionally crossvalidation technique gets wrong 41 adapting primary scale underlying true function simulation walsh function w p x defined piecewise constant function taking values 0 1 starting 0 switching 1 back 0 number switches interval parametrised p distance switch 1p convenient way think walsh functions sine wave blocked parameter p formally known sequency number walsh function akin frequency parameter sine wave exactly walsh function always periodic 0 1 see stoffer 1991 information walsh functions applications statistics table 2 demonstrates selected primary resolution increases fineness true walsh function although appears quite variability selected primary resolution values 2 however notice eg 2the width walsh peaks 364 primary resolution matching 4 5 corresponding nearest widths 8256 4256 haar wavelet resolution level might expected however algorithm chooses primary resolution 7 indeed primary resolutions also overestimate way effect presumably addition noise causes procedure conservative use finer scale wavelets conceptually best wavelet basis representing set walsh functions haar basis table 3 shows crossvalidation method nearly always selects haar best basis situation ten simulations tables 2 3 based sample sizes variance deliberately large signal noise ratio simulation verify lownoise situations crossvalidation procedure chooses reasonable values primary resolution number vanishing moments clearly signal noise ratio decreases procedure choose best values far less often leave work interesting questions values signal noise ratio become hard select good parameters whether competitors sure better job table 2 best primary resolution walsh function w p x sequency number 2simulation number table 3 best number vanishing moments walsh function w p x sequency number 2simulation number 42 adapting smoothness derivatives underlying true function example x x 2 0 1 function continuous 0 1 differentiable everywhere first derivative discontinuity 1 variance noise simulations best primary resolutions numbers vanishing moments simulationsample size combination shown tables 4 5 sample sizes modal number vanishing moments 3 associated approximate wavelet smoothness 06 however smaller sample sizes wavelet 7 vanishing moments selected 4 times 10 many times wavelet modal primary resolution appears 4 chosen table 4 best primary resolution function derivatives different sample sizes table 5 best number vanishing moments function derivatives different sample sizes simulation number table primary resolution function one derivative different sample sizes simulation number 43 adapting smoothness one derivative underlying true function example function continuous 0 1 first derivative continuous 0 1 first derivative differentiable everywhere second derivative discontinuities 1and 3 variance added noise simulations best primary resolutions numbers vanishing moments simulationsample size combination shown tables 6 7 sample sizes modal number vanishing moments 9 associated approximate wavelet smoothness approximately 18 however smaller sample sizes wavelet 10 vanishing moments selected 5 times 10 interesting note smoothness wavelets selected example one derivative approximately twice example derivatives one would expect modal primary resolution table 7 best number vanishing moments function one derivative different table 8 best primary resolution function mixed derivatives different sample sizes simulation number table best number vanishing moments function mixed derivatives different sample sizes simulation number 2 44 adapting smoothness mixed derivatives underlying true function example mixes two functions f 0 x f 1 x previous two examples f mixed function continuous 0 1 first derivative discontinuity 1and second derivative discontinuities 5and 7 variance added noise simulations best primary resolutions numbers vanishing moments simulationsample size combination shown tables 8 9 modal primary resolution 4 smaller sample sizes agreeing primary resolution derivative case section 42 5 size primary resolution greatly influenced lowestorder derivative one might expect work hall patil 1995 number vanishing moments sample size 3 four simulations around 67 others lower sample sizes number vanishing moments generally larger ones selected f 0 x example high f 1 x example indicates maybe sort compromise made 5 conclusions work article introduced fast crossvalidation method performs wavelet shrinkage data sets irregular design arbitrary size also selects good values number vanishing moments primary resolution crossvalidation method shown work well ethanol data set simulated data scale primary resolution smoothness vanishing mo ments underlying true function controlled work could performed investigate conditions method would break terms diminishing signal noise ratio nongaussian correlated noise situations method could easily extended use leveldependent thresholds would use correlated data would also interesting see well mise estimator globalsure would perform place crossvalidation estimate herrick 1999 uses crossvalidation kovacsilverman 2000 algorithm twodimensional case although using fast version described implementation slow remains seen whether fast version plausible fast point insertiondeletion techniques green sibson 1978 voronoi dia gramdelaunay triangulation used data interpolation would certainly value acknowledgments author would like particularly thank arne kovac helpful discussions access code would like thank bernard silverman david herrick others bristol statistics group audience wavelets contributed papers session 1999 isi helsinki meeting helpful conversations suggestions author would also like thank referees editors supplying extremely helpful comments suggestions r wavelet analysis statistical applications random design wavelet curve smoothing statistics probability letters computational statistics data analysis adaptive bayesian wavelet shrink age models orthonormal bases compactly supported wavelets wavelets irregular point sets spatial adaptation wavelet shrinkage adapting unknown smoothness via wavelet shrinkage wavelet shrinkage asymptopia waveshrink firm shrinkage computing dirichlet tesselations plane choosing noninteger resolution level using wavelet methods mean integrated squared error nonlinear waveletbased density estimators generalized additive models wavelet methods curve surface estimation generalised cross validation wavelet thresholding errors involved computing empirical characteristic function extending scope wavelet regression methods coefficientdependent thresholding theory multiresolution signal decomposition wavelet rep resentation wavelet shrinkage using crossvalidation discrete wavelet transform wavelet shrinkage unequally spaced data density estimation statistical modeling wavelets function estimation via wavelet shrinkage longmemory data wavelet shrinkage generalized cross validation image denoising tr ctr matthew nunes marina knight guy p nason adaptive lifting nonparametric regression statistics computing v16 n2 p143159 june 2006