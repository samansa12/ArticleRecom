feature selection neural networks present neural network based approach identifying salient features classification feedforward neural networks approach involves neural network training augmented crossentropy error function augmented error function forces neural network keep low derivatives transfer functions neurons learning classification task approach reduces output sensitivity input changes feature selection based reaction crossvalidation data set classification error due removal individual features demonstrate usefulness proposed approach one artificial three realworld classification problems compared approach five feature selection methods banks different concept algorithm developed outperformed methods achieving higher classification accuracy problems tested b introduction learning systems primary source information data numerical systems like neural networks nns data usually represented vectors subspace r k whose components features may correspond example measurements performed physical system information gathered observation phenomenon usually features equally informative may noisy meaningless correlated irrelevant task feature selection aims selecting subset features relevant given problem often important issue amount data gather process may reduced training may easier better estimates obtained using relevant features case small data sets sophisticated processing methods may used smaller dimensional spaces original measure space performances may increase non relevant information interfere etc feature selection subject intensive researches statistics application domains like pattern recognition process identification time series modelling econometrics recently began investigated machine learning community developed methods whatever domain feature selection remains difficult problem time non monotonous problem ie best subset p variables always contain best subset q variables q p also best subset variables depends model used process data usually two steps treated sequentially methods variable selection rely heuristics perform limited exploration whole set variable combinations field nns feature selection studied last ten years classical well original methods employed discuss problem feature selection specifically nns review original methods developed field certainly exhaustive since literature domain already important main ideas proposed described describe sections 2 3 basic ingredients feature selection methods notations briefly present section 4 statistical methods used regression classification used baseline techniques describe section 5 families methods developed specifically neural networks may easily implemented either regression classification tasks representative methods compared different test problems section 6 basic ingredients feature selection methods feature selection technique typically requires following ingredients feature evaluation criterion compare variable subsets used select one subsets search procedure explore subspace possible variable combinations stop criterion model selection strategy 211 feature evaluation depending task eg prediction classification model linear logistic neural networks several evaluation criteria based either statistical grounds heuristics proposed measuring importance variable subset classification classical criteria use probabilistic distances entropy measures often replaced practice simple interclass distance measures regression classical candidates prediction error measures survey classical statistical methods may found thomson 1978 regression mclachlan 1992 classification methods rely data computing relevant variables take consideration model used processing data selection step may rely hypothesis data distribution parametric methods non parametric methods methods take account simultaneously model data usually case nn variable selection 212 search general since evaluation criteria non monotonous comparison feature subsets amounts combinatorial problem 2 k 1 possible subsets k variables rapidly becomes computationally unfeasible even moderate input size branch bound exploration narendra fukunaga 1977 allows reduce search monotonous criteria however complexity procedures still prohibitive cases due limitations algorithms based upon heuristic performance measures evaluation suboptimal search suboptimal search methods follow one following sequential search techniques see eg kittler start empty set variables add variables already selected variable set forward methods start full set variables eliminate variables selected variable set backward methods start empty set alternate forward backward steps stepwise methods plus l take away r algorithm generalisation basic stepwise method alternates l forward selections r backward deletions 213 subset selection stopping criterion let given feature subset evaluation criterion search procedure several methods examine subsets provided search eg 2 k 1 exhaustive search k simple backward search select relevant according evaluation criterion empirical distribution evaluation measure related statistics known tests may performed irrelevance hypothesis input variable classical sequential selection procedures use stop criterion examine variables sequentially stop soon variable found irrelevant according statistical test classical parametric methods distribution characteristics eg estimates evaluation measure variance easily derived see sections 41 42 non parametric flexible methods like nns distributions difficult obtain confidence intervals would allow perform significance testing might computed via monte carlo simulations bootstrapping extremely prohibitive practical use except particular cases eg baxt white 1996 hypothesis testing thus seldom used models many authors use instead heuristic stop criteria better methodology whose complexity still reasonable applications compute successive variable subsets provided search algorithm estimate generalization error prediction risk obtained subset selected variables giving best performances generalization error estimate may computed using validation set crossvalidation algebraic methods although latter easy obtain non linear models note strategy involves retraining nn subset 3 notations denote k g realization random variable pair xy probability distribution p x th component x x l l th pattern given data set cardinality n following restrict one hidden layer nns number input output units denoted respectively k g transfer function network denoted f training performed according mean squared error criterion mse although restrictive consider selection methods classification regression tasks 4 model independent feature selection introduce methods perform selection classification regression steps sequentially ie take account classification regression model selection methods nn oriented used experimental comparison nn specific selection techniques section 6 first two basic statistical techniques aimed respectively regression classification methods well fitted nns since hypothesis rely correspond situations nns might useful however since nn specific methods heuristics used baseline comparison third one developed recently general selection technique data hypothesis free might used system either regression classification based probabilistic dependence measure two sets variables 41 feature selection linear regression consider linear regression approach described may trivially extended multiple regression let x 1 x 2 x k real variables supposed centered let us denote current approximation p selected variables x renumbered p first selected variables correspond numbers 1 p residuals x assumed identically independently distributed let us denote l l 1 forward selection choice p th variable usually based r p 2 partial correlation coefficient table 1 regressor f p adjusted coefficient 1 coefficient represents proportion total variance explained regressor f p p th variable select one f p maximizes coefficient importance new variable usually measured via fisher test thompson 1978 compares models p1 p variables fsp forward table 1 selection stopped 1 adjusted coefficient r often used instead r p fsp forward f1npa fisher statistics 1np degrees freedom confidence level choice stop l 2n backward ssr l 1 table 1 choice stop criteria used statistical forward backward methods note f could also used place r p 2 choice criterion forward p1 variables already selected r p1 2 constant value 01 maximizing f similar maximizing r p 2 equation 413 selects variables order r p 2 backward elimination variable eliminated remaining p less significant terms fisher test ie one smallest value ssr p1 equivalently fsp backward table 1 selection stopped backward f1npa 42 feature selection classification classification shall select variable subset allows best separation data variable selection usually performed considering class separation criterion choice criterion associated ftest stopping criterion regression forward backward stepwise methods may used data separation usually computed interclass distance measure kittler 1986 frequent discriminating measure wilks lambda wilks 1963 l sv p defined follows w intraclass matrix dispersion corresponding selected variable set sv p b corresponding interclass matrix 2 determinant matrix determinant covariance matrix measure volume occupied data w measures mean volume different classes wb volume whole data set quantities computed selected variables good discriminating power corresponds small value l sv p different classes represented compact clusters well separated criterion well suited case multinormal distributions equal covariance class meaningless eg multimodal distributions clearly restrictive hypothesis measurement statistic f defined fg1ngp1a distribution mclachlan 1992 use wilks lambda estimating discriminating power variable stopping selection forward backward habbema hermans 1977 stepwise methods comparisons section 6 used stepdisc stepwise method based 422 95 confidence level 43 mutual information data considered realization random process probabilistic information measures may used order compute relevance set two quantities defined x l class j g number classes n j number samples class j j mean class j global mean variables respect variables mutual information measure defined ab b two variables probability density pa pb mutual information independent inversible differentiable transformation variables measures uncertainty reduction b known also known kullbakleibler distance joint distribution pab marginal distribution product papb method described make use restrictive assumptions data therefore general attractive ones described sections 41 42 especially hypothesis correspond data processing model usually case nns may used either regression discrimination hand non parametric methods computationally intensive main practical difficulty estimation joint density pab marginal densities pa pb non parametric density estimation methods costly high dimensions necessitate large amount data algorithm presented uses shannon entropy denoted h compute mutual information possible use entropy measures like quadratic cubic entropies kittler 1986 battiti 1994 proposed use mutual information forward selection algorithm called mifs mutual information based feature selection pab estimated fraser algorithm fraser swinney 1986 recursively partitions space using c 2 tests data distribution algorithm compute mutual information two variables order compute mutual information x p selected variable set sv p1 belong sv p1 battiti uses simplifying assumptions moreover number variables select fixed selection algorithms uses forward search variable x p one maximises value sv p1 set p 1 already selected variables bonnlander weigend 1994 use epanechnikov kernels density estimation hrdle 1990 branchbound bb algorithm search narendra fukunaga 1977 bb warrants optimal search criterion used monotonous less computationally intensive exhaustive search search algorithm one also consider suboptimal floating search techniques proposed pudil et al 1994 offer good compromise sequential methods simplicity relative computational cost branchbound algorithm comparisons section 6 used epanechnikov kernels density estimation 432 forward search selection stopped mi increase falls fixed threshold 099 5 model dependent feature selection neural networks model dependent feature selection attempts perform simultaneously selection processing data feature selection process part training process features sought optimizing model selection criterion global optimization looks attractive modelindependent selection adequacy two steps user however since value choice criterion depends model parameters might necessary train nn different sets variables selection procedures alternate variable selection retraining model parameters forbids use sophisticated search strategies would computationally prohibitive specificities nns also taken consideration deriving feature selection algorithms nns usually non linear models since many parametric modelindependent techniques based hypothesis inputoutput variables dependency linear input variables redundancy well measured linear correlation variables methods clearly ill fitted nns search space usually many local minima relevance measures depend minimum nn converged measures averaged several runs applications prohibitive considered except white 1989 derives results weight distribution work nn community might used hypothesis testing nn feature selection algorithms choice criteria mainly based heuristic individual feature evaluation functions several proposed literature made attempt classify according similarity distinguish zero order methods use network parameter values first order methods use first derivatives network parameters second order methods use second derivatives network parameters feature evaluation criteria allow rank variables given time value criterion non informative however see methods work reasonably well feature selection methods neural networks use mostly backward search although forward methods also proposed moody 1994 goutte 1997 several methods use individual evaluation features ranking take consideration dependencies correlations may problematic selecting minimal relevant sets variables using correlation simple dependence measure enough since nns capture non linear relationships variables hand measuring non linear dependencies trivial authors simply ignore problem others propose select one variable time retrain network new selected set evaluating relevance remaining variables allows take account dependencies network discovered among variables critical difficulty defining sound stop criterion model choice many methods use crude techniques stopping selection eg threshold choice criterion value rank different subsets using estimation generalization error latter expected error performed future data defined case rxy euclidean error desired computed outputs estimates computed using validation set crossvalidation algebraic approximations risk like final prediction 1970 several estimates proposed statistical gustafson hajlmarsson 1995 nn moody 1991 larsen hansen 1994 literature comparison section 6 used simple threshold authors gave indication stop criterion validation set approximation risk otherwise 51 zero order methods linear regression models partial correlation coefficient expressed simple function weights although sound non linear models attempts using input weight values computation variable relevance observed inefficient heuristic weights cannot easily interpreted models sophisticated heuristic proposed yacoub bennani 1997 exploits weight value network structure multilayer perceptron derived following criterion h denote respectively input hidden output layer better understanding measure let us suppose hidden output unit incoming weight vector unitary l 1 norm equation written 512 inner term product weights input hidden unit j j output importance variable output sum absolute values products paths nn unit unit importance variable defined sum values outputs denominators 511 operate normalizing factors important using squashing functions since functions limit effect weight magnitude note measure depend magnitude input different variables similar range two weight layers different role mlp reflected 511 example outputs linear normalization suppressed inner summation 511 used backward search nn retrained variable deletion stop criterion based evolution performances validation set elimination stopped soon performances decrease 52 first order methods several methods propose evaluate relevance variable derivative error output respect variable evaluation criteria easy compute lead similar results derivatives measure local change outputs wrt given input inputs fixed since derivatives constant like linear models must averaged training set measures fully meaningful inputs independent since measures average local sensitivity values training set representative input space 521 saliency based pruning sbp backward method moody utans 1992 uses evaluation criterion variation learning error variable x replaced empirical mean since variables assumed centered mse x l l l l l direct measure usefulness variable computing output large values n computing costly linear approximation may used f x l l l l l x variables eliminated increasing order feature set nn trained estimate generalization error generalization final prediction error criterion computed model minimum generalization error selected changes mse ambiguous inputs correlated variable relevance computed method take account possible correlations variables relevance could computed successive nns sequence computational extracost ok 2 computations instead ok present method 522 methods using computation output derivatives linear model output derivative wrt input constant case non linear nns several authors proposed measure sensitivity network transfer function respect input x computing mean value outputs derivative respect x whole training set case multilayer perceptrons derivative computed progressively learning hashem 1992 since derivatives may take positive negative values may compensate produce average near zero measures use average squared absolute derivatives tenth measures based derivatives proposed many others could defined thus give representative sample measures sum derivative absolute values used eg ruck et al l 1 classification priddy et al 1993 remark since error decision j x may estimated 1 f j x 523 may interpreted absolute value error probability derivative averaged decisions outputs data squared derivatives may used instead absolute values refenes et al 1996 example proposed regression normalized sum x f f x l l x x var holds variance also proposed series related criteria among normalized standard deviation derivatives f x f x f x l l l l x weighted average derivatives absolute values weights reflect relative magnitude x fx f x x f l l l x x measures sensitive input space representativeness sample set several authors proposed use subset sample order increase significance relevance measure order obtain robust methods nonpathological training examples discarded regression radial basis function networks dorizzi et al 1996 propose use 95 percentile derivative absolute value f aberrant points eliminated contributes robustness measure note idea could used relevance measures proposed paper following line czernichow 1996 proposed heuristic criterion regression estimated set non pathological examples whose cardinality n proposed choice criterion f l f l classification rossi 1996 following proposition made priddy et al 1993 considers patterns near class frontiers proposes following relevance measure f x f x l frontier l x x x frontier defined set point x l f x e e fixed threshold several authors also considered relative contribution partial derivatives gradient 529 methods use simple backward search stopping criteria authors use heuristic rules except refenes et al 1996 define statistical tests relevance measures non linear nns necessitates estimation relevance measure distribution costly opinion usually prohibits approach even looks attractive 523 links methods methods use simple relevance measures depend upon gradient network outputs respect input variables difficult rank different criteria said wise use reasonable rules like discarding aberrant points robustness retraining nn discarding variable computing new relevance measures nn sequence order take account dependencies variables practice methods give similar results shown section 6 summarize table 2 main characteristics relevance measures different methods derivative used task cr data used moody 521 f cr refenes 525 f cr dorizzi 527 f cr non pathological data refenes 526 f cr czernichow 528 f cr non pathological data refenes 524 f ruck rossi c frontier classes table 2 computation relevance variable different methods using derivative network function c r denote respectively classification regression tasks 53 second order methods several methods propose evaluate relevance variable computing weight pruning criteria set weights input node present three methods first one bayesian approach computing weight variance two use hessian cost function computing cost function dependence upon input unit weights 531 automatic relevance determination ard method proposed mackay 1994 framework bayesian learning approach weight considered random variables regularization terms taking account input included cost function assuming prior probability distribution group weights th input gaussian input posterior variance 2 estimated help hessian matrix ard successful time serie prediction learning regularization terms improved prediction performances however ard really used feature selection method since variables pruned training 532 optimal cell damage several neural selection methods inspired weight pruning techniques latter decision pruning weight made according relevance criterion often named weight saliency weight pruned saliency low similarly saliency input cell usually defined sum weights saliencies fanouti set weights input optimal cell damage ocd proposed cibas et al 1994a 1996 similar method also proposed mao et al 1994 feature selection method inspired optimal brain damage obd weight pruning technique developed lecun 1990 obd connection saliency defined order two taylor expansion mse variation around local minimum hessian matrix h easily computed using gradient descent may computationally intensive large networks obd authors use diagonal approximation hessian computed saliency input variable defined accordingly cibas et al 1994 proposed use 535 choice criterion eliminating variables nn trained reach local minimum variables whose saliency given threshold eliminated threshold value fixed cross validation process repeated variable found threshold method tested several problems gave satisfying results difficulty lies selecting adequate threshold furthermore since several variables eliminated simultaneously whereas individual variable pertinence measures used significant sets dependent variables may eliminated stopping generalization performances nn sequence estimated via validation set variable set corresponding nn best performances chosen hessian diagonal approximation questioned several authors hassibi stork 1993 example proposed weight pruning algorithm optimal brain surgeon obs similar obd uses whole hessian computing weight saliencies stahlberger riedmiller 1997 proposed feature selection method similar ocd except takes account non diagonal terms hessian methods saliency computed using performance measure error variation training set weight estimation model selection use data set optimal pedersen et al 1996 propose two weight pruning methods gobd gobs compute weight saliency according estimate generalization error final prediction error akaike 1970 similarly obd obs methods could also transformed feature selection methods 533 early cell damage ecd using second order taylor expansion obd family methods justified local minimum reached cost locally quadratic minimum hypothesis barely met practice tresp et al 1997 propose two weight pruning techniques family coined ebd early brain damage ebs early brain surgeon use heuristic justification take account early stopping adding new term saliency computation methods extended feature ranking call ecd early cell damage ebd extension ecd saliency input defined algorithm propose slightly different ocd one variable eliminated time nn retrained deletion choosing best set variables used variation selection according estimate generalization error method estimate computed using validation set since performances may oscillate significantly different several subsets may performances see eg figure 1 using fisher test compare model performances best model select set networks whose performances similar best ones choose among networks one smallest number input variables 6 experimental comparison present comparative performances different feature selection methods comparing methods difficult task unique measure characterizes importance input selection accuracy also depends search technique variable subset choice criterion case nns different steps rely heuristics could exchanged one method nns used multilayer perceptrons one hidden layer 10 neurons comparison provide intended definite ranking different methods illustrating general behavior methods described used two synthetic classification problems illustrate different difficulties variable selection first one frontiers nearly linear dependent variables well pure noise variables second problem non linear frontiers variables chosen independent correlated first problem originally proposed breiman et al 1984 three class waveforms classification problem 19 noisy dependent features also used variation problem 21 pure noise variables added 19 initial variables 40 inputs variant training set 300 patterns test set 4300 description problem provided appendix performances optimal bayes classifier estimated test set 86 correct classification performance comparison appears tables 3 4 two instances method p selected variables perf stepdisc 422 14 000110111111111011100 0000000000000000000 bonnlander 432 12 000011101111111110000 0000000000000000000 moody ruck 523 dorizzi 527 czernichow 528 17 010111111111111111100 0000000000000000000 cibas 535 9 000001111110111000000 0000000000000000000 8226 leray 536 11 000001111111111100000 0000000000000000000 table 3 performance comparison different variable selection noisy wave problem noisy problem methods eliminate pure noise variables except two methods bottom table 3 give slightly lower performances select fewer variables give similar values around 85 correct stepdisc also gives good performances since problem data unimodal distribution frontiers nearly linear non noisy problem performances methods ordering change two techniques bottom table 4 give slightly better performances method p selected variables perf none 21 111111111111111111111 stepdisc 422 14 001110101111111011100 bonnlander 432 8 000001100111101010000 moody 521 ruck 523 dorizzi czernichow cibas 535 15 001011111111111110100 leray table 4 performance comparison different variable selection methods original wave problem number remaining variables ecd ocd figure 1 performance comparison two variable selection methods ocd ecd according number remaining variables noisy wave problem figure shows performance curves two methods ocd ecd estimated validation set since used single validation set small fluctuations performances form cross validation used order get better estimates test strategy proposed ecd looks also attractive case seen problem performances less similar backward elimination slightly rise quickly drop relevant variables removed842060100 none yacoub moody cibas leray ruck stepdisc bonnlander czernichow figure 2 performance comparison different variable selection methods vs percentage selected variables original wave problem x axis percentage variables selected axis percentage correct classification figure 2 gives repartition different variable selection methods original wave problem according performances axis percentage selected variables x axis best methods best performances lower number variables problem leray satisfying see figure 2 yacoub delete enough variables bonnlander deletes much variables second problem two class problem 20 dimensional space classes distributed according two gaussians respectively 1 00 chosen 1 2 problem variable relevance ordered according index x 1 useless x i1 relevant x method p selected variables perf stepdisc 422 17 10001111111111111111 bonnlander 432 5 00010000000000011011 9060 9486 moody 521 9 01000100011000110111 9294 ruck 9486 dorizzi 527 11 00000000101111111111 czernichow 528 9 00000000001101111111 cibas leray 536 15 01011011101110111111 table 5 performance comparison different variable selection methods two gaussian problem uncorrelated variables9193954080 none yacoub stepdisc leray cibas dorizzi ruck czernichow moody bonnlander figure 3 performance comparison different variable selection methods vs percentage selected variables two gaussian problem uncorrelated variables x axis percentage variables selected axis percentage correct classification table 5 shows stepdisc adapted non linear frontier method selects x 1 useless problem remark figure 3 bonnlanders method deletes many variables whereas yacoubs stop criterion rough delete enough variables experiment replaced matrix 1 2 block diagonal matrix block 5x5 four groups five successive correlated variables new problem method p selected variables perf 9058 stepdisc 422 11 00001101011010110111 bonnlander 432 5 00001001010000100001 ruck 9106 leray 9072 table 6 performance comparison different variable selection methods two gaussian problem correlated variables table 6 gives results representative methods problem stepdisc still gives model good performances selects many correlated variables bonnlanders method selects 5 variables gives significantly lower results rucks method obtains good performances selects correlated variables lerays method thanks retraining variable deletion find models good performances variables 7 compared 10 11 ruck stepdisc 7 conclusion reviewed variable selection methods developed field neural networks main difficulty nns non linear systems make use explicit parametric hypothesis consequence selection methods rely heavily heuristics three steps variable selection relevance criterion search procedure nn variable selection use mainly backward search choice final model first discussed main difficulties developing steps introduced different families methods discussed strengths weaknesses believe variable selection method must remain computationally feasible useful considered techniques rely computer intensive methods like eg bootstrap step selection instead proposed series rules could used order enhance several methods described reasonable extra computational cost eg retraining nn sequence computing relevance nn allows take account correlations variables simple estimates generalization error may used evaluation variable subset simple tests estimates allow choose minimal variable sets section 533 finally performed comparison representative nn selection techniques synthetic problems r statistical predictor identification using mutual information selecting features supervised neural net learning bootstrapping confidence intervals clinical input variable effects network trained identify presence acute myocardial infraction selecting input variables using mutual information nonparametric density evaluation classification regression trees fogelman souli fogelman souli architecture selection statistical sensitivity analysis independent coordinates strange attractors mutual information extracting relevant decays time series modelling gustafson hajlmarsson selection variables discriminant analysis fstatistic error rate applied nonparametric regression econometric society monograph n sensitivity analysis feedforward artificial neural networks differentiable activation functions second order derivatives network pruning feature selection extraction generalized performances regularized neural networks models bayesian nonlinear modelling energy prediction competition discriminant analysis statistical pattern recognition note generlization principled architecture selection neural networks application corporate bond rating prediction prediction risk architecture selection neural networks statistics neural networks theory pattern recognition applications branch bound algorithm feature subset selection floating search methods feature selection pattern recognition letters attribute suppression multilayer perceptron fast network pruning feature extraction using unitobs algorithm selection variables multiple regression learning artificial neural networks mathematical statistics hvs heuristic variable selection multilayer artificial neural network classifier tr feature selection automatic classification nongaussian data image enhancement thresholding optimization fuzzy compactness introduction statistical pattern recognition 2nd ed floating search methods feature selection merging backpropagation hebbian learning rules robust classifications feature selection neural networks pattern recognition ctr e gasca j snchez r alonso rapid brief communication eliminating redundancy irrelevance using new mlpbased feature selection method pattern recognition v39 n2 p313315 february 2006 rajen b bhatt gopal fuzzyrough sets approach feature selection pattern recognition letters v26 n7 p965975 15 may 2005 bacauskiene verikas selecting salient features classification based neural network committees pattern recognition letters v25 n16 p18791891 december 2004 chaoton su longsheng chen tailin chiang neural network based information granulation approach shorten cellular phone test process computers industry v57 n5 p412423 june 2006 franois f rossi v wertz verleysen resampling methods parameterfree robust feature selection mutual information neurocomputing v70 n79 p12761288 march 2007 r e abdelaal gmdhbased feature ranking selection improved classification medical data journal biomedical informatics v38 n6 p456468 december 2005