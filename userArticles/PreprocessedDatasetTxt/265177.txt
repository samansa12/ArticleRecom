parallelization static scheduling algorithms abstractmost static algorithms schedule parallel programs represented macro dataflow graphs sequential paper discusses essential issues pertaining parallelization static scheduling presents two efficient parallel scheduling algorithms proposed algorithms implemented intel paragon machine performances evaluated algorithms produce highquality scheduling much faster existing sequential parallel algorithms b introduction static scheduling utilizes knowledge problem characteristics reach global optimal near optimal solution although many people conducted research various manners share similar underlying idea take directed acyclic graph representing parallel program input schedule onto processors target machine minimize completion time npcomplete problem general form 7 therefore many hueristic algorithms produce satisfactory performance proposed 11 13 5 14 12 4 9 although scheduling algorithms apply parallel programs algorithms sequential executed single processor system sequential algorithm slow scalability static scheduling restricted since large memory space required store task graph natural solution problem using multiprocessors schedule tasks multiprocessors fact without parallelizing scheduling algorithm running parallel computer scalable scheduler feasible parallel scheduling algorithm following features ffl high quality able minimize completion time parallel program ffl low complexity able minimize time scheduling parallel program two requirements contradict general usually highquality scheduling algorithm high complexity modified criticalpath mcp algorithm introduced 13 offered good quality relatively low complexity paper propose two parallelized versions mcp describe mcp algorithm next section discuss different approaches parallel scheduling well existing parallel algorithms section 3 sections 4 5 present vpmcp hpmcp algorithms respectively comparison two algorithms presented section 6 2 mcp algorithm macro dataflow graph directed acyclic graph starting point end point 13 macro dataflow graph consists set nodes fn 1 connected set edges denoted en node represents task weight node execution time task edge represents message transferred one node another node weight edge equal transmission time message two nodes scheduled processing element pe weight edge connecting becomes zero define scheduling algorithm succinctly first define aslateaspossible time node alap time defined critical length critical path leveln length longest path node n end point including node n 6 fact highquality scheduling algorithms less rely alap time level modified criticalpath mcp algorithm designed schedule macro dataflow graph bounded number pes mcp algorithm 1 calculate alap time node 2 sort node list increasing alap order ties broken using smallest alap time successor nodes successors successor nodes 3 schedule first node list pe allows earliest start time considering idle time slots delete node list repeat step 3 list empty 2 step 3 determining start time idle time slots created communication delays also considered node inserted first feasible idle time slot method called insertion algorithm mcp algorithm compared four wellknown scheduling algorithms assumption ish 10 etf 8 dls 12 last 3 shown mcp performed best 2 complexity mcp algorithm 2 logn n number nodes graph second step ties broken randomly simplified version mcp scheduling quality varies little complexity reduced 2 following use simplified version mcp 3 approaches parallelization scheduling algorithms basic idea behind parallel scheduling algorithms instead identifying one node scheduled time identify set nodes scheduled parallel following pes execute parallel scheduling algorithm called physical pes ppes order distinguish target pes tpes macro dataflow graph scheduled quality speed parallel scheduler depend data partitioning two major data domains scheduling algorithm source domain target domain source domain macrodataflow graph target domain schedule target processors consider two approaches parallel scheduling first one called vertical scheme ppe assigned set graph nodes using space domain partitioning also maintains schedules one tpes second one called horizontal scheme ppe assigned set graph nodes using time domain partitioning resultant schedule also partitioned ppe maintains portion schedule every tpe ppe schedules portion graph ppes exchange information determine final schedule vertical horizontal schemes illustrated figure 1 task graph mapped timespace domain assume three ppes schedule graph six tpes thus vertical scheme ppe holds schedules two tpes horizontal scheme ppe holds portion schedules six tpes vertical scheme horizontal scheme outlined figure 2 3 respectively vertical scheme ppes exchange information schedule graph nodes tpes frequent information exchange results large communication overhead horizontal partitioning ppe schedule graph partition without exchanging information another ppe last step ppes exchange information subschedules concatenate obtain final schedule problem method start times partitions first one unknown time needs estimated scheduling quality depends estimation almost work designing parallel algorithm scheduling fact algorithm vertical scheme yet algorithm area horizontal space time space time vertical scheme b horizontal scheme p1target processors target processors physical processors physical processors task schedule task schedule task graph task graph figure 1 vertical horizontal schemes 1 partition graph p equal sized sets using space domain partitioning 2 every ppe cooperates together generate schedule ppe maintains schedules one tpes figure 2 vertical scheme parallel scheduling 1 partition graph p equal sized sets using time domain partitioning 2 ppe schedules graph partition generate subschedule 3 ppes exchange information concatenate subschedules figure 3 horizontal scheme parallel scheduling scheme parallel bsa pbsa algorithm 1 bsa algorithm takes account link contention communication routing strategy pe list constructed breadthfirst order pe highest degree pivot pe algorithm constructs schedule incrementally first injecting nodes pivot pe tries improve start time node migrating adjacent pes pivot pe migration improve start time node node migrated another pe successors also moved next pe pe list selected new pivot pe process repeated pes pe list considered complexity bsa algorithm op 2 en p number tpes n number nodes e number edges graph pbsa algorithm parallelizes bsa algorithm horizontal scheme nodes graph sorted topological order partitioned p equal sized blocks partition graph scheduled target system independently pbsa algorithm resolves dependencies nodes partitions calculating estimated start time parent node belonging another partition called remote parent node rpn time estimated earliest possible start time latest possible start time partitions scheduled independently developed schedules concatenated complexity pbsa algorithm op 2 enp 2 p number ppes 4 vpmcp algorithm mcp list scheduling algorithm list scheduling nodes ordered list according priority node front list always scheduled first scheduling node depends nodes scheduled node therefore basically sequential algorithm heavy dependences make parallelization mcp difficult mcp nodes must scheduled one one however scheduling node start times node different tpes calculated simultaneously parallelism exploited vertical scheme multiple nodes scheduled simultaneously resultant schedule may one produced mcp scheduling length vary general longer produced sequential mcp exploiting parallelism may lower scheduling quality study degree quality degradation increasing parallelism vertical scheme multiple nodes may selected scheduled time way parallelism increased overhead reduced horizontal scheme different partitions must scheduled simultaneously parallel execution therefore resultant schedule cannot sequential one call vertical version parallel mcp vpmcp algorithm horizontal version hpmcp algorithm vpmcp algorithm described section hpmcp algorithm presented next section describing vpmcp algorithm present simple parallel version mcp algorithm version schedules one node time produces schedule sequential mcp algorithm ppe maintains schedules one tpes therefore vertical scheme call algorithm vpmcp1 shown figure 4 nodes first sorted alap time cyclicly divided p partitions nodes places sorted list assigned ppe nodes scheduled one one node broadcast ppes along parent information including scheduled tpe number time start times node different tpes calculated parallel node scheduled tpe allows earliest start time consequently ppe node child newly scheduled node corresponding parent information node updated 1 compute alap time node sort node list increasing alap order ties broken randomly b divide node list cyclic partitioning equal sized partitions partition assigned ppe 2 ppe first node list broadcasts node along parent information ppes b ppe obtains start time node tpes earliest start time obtained parallel reduction minimum c node scheduled tpe allows earliest start time parent information children scheduled node updated delete node list repeat step list empty figure 4 vpmcp1 algorithm vpmcp1 algorithm parallelizes mcp algorithm directly produces exactly schedules mcp however since time one node scheduled parallelism limited granularity fine solve problem number nodes could scheduled simultaneously increase granularity reduce communication nodes scheduled simultaneously may conflict conflict may result degradation scheduling quality following lemma states condition allows nodes scheduled parallel without reducing scheduling quality lemma 1 node scheduled earliest start time without conflicting former nodes scheduled place would scheduled sequential mcp proof mcp scheduling sequence node obtains earliest start time former nodes list scheduled set nodes scheduled parallel node obtains earliest start time independently node may obtain earliest start time earlier one mcp scheduling former nodes set scheduled case must conflict one former nodes therefore node scheduled place conflict former nodes obtains earliest start time scheduled place would mcp scheduling sequence 2 lemma set nodes obtain earliest start time simultaneously scheduled accordingly node conflicts former nodes node rest nodes scheduled obtain new earliest start times way one node scheduled time however many nodes may earliest start time tpe therefore could many conflicts cases one two nodes scheduled time increase number nodes scheduled parallel may allow conflict node scheduled suboptimal place therefore node found conflict former nodes scheduled next nonconflict place strategy p nodes scheduled time p number tpes use strategy vertical version parallel mcp called vpmcp algorithm details algorithm shown figure 5 besides sorted node list ready list constructed sorted alap times set nodes selected ready list broadcast ppes start time node calculated independently start times made available every ppe another parallel concatenation nodes may compete time slot tpe conflict resolved smallestalaptimefirst rule node get best time slot try second best place scheduled nonconflict place time calculation alap time sorting oen log n parallel scheduling step 2 p therefore complexity vpmcp algorithm oen log nn 2 p n number nodes e number edges p number ppes number communications 2n vpmcp1 2np vpmcp p number tpes vpmcp algorithm compared vpmcp1 table workload testing consists random graphs various sizes use random graph generator 1 three values communicationcomputationratio ccr selected 01 1 10 weights nodes edges generated randomly average value ccr corresponded 01 1 10 set graphs used subsequent experiment section use set graphs 2000 nodes graph scheduled four tpes vertical scheme number ppes cannot larger number tpes 1 compute alap time node sort node list increasing alap order ties broken randomly b divide node list cyclic partitioning equal sized partitions partition assigned ppe initialize ready list consisting nodes parent sort list increasing alap order 2 first p nodes ready list broadcast ppes using parallel concatenation operation along parent information less p nodes ready list broadcast entire ready list b ppe obtains start time node tpes start times node made available every ppe parallel concatenation c node scheduled tpe allows earliest start time one node competes time slot tpe node smaller alap time gets time slot node get time slot scheduled time slot allows second earliest start time ans parent information updated children scheduled node delete nodes ready list update ready list adding nodes freed nodes repeat step ready list empty figure 5 vpmcp algorithm table comparison vertical strategies number scheduling length running time second ccr ppes vpmcp1 vpmcp vpmcp1 vpmcp tpe must maintained single ppe seen table vpmcp1 produces better scheduling quality however heavy communication results low speedup speedup vpmcp reduces running times still provides acceptable scheduling quality scheduling lengths 03 12 longer produced vpmcp1 5 hpmcp algorithm horizontal scheme different partitions must scheduled simultaneously parallel exe cution call horizontal version parallel mcp hpmcp algorithm shown figure 6 1 compute alap time node sort node list increasing alap order ties broken randomly b partition node list equal sized blocks partition assigned ppe 2 ppe applies mcp algorithm partition produce subschedule edges node rpns ignored 3 concatenate pair adjacent subschedules walk schedule determine actual start time node figure hpmcp algorithm hpmcp algorithm nodes first sorted alap time therefore node list topological order partitioned p equal sized blocks assigned p ppes way graph partitioned horizontally graph partitioned ppe schedule partition produce subschedule subschedules concatenated form final schedule three problems addressed scheduling concatenation information estimation concatenation permuta tion postinsertion information estimation major problem horizontal scheme resolve dependences par titions general latter partition depends former partitions schedule partitions parallel ppe needs schedule information former partitions since impossible obtain information schedules former partitions produced estimation necessary although latter ppe know exact schedules former partitions estimation help node determine earliest start time latter partition pbsa algorithm 1 start time rpn remote parent node estimated done calculating two parameters earliest possible start time epst latest possible start time lpst epst node largest sum computation times start point node excluding node lpst node sum computation times nodes scheduled node excluding node estimated start time est rpn defined ffepst equal 1 rpn critical path otherwise equal length longest path start point rpn end point divided length critical path given estimated start time rpn still necessary estimate tpe rpn scheduled rpn critical path node assumed scheduled tpe highest level critical path node local partition otherwise tpe randomly picked one rpn scheduled call estimation pbsa estimation estimation necessarily accurate better simpler estimation used hpmcp hpmcp simply ignore dependences partitions therefore entry nodes partition start time furthermore assume schedules former pe end time call estimation hpmcp estimation table ii comparison estimation algorithms number scheduling length running time second ccr ppes hpmcp est pbsa est hpmcp est pbsa est compare two approaches estimation section number nodes graph 2000 graph scheduled four tpes comparison shown table ii column hpmcp est shows performance hpmcp column pbsa est shows performance hpmcp pbsa estimation scheduling lengths running times compared running time pbsa estimation longer notice ppes produce longer scheduling lengths shows tradeoff scheduling quality parallelism hand superlinear speedup observed due graph partitioning scheduling length produced pbsa estimation always longer produced hpmcp estimation implies complex estimation algorithm cannot promise good scheduling simpler algorithm may better however say use simplest one future still possible find good estimation improve performance simple estimation used hpmcp sets baseline future estimation algorithms concatenation permutation ppe produces subschedule final schedule constructed concatenating subschedules accurate information former subschedules easy determine optimal permutation tpes adjacent subschedules determine latter tpe concatenated former tpe hueristics necessary pbsa algorithm tpe earliest node concatenated tpe former subschedule allows earliest execution tpes concatenated tpes former subschedule breadthfirst order 1 hpmcp algorithm assume start time node within partition therefore algorithm cannot applied simply perform permutation tpes hpmcp tpe latter subschedule concatenated tpe former sub schedule alternative hueristics described follows ppe finds within subschedule tpe criticalpath nodes permutes tpe tpe 0 permutation many critical path nodes possible scheduled tpe critical path length could reduced permutation algorithm compared non permutation algorithm table iii time spent permutation step causes algorithm slower since extra time spent determine weather node critical path terms scheduling length permutation algorithm makes four test cases better nonpermutation algorithm two cases worse permutation algorithm improve performance much therefore permutation performed hpmcp algorithm postinsertion finally walk entire concatenated schedule determine actual start time node refinement performed step horizontal scheme latter ppe able insert nodes former subschedules due lack information leads table iii comparison permutation algorithms number scheduling length running time second ccr ppes permut permut permut permut performance loss partially corrected concatenation time inserting nodes latter subschedule former subschedules improvement postinsertion algorithm shown table iii compared noninsertion postinsertion algorithm reduces scheduling length eight test cases increases four cases overall postinsertion algorithm improve scheduling quality however spends much time postinsertion following perform postinsertion time calculation alap time sorting oe n second step parallel scheduling 2 p 2 third step spends oen 2 p 2 time postinsertion oe time noninsertion therefore complexity hpmcp algorithm without postinsertion oe number nodes e number edges graph p number tpes p number ppes 6 performance vpmcp hpmcp algorithms implemented intel paragon present performance three measures scheduling length running time speedup table iv comparison postinsertion algorithms number scheduling length running time second ccr ppes insert insert insert insert first performance vpmcp algorithm presented tables v vi graphs 1000 2000 3000 4000 nodes scheduled four tpes scheduling length provides measure scheduling quality results shown table v scheduling lengths ratios scheduling lengths produced vpmcp algorithm scheduling lengths produced mcp algorithm ratio obtained running vpmcp algorithm 2 4 ppes paragon taking ratios scheduling lengths produced mcp running one ppe one see table almost effect graph size scheduling quality cases scheduling lengths vpmcp 1 longer produced mcp running time speedup vpmcp algorithm shown table vi speedup defined sequential execution time optimal sequential algorithm p parallel execution time running times vpmcp one ppe compared mcp running time one ppe mcp running time single processor sequential version without parallelization overhead low speedup vpmcp caused large number communications next experiment study vpmcp performance different numbers tpes tables vii viii show scheduling lengths running times graphs 4000 nodes 2 4 table v scheduling lengths produced vpmcp ratios mcp graph size number nodes ccr number 1000 2000 3000 4000 ppes length ratio length ratio length ratio length ratio table vi running time second speedup vpmcp graph size number nodes ccr number 1000 2000 3000 4000 ppes time time time time 8 16 tpes difference scheduling lengths vpmcp mcp within 1 cases noticed table number tpes increases scheduling lengths decrease ratio increases slightly therefore scheduling quality scales quit well higher speedups obtained ppes next study performance hpmcp algorithm number tpes four tables ix x table ix ratio obtained running hpmcp algorithm 2 4 8 16 ppes paragon taking ratios scheduling lengths produced mcp running one ppe deterioration performance hpmcp due estimation start time rpns concatenation 48 test cases shown table one case hpmcp performed 10 worse mcp 31 table vii scheduling lengths ratios different number tpes produced vpmcp number tpes ccr number 2 ppes length ratio length ratio length ratio length ratio table viii vpmcp running time second speedup different number tpes number tpes ccr number 2 ppes time time time time within 1 16 1 10 interest two cases hpmcp produced even better results mcp since mcp heuristic algorithm possible sometimes parallel version could produce better result corresponding sequential one running time speedup hpmcp algorithm shown table x superlinear speedup cases table hpmcp lower complexity mcp complexity mcp 2 complexity hpmcp p ppes oen log nn 2 p 2 therefore speedup bounded p 2 instead p speedup 16 ppes good expected graph size large enough relative overhead large tables xi xii show hpmcp scheduling lengths running times graphs 4000 nodes 2 4 8 16 tpes speedups decrease number tpes caused increasing dependences tpes compare performance vpmcp hpmcp performance shown figures 7 8 graphs 4000 nodes number tpes number ppes figure 7 shows percentage scheduling length produced mcp negative numbers indicate scheduling lengths shorter produced mcp 2 4 ppes hpmcp produces shorter scheduling lengths however ppes vpmcp produces better scheduling quality produced hpmcp general vpmcp provides stable scheduling quality figure 8 compares speedups vpmcp hpmcp algorithms hpmcp faster vpmcp higher speedup scheduling nodes ppes executed horizontal scheme major communication step necessary move nodes however number ppes equal number tpes vertical scheme avoid communication step nodes reside ppes executed becomes important scheduling algorithms used runtime next compare two algorithms horizontal scheme hpmcp pbsa pbsa algorithm takes account link contention communication routing strategy hpmcp consider factors therefore edge weights pbsa vary different topologies whereas hpmcp constant comparison purposes implemented simplified version pbsa assumes edge weights constant pbsa much slower hpmcp complexity much higher complexity pbsa op 2 enp 2 hpmcp oe 50 120 times faster pbsa set graphs compare scheduling lengths produced hpmcp pbsa results shown table xiii table scheduling lengths produced sequential mcp bsa algorithms running single processor also compared table ix scheduling lengths produced hpmcp ratios mcp graph size number nodes ccr number 1000 2000 3000 4000 ppes length ratio length ratio length ratio length ratio table x running time second speedup hpmcp graph size number nodes table xi scheduling lengths ratios different number tpes produced hpmcp number tpes ccr number 2 ppes length ratio length ratio length ratio length ratio table xii hpmcp running time second speedup different number tpes number tpes 40 45 50 number ppes 05 10 15 20 25 30 35 40 45 number ppes 30 20 10 10 20 30 40 number ppes figure 7 comparison scheduling quality produced vpmcp hpmcp number ppes number ppes number ppes figure 8 comparison speedups vpmcp hpmcp ccr 01 1 scheduling lengths produced mcp slightly shorter produced bsa ccr 10 mcp much better bsa parallel versions two algorithms perform differently number ppes increases scheduling lengths produced hpmcp increases slightly pbsa increases significantly figure 9 compares hpmcp pbsa sum scheduling lengths four graphs 1000 2000 3000 4000 nodes different ccrs different number ppes table xiii scheduling lengths produced hpmcp pbsa graph size number nodes ccr number 1000 2000 3000 4000 ppes hpmcp pbsa hpmcp pbsa hpmcp pbsa hpmcp pbsa 7 concluding remarks parallel scheduling faster able schedule large macro dataflow graphs parallel scheduling new approach still development many open problems need solved highquality parallel scheduling algorithms low complexity developed achieved parallelizing existing sequential scheduling algorithms designing new parallel scheduling algorithms developed vpmcp hpmcp algorithms parallelizing sequential mcp algorithm performance approach studied vpmcp hpmcp algorithms much faster pbsa produce highquality scheduling terms scheduling length 300000 scheduling length number ppes pbsa 300000 scheduling length number ppes pbsa 300000 500000 scheduling length number ppes pbsa figure 9 comparison scheduling lengths produced hpmcp pbsa acknowledgments grateful yukwong kwok ishfaq ahmad providing pbsa program random graph generator testing r parallel approach multiprocessor scheduling performance comparison algorithms static scheduling dags multiprocessors last algorithm heuristicsbased static task allocation algorithm applications performance analysis compiletime optimization approach list scheduling algorithms distributed memory multiprocessors scheduling parallel program tasks onto arbitrary target machines task scheduling parallel distributed systems computers intractability guide theory npcompleteness scheduling precedence graphs systems interprocessor communication times comparison multiprocessor scheduling heuristics duplication scheduling heuristics dsh new precedence task scheduler parallel processor systems partitioning scheduling parallel programs multiprocessors compiletime scheduling heuristic interconnectionconstrained heterogeneous processor architectures programming aid messagepassing systems dsc scheduling parallel tasks unbounded number processors tr ctr sukanya suranauwarat hideo taniguchi design implementation initial evaluation advanced knowledgebased process scheduler acm sigops operating systems review v35 n4 p6181 october 2001