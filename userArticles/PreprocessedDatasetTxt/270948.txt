viennafortranhpf extensions sparse irregular problems compilation abstractvienna fortran high performance fortran hpf data parallel languages introduced allow programming massively parallel distributedmemory machines dmmp relatively high level abstraction based spmd paradigm main features include directives express distribution data computations across processors machine paper use viennafortran general framework dealing sparse data structures describe new methods representation distribution data dmmps propose simple language features permit user characterize matrix sparse specify associated representation together data distribution matrix enables compiler runtime system translate sequential sparse code explicitly parallel messagepassing code develop new compilation runtime techniques focus achieving storage economy reducing communication overhead target program overall result powerful mechanism dealing efficiently sparse matrices data parallel languages compilers dmmps b introduction past years significant efforts undertaken academia government laboratories industry define highlevel extensions standard programming languages particular fortran facilitate data parallel programming wide range parallel architectures without sacrificing performance important results work vienna fortran 10 28 fortran 15 high performance fortran hpf 18 intended become defacto standard languages extend fortran 77 fortran 90 directives specifying alignment distribution programs data among processors thus enabling programmer influence locality computation whilst retaining single thread control global name space lowlevel task mapping computation target processors framework singleprogrammultipledata spmd model inserting communication nonlocal accesses left compiler hpf1 original version high performance fortran focussed attention regular com putations providing set basic distributions block cyclic replication although approved extensions hpf2 include facilities expressing irregular distributions using indirect special support sparse data structures proposed paper consider specific requirements sparse computations arise variety problem areas molecular dynamics matrix decompositions solution linear systems image reconstruction many others order parallelize sequential sparse codes effectively three fundamental issues must addressed 1 must distribute data structures typically used codes 2 necessary generalize representation sparse matrices single processor distributedmemory machines way savings memory computation also achieved parallel code 3 compiler must able adapt global computation local computation processor resolving additional complexity sparse methods introduce paper presents approach solve three problems first new data type introduced vienna fortran language representing sparse matrices data distributions explicitly designed map data type onto processors way exploit locality sparse computations preserve compact representation matrices vectors thereby obtaining efficient workload balance minimizing communication experiments parallelizing sparse codes hand 2 confirmed suitability distributions also excessive amount time spent development debugging stages manual parallelization encouraged us build compiler specify algorithms highlevel dataparallel language way new elements introduced vienna fortran extend functionality expressivity irregular problems subsequently compiler runtime techniques developed enable specific optimizations handle typical features sparse code including indirect array accesses appearance array elements loop bounds result powerful mechanism storing manipulating sparse matrices used dataparallel compiler generate efficient spmd programs irregular codes kind paper assume representation distribution sparse data invariant however fact representation sparse data computed runtime simplifies additional support handling complex features dynamic redistribution matrix fillin ie runtime insertion additional nonzero elements sparse matrix rest paper organized follows section 2 introduces basic formalism background handling sparse matrices section 3 presents several data distributions sparse problems section 4 describes new directives specification distributions vienna fortran language section 5 6 respectively outline runtime support compilation technology required implementation features sections 7 8 present experimental results finish sections 9 10 discussion related work conclusions representing sparse matrices distributedmemory machines matrix called sparse small number elements nonzero range methods developed enable sparse computations performed considerable savings terms memory computation 16 solution schemes often optimized take advantage structure within matrix consequences parallelization firstly want retain much savings possible parallel code secondly order achieve good load balance runtime necessary understand achieved terms data structures occur sparse problem formulations section discuss methods representing sparse matrices distributedmemory machines assume reader familiar basic distribution functions vienna fortran hpf 10 28 18 namely block cyclick throughout paper denote set target processors procs assume data distributed twodimensional mesh procs x processors numbered 0 dimension specifically assume vienna fortran hpf code include following declaration note abstract processor declaration imply specific topology actual processor interconnection network 21 basic notation terminology array associated index domain denote replicationfree distribution total function maps array element processor becomes owner element capacity stores element local memory processor p 2 procs denote p set elements local p called local segment p following assume twodimensional real array representing sparse matrix declared index domain notation introduced related without explicitly reflecting dependence begin defining set auxiliary functions definition 1 1 symbolic matrix associated total predicate ftrue falseg 2 2 ff j fiji 2 ig j specifies number matrix elements nonzero value bijective enumeration numbers elements consecutively order starting 1 4 assume enumeration selected total function tth index associated nonzero element default use enumeration following numbers elements row wise ie assume sparse matrix mapped distributedmemory machine approach require two kinds information specified user 1 representation sparse matrix single processor called sparse format 2 distribution matrix across processors machine context concept distribution used matrix dense combination sparse format distribution called distributed sparse representation matrix 22 sparse formats discuss data distribution strategies sparse data must understand data usually represented single processor numerous storage formats proposed sparsematrix literature work used commonly used crs compressed row storage format approach extended ccs compress column storage swapping rows columns text following simplicity consider sparse matrices real elements immediately generalized include element types logical integer complex definition 2 compressed row storage crs sparse format determined triple functions dacoro total data function defined dat 1 denotes set real numbers 2 co total column function defined cot t2 total row function defined follows let denote arbitrary row number roi least one specified property exists otherwise roi roi 1 three functions represented obvious way vectors ff real numbers data vector ff column numbers column vector numbers range row vector respectively see figure 1b data vector stores nonzero values matrix traversed rowwise fashion column vector stores column indices elements data vector finally row vector stores indices data vector correspond first nonzero element row element exists storage savings achieved approach usually significant instead storing n elements need locations sparse matrix algorithms designed crs format typically use nested loop outer loop iterating rows matrix inner loop iterating nonzeros row see examples section 4 matrix elements identified using twodimensional index set say ijj denotes ith row matrix jj denotes jjth nonzero row matrix element referred ijj one row number r column number coroijj nonzero value stored daroijj heavy use indirect accesses sparse representations require introduces major source complexity inefficiency parallelizing codes distributedmemory machines number optimizations presented later overcome 3 distributed sparse representations let denote sparse matrix discussed ffi associated distribution distributed sparse representation results combining ffi sparse format understood follows distribution ffi interpreted conventional sense ie 2 pair z x numbers dense fortran array ffi determines locality function p 2 procs specifies local segment p p sparse matrix distributed sparse representation obtained constructing representation elements p based given sparse format da co ro automatically converted sets vectors da p co p ro p p 2 procs hence parallel code save computation storage using mechanisms applied original program sparse format use crs illustrate ideas data distributions introduce two different schemes subsequent sections decomposing sparse global domain many sparse local domains required 31 multiple recursive decomposition mrd common approaches partitioning unstructured meshes keeping neighborhood properties based upon coordinate bisection graph bisection spectral bisection 8 19 spectral bisection minimizes communication requires huge tables store boundaries local region expensive algorithm compute graph bisection algorithmically less expen sive also requires large data structures coordinate bisection significantly tends reduce time compute partition expense slight increase communication time binary recursive decomposition brd proposed berger bokhari 4 belongs last categories brd specifies distribution algorithm sparse matrix recursively bisected alternating vertical horizontal partitioning steps many submatrices processors submatrix mapped unique processor flexible variant algorithm produces partitions shapes individual rectangles optimized respect userdetermined function 7 section define multiple recursive decomposition mrd generalization brd method also improves communication structure code assume processor array declared prime factor decomposition x ordered way prime factors x sorted descending order come first followed factors sorted fashion mrd distribution method produces x partition matrix k steps recursively performing horizontal divisions matrix prime factors x vertical ones prime factors matrix partitioned p 1 submatrices way nonzero elements spread across submatrices evenly possible submatrix partitioned horizontally rows nonzero entries uniquely assigned either partition included lower one vertical step columns assigned right partition submatrix resulting step i1 partitioned p submatrices using criteria process terminates created q k submatrices enumerate consecutively 0 x using horizontal ordering scheme submatrix number r mapped processor procsrs 2 distribution defines local segment processor rectangular matrix preserves neighborhood properties achieves good load balance see figure 2 fact perform horizontal partitioning steps vertical ones reduces number possible neighbors submatrix may hence simplifies analysis performed compiler runtime system combined crs representation local segments mrd distribution produces mrdcrs distributed sparse representation inmediately generalized storage formats however since use crs illustrate ideas refer mrdcrs mrd 32 brs distributed sparse representation second strategy based cyclic distribution see figure 4a retain locality access regular case suitable workload spread evenly across matrix presents periodicity density matrix varies time many common algorithms nature including sparse matrix decompositions lu cholesky qr wz image reconstruction algorithms section assume dimensions 0 distributed cyclically block length 1 see figure 4b several variants representation distribution segment context described literature including mm ess bbs methods 1 consider crs sparse format results brs block row scatter distributed sparse representation similar distributed representation bcs block column scatter 26 sparse format compressed columns changing rows columns vice versa mapping established brs choice requires complex auxiliary structures translation schemes within compiler however data used together cyclicallydistributed dense arrays structures properly aligned leading savings communication extensions support sparse matrix computation 41 language considerations section proposes new language features specification sparse data data parallel language clearly block cyclic distributions offered hpf1 adequate purpose hand indirect distributions 15 28 included approved extensions hpf2 allow specification structure inherent distributed sparse representations thus introduce unnecessary complexity memory consumption execution time proposal makes structure explicit appropriate new language elements seen providing special syntax important special case userdefined distribution function defined vienna fortran hpf 11 12 new language features provide following information compiler runtime system ffl name index domain element type sparse matrix declared done using regular fortran declaration syntax array actually appear original code since represented set arrays name introduced referred specifying distribution ffl annotation specified declares array sparse provides information representation array includes names auxiliary vectors order data column row declared explicitly program sizes determined implicitly matrix index domain ffl dynamic attribute used manner analogous meaning vienna fortran hpf specified distributed sparse representation determined dynamically result executing distribute statement otherwise components distributed sparse representation constructed time declaration processed often information contained file whose name indicated annotation addition input sparse matrix available compiletime must read file standard format distributed runtime name file may provided compiler additional directive concrete examples typical sparse codes illustrating details syntax well hpf given figures 5 6 42 solution sparse linear system wide range techniques solve linear systems among iterative methods use successive approximations obtain accurated solutions step conjugate gradient cg 3 oldest best known effective nonstationary iterative methods symmetric positive definite systems convergence process speeded using preconditionator computing cg include figure 5 dataparallel code unpreconditioned cg algorithm involves one matrixvector product three vector updates two inner products per iteration input coefficient matrix vector scalars b also initial estimation must computed xvec solution vector elements initial residuals r defined every iteration two inner products performed order update scalars defined make sequences fulfill certain orthogonality conditions end iteration solution residual vectors updated 43 lanczos algorithm figure 6 illustrates algorithm extended hpf tridiagonalization matrix lanczos method 24 use new directive indicated nsd specify required declarative information execution distribute directive results computation distributed sparse representation point matrix legally accessed program several matrixvector vectorvector operations performed compute diagonals output matrix 5 runtime analysis based language extensions introduced section shows access sparse data efficiently translated vienna fortran hpf explicitly parallel message passing code context data parallel spmd paradigm rest paper assume input matrix available compiletime assumption matrix distribution postponed runtime obviously enforces global local index translation also performed runtime parallelize codes use indirect addressing compilers typically use inspectorexecutor strategy 22 loop accessing distributed variables tranformed inserting additional preprocessing loop called inspector inspector translates global addresses accessed indirection processor offset tuple describing location element computes communication schedule executor stage uses preprocessed information fetch nonlocal elements access distributed data using translated addresses obvious penalty using inspectorexecutor paradigm runtime overhead introduced inspector stage become significant multiple levels indirection used access distributed arrays seen frequently case sparsematrix algorithms using compact storage formats crs example xvecdaroijj reference encountered figure 5 requires three preprocessing steps one access distributed array ro second access da yet third access xvec pay special attention issue section outline efficient solution parallelization 51 sar approach though based inspectorexecutor paradigm solution translating crs like sparse indices runtime within dataparallel compilers significantly reduces time memory overhead compared standard generalpurpose chaos library 23 technique called sparse array rolling sar encapsulates small descriptor information input matrix distributed across processors allows us determine processor offset location sparse matrix element without plod distributed auxiliary array datastructures thus saving preprocessing time required intermediate arrays figure 7 provides overview sar solution approach distribution matrix represented crs format carried partitioner routine responsible computing domain decomposition giving output distributed representation well associated descriptor descriptor indexed translation process using row number nonzero index x locate processor offset matrix element stored element found nonlocal dereference process assigns address local memory element placed fetched executor stage uses preprocessed information inside couple gatherscatter routines fetch marked nonlocal elements place assigned locations finally loop computation accesses distributed data using translated addresses efficiency translation function memory overheads descriptor largely dependent matrix distributed following sections provide details distributions studied paper 52 mrd descriptor translation mrd distribution maps rectangular portion dense index space n theta onto virtual processor space x theta corresponding descriptor replicated processors consists two parts vector parth stores row numbers x horizontal partitions made two dimensional array partv size n theta keeps track number nonzero elements vertical partition row example 1 mrd distributed matrix figure 3 corresponding descriptor replicated among processors following parth18 denotes horizontal partition made row 8 row two vertical partitions values partv912 23 say first section row 9 two nonzero elements second section one 3 assume parth01 parthxn1 partvk00 1 k n given nonzero element identified ijj perform translation means descriptor determine processor owns nonzero element assuming processors identified position myr myc x theta virtual processor mesh values myr myc processor owns element satisfies following inequalities searching right myr myc satisfies inequalities require search space size x theta search optimized first checking see element local plugging local processors values myr myc assuming high degree locality check frequently succeeds immediately fails binary search mechanism employed offset element located xvecpartvimyc thus column number element ijj found coxvecpartvimyc processor myr myc nonzero value accessed daxvecpartvimyc processor without requiring communication additional preprocessing steps 53 brs descriptor translation unlike mrd brs descriptor different processor processor myrmyc elements nx rows mapped onto brs descriptor stores local row matrix entry every nonzero element row regardless whether element mapped locally elements local entry stores local index da nonlocal elements entry stores global column number element original matrix distinguish local entries nonlocal entries swap sign local indices become negative actual datastructure used crslike twovector representation vector called cs stores entries elements mapped local rows another vector ra stores indices row starts cs example 2 sparse matrix partitioning showed figure 4 values cs ra processor 00 following cs12 says element 53 stored global column 2 nonlocal cs21 signifies element 19 mapped locally stored local index 1 remaining entries similar interpretations processor owning element rx identified follows first local row identified using simple formula x entry element obtained using csrarjj negative implies element local accessed dam positive global row column number element implies processor owning element save ijj indices list indices marked later retrieval processor q executor gather routine send ijj indices q similar translation process repeated time however element locally found sent requesting processor 6 compilation section describes compiler implementation within vienna fortran compilation system vfcs input compiler viennafortran code extended sparse annotations described section 4 compilation process results fortran 77 code enhanced messagepassing routines well runtime support already discussed previous section tool structured set modules shown figure 8 describe module separately 61 frontend first module part tool interacts declaration part program responsible 1 scanning parsing new language elements presented section 4 operations generate abstract syntax tree annotations table summarizing compiletime information extracted table built sparse directives needed compiler proceeds remove code 2 insertion declarations local vectors auxiliary variables target code runtime support utilize 62 parallelizer stage compiler first scans code searching sparse references extracting information available compiletime ie indirections syntax indices loops conditionals inside reference etcetera information organized database later lookup parallelization process done loop decomposition starts goal consists distributing workload source code evenly possible among processors task turns particularly complex compiler handling sparse codes mainly frequent use indirections accessing sparse data frequent use sparse references loop bounds cases multiple queries distributed sparse data required processors order determine iteration space leading large number communications overcome problem address problem different way rather trying access actual sparse values requested loop headers apply loop transformations determine local iteration space also map values semantically equivalent information local distribution descriptor approach double advantage reusing compiler auxiliary structures ensuring locality accesses performed loop boundaries result much faster mechanism accessing data extra memory overhead mrd case example arrays parth partv determine local region data sparse matrix based global coordinates way loop partitioning driven similar strategies block difference regions different size similar workload determined runtime descriptor generated runtime support brs case solution straightforward let us take example conjugate gradient cg algorithm figure 5 dense vectors distributed dense cyclic sparse matrix follows brs scheme note cg loops refer dense structures decomposition performed enforcing stride loops number processors data dimension traversed loop distributed consecutive local data cyclic always separated constant distance terms global coordinates however references sparse vectors included loops fact true first matrix dimension second one actual sparsity degree matrix determines distance consecutive data terms global columns since becomes unpredictable compiletime recall assumption sparse matrix pattern available runtime runtime checking defined function brs distribution descriptor needs inserted loops traversing second matrix dimension successfully parallelized checking eventually moved inspector phase executor computed number iterations thus decreasing overall runtime overhead see transformation final code generation figure 10 figure 9 provides code excerpt outlines loop decomposition performed within vfcs two sparse loops figure 5 ra cs vectors brs descriptor processor coordinates myr myc ra stores indices way local ro considering elements placed global rows theta x myr given local row cycliclike approach followed extract local iterations first loop ra traverses elements second loop cs delimits local iterations subsequent note different criteria followed parallelizing loops first loop wellknown owners compute rule applied second loop though underlying idea avoid replication computation first calculating local partial sum given local elements accumulate values single reduction phase way computations distributed based owner every single da p value given index k makes match always processor achieves complete locality 63 backend workload assigned processor compiler enters last stage whose output target spmd code reach goal code transformed inspector executor phases loops figure shows final spmd code sparse loops parallelized figure 9 overall next sequence steps carried compiler module 1 inspector loop inserted prior loop computation header loop obtained syntax tree parallelization statements inside loop generated collect indices distributed arrays auxiliary vectors vectors taken input translation process 2 calls translate dereference scattergather routines placed inspector executor loops complete runtime job 3 references distributed variables executor loop sintactically changed indexed translation functions produced output inspector see functions f g figure 10 4 additional io routines must inserted beginning execution part merge processor local data descriptors sar scheme done partitioner routine 7 evaluation distribution methods choice distribution strategy matrix crucial determining performance controls data locality load balance executor preprocessing costs inspector memory overhead runtime support section discuss brs mrd distributions affect aspects particular case sparse loops conjugate gradient algorithm account effects different sparsity structures chose two different matrices coming harwellboeing collection 14 identified psmigr1 bcsstk29 former contains population migration data relatively dense whereas latter sparse matrix used large eigenvalue problems matrix characteristics summarized table 1 71 communication volume executor table 2 shows communication volume executor 16 processors 4 theta 4 processors mesh computing sparse loops cg algorithm communication necessary accumulating local partial products array q operation implemented like typical reduction operation local matrix rows processor rows note two things first relation communication volume processor mesh configuration second balance communication pattern note comparisons communication volumes across two matrices relative number rows general x theta processor mesh n theta sparse matrix communication volume roughly proportional nx theta logy thus 8 theta 2 processor mesh 4 times less total communication volume 4 theta 4 mesh brs processor accumulates exactly amount data mrd minor imbalances stemming slightly different sizes horizontal partitions see figure 11 communication time executor showed black figure 13 72 loop partitioning workload balance explained section 62 iteration sparse loops conjugate gradient algorithm mapped owner da element accessed iteration results perfect workload balance mrd case since processor owns equal number nonzeros brs workload balance relies random positioning elements except pathological cases results good load balance table 3 shows load balance index brs maximum variation average divided average 73 memory overhead vectors storing local submatrix processor require similar amounts memory distributions however distribution descriptor used runtime support require substantially different amounts memory table 4 summarizes requirements first row indicates expected memory overhead next two rows show actual overhead terms number integers required overhead column represents memory overhead percentage amount memory required store local submatrix vectors partv cs responsible overhead distribution since keep track positions nonzero elements mrd brs respectively overhead much higher brs cs vector stores column numbers even offprocessor nonzeros length vector reduced using processor meshes 8 runtime evaluation section describes performance evaluation sparse loops conjugate gradient algorithm parallelized using vfcs compiler brs mrd especifications intent study effect distribution choice inspector executor performance within dataparallel compiler finally manual version application used baseline determine overhead semiautomatic parallelization platform intel paragon using nxlib communication library experi ments account io time read matrix perform distribution 81 inspector cost figure 12 shows preprocessing costs sparse loops mrd brs versions cg algorithm two matrices preprocessing overheads reduce increasing parallelism though efficiencies drop high end also note brs incurs higher preprocessing overheads mrd also scales better understand relative costs brs relative mrd recall brs translation mechanism involves preprocessing nonzeros local rows mrd dereferencing requires binary search distribution descriptor local nonzeros though processes fewer elements size mrd search space proportional size processor mesh processors added translation requires search larger space though shown table measurements indicate brs inspector actually faster mrd 64 processors 82 executor time since schemes distribute nonzeros equally across processors found computational section executor scaled well distributions 32 processors communication overheads start reduce efficiency figure 13 shows executor time sparse loops two cg versions indicates good load balance fact find cases superlinear speedup attributable cache effects executor communication time shown dark figure 13 brs communication overhead remains essentially invariant across processor sizes suggests overhead extra communication startups offset reduced communication volume maintaining total overhead mrd communication much unbalanced leads much poorer scaling communication costs indeed effect particularly apparent bcsstk29 redistribution extremely unbalanced becomes severe bottleneck processor size increased 83 comparison manual parallelization efficiency sparse code parallelized within vfcs compiler depends largely primary factors ffl distribution scheme selected parallelization either mrd brs ffl sparsity rate input matrix ffl cost inspector phase figure access pattern hand seen parallelization sparse loops cg algorithm within vfcs leads target code executor perform communication gatherscatter routines consequence full locality achieved data distribution local representation loop partitioning strategy apart actual computation executor contains communication accumulating local partial products implemented reduction routine exactly programmer would thus executor time becomes accurated estimation efficiency smart programmer attain additional cost using automatic compilation lies intirely preprocessing time inspector loops plus subsequents runtime calls figure 10 figure 14 tries explain impact major factors influence parallel efficiency providing comparison manual compilerdriven parallelization execution times compiler include cost single inspector plus executor per iteration whereas manual version inspector required far distribution concerned figure 14 shows brs introduces bigger overhead direct consequence expensive inspector slower global local translation process however even brs case overall results quite efficient number iterations practice convergence cg algorithm starts exhibit stationary behaviour less one hundred iterations time inspector cost already widely amortized total compiler overhead always kept 10 regardless input matrix distribution chosen number processors parallel machine respect matrix sparsity conclude higher degree sparsity matrix better result produced compiler compared manual version overall comparison manual parallelization also reflects good scalability manual gain small number iterations summarizing say cost paid automatic parallelization worthwhile long algorithm amortize inspector costs minimum number iterations remaining cost conjugate gradient algorithm lies multiple loops dealing dense arrays distributed cyclic however computational weight part never goes 10 total execution time even though compiler efficiency expected improved cases influence minimum lead significant variation full algorithm additional experiments demonstrate efficiency schemes developed trenas 24 implemented manual version lanczos algorithm see figure 6 using pvm routines brs scheme 9 related work programs designed carry range sparse algorithms matrix algebra outlined 3 codes require optimizations described paper efficient target code generated parallel system variety languages compilers targeted distributed memory multiprocessors 28 9 15 18 attempt deal loops arise sparse irregular computation one approach originating fortran vienna fortran based indirect data distributions cannot express structure sparse data resulting memory runtime overhead scheme proposed paper provides special syntax special class userdefined data distributions proposed vienna fortran hpf 12 hand area automatic parallelization outstanding tools know parafrase 20 polaris 6 intended framework parallelization sparse algorithms addressed present work methods proposed saltz et al handling irregular problems consists endowing compiler runtime library 23 facilitate search capture data located distributed memory major drawback approach large number messages generated consequence accessing distributed data addressing table associated overhead memory 17 order enable compiler apply optimizations simplify task programmer bik wijshoff 5 implemented restructuring compiler automatically converts programs operating dense matrices sparse code method postpones selection data structure compilation phase though friendly end user approach risk inefficiencies result allowing programmer choose appropriate sparse structures way dealing problem different define heuristics perform efficient mapping data language extension describe mapping data parallel languages 18 28 produced benchmarked prototype compiler integrated vfcs able generate efficient code irregular kernels compiler transformations insert procedures perform runtime optimizations implementation qualitatively different efforts cited number important respects particular respect use new data type sparse format data distributions distributed sparse representations irregular computation basic ideas distributions take account way sparse data accessed map data pseudoregular way compiler may perform number optimizations sparse codes specifically pseudoregularity distributions allows us describe domain decomposition using small descriptor addition accessed locally saves memory overhead distributed tables well communication cost needed lookup general application codes irregular problems normally code segments loops complex access functions advanced analysis technique known slicing analysis 13 deal multiple levels indirection transforming code contains references code contains single level indirection however multiple communication phases still remain sar technique implemented inside sparse compiler novel able handle multiple levels indirection cost single translation key attaining goal consists taking advantage compiletime information semantic relations elements involved indirect accesses conclusions paper sparse data distributions specific language extensions proposed dataparallel languages vienna fortran hpf improve handling sparse irregular computation features enable translation codes use typical sparse coding techniques without necessity rewriting show detail code may translated resulting code retains significant features sequential sparse applications particular savings memory computation typical techniques retained lead high efficiency run time data distributions designed retain data locality appropriate support good load balance avoid memory wastage compile time run time support translates structures permit sparse representation data processors parallel system language extensions required minimal yet sufficient provide compiler additional information needed translation optimization number typical code kernels shown paper 26 demonstrate limited amount effort required port sequential code kind extended hpf vienna fortran results demonstrate data distributions language features proposed supply enough information store access data distributed memory well perform compiler optimizations bring great savings terms memory communication overhead lowlevel support sparse problems described proposing implementation optimizing compiler performs translations compiler improves functionality dataparallel languages irregular computations overcoming major weakness field runtime techniques used context inspectorexecutor paradigm however set lowlevel primitives differ used several existing implementations order take advantage additional semantic information available approach particular runtime analysis able translate multiple indirect array accesses single phase make use expensive translation tables final result optimizing compiler able generate efficient parallel code computations close expected manual parallelization much faster comparison existing tools area r scheduling sparse matrixvector multiplication massively parallel dap computer partitioning strategy nonuniform problems multiprocessors automatic data structure selection transformation sparse matrix computations massively parallel methods engineering science problems vienna fortran compilation system programming vienna fortran user defined mappings vienna tran extending hpf advanced data parallel applications index array flattening program transformations users guide harwellboeing sparse matrix collection fortran language specification computer solution large sparse positive definite sys tems high performance language specification numerical experiences partitioning unstructured meshes structure parafrase2 advanced parallelizing compiler c fortran data distributions sparse matrix vector multiplication solvers parallel algorithms eigenvalues computation sparse matrices efficient resolution sparse indirections dataparallel compilers evaluation parallelization techniques sparse applications vienna fortran language specification version 11 tr ctr chunyuan lin yehching chung jenshiuh liu efficient data compression methods multidimensional sparse array operations based ekmr scheme ieee transactions computers v52 n12 p16401646 december rongguey chang tyngruey chuang jenq kuen lee efficient support parallel sparse computation array intrinsic functions fortran 90 proceedings 12th international conference supercomputing p4552 july 1998 melbourne australia gerardo bandera manuel ujaldn emilio l zapata compile runtime support parallelization sparse matrix updating algorithms journal supercomputing v17 n3 p263276 nov 2000 manuel ujaldon emilio l zapata efficient resolution sparse indirections dataparallel compilers proceedings 9th international conference supercomputing p117126 july 0307 1995 barcelona spain roxane adle marc aiguier franck delaplace toward automatic parallelization sparse matrix computations journal parallel distributed computing v65 n3 p313330 march 2005 v blanco p gonzlez j c cabaleiro b heras f pena j j pombo f f rivera performance prediction parallel iterative solvers journal supercomputing v28 n2 p177191 may 2004 chunyuan lin yehching chung data distribution schemes sparse arrays distributed memory multicomputers journal supercomputing v41 n1 p6387 july 2007 bradford l chamberlain lawrence snyder array language support parallel sparse computation proceedings 15th international conference supercomputing p133145 june 2001 sorrento italy chunyuan lin yehching chung jenshiuh liu efficient data distribution schemes ekmrbased sparse arrays distributed memory multicomputers journal supercomputing v34 n3 p291313 december 2005 garz garca approaches based permutations partitioning sparse matrices multiprocessors journal supercomputing v34 n1 p4161 october 2005 thomas l sterling hans p zima gilgamesh multithreaded processorinmemory architecture petaflops computing proceedings 2002 acmieee conference supercomputing p123 november 16 2002 baltimore maryland ali pinar cevdet aykanat fast optimal load balancing algorithms 1d partitioning journal parallel distributed computing v64 n8 p974996 august 2004 bradford l chamberlain steven j deitz lawrence snyder comparative study nas mg benchmark across parallel languages architectures proceedings 2000 acmieee conference supercomputing cdrom p46es november 0410 2000 dallas texas united states ken kennedy charles koelbel hans zima rise fall high performance fortran historical object lesson proceedings third acm sigplan conference history programming languages p71722 june 0910 2007 san diego california