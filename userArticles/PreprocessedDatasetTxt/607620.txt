latent semantic kernels kernel methods like support vector machines successfully used text categorization standard choice kernel function inner product vectorspace representation two documents analogy classical information retrieval ir approacheslatent semantic indexing lsi successfully used ir purposes technique capturing semantic relations terms inserting similarity measure two documents one main drawbacks ir computational costin paper describe lsi approach implemented kerneldefined feature spacewe provide experimental results demonstrating approach significantly improve performance impair b introduction kernelbased learning methods kms stateoftheart class learning algo rithms whose best known example support vector machines svms 3 approach data items mapped highdimensional spaces information mutual positions inner products used constructing classification regression clustering rules modular systems formed general purpose learning module eg classification clustering dataspecific ele ment called kernel acts interface data learning machine defining mapping feature space kernelbased algorithms exploit information encoded innerproduct pairs data items somewhat surprisingly information sufficient run many standard machine learning algorithms perceptron convergence algorithm principal components analysis pca ridge regression nearest neighbour advantage adopting alternative representation often efficient method compute inner products complex cases even infinite dimensional vectors since explicit representation feature vectors corresponding data items necessary kms advantage accessing feature spaces would otherwise either expensive complicated represent strong model selection techniques based statistical learning theory 26 developed systems order avoid overfitting high dimensional spaces surprising one areas systems work naturally text categorization standard representation documents highdimensional vectors standard retrieval techniques based precisely innerproducts vectors combination two methods pioneered joachims 10 successively explored several others 6 11 approach documents representation known bag words based mapping documents large vectors indicating words occur text vectors many dimensions terms corpus usually several thousands corresponding entries zero term occur document hand positive otherwise two documents hence considered similar use approximately terms despite high dimensionality spaces much higher training set size support vector machines shown perform well 10 paper investigates one possible avenue extending joachims work incorporating information kernel used information retrieval ir representation known suffer drawbacks particular fact semantic relations terms taken account documents talk related topics using different terms mapped distant regions feature space map captures semantic information would useful particularly could achieved semantic kernel computes similarity documents also considering relations different terms using kernel somehow takes fact consideration would enable system extract much information documents one possible approach one adopted 23 semantic network used explicitly compute similarity level terms information encoded kernel defines new metric feature space equivalently mapping documents another feature space paper propose use technique known information retrieval latent semantic indexing lsi 4 approach documents implicitly mapped semantic space documents share terms still close terms semantically related semantic similarity two terms inferred analysis cooccurrence pat terns terms cooccur often documents considered related statistical cooccurrence information extracted means singular value decomposition term document matrix way described section 3 show step performed implicitly kernelinduced feature space amounts kernel adaptation semantic kernel learning step fixed dimension new feature space computation equivalent solving convex optimization problem eigenvalue decomposition one global maximum found efficiently since eigenvalue decomposition become expensive large datasets develop approximation technique based gramschmidt orthogonalisation procedure practice method actually perform better lsi method provide experimental results text nontext data showing techniques deliver significant improvements datasets certainly never reduce performance discuss advantages limitations relationships methods 2 kernel methods text kernel methods new approach solving machine learning problems developing algorithms make use inner products images different inputs feature space application becomes possible rich feature spaces provided inner products computed way avoid need explicitly compute feature vector given input one key advantages approach modularity decoupling algorithm design statistical analysis problem creating appropriate functionfeature spaces particular application furthermore design kernels performed modular fashion simple rules exist combine adapt basic kernels order construct complex ones way guarantees kernel corresponds inner product feature space main result paper also regarded one kernel adaptation procedure though idea using kernel defined feature space new 1 recently full potential begun realised first problem considered classification labelled examples socalled support vector machine 2 3 corresponding statistical learning analysis described 20 however turned beginning development portfolio algorithms clustering 17 using principal components analysis pca feature space regression 24 novelty detection 19 ordinal learning 7 time links made statistical learning approach bayesian approach known gaussian processes 13 classical krieging known ridge regression 16 hence first time providing direct link distinct paradigms view developments clear defining appropriate kernel function allows one use range different algorithms analyse data concerned potentially answering many practical prediction problems particular application choosing kernel corresponds implicitly choosing feature space since kernel function defined feature map oe given training set g information available kernel based algorithms contained entirely matrix inner products known gram kernel matrix matrix represents sort bottleneck information exploited operating matrix one fact virtually recode data suitable manner solutions sought linear functions feature space weight vector w 0 denotes transpose vector matrix kernel trick applied whenever weight vector expressed linear combination training points implying express f follows given explicit feature map oe use equation 1 compute corresponding kernel often however methods sought provide directly value kernel without explicitly computing oe show many standard information retrieval feature spaces give rise particularly natural set kernels perhaps best known method type referred polynomial kernel given kernel k polynomial construction creates kernel k applying polynomial positive coefficients k example consider fixed values integer p suppose feature space k f feature space k indexed ttuples features f hence relatively small additional computational cost time inner product computed one addition exponentiation required algorithms applied feature space vastly expanded expressive power even extreme example consider gaussian kernel k transforms kernel k follows whose feature space infinitely many dimensions 3 vector space representations given document possible associate bag terms bag words simply considering number occurrences terms contains typically words stemmed meaning inflection information contained last letters removed bag words natural representation vector following way number dimensions number different terms corpus entry vector indexed specific term components vector formed integer numbers representing frequency term given document typically vector mapped space word frequency information merged information eg word importance uninformative words given low weight way document represented column vector entry records many times particular word stem used document typically tens thousands entries often number documents furthermore particular document representation typically extremely sparse relatively nonzero entries basic vectorspace model bvsm document represented vertical vector indexed elements dictionary corpus matrix whose columns indexed documents whose rows indexed also call data matrix term document matrix define document document matrix term term matrix consider feature space defined basic vectorspace model corresponding kernel given inner product feature vectors case gram matrix document document matrix gen erally consider transformations document vectors mapping oe simplest case involves linear transformations type p appropriately shaped matrix case kernels form call representations vector space models vsms gram matrix case given 0 p 0 pd definition symmetric positive definite class models obtained varying matrix p natural one corresponding different linear mappings standard vector space model hence giving different scalings projections note jiang littman 9 use framework present collection different methods although without viewing kernels throughout rest paper use p refer matrix defining vsm describe number different models case showing appropriate choice p realises vsm basic vector space model basic vector space model bvsm introduced 1975 salton et al 15 used kernel joachims 10 uses vector representation mapping words vsm matrix case performance retrieval systems based simple representation surprisingly good since representation document vector sparse special techniques deployed facilitate storage computation dot products vectors common map p obtained considering importance term given corpus vsm matrix hence diagonal whose entries p weight term several methods proposed known strong influence generalization 11 often p function inverse document frequency idf total number documents corpus divided number documents contain given term example word appears document would regarded informative one distance uniform distribution good estimation importance better methods obtained studying typical term distributions within documents corpora simplest method given p measures obtained information theoretic quantities empirical models term frequency since measures use label information could also estimated external larger unlabelled corpus provides background knowledge system described previous section soon defined kernel apply polynomial gaussian construction increase expressive power joachims 10 dumais et al 5 applied technique basic vector space model classification task impressive results particular use polynomial kernels seen including features tuple words degree chosen polynomial one problems representation treats terms uncorrelated assigning orthogonal directions feature space means cluster documents share many terms reality words correlated sometimes even synonymous documents common terms potentially closely related topics similarities cannot detected bvsm raises question incorporate information semantics feature map link documents share related terms one idea would perform kind document expansion adding expanded version synonymous closely related words existing terms another somehow similar method would replace terms concepts information could potentially gleaned external knowledge correlations example semantic network however ways address problem also possible use statistical information termterm correlations 7derived corpus external reference corpus approach forms basis latent semantic indexing next subsections look two different methods case showing implemented directly kernel matrix without need work explicitly feature space allow combined kernel techniques polynomial gaussian constructions described generalised vector space model early attempt overcome limitations bvsms proposed wong et al 27 name generalised vsm gvsm document characterised relation documents corpus measured bvsm method aims capturing termterm correlations looking cooccurrence information two terms become semantically related cooccur often documents effect two documents seen similar even share terms gvsm technique provide one metric easy see also constitutes kernel function given term document data matrix gvsm kernel given matrix dd 0 term term matrix nonzero ij entry document corpus containing ith jth terms two terms cooccurring document considered related new metric takes cooccurrence information account documents mapped feature space indexed documents corpus document represented relation documents corpus reason also known dual space method 22 common case less documents terms method act bottleneck mapping forcing dimensionality reduction gvsm vsm matrix p chosen 0 document term matrix method combined polynomial gaussian kernel construction techniques example degree p polynomial kernel would features ptuple documents nonzero feature document shares terms document tuple knowledge combination previously considered either polynomial gaussian construction semantic smoothing vector space models perhaps natural method incorporating semantic information directly using external source like semantic network section briefly describe one approach siolas dalchebuc 23 used semantic network wordnet 12 way obtain termsimilarity information network encodes word dictionary relation words hierarchical fashion eg synonym hypernym etc example word husband wife special cases hypernym spouse way distance two terms hierarchical tree provided wordnet gives estimation semantic proximity used modify metric vector space documents mapped bagofwords approach siolas dalchebuc 23 included knowledge kernel handcrafting entries square vsm matrix p entries semantic proximity terms j semantic proximity defined inverse topological distance graph length shortest path connecting cases deserve special attention modified metric gives rise following kernel following distance siolas dalchebuc used distance order apply gaussian kernel construction described though polynomial construction could equally well applied kernel siolas dalchebuc used termterm similarity matrix incorporate semantic information resulting square matrix p would also possible use conceptterm relation matrix rows would indexed concepts rather terms example one might consider husband wife examples concept spouse matrix p would case longer square symmetric notice gvsms regarded special case concepts correspond documents corpus term belongs ith concept occurs document 4 latent semantic kernels latent semantic indexing lsi 4 technique incorporate semantic information measure similarity two documents use construct kernel functions conceptually lsi measures semantic information cooccurrence analysis corpus technique used extract information relies singular value decomposition svd term document matrix document feature vectors projected subspace spanned first singular vectors feature space hence dimension feature space reduced k control dimension varying k define kernel feature space particular choice vsm matrix p see p computed directly original kernel matrix without direct computation svd feature space order derive suitable matrix p first consider termdocument matrix svd decomposition sigma diagonal matrix dimensions u v orthogonal ie u columns u singular vectors feature space order decreasing singular value hence projection operator onto first k dimensions given k identity matrix first k diagonal elements nonzero u k matrix consisting first k columns u new kernel expressed motivation particular mapping identifies highly correlated dimensions ie terms cooccur often documents corpus merged single dimension new space creates new similarity metric based context information case lsi also possible isometrically reembed subspace back original feature space defining p square symmetric u k u 0 gives rise kernel since view p termterm similarity matrix making lsi special case semantic smoothing described solias dalchebuc 23 need explicitly work entries termbyterm similarity matrix help semantic network however infer semantic similarities directly corpus using cooccurrence analysis interesting kernel methods mapping instead acting termterm matrices obtained implicitly working smaller documentdocument gram matrix original term document matrix gives rise kernel matrix since feature vector document j jth column svd decomposition related eigenvalue decomposition k follows ith column v eigenvector k corresponding eigenvalue feature space created choosing first k singular values lsi approach corresponds mapping feature vector vector ui k u 0 gives rise following kernel matrix k matrix diagonal entries beyond kth set zero hence new kernel matrix obtained directly k applying eigenvalue decomposition k remultiplying component matrices set first k eigenvalues zero hence obtain kernel corresponding lsi feature space without actually ever computing features relations computation kernel pca 18 immediate similar analysis possible verify also evaluate new kernel novel inputs without reference explicit feature space order evaluate learned functions novel examples must show evaluate new kernel k new input training example kd function wish evaluate form expression still however involves feature vector would like avoid evaluating explicitly consider vector inner products new feature vector training examples original space inner products evaluated using original kernel showing evaluate fd follows hence evaluate f new example first create vector inner products original feature space take inner product precomputed row vector ff 0 v k v 0 none computation involves working directly feature space combination lsk technique polynomial gaussian construction opens possibility performing lsi high dimensional feature spaces example indexed tuples terms experiments applying approach reported experimental section paper think polynomial mapping taking conjunctions terms view lsk step soft disjunction since projection links several different conjunctions single concept hence combination polynomial mapping followed lsk step produces function form reminiscent disjunctive normal form alternatively one could perform lsk step polynomial mapping applying polynomial mapping entries gram matrix obtained lsk step obtaining space indexed tuples concepts function obtained reminiscent conjunctive normal form applied approach ionosphere data obtained improvement performance conjecture results obtained depend strongly fit style function particular data main drawback approaches computational complexity performing eigenvalue decomposition kernel matrix although matrix smaller term document matrix usually longer sparse makes difficult process training sets much larger thousand examples present next section techniques get round problem evaluating approximation lsk approach 5 algorithmic techniques experiments performed using eigenvalue decomposition routine provided numerical recipes c 14 complete eigendecomposition kernel matrix expensive step possible one try avoid working real world data efficient methods developed obtain approximate lsk solution view lsk technique one method obtaining low rank approximation kernel matrix indeed projection onto first k eigenvalues rank k approximation minimises norm resulting error matrix projection onto eigensubspaces one method obtaining lowrank approximation also developed approximation strategy based gramschmidt decomposition similar approach unsupervised learning described smola et al 25 projection built span subset projections set k training examples selected performing gramschmidt orthogonalisation training vectors feature space hence vector selected remaining training points transformed become orthogonal next vector selected one largest residual norm whole transformation performed feature space using kernel mapping represent vectors obtained refer method gk algorithm table 1 gives complete pseudocode extracting features kernel defined feature space lsk method parametrised number dimensions selected table 1 gsk algorithm given kernel k training set return feati j jth feature input classify new example x return newfeatj jth feature example 51 implicit dimensionality reduction interesting solution problem approximating latent semantic solution possible case directly interested lowrank matrix unlike information retrieval case plan use kernel conjunction optimization problem type h hessian obtained pre postmultiplying gram matrix diagonal matrix containing f1 gamma1g labels note h k eigenvalues since possible easily cheaply modify gram matrix obtain nearly solution one would obtain using much expensive low rank approximation minimum error function occurs point ff satisfies qhff 0 matrix h replaced h minimum moves new point e ff satisfies q us consider expansion h eigenbasis expansions ff e ff basis substituting formulae equating coefficients ith eigenvalue gives ff implying e ff fraction equation squashing function approaching zero values approaching 1 ae first case e second case e ff ff overall effect map parameter chosen carefully region spectrum eigenvalues decrease rapidly effectively project solution onto space spanned eigenvectors larger eigenvalues algorithmic point view much efficient explicitly performing lowrank approximation computing eigenvectors derivation provides cheap approximation algorithm latent semantic kernel also highlights interesting connection algorithm 2norm soft margin algorithm noise tolerance also obtained adding diagonal kernel matrix 21 note several approximations view since example svm solution constrained optimisation ff constrained positive case effect may different support vectors nearly orthogonal eigenvectors corresponding large eigenvalues fact procedure distinct standard soft margin approach borne experiments described next section 6 experimental results empirically tested proposed methods text nontext data order demonstrate general applicability method test effectiveness different conditions results generally positive cases improvements significant worth additional computation cases significant advantage using latent semantic gramschmidt kernels certainly use never hurts performance 61 experiments text data section describes series systematic experiments performed text data selected two text collections namely reuters medline described datasets reuters21578 conducted experiments set documents containing stories reuters news agency namely reuters dataset used reuters 21578 newer version corpus compiled david lewis 1987 publicly available httpwwwresearchattcomlewis obtain training set test set exists different splits corpus used modified apte modeapte split modeapte split comprises 9603 training 3299 test documents reuters category contain 1 many 2877 documents training set similarly test set category 1 many 1066 relevant documents medline1033 medline1033 second dataset used experi ments dataset comprises 1033 medical documents queries obtained national library medicine focused query23 query20 two queries contain 39 relevant documents selected randomly 90 data training classifier 10 evaluation always 24 relevant documents training set 15 relevant documents test set performed 100 random splits data experiments reuters documents preprocessed removed punctuation words occurring stop list also applied porter stemmer words weighted terms according variant tfidf scheme given tf represents term frequency df used document frequency total number documents documents unit length feature space preprocessed medline documents removing stop words punctuation weighted words according variant tfidf described preceding paragraph normalised documents bias occur length documents evaluation used f1 performance measure given 2prp precision r recall first set experiment conducted subset 3000 documents reuters21578 data set selected randomly 2000 documents training remaining 1000 documents used test set focused top 097dimension baseline figure 1 generalisation performance svm gsk lsk linear kernel earn 5 reuters categories earn acq moneyfx grain crude trained binary classifier category evaluated performance new documents repeated process 10 times category used svm linear kernel baseline experiments parameter c controls trade error maximisation margin tuned conducting preliminary experiments chose optimal value conducting experiments ten splits one category ran svm reduced feature space also feature space full dimension value c showed best results full space selected used experiments medline1033 text corpus selected value c conducting experiments one split data ran svm feature space full dimension optimal value c showed best results selected note use split experiments choice seem perfect basis experimental observation reuters conclude method gives optimal value c results experiments reuters shown figures 1 4 note results averaged 10 runs algorithm started small dimensional feature space increased dimensionality feature space intervals extracting features figures demonstrate performance lsk method comparable baseline method generalisation performance svm classifier varies varying dimensionality semantic space increasing value k f1 numbers rise reaching maximum falls number equivalent baseline method however maximum substantially different baseline method words sometimes obtain modest gain incorporating information kernel matrix figure 6 figure 7 illustrate results experiments conducted two medline1033 queries results averaged 100 random runs algorithm experiments start small number dimensions dimensionality increased intervals extracting features results dimesnsion baseline figure 2 generalisation performance svm gsk lsk linear kernel acq baseline figure 3 generalisation performance svm gsk lsk linear kernel moneyfx dimension figure 4 generalisation performance svm gsk lsk linear lkernel grain dimension baseline figure 5 generalistaion performance svm gsk lsk linear kernel crude baseline figure 6 generalisation performance svm gsk lsk linear kernel query23 baseline figure 7 generalisation performance svm gsk lsk linear kernel query20 table 2 f1 numbers varying dimensions feature space svm classifier lsk svm classifier linear kernel baseline ten reuters categories category k baseline 100 200 300 moneyfx 062 0673 0635 06 grain 0664 0661 067 0727 crude 0431 0558 0576 0575 trade 0568 0683 066 0657 interest 0478 0497 05 0517 ship 0422 0544 0565 0565 wheat 0514 051 0556 0624 microavg 0786 0815 0815 0819 query23 encouraging showing lsk potential show substantial improvement baseline method thus results reuters show cases improvements performance others significant improvements results reuters medline1033 datasets demonstrates gsk effective approximation strategy lsk cases results approximately lsk however worth noting cases figure 6 gsk may show substantial improvement baseline method also lsk hence results demonstrate gsk good approximation strategy lsk improve generalisation performance lsk evident results medline data extract informative features useful classification gsk achieve maximum high dimension situations phenomenon may cause practical limitations large data sets addressed issue developed generalised gsk algorithm text classification furthermore conducted another set experiments study behaviour svm classifier semantic kernel svm classifier linear kernel scenario classifier learnt using small training set selected randomly 5 training data 9603 documents focused top 10 categories earn 144 acq 85 moneyfx 29 grain 18 crude 16 trade 28 interest 19 ship 12 wheat 8 corn 6 note number relevant documents shown name categories binary classifier learnt category evaluated full test set 3299 documents c tuned one category f1 numbers obtained results experiments reported table 2 microaveraged f1 numbers also given set value noted gain categories loss performance others worth noting svm classifier trained semantic kernel perform approximately baseline method even 200 dimensions results demonstrate proposed method capable performing reasonably well environments labelled documents 62 experiments nontext data figure 8 generalization error polynomial kernels degrees 23 4 ionosphere data aver aged 100 random splits function dimension feature space present experiments conducted nontext ionosphere data set uci repository ionosphere contains 34 features 315 points measured gain lsk comparing performance svm polynomial kernel parameter c set conducting preliminary experiments one split data keeping dimensionality space full tried optimal value demonstrated minimum error chosen value used splits reduced feature space note split data used tuning parameter c used experiments results shown figure 8 results averaged 100 runs begin experiments setting k small value increased dimensionality space intervals results show test error greatly reduced dimension feature space reduced curves also demonstrate classification error svm classifier semantic kernel reaches minimum makes peaks valleys showing results equivalent baseline method results demonstrate proposed method general applied domains text potential improve performance svm classifier reducing dimension however cases show gain may successful reducing dimension 7 generalised version gsk algorithm text classification section present generalised version gsk algorithm algorithm arose result experiments reported section 6 preliminary experiments also contributed development algorithm gsk algorithm presented previous section extracts features relative documents irrespective relevance category words features computed respect label document generally category distribution skewed text corpora establishes need bias feature computation towards relevant documents words introduce bias feature extraction process computed features useful informative text classification main goal developing generalised version gsk algorithm extract informative features fed classifier show high effectiveness low number dimensions achieve goal described preceding paragraph propose algorithm shown figure 9 gsk iterative procedure greedily selects document iteration extracts features iteration criterion selecting document maximum residual norm generalised version gsk algorithm focuses relevant documents placing weight norm relevant documents algorithm transforms documents new reduced feature space taking set documents input underlying kernel function number bias b also fed algorithm number specifies dimension reduced feature space b gives degree feature extraction biased towards relevant documents algorithm starts measuring norm document concentrates relevant documents placing weight norm documents next step document maximum norm chosen features extracted relative document process repeated times finally documents transformed new dimensional space dimension new space much smaller original feature space note enough positive data available training equal weights given relevant irrelevant documents generalised version gsk algorithm provides practical solution problem may occur gskalgorithm algorithm may show good require kernel k training set fd 1 number n end n 1 else end n end end return feati j jth feature input classify new example end return newfeatj jth feature example figure 9 generalised version gsk algorithm generalisation high dimension enough training data scenario generalised version gskalgorithm shows similar performance lower dimensions complete pseudocode algorithm given figure 9 8 experiments generalised gskalgorithm employed generalise gsk algorithm transform reuters documents new reduced feature space evaluated proposed method conducting experiments full reuters data set used modeapte version performed experiments 90 categories contain least one relevant document training set test set order transform documents new space two free parameters dimension reduced space b bias need tuned analysed generalistion performance svm classifier respect b conducting set experiments 3 reuters categories results experiments shown table 3 set experiments set dimensionality space 500 varied b results demonstrate extraction features biased environment informative useful insufficient training data basis experiments selected optimal value b next set experiments note selected optimal value c conducting preliminary experiments one reuters category set value 1000 results set experiments given table 4 given f1 value 500 1000 dimensional space microaveraged f1 values also shown table order learn svm classifier used sv light 8 experiments described section results show generalised gsk algorithm viewed substantial dimensionality reduction technique observation proposed method shows results comparable baseline method dimensionality 500 note baseline method employed svm linear kernel noted 500 dimensionality slow improvement generalisation performance svm microaveraged f1 values svm generalised gsk 0822 500 dimensions whereas microaveraged f1 number svm linear kernel 0854 results show performance proposed technique comparable baseline method results show generalised gsk algorithm practical approximation lsk learning algorithm provided enough positive training data need bias feature extraction process however learning algorithm enough positive training data svm may show good performance high dimensionality leading practical limitations however introduction bias towards relevant documents overcome problem hence making technique applied large data sets table 3 f1 numbers acq money fx wheat different values b 10 0922 0569 0707 11 0864 0695 0855 12 0864 0756 0846 20 0864 0748 0846 22 0864 0752 0855 24 0864 0756 0846 26 0864 0748 0846 28 0864 0752 0846 60 0864 0752 0857 10 table 4 f1 numbers topten reuters categories category baseline 500 1000 acq 0923 0934 0948 moneyfx 0755 0754 0775 grain 0894 0902 093 crude 0872 0883 0880 trade 0733 0763 0761 interest 0627 0654 0691 ship 0743 0747 0797 wheat 0864 0851 087 corn 0857 0869 0895 microavg 9 conclusion paper studied problem introducing semantic information kernel based learning method technique inspired approach known latent semantic indexing borrowed information retrieval lsi projects data subspace determined choosing first singular vectors singular value decomposition shown obtain inner products derived projection performing equivalent projection onto first eigenvectors kernel matrix hence possible apply technique kernel defined feature space whatever original dimensionality refer derived kernel latent semantic kernel lsk experimentally demonstrated efficacy approach text nontext data datasets substantial improvements performance obtained using method others little effect observed eigenvalue decomposition matrix relatively expensive compute also considered iterative approximation method equivalent projecting onto first dimension derived gramschmidt othogonalisation data perform projection efficiently kernel defined feature space experiments show datasets socalled gramschmidt kernel gsk effective lsk method despite success large imbalanced datasets encountered text classification tasks number dimensions required obtain good performance grows quite large relevant features drawn small number positive documents problem addressed biasing gsk feature selection procedure favour positive documents hence greatly reducing number dimensions required create effective feature space methods described paper similar flavour demonstrated impressive performance datasets question dataset makes different semantic focusing methods effective fully understood remains subject ongoing research acknowledgements authors would like thank thorsten joachims chris watkins useful discussions work supported epsrc grant number grn08575 european commission esprit working group neural computational learning neurocolt2 nr 27150 kermit 1st project kernel methods images text kermit nr 1st200025431 r theoretical foundations potential function method pattern recognition learning training algorithm optimal margin classifiers introduction support vector machines indexing latent semantic analysis inductive learning algorithms representations text categorization automatic crosslanguage retrieval using latent semantic indexing large margin rank boundaries ordinal regression making largescale svm learning practical approximate dimension equalization vectorbased information retrieval text categorization support vector machines five papers wordnet gaussian processes svm mean field leaveoneout numerical recipes c art scientific computing vector space model information retrieval ridge regression learning algorithm dual variables kernel pca pattern reconstruction via approximate preimages kernel principal component analysis sv estimation distri butions support structural risk minimization datadependent hierarchies margin distribution soft margin experiments multilingual information retrieval using spider system support vectors machines based semantic kernel text categorization tutorial support vector regression sparse kernel feature analysis statistical learning theory generalized vector space model information retrieval tr training algorithm optimal margin classifiers nature statistical learning theory experiments multilingual information retrieval using spider system generalized vector spaces model information retrieval inductive learning algorithms representations text categorization making largescale support vector machine learning practical kernel principal component analysis introduction support vector machines vector space model automatic indexing text categorization support vector machines represent texts input space text categorization suport vector machines ridge regression learning algorithm dual variables approximate dimension equalization vectorbased information retrieval support vector machines based semantic kernel text categorization ctr qiang sun gerald dejong explanationaugmented svm approach incorporating domain knowledge svm learning proceedings 22nd international conference machine learning p864871 august 0711 2005 bonn germany yaoyong li john shawetaylor using kcca japaneseenglish crosslanguage information retrieval document classification journal intelligent information systems v27 n2 p117133 september 2006 yaoyong li john shawetaylor advanced learning algorithms crosslanguage patent retrieval classification information processing management international journal v43 n5 p11831199 september 2007 yonghong tian tiejun huang wen gao latent linkage semantic kernels collective classification link data journal intelligent information systems v26 n3 p269301 may 2006 mehran sahami timothy heilman webbased kernel function measuring similarity short text snippets proceedings 15th international conference world wide web may 2326 2006 edinburgh scotland haixian wang zilan hu yue zhao efficient algorithm generalized discriminant analysis using incomplete cholesky decomposition pattern recognition letters v28 n2 p254259 january 2007 kevyn collinsthompson jamie callan query expansion using random walk models proceedings 14th acm international conference information knowledge management october 31november 05 2005 bremen germany serhiy kosinov stephane marchandmaillet igor kozintsev carole dulong thierry pun dual diffusion model spreading activation contentbased image retrieval proceedings 8th acm international workshop multimedia information retrieval october 2627 2006 santa barbara california usa kristen grauman trevor darrell pyramid match kernel efficient learning sets features journal machine learning research 8 p725760 512007 vikramjit mitra chiajiu wang satarupa banerjee text classification least square support vector machine approach applied soft computing v7 n3 p908914 june 2007 francis r bach michael jordan kernel independent component analysis journal machine learning research 3 p148 312003