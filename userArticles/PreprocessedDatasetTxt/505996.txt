improving latency tolerance multithreading decoupling abstractthe increasing hardware complexity dynamically scheduled superscalar processors may compromise scalability organization make efficient use future increases transistor budget smt processors designed superscalar core therefore directly concerned problem work presents evaluates novel processor microarchitecture combines two paradigms simultaneous multithreading accessexecute decoupling since decoupled units issue instructions inorder architecture significantly less complex terms critical path delays centralized outoforder design effective future growth issuewidth clock speed investigate techniques complement since decoupling features excellent memory latency hiding efficiency large amount parallelism exploited multithreading may used hide latency functional units keep fully utilized study shows adding decoupling multithreaded architecture fewer threads needed achieve maximum throughput therefore addition obvious hardware complexity reduction places lower demands memory system since one problems multithreading degradation memory system performance terms miss latency bandwidth requirements improvement becomes critical high miss latencies bandwidth might become bottleneck finally although may seem rather surprising study reveals multithreading exhibits little memory latency tolerance results suggest latency hiding effectiveness smt architectures comes dynamic scheduling hand decoupling effective hiding memory latency increase cache miss penalty 1 cycles reduces performance 4context multithreaded decoupled processor less 2 percent nondecoupled multithreaded processor loss performance 23 percent b introduction gap speeds processors memories kept increasing past decade expected sustain trend near future divergence implies terms clock cycles increasing latency memory operations cross chip boundaries addition processors keep growing capabilities exploit parallelism means greater issue widths deeper pipelines makes even higher negative impact memory latencies performance alleviate problem current processors devote high fraction transistors onchip caches order reduce average memory access time several prefetching techniques also developed hardware software 3 processors commonly known outoforder processors 40 20 18 8 9 include dynamic scheduling techniques based tomasulos algorithm 34 variations allow tolerate memory functional unit latency overlapping useful computations independent instructions implement processor capable filling issue slots independent instructions looking forward instruction stream limited instruction window general mechanism aggressively extracts instruction parallelism available instruction window memory latencies continue grow future outoforder processors need larger instruction windows find independent instructions fill increasing number empty issue slots number grow even faster greater issue widths increase instruction window size obvious influence chip area major negative impact strike processor clock cycle time reported recently 21 networks involved issue wakeup bypass mechanisms also although less extent renaming stage critical path determines clock cycle time analysis authors study state delay function networks component increases quadratically window length although linearly also depends strongly issue width moreover higher density technologies accelerate increase latencies analysis suggest outoforder architectures could find future serious boundary clock speeds different kinds architectures proposed recently either inorder outoforder address clock cycle problem partitioning critical components architecture andor providing less complex scheduling mechanisms 30 6 16 21 41 follow different partitioning strategies one accessexecute paradigm first proposed early scalar architectures provide dual issue limited form dynamic scheduling especially oriented tolerate memory latency believe decoupled accessexecute architectures regain progressively interest far issue widths memory latencies keep growing demanding larger instruction windows trends make worth trading issue complexity clock speed typically decoupled accessexecute architecture 26 27 7 39 38 23 2 12 splits either statically dynamically instruction stream two access stream composed instructions involved fetch data memory runs asynchronously respect execute stream formed instructions process data streams executed independent processing units called ap ep respectively paper ap expected execute advance ep prefetch data memory appropriate buffering structures ep consume without delay anticipation slippage may involve multiple conditional branches however amount slippage ap ep highly depends program ilp data control dependences force units synchronize called loss decoupling events 2 35 producing serious performance degradation decoupling model presented paper performs dynamic code partitioning 27 12 following simple scheme based instruction data types ie integer fp although rather simplistic scheme mostly benefits numerical programs still provides basis study mainly focused latency hiding potential decoupling synergy multithreading recent studies 22 24 proposed alternative compilerassisted partitioning schemes address partitioning integer codes since one main arguments decoupled approach reduced issue logic complexity chosen issue instructions inorder within processing unit decoupled architecture adapts higher memory latencies scaling much simpler structures outoforder ie scaling lower hardware cost conversely scaling higher degree similar cost may argued inorder processors limited potential exploit ilp however current compiling techniques extract much ilp thus compiler pass information hardware instead using runtime schemes approach emerging epic explicitly parallel instruction computing architectures take 10 propose new decoupled architecture provides ap ep powerful dynamic scheduling mechanism simultaneous multithreading 37 36 processing unit several contexts issuing instructions mentioned decoupled mode active simultaneously compete issue slots instructions different contexts issued cycle show study combination decoupling mulithreading takes advantage best features decoupling simple effective technique hiding high memory latencies reduced issue complexity multithreading provides enough parallelism hide functional unit latencies keep functional units busy addition multithreading also helps hide memory latency program decouples badly however far decoupling succeeds hiding memory latency threads needed keep functional units busy achieve nearpeak issue rate important result since threads reduces memory pressure reported major bottleneck multithreading architectures reduces hardware cost complexity rest paper organized follows section 2 describes base decoupled architecture analyzed section 3 providing justification multithreading section 4 describes evaluates proposed multithreaded decoupled architecture finally summarize conclusions section 5 2 basic decoupled architecture model baseline decoupled architecture considered paper figure 1 consists two superscalar decoupled processing units address processing unit ap execute processing unit ep decoupled processor executes single instruction stream based decalpha isa 5 splitting dynamically dispatching instructions either ap ep two separate physical register files one ap 64 integer registers ep 96 fp registers units share common fetch dispatch stage separate issue execute writeback stage pipelines next brief description stage memory subsystem store addres figure 1 scheme base decoupled processor fetch decode rename instruction reg file reg file map table register fetch stage reads 4 consecutive instructions per cycle less 4 taken branch among infinite icache notice icache miss ratios spec fp95 usually low approximation introduces small perturbation also provided conditional branch prediction scheme based 2k entry branch history table 2 bit saturating counter per entry 25 dispatch stage decodes renames 4 instructions per cycle sends either ap instruction queue iq 48 entries ep depending whether integer floating point instructions memory instructions dispatched ap iq allows ap execute ahead ep providing necessary slippage hide memory latency exceptions kept precise means reorder buffer graduation mechanism register renaming map table 13 28 decoupled architectures 27 chosen steer memory instructions units allow copying data load queue registers since preliminary studies showed code expansion would significantly reduce performance implemented dynamic register renaming avoids duplication data fetched memory written physical register rather data queue eliminating need copying also convenient way manage disordered completion loads lockupfree cache present duplication conditional branch instructions also used 27 may avoided incorporating similar speculation recovery mechanisms uses mips r10000 identify instructions squash case misprediction ap ep provided 2 general purpose fully pipelined functional units whose latencies 1 cycle ap 4 cycles ep respectively processing unit read issue 2 instructions per cycle better exploit parallelism ap ep instructions issue execute speculatively beyond four unresolved branches mips r10000 40 powerpc 620 20 feature may become sometimes key factor enable ap slip ahead ep store addresses held saq queue 32 entries stores graduate loads issued cache disambiguated addresses held saq whenever dependence encountered data pending store immediately bypassed register available otherwise load put aside data forwarded primary data cache onchip 2ported 31 directmapped 64 kb sized block length implements writeback policy minimize offchip bus traffic lockupfree cache 17 modelled similarly maf alpha 21164 5 hold outstanding primary misses different lines capable merge 4 secondary per pending line assume l1 cache misses always hit infinite multibanked offchip l2 cache 16 cycle latency plus penalty due bus contention l1l2 interface consists fast 128bit wide data bus capable deliver 16 bytes per cycle like r10000 bus busy 2 cycles line fetched copied back 3 quantitative evaluation decoupled processor section first characterized major sources wasted cycles typical singlethreaded decoupled processor next latency hiding effectiveness architecture evaluated identifying main factors influence latency tolerance architecture studies decoupled machines carried 1 26 7 29 27 39 38 19 14 incorporate techniques like storeload forwarding control speculation lockupfree caches section also provides motivation multithreaded decoupled architecture analyzed section 4 31 experimental framework experiments carried trace driven simulator binary code obtained compiling spec fp95 benchmark suite 33 dec alphastation 600 5266 dec compiler applying full optimizations trace generated running code previously instrumented atom tool 32 simulator modelled cyclebycycle architecture described previous section run spec fp95 benchmarks fed largest available input data sets since slow due detail simulations run portion 100m instructions benchmark skipping initial startup phase determine appropriate initial discarded offset compared instructiontype frequencies fragment starting different points full run frequencies found phase length benchmarks 5000 instructions 101tomcatv 1000 104hydro2d 146wave5 100 rest benchmarks 32 sources wasted cycles figure 2 shows throughput issue stage terms percentage committed instructions total issue slot count ie percent issue slots really useful work ap ep wasted throughput also characterized identifying cause empty issue slot four different configurations evaluated differ whether lockupfree cache included whether storeload forwarding mechanism enabled stress memory system section assume 8 kb l1 data cache shown figure 2 lockupfree cache present first second bars ap stalled load misses ep starved time miss latency increases ap cycle count far ep cycle count ap execution time becomes bounding limit global performance decoupling hardly hide memory latencies nature figure 2 issue slot breakdown several decoupled architectures show effects lockupfree cache storeload forwarding mechanism 8 kb l1 cache size none forwd lfree lfree forwd none forwd lfree lfree forwd configuration1030507090of issue slots wrongpath instr idle wait operand fu wait operand memory blocking miss stld hazard useful work stalls structural hazard lockupfree cache used kind stalls almost eliminated third fourth bars course uncovers overlapped causes overall improvement performance achieves impressive 23 speedup 098 232 ipc memory data hazard occur store fp load detected memory disambiguation storeload forwarding enabled first third bars memory hazard produces stall ap store issued cache addition causes slippage reduction two units call event loss decoupling lod 2 35 may expose ep penalized memory latency case subsequent load miss amount slippage reduction ap ep caused memory hazard depends close load scheduled matching store results depicted figure 2 show ap stalls labelled stld hazards almost completely removed storeload forwarding enabled however average improvement ep performance almost negligible overall ipc increases 18 latter fact suggests either stores scheduled enough advance matching loads little probability get subsequent miss finally full featured configuration fourth bar graph observed major source wasted slots ep true data dependences register operands labelled wait operand fu stalls less caused misses labelled wait operand memory notice although many loads ep registers loads ap registers stalls caused misses similar processor units integer load miss produces higher penalty clearly illustrated next section 33 latency hiding effectiveness interest decoupled architecture closely related ability hide high memory latencies without resorting complex issue mechanisms latency hiding potential decoupled processor depends strongly decoupling behaviour programs tested programs scheduling ability compiler remove lod events force ap ep synchronize also key factor however compiler used digital f77 especially tailored decoupled processor therefore since latency hiding effectiveness decoupling provides basis proposed multithreaded decoupled architecture order validate conclusions interested assessment base architecture without specific compiler support purpose run 10 benchmarks external l2 cache latency varying 1 256 cycles simulations assume architectural parameters described section 2 except architectural51525 l2 latency cycles51525 average perceived fpload miss latency cycles tomcatv su2cor hydro2d mgrid applu turb3d apsi tomcat swim su2cor hydro mgrid applu turb3d apsi fpppp wave5 benchmark2060100 miss latency stores loads20601001401 l2 latency cycles2060100140average perceived iload miss latency cycles tomcatv su2cor hydro2d mgrid applu turb3d apsi l2 latency cycles loss su2cor hydro2d mgrid applu turb3d apsi figure 3a perceived miss latency fp loads figure 3b perceived miss latency integer loads figure 3c miss ratios loads stores l2 latency 256 cycles figure 3d impact latency performance loss relative 1cycle l2 latency case queues physical register files scaled proportionally l2 latency addition performance also measured separately average perceived latency integer fp load misses since interested particular benefit decoupling independently cache miss ratio average include load hits perceived latency fp load misses measures ep stalls caused misses reveals decoupled behavior program ie amount slippage ap respect ep shown figure 3a except fpppp 96 fp load miss latency always hidden perceived latency integer load misses measures ap stalls caused misses depends ability compiler schedule integer loads ahead dependent instructions shown figure 3b fpppp su2cor turb3d wave5 programs experience largest integer load miss stalls regarding impact l2 latency performance see figure 3d although programs like fpppp turb3d quite high perceived load miss latencies hardly performance degraded due extremely low miss ratios depicted figure 3c performance degraded programs high perceived miss latencies significant miss ratios hydro2d wave5 su2cor summarize performance little affected l2 latency either hidden efficiently tomcatv swim mgrid applu apsi miss ratio low fpppp turb3d seriously degraded programs lack features su2cor wave5 hydro2d hidden miss latency fp loads depends good decoupling behavior programs integer loads relies exclusively static instruction scheduling 4 multithreaded decoupled architecture shown previous section stalls decoupled processor may removed except caused true data dependences register operands ep figure 2 right labelled wait operand fu restricted ability inorder issue model exploit ilp ap ep provided dynamic scheduling capability stalls could also removed simultaneous multithreading smt dynamic scheduling technique increases processor throughput exploiting thread level parallelism multiple contexts simultaneously active compete issue slots functional units previous studies smt focused several dynamic instruction scheduling mechanisms 4 11 37 36 among others decoupling paper analyze potential implemented decoupled processor still refer simultaneous although obvious substantial differences original smt retains key concept issuing different threads single cycle since decoupling provides excellent memory latency tolerance multithreading supplies enough amounts parallelism remove remaining stalls expect important synergistic effects new microarchitecture combines two techniques section present evaluate performance memory latency tolerance multithreaded decoupled accessexecute architecture analyze mutual benefits techniques especially miss latency large 41 architecture overview proposal multithreaded decoupled architecture figure 4 thread executes decoupled mode sharing functional units data cache threads base memory subsystem store addres figure 4 scheme multithreaded decoupled processor instruction reg files reg files map tables register fetch dispatch rename multithreaded decoupled architecture based decoupled design previous section extensions run 6 threads issue 8 instructions per cycle 4 ap 4 ep 8 functional units l1 lockupfree data cache augmented 4 ports fetch dispatch stages including branch prediction register map tables register files queues replicated context issue logic functional units data cache shared threads model threads allowed compete 8 issue slots cycle priorities among determined pure roundrobin order similar full simultaneous issue scheme reported 37 cycle two threads access cache fetch 8 consecutive instructions first taken branch chosen threads less instructions pending dispatched similar rr28 icount schemes reported 36 42 experimental evaluation multithreaded decoupled simulator fed different traces corresponding independent threads trace every thread built concatenating first 10 million instructions 10 traces used previous section thread using different permutation thus totalling 100 million instructions per thread way threads different traces balanced workloads similar missratios etc figure 5 shows wasted issue slots varying number threads 1 6 since different threads may candidates slot lose different cause order characterize loss performance classified wasted issue slots proportionally causes prevent individual threads issuing 43 wasted issue slots multithreaded decoupled architecture first column figure 5 represents case single thread reveals expected major bottleneck caused ep functional units latency caused lack parallelism inorder issue policy discussed section 3 two contexts added multithreading mechanism reduces drastically stalls units produces 231 speedup 268 ipc 619 ipc since 3 threads ap functional units nearly saturated 907 negligible additional speedups obtained adding contexts 665 ipc achieved 4 threads notice although ap almost achieves maximum throughput ep functional units saturate due load imbalance ap ep therefore effective peak performance reduced 17 8 665 ipc problem could addressed different choice number functional units processor unit beyond scope study another important remark number threads increased combined working set larger miss ratios increase progressively putting greater demands external bus bandwidth average pending misses thus increasing effective load miss latency increasing ep stalls caused waiting operands memory see rightmost graph figure 5 hand ap stalls due integer load misses see operands memory leftmost graph figure 5 almost eliminated multithreading since loads benefit decoupling number threads1030507090of issue cycles idle wait operand fu wait operand memory useful work number threads1030507090of issue cycles empy iqueue wait operand fu wait operand memory useful work figure 5 ap left ep right issue slots breakdown multithreaded decoupled architecture 44 latency hiding effectiveness multithreading decoupling two different approaches tolerate high memory latencies run experiments similar section 33 multithreaded decoupled processor 1 4 contexts quantify latency tolerance addition experiments also carried reveal contribution mechanism latency hiding effect consist set identical runs degenerated version multithreaded architecture instruction queues disabled ie nondecoupled multithreaded architecture figure 6a shows average perceived load miss latency point view individual thread 8 configurations mentioned varying l2 latency 1 256 cycles metric expresses average number times instruction scheduled thread cannot issue operand depends pending load miss figure 6b shows corresponding relative performance loss respect 1cycle l2 latency 8 configurations notice metric compares tolerance architectures memory latency rather absolute performance several conclusions drawn graphs first observe figure 6a average load miss latency perceived individual thread quite low decoupling enabled less 6 cycles l2 latency 256 cycles much higher decoupling disabled second load miss latency perceived individual thread slightly longer threads running although threads effectively reduces number stall cycles thread also increases miss ratio due larger combined working set produces longer bus contention delays becomes slightly dominant effect third shown figure 6b l2 memory latency increased 1 cycle cycles decoupled multithreaded architecture experiences performance drops less 36 less 15 4 threads performance degradation observed non decoupled configurations greater 23 even huge memory latency 256 cycles performance loss decoupled configurations lower 39 greater 79 nondecoupled configurations fourth multithreading provides additional latency tolerance improvements especially nondecoupled configurations much lower latency tolerance provided decoupling conclusions drawn figure 6c multithreading raises performance curves decoupling makes flatter words main effect l2 latency cycles1030507090110 perceived load miss l2 latency cycles loss relative tocycle latency figure 6a average perceived load miss latency individual threads figure 6b latency tolerance performance loss relative 1cycle l2 latency case figure 6c contribution decoupling multithreading performance13571 l2 latency cycles1357ipc 4 decoupled 3 decoupled decoupled 4 nondecoupled 3 nondecoupled nondecoupled nondecoupled multithreading provide throughput exploiting thread level parallelism major contribution memory latency tolerance related slope curves comes decoupling precisely specific role decoupling plays hybrid architecture 45 hardware context reduction external bus bandwidth bottleneck multithreading powerful mechanism highly improves processor throughput cost needs considerable amount hardware resources run experiments illustrate decoupling reduces hardware context requirements measured performance several configurations 1 8 contexts decoupled multithreaded architecture nondecoupled multithreaded architecture see figure 7a decoupled configuration achieves maximum performance 3 4 threads non decoupled configuration needs 6 threads achieve similar ipc ratios one traditional claims multithreading approach ability sustain high processor throughput even systems high memory latency since hiding longer latency may require higher number contexts well known strong negative impact memory performance reduction hardware context requirements obtained decoupling may become key factor l2 memory latency high illustrate run previous experiment l2 memory latency 64 cycles shown figure 7b13571 2 3 4 5 6 7 8 number threads1357ipc decoupled nondecoupled number threads1357ipc decoupled nondecoupled figure 7a decoupling reduces number hardware contexts figure 7b maximum performance without decoupling cannot reached due external bus saturation decoupled architecture achieves maximum performance 4 5 threads nondecoupled architecture cannot reach similar performance number threads would need many would saturate external l2 bus average bus utilization 89 12 threads 98 threads moreover notice decoupled architecture requires 3 threads achieve performance nondecoupled architecture 12 threads thus decoupling significantly reduces amount parallelism required reach certain level performance previous result suggests external l2 bus bandwidth potential bottleneck kind architectures describe impact measured performance bus utilization several configurations 1 6 hardware contexts three different external bus bandwidths 8 16 32 bytescycle results shown figure 8a figure 8b 8 bytescycle bandwidth bus becomes saturated 3 threads running performance degraded beyond point summarize decoupling multithreading complement hide memory latency increase ilp reduced amounts threadlevel parallelism low issue logic complexity figure 8a ipc several bus bandwidths figure 8b external l2 bus utilization several bus bandwidths2060100 number threads2060100 external bus utilization cycles 8 bytescycle number threads1357ipc 8 bytescycle 5 summary conclusions paper analized synergy multithreading accessexecute decoupling multithreaded decoupled architecture aims taking advantage latency hiding effectiveness decoupling potential multithreading exploit ilp analyzed important factors determine performance synergistic effect paradigms multithreaded decoupled architecture hides efficiently memory latency average load miss latency perceived individual thread less 6 cycles worst case 4 threads l2 latency 256 cycles also found l2 latencies lower cycles impact performance quite low less 35 ipc loss relative 1 cycle latency scenario quite independent number threads however impact greater 23 ipc loss decoupling disabled latter fact shows main contribution memory latency tolerance corresponds decoupling mechanism architecture reaches maximum performance threads significantly less nondecoupled architecture number simultaneously active threads supported architecture significant impact hardware chip area eg number registers instruction queues complexity eg instruction fetch issue mechanisms consequently clock cycle reducing number threads also reduces cache conflicts required memory bandwidth usually one potential bottlenecks multithreaded architecture shown external l2 bus bandwidth becomes bottleneck miss latency 64 cycles decoupling disabled preventing achieving maximum performance number threads summary conclude decoupling multithreading techniques complement exploit instruction level parallelism hide memory latency particular combination obtains maximum performance threads reduced issue logic complexity hardly performance degraded wide range l2 latencies features make promising alternative future increases clock speed issue width 6 r decoupled accessexecute architecture efficient access structured data effectiveness decoupling performance study software hardware data prefetching schemes concurrent execution multiple execution streams superscalar processors alpha 21164 microprocessor hardware reference manual multicluster architecture reducing cycle time partitioning pipe vlsi decoupled architecture p6 uses decoupled superscalar design digital 21264 sets new standard hp make epic disclosure elementary processor architecture simultaneous instruction issuing multiple threads designing tfp microprocessor superscalar microprocessor design limitation study access decoupling improving directmapped cache performance addition small fullyassociative cache prefetch buffers pews decentralized dynamic scheduler ilp processing memory latency effects decoupled architectures powerpc 620 decoupling integer execution superscalar processors structured memory access architecture exploiting idle floatingpoint resources integer execution study branch prediction strategies decoupled accessexecute computer architectures implementation precise interrupts pipelined processors simulation study decoupled architecture computers multiscalar processors atom system building customized program analysis tools standard performance evaluation corporation efficient algorithm exploiting multiple arithmetic units compiling optimizing decoupled architectures exploiting choice instruction fetch issue implementable simultaneous multithreading processor simultaneous multithreading maximizing onchip par allelism misc multiple instruction stream computer evaluation wm architecture mips r10000 superscalar microprocessor tr simulation study decoupled architecture computers zs1 central processor highbandwidth data memory systems superscalar processors elementary processor architecture simultaneous instruction issuing multiple threads evaluation wm architecture misc effectiveness decoupling atom designing tfp microprocessor compiling optimizing decoupled architectures simultaneous multithreading multiscalar processors decoupling integer execution superscalar processors exploiting choice complexityeffective superscalar processors trace processors multicluster architecture exploiting idle floatingpoint resources integer execution performance modeling code partitioning ds architecture improving directmapped cache performance addition small fullyassociative cache prefetch buffers implementation precise interrupts pipelined processors decoupled accessexecute computer architectures mips r10000 superscalar microprocessor memory latency effects decoupled architectures limitation study access decoupling powerpc 620 microprocessor lockupfree instruction fetchprefetch cache organization study branch prediction strategies costeffective clustered architecture latency hiding effectiveness decoupled accessexecute processors