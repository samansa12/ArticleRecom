tracking best expert generalize recent relative loss bounds online algorithms additional loss algorithm whole sequence examples loss best expert bounded generalization allows sequence partitioned segments goal bound additional loss algorithm sum losses best experts segment model situations examples change different experts best certain segments sequence examples single segment case additional loss proportional log n n number experts constant proportionality depends loss function algorithms produce best partition however loss bound shows predictions close best partition number segments k1 sequence length ell bound additional loss algorithm best partition ok log nk logellk case loss per trial bounded one obtain algorithm whose additional loss loss best partition independent length sequence additional loss becomes oklog n k loglk l loss best partitionwith k1 segments algorithms tracking predictions best expert aresimple adaptations vovks original algorithm single best expert case original algorithms keep one weight per expert spend o1 time per weight trial b introduction consider following online learning model learning occurs series trials labeled trial goal predict outcome 2 0 1 received end trial beginning trial algorithm receives ntuple x element x ti 2 0 1 ntuple x represents prediction expert e value outcome trial algorithm produces prediction based current expert prediction tuple x past predictions outcomes end trial algorithm receives outcome algorithm incurs loss measuring discrepancy prediction outcome similarly expert incurs loss well possible goal minimize total loss algorithm trials arbitrary sequence instance outcome pairs pairs called examples since make assumptions relationship prediction experts outcome always sequence authors supported nsf grant ccr9700201 extended abstract appeared herbster warmuth 1995 herbster k warmuth far away predictions particular algorithm thus minimizing total loss arbitrary sequence examples unreasonable goal refined relativized goal minimize additional loss algorithm loss best expert whole sequence experts large loss goal might actually easy achieve since algorithms additional loss loss best expert may small however least one expert predicts well algorithm must learn quickly produce predictions close predictions best expert sense additional loss algorithm loss best expert bounded expert framework might used various settings example experts might predict chance rain likelihood stock market rise fall another setting experts might various subalgorithms recognizing particular patterns master algorithm combines experts predictions need know particular problem domain simply keeps one weight per expert representing belief experts prediction decreases weight function loss expert previous work vovk vovk 1998 others littlestone warmuth 1994 haussler kivinen warmuth 1998 produced algorithm upper bound additional loss algorithm loss best expert algorithms compare loss best expert called staticexpert algorithms paper additional loss bounds algorithms form c ln n large class loss functions c constant depends loss function l n number experts class loss functions contains essentially common loss functions except absolute loss discrete loss 1 counting prediction mistakes treated special cases littlestone warmuth 1994 vovk 1995 cesabianchi freund haussler helmbold schapire warmuth 1997 example loss function square relative entropy loss respectively see section 2 definitions loss functions paper consider modification goal introduced littlestone warmuth littlestone warmuth 1994 sequence examples subdivided k segments arbitrary length distribution segment associated expert sequence segments associated sequence experts called partition loss partition sum total losses experts associated segment best partition size k partition k segments smallest loss modified goal perform well relative best partition size k goal model real life situations nature examples might change different expert produces better predictions example patterns might change different subalgorithms may predict better different segments online sequence patterns seek design master algorithms track performance best sequence experts sense incur small additional loss best partition size k whole sequence examples given ahead time one could compute best partition certain size associated experts using dynamic programming algorithms get examples online never produce best partition even able bound additional loss best offline partition arbitrary sequence examples trials k experts distinct partitions immediately get good bound problem expanding set n experts experts partitionexpert represents single partition trial sequence predicts trial expert associated segment contains current trial thus using staticexpert algorithm obtain bound c ln additional loss algorithm loss best partition two problems first algorithm inefficient since number partitionexperts exponential number partitions second bound additional loss grows sequence length able overcome problems instead keeping one weight exponentially many partitions get away keeping one weight per expert done staticexpert algorithm tracking predictions best partition essentially free n subalgorithms experts whose predictions want combine staticexpert algorithm new master algorithm takes additional time per trial time required simulating n subalgorithms develop two main algorithms fixedshare algorithm variable share algorithm based staticexpert algorithms maintain weight form e gammajt expert cf littlestone warmuth 1994 vovk 1995 total past loss expert past trials trial master algorithm combines experts predictions using current weights experts outcome trial received multiply weight every expert e gammajl l loss expert current trial call update weights loss update modify staticexpert algorithm adding additional update obtain algorithms since model best expert may shift series trials cannot simply use weights form e gammajt expert optimal segment loss prior segments may arbitrarily large thus weight may become arbitrarily small need modify staticexpert algorithm small weights recovered quickly reason expert shares portion weight experts loss update call share update fixedshare variableshare algorithm first loss update followed share update differs algorithm share update fraction experts weight added weight expert fixedshare algorithm experts share fixed fraction weights guarantees ratio weight expert total weight experts may bounded different forms lower bounding weights used wml algorithm companion paper learning shifting disjunctions auer warmuth 1998 appears journal issue latter two methods applied learning problems loss discrete 4 herbster k warmuth loss ie counting mistakes contrast methods work general class continuous loss functions staticexpert algorithms handle vovk 1998 haussler et al 1998 class includes common loss functions square loss relative entropy loss hellinger loss class tight bounds additional loss haussler et al 1998 algorithm loss best expert ie nonshifting case fixed share algorithm obtains additional loss ock1 log nk log k k essentially sketched algorithm uses staticexpert algorithm exponentially many partitionexperts salient feature fixedshare algorithm still uses o1 time per expert per trial however algorithms additional loss still depends length sequence lower bounds give partial evidence seems unavoidable loss functions loss single trial unbounded relative entropy loss case loss particular trial one square loss develop second algorithm called variable share algorithm algorithm obtains bounds additional loss independent length sequence also shares weights loss update however amount expert shares commensurate loss expert current trial particular expert loss share weight versions share update trivial implement cost constant amount time n weights although algorithms easy describe proving additional loss bounds takes care believe techniques constitute practical method tracking predictions best expert provable worstcase additional loss bounds essential ingredient success nonstationary setting seems algorithm stationary setting multiplicative weight update whose loss bound grows logarithmically dimension problem besides vovks aggregating algorithm vovk 1998 weighted majority algorithm littlestone warmuth 1994 use loss update basis work number algorithms developed examples algorithms learning linear threshold functions littlestone 1988 littlestone 1989 algorithms whose additional loss bound loss best linear combination experts sigmoided linear combination experts bounded kivinen warmuth 1997 helmbold kivinen warmuth 1995 significant progress recently achieved nonstationary settings building techniques developed paper see discussion conclusion section paper outlined follows preliminaries section 2 present algorithms section 3 give basic proof techniques section 4 sections 5 6 contain detailed proofs fixedshare variable share algorithms respectively absolute loss treated special case section 7 section 8 discusses subtle powerful generalization variable share algorithm called proximityvariableshare algorithm generalization leads improved bounds case best expert next segment always likely close previous expert preliminary lower bounds given section 9 simulation results artificial data exemplify methods given section 10 finally section 11 conclude discussion recent work casual reader might interested detailed proofs recommended read sections containing preliminaries section 2 algorithms section 3 simulations section 10 2 preliminaries let denote number trials n denote number experts labeled convenient simply refer expert index thus expert refers expert e prediction n experts trial referred prediction tuple x prediction expert trial denoted x ti experts may viewed oracles external algorithm thus may represent predictions neural net decision tree physical sensor perhaps even human expert outcome trial prediction algorithm trial instanceoutcome pair called tth example paper outcomes expert predictions predictions algorithm 0 1 throughout paper always denotes arbitrary sequence examples ie sequence elements 0 1 n theta 0 1 length loss function lp q function consider four loss functions paper square relative entropy hellinger absolute loss ent p hel p trial loss algorithm ly similarly loss expert trial ly x ti call subsequence contiguous trials segment notation nonnegative integers 0 denotes segment starting trial number ending trial 0 rounded parens used ending trial included segment current sequence abbreviate loss expert segment tt 0 ltt 0 loss algorithm whole trial sequence defined ls ready give main definition paper used scenarios best expert changes time informally kpartition slices sequence k segments expert associated segment formally kpartition denoted p nkte consists three positive integers n k two tuples e positive integers number length trial sequence n size expert pool k number target shifts tuple k elements refers one trials convention use 1 tuple divides trial sequence 6 herbster k warmuth parameters initialization initialize weights w ti predict loss update receiving tth outcome share updates three algorithms staticexpert ti share update fixedshare 4 variableshare 5 figure 1 staticexpert fixedshare variableshare algorithms called ith segment 0th segment also referred initial segment tuple e k1 elements element e denotes expert associated ith segment i1 loss given kpartition loss function l trial sequence 3 algorithms four algorithms considered paper staticexpert fixed share variableshare proximityvariableshare first three summarized figure 1 proximityvariableshare algorithm generalization variableshare algorithm algorithm given figure 3 discussion generalization deferred section 8 algorithms learning process proceeds trials 1 denotes trial number algo rithms maintain one positive weight per expert weight w ti normalized version v thought measurement algorithms belief quality ith experts predictions start trial weight expert initialized 1n algorithms following three parameters j c ff parameter j learning rate quantifying drastic first update parameter c set 1j loss functions absolute loss exception treated separately section 7 parameter ff quantifies rate shifting expected occur fixedshare algorithm designed potentially unbounded loss functions relative entropy loss variableshare algorithm assumes loss per trial lies 0 1 fixedshare al gorithm ff rate shifting per trial thus five shifts expected 1000 trial sequence 1200 variableshare algorithm ff approximately rate shifting per unit loss best partition five shifts expected occur partition total loss 80 ff 116 tunings parameters j c considered greater depth section 4 ff sections 5 6 finally staticexpert algorithm use parameter ff since assumes shifting occurs trial algorithm receives instance summarizing predictions n experts x algorithm plugs current instance x normalized weights v prediction function predv x order produce prediction simplest case algorithm predicts weighted mean experts predictions ie predv sophisticated prediction function introduced vovk vovk 1998 discussed section 4 predicting algorithm performs two update steps first update loss update second share update loss update weight expert multiplied e gammajl l loss ith expert current trial thus update occurs l learning rate j intensifies effect update use w ti denote weights middle two updates weights referred intermediate weights share update staticexpert algorithm vacuous however algorithms share update crucial briefly argue necessity share updates nonstationary setting give intuitive description function move predicting well best expert predicting well sequence experts loss update longer appropriate sole update assume two experts two segments first segment expert 1 small loss expert 2 large loss roles reversed second segment end first segment loss update caused weight expert 2 almost zero however second segment predictions expert 2 important weight needs recovered quickly share updates make sure possible simulation section 10 furthers intuition share updates needed two share updates summarized 8 herbster k warmuth straightforward implementation costs time per expert per trial fixedshare w ff variableshare w contrast implementations figure 1 use intermediate variable pool cost o1 time per expert per trial loss update every expert shares fraction weight equally every expert received weight enables expert recover weights quickly relative experts fixedshare update 6 expert shares fraction ff weight trial one expert perfect long segment type sharing optimal since perfect expert keeps sharing weight possible nonperfect experts variableshare update 7 sophisticated roughly expert shares weight loss large perfect expert doesnt share experts high loss eventually collect weight however perfect expert starts incur high loss rapidly begin share weight experts allowing good expert previously small relative weight recover quickly discussed parameter ff shifting rate introduction discussed algorithm uses exponentially many static experts one partition goal achieve bounds close inefficient algorithm using n weights bounds obtain share algorithms slightly weaker partitionexpert algorithm gracefully degrade neither length sequence number shifts k known advance 4 prediction functions proof techniques consider two choices prediction functions simplest prediction weighted mean warmuth 1997 pred wmean v sophisticated prediction function giving slightly better bounds introduced vovk vovk 1998 haussler et al 1998 define l 0 z functions must monotone let l gamma1 1 z denote inverses l 0 z l 1 z vovks prediction defined two steps pred vovk v loss c values j 1c functions pred wmean v x pred vovk v x ent p hel p q 1 1 figure 2 c 1crealizability c values loss prediction function pairings following definition technical condition relation prediction function predv x loss function l constants c j et al 1998 vovk 1998 loss function l prediction function pred c jrealizable constants c j total weight 1 consider four loss functions paper square relative entropy hellinger absolute loss see section 2 however algorithms limited loss functions techniques vovk 1998 haussler et al 1998 warmuth 1997 determine constants c j wide class loss functions algorithm also easy adapt classification using majority vote littlestone warmuth 1994 prediction function counting mistakes loss practical application worstcase loss bounds may provable given loss function however share updates may still useful interesting application prediction disk idle time see work helmbold et al helmbold long sherrod 1996 square relative entropy hellinger losses c jrealizable pred wmean pred vovk j 1c values c hence two prediction functions summarized figure 2 since absolute loss complex bounds treat section smaller value c leads smaller loss bound see lemma 1 c values pred vovk cf column two figure 2 optimal large class loss functions haussler et al 1998 proof loss bounds algorithms based following lemma lemma embodies key feature algorithms prediction done loss incurred algorithm tempered corresponding change total weight lemma gives inequality lemmas used vovk 1998 haussler et al 1998 proof essentially since share updates change total weight w ti haussler et al 1998 sequence examples expert total loss master algorithms figure 1 may herbster k warmuth bounded loss function l prediction function pred c jrealizable cf definition 1 figure 2 proof since l pred c jrealizable definition 1 since share updates change total weight ti w t1 implies hence since w far used basic technique littlestone warmuth 1994 vovk 1995 cesabianchi et al 1997 haussler et al 1998 ie c ln w becomes potential function amortized analysis static expert case 1c final weights form w n thus lemma leads bound relating loss algorithm loss static expert share updates make much difficult lower bound final weights intuitively sufficient sharing weights recover quickly however much sharing final weights low following sections bound final weights individual experts terms loss partition loss partition lp nkte sum sequence losses defined sequence experts partition expert accumulates loss segment bound weight using lemma 2 fixedshare algorithm lemma 7 variable share algorithm since partition composed distinct segments must also quantify weight transferred expert associated segment expert associated following segment done lemma 3 fixedshare algorithm lemma 8 variableshare algorithm lower bounds weights combined lemma 1 bound total loss fixedshare algorithm theorem 1 variableshare algorithm theorem 2 5 fixedshare analysis algorithm works unbounded loss functions total additional loss grows length sequence lemma 2 sequence examples intermediate weight expert trial 0 least e gammajltt 0 times weight expert start trial 0 formally proof combined loss fixedshare update equation 6 rewritten drop additive term produced share update apply iteratively trials tt 0 since bounding w weights trial 0 loss update weight trial 0 reduced factor e gammajly 0 x 0 therefore rt simple algebra definition lab bound lemma follows lemma 3 sequence examples weight expert start trial 1 least ff times intermediate weight expert j trial proof expanding fixedshare update 4 thus w ff done bound additional loss herbster k warmuth theorem 1 let sequence examples let l pred c j realizable kshift sequence partition p nkte total loss fixedshare algorithm parameter ff satisfies proof recall e k expert last segment lemma 1 bound w 1ek noting follows weight arbitrary partition expressed following telescoping product thus applying lemmas 3 2 final term w equals one since apply share update final trial therefore definition lp nkte e gammajlp nkte substitute bound w 1ek 16 simplify obtain 15 bound theorem 1 holds k tradeoff terms ck ln n cjlp nkte ie k small ck ln n term small cjlp nkte term large viceversa optimal choice ff obtained differentiating bound theorem 1 ff following corollary rewrites bound theorem 1 terms optimal parameter choice ff corollary gives interpretation theorems bound terms code length introduce following notation let 1gammap binary entropy measured nats 1gammaq binary relative entropy nats 2 corollary 1 let sequence examples let l pred c j realizable kshift sequence partition p nkte total loss fixedshare algorithm parameter ff satisfies ck ff bound becomes interpretation bound ignore constants c j difference nats bits terms ln n k lnn gamma 1 account encoding experts partition log n bits initial expert logn gamma 1 bits expert thereafter finally need encode k shifts occur inner boundaries partition ff interpreted probability shift occurs gamma1 trials term gamma1 hff dff kff corresponds expected optimal code length see chapter 5 cover thomas 1991 code shifts estimate ff instead true probability ff bound thus example close similarity prediction coding brought many papers eg feder merhav gutman 1992 note ff minimizes bound theorem 1 depends k unknown learner practice good choice ff may determined experimentally however upper bound lower bound k may tune ff terms bounds corollary 2 let sequence examples k positive integers 1 setting 1 loss fixed share algorithm bounded p nkte partition k k proof recall loss bound given theorem 1 setting separate term apply inequality last inequality follows condition obtain bound corollary replacing equation 20 upper bound k 3 14 herbster k warmuth 6 variableshare analysis variableshare algorithm assumes loss expert per trial lies 0 1 hence variableshare algorithm works combination square hellinger absolute loss functions relative entropy loss function variableshare algorithm upper bound additional loss algorithm independent length trial sequence abbreviate w ti w ti since section need refer weight expert middle trial first give two technical lemmas follow convexity r fi r c applying first inequality lemma 4 rhs c db b 1gammac thus lemma 6 beginning trial 1 may lower bound weight expert either expression expression b j expert different ae w ti e gammajly x ti proof expanding loss update variableshare update trial cf 7 expression obtained dropping summation term expression b drop one summand second term w apply lemma 4 obtain b lemma 7 weight expert start trial start trial 0 reduced factor e gammaj theta proof lemma 6a trial weight expert reduced follows w t1i apply iteratively rt theta lemma 6b lower bound weight transferred expert p expert q single trial next lemma show weight transferred sequence trials lemma 8 distinct experts p q ltt 0 2 trial may lower bound weight expert q theta e gammaj proof expert p accumulates loss trials tt 0 transfers part weight specifically expert q via variableshare update let 0 denote weight transferred expert p expert q trial denote total weight transferred expert p expert q trials tt 0 transferred weight however still reduced function loss expert q successive trials lemma 7 weight added trial reduced factor e gammaj theta lower bound factor e gammaj e gammaj thus theta e gammaj complete proof lemma still need lower bound total transferred weight w tp ff l loss expert p trial ie assumption 1 2 direct application lemma 6b weight transferred expert p expert q first trial segment least w tp ff l e gammajl likewise apply lemma 7 trials ti expert p apply lemma 6b trial gives us lower bound transferred weights total transferred weight ff ff l herbster k warmuth split last sum two terms ff l ff upper bound exponents 1 gamma ff one also replace sum first exponent upper bound substitutions 1 lead application lemma 5 thus rewrite inequality ff theta ff apply lemma 5 gives us ff proof loss bound variableshare algorithm proceeds analogously proof fixedshare algorithms loss bound cases follow weight sequence experts along sequence segments within segment bound weight reduction expert lemma 2 fixedshare analysis lemma 7 variableshare analysis pass one segment next bound weight expert corresponding new segment weight expert former segment lemmas 3 8 respectively former lemma used fixed share algorithm simple since trial expert always shared fixed fraction weight however since weight shared every trial produced bound dependent sequence length variableshare algorithm produce bound independent length accomplished expert sharing weight accordance loss however expert accumulate significant loss cannot use lemma 8 bound weight following expert terms previous expert nevertheless former expert make significant loss current segment implies may bound current segment former expert collapsing segments together words collapsing two consecutive segments creates single segment associated expert first segment original two consecutive segments segment thus determine bound terms related collapsed partition whose loss much worse lemma 9 partition p nkte exists collapsed partition p nk 0 segment except initial segment expert associated prior segment incurs least one unit loss loss whole sequence collapsed partition exceeds loss original partition following properties hold proof recall e expert associated ith segment comprised trials i1 segment loss expert e associated prior segment gamma 1 less one merge segment segment combined segment new partition associated expert e igamma1 formally iteration decrement k one delete e tuples e continue 24 holds bound loss collapsed partition p nk 0 noting loss new expert subsumed segment one thus per application transformation loss increases one thus since applications done theorem 2 4 let sequence examples let l pred c jrealizable let l 01 range partition p nkte total loss variableshare algorithm parameter ff satisfies proof lemma 1 let p nkte arbitrary partition proof need property loss segment except initial segment regard expert associated prior segment least one cf 24 property hold use lemma 9 replace p nkte collapsed partition p nk 0 property hold property holds already p nkte notational convenience refer p nkte p nk 0 recall loss exceeds loss p nkte since 24 holds exists trial q ith segment 1 lt 0 1 express w 1e 0 telescoping product applying lemmas 7 8 ii ff herbster k warmuth simplifies following bound ff last inequality follows 25 thus substitute bound simplify obtain bound theorem cannot optimize upper bound function ff since k lp nkte known learning algorithm tune ff based upper bound lp nkte approach used corollary 2 5 corollary 3 let sequence examples l k positive reals setting l loss variableshare algorithm bounded follows ck p nkte partition lp nkte l addition l partition p nkte lp nkte l obtain upper bound ck proof proceed upper bounding three terms containing ff bound theorem 2 use rewrite apply identity ln1 x bound lp nkte l giving following upper bound previous expression l l therefore upper bounded using expression upper bound equation 29 obtain equation 27 l upper bound equation 29 first term bounded 1 k second term 2 k k ln 9 2 region thus upper bounded use expression upper bound equation 29 gives us equation 28 done 7 absolute loss analysis absolute loss function l abs p jrealizable prediction functions pred vovk pred wmean however cj 1 thus tuning complex sake simplicity use weighted mean prediction littlestone warmuth 1994 section theorem 3 littlestone warmuth 1994 1 absolute loss function l abs p jrealizable prediction function pred wmean v x obtain slightly tighter bound could also used vee algorithm absolute loss 2 jrealizable haussler et al 1998 algorithm takes log n time produce prediction weighted mean vee prediction allow outcomes lie 0 1 binary outcomes absolute loss time prediction functions exist realizability criterion vee prediction vovk 1998 cesabianchi et al 1997 unlike c 1crealizable loss functions discussed earlier cf figure 2 absolute value loss constant parameters thus must tuned practice tuning j may produced numerical minimization upper bounds however use tuning j produced freund schapire theorem 4 lemma 4 p q q herbster k warmuth use tuning bound variableshare algorithm theorem 2 theorem 5 let loss function absolute loss let sequence examples l k positive reals k k lp nkte l k l set two parameters variableshare algorithm ff j k respectively k k loss algorithm weighted mean prediction bounded follows alternatively let l k positive reals k k lp nkte l k l set two parameters variableshare algorithm ff j k respectively k k loss algorithm weighted mean prediction bounded follows 8 proximityvariableshare analysis section discuss proximityvariableshare algorithm see figure 3 recall variableshare algorithm expert shared fraction weight dependent loss trial fraction shared uniformly among remaining experts proximityvariableshare algorithm enables expert share nonuniformly experts proximityvariableshare update costs per expert per trial instead o1 see figure 3 algorithm allows us model situations prior knowledge likely pairs consecutive experts let us consider parameters algorithm ntuple contains initial weights algorithm ie w parameters initialization initialize weights w n ti predict loss update receiving tth outcome proximityvariableshare update figure 3 proximityvariableshare algorithm second additional parameter besides j c complete directed graph size n without loops edge weight jk fraction weight shared expert j expert k naturally vertex outgoing edges must nonnegative sum one 0 probability distribution prior initial expert probability distribution prior expert follow expert j upper bound proximityvariableshare algorithm fixed share algorithm could generalized similarly take proximity account theorem 6 let sequence examples let l pred c jrealizable let l 01 range partition p nkte total loss proximityvariableshare algorithm parameter ff satisfies proof omit proof bound since similar corresponding proof theorem 2 variableshare algorithm change 1 fractions replaced corresponding parameters note setting gives previous bound variableshare algorithm theorem 2 case last sum ok ln n accounting code length names best experts except first one using proximityvariableshare algorithm get last sum ok cases 22 herbster k warmuth simple example assume processors circular list two processors distance processor iid mod 1d 2 next best expert always constant away previous one last sum becomes ok course notions closeness choices parameters might suitable note price decreasing last sum update time 2 per trial however expert arrows end labeled value share update proximityvariableshare algorithm still 9 lower bounds upper bounds fixedshare algorithm grow length sequence additional loss algorithm loss best kpartition approximately holds unbounded loss functions relative entropy loss restricting loss lie 0 1 variableshare algorithm gives additional loss bound approximately loss best kpartition k l one natural question whether similar reduction possible unbounded loss functions words whether unbounded loss function bound form possible replaced minf lg give evidence contrary give adversary argument forces algorithm make loss best onepartition adversary sets section limit giving construction easily extended adversary forces lnnln gamma log 2 n additional loss best onepartition n experts iterating adversary may force additional loss best kpartition assume log 2 n gamma 1 positive integers theorem 7 relative entropy loss exists example sequence length two experts lp 21te partition single shift loss 0 furthermore algorithm proof adversarys strategy described figure 4 use denote prediction arbitrary learning algorithm l loss trial convenience number trials two experts one always predicts 0 always predicts 1 adversary returns sequence 0 outcomes followed sequence 1 outcomes neither sequence empty thus single shift best partition partition loss 0 2 trial 2 1 otherwise assume without loss generality 2 thus 3 new trial 4 gammat 5 else go step 7 go step 3 7 let remaining trials exit figure 4 adversarys strategy prove ls thus proving lemma clearly loss generality assume note threshold 1 gammat furthermore l ent 0 1 l ent 1 1 thus conditions 4i 5i follow condition 4ii holds simple induction shift occurs condition 5ii holds since condition 4ii gammat therefore add l least ln gamma condition 5i obtain condition 5ii done step 5 never executed shift occurs last trial step 6 skipped thus step 5 never executed trial condition 4ii bound lemma first reason lower bound tight showing upper bounds algorithms discussed paper close lower bound number partitions 1 thus may expand set experts partitionexperts discussed introduction using staticexpert algorithm weighted mean prediction gives upper bound total loss algorithm loss best partition zero matches lower bound second bound fixedshare algorithm cf corollary 1 larger lower bound gamma2 additional term may upper bounded 1 herbster k warmuth total loss algorithms trials loss variable share algorithm loss static algorithm vovk loss fix share algorithm loss typical expert loss best partition k3 variable share loss bound fix share loss bound figure 5 loss variableshare algorithm vs staticexpert algorithm scaled weights figure 6 relative weights variableshare algorithm 10 simulation results section discuss simulations artificial data simulations mainly meant provide visualization algorithms track predictions best expert seen empirical evidence practical usefulness algorithms believe merits algorithms clearly reflected strong upper bounds prove theorems 8000103050709scaled weights trials vovk relative weights figure 7 relative weights staticexpert algorithm earlier sections simulations show loss algorithm typical sequence examples bounds paper worstcase bounds hold even adversariallygenerated sequences examples surprisingly losses algorithms simulations random sequences close corresponding worstcase bounds proven paper thus simulations show loss bounds tight sequences compared performance staticexpert algorithm two share algorithms following setting chose use square loss loss function widespread use task tuning learning rate loss function simple used vovk prediction function cf equation 9 chose accordance figure 2 considered sequence 800 trials four distinct segments beginning trials 1 201 401 601 trial outcome 0 prediction tuple contained predictions 64 experts generated predictions 64 experts chose different expert best one segment best experts always expected loss 1120 per trial 63 experts expected loss 112 per trial end segment new best expert chosen since outcome always 0 generated expected losses sampling predictions uniform random distribution 0 1 typical best experts respectively thus expected loss best 6 partition denoted segment boundaries 800 variance oe 2 044 actual loss best partition particular simulation used plots 647 fixedshare algorithm tuned f based values using ff f 26 herbster k warmuth tuning suggested corollary 1 variableshare algorithm tuned ff v based values using ff v tuning suggested corollary 3 using theorems 1 2 calculated worst case upper bound loss fixedshare algorithm variableshare algorithm 2489 2150 respectively see theta marks figure 5 simulations artificial data show worstcase bounds rather tight even simple artificial data many heuristics finding suitable tuning used tunings prescribed theorem noticed types simulations results relatively insensitive tuning ff example calculating ff v variableshare algorithm overestimated 10 standard deviations loss bound algorithm increased 002 actual loss algorithm simulation increased 017 figure 5 plotted loss staticexpert algorithm versus loss two share algorithms examination figure shows first segment staticexpert algorithm performed comparably share algo rithms however remaining three segments staticexpert algorithm performed poorly loss essentially bad loss typical expert slope total loss typical expert staticexpert algorithm essentially later segments share algorithms performed poorly beginning new segment however quickly learned new best expert current segment share algorithms loss plateaued almost slope slope total loss best expert two share algorithms qualitative behavior even though fixedshare algorithm incurred approximately 10 additional loss variableshare algorithm simulations tried learning rates j slightly smaller two verified even choices learning rates total loss staticexpert algorithm improve significantly figures 6 7 plotted weights normalized weight vector w maintained variableshare algorithm staticexpert algorithm trial sequence figure 6 see variableshare algorithm shifts relative weights rapidly latter part segment relative weight best expert almost one corresponding plot fixedshare algorithm similar hand see figure 7 staticexpert algorithm also learned best expert segment 1 however staticexpert algorithm unable shift relative weight sufficiently quickly ie takes length second segment partially unlearn best expert first segment relative weights best experts segments one two essentially perform random walk third segment final segment relative weight best expert segment three also performs random walk summary see simulations evidence fixedshare variableshare updates necessary track shifting experts 11 conclusion paper essentially gave reduction multiplicative update algorithm works well compared best expert arbitrary segments examples algorithm works well compared best partition ie concatenation segments two types share updates analyzed fixed share algorithm works well loss function unbounded variableshare algorithm suitable case range loss lies 01 first method essentially one used wml algorithm littlestone warmuth 1994 recent alternate developed auer warmuth 1998 learning shifting disjunctions loss discrete loss classification problems methods simple effective algorithm updates mistake occurs ie conservative updates second method variableshare update sophisticated particular one expert predicts perfectly collect weight however expert starting incur large loss shares weight experts helping next best expert recover weight zero methods presented littlestone warmuth 1994 inspired number recent papers auer warmuth 1998 adapted winnow algorithm learn shifting disjunctions comparing best shifting disjunction complicated comparing best expert however since classification problem simple sharing update similar fixedshare update sufficient focus paper track prediction best expert class loss functions original staticexpert algorithm vovk developed vovk 1998 haussler et al 1998 share updates applied experimentally predicting disk idle times helmbold et al 1996 online management investment portfolios singer 1997 addition reduction shown expert metrical task systems algorithms blum burch 1997 share update used successfully new domain metrical task systems natural probabilistic interpretation share algorithms recently given vovk 1997 particular application share algorithms necessary consider choose parameter ff theoretical techniques exist fixedshare algorithm eliminating need choose value ff ahead time one method tuning parameters among things specialist framework freund schapire singer warmuth 1997 even though bounds produced way always optimal another method incorporates prior distribution possible values ff sake simplicity discussed methods herbster 1997 vovk 1997 singer 1997 paper 28 herbster k warmuth acknowledgments would like thank peter auer phillip long robert schapire volodya vovk valuable discussions also thank anonymous referees helpful comments notes 1 discrete loss defined ae 2 note lent p q use dpkq notation customary information theory 3 replace assumption k k 2 k obtain bound final term c k replaced 2c k 4 vovk recently proved sharper bound algorithm vovk 1997 ff 5 unlike corollary 2 need lower bound k 6 call partition described segment boundaries 1 201 401 601 best partition respect tradeoff k lp nkte expressed implicitly theorem 2 r tracking best disjunction use expert advice elements information theory universal prediction individual sequences ieee transactions information theory decisiontheoretic generalization online learning application boosting using combining predictors specialize sequential prediction individual sequences general loss functions dynamic disk spindown technique mobile computing tracking best expert ii additive versus exponentiated gradient updates linear prediction learning irrelevant attributes abound new linearthreshold algorithm mistake bounds logarithmic linearthreshold learning algorithms phd thesis weighted majority algorithm towards realistic competitive portfolio selection algorithms game prediction expert advice derandomizing stochastic prediction strategies predicting dotproduct experts framework tr ctr atsuyoshi nakamura learning specialist decision lists proceedings twelfth annual conference computational learning theory p215225 july 0709 1999 santa cruz california united states jeremy z kolter marcus maloof using additive expert ensembles cope concept drift proceedings 22nd international conference machine learning p449456 august 0711 2005 bonn germany v vovk probability theory brier game theoretical computer science v261 n1 p5779 06172001 peter auer manfred k warmuth tracking best disjunction machine learning v32 n2 p127150 aug 1998 olivier bousquet manfred k warmuth tracking small set experts mixing past posteriors journal machine learning research 3 312003 avrim blum carl burch online learning metrical task system problem machine learning v39 n1 p3558 april 2000 chris mesterharm tracking linearthreshold concepts winnow journal machine learning research 4 1212003 peter auer using confidence bounds exploitationexploration tradeoffs journal machine learning research 3 312003 giovanni cavallanti nicol cesabianchi claudio gentile tracking best hyperplane simple budget perceptron machine learning v69 n23 p143167 december 2007 marco barreno blaine nelson russell sears anthony joseph j tygar machine learning secure proceedings 2006 acm symposium information computer communications security march 2124 2006 taipei taiwan wei yan christopher clack diverse committees vote dependable profits proceedings 9th annual conference genetic evolutionary computation july 0711 2007 london england mark herbster manfred k warmuth tracking best regressor proceedings eleventh annual conference computational learning theory p2431 july 2426 1998 madison wisconsin united states claudio gentile robustness pnorm algorithms machine learning v53 n3 p265299 december mark herbster manfred k warmuth tracking best linear predictor journal machine learning research 1 p281309 912001 amol deshpande zachary ives vijayshankar raman adaptive query processing foundations trends databases v1 n1 p1140 january 2007