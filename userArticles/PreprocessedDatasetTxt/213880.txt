stability linear equations solvers interiorpoint methods primaldual interiorpoint methods linear complementarity linear programming problems solve linear system equations obtain modified newton step iteration linear systems become increasingly illconditioned later stages algorithm computed steps often sufficiently accurate useful use error analysis techniques tailored special structure linear systems explain observation examine theoretically superlinear convergence pathfollowing algorithm affected roundoff errors b introduction monotone linear complementarity problem lcp problem finding vector pair x 2 r l n theta r l n 1 real n theta n positive semidefinite matrix q real vector n given note need symmetric well known 1 includes linear programming problem special case specifically linear programming formulation min z c z subject az b z 0 2 2 r l mthetap introduce dual variable l constraint obtain following necessary sufficient conditions optimality primaldual pair z z c gammab z 3a z 0 appropriate definitions q 3 form 1 little lost either practical theoretical point view applying interiorpoint algorithms 1 special cases linear convex quadratic programming provided special structure problem exploited solution linear systems iteration interiorpoint methods 1 generate sequence iterates strictly positive many methods require linear system form u work based research supported office scientific computing us department energy contract w31109eng38 mathematics computer science division argonne national laboratory 9700 south cass avenue argonne il 60439 solved search direction u v iteration affinescaling methods solve 4 find search direction step fraction distance along direction boundary nonnegative orthant defined x 0 affine scaling steps u v simply newton steps system nonlinear equations pathfollowing methods see example monteiro adler 10 zhang 19 wright 14 generate steps using generally positive values oe 4 see later algorithm 14 allows iterates attempt attain rapid local convergence associated newtons method potentialreduction methods see example kojima mizuno yoshise 6 kojima kurita mizuno 5 also determine search directions solving systems like 4 refer logarithmic potential function decide far move along computed direction predictorcorrector methods see example ye anstreicher 18 ji potra huang 4 potra 12 take steps either system 4 highly structured since diagonals x strictly positive rearrange system obtain 6a e case linear programming 2 equation 6a contains even structure form u z positive diagonal matrices matrix 7 made symmetric indefinite multiplying first block row gamma1 system reduced even eliminating either u z u instance u z eliminated obtain z z interiorpoint codes linear programming use formulation 8 modifications handling dense columns dealing nonstandard linear programming formulations see lustig marsten shanno 7 8 xu hung ye 17 codes notably fourer mehrotra 1 vanderbei 13 handle formulation 7 analysis algorithms formulations discussed another preprint 16 paper focus system arising general monotone lcp 6 analyze behavior gaussian elimination pivoting applied system since index least one x zero solution would expect diagonal elements x gamma1 approach zero approach 1 solution set approached hence coefficient matrix 6a tends become increasingly illconditioned later stages algorithm standard error analysis linear systems might therefore expect linear equations interiorpoint methods 3 rounding errors step u v make useless advancing algorithm towards convergence paper show theoretical superlinear properties suggested exact analysis generally observed implementations algorithms still exhibit rapid convergence parameters set appropriate values particular pathfollowing infeasibleinteriorpoint algorithm strong theoretical convergence properties conclusions presented section 4 confirmed computational experiments section 5 section 3 lays groundwork deriving bounds rounding errors computed values u v wide class algorithms includes algorithm sections 4 5 section 2 presents assumptions fundamental result error analysis linear systems arise logarithmic barrier methods constrained optimization methods analyzed ponceleon 11 newton equations logarithmic subproblem similar 6a large elements occur diagonal despite apparent illconditioning systems ponceleon showed sensitivity structured perturbations certain class governed conditioning underlying problem depend current value barrier parameter ponceleons analysis somewhat different section 3 looks relative error components solution rather starting absolute error conclusions consistent obtained section 3 subsequent sections subscripts denote components vector iteration indices usually appear subscripts scalars superscripts vectors matrices sets b n form partition index set f1 ng defined assumption 1 x 2 r l matrix 2 r l nthetan similarly mbb mnb mnn given matrix define denote jth column h h deltaj notation k delta k denotes 1 2 1norm vector matrix delta denotes corresponding condition number two nonnegative numbers moderate constant w matrix vector write denote use indicate use u denote unit roundoff define implicitly statement x two floating point numbers op denotes gamma theta flz denotes floating point representation real number z see golub van loan 2 section 242 assume throughout u small enough ou 1 odelta order notation defined 2 assumptions basic results remainder paper focus pathfollowing interiorpoint methods infeasible variants methods among widely used practical algorithms linear programming lcps 7 8 also subject extensive theoretical investigations shown strong convergence properties weak assumptions 19 15 14 pathfollowing infeasibleinteriorpoint framework stated also includes class predictorcorrector methods appropriate choices initial point parameters pathfollowing algorithms restrict iterates neighborhoods denotes nonnegative orthant r l n iterates generated algorithms lie n fl min fl min 2 0 12 constant quantities needed define general algorithm include define algorithmic framework follows algorithm pfi given choose oe k 2 0 1 find gammai choose decrease kr k k iteration linear fact r last condition algorithm pfi equivalent hence kr k infeasibility always bounded multiple complementarity gap initial point feasible r predictorcorrector algorithms ji potra huang 4 special cases algorithm pfi framework also includes infeasibleinteriorpoint algorithms zhang 19 wright 15 14 algorithms choose fl k1 oe k step ff k nontrivial length always taken without violating required conditions practical implementations interiorpoint methods framework algorithm pfi usually modified slightly linear programming codes different step lengths usually chosen primal dual components x experience shown strategy tends reduce number iterations slightly moreover explicit membership neighborhood 10 usually enforced strategy supporting theory find largest value linear equations interiorpoint methods 5 ff 0 1 keeps nonnegative orthant choose ff k fixed fraction length predictorcorrector strategy mehrotra 9 used also codes lustig marsten shanno 8 vanderbei 13 xu hung ye 17 adds extra terms lower part righthand side corrector iterations nevertheless coefficient matrices used practical algorithms 11 conclusions accuracy computed steps continue hold minor modifications analysis section 3 analysis make following assumptions data problem 1 solution set assumption 1 problem 1 unique solution strict complementarity holds define associated partition b n index set ng x b quantities moderate size assumption 1 implies coefficient matrix 11 approaches nonsingular limit since 2n theta 2n permutation matrices p pi x gammai mbn mbb 0 submatrices diagonal 13 nonsingular problem 1 derived linear program 3 existence solution implies existence strictly complementary solution however special case general case symmetric positive semidefinite uniqueness solution wellconditioning mbb often satisfied practice assumption 1 quite strong see however assumption plays important role showing errors computed solutions disastrous interiorpoint algorithm wellconditioning square coefficient matrix linear system needed ensure relative errors computed version z large computational experience section 5 tends indicate assumption 1 necessary well sufficient rapid local convergence algorithm make one assumption iterates generated basic algorithm assumption 2 iterates generated algorithm pfi satisfy lim course necessary make assumption reasonable instance algorithm pfi since convergence solution one properties implied particular schemes choosing oe k ff k fl k make assumption merely divorce error estimates next section particular variant algorithm pfi implicit function theorem nonsingularity matrix 13 equation 11 kr k 6 stephen j wright also following simple result lemma 21 positive constants c 1 c 2 k sufficiently large 15a proof assumption 2 define index k positive constants c 2 therefore since also therefore 15a holds proof 15b similar finally state result roundoff error analysis gaussian elimination reference next section theorem 22 suppose theta linear system solved using gaussian elimination possibly row andor column pivoting let us denote row permutation matrix p column permutation matrices pi computed unit lower triangular factor l computed upper triangular factor u computed solution z solves perturbed system u proof follows immediately theorem 64 higham 3 gaussian elimination size largest element column remaining submatrix may grow multiples pivot rows added later rows matrix quantify growth growth factor ae defined smallest positive number following simple corollary theorem 22 corollary 23 let system theorem 22 suppose pivots u jj chosen j linear equations interiorpoint methods 7 3 error bounds steps section derive estimates difference step actually computed solving 6 denote corresponding exact values denoted u v treat cases u v determined gaussian elimination row partial pivoting complete pivoting start purely technical result lemma 31 let g square matrix partitioned g g 11 g 22 also square suppose g 11 g 22 gamma g nonsingular g nonsingular g gamma1 form first main result concerning components u following theorem 32 let u computed applying gaussian elimination row partial pivoting 6a suppose growth factor ae large sufficiently small proof assume much smaller quantities assumption 1b 1 retain lowestorder terms u analysis since assumptions higher order terms small enough absorbed lowerorder terms minor perturbations coefficients theorem 22 18 permuting rows columns 6a un 20a 20c 20d assumption 1b lemma 21 kmbb k kmbn k kmnb k kmnn k o1 combining observations 20 obtain 8 stephen j wright therefore 19 rewritten ebn ub e denote coefficient matrix 21 g g ebb assumption 1b lemma 31 since since kxn gamma1 15a theta theta hence substitution lemma 31 together assumption 1b manipulation yields exact arithmetic roundoff errors calculation restrict us assuming ou using fact together assumption 1b formula 14 lemma 21 therefore 21 required next result bounds difference u u linear equations interiorpoint methods 9 theorem 33 suppose assumptions theorem 32 hold sufficiently small 23a 2 n 24a proof expression 23a follows immediately 14 theorem 32 since 23b note first 6 21 enn hence 14 22 23a assumption 1b theorem 32 15a 23b proved 4 therefore x o1 also assumption 2 14 2 n v hence 24a obtained using estimates 25 24b 15a 23b 24a 2 n note theorems 32 33 ignored possible errors introduced computation formation righthand side b vectors r x scalars oe since formation process introduces relative perturbation ou component b lose nothing ignoring perturbations turn recovery step v exact formula 6b actual computation v computed value u available us moreover errors introduced five six floatingpoint operations righthand side 26 performed exact nature errors depend order operations 26 performed two possibilities suggested parentheses expressions gammay however perform analysis takes possibilities account show following theorem theorem 34 suppose assumptions theorem 33 hold v computed formula 6b equivalently 26 u replacing u 27a 2 b 28a proof formulae proof use notation represent scalar quantities order u certainly proof recall relative errors ou incurred whenever real number approximated floatingpoint number arithmetic operation involving two floatingpoint numbers performed cf 9 therefore regardless order operations required recover performed gammay rearranging combining terms 29 obtain substituting 26 obtain consider first case 2 b 30 together assumption 2 theorem 32 expressions 15 23a ou proving 27a 2 n assumption 2 theorem 32 expressions 23b linear equations interiorpoint methods 11 therefore obtain 30 required inequalities 28 proved way 24 gaussian elimination complete pivoting possibly relevant practical algorithms since sparse elimination algorithms rearrange rows columns hence regarded approximations complete pivoting strategy main error results complete pivoting partial pivoting justify claim note first nonbasic indices eventually used pivots basic indices used large sizes moreover error matrices ebn enn ou rather elements cannot appear pivot row except diagonal cannot contaminate elements nonbasic columns x gamma1 words actually solves system un 32a 32c 32d defining g coefficient matrix 31 partitioning obtain manipulation therefore using kb o1 error result one obtained partial pivoting theorem 32 results section also hold complete pivoting minor modifications proofs 4 effect roundoff error local convergence consider algorithm 14 described follows given parameters fl k1 2 1 step ff k chosen subject 34a ff 2 0 ff k choices oe k fl k1 fi k iteration made according following scheme fl find k 11 33 34 accept step k1 next k find k 11 33 34 accept step k1 k go next k algorithm takes two types steps safe steps oe k oe fast steps oe theoretically safe steps ensure good global convergence properties complexity fast steps ensure asymptotic superlinear conver gence counter k keeps track number fast steps taken prior iteration k choice step length ff k ensures kr k1 see note condition 34b since fikr condition 12 holds linear equations interiorpoint methods 13 focus algorithm strong theoretical properties namely global convergence positive starting point properly initialized superlinear local convergence also method performs well computational tests quite similar least nonsuperlinear phase oe k oe algorithm implemented lustig marsten shanno 8 assume throughout finite termination occur algorithm generates infinite sequence strictly positive iterates section examine behavior algorithm affected computed steps u used place exact steps start showing nearunit steplengths eventually taken algorithm without violating positivity condition consequently exists possibility rapid convergence sequence complementarity gaps k zero even presence roundoff error refine results show safe steps oe actually ff sufficiently small analysis convention use iteration index k statement result omit proofs lemma 41 k sufficiently large ff 2 0 ff k proof consider first indices 2 n 27b assumption 2 large k k ff 2 0 1 k sufficiently large hand 24b therefore x must hence ff satisfying 35 case 2 b proved similar way using 28b show nearunit steps produce fast linear convergence complementarity gap zero theorem 42 k sufficiently large proof second part equation 4 14 stephen j wright assumption 2 relations 15a 27 similar result holds jy u j last term 36 14 27 theorem 32 also x using estimates 36 obtain summing obtain yields desired result examine safe steps oe k oe show ff criteria 33 34 large enough k even computed search direction used place exact direction unit step taken theorem 43 suppose oe substantially larger u sense defined sufficiently large k safe step oe k oe computed step length parameter satisfying 33 34 proof 38 therefore 34b hold ff 2 0 1 u v replaced u v fi provided term square brackets 39 nonnegative nonnegativity guaranteed u oe sufficiently small ff inequality consider 34a 37 38 positive constants c 34a satisfied equivalently oe linear equations interiorpoint methods 15 last inequality therefore 34a holds provided u small enough respect oe assumed finally show decreasing interval ff 2 0 1 since 38 ff derivative nonpositive provided assumptions first part proof oe dominates 41 holds shown ff 2 0 1 satisfy conditions 34 u v replaced u v moreover function 33 decreasing interval conclude ff step chosen line search procedure giving result turn fast step oe exact analysis wright 14 shows fast steps eventually always taken algorithm note fast step accepted following theorem gives estimate length fast step theorem 44 fast step attempted iteration k k sufficiently large u 1 proof 37 oe 38 putting 43 44 together deduce 34a holds provided turn true last inequality implied following bound ff used 42 derive final inequality using 44 inequality 34b satisfied providing show decreasing 0 1 45 46 result holds defined however k ff certainly decreasing 0 1 result proved result accurately indicates behavior fast steps later iterations algorithm quantity j k typically either extremely small else quite significant omegagamma1699 depending sign certain products j k tiny value ff k close 1 fast step accepted large reduction j k larger ou fast step may lead large decrease may even rejected favor safe step summarize results section following theorem theorem 45 suppose u much smaller oe sense theorem 43 sufficiently large k either fast step taken ii safe step taken proof condition faststep acceptance yields 47 estimate 48 follows theorems 42 44 identity oe safestep estimate 49 follows theorem 42 use ff linear equations interiorpoint methods 17 5 computational results algorithm section 4 implemented doubleprecision fortran using lapack routines dgetrf dgetrs solve linear system 6a test problems two types matrix form dense elements drawn uniform distribution gamma1 1 diagonal diagonal elements form 10 drawn uniform distribution 0 1 since choose n r n rank n r rank deficiency artificial feature certain applications 1 including 3 structurally rankdeficient solutions x chosen evennumbered components x oddnumbered components zero remaining components uniform ii matrix form 3a matrix dense elements form drawn uniform distributions respectively p denote dimensions z respectively choose evennumbered components z nonzero consequence nondegeneracy solution 2 number components z nonzero requirement also necessary nonsingularity mbb nonzero components z complementary vector pair 3 drawn 0 1 use following values constants choose centering parameter oe k safe iteration according formula though made special effort tune constants optimal values experience indicates choices 50 51 efficient types problems tables 14 tabulate behavior algorithm section 4 many uninteresting safe iterates omitted asterisk last column indicates fast step taken iterate terminate algorithm falls tables indicate rapid convergence algorithm final stages typically algorithm takes fast steps decreased certain threshold experience author others indicates threshold quite small linear quadratic problems superlinear convergence set quite late process preliminary experience nonlinear problems indicates fast steps typically taken earlier stage threshold small behavior observed tables 14 certainly confirms efficacy gaussian elimination partial pivoting context interiorpoint method linear algebra continues produce good steps even extremely small convergence zero appears superlinear case even quadratic case table 4 tables however show asymptotic behavior suggested theorem 45 see must continue run algorithm past point convergence table 5 shows happens continue iterate problem table 3 reduced 10 gamma100 late asymptotic table convergence algorithm problem type fast step 43 32 33 3 27 117 19 95 132 table convergence algorithm problem type fast step 41 55 4 33 98 22 67 116 linear equations interiorpoint methods 19 table convergence algorithm problem type fast step 43 58 41 55 4 32 39 26 44 117 28 67 117 29 81 117 table convergence algorithm problem type ii fast step 3 31 42 4 21 31 6 14 22 9 table later iterates problem table 3 fast step 34 421 119 36 509 118 42 1024 119 terminate convergence qualitatively similar problems tried report one instance note fast steps taken iteration decrease iteration 38th safe step taken decrease ratio almost exactly oe existence two kinds steps effects k close accord predictions theorem 45 note tables residual norm kr k k decreases ou discussed proof theorem 32 behavior due roundoff error calculation r k via formula r experimented version code modified complete pivoting strategy used solving 6a columns coefficient matrix ordered decreasing value k delta k1 gaussian elimination partial pivoting applied asymptotically strategy effect ordering nonbasic columns first analysis end section 3 still applies predicted analysis version algorithm behaves slightly differently partial pivoting version described assumption mbb nonsingular indeed well conditioned plays important role analysis sections 3 4 theoretically algorithm section 4 known fast local convergence even mbb singular solution unique tested see whether fast convergence attainable practice forming problem class n contains 50 indices submatrix mbb certainly rank deficient result run summarized table 6 clear behavior indicated theorem 45 occur taking two fast steps converging iteration 14 algorithm stalls makes little progress point similar examples suggest assumption mbb nonsingular probably cannot relaxed acknowledgement thank editor referees insightful comments earlier draft linear equations interiorpoint methods 21 table convergence case mbb rankdeficient problem type fast step 42 59 3 27 42 14 73 70 100 85 83 r solving symmetric indefinite systems interiorpoint method linear programming matrix computations predictorcorrector method linear complementarity problems polynomial complexity superlinear convergence p nl iteration potential reduction algorithm linear complementarity problems computational experience primaldual interior point method linear programming implementation primaldual interior point method interior pathfollowing primaldual algorithms onl infeasibleinteriorpoint algorithm lcp quadratic convergence technical report sor 925 simplified homogeneous selfdual linear programming algorithm implementation quadratic p nl convergence predictorcorrector algorithm lcp convergence class infeasibleinteriorpoint methods horizontal linear complementarity problem tr