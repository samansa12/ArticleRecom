hector distributed runtime environment abstractharnessing computational capabilities network workstations promises offload work overloaded supercomputers onto largely idle resources overnight several capabilities needed including support architectureindependent parallel programming environment task migration automatic resource allocation fault tolerance hector distributed runtime environment designed present capabilities transparently programmers mpi programs run environment homogeneous clusters modifications source code needed design hector internal structure several benchmarks tests presented b introduction previous work networks workstations networked workstations available many years since modern network workstations represents total computational capability order supercomputer strong motivation use network workstations type lowcost supercomputer note typical institution access variety computer resources networkinter connected range workstations sharedmemory multiprocessors highend parallel supercomputers context paper network workstations considered network heterogeneous computational resources may actually worksta tions runtime system parallel programs must several important properties first scientific programmers needed way run supercomputerclass programs system little sourcecode modifications common way use architectureindependent parallel programming standard code applications permits program source code run sharedmemory multiprocessor latest generations parallel supercomputers example two major messagepassing standards emerged pvm 12 mpi 3 well numerous distributed shared memory dsm systems second ability run large jobs networked resources proves attractive workstation users individual workstations still used mundane tasks word proces work funded part nsf grant eec8907070 amendment 021 onr grant n000149710116 r sing task migration needed offload work users workstation return station owner ability also permits dynamic load balancing fault tolerance note paper task piece parallel program program job complete parallel pro gram messagepassing model used program always decomposed multiple communicating tasks platform host computer run jobs idle range workstation smp third runtime environment nows needs ability track availability relative performance resources programs run needs information conduct ongoing performance optimizations true several reasons relative speed nodes network workstations even homogeneous workstations vary widely workstation availability function individual users users create external load must worked around programs may run efficiently redistributed certain ways fourth fault tolerance extremely important systems involve dozens hundreds workstations example 1 chance single workstation go overnight 099 chance network 75 stations stay overnight complete runtime system computing must therefore include architectureindependent coding interface task migration automatic resource allocation faulttolerance also desirable system support features sourcecode modifications individual components already exist various forms review order goal work combine individual components single system b parallel programming standards wide variety parallel distributed architectures exist today run parallel programs varying degree interconnection interconnection topology bandwidth latency scale geographic distribution offer wide range performance cost tradeoffs applicability solve different classes problems generally characterized basis support physical shared memory two major classes parallel programming paradigms emerged two major classes parallel architectures sharedmemory model origins programming tightly coupled processors offers relative convenience messagepassing model well suited loosely coupled architectures coarsegrained applications programming models implement paradigms classified manifestationeither extensions existing programming languages custom languages system intended network workstations felt messagepass ingbased parallel programming paradigm closely reflected underlying physical structure also felt paradigm expressed existing scientific programming languages order draw existing base scientific programmers pvm mpi standards support goals mpi parallel programming environment selected able called c fortran programs provides robust set extensions send receive messages tasks working parallel 3 discussion relative merits pvm mpi outside scope paper decision partially driven ongoing work mpi implementations already conducted mississippi state argonne national laboratory 4 detailed discussion taxonomy parallel paradigms systems rationale decision found 5 one desirable property runtime system services offered transparently applications programmers programs written common programming standard modified access advanced runtimesystem features level transparency permits programs maintain conformity programming standard simplifies code development r c computing systems systems harness aggregate performance networked resources development quite time example one good review cluster computing systems conducted 1995 prepared syracuse university listed 7 commercial 13 research systems 6 results survey along comparison hector environment discussed paper summarized 7 found hector environment many features fullfeatured systems platform computings lsf florida states dqs ibms load leveler genias codine university wisconsinmadisons condor support simultaneous combination programmertransparent task migration loadbal ancing fully automatic dynamic loadbalancing support mpi job suspension unique additionally commitment programmer transparency led development extensive runtime informationgathering programs run breadth depth information gathered unique also added support typical commercial features gui configuration management tools noticeably lacking hector research project also mentioned point least two research projects using name hector work distributed computing multiprocessing first wellknown hector multiprocessor project university toronto 89 second system supporting distributed objects python crc distributed systems technology university queensland 10 hector environment described paper unrelated either three research systems allocate tasks across nows degree support task migration figure 1 summarizes systems condor carmi prospero mist dqs fully dynamic processor allocation reallocation stops task migration usertransparent load balancing usertransparent fault tolerance works mpi modifications existing mpipvm program uses existing operating system figure 1 comparison existing task allocators one system special version condor 11 named carmi 12 carmi allocate pvm tasks across idle workstations migrate pvm tasks workstations fail become nonidle two limitations first cannot claim newly available resources example attempt move work onto workstations become idle second checkpoints tasks one task needs migrate 13 stopping tasks one task needs migrate slows program execution since migrating task must stopped another automated allocation environment prospero resource manager prm 14 parallel program run prm job manager customwritten program job manager acts like purchasing agent negotiates acquisition system resources additional resources needed prm scheduled use elements condor support task migration checkpointing uses information gathered job node managers reallocate r resources notice use prm requires custom allocation program parallel applica tion future versions may require modified operating systems kernels mist system intended integrate several development efforts develop automated task allocator 15 uses prm allocate tasks user must custombuild allocation scheme program mist built top pvm pvms support indirect commu nications potentially lead administrative overhead message forwarding task migrated 16 implementation mpi hector uses globally available list task hosts incur overhead note mpi indirect communications hector overhead support every task sends messages directly receiving task overhead required task migrated notify every task new location shown process little overhead even large parallel applications distributed queuing system dqs designed manage jobs across multiple computers simultaneously 17 support one queue masters process user requests resources firstcome firstserved basis users prepare small batch files describe type machine needed particular applications example application may require certain amount memory thus resource allocation performed jobs started currently builtin support task migration fault tolerance issue commands applications migrate andor checkpoint support pvm mpi applications differences systems hector highlight two key differences among cluster computing systems first tradeoff task migration mechanisms programmerwritten versus supported automatically environment second tradeoff centralized decentralized decisionmaking informationgathering task migration context networked resources two strategies emerged creating programs checkpoint task migration image first checkpointing routines written application programmer presumably sufficiently familiar program know checkpoints created efficiently ex ample places running state relatively small free local variables know variables needed create complete checkpoint second checkpointing routines transfer entire program state automatically onto another machine entire address space registers copied carefully restored way comparison userwritten checkpointing routines inherent space advan tages states size inherently minimized may crossplatform compatibility advantages state written architectureindependent format userwritten routines two disadvantages first add coding burden onto programmer must write also maintain checkpointing routines second checkpoints available certain predetermined places program thus program cannot checkpointed immediately demand would appear syracuse survey systems 6 commercial systems support userwritten checkpointing checkpointing migration one guesses userwritten checkpointing much easier resource management system developersre sponsibility correct checkpointing transferred applications programmer noted earlier discussion research systems prm dqs also userwritten checkpointing least one system discussed syracuse report used capability perform crossarchitecture task migration 18 condor hector use completestatetransfer method form state transfer inherently works across homogeneous platforms involves actual replacement programs r state also completely transparent programmer requiring modifications additions source code alternate approach modify compiler order retain necessary type information pointer information two pieces information data structures contain pointers pointers point needed migration accomplished across heterogeneous platforms least one system msrm library louisiana state university implemented 19 msrm approach may make automatic crossplatform migration possible expense requiring custom compilers increased migration time system correct consistent task migration capability simple add check pointing tasks transfer state disk instead another task becomes possible create checkpoint program used fault recovery job suspension thus hector condor provide checkpointandrollback capability however task migration accomplished two technical issues deal first programs state must transferred completely correctly second case parallel pro grams communications progress must continue consistently tasks must agree status communications among hectors solutions issues discussed section iib e automatic resource allocation tradeoff centralized allocation mechanism tasks programs scheduled centrally policy centrally designed administered competitive distributed model programs compete andor bid needed resources bidding agents written part individual programs besides classic tradeoffs centralized distributed processing overhead scalability implied tradeoff degree support required applications programmer intelligence programs acquire resources need customwritten allocation approach places larger burden applications programmer permits wellinformed acquisition needed resources since overall goal hector minimize programmer burden use priori information customwritten allocation policies discussed section iic degree priori applications information boost runtime performance explored time 20 example nguyen et al shown extracting runtime information minimally intrusive substantially improve performance parallel job scheduler 21 approach used combination software specialpurpose hardware parallel computer measure programs speedup efficiency used information improve program performance however nguyens work relevant applications vary number tasks response optimization many parallel applications launched specific number tasks vary program runs addition ally requires use ksrs specialpurpose timing hardware gibbons proposed simpler system correlate runtimes different job queues 22 even relatively coarse measurement shown improve scheduling permits scheduling model closely approaches wellknown shortest job first sjf algorithm systems develop reasonably accurate estimates jobs completion time based historical traces programs submitted queue since information coarse gathered historically cannot used improve performance single application runtime however improve efficiency scheduler manages several jobs recent results feitelson weil shown surprising result user estimates runtime make performance job scheduler worse performance estimates 23 authors concede additional work needed area highlight r usersupplied information unreliable additional reason hector use approaches shown ability detailed performance information improve job scheduling however summarize approaches several shortcomings first require specialpurpose hardware second systems require user modifications applications program order keep track relevant runtime performance information third information gathered relatively coarse fourth systems require applications dynamically alter number tasks use fifth usersupplied information inaccurate also misleading goal hectors resource allocation infrastructure overcome shortcomings another tradeoff degrees support dynamically changing workloads computational resource availability ideal distributed runtime system automatically allocate jobs available resources move around program run order maximize performance order release workstations back users current clustering systems support goal varying degrees example systems launch programs enough resources enough processors become available approach taken ibms loadleveler example 6 systems migrate jobs workstations become busy condor 11 appears time syracuse survey hector attempts move jobs onto newly idle resources well f goals objectives hector desire design system supports fully transparent task migration fully automatic dynamic resource allocation transparent fault tolerance hector distributed runtime environment developed tested mississippi state university requirements necessitated development taskmigration method modified mpi implementation would continue correct communications task migration runtime infrastructure could gather process runtime performance information simultaneously created primary aim paper discuss steps detail well steps needed add support fault tolerance hector designed use central decision maker called master allocator perform optimizations make allocation decisions uses small task running candidate processor detect idle resources monitor performance programs execution tasks called slave allocators sas amount overhead associated sa important design consideration individual sa currently updates statistics every 5 seconds time interval compromise timeliness overhead process takes 5 ms sun sparcstation 5 corresponds extra cpu load 01 24 process reading tasks cpu usage adds 581 per task every time sa updates statistics every 5 adding reading detailed usage information therefore adds 001 cpu load per task example sa supervising 5 mpi tasks add cpu load 015 mpi program launched individual pieces tasks allocated available machines migrated needed sas communicate maintain awareness state running programs structure diagrammed figure 2 r slave allocator master allocator performance information system info commands commands performance information slave allocators figure 2 structure hector running mpi programs key design features design process described section ii benchmarks tests measure hectors performance described section iii paper concludes discussion future plans ii goals obstacles accomplishments ease use system must easy use gain widespread acceptance context ease use supported two different ways first adherence existing widely accepted standards allows programmers use environment minimal amount extra training second complexities task allocation migration fault tolerance hidden unsophisticated scientific programmers scientific programmers able write programs submit resource management system without provide additional information program 1 using existing standards hector runs existing workstations smps using existing operating systems currently sun systems running sunos solaris sgi systems running irix several parts system task migration correctness socket communications would simpler support modifications made operating system however would dramatically limit usefulness system using existing resources decision made modify operating system mpi pvm standards provide architectureindependent parallel coding capability c fortran mpi pvm supported wide growing body parallel architectures ranging networks workstations highend smps parallel mainframes represent systems gained gaining widespread acceptance already exists sizable body programmers use hector supports mpi coding standard 2 total transparency task allocation fault tolerance experience mississippi state nsf engineering research center indicates scientific programmers unwilling unable provide information program size estimated rruntime communications topology situation exists two reasons first programmers solving physical problem programming means end second may enough detailed knowledge internal workings computers provide information useful computer engineers scientists hector therefore designed operate priori knowledge program executed considerably complicates task allocation process almostnecessary step order promote transparency task allocation programmer result ease use scientific programmer currently supported future versions hector may able benefit usersupplied priori information new implementation mpi named mpitm created support task migration 25 fault tolerance mpitm based mpich implementation mpi 4 order run features programmer merely relink application hectormodified version mpi modified mpi implementation hector central decisionmaker handle allocation migration automatically programmer simply writes normal mpi program submits hector execution hector exists mpitm library collection executables library linked applications provides selfmigration facility complete mpi implementation interface runtime system executables include sa textbased commandline interface rudimentary motifbased gui installation roughly complicated installing new mpi implementation complete applications package 3 support multiple platforms hector supported sun computers running sunos solaris sgi computers running irix greatest obstacle solaris dynamic linker due ability link runtime create incompatible versions executable file creates undesirable situation migration impossible nearly completely identical machines consequence dividing sun computers many smaller clusters situation exists combination two factors first hector performs automat ic programmertransparent task migration without compiler modifications thus cannot move pointers must treat programs state unalterable binary image second dynamically linked programs may map system libraries associated data segments different virtual addresses runs one program different machines solution adopted condor rewrite linker accurately replace solaris dynamic linker customwritten one make migration system library data segments possible 26 option consideration hector currently supported b task migration 1 correct state transfer state running program unix environment considered six parts first actual program text may dynamically linked references data may statically dynamically located second programs static data divided initialized uninitialized sections third use dynamically allocated data stored heap fourth programs stack grows subroutines functions called used locally visible data dynamic data storage fifth cpu contains internal registers usually used hold results intermediate calculations sixth unix kernel maintains systemlevel information program file descriptors summarized figure 3 r cpu user memory kernel memory visible user visible user text static data heap registers stack kernel structs wrapper functions keep track kernel information place visible user figure 3 state program execution first five parts state principle transferred two communicating userlevel programs one exception occurs programs dynamically linked parts program text data may reside virtual address two different instantiations program discussed matter investigation sixth part programs state kernelrelated information difficult transfer invisible userlevel program information may include file descriptors pointers signal handlers memorymapped files without kernel source code almost impossible read structures directly operating system unmodified solution create wrapper functions let program keep track kernelrelated structures user code modifies kernel structures must pass trap interface traps way userlevel code execute supervisorlevel functions unix syscallh file documents system calls use traps system calls built top one create function name arguments system call open arguments function passed assemblylanguage routine calls system trap proper ly remainder function keeps track file descriptor path name permissions information lseek function keep track location file pointer calls change file pointer read write also call instrumented lseek file pointer information updated automatically permits migrated tasks resume reading writing files proper place discovered mpi environment task migration added 5 also uses signals memory mapping latter due fact gethostbyname makes call mmap system calls affect signal handling memory mapping replaced wrapper functions well task migration system requires knowledge running programs image particular operating system development small amount assembly language reliance certain properties pertaining signalhandling affect portability task migration sys tem assembly language needed way save restore registers call traps since task migration routine inside signal handler also necessary restarted program able exit signal handler coherently systems perform similar style systemsupported task migration mist condor also ported linux alpha hp environments 2715 seems indicate style task migration reasonably portable among unixbased operating sys tems probably different operating systems strong structural similarities r interesting note systemlevel migration support windowsntbased systems reported exact sequence steps involved actual state transfer described detail 25 two tests confirm methods speed stability described section iii 2 keeping mpi intact task migration protocol state restoration process described guaranteed preserve communications sockets point execution program fragments messages may reside kernels buffers either sending receiving side solution notify tasks single task migrate task communicating task migration sends endofchannel message migrating task closes socket connects tasks mark migrating task migration table tasks attempts initiate communications block migration complete task migration receives endofchannel messages assured messages trapped buffers knows messages reside data segment migrated safely state transferred another global update needed tasks know new location know communications resumed tasks migrating remain able initiate connections communicate one another mpi11 standard original mpi standard permits static task tables number tasks used parallel program fixed program launched important note static number tasks program mpi11 limit limitation hector also one important differences pvm mpi11 thus updates table require synchronization mpi confuse mpi program mpi2 standard newer standard currently development permits dynamically changing task tables proper use critical sections task migration interfere programs written mpi2 standard series steps needed update task table globally atomically hectors sas used provide synchronization machinetomachine communications migration task termination exact sequence steps required synchronize tasks update communication status consistently described detail 25 noted crashes middle migration program deadlock used global synchronization guarantee intertask consistency 3 task termination protocol task termination presents another complication task migrating another task terminates task migration never receives endofchannel message terminated task two measures taken provide correct program behavior first limits mpi program one migration one termination time handshaking needed migrate terminate second protocol involving sas mas developed govern task termination described 1 task preparing terminate notifies sa task receive process table updates requests endofchannel eoc messages block requests migrate cannot allowed migrate send termination signal 2 sa notifies task ready terminate 3 pending migrations terminations finished notifies sa task permission terminate block enqueue termination migration requests termination ended 4 sa notifies task 5 task sends sa final message exiting 6 sa notifies task exiting permit migrations terminations r notice improperly written program may attempt communicate task task ended world messagepassingbased parallel programming programmers mistake behavior program undefined point program deadlock hector program deadlocks hector 4 minimizing migration time operating system already one mechanism storing programs state core dump creates file programs registers data segment stack first version state transfer used capability move programs around two advantages approach first built operating system second symbolic debuggers tools extract useful information core files disadvantages approach first multiple network transfers needed disk space shared network means state actually copied multiple times second speed transfer limited speed disk unrelated programs sharing disk one way around shortcomings transfer state directly network originally implemented mist team 15 network state transfer overcomes disadvantages information written network slightly modified corefile format modification unused stack space transmitted penalty using sunos core file format information written network socket connection application instead written file operating system notice retains advantage corefile tool compatibility experiments show three times faster 25 shown c automatic resource allocation 1 sources information hectors overall goal attempt minimize programmer overhead context awareness program behavior incurs expense access potentially beneficial programspecific information approach used based experiences scientific programmers within authors research center unwilling invest time effort use new systems perceived burden sourcecode modifications approach dictates hector able operate priori applications knowledge turn increases requirement depth breadth information gathered runtime lack priori information makes hectors allocation decisionmaking difficult however experiments confirm information able extract runtime improve performance ability exploit newly idle resources especially helpful 2 structure range ways resource allocation structured completely centrally located completely distributed hectors resource allocation uses features decisionmaking portion global synchronization resides therefore completely central ized advantage single central allocation decisionmaker easier modify test different allocation strategies since unix operating system permit signals sent hosts necessary executive process running candidate host since necessary pro cesses used gather performance information well thus informationgathering informationexecution portions fully distributed carried sas 3 collecting information two types information master allocator needs order make decisions first needs know resources potentially available hosts consider rand powerful second needs know efficiently extent resources used much external nonhector load much load various mpi programs control imposing relative performance candidate host determined slave allocator started running livermore loops 28 actually measure floating point per formance also possible slave allocator measure disk availability physical memory size example information transmitted master allocator maintains centralized database information current resource usage monitored analyzing information kernel candidate processor allocation algorithms draw idle time information cpu time information percentage cpu time devoted nonhectorrelated tasks percentage cpu time used detect external workload interactive user logging criterion automatic migration hector mpi library contains additional detailed selfinstrumentation logs amount computation communication time task expends data gathered sas using sharedmemory forwarded detailed discussion agentbased approach informationgathering well testing results may found 29 4 making decisions one primary advantages performancemonitoringbased approach ability claim idle resources rapidly shown tests busy workstations day show migrating newly available resources reduce run time promote effective use workstations implementation testing sophisticated allocation policies also way fault tolerance ability migrate tasks midexecution used suspend tasks fact fault tolerance historically one major motivation task migration effect task transfers state file checkpoint program node failure detected files used roll back program state last checkpoint calculations checkpoint node failure lost calculations checkpoint may represent substantial time savings also known unrelated failures andor routine maintenance may occur needed middle large program run ability suspend tasks helpful shown order guarantee program correctness tasks must checkpointed consistently13 tasks must consistent point execution message exchange status example messages transit must fully received transmission new messages must suspended case migration termination series steps needed checkpoint roll back parallel programs 1 checkpointing protocol following steps taken checkpoint program 1 decides checkpoint program whatever reason currently supported manual user command may eventually done periodic basis waits pending migrations terminations finished notifies tasks program via sas prepare checkpointing 2 tasks send endofchannel eoc messages connected tasks receive eocs connected tasks guarantees messages transit 3 eocs exchanged task notifies sa ready checkpointing informs sa size state information information passed 4 received confirmation every task ready begin actual checkpointing process notifies task task begin checkpointing 5 task finishes transmitting state writing file notifies note possible one task checkpoint time experiments ordering checkpointing described 6 tasks checkpointed writes small bookkeeping file contains state information pertinent sas example contains execution time point checkpointing total execution time accurate job rolled back checkpoint 7 broadcasts either resume suspend command tasks tasks either resume execution stop respectively former used create backup copy task event future node failure latter used necessary remove job temporarily system 2 rollback protocol following steps taken roll back checkpointed program 1 given name checkpoint file provides necessary information restart program 2 allocates tasks available workstations program launched 3 based allocation notifies sa first machine 4 sa restarts task state file name found checkpoint file sent sa 5 task notifies sa restarted properly waits table update 6 receives confirmation one tasks successful restart notifies sa next task continues tasks restarted task rollback sequential primarily performance reasons file server reading checkpoints sending sockets newly launched tasks perform efficiently one checkpoint sent time 7 confirmation arrives builds table similar used mpi tasks lists hostnames unix pids tasks parallel program tasks restarted table broadcast tasks note broadcast occurs via hector runtime infrastructure invisible mpi program use mpi broadcast mpi inactive rollback 8 task resumes normal execution receives table update entire program restarted 3 checkpoint server case task migration two ways save programs state one way program write directly checkpoint file way launch checkpoint server machine large amount physically mounted disk space latter concept first implemented condor group 30 task transmits state via network directly server server writes state directly local disk reason task cannot write state local disk obviousif machine crashes backup copy state would lost well checkpoint server method expected faster uses direct socket connections local disk writes efficient writing files network note many local disk caching strategies used systems like nfs work well checkpoints checkpoint files typically written read back 30 different novel scheduling strategies checkpoint service described tested 4 issues faulttolerant represents single point failure sas modified terminate tasks running lose contact r feature intentionally added total termination programs distributed across dozens workstations quite tedious unless automated feature disabled sas tasks could continue working without although task migrations job launches would cease job termination would deadlock checkpoints collected system enable job restarted sas restarted check pointandrollbackbased fault tolerance tolerate fault runtime infrastructure another approach solve problem support rapid fault tolerance would incorporate existing group communication library isis horus 31 use mes sageduplication facility one possible design described 32 means rapid fault detection also added future versions hector 32 sa sends performance information periodically current period 5 seconds may grow larger tests performed sa send performance update suitable timeout assumed node running properly jobs node rolled back strategy detect heavily overloaded nodes well catastrophically failed nodes iii benchmarks tests ease use example existing computational fluid dynamics simulation obtained large complex total data size around 1 gbyte 13000 lines fortran source code simulation already parallelized coded mpi tested parallel computer correctness modifications source code program relinked run completely correctly hector highlights ability run real existing mpi programs b task migration two different task migration mechanisms proposed implemented tested first used core dump write programs state transfer different machine second transferred information directly socket connection order compare relative speeds tasks different sizes migrated times two sparcstations 10s connected ethernet tests run normal daily operations engineering research center 25 results show figure 4 program size kbytes515 time tomigrate sec core file transfer network transfer figure 4 time migrate tasks different sizes r consider example program size 24 mbytes core dump version migrated 126 131 seconds information transfer rate roughly 15 mbitsec net work state transfer version migrated 41 seconds information rate 46 mbitsec latter close practical limit ordinary 10 mbit ethernet typical conditions shows task transfer limited network bandwidth overall network state transfer 32 times faster test program run minimal size network state transfer task migrated 400000 times without error stopped file server went due unrelated fault file server crash effect locking local machine hector became locked well forkandexec needed launch new task could executed overhead associated broadcasting table updates measured migrating tasks inside increasingly large parallel programs test run sparcstation 5 ether netconnected sparcstation 10 separate subnets run normal daily operating conditions engineering research center program following first one task establishes communications number tasks second task migrated explained entails sending notification receiving endofchannel messages transferring state program third newly migrated task reestablishes communications tasks time ordering task migrate reestablishing connections measured test repeated 50 times total number tasks program ranged 10 50 results shown figure 5 crosses actual data points line linear regression bestfit data linear regression chosen time migrate linear function number tasks size migration image135 number tasks time tomigrate sec figure 5 time migrate task number tasks increases slope bestfit line corresponds incremental amount time required notify one task impending migration wait eoc message notify task migration completed since program size changed negligibly number tasks increased time migrate effectively constant results show figure 5 slope corresponds 75 ms per task per migration process notifying receiving endofchannel reestablishing communications takes average 75 ms 10 mbits ethernet yintercept roughly time took task migrate 184 ms transfer took place 10 mbits corresponds migration image size 230 kbytes c automatic resource allocation one early test mpi program run two different scenarios determine performance benefits optimal scheduling first come first served scheduling 5 threeprocess matrix multiply program used 1000 x 1000 matrix supplied test case machines used test ranged 40 mhz sparcbased system 4processor 70 mhz hypersparcbased system free external loads except noted matrix multiply commonly used test program data size quite scalable programs structure simple easily modified first tests run validate advantages using processors load criterion task migration third workstation loaded external workload runtime dropped 1316 seconds 760 seconds third task migrated onto idle workstation second tests run show advantages migrating tasks event faster workstations come available faster machine became available two minutes computation runtime dropped 1182 seconds 724 seconds algorithms used task allocation systems would migrated slow tasks program launched extensive tests run master allocator completely developed matrix multiply program used allocation policy switched two varia tions first variation migrated tasks workstations became busy usually exter nal interactive loads second variation considered migrations whenever workstation became idle test system written switched two allocation policies alternate program runs total 108 runs made several days research center 3 fairly wellused sparcstation 2s onlymigratewhenbusy policy used run times varied 164 1124 seconds alsomigratewhenidle policy used run times varied 154 282 seconds latter case program averaged 085 task migrations per run idle worksta tions migration idle workstations little effect minimum run times dramatically reduced maximum run time average run time dropped 359 seconds 177 seconds due reduction maximum run time distribution run times shown figure 6 run times 1st policy number trials run times 2nd policy figure distribution run times two task migration policies improvement run time noticed cases relatively small programs migrated programs increase size penalty migration increases example runs fairly large electromagnetics simulation showed noticeable increase run time number migrations increased 7 r one point interest interactive sometimes unaware machines also used run jobs hector including papers primary author users accustomed small wait applications word processors page virtual memory since time migrate tasks machines order time required page pro grams additional interactive delay noticed noted case small test programs truly rapid migration practical program state fits within physical memory shown tests larger programs program size exceeds physical memory process migration causes thrashing 7 extensive testing showing runs variety applications sun sgi workstation clusters runs alongside actual student use found 7 fault tolerance series tests run evaluate relative performance different means creating scheduling checkpoints matrix multiply program used test different combinations strategies matrices 2 100x100 500x500 1000x1000 matrices small medium large checkpoint sizes respectively total checkpoint sizes 2 mbytes 11 mbytes mbytes respectively following tests also run day normal network usage conditions first two strategies tested involved writing directly checkpoint file using nfs tasks commanded checkpoint simultaneously allatonce task commanded checkpoint oneatatime second three serverbased strategies evaluated overall effectiveness first server notified single task checkpoint tasks checkpointed one time second server notified tasks sizes one task time permitted checkpoint labelled combined figures third server notified tasks tasks ordered checkpoint simultaneously individual tasks instrumented report amount time required send data either file nfs checkpoint server size program state time checkpoint state used calculate bandwidth usage shown cases without figure 7 figure 8 checkpoint server strategy small medium large allatonce 221 146 145 oneatatime 483 512 555 figure 7 bandwidth usage different strategies server mbitsec strategy small medium large allatonce 237 155 171 combined 657 715 725 oneatatime 726 747 720 figure 8 bandwidth usage different strategies server mbitsec results easy understand first using nfs instead server involves bandwidth penalty using server permits data transferred without additional protocol overheads se cond allatonce strategy clogs ethernet substantially reduces performance combined oneatatime strategies essentially identical performance point view parallel program task sends data server without interference tasks r server works forking separate process task checkpointed process mallocs space entire state reads state socket writes state disk knows size program state task notifies sa size indicates readiness checkpoint step 3 checkpoint protocol information passed via checkpoint server data captured checkpoint server results shown figure 9 individual points actual data points lines linear regression curves fit actual data policy tested times range sizes shown graph200 10000 20000 30000200 10000 20000 30000 total size program checkpoint kbytes time checkppoint program sec oneatatime allatonce combined figure 9 time checkpoint tasks different checkpoint server policies reciprocal slope line corresponds bandwidth obtained strategy summarized figure 10 bandwidth mbitsec oneatatime 543 combined 686 allatonce 487 figure 10 impact different server strategies combined strategy makes best use available bandwidth allatonce strategy worst performance typical ethernetbased communications fact penalty shown less expected authors conclusion traffic network rat time program runs already significant impact using allatonce strategy compounds problem note time checkpoint measured server shown figure 9 includes time fork processes malloc space time read data network time write disk bandwidth actually function times reason combined strategy shows improved performancethe process forking new processes making space memory state images done parallel also notice time required write disk fork tasks added bandwidth combined strategy drops slightly indicates overhead forkandmalloc process small iv future work conclusions testing dynamic loadbalancing initial concepts dynamic load balancing implemented undergoing testing probably modified testing proceeds since one function needed optimize tasks simple provide alternate optimization strategies testing b job priority currently jobs run priority level concept priority needs added future versions permit job scheduling like found industry combination prioritybased decision making builtin fault tolerance explored basis faulttoler ant highly reliable computing system c conclusions running parallel programs efficiently networked computer resources requires architectureindependent coding standards task migration resource awareness allocation fault toler ance modifying mpi implementation developing task migration technology developing complete runtime infrastructure gather performance information make optimiza tions became possible offer services transparently unmodified mpi programs resulting system features automatic fully dynamic loadbalancing task migration ability release reclaim resources demand checkpointandrollbackbased fault tolerance demonstrated work large complex applications sun sgi workstations smps tests confirm ability reclaim idle resources rapidly beneficial effects program performance novel checkpoint server protocol also developed tested making fault tolerance efficient since combines supports varying degree features needed parallel computing forms basis additional work distributed computing v r visualization debugging heterogeneous environment pvm parallel virtual machine cambridge mass mit press monitors message cluster p4 parallel program system hector automated task allocation mpi cluster computing review using hector run mpi programs networked workstations experiences hector multiprocessor experiences hector multiprocessor hector distributed objects python condor distributed processing system providing resource management services parallel applica tions consistent checkpoints pvm applications prospero resource manager scalable framework processor allocation distributed systems mist pvm transparent migration checkpointing mpvm migration transparent version pvm dqs user manual dqs version 31 portable checkpointing recovery memory space representation heterogeneous network process migration theory practice parallel job scheduling using runtime measured workload characteristics parallel processing scheduling historical application profiler use parallel schedulers utilization predictability scheduling ibm sp2 backfilling usertransparent runtime performance optimization task migration implementation messagepassing interface checkpoint migration unix processes condor distributed processing system page httpwww livermore fortran kernels computer test numerical performance range agentbased architecture dynamic resource management managing checkpoints parallel programs softwarebased replication fault tolerance architecture rapid distributed fault tolerance tr ctr samuel h russ katina reece jonathan robinson brad meyers rajesh rajan laxman rajagopalan chunheong tan hector agentbased architecture dynamic resource management ieee concurrency v7 n2 p4755 april 1999 angela c sodan lin han atopspace time adaptation parallel grid applications via flexible data partitioning proceedings 3rd workshop adaptive reflective middleware p268276 october 1919 2004 toronto ontario canada hyungsoo jung dongin shin hyuck han jai w kim heon yeom jongsuk lee design implementation multiple faulttolerant mpi myrinet m3 proceedings 2005 acmieee conference supercomputing p32 november 1218 2005 kyung dong ryu jeffrey k hollingsworth exploiting finegrained idle periods networks workstations ieee transactions parallel distributed systems v11 n7 p683698 july 2000