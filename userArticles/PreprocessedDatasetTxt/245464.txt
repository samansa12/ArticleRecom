realtime focus range sensor abstractstructures dynamic scenes recovered using realtime range sensor depth defocus offers effective solution fast dense range estimation however accurate depth estimation requires theoretical practical solutions variety problems including recovery textureless surfaces precise blur estimation magnification variations caused defocusing textured textureless surfaces recovered using illumination pattern projected via optical path used acquire images illumination pattern optimized maximize accuracy spatial resolution computed depth relative blurring two images computed using narrowband linear operator designed considering optical sensing computational elements depth defocus system defocus invariant magnification achieved use additional aperture imaging optics prototype focus range sensor developed workspace 1 cubic foot produces 512 480 depth estimates hz average rms error 02 several experimental results included demonstrate performance sensor b introduction pertinent problem computational vision recovery threedimensional scene structure twodimensional images problems studied vision far attracted attention resulted panoply sensors algorithms jarvis1983 besl 1988 broadly classified two categories passive active passive techniques shape shading shape texture attempt extract structure single image algorithms still investigation given assumptions forced invoke expected prove complementary techniques serve standalone strategies passive methods stereo structure motion use multiple views resolve shape ambiguities inherent single image primary bottleneck methods proved correspondence feature tracking addition passive algorithms yet demonstrate accuracy robustness required highlevel perception tasks object recognition pose estimation hitherto high quality depth maps resulted use active sensors based time flight light striping jarvis1983 practical perspective light stripe range finding emerged clear winner structured environments active radiation scene feasible offers robust yet inexpensive solution variety problems however suffered one inherent drawback namely speed achieve depth maps sufficient spatial resolution large number say n closely spaced stripes used stripes projected simultaneously impossible associate unique stripe given image point process necessary compute depth triangulation classical approach obtain n images one stripe f time required sense digitize image scanning n stripes takes least n f substantial improvements made assigning gray codes stripes scanning entire collection stripes sets inokuchi et al1984 information needed acquired log 2 nt f significant improvement alternative approach uses colorcoded stripe patterns boyer kak1987 however practical grayworld reflects wavelengths light new hope light stripe range finding instilled advances vlsi based notion cell parallelism kanade et al1991 computational sensor developed sensor element records stripe detection timestamp single laser stripe sweeps scene high speed depth maps produced little 1 msec though present day silicon packaging limits total number cells hence spatial depth resolution 28x32 gruss et al1993 future advances vlsi expected yield highresolution depth maps unprecedented speeds paper present range sensor based focus analysis produces 512x480 depth map framerate sensor uses inexpensive offtheshelf imaging processing hardware shown accuracy approximately 03 focus analysis major advantage stereo structure motion two images scene taken different optical settings viewpoint initially demonstrated pentland1987subbarao1988pentland et al1989 circumvents need correspondence feature tracking algorithm presented uses two scene images images correspond different levels focus local frequency analysis implemented typically via linear operators yields depth estimates however differences two images tend subtle believe previous solutions depth defocus met limited success based rough approximations optical sensing mechanisms involved focus analysis contrast approach based careful physical modeling optical sensing computational elements work optical transfer function defocus image sensing sampling focus measure operators depth defocus shares one inherent weakness stereo motion requires scene high frequency textures textureless surface appears focused defocused resulting images contain information necessary depth computation prompted us develop focus range sensor uses active illumination key idea force texture scene analyze relative defocus texture two images illumination projection suggested past girod scherock1989pentland et al1994 depth defocus depth pattern size distortion perspective projection however projected patterns selected adhoc fashion guarantee desired precision computed depth critical problem therefore determining illumination pattern would maximize accuracy robustness depth defocus paper solution problem arrived detailed fourier analysis entire depth defocus system first theoretical models developed optical computational elements system expressed spatial fourier domains derivation illumination pattern filter posed optimization problem fourier domain optimal pattern one maximizes sensitivity focus measure depth variations minimizing size focus operator achieve high spatial resolution computed depth prototype realtime focus range sensor developed uses two ccd image detectors view scene optical elements derived illumination pattern fabricated using microlithography incorporated sensor illumination pattern projected onto scene via optical path used image scene results several advantages enables precise registration illumination pattern sampling grid image sensors light rays projected imaging optics subjected similar geometric distortions rays reflected back sensors therefore despite everpresent lens distortions illumination pattern sensing grid detector well registered coaxial illumination imaging also results shadowless image surface regions visible sensor also illuminated furthermore since images acquired viewing direction missing part occlusion problem stereo avoided figure 1 shows two brightness images computed depth map cup milk flowing structures dynamic scenes recovered highspeed sensor numerous experiments conducted evaluate accuracy realtime capability sensor addition quantitative error analysis realtime depth map sequences moving objects presented performance sensor demonstrates relevance problems recognition inspection also automatic cad model generation vision graphics remote visualization realtime tracking threedimensional objects applications presently explored b figure 1 two images scene taken using different focus settings b depth map scene computed 33 msec focus range sensor 2 depth defocus fundamental depth defocus relationship focused defocused images born wolf1965 figure 2 shows basic image formation geometry light rays radiated object point p pass aperture refracted lens converge point q image plane thin lens relationship object distance focal length lens f image distance given gaussian lens lawd f f 1 2 r f b figure 2 image formation depth defocus point object plane projected onto single point image plane causing clear focused image f formed however sensor plane coincide image plane displaced energy received p lens distributed patch sensor plane result blurred image p clear single image include sufficient information depth estimation two scenes defocused different degrees produce identical images solution depth achieved using two images 1 2 separated known physical distance fi pentland1987 subbarao surya1992 problem reduced analyzing relative blurring scene point two images computing distance ff focused image using ff lens law 1 yields depth scene point simple procedure may appear several technical problems emerge implementing algorithm practical value ffl determining relative defocus frequency domain blurring viewed lowpass filtering scene texture relative blurring thus principle estimated frequency analysis problem nontrivial since local scene texture includes frequencies unknown magnitudes phases since effect blurring frequency dependent meaningful investigate net blurring entire collection frequencies constitute scene texture observation forced investigators use narrowband filters isolate less single frequencies estimate relative attenuation due defocus two images given dominant frequencies scene unknown possibly spatially varying one forced use large bank tuned filter gabor filters gokstorp1994 xiong shafer1994b hypergeometric filters xiong shafer1994a three problems surface approach rigorous necessity use scores times 100 filters makes impractical realtime application without use expensive customized hardware b filters typically chosen assuming images continuous filter design discrete images requires analysis carried avoid undesirable artifacts computed depth c irrespective reliability filter extracting focus measures output put good use optical sensing elements depth defocus system accurately modeled instance previous work relied heavily gaussian blur function approximation may suffice depth focus 1 severely limits accuracy depth defocus ffl textureless surfaces depth defocus shares severe weakness stereo structure motion imaged surface textureless white sheet paper instance defocus focus produce identical images number filters would prove ineffective estimating relative blurring similar situation would arise stereo motion correspondence feature tracking would illposed particularly structured environments problem obviated projecting illumination pattern scene interest ie forcing scene texture girod scherock1989 pentland1987 however careful attention must given pattern used else problem best reduced applying depth defocus scene unknown texture work interested textured textureless scenes hence adopt illumination projection contrast previous work however seek optimal pattern would ensure scene points dominant texture one maximizes spatial resolution accuracy computed depth derivation optimal projected pattern posed optimization fourier domain ffl varying magnification finally relation magnification focus well worth mentioning imaging system shown figure 2 effective image location work focus based depth computation broadly classified depth focus depth defocus former relies large number images taken varying ff figure 2 small increments search uses focus operator detect image maximumfocus scene point see krotkov 1987 darrell wohn1988 nayar nakagawa1994 nair stewart1991 krishnan ahuja1993 asada et al1993 xiong shafer1994b noguchi nayar1994 contrast depth defocus typically uses two images estimates relative blurring get depth see pentland1987 subbarao1988 grossman1987 pentland et al1989 bove jr1993 ens lawrence1991 xiong shafer1994b gokstorp1994 point p moves along ray r sensor plane displaced causes shift image coordinates p turn depends unknown scene coordinates p variation image magnification defocus manifests correspondencelike problem depth defocus right set points images 1 2 needed estimate blurring problem underemphasized previous work exception willson shafer1994 precise focusmagnification calibration motorized zoomlenses suggested darrell wohn1988 registrationlike correction image domain proposed calibration approach effective cumbersome viable many offtheshelf lenses present simple effective solution based first principles optics 3 constant magnification defocus begin last problems raised discussion variation image magnification defocus approach problem optical perspective rather computational one consider image formation model shown figure 3 modification made respect model figure 2 use external aperture 0 aperture placed frontfocal plane ie focal length front principal point lens simple addition solves prevalent problem magnification variation distance ff sensor plane lens simple geometrical analysis reveals ray light r 0 scene point passes center 0 aperture 0 emerges parallel optical axis image side lens kingslake1983 furthermore parallel ray axis cone includes light rays radiated scene point passed 0 intercepted lens result despite blurring effective image coordinates point p images 1 2 namely coordinate focused image q f invariance magnification defocus holds true depth defocus configuration values ff fi also shown constantmagnification property unaffected aperture radius 0 used furthermore lens law 1 remains valid modification realizable single lens systems compound lens system see kingslake1983 given offtheshelf lens aperture easily appended casing lens resulting optical system called telecentric lens nominal effective fnumbers classical optics figure 2 fa respectively equal fa 0 telecentric case modeling effective solutions illumination projection depth estimation require careful modeling analysis physical phenomena involved depth defocus five different elements components play critical role briefly describe proceeding model f 1 2 r f b f figure 3 constantmagnification imaging system depth defocus achieved simply placing aperture frontfocal plane optics 1 illumination pattern exact pattern used illuminate scene determines final texture spatial frequency characteristics texture determine behavior focus measure hence accuracy depth estimation parameters component set optimize achieve maximum depth accuracy 2 optical transfer function finite size lens aperture imposes restrictions range spatial frequencies detectable imaging system restrictions play critical role optimization illumination pattern upon initial inspection optical transfer function otf seems severely constrain range useful illumination patterns however shall see otfs limited range also enables us avoid serious problems image aliasing 3 defocusing depth surface point directly related defocus lack image plane phenomenon enables us recover depth defocus wellknown defocus essentially lowpass filter however realistic model phenomenon imperative focus analysis objective determine depth two images estimating plane best focus scene point 4 image sensing two images used shape recovery course discrete relationship continuous image formed sensor plane discrete image used computations determined shape spatial arrangement sensing elements pixels image detector shall see final illumination pattern include elements comparable size pixel sensor therefore accurate model image sensing essential illumination optimization 5 focus operator relative degree defocus two images estimated using focus operator operator typically highpass filter applied discrete images interestingly optimal illumination pattern also dependent parameters focus operator used components together determine relation depth scene point two focus measures therefore optimal illumination viewed one maximizes sensitivity robustness focus measure function achieve component modeled spatial well fourier domains since used telecentric lens figure 3 implementation parameters used developing model however following expressions made valid classical lens system figure simply replacing factor f 0 41 illumination pattern parameters illumination pattern determined illumination model must defined model must flexible must subsume large enough variety possible illumination patterns defining model meaningful take characteristics components consideration describe shortly image sensor used rectangular pixels arranged rectangular spatial grid mind define following illumination model basic building block model rectangular illuminated patch cell uniform intensity xb 2 twodimensional rectangular function bracewell1965 unknown parameters illumination cell b x b length width cell cell assumed repeated twodimensional grid obtain periodic pattern periodicity essential since goal achieve spatial invariance depth accuracy ie image regions irrespective distance must possess textural characteristics periodic grid defined 2dimensional shah function bracewell1965 2t x 2t determine periods grid x directions note grid rectangular vertical horizontal symmetry xy plane final illumination pattern ix obtained convolving cell c x grid g x exact pattern therefore determined four parameters namely b x b x illumination grid restrictive may appear upon initial inspection instance parameters b x b 2t x 2t stretched obtain repeated illumination non illumination stripes horizontal vertical directions respectively alternatively also adjusted obtain checkerboard illumination pattern large small illuminated patches exact values b x b x evaluated optimization procedure described later practice illumination pattern determined optimization used fabricate filter pattern optimization procedure requires analysis component system spatial domain well frequency domain u v fourier transforms illumination cell grid pattern denoted c u v g u v iu v respectively found c u sin b x u sin b v g u 42 optical transfer function adjacent points illuminated surface reflect light waves interfere produce diffraction effects angle diffraction increases spatial frequency surface texture since lens aperture imaging system figure 3 finite radius 0 capture higher order diffractions radiated surface see william becklund1989 details effect places limit optical resolution imaging system characterized optical transfer function otf f f 0 u v spatial frequency twodimensional surface texture seen image side lens f focal length lens wavelength incident light clear expression spatial frequencies limit 2a 0 f imaged optical system figure 4 turn places restrictions frequency illumination pattern frequency limit used cut desired number higher harmonics produced illumination pattern short otf curse blessing limits detectable range frequencies time used minimize detrimental effects aliasing highorder harmonics spatial domain frequency domain 1px 1py sampling x c 1wx 1wy x optical transfer function 2a l x x f f f f figure 4 spatial frequency models optical sensing elements depth defocus 43 defocusing defocus function described detail previous work see born wolf1965 horn 1986 example figure 3 let ff distance focused image surface point defocused image formed sensor plane light energy radiated surface point collected imaging optics uniformly distributed circular patch sensor plane patch also called pillbox defocus function figure 4 0 radius telecentric lens aperture fourier domain defocus function given f f j 1 firstorder bessel function born wolf1965 evident expression defocus serves lowpass filter bandwidth filter increases ff decreases ie sensor plane gets closer plane focus extreme case ff 0 hu v frequencies without attenuation producing perfectly focused image note defocused image frequencies attenuated time case passive depth focus defocus poses serious problem different frequencies unknown scene bound different unknown magnitudes phases difficult therefore estimate degree defocus image region without use large set narrowband focus operators analyze frequency isolation indicates would desirable illumination pattern single dominant frequency enabling robust estimation defocus hence depth 44 image sensing assume image sensor typical ccd tv camera sensor modeled rectangular array rectangular sensing elements pixels quantum efficiency horn1968 pixel assumed uniform area pixel let mx continuous image formed sensor plane finite pixel area effect averaging continuous image mx spatial domain averaging function rectangular cell xw w x w length width pixel respectively discrete image obtained sampling convolution mx c x sampling function rectangular grid spacings discrete samples two spatial dimensions x phase shift grid final discrete image therefore parameters w x w p x p determined particular image sensor used parameters therefore known values substituted optimization done hand phase shift x sampling function respect illumination pattern also viewed illumination parameters optimization recognize importance phase parameters one visualize variations discrete image arise simply translating highfrequency illumination pattern respect sensing grid fourier domain averaging sampling functions sin w x u w x u sin w v final discrete image 45 focus operator since defocusing effect suppressing highfrequency components focused image desirable focus operator respond high frequencies image purpuse illumination optimization use laplacian however derived pattern remain optimal large class symmetric focus operators spatial domain discrete laplacian q x q spacings neighboring elements discrete laplacian kernel optimization spacings related illumination parameters fourier transform discrete laplacian required discrete nature focus operator comes price tends broaden bandwidth operator pattern determined filter tuned maximize sensitivity fundamental illumination frequency minimizing effects spurious frequencies caused either scenes inherent texture image noise 46 focus measure focus measure simply output focus operator related defocus ff hence depth via components modeled note illumination pattern c g projected optics similar used image formation consequently pattern also subjected limits imposed optical transfer function defocus function h therefore texture projected scene ff 0 represents defocus illumination depends depth illuminated point however illumination pattern incident surface patch plays role surface texture hence defocus ff 0 illumination significant effect depth estimation projected texture reflected scene projected optics back onto image plane produce discrete image lambdas c x final focus measure function gx result applying discrete laplacian discrete image deltas g x since distance adjacent weights laplacian kernel must integer multiples period image sampling function g expression rearranged l expressed fourier domain expression gives us final output focus operator value defocus parameter ff used following section determine optimal illumination pattern optimization implementation illumination pattern projected scene using high power light source telecentric lens identical one used image scene allows us assume projected illumination primary cause surface texture stronger natural texture surface result results applicable textureless surfaces also textured ones illumination optimization problem formulated follows establish closedform relationships illumination parameters b maximize sensitivity robustness spatial resolution focus measure gx high sensitivity implies small variation degree focus results large variation gx would ensure high depth estimation accuracy presence image noise ie high signaltonoise ratio robustness mean pixels degree defocus produce focus measure independent location image plane ensures depth estimation accuracy invariant location image plane lastly high spatial resolution achieved minimizing size focus operator ensures rapid depth variations surface discontinuities detected high accuracy order minimize smoothing effects maximize spatial resolution computed depth support span discrete laplacian must small possible turn requires frequency illumination pattern high possible however optical transfer function described section 42 imposes limits highest frequency imaged optical system maximum allowable frequency 2a 0 f determined numerical aperture telecentric lens mind let us examine fourier transform illumination pattern since pattern periodic fourier transform must discrete may zerofrequency component safely ignored since laplacian operator sum secondorder derivatives eventually remove zerofrequency component final image objective maximize fundamental spatial frequency 1t x 1t illumination pattern order maximize frequency maintaining high detectability must close optical limit f turn pushes higher frequencies illumination pattern outside optical limit left surface texture whose image quadrapole fundamental frequencies sigma1t x sigma1t result frequencies need consider analysis focus measure function gu v consider final measure gu v examine g 0 u v focus measure prior image sampling reasons given twodimensional g 0 u v reduced four discrete spikes 1t x components h c l g 0 reflection symmetric deltah deltas c x therefore frequency domain focus measure function prior image sampling reduces function g 0 x image domain simply inverse fourier transform g 0 u v note g 0 x product cosine functions weighted coefficient g 0 1t x 1t defocus function h effect reducing coefficient g 0 1t x 1t focus measure clearly sensitivity focus measure depth defocus optimized maximizing coefficient g 0 1t x 1t respect unknown parameters system optimization procedure summarized b since x show components 25 first two partial derivatives equation 28 difficult evaluate fortunately derivatives 29 30 sufficient obtain relations system parameters details optimization procedure refer reader appendix following result maximizes sensitivity spatial resolution focus measure gx next examine spatial robustness gx imagine imaged surface planar parallel image sensor would like image sampling produce absolute value gx discrete sampling points image entails relating illumination sensing parameters facilitate careful sampling product cosine functions 27 note final focus measure samples gx absolute value two cosines expression sampled peak values sampling possible alternatively cosines sampled period 2 phase shift 4 yields second solution equations give two solutions checkerboard illumination patterns differ fundamental frequencies size illumination cell phase shift respect image sensor equations 31 32 34 35 yield filter pattern shown figure 5a case filter detector registered zero phase shift illumination cell size shape sensor elements pixels second solution shown figure 5b obtained using sampling solutions 36 37 yielding filter pattern illumination cell two times size sensor element phase shift half sensor element size exactly patterns projected perfectly registered image detector described experimental section 6 tuned focus operator purpose illumination optimization used laplacian operator resulting illumination pattern single dominant absolute frequency 1t x 1t given position refine focus operator minimize effects frequencies caused either physical texture scene image noise end let us consider properties 3x3 discrete laplacian see figure 6a b see b figure 5 optimal illumination filter patterns illumination period p x p pixel size x illumination phase shift respect image sensing grid though laplacian peaks exactly 1t x fairly broad bandwidth allowing spurious frequencies contribute focus measure g 23 seek narrow band operator sharp peaks four coordinates frequency space given operator must eventually discrete finite support limit extent tuned constrain problem impose following conditions maximize spatial resolution computed depth force operator kernel 3x3 b since fundamental frequency illumination pattern symmetric quadrapole arrangement focus operator must rotationally symmetric two conditions force operator structure shown figure 6c c operator must respond dc component image brightness last condition satisfied sum elements operator equals zero also imperative response lu v operator fundamental frequency zero cos 2q yt given 32 reduces expressions 38 40 imply b 6 0 without loss generality set therefore tuned operator determined single unknown parameter c shown figure 6d problem find c operators fourier transform sharp peak 1t x 1t rough measure sharpness given 0505048 0505048 c c c c c c c c 41c b c c figure 3x3 laplacian b fourier transform c kernel structure 3x3 operator symmetric kernel 3x3 operator insensitive zerofrequency component see texte second moment four operator peaks minimized response tuned focus operator much sharper peaks laplacian secondorder moment power jj lu v jj 2 respect 1t x 1t measure minimized shown figure 6e resulting tuned focus operator response shown figure 6f substantially sharper peaks discrete laplacian given operator 3x3 discrete sharpness peaks limited derivation brings light fundamental difference designing tuned operators continuous discrete domains general operator deemed optimal continuous domain likely suboptimal discrete images 7 depth two images depth estimation uses two images scene 1 x 2 x correspond different effective focal lengths shown figure 3 depth scene point determined estimating displacement ff focused plane f scene point tuned focus operator applied images get focus measure images g 1 x g 2 x 33 see 23 see factor g 0 affected parameter ff defocus function h therefore note measure bounded poses problem computational viewpoint easily remedied using following normalization shown figure 7 q monotonic function ff gammap q p p 1 practice relation precomputed stored lookup table maps q computed image point unique ff since ff represents position focused image lens law 1 yields depth corresponding scene point note tuned focus operator designed previous section linear filter making feasible compute depth maps scenes realtime using simple image processing hardware figure 7 relation focus measures g 1 g 2 defocus parameter ff 8 real time range sensor based results implemented realtime focus range sensor shown figure 8 scene imaged using standard 125 mm fujinon lens additional aperture added convert telecentric light rays passing lens split two directions using beamsplitting prism produces two images simultaneously detected using two sony xc77rr 8bit ccd cameras positions two cameras precisely fixed one obtains nearfocus image farfocus image setup physical displacement 025mm effective focal lengths two ccd cameras translates sensor depth field approximately cms detectable range sensor varied either changing sensor displacement focal length imaging optics illumination pattern shown figure 5b etched glass plate using mi crolithography process widely used vlsi filter placed path 300 w xenon arc lamp illumination pattern generated projected using telecentric lens identical one used image formation halfmirror used ensure illumination pattern projects onto scene via optical path used acquire images result pattern almost perfectly registered respect pixels two ccd cameras furthermore arrangement ensures every scene point visible sensor also illuminated avoiding shadows thus undetectable regions objects scene strong specular reflection component crosspolarized filters attached illumination imaging lens filter specularities produce images mainly include diffuse reflection component images two ccd cameras digitized processed using mv200 datacube image processing hardware present configuration includes equivalent two 8bit dig itizers two ad convertors one 12bit convolver hardware enables simultaneous digitization two images convolution images tuned focus operator computation 256x240 depth map within single frametime 33 msec lag 33 msec lookup table used map pair focus measures g 1 g 2 unique depth estimate see authors1994 imlpementation details alternatively 512x480 depth map computed rate two images taken succession simultaneous image acquisition clearly advantageous since makes sensor less sensitive variations illumination scene structure frames minor additions present processing hardware easy obtain 512x480 depth maps using simultaneous image grabbing depth maps produced sensor visualized wireframes framerate dec alpha workstation imaging optics filter b figure 8 realtime focus range sensor key components b sensor produce depth maps 512x480 resolution 9 experiments numerous experiments conducted test performance sensor briefly summarize results figure 9a shows near far focused images planar surface half surface textureless half strong random texture computed depth map surface shown figure 9b expected textureless area estimated almost free errors textured area small errors due texture frequencies lie close illumination frequency may noted texture used experiment includes wide spectrum frequencies scenes weaker textures expected produce even accurate results several depth maps plane figure 9a computed varying position 30cm workspace sensor average accuracy repeatability sensor estimated simultaneous successive image grabbing configurations see table 9c results clearly demonstrate superior performance sensor previous implementations depth defocus improvement results several factors including accurate modeling sensor optics use optimized illumination pattern careful imlementation sensor since depth computed subtle differences near far focused images high accuracy achieved careful analysis system components figure shows scene polyhedral objects computed depth map figure 10b fairly accurate despite complex textural properties objects filtering applied depth map 5x5 smoothing function reduce high frequency noise computed depth results low signaltonoise ratio ccd cameras spurious frequencies caused surface texture surface discontinuities orientation discontinuities well preserved recovered shapes precise enough variety visual tasks including recognition inspection similar results shown figure 11 shapes curved objects recovered case dynamic scenes structure estimated using realtime sensor figure 12 shows objects depth map computed rotates motorized turntable depth map sequences valuable automatic cad model generation sample objects computed cad models useful visual recognition tasks also graphics rendering cases object models often manually designed input system process tedious also impractical large numbers complex objects furthermore realtime depth computation clearly enhances capability vision system enables recovery deforming shape precise tracking moving objects robust navigation dynamic scenes summary reported theoretical results variety issues related depth estimation focus analysis accurate modeling optics sensing shown essential precise depth estimation textured textureless surfaces recovered using optimized illumination pattern registered image sensor also presented elegant solution constant magnification defocusing problem limited precision depth defocus algorithms results used implement realtime focus range sensor produces high resolution depth maps frame rate sensor unique ability produce fast dense precise depth information low cost time expect sensor find applications ranging visual recognition robot control automatic cad model generation visualization virtual reality obvious extension work development passive focus range finder outdoor scenes sensor cannot afford luxury projected illumination must rely complex scene textures depth estimation problem shall pursued future work c depth accuracy rms repeatability rms spatial resolution speed delay simulatneous image grab successive image grab figure 9 near focused image planar surface includes highly textured textureless areas b depth surface computed using focus range sensor c performance characteristics sensor figure 10 near far focused images set polyhedral objects b computed depth map figure 11 near far focused images set curved objects b computed depth map e f b c figure 12 depth maps generated sensor object rotates motorized turntable details illumination optimization consider expression 29 section 5 25 see b x figures illumination model illumination cell function c function b x illumination grid function g therefore c 45 b x sin b xt x b xt x sin b yt b yt sin b yt b yt sin b xt x sin b yt b yt yields constraint b xt x note illumination cell size b x cannot exceed illumination pitch x else cells overlap causing resulting illumination lie outside range illuminations implemented practice overlap cells occurs model defined section 41 predicts illumination intensity overlap region exceeds intensity within nonoverlapping regions cell situation though easy express theory difficult realize practice therefore additional constraint namely illumination cell must smaller equal illumination pitch 0 given 46 gives unique solution using exactly procedure get next evaluate partial derivatives 30 25 seen parameter q x occurs focus operator l hence sin 2q xt x gives constraint note objective maximize absolute value g 0 could happen g 0 either maximum minimum since g 0 x product cosines maximum absolute value positive negative also goal achieve highest depth resolution requires focus operator l minimum possible support ensuring depth computed using smallest neighborhood therefore 50 leads solution similarly order verify solutions maximize absolute value g 0 following second partial derivatives g 0 respect parameters used sin b xt x sin b yt sin b xt x sin b yt deltab x sin b xt x b xt x sin b 1 b yt deltab x sin b xt x b xt x sin b yt b yt b q x b q parameters values given 47 48 51 52 second derivatives 53 54 55 56 sign verifying solutions 47 48 51 52 maximize j g 0 j summarize sensitivity focus measure function gx maximized r range imaging sensors principles optics fourier transform applications pyramid based matrix based method determining depth focus structured light computing depth outoffocus blur using local frequency representation depth focus vlsi smart sensor fast range imaging focusing technical report memo 160 robot vision range imaging system 3d object recognition perspective range finding techniques computer vision fast vlsi rangefinder optical system design range estimation focus using nonfrontal imaging camera international journal computer vision robust focus ranging shape focus effective approach rough surfaces microscopic shape focus using active illumination simple simple range cameras based focal error new sense depth field application spatialdomian con volutiondeconvolution transform determining distance image defocus parallel depth recovery changing camera parameters introduction optical transfer function modeling calibration automated zoom lenses moment hypergeometric filters high precision computation focus variable window gabor filters use focus correspondence tr ctr modified fuzzy cmeans image segmentation algorithm use uneven illumination patterns pattern recognition v40 n11 p30053011 november 2007 aamir saeed malik taesun choi consideration illumination effects optimization window size accurate calculation depth map 3d shape recovery pattern recognition v40 n1 p154170 january 2007 soonyong park imagebased calibration technique spatial domain depthfromdefocus pattern recognition letters v27 n12 p13181324 september 2006 marc proesmans luc van gool reading linesa method extracting dynamic 3d texture proceedings acm symposium virtual reality software technology p95102 september 1997 lausanne switzerland jerome edward lengyel compression timedependent geometry proceedings 1999 symposium interactive 3d graphics p8995 april 2629 1999 atlanta georgia united states rajan subhasis chaudhuri simultaneous estimation superresolved scene depth map low resolution defocused observations ieee transactions pattern analysis machine intelligence v25 n9 p11021117 september morgan mcguire wojciech matusik hanspeter pfister john f hughes frdo durand defocus video matting acm transactions graphics tog v24 n3 july 2005 vinay p namboodiri subhasis chaudhuri defocus diffusion depth estimation pattern recognition letters v28 n3 p311319 february 2007 n rajagopalan chaudhuri performance analysis maximum likelihood estimator recovery depth defocused images optimal selection ofcamera parameters international journal computer vision v30 n3 p175190 dec 1998 szymon rusinkiewicz olaf hallholt marc levoy realtime 3d model acquisition acm transactions graphics tog v21 n3 july 2002 paolo favaro stefano soatto geometric approach shape defocus ieee transactions pattern analysis machine intelligence v27 n3 p406417 march 2005 zhang shree nayar projection defocus analysis scene capture image display acm transactions graphics tog v25 n3 july 2006 zhang noah snavely brian curless steven seitz spacetime faces high resolution capture modeling animation acm transactions graphics tog v23 n3 august 2004 vincent lepetit pascal fua monocular modelbased 3d tracking rigid objects foundations trends computer graphics vision v1 n1 p189 september 2006