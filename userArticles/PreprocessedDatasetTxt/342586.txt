class highly scalable optical crossbarconnected interconnection networks socns parallel computing systems abstracta class highly scalable interconnect topologies called scalable optical crossbarconnected interconnection networks socns proposed proposed class networks combines use tunable vertical cavity surface emitting lasers vcsels wavelength division multiplexing wdm scalable hierarchical network architecture implement largescale optical crossbar based networks freespace optical waveguidebased crossbar interconnect utilizing tunable vcsel arrays proposed interconnecting processor elements within local cluster similar wdm optical crossbar using optical fibers proposed implementing intercluster crossbar links combination two technologies produces largescale optical fanout switches could used implement relatively low cost large scale high bandwidth low latency fully connected crossbar clusters supporting hundreds processors extension crossbar network architecture also proposed implements hybrid network architecture much scalable could used connect thousands processors multiprocessor configuration maintaining low latency high bandwidth architecture could suitable constructing relatively inexpensive highly scalable high bandwidth faulttolerant interconnects largescale massively parallel computer systems paper presents thorough analysis two example topologies including comparison two topologies popular networks addition overview proposed optical implementation power budget presented along analysis proposed media access control protocols corresponding optical implementation b opposed l n2 log2nlinks required standard binary hypercube multiplexing greatly reduces link complexity entire network reducing implementation costs proportionately 42 network diameter diameter network defined minimum distance two distant processors network since processor ohc2n cluster communicate directly every processor directly connected cluster diameter ohc2n containing nh n 2d processors kh log2 9 dependent degree hypercube diameter degree hypercube network 43 bisection width bisection width network defined minimum number links network must broken partition network two equal sized halves bisection width ddimensional binary hypercube 2d1 since many links connected two 1dimensional hypercubes form ddimensional hypercube since link ohc2n contains n channels bisection width ohc2n increases linearly number processors major benefit topology large number processors connected relatively small diameter relatively fewer intercluster connec tions example n processors per cluster fiber links per cluster 1024 processors connected high degree connectivity high bandwidth diameter network 6 implies low network latency large system 192 bidirectional intercluster links required system containing number processors constructed using pure binary hypercube topology would require network diameter 10 5120 interprocessor links webb louri class highly scalable optical crossbarconnected interconnection networks socns 449 44 average message distance cluster network 1 would increase average message distance network defined size network c average number links message traverse network slightly better measure network latency diameter aggregates would effect cluster node degree distances entire network rather looking network similar fixedc case maximum distance average message distance l oc2n configuration granularity size scaling calculated 27 case also number clusters c easiest method scaling require addition network hardware fully utilizes l ini 11 inherently high bandwidth wdm optical links two topologies presented paper ni represents number processors distance means two topologies could utilized reference processor n total number construct socn class network example assume processors network k diameter socn network exists configured torus network configuration addition number since ohc2n hybrid binary hypercube processors required total number processors crossbar network equation number required final network may number supported processors given distance ohc2n derived ohc2n configuration case network equation binary hypercube could reconfigured ohc2n configuration simply changing routing intercluster links changing routing algorithms reconfigurability makes conceivable reconfigure socn class network since cluster ohc2n hypercube topology relatively arbitrary granularity size scaling contains n processors number processors 46 fault tolerance congestion avoidance distance ohc2n calculated since ohc2n architecture combines edges k hypercube network edges crossbar network tolerance congestion avoidance schemes architectures combined even powerful addition n 1 1 account congestion avoidance scheme hypercube routers typically processors within local cluster substituting 11 scan bits destination address looking computing summation gives equation difference bits destination address average messages distance ohc2n routers address difference found kn message routed along dimension multiple bits differ router may choose dimensions along route message number substituting diameter ohc2n produces redundant links available source processor along optimum path destination processor equal hamming distance addresses two respective processors one links 45 granularity size scaling ohc2n one links congested due traffic routed connecting router message ohc2n hypercube connected crossbar network routed along one dimensions containing fixed number processors per cluster nwe addition crossbar network connections increase network size increasing size clusters greatly increases routing choices routers second level hypercube topology since granularity message must transmitted using wave size scaling cprocessor hypercube c would length destination processor transmitted require addition c clusters increase size last link transmission link directly ohc2n fixedn case c2 2c1 increasing size connected destination processor message ohc2n fixedn case would also require adding transmitted channel link along routing path means router along path another intercluster link cluster network message traversal choice links based increasing intercluster node degree one case hypercube routing algorithm also choice n granularity size scaling different channels along links router may choose n links connect local cluster remote cluster feature greatly increases fault assume instead fixedc case increase tolerance network well link load balancing network size adding another processor congestion avoidance properties network 450 ieee transactions parallel distributed systems vol 11 5 may 2000 comparison size degree diameter number links several popular networks number processors per cluster number clusters number processors per busringmultichannel link total number processors example hamming distance cluster address current router destination processor cluster address equal b router choice b n different channels choose route even links along routing dimensions given message congested message still routed around failurecongestion via links along nonoptimal paths long network partitioned addition nonshortest path routing algorithms used reduce network congestion many route choices made available 5comparison popular networks section present analysis scalability socn architecture respect several scalability parameters bisection width used measure bandwidth network diameter average message distance used measures latency network common measures cost complexity interconnection network node degree network number interconnection links node degree number links network relates number parts required construct network cost though also determined technology routing algorithms communication protocols used construct network traditionally optical interconnects considered costly alternative electrical interconnects recent advances highly integrated low power arrays emitters eg vcsels tunable vcsels detectors inexpensive polymer waveguides low cost microoptical components reduce cost increase scalability high performance computer networks make higher node degrees possible also cost effective oc3n ohc2n configurations compared several wellknown network topologies shown implementable optics network topologies include traditional crossbar network cb binary hypercube bhc 27 cube connected cycles ccc 28 torus 29 spanning bus hypercube sbh 30 spanning multichannel linked hypercube smlh 25 networks compared respect degree diameter number links bisection bandwidth average message dis tance tradeoffs oc3n ohc2n configuration configurations socn class network might considered various applications shown oc3n ohc2n provide distinct advantages medium sized largescale parallel computing architectures various topological characteristics compared networks shown tables 1 2 notation oc3nn 16c implies number processors per cluster n fixed number clusters c changed order vary number processors n notation ohc2nn 16d implies number processors per cluster n fixed dimensionality comparison bisection bandwidth average message distance several popular networks webb louri class highly scalable optical crossbarconnected interconnection networks socns 451 hypercube varied number processors variable standard crossbar cbn implies crossbar containing n processors binary hypercube dimensionality hypercube varies size network notation cccd implies number dimensions cube connected cycles varies notation torusw 3 implies dimensionality fixed size rings n varies number processors notation sbhw 3d implies size buses sbh network w remains constant dimensionality changes notation smlhw 32d denotes number multichannel links w kept constant dimensionality hypercube varied 51 network degree fig 4 shows comparison node degree various networks respect system size number processing elements seen medium size networks containing 128 processors less two examples oc3n networks provide respectable cluster degree 4 oc3nn 16c configuration 8 oc3nn 32c configuration implies fully connected crossbar network constructed system containing 128 processors node degree low 4 traditional crossbar would course require node degree 127 size system node degrees ohc2nn 16d ohc2nn 32d configurations respectable much larger system sizes system containing order 10 000 processor ohc2nn 16d ohc2nn 32d configurations would require node degree around 78 comparable networks much better 52 network diameter fig 5 shows comparison diameter various networks respect system size network diameter good measure maximum latency network length shortest path two distant nodes network course diameter oc3n network best node directly connected every node diameter oc3n network identically 1 expected diameter various ohc2n networks scale bhc network fixed negative bias due number channels crossbar smlhw networks also scale bhc network larger fixed bias 10 000 processor configuration various ohc2n networks comparable better comparison net works although smlhw networks better larger inherent fixed bias 53 number network links number links along degree network good measure overall cost implementing network ultimately link must translate sort wires waveguides optical fibers least set optical components lenses gratings etc noted comparison number interprocessorintercluster links network link could consist multiple physical data paths example electrical interface would likely consist multiple wires proposed optical implementation socn crossbar consists optical fiber pair send receive per intercluster link fig 6 shows plot number network links respect number processors system oc3n network compares well small medium sized systems although number links could become prohibitive number processors gets large ohc2n network configurations show much better scalability number links largescale systems case around 10 000 processors ohc2nn 32d network shows greater order magnitude less links network architecture 54 bisection width bisection width network good measure overall bandwidth network bisection width network scale close linearly number processors scalable network bisection width scale well interconnection network become bottleneck number processors increased fig 7 shows plot bisection width various network architectures respect number processors system course oc3n clearly provides best bisection width number interprocessor links oc3n increases factor on2with respect number processors ohc2n configurations comparable best remaining networks much better less scalable networks 55 average message distance average message distance within network good measure overall network latency average message distance better measure network latency diameter network average message distance aggregated entire network provides average latency rather maximum latency fig 8 shows plot average message distance respect number processors system course oc3n provides best possible average message distance 1 processor connected every processor ohc2n network configurations displays good average message distance medium largescale configurations good average message distance smlh networks much better remaining networks 6optical implementation socn tunable vcsels provide basis designing compact alloptical crossbars high speed multiprocessor intercon nects overview compact alloptical crossbar seen fig 9 single tunable vcsel single fixed frequency optical receiver integrated onto processor network tight coupling optical 452 ieee transactions parallel distributed systems vol 11 5 may 2000 fig 4 comparison network degree respect system size various networks fig 5 comparison network diameter respect system size various networks fig 6 comparison total number network interconnection links respect system size various networks webb louri class highly scalable optical crossbarconnected interconnection networks socns 453 fig 7 comparison bisection width respect system size various networks fig 8 comparison average message distance within network respect system size various networks transceivers processor electronics provides alloptical path directly processor processor taking full advantage bandwidth latency advantages optics network optical signal processor directly coupled polymer waveguides route signal around pc board waveguide based optical combiner network polymer waveguides used design provide potentially low cost alloptical signal path constructed using relatively standard manufacturing techniques shown polymer waveguides constructed relatively small losses greater 30db crosstalk isolation waveguide dimensions order 50m 50m 60m pitch 31 implying relatively largescale crossbar optical combiner network could constructed within area square centimeters combined optical signal optical combiner routed freespace optical demultiplexercrossbar within optical demultiplexer passive freespace optics utilized direct beam appropriate destination waveguide seen inset fig 9 beam emitted input optical waveguide shines concave reflective diffraction grating diffracts beam diffraction angle dependent wavelength beam focuses beam appropriate destination waveguide diffraction angle varies wavelength beam wavelength beam define destination waveguide hence processor receives transmitted signal processor assigned particular wavelength receive based location waveguide output waveguide array example processor 1 transmit processor 3 processor 1 would simply transmit wavelength assigned processor 3 eg 3 processor transmitting different wavelength signal routed simultaneously appropriate destination processor ensuring two processors transmitting wavelength function 454 ieee transactions parallel distributed systems vol 11 5 may 2000 fig 9 proposed compact optical crossbar consisting polymer waveguides directly coupled processor mounted vcsels polymer waveguide based optical combiner compact freespace optical crossbardemultiplexer proposed optical crossbar connected remote processors using single optical fiber connected locally eliminating optical fiber media access control mac protocol detailed later section routing freespace optical demultiplex er separate optical signals routed appropriate destination processor via additional integrated optical waveguides seen fig 9 combined optical signal optical combiner network demultiplexer coupled single optical fiber route remote pc board implement intercluster optical crossbar short length polymer waveguide could replace optical fiber implement local intracluster optical crossbar power budget signaltonoise ratio snr analysis conducted intracluster intercluster optical crossbars 32 33 34 result power budget analysis shown table 3 assuming necessary receiver power 30dbm vcsel power 2dbm 35 required biterror rate ber 1015itwas determined current research level technology processors could supported network processors nearly possible details optical implementation socn crossbar interconnect thorough analysis optical implementation found references 32 33 34 36 7media access socn socn network contains local intracluster wdm subnetwork multiple intercluster wdm subnetworks processing node intracluster intercluster subnetworks medium shared processors connected subnetwork subnetworks optically isolated media access handled independently subnetwork one advantage socn network subnetwork connects processors cluster processors single remote cluster optical media shared among processors cluster implies media access control interaction required processors cluster processors different clusters transmit remote processor time transmitting different media could cause conflicts contention receiving processor conflicts issue flow control scope paper 71 socn mac overview socn network processors cannot directly sense state communication channels access must method processors coordinate access shared media one method accomplishing secondary broadcast controlreservation channel particularly advantageousinasocnclasnetworkbecausethe coordination need happen among processors local cluster implies control channel local cluster saving cost running intercluster cabling ensuring constructed least latency possible controlchannel based networks latency control channel particularly critical channel must reserved control channel message transmitted data network latency control channel adds directly data transfer latency determining overall network latency since multiple physical channels cluster local intracluster network various intercluster network connections conceivable physical data channel could require dedicated control channel webb louri class highly scalable optical crossbarconnected interconnection networks socns 455 db component optical crossbar fortunately physical data channel given cluster shared set processors possible control access data channels cluster using single control channel cluster wdm channel physical channel treated shared channel mac arbitration controlled globally control channel 72 carrier sense multiple access collision detection csmacd mac protocol assume control channel required one possible implementation mac protocol would allow processors broadcast channel allocation requests control channel prior transmitting data channel case protocol would need devised resolve conflicts control channel one candidate might carrier sense multiple accesscollision detection csma cd protocol running csmacd control channel request access shared data channels similar standard csmacd protocols used ethernet net works except ethernet broadcast network node see everything transmit csmacd used within ethernet run data network separate control channel required advantages using csmacd media access control protocol primary advantage minimum latency accessing control channel zero primary disadvantage using protocol socn based system requires state information maintained node network processing node must monitor control channel track channels requested channel quested processor must remember request know channel busy wishes transmit also question data channel becomes available requested node could required relinquish data channel finished transmitting data channel available message control channel would double utilization control channel increasing chances conflicts increasing latency requirement large amount state information maintained node also increases chances node could get ofsync creating conflicts errors data network 721 thornbased media access control protocol another promising control channel based media access control protocol proposed horn network 24 protocol referred token hierarchical optical ring network protocol thorn token based protocol based decoupled multichannel optical network dmon protocol 37 thorn protocol tokens passed control channel virtual token ring seen fig 10 thorn tokens contain bit field containing activeinactive state data channels also bit field token used request access channel currently busy addition optional payload field used transmit small high priority data packets directly control channel state information maintained token local state information required processing nodes network although processors may store previous token state eventuality token might lost processor going network error eventuality previous token state could used regenerate token still requires processors maintain small amount state information state information would constantly refreshed would seldom used chances state becoming outofsync minimal seen fig 11 single control channel number data channels tokens continuously passed control channel hold entire state data channels processing node wishes transmit particular data channel must wait token received control channel checks fig 10 layout thornbased token request packet token packet contains one bit per channel busy status one bit per channel channel requests token packet also contains optional payload small low latency messages 456 ieee transactions parallel distributed systems vol 11 5 may 2000 fig 11 timing diagram control tokens data transfers socn architecture using form thorn protocol node may transmit data channel soon acquires appropriate token bit setting request bit forces relinquishing data channel busy bit requested data channel see set busy bit set data channel currently active processing node immediately begin transmitting data channel must also broadcast token setting busy bit data channel transmitting busy bit already set implies transmitter currently using requested data channel case processing node must set request bit desired data channel indicates processing node currently transmitting desired channel another transmitter requesting channel disadvantage tokenring based media access control protocol average latency requesting channels likely higher csmacd protocol assume single control channel per cluster cluster containing n processors physical data channels one intracluster subnetwork 1 intercluster subnetworks control token would contain n bits busy bits n request bits example system contains n processors per cluster 8 wdm subnetwork links control token would require 128 busy bits 128 request bits assume control channel bandwidth 2gbps ignore possibility token payload achieve maximum token rotation time trt 128ns assuming node starts retransmitting token soon starts receiving token eliminating token holding latency would imply minimum latency requesting channel close zero assuming token arrive requesting processing node maximum 128ns would give average control channel imposed latency approximately 64ns lower latency required csmacd protocol could implemented multiple control channels could constructed would reduce latency proportionally 73 control channel optical implementation irrespective media access control protocol dedicated control channel required broadcast processor sharing transmit access data channel since physical data channel shared among processors within cluster control channel implemented local cluster simplify design implementation control channel require routing extra optical fibers clusters impose optical loss penalties associated routing optical signals local cluster implementation broadcast optical control channel depicted fig 12 optical signal dedicated vcsel processor routed polymer waveguide based star coupler combines signals processors cluster broadcasts combined signals back processor creating essentially optical bus primary limitation broadcast based optical network optical splitting losses encountered star coupler using similar system basis power budget estimation 38 yields estimated optical loss control network approximately 8db 3db log2n table 4 would support approximately 128 processor per cluster control channel assume minimum required receiver power 30dbm vcsel power 2dbm optical implementation socn mac network throughly analyzed due page limitation analysis could included article fig 12 optical implementation dedicated optical control bus using integrated polymer waveguidebased optical star coupler webb louri class highly scalable optical crossbarconnected interconnection networks socns 457 db component optical crossbar paper presents design proposed optical 10 network utilizes dense wavelength division multi plexing intracluster intercluster communication links novel architecture fully utilizes benefits wavelength division multiplexing produce highly scalable high bandwidth network low overall latency could cost effective produce 13 design intracluster links utilizing simple grating multiplexerdemultiplexer implement local free space 14 crossbar switch presented cost effective implementation intercluster fiber optic links 15 also presented utilizes wavelength division multiplexing greatly reduce number fibers required interconnecting clusters wavelength reuse 16 utilized multiple fibers provide high degree scalability fiberbased intercluster interconnects 17 presented could configured produce fully connected crossbar network consisting tens hundreds 18 processors could also configured produce hybrid network interconnected crossbars could 19 scalable thousands processors network architecture could provide high bandwidth low latency communications required produce large distributed shared memory parallel processing systems r interconnection networks engineering approach high performance computing challenges future stanford dash homogeneous hierarchical interconnection structures cray research inc hierarchical multiprocessor interconnection networks area interconnection network hypercubes parallel distributed systems multicomputer systems scalable photonic architectures high performance processor newsletters computer architecture technical committee computer architecture quantitative approach optical information processing optical computer architectures application optical concepts next generation computers introduction photonic switching fabrics multicomputers gradually scalable optical interconnection network massively parallel computing distributed systems choices versatile network parallel computation ieee transactions parallel distributed systems projects involved parallel distributed processing current member ieee published numerous journal conference articles topics article recipient advanced telecommunications organization japan fellowship scientifique cnrs japanese society promotion science fellowship computer research institute university southern served member technical program committee several osaieee conference massively parallel processors using optical edudepartmentocppl tr ctr lachlan l h andrew fast simulation wavelength continuous wdm networks ieeeacm transactions networking ton v12 n4 p759765 august 2004 roger chamberlain mark franklin praveen krishnamurthy abhijit mahajan vlsi photonic ring multicomputer interconnect architecture signal processing performance journal vlsi signal processing systems v40 n1 p5772 may 2005 david erel dror g feitelson communication models freespace optical crossconnect switch journal supercomputing v27 n1 p1948 january 2004 ahmed louri avinash karanth kodi optical interconnection network modified snooping protocol design largescale symmetric multiprocessors smps ieee transactions parallel distributed systems v15 n12 p10931104 december 2004 peter k k loh w j hsu faulttolerant routing complete josephus cubes parallel computing v30 n910 p11511167 septemberoctober 2004 nevin kirman meyrem kirman rajeev k dokania jose f martinez alyssa b apsel matthew watkins david h albonesi leveraging optical technology future busbased chip multiprocessors proceedings 39th annual ieeeacm international symposium microarchitecture p492503 december 0913 2006