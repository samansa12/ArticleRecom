data fitting problems bounded uncertainties data analysis class data fitting problems data uncertainties subject known bounds given general setting shown problems posed computationally convenient form connection conventional data fitting problems examined problems attracted interest far special case underlying norm least squares norm special structure exploited computational advantage include observations contribute algorithmic development particular case also consider variants main problems show posed form facilitates numerical solution b introduction let 2 r mn arise observed data conventional tting problem minimize krk x 2 r n norm norm r involves assumption exact errors b may case many practical situations eect errors well b recognized studied many years mainly statistics literature one way take general case account solve problem subject matrix norm one mn1 matrices problem matrix norm frobenius norm rst analyzed golub van loan 10 used term total least squares developed algorithm based singular value decomposition b since problem attracted considerable attention see example 18 19 formulation 11 often satisfactory lead solution perturbations e quite large however may case exam ple known nearly exact resulting correction may therefore excessive particular bounds known size perturbations makes sense incorporate problem formulation means equality constraints 11 relaxed satised approxi mately observations motivated new parameter estimation formulations b subject errors addition quantities e bounded known bounds idea gives rise number dierent received editors may 25 1999 accepted publication revised form l el ghaoui december 5 2000 published electronically april 6 2001 httpwwwsiamorgjournalssimax22435659html department mathematics university dundee dundee dd1 4hn scotland gawatson mathsdundeeacuk closely related problems algorithms analysis problems type based least squares norms given example 1 2 3 4 5 8 9 15 17 general problem 11 amenable analysis algorithmic development wide class matrix norms known separable norms concept introduced osborne watson 13 main purpose paper show problems bounded uncertainties also considered general setting particular shown problems posed computationally convenient form well facilitating numerical solution enables connections conventional data tting problems readily established motivation extending ideas beyond familiar least squares setting provided important role norms play conventional data tting contexts continue introductory section dening separable norms introducing necessary notation tools rst introduce concept dual norm let kk norm r v 2 r dual norm norm r dened r v relationship norm r dual symmetric r r v definition 11 matrix norm n matrices said separable given vectors u vector norms kk r kk b r n commonly occurring matrix norms operator norms orthogonally invariant norms norms based l p vector norm elements matrix treated extended vector r mn separable result holds separable norms subsequently useful see example 13 20 another valuable tool subdierential vector norm extends idea derivative nondierentiable case useful characterization subdierential kk follows definition 12 subdierential set subgradients krka set 1g norm dierentiable r subdierential unique vector partial derivatives norm respect components r main emphasis paper problems address eects worst case perturbations gives rise problems minmax type section 2 consider problems separate bounds kek kdka assumed known section 3 consider similar problem except single bound ke dk given cases matrix norm assumed separable section 4 variants original problems considered nally section 5 consider related class problems minmin rather minmax type 1276 g watson 2 known bounds kek kdka suppose underlying problem know bounds uncertainties b matrix norm separable norm denition 11 instead forcing equality constraints 11 satised wish satisfy approximately minimizing anorm dierence left righthand sides perturbations satisfying bounds leads problem therefore x minimizes worst case residual interpreted permitting robust solution obtained underlying data tting problem explanation signicance term robustness context least squares case see example 9 minimizing x referred robust least squares solution another interpretation problem solved guarantees eect uncertainties data never overestimated beyond assumptions made knowledge bounds show 21 restated much simpler form unconstrained problem x alone theorem 21 x maximum 21 attained otherwise u arbitrary 1 maximum value proof e kek kdka let e statement theorem result follows immediate consequence result problem 21 solved minimizing respect x kax bk therefore appropriate analyze problem particular give conditions x solution also conditions solution results consequence standard convex analysis found example 14 theorem 22 function 22 minimized x exists kxkb theorem 23 let exist v 2 kbka proof give minimum must v 2 kbka 23 satised kwk 1 result follows 21 connections least norm problems next establish connections solutions 22 solutions traditional minimum norm data tting problems 9 coincidence solutions least squares case said mean usual least squares solution may considered robust consider least norm problem minimize kax bk x solution exists v 2 kax bk solves 24 clearly also solves 22 note take 23 otherwise kk smooth solutions 22 24 coincide unique case let b solution denote minimum anorm solution x minimizes 22 23 satised w 2 kxkb kvk 1 otherwise v unrestricted follows must words also solution 22 b 2 rangea example norms least squares norms condition note b minimum bnorm solution immediately solves 22 must exist w inequality satised independently 1278 g watson case kk nonsmooth complicated example 21 let corresponds separable norm sum moduli components 24 solved x 1 x 2 solution 22 provided 2 summarize augment theorem 23 following interpreted generalization result 9 theorem 24 b 2 rangea solution provided also prove following connects theorems 23 24 theorem 25 let b 2 rangea min proof let otherwise arbitrary follows denition thus using 25 26 result follows consequence results b 2 rangea min point convex hull f0 bg solution 22 methods solution practical point view obviously importance eciently solve 21 equivalently 22 appropriate cases let commonly occurring norms either smooth typied l p norms 1 p 1 polyhedral typied l 1 l 1 norms norms denition f smooth derivative methods natural ones use reasonable assumption practical situations rangea would give derivative discontinuity solution f dierentiable neighborhood solution inside neighborhood derivative methods implemented straightforward manner theorem 23 tells us solution following theorem gives way identifying descent direction point event applies arbitrary norms theorem 26 assume vk b g descent direction f proof let stated conditions satised let dened statement theorem theorem 23 solution descent direction directional derivative f direction must negative arbitrary vk 0 result follows kk smooth v unique construction using result straightforward norm e norm given 1p f becomes g watson fact minimization f p q satisfying 1 readily achieved derivative methods using theorem 26 get started indeed normally case second derivatives exist easily calculated newtons method damped necessary used following step based theorem 26 hessian matrix f positive denite f convex newton direction descent direction away minimum numerical results given 21 polyhedral norms typied l 1 l 1 norms convex objective function 22 piecewise linear function therefore may posed linear programming problem solved appropriate methods arguably interesting case practical point view special case kk kk b least squares norm case particular features greatly facilitate computation chandrasekaran et al 1 3 exploit numerical method contrast problems considered involve minimization problem r n special features l 2 case exploited problem reduces one r 27 dierentiable 23 becomes let singular value decomposition u 2 r mm v 2 r nn orthogonal ng matrix singular values descending order magnitude let assumed follows rank n solution means particular b 1 6 means b 2 6 0 theorem 23 require shown 3 satises equation rearranged also shown 3 28 necessary sucient 210 exactly one positive root addition g 0 0 dierent methods used nding case one possibility suggested 29 simple iteration process interest investigate whether likely useful turns method always locally convergent following result shows theorem 27 let 210 exactly one positive root 211 locally convergent proof let satisfy 212 210 unique positive root dieren tiating g gives using g g g related using g g watson table simple iteration stack loss data 9 substituting 213 gives 0 follows using 215 g 0 0 result proved indeed simple iteration seems remarkably eective problems tried converged satisfactory way obvious starting points example stack loss data set daniel wood 6 4 performance dierent values shown table 1 iteration terminated new value diers previous one less 10 6 another example given using iowa wheat data draper smith 9 performance simple iteration case shown table 2 although simple iteration ways suggested formulation course higher order methods readily implemented secant method newtons method actual performance depend largely factors nature size problem relative goodness starting points 3 known bound ke dk suppose underlying problem know upper bounds uncertainties b form separable matrix norm given consider problem deter mining table simple iteration iowa wheat data 6 5912536 12925885 30839779 9 12927008 30881951 14 30882685 anorm r dened particular choice separable norm vice versa problem variants considered example el ghaoui lebret 8 9 matrix norm frobenius norm aand bnorms least squares norms arguing theorem 21 gives following result theorem 31 x maximum 31 attained vector 1 maximum value problem 31 therefore equivalent problem minimizing respect x kax bk standard convex analysis gives following result theorem 32 function 34 minimized x exists u 1 denotes rst n components u 31 connection least norm problems interest establish connections corresponding least norm problems 24 also minimize 34 monotonic norms kk b kk b monotonic norm r n1 kck b kdkb whenever jc solve 24 kk smooth solutions problem 31 cannot coincide unless b 2 rangea case section 21 let solution denote minimum anorm solution solution 34 must exist v kvk otherwise unrestricted 1284 g watson consists rst n components u therefore words solutions coincide b 2 rangea note kk b smooth u unique example norms least squares norms gives given 9 situation kk smooth course complicated consider example 21 0 arbitrary recall 24 solved x 1 x 2 unique solution problem minimizing 34 32 connection total approximation problems nature bound 31 means connection made total approximation problem 11 known 13 20 minimum value 11 coincides minimum problem subject smallest generalized singular value matrix b particular vector norms least squares norms smallest singular value minimum 11 obtained z z minimum 37 scaling z corresponds nonexistence solution 11 known also minimizing pair e given consider problem 3 equivalently kk smooth x solution problem provided consequence previous analysis example norms least squares norms gives see also 9 least squares case el ghaoui lebret 8 suggest using robust methods conjunction total approximation identify appropriate value idea rst solve total approximation problem 38 constructed total approximation solution solved set minimum value 37 course exceed righthand side 39 nothing solve 33 methods solution special case 34 norms kk kk b possibly dierent l p norms derivative methods may used let us make reasonable assumption x makes kax bk kax bk p dierentiable x contrast earlier problem since second term cannot identically zero f dierentiable x easily compute rst second derivatives f newtons method example implemented line search direction newton step always guarantee descent f convex eventually must able take full steps get second order convergence rate numerical results given 21 polyhedral norms occurring 34 linear programming techniques may used consider special case 2 analysis similar given section 22 given case leading similar numerical method particular problem considered el ghaoui lebret 8 9 main emphasis papers structured perturbations harder problem exact solution problem obtained present case method suggested similar given problem section 2 chandrasekaran et al 1 3 let singular value decomposition full rank assume also rangea optimality conditions 1286 g watson shown satises equation rearranged g dened 210 easily seen h least one positive root 0 3 may shown h fact exactly one positive root h 0 note restriction except positive consider simple iteration process theorem 33 iteration scheme 311 locally convergent proof rst show 2 show h h related h thus using substituting 312 gives follows using 313 h 0 0 result proved performance simple iteration case course similar method applied previous situation methods like secant method newtons method complicated give potentially better performance 4 modications dierent ways additional information may incorporated problems last two sections resulting appropriate modications problems example components b may exact case corresponding components e zero bounds may take dierent forms may submatrices e rather e also perturbation matrices may known structure want preserve examples possibilities considered section 41 exact columns rows problems columns possibly rows known exact see example 3 treatment given problems sections 2 3 demonstrate section 2 appropriate requirements problems section 3 obvious begin considering case certain columns known exact case following suitable reordering columns necessary general problem minimize separable matrix norm one dened matrices partition x x arguing theorem 21 following theorem 41 x maximum 41 attained otherwise u arbitrary 1 maximum value 1288 g watson therefore problem solved minimizing respect x kax bk consider case columns rows exact corresponds requirement perturb submatrix assume lower righthand submatrix appropriate problem minimize b 2 4 columns 3 4 rows matrix norm separable norm matrices unfortunately separable norm dened terms two vector norms kk r kk b r kk used 43 r get around potential con ict assuming kk dened length vector also assume introduction additional zero components change value norm attainment maximum 43 quite straightforward however prove following result theorem 42 let denote rst components r let r 2 denote last components let x solve problem subject r x solves 43 proof arguing previous results upper bound maximum subject constraints 43 dene set x 2 x dene u rst components zero last components forming vector u 2 arbitrary except 3 4 e b result proved course set x may empty case problem 43 still well dened clear matrix e vector dened maximum problem attained case obvious equivalent simpler problem 42 bounded columns e suppose columns e individually bounded e ith unit vector consider problem nding theorem 21 prove following result theorem 43 x maximum 45 attained otherwise u arbitrary 1 maximum value even least squares case objective function normally dieren tiable combination least squares norm weighted l 1 norm reposed smooth constrained optimization problem solved standard techniques 43 structured problems applications perturbation matrices known structure following problem considered el ghaoui lebret 9 given min kk x kk given norm r kk given norm r p dene consider maximum 46 attained solution problem maximize kr 0 mka subject assuming maximizing kr exceeds norm functions involved convex necessary conditions solution readily given exists r 1290 g watson using conditions easily seen therefore equivalent sense dual problem subject consider special case norms least squares norms necessary conditions written thus provided f nonsingular way solving problem based results given el ghaoui lebret 9 also consider problem kk chebyshev norm extending ideas general norms however look straightforward 5 minmin problem problems 21 31 examples minmax problems minimization carried respect x allowed perturbations data justied emphasis robustness however considerations may sucient minimize respect x simultaneously minimizing respect perturbations gives rise minmin problem considered least squares case 2 3 5 nal section brie consider problem two versions consistent treated sections 2 3 illustrate ideas involved consider nding min 5 contrast minmax case seeking nd solution x gives smallest possible error allowable perturbations problem replaced equivalent unconstrained optimization problem theorem 51 let small enough 51 equivalent problem minimizing respect x kax bk kx proof let 52 satised let x arbitrary let otherwise arbitrary x otherwise u arbitrary using 52 result follows two important dierences 53 34 rst relationship leading 53 requires condition second resulting problem convex problem nonconvexity 53 interpreted 2 equivalent using indenite metric spirit recent work robust estimation ltering see example 11 12 16 condition 52 satised exceed see section 32 attained indeed set local minimum 37 value corresponding point x generated local minimizer z stationary point 51 following argument shows necessary conditions x solve 37 exist v lagrange multiplier g watson multiplying z shows relationship z implies signv 2 kax bk words exist v 2 kax bk w 2 denotes rst n components w follows standard convex analysis x stationary point problem minimizing similar treatment given 51 replaced related problem nding min provided small enough equivalent problem nding fkax bk kxkb g algorithm given 2 solving least squares case problem similarities algorithms given involving solution nonlinear equation linear system x indeed clear many ideas apply minmax problems carry problems present type however consider 6 conclusions given analysis general setting range data tting problems attracted interest far special case least squares norms involved case likely useful practice consideration possibilities motivated valuable role norms play general data tting context main thrust analysis show original problems may posed simpler form permits numerical treatment wide range problems involving norms example l p norms also included observations contribute algorithmic development important least squares case acknowledgment grateful referees helpful comments improved presentation r parameter estimation presence bounded modeling errors parameter estimation presence bounded data uncertainties degenerate bounded errorsinvariables model fitting equations data applied regression analysis robust solutions least squares problems uncertain data robust solutions leastsquares problems uncertain data analysis total least squares problem recursive linear estimation krein spacespart theory filtering smoothing h 1 setting analysis total approximation problem separable norms convex analysis estimation presence multiple sources uncertainties applications inertia conditions minimization quadratic forms inde estimation control presence bounded data uncertainties ed recent advances total least squares techniques errorsin variables modeling total least squares problem computational aspects analysis choice norms data solving data tr