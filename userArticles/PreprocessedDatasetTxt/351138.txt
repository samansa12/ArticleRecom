modeling performance comparison reliability strategies distributed video servers abstractlarge scale video servers typically based disk arrays comprise multiple nodes many hard disks due large number components disk arrays susceptible disk node failures affect server reliability therefore fault tolerance must already addressed design video server fault tolerance consider paritybased well mirroringbased techniques various distribution granularities redundant data identify several reliability schemes compare terms server reliability per stream cost compute server reliability use continuous time markov chains evaluated using sharpe software package study covers independent disk failures dependent component failures propose new mirroring scheme called grouped onetoone scheme achieves highest reliability among schemes considered results paper indicate dividing server independent groups achieves best compromise server reliability cost per stream find smaller group size better tradeoff high server reliability low per stream cost b introduction 11 server design issues many multimedia applications online news interactive television videoondemand require large video servers capable transmitting video data thousands users contrary traditional file systems video servers subject realtime constraints impact storage retrieval delivery video data furthermore video servers must support high disk bandwidth data retrieval order serve large number video streams simultaneously attractive approach implementing video server relies disk arrays achieve high io performance high storage capacity ii gradually grow size iii cost efficient unfortunately large disk arrays vulnerable disk failures results poor reliability video server challenging task therefore design video servers provide high performance also high reliability video server considered paper composed many disk arrays also referred server nodes server node comprises set magnetic disk drives illustrated figure 1 directly attached network video store divided many blocks blocks distributed among disks nodes video server round robin fashion server nodes identical node containing number disks n total number server disks denotes number server nodes client client client client network node node node figure 1 video server architecture client consumes video server connected server nodes served every time interval called service round video block received client service round consumed service round i1 since transfer rate single disk much higher stream playback rate disk serve many streams one service round order efficiently retrieve multiple blocks disk one service round video server applies well known scan algorithm optimizes seek overhead reordering service requests important decision video server design concerns way data distributed striped disks avoid hot spots video partitioned video blocks distributed disks server already mentioned based way data retrieved disks literature distinguishes finegrained fgs coarsegrained cgs striping algorithms fgs retrieves one stream client multiple typically small blocks many disks single service round typical example fgs raid3 defined katz et al 1 derivations fgs include streaming raid tobagi et al 2 staggeredgroup scheme muntz et al 3 configuration planner scheme ghandeharizadeh et al 4 main drawback fgs suffers large buffer requirements proportional number disks server 5 6 cgs retrieves one stream one large video block single disk single service round next service round next video block retrieved possibly different disk raid5 classical example cgs oezden et al 7 6 showed cgs results higher throughput fgs amount resources see also vin et al 8 beadle et al 9 contribution 5 accordingly adopt cgs store original video data video server paper organized follows section 2 classifies reliability schemes based whether mirroring parity used ii distribution granularity redundant data related work discussed end section section 3 studies reliability modeling reliability schemes considered reliability modeling based continuous time markov chains ctmc concerns case independent disk failures well case dependent component failures focus section 4 server performance compare per stream cost different reliability schemes section 5 emphasizes tradeoff server reliability per stream cost studies effect varying group size server reliability per stream cost results section 5 lead conclusions paper presented section 6 12 contribution context video servers reliability addressed previously either applying paritybased techniques raid26 eg 10 8 6 11 5 applying mirroringbased techniques raid1 eg 12 13 14 however following aspects considered together ffl comparison several paritybased mirroringbased techniques consideration video server performance cost issues cost analysis concerns storage buffering costs achieve given server throughput ffl reliability modeling based distribution granularity redundant data order evaluate server reliability scheme considered perform detailed reliability modeling incorporates case independent disk failures case dependent component failures ffl performance cost reliability tradeoffs different paritybased well mirroringbased techniques study effect varying group size server reliability per stream cost determine best value group size technique use cgs storeretrieve original video blocks since outperforms fgs terms server throughput adding faulttolerance within video server implies storage redundant replicatedparity blocks remains decided redundant data going storedretrieved onfrom server mirroring limit interleaved declustering schemes 15 original data replicated data spread disks server parity limit raid5like schemes parity blocks evenly distributed server disks retain schemes since distribute load uniformly among server components additionally consider case original blocks video used normal operation mode disk failure mode replicatedparity blocks needed reconstruct lost original blocks stored failed component mirroring also called consists storing copies original data server disks main disadvantage mirroring schemes 100 storage overhead due storing complete copy original data reliability based parity consists storing parity data addition original data raid26 disk failure occurs parity blocks together remaining original data used reconstruct failed original blocks raid5 18 parity scheme requires one parity block gamma 1 original blocks total number server disks gamma 1 original blocks one parity block constitute parity group although additional storage volume small paritybased reliability server needs additional resources terms io bandwidth main memory working disk failure mode fact worst case whole parity group must retrieved temporarily kept buffer reconstruct lost block 5 distinguished second read strategy buffering strategy second read strategy doubles io bandwidth requirement 19 whereas buffering strategy increases buffer requirement compared failure free mode restrict discussion buffering strategy since achieves twice much throughput second read strategy 5 20 see buffering strategy becomes attractive terms server performance lower buffer requirements also regarding server reliability size parity group decreases 21 classification reliability schemes reliability schemes differ technique paritymirroring distribution granularity redundant data define distribution granularity redundant data ffl parity technique distribution granularity redundant data determined whether parity group comprises c disks server latter case assume server partitioned independent groups groups size containing c disks let c denote number groups server ffl mirroring technique distribution granularity redundant data two different aspects first aspect concerns whether original blocks one disk replicated one c remaining gamma 1 disks server second aspect concerns single original block replicated two ways distinguished first way replicates one original block entirely one replicated block 13 call entire block replication second way partitions one original block many subblocks stores subblock different disk 14 call subblock replication show later distinction entire block subblock replication decisive terms server performance throughput per stream cost table classifies mirroring parity schemes based distribution granularity use terms oneto one onetoall onetosome describe whether distribution granularity redundant data concerns one disk mirroring disks mirroringparity disks mirroringparity disks onetoone scheme mirroring possible since onetoone parity would mean size parity group equals 2 consists replicating original block mirroring hence symbol xxx table mirroring parity onetoone chained declustering 21 22 xxx onetoall entire block replication doubly striped 13 23 raid5 one group 18 subblock replication 15 onetosome entire block replication raid5 many groups 3 8 7 subblock replication 14 table 1 classification different reliability schemes table distinguishes seven schemes give schemes example data layout thereby assume video server contains 6 disks stores single video stored video assumed divided exactly blocks schemes store original blocks order round robin starting disk 0 figures 2 3 remains describe storage redundant data schemes figure presents examples mirroringbased schemes schemes common disk partitioned two separate parts first part storing original blocks second part storing replicated blocks illustrated figure 2a onetoone mirroring scheme mirr one simply replicates original blocks one disk onto another disk one disk fails load entirely shifted replicated disk creates loadimbalances within server main drawback onetoone scheme original blocks copies organization mirrone 0000001111110000001111110000001111110000001111110000001111110000001111110000001111110000001111110000001111110000001111118315105171220 21 22 23 24 26 original blocks copies b onetoall organization entire block replication mirr allgammaentire 0000001111110000001111110000001111110000001111110000001111118315105171220 21 22 23 24 26 28 29 30719original blocks copies 71 72 73 74 75 131 132 133 134 135 191 192 193 194 195 251 252 253 254 255 11 12 13 14 15 c onetoall organization subblocks replication mirr allgammasub 0000001111110000001111110000001111110000001111110000001111110000001111110000001111110000001111110000001111110000001111118315105171220 21 22 23 24 26 original blocks copies onetosome organization entire block replication mirrsomegammaentire 26 28 29 30719original blocks copies 71 72 131 132 191 192 251 252 11 12 41 42 281 282 onetosome organization subblocks replication mirrsomegammasub figure 2 mirroringbased schemes order distribute load failed disk evenly among remaining disks server onetoall mirroring scheme applied shown figures 2b 2c figure 2b depicts entire block replication mirr allgammaentire figure 2c depicts subblock replication mirr allgammasub figure 2c show original blocks disk 0 replicated disks 1 2 3 4 5 look figures 2b 2c realize single disk failure allowed two disk failures occur server cannot ensure delivery video data onetosome mirroring scheme tradesoff loadimbalances onetoone mirroring scheme low reliability onetoall mirroring scheme fact shown figures 2d entire block replication mir somegammaentire 2e subblock replication mir somegammasub server divided multiple 2 independent groups group locally employs onetoall mirroring scheme thus original blocks one disk replicated remaining disks group therefore load failed disk distributed remaining disks group since group tolerates single disk failure server may survive multiple disk failures figure 3 presents two layout examples raid5 correspond onetoall parity scheme par figure 3a onetosome parity scheme par figure 3b figure 3a parity group size 6 eg 5 original blocks 16 17 18 19 20 parity block p 4 build parity group figure 3b parity groups size 3 eg 2 original blocks 17 parity block p 8 build parity group000111111000111111000111111000111111000111111000011111111 26 onetoall organization par 000111111000111111000111111000111111000111111 000000111000000111000000111000000111000000111000000111000000111000000111000111000000111p1 1 2 p2 3 4 26 p13 27 28 29 p14 b onetosome organization parsome figure 3 paritybased schemes looking figures 2 3 observe onetoall schemes mirroring entire block replication mirr allgammaentire mirroring subblock replication mirr allgammasub raid5 one group par tolerate one disk failure schemes therefore server reliability property holds onetosome schemes mirroring entire block replication mirroring subblock replication raid5 c groups since tolerate single disk failure group consequently enough reliability study consider three schemes classes onetoone onetoall oneto however performance study consider section 4 different schemes table 1 22 related work based raid 1 reliability addressed previously literature either general context file storage video server architectures mechanisms ensure faulttolerance adding redundant information original content classified paritybased schemes mirroringbased schemes extensive amount work carried context paritybased reliability see eg 24 2 17 10 25 26 7 27 28 papers ensure reliable realtime data delivery even one components fail papers differ way stripe data raid3 also called fgs raid5 also called cgs ii allocate parity information within server dedicated shared declustered randomly sequentially sid etc iii optimization goals throughput cost buffer requirement loadbalancing startup latency new client requests disk bandwidth utilization etc video servers using mirroring proposed previously see eg 12 15 23 14 29 21 22 however reliability modeling carried many mirroring schemes compared merchant et al 15 striping strategies replicated disk arrays analyzed depending striping granularity original replicated data distinguish uniform striping cgs original replicated data dedicated chained form dual striping original data striped using cgs replicated data striped using fgs however work different study many regards first authors assume copies used normal failure operation mode second comparison different mirroring schemes based mean response times throughput achieved without taking account server reliability finally authors analyze impact varying distribution granularity redundant data server reliability server performance general context redundant arrays inexpensive disks trivedi et al 30 analyzed reliability raid15 focused relationship disks mttf system reliability study based assumption raid system partitioned cold hot disks hot disks active normal operation mode case study reliability strategies video servers store redundant data separately dedicated disks distribute original redundant data evenly among server disks gibson 31 uses continuous markov models dissertation evaluate performance reliability paritybased redundant disk arrays context video servers reliability modeling paritybased schemes raid3 raid5 performed 32 raid3 raid5 compared using markov reward models calculate server avail ability results show raid5 better raid3 terms socalled performability availability combined performance best knowledge previous work context video servers compared several mirroring parity schemes various distribution granularities terms server reliability server performance costs 3 reliability modeling 31 motivation define server reliability time probability video server able access videos stored provided server components initially operational server survives long working components deliver video requested clients already mentioned server reliability depends distribution granularity redundant data independent whether mirroring parity used fact counts server reliability number disksnodes allowed fail without causing server fail example onetoall mirroring scheme entire block replication mirr allgammaentire tolerates single disk failure also case mirr allgammasub par three schemes therefore server reliability light fact use term onetoall denote three schemes purpose reliability study analogous onetoall term oneto represent three schemes mirr somegammaentire mirr somegammasub par term onetoone denotes onetoone mirroring scheme mirr one use continuous time markov chains ctmc server reliability modeling 33 ctmc discrete state spaces continuous time also referred markov process 34 35 assume mean time failure mttf every component exponentially distributed server mttf server reliability r following relationship assuming mttf 1 mttf mean time disk failure mttf equals 1 mean time disk repair mttr equals 1 build statespace diagram 34 corresponding ctmc introduce following parameters denotes total number states server denotes state markov chain probability server state time assume server fully operational time initial state additionally state gamma 1 denotes system failure state assumed absorbing state unlike previous work 32 markov model used compare performance raid3 raid5 allowed repair overall server failure video server attains state gamma 1 assumed stay infinite time previous assumption allows concentrate reliability study interval initial start time time first server failure occurs thus p 0 1 server reliability function r computed 34 r present remainder section markov models three schemes onetoall onetoone onetosome assuming independent disk failures section 32 ii dependent component failures section 33 32 reliability modeling independent disk failures 321 onetoall scheme onetoall scheme mirr allgammaentire mirr allgammasub par data lost least two disks failed corresponding statespace diagram shown figure 4 states 0 1 f denote respectively initial state one disk failure state server failure state l l d1 figure 4 statespace diagram onetoall scheme generator matrix q ctmc gammad 322 onetoone scheme onetoone scheme mirr one relevant mirroring onetoone schemes stores original data disk disk server fails two consecutive disks fail depending location failed disks server therefore tolerates number disk failures take values 1 dthus number disks allowed fail without making server fail known advance makes modeling onetoone scheme complicated let server disks numbered 0 gamma 1 assume server continues operate failures let p k probability server fail k th disk failure p k also probability disks failed consecutive adjacent calculate appendix probability p k k 2 2 2 obvious figure 5 shows statespace diagram mirr one server state 1 failure probability server fails state f equals probability server continuous operating n1 d2 f l l l figure 5 statespace diagram onetoone scheme parameters figure 5 following values corresponding generator matrix q ctmc 1 323 onetosome scheme onetosome scheme mirroringparity builds independent groups server fails one c groups fails group failure distribution assumed exponential first model group reliability derive server reliability figures 6a 6b show statespace diagrams one group server respectively figure 6b parameter c denotes number groups server c denotes group failure rate generator matrix q c ctmc single group l c c l statespace diagram one group l c c b statespace diagram server figure statespace diagrams onetosome scheme group reliability function r c time r c group mean lifetime mttf c derived r c calculate overall server reliability function assume group failure distribution exponential server failure rate thus c mttfc server generator matrix q ctmc figure 6b therefore gammac 2 33 reliability modeling dependent component failures dependent component failures mean failure single component server affect server components recall server consists set n server nodes node contains set disks disks belong node common components nodes cpu disk controller network interface etc components fails disks contained affected node unable deliver video data therefore considered failed consequently single disk deliver video data anymore fails one components node fails disk belongs present models different schemes case dependent component failures similarly disk failure node failure assumed repairable failure rate n node exponentially distributed mttfn mttf n mean life time node repair rate n failed node exponentially distributed mttrn mttr n mean repair time node mirroring parity schemes apply called orthogonal raid mechanism whenever groups must built orthogonal raid discussed 31 17 based following idea disks belong group must belong different nodes thus disks single group share common node hardware components orthogonal raid property video server survives complete node failure one node fails disks considered failed since disks belong different groups group experience one disk failure knowing one group tolerates single disk failure groups survive therefore server continue operating orthogonal raid applied context parity groups generalize usage orthogonal raid mirroring parity order distinguish disk node failure building models schemes considered present state tuple j gives number disks failed j gives number nodes failed failure absorbing state represented letter f 331 onetoall scheme onetoall schemes mirr allgammaentire mirr allgammasub par double disk failures allowed therefore complete node failure causes server fail figure 7 shows statespace diagram oneto scheme case dependent component failures states model denote respectively initial state 0 0 one disk failure state 1 0 server failure state l l n l l n n d1 n figure 7 statespace diagram onetoall scheme dependent component failures generator matrix q gammad 332 grouped onetoone scheme considering dependent component failures onetoone scheme presented figure 2a would achieve low server reliability since server immediately fails single node fails propose following organization onetoone scheme tolerates complete node failure even nnode failures best case call new organization grouped onetoone scheme grouped onetoone organization keeps initial property onetoone scheme consists completely replicating original content one disk onto another disk grouped onetoone organization divides server independent groups disks belonging group replica inside group groups built based orthogonal raid principle thus disks group belong different nodes figure 8 shows figure 8 assumes one video containing 40 original blocks stored server made four nodes containing two disks inside one group upto dcdisk failures tolerated number disks inside group grouped onetoone scheme therefore survive n failures best case server figure 8 continues operating even nodes n 1 n 2 fail order distribute load failed node among possibly many one surviving nodes grouped onetoone scheme ensures disks belonging node replica disks belong node 1 1 assume node n1 failed load shifted node n3 replica disk 0 stored disk 4 node n4 replica disk 1 stored disk 7 original blocks copies 6 79251329 figure 8 grouped onetoone scheme server 4 nodes 2 disks order model reliability grouped onetoone scheme first study behavior single group derive overall server reliability one group fails two consecutive disks inside group fail remind two disks consecutive original data one disks replicated disk group 1 disks 0 4 consecutive whereas disks 0 2 note failure one disk inside group due failure disk ii failure whole node disk belongs first disk failure group continues operating second disk failure occurs inside group group may fail depending whether two failed disks consecutive let p 2 denotes probability two failed disks group consecutive generally p k denotes probability group fail k th disk failure inside group p k calculated appendix statespace diagram one group example figure 8 presented figure 9 number disks inside group disks failed inside group disks failed j nodes failed obviously disks failed belong different nodes j nodes failed describe following built statespace diagram figure 9 time 0 group state 0 0 first disk failure within group due single disk failure state 1 0 due whole node failure state 0 1 assume group state 1 0 one disk group fails four transitions possible group goes state 2 0 second disk group failed two failed disks consecutive ii group goes state node failed second failed disk group contained two failed disks consecutive iii group goes state f second disk group failed disk failure node failure two failed disks consecutive finally iv group goes state 0 1 node failed first failed disk group belongs thus number failed disks group increase one disk failed remaining statespace diagram derive analogous way parameters figure 9 following generator matrix q c group l n l n l n f f f l 5 l 7 l 7 l 7 l 7 l 3 l 1 l 2 l 6 l 4 figure 9 statespace diagram one group grouped onetoone scheme dependent component failures matrix q c get group mean life time mttf c used calculate overall server reliability statespace diagram server one figure 6b parameter takes value denotes failure rate one group mttfc server reliability calculated analogously eq 2 note example described figure 9 considers small group size c 4 increasing c increases number states contained statespace diagram group general number states given present appendix b general method building statespace diagram one group containing c disks 333 onetosome scheme use orthogonal raid onetosome schemes mirr somegammaentire mirr somegammasub par consider data layouts figures 2d 2e 3b orthogonal raid ensured following holds node 1 contains disks 0 3 node 2 contains disks 1 4 node 3 contains disks 2 5 reliability modeling onetosome scheme first build statespace diagram single group figure compute overall server reliability figure 10b states figure 10a denote following initial state 0 0 state one disk fails 1 0 state one node fails resulting single disk failure within group 0 1 state one disk one node failed failed disk belongs failed node 0 1 one group failure state parameter values used figure l 4 l 4 l 4 l 1 l 2 l 3 statespace diagram one group l c c b statespace diagram server figure 10 statespace diagrams onetosome scheme case dependent component failures generator matrix q c group 34 reliability results resolve continuous time markov chains using sharpe symbolic hierarchical automated reliability performance evaluator 33 tool specifying evaluating dependability performance models sharpe takes input generator matrix computes server reliability certain time results server reliability shown figures 11 12 total number server disks considered number server nodes 10 node containing 10 disks examine server reliability two failure rates 1 hours 1 100000 hours pessimistic values figure 11 plots server reliability onetoone onetoall onetosome schemes case independent disk failures expected server reliability onetoone scheme highest onetosome scheme exhibits higher server reliability onetoall scheme figures 11a 11b also show much server reliability improved mean time disk failure increases decreases example onetoone scheme 10 4 days operation server reliability 03 hours 066 100000 hours figure 12 depicts server reliability grouped onetoone onetoall onetosome schemes case dependent component failures observe grouped onetoone scheme provides better reliability onetosome scheme onetoall scheme lowest server reliability 0408time days reliability function server reliability independent disk failures one2one one2some one2all server reliability hours 72 hours time days reliability function server reliability independent disk failures one2one one2some one2all b server reliability 100000 hours 72 hours figure 11 server reliability three schemes assuming independent disk failures 10 eg 100000 hours three years server reliability 0 onetoall scheme 051 onetosome scheme 085 grouped onetoone scheme figures 12a 12b show server reliability increases n decreases time days reliability function server reliability dependent component failures grouped one2one one2some one2all server reliability hours 72 hours time days reliability function server reliability dependent component failures grouped one2one one2some one2all b server reliability 100000 hours 72 hours figure 12 server reliability three schemes assuming dependent component failures comparing figures 11 12 see expected server reliability higher independent disk failure case dependent component failure case restrict discussion case dependent component failures since realistic case independent disk failures 4 server performance important performance metric designer operator video server maximum number streams q server simultaneously admit referred server throughput adding faulttolerance within server requires additional resources terms storage volume main memory io bandwidth capacity see reliability schemes discussed differ throughput achieve also amount additional resources need guarantee uninterrupted service disk failure mode throughput therefore enough compare server performance schemes instead use cost per stream calculate section 41 server throughput schemes section 42 focuses buffer requirements section 43 compares different schemes respect cost per stream 41 server throughput admission control policy decides based remaining available resources whether new incoming stream admitted cgs striping algorithm serves list streams single disk one service round next service round list streams shifted next disk q denotes maximum number streams single disk serve simultaneously disk throughput non faulttolerant server overall server throughput q simply q accordingly restrict discussion disk throughput disk throughput q given eq 3 5 meaning value different parameters listed table 2 disk parameter values seagate hp scsi ii disk drives 36 r r parameter meaning parameter value r p video playback rate 15 mbps r inner track transfer rate 40 mbps stl settle time 15 ms ms rot worst case rotational latency 933 ms b block size 1 mbit service round duration b dr rp sec table 2 performance parameters allow faulttolerance disk reserves portion available io bandwidth used disk failure mode since amount reserved disk io bandwidth schemes disk throughput also different let us start grouped onetoone scheme mirr one since original content single disk entirely replicated onto another disk half disks io bandwidth must kept unused normal operation mode available disk failure mode consequently disk throughput q mirr one simply half q onetoall mirroring scheme mirr allgammaentire entire block replication original blocks one disk spread among server disks however may happen original blocks would required failed disk particular service round replicated disk worst case situation order guarantee deterministic service worst case half disk io bandwidth must reserved disk failure mode therefore corresponding disk throughput q mirr allgammaentire q mirr worst case retrieval pattern onetosome mirroring scheme mirr somegammaentire entire block replication previous scheme get q mirr since three schemes mirr one mirr allgammaentire mirr somegammaentire achieve throughput use term mirrentire denote q mirr entire 2 denote disk throughput onetoall mirroring scheme mirr allgammasub subblock replication situation changes fact disk failure mode disk retrieves q mirr allgammasub original blocks q mirr allgammasub replicated subblocks one service round let us assume subblocks size b sub ie sub admission control formula becomes allgammasub delta r b sub r bb sub r similarly disk throughput q mirr somegammasub onetosome mirroring subblock replication mirr somegammasub bb sub r b sub denotes size subblock c number disks contained group consider disk throughput parity schemes recall study buffering strategy second read strategy lost block reconstruction onetoall parity scheme par one parity block needed every gamma 1 original blocks additional load disk consisting retrieving parity blocks needed seen figure 3a fact one stream worst case requirements parity blocks concern disk means one parity block retrieved disk every service rounds consequently disk must reserve 1 io bandwidth disk failure mode disk calculated analogous onetoall parity scheme onetosome parity scheme par following disk three schemes share common property original block entirely replicated one block figure 13a take throughput value q mirr entire mirrentire base line comparison plot ratios server throughput function total number disks server 8000515number disks server throughput server throughput ratio par par mirr allsub mirr somesub mirr entire throughput ratios number disks server number disks throughput mirr entire mirr somesub mirr allsub par par b number disks required server throughput figure 13 throughput results reliability schemes mirroring schemes use entire block replication mirrentire provide lowest throughput two mirroring schemes mirr allgammasub mirr somegammasub use subblock replication throughput ratios 15 performance mirr allgammasub slightly higher one mirr somegammasub since subblock size b sub parity schemes achieve higher throughput ratios mirroring schemes onetoall parity scheme par results highest throughput throughput onetosome parity scheme par slightly smaller throughput par fact parity group size gamma 1 par larger c consequence amount disk io bandwidth must reserved disk failure smaller par par order get quantitative view regarding io bandwidth requirements reverse axes figure 13a obtain figure 13b scheme number disks needed achieve given server throughput 42 buffer requirement another resource affects cost video server therefore cost per stream main memory due speed mismatch data retrieval disk transfer rate data consumption playback rate main memory needed server temporarily store blocks retrieved scan retrieval algorithm worst case buffer requirement one served stream twice block size b assuming normal operation mode component failures buffer requirement b server therefore denotes server throughput mirroringbased schemes replicate original blocks belong single disk one set disks disk failure mode blocks would retrieved failed disk retrieved disks store replica thus mirroring requires amount buffer normal operation mode component failure mode independently distribution granularity replicated data therefore mirroring schemes considered grouped onetoone mirr one onetoall entire block replication mirr allgammaentire onetoall subblock replication mirr allgammasub onetosome entire block replication mirr somegammaentire onetosome subblock replication mirr somegammasub buffer requirement component failure b unlike mirroringbased schemes paritybased schemes need perform xor operation set blocks reconstruct lost block fact normal operation mode buffer immediately liberated consumption disk fails original blocks well parity block belong parity group sequentially retrieved consecutive service rounds consecutive disks must temporarily stored buffer many service rounds elapse lost original block reconstructed since buffer overflow must avoided buffer requirement calculated worst case situation whole parity group must contained buffer lost block gets reconstructed additional buffer size one block must also reserved store first block next parity group consequently component failure buffer requirement b par par b par b buffer requirement b par par b par note buffer requirement par depends therefore increases linearly number disks server par however group size c kept constant total number disks varies result buffer requirement par remains unchanged increases 43 cost comparison performance metric use per stream cost first compute total server cost server derive cost per stream stream qs define server cost cost hard disks main memory dimensioned component failure mode pmem price 1 mbyte main memory b buffer requirement mbyte p price 1 mbyte hard disk v disk storage volume single disk mbyte finally total number disks server current price figures 1998 pmem 13 p prices change frequently consider relative costs introducing cost ratio ff pmem thus server cost function becomes pmem ff per stream cost pmem delta ff evaluate cost five different schemes compute scheme given value achieved amount buffer b required support throughput note take schemes mirr somegammaentire mirr somegammasub par figure 14 plots per stream cost schemes par par mirrentire mirr somegammasub mirr allgammasub different values cost ratio ff recall notation mirrentire includes three mirroring schemes mirr allgammaentire mirr somegammaentire grouped onetoone mirr one experience throughput require amount resources figure 14a consider 26 presents current memoryhard disk cost ratio increasing value means price disk storage drops faster price main memory figure 14b multiply current cost ratio five get hand decreasing value ff means price main memory drops faster price hard disk figure 14c divide current cost ratio five get 3 illustrate faster decrease price hard disk compared one main memory consider current price main memory pmem 13 calculate new reduced price hard disk 4 analogously illustrate faster decrease price memory compared price hard disk take current price hard disk calculate new reduced price memory pmem number disks server cost per stream cost par mirr entire par mirr somesub mirr allsub 05 number disks server cost per stream cost par par mirr entire mirr somesub mirr allsub 01 number disks server cost per stream cost par mirr entire mirr somesub mirr allsub par c 05 52 figure 14 per stream cost different values cost ratio ff results figure 14 indicate following ffl increase decrease value ff defined means decrease either price hard disk price main memory respectively hence overall decrease per stream cost figures 14b 14c compared figure 14a figure shows onetoall parity scheme par results highest per stream cost increases grows fact buffer requirement par highest also increases linearly number disks thus resulting highest per stream cost mirroring schemes entire block replication mirrentire second worst per stream cost per stream cost remaining three schemes mirr allgammasub mirr somegammasub par roughly equal low est best scheme onetoall mirroring scheme subblock replication mirr allgammasub slightly smaller per stream cost onetosome mirroring scheme subblock replication due difference size b sub b sub see explanation section 41 ffl increase cost ratio ff factor five figure 14b slightly decreases per stream cost par results dramatic decrease per stream cost three mirroring schemes also parity scheme par instance per stream cost par decreases downto 2872 per stream cost mirr somegammasub decreases 7979 downto 1855 three mirroring schemes become cost efficient two parity schemes ffl decrease cost ratio ff factor five figure 14c affects cost three mirroring schemes little example per stream cost mirr somegammasub 7979 figure 14a 7719 figure 14c hand decreasing ff ie price main memory decreases faster price hard disk clearly affects cost two parity schemes fact par becomes cost efficient scheme cost per stream 6564 although per stream cost par decreases significantly still remains expensive high values since par highest per stream cost linearly increases consider scheme cost discussion 5 server reliability performance 51 server reliability vs per stream cost figure 15 table 3 depict server reliability per stream cost different reliability schemes discussed herein server reliability computed 1 year figure 15a 3 years figure 15b server operation results figure 15 obtained follows given server throughput calculate reliability scheme number disks required achieve throughput compute server reliability scheme respective number disks required table 3 shows normalized per stream cost different values ff take per stream cost mirr one base line comparison divide cost values schemes cost mirr one recall three schemes mirr one mirr allgammaentire mirr somegammaentire per stream cost since achieve throughput given amount resources see section 4 server throughput reliability function mirr one par mirr somesub mirr someentire par mirr allsub mirr allentire server reliability 1 year server operation 100000 hours server throughput reliability function b server reliability 3 years server operation 100000 hours figure 15 server reliability server throughput mirr one mirr allgammaentire par 0688 1129 0588 mirr somegammasub 0698 0729 0691 mirr allgammasub 0661 0696 0653 table 3 normalized stream cost mirr one different values ff three onetoall schemes par mirr allgammasub mirr allgammaentire poor server reliability even low values server throughput since survive single disk failure difference reliability schemes due fact par requires throughput fewer disks mirr allgammasub turn needs fewer disks mirr allgammaentire see figure 13b server reliability three schemes decreases dramatically three years server operation illustrated figure 15b accord ingly schemes attractive ensure fault tolerance video servers hence going discuss remainder paper discuss three onetosome schemes par mirr somegammasub mirr somegammaentire grouped onetoone scheme mirr one based figures 15a 15b mirr one higher server reliability three onetosome schemes par mirr somegammasub mirr somegammaentire table 3 see mirr one per stream cost mirr somegammaentire per stream cost 15 higher mirr somegammasub par highest per stream cost high value ff cost effective small value ff summary see best scheme among onetosome schemes par since low per stream cost requires fewer disks mirr somegammasub thus provides higher server reliability mirr somegammasub mirr somegammaentire since mirr somegammaentire achieves much lower server reliability mirr one per stream cost conclude mirr somegammaentire good scheme achieving fault tolerance video server based results figure 15 table 3 conclude three schemes mirr one par mirr somegammasub good candidates ensure fault tolerance video server note assumed figure 15 three schemes value mirr one highest server reliability higher per stream cost compared per stream cost par mirr somegammasub value two schemes par mirr somegammasub lower per stream cost also lower server reliability mirr one difference server reliability becomes pronounced number disks video server increases see next section determine parameter c schemes par mirr somegammasub order improve tradeoff server reliability cost per stream 52 determining group size c section evaluates impact group size c server reliability per stream cost limit discussion three schemes mirr one par mirr somegammasub remember use orthogonal raid principle build independent groups see section 33 accordingly disks belong group attached different nodes assumed group size c number nodes n constant terms increasing leads increase number disks n per node however maximum number disks n limited nodes io capacity assume video server disks 5 plot figure 16 two different ways configure video server figure 16a server contains five nodes 5 node consists disks one group contains disks belonging different node hand figure 16b configures video server nodes containing disks group size single group stretch across nodes note number groups c configurations 20 video server grows second alternative suggests add new nodes containing new disks server whereas first alternative suggests add new disks existing nodes since n must kept certain limit given nodes io capacity believe second alternative appropriate configure video server consider two values group size remaining three schemes mirr one par mirr somegammasub figures 17a 17b depict server reliability mirr one par mirr somegammasub one year three years server operation respectively table 4 shows schemes normalized per stream cost different values ff per stream cost mirr one base line comparison divide cost values schemes group 20 group 20 group 19 figure configuring video server 5 cost mirr one server throughput reliability function mirr one par somemirr somesub 5 par somemirr somesub 20 server reliability 1 year server operation 100000 hours server throughput reliability function b server reliability 3 years server operation hours figure 17 server reliability results figure 17 table 4 summarized follows ffl server reliability mirr one higher two schemes expected server mirr one par 20 0798 1739 0584 par 5 0695 0879 0653 mirr somegammasub 20 0678 0717 0671 mirr somegammasub 5 0745 0771 0739 table 4 normalized stream cost mirr one different values ff c liability increases par mirr somegammasub decreasing c fact c decreases number groups grows thus number disk failures one disk failure per group tolerated increases well ffl depending value ff impact varying group size c per stream cost differs par mirr somegammasub mirr somegammasub cost per stream decreases group size c grows three values ff considered indeed subblock size read disk failure inversely proportional value c consequently server throughput becomes smaller decreasing c per stream cost increases par 26 ff 130 per stream cost decreases c decreases however result reversed per stream cost par higher 20 following explains last observation 1 small value ff eg signifies price main memory decreases faster one hard disk therefore main memory significantly affect per stream cost par independently group size c 2 group size c decreases amount io bandwidth must reserved disk disk failure mode increases consequently throughput smaller result per stream cost par increases group size c decreases 3 since memory cost affects little per stream cost par small value ff weight amount io bandwidth reserved per stream cost becomes visible therefore per stream cost par increases c decreases note per stream cost par lowest highest par always higher server reliability mirr somegammasub values 26 par higher cost per stream mirr somegammasub given value c however value becomes cost effective mirr somegammasub ffl based reliability results high values ff eg observe small group size c 5 considerably increases server reliability decreases per stream cost par mirr somegammasub server reliability increases c decreases also per stream cost slightly increases whatever value ff summary shown three schemes mirr one par mirr somegammasub good candidates ensure faulttolerance video server grouped onetoone scheme mirr one achieves higher reliability two schemes expense per stream cost 15 times high par value c must small achieve high server reliability low per stream cost mirr somegammasub value c must small achieve high server reliability detriment slight increase per stream cost 6 conclusions first part presented overview several reliability schemes distributed video servers schemes differ type redundancy used mirroring parity distribution granularity redundant data identified seven reliability schemes compared terms server reliability cost per stream modeled server reliability using continuous time markov chains evaluated using sharpe software package considered cases independent disk failures dependent component failures performance study different reliability schemes led us introduce novel reliability scheme called grouped onetoone mirroring scheme derived classical onetoone mirroring scheme grouped onetoone mirroring scheme mirr one outperforms schemes terms server reliability seven reliability schemes discussed mirr one par mirr somegammasub schemes achieve high server reliability low per stream cost compared three schemes terms server reliability per stream cost several memory hard disk prices various group sizes found smaller group size better tradeoff high server reliability low per stream cost calculation p k onetoone scheme calculate following probability p k video server uses onetoone mirroring scheme fail k disknode failures let us consider suite n units note term unit denote disk used independent disk failure case node used dependent component failure mode since want calculate probability server fail k units want units adjacent therefore looking subsuites let us call set suites introduce bijection set suites set introducing second suite j allows suppress condition 10 suite l since suite j strictly growing whose number elements thus easy count number strictly growing functions 1k 1 result though doesnt take account fact units number 1 n adjacent fact two scenarii possible ffl first scenario unit 1 already failed case units 2 n allowed fail otherwise server fail set n gamma 3 units among allowed pick nonadjacent units referring case solved obtain cn ffl second scenario first unit still works case k units chosen among remaining ones leads us value number possibilities n k looking given consequently given number k failed units disksnodes probability p k server fail k unit failures calculated building group statespace diagram grouped onetoone scheme show following build statespace diagram one group containing c disks grouped onetoone scheme focus transitions state j higher states back know state represents total number j disks failed inside group failures due disk failures j node failures also know number states statespace diagram 2 dc parameters j must respect dc since dcdisk failures tolerated inside one group distinguish two cases figure shows possible transitions corresponding rates case whereas figure 19 shows transition case ii l n f l 1 l 2 figure 18 transitions state j higher states back l figure 19 transition failure state parameters two figures following values r case redundant arrays inexpensive disks raid streaming raidtm disk array management system video files fault tolerant design multimedia servers striping multidisk video servers data striping reliablity aspects distributed video servers disk striping video server environments faulttolerant architectures continuous media servers design performance tradeoffs clustered video servers predictive call admission control disk array based video server architectures algorithms online failure recovery redundant disk arrays survey approaches fault tolerant design vod servers techniques analysis comparison disk shadowing doublystriped disk mirroring reliable storage video servers tiger video fileserver analytic modeling comparisons striping strategies replicated disk arrays performance modeling analysis disk arrays raid highperformance reliable secondary storage raid highperformance reliable secondary storage architectures algorithms online failure recovery redundant disk arrays performance cost comparison mirroring paritybased reliability schemes video servers chained declustering new availability strategy multiprocessor database chines chained declustering load balancing robustness skew failures issues design storage server videoondemand raidii scalabale storage architecture highbandwidth network file service striping multidisk video servers segmented information dispersal sid efficient reconstruction faulttolerant video servers high availability clustered multimedia servers random raids selective exploitationof redundancy high performance video servers using rotational mirrored declustering replica placement diskarraybased video server reliability analysis redundant arrays inexpensive disks performability diskarraybased video servers performance reliability analysis computer systems examplebased approach using sharpe software package system reliability theory models statistical methods placement continuous media multizone disks tr ctr yifeng zhu hong jiang xiao qin dan feng david r swanson design implementation performance evaluation costeffective faulttolerant parallel virtual file system proceedings international workshop storage network architecture parallel ios p5364 september 2828 2003 new orleans louisiana xiaobo zhou chengzhong xu efficient algorithms video replication placement cluster streaming servers journal network computer applications v30 n2 p515540 april 2007 seon ho kim hong zhu roger zimmermann zonedraid acm transactions storage tos v3 n1 p1es march 2007 stergios v anastasiadis kenneth c sevcik michael stumm maximizing throughput replicated disk striping variable bitrate streams proceedings general track 2002 usenix annual technical conference p191204 june 1015 2002 xiaobo zhou chengzhong xu harmonic proportional bandwidth allocation scheduling service differentiation streaming servers ieee transactions parallel distributed systems v15 n9 p835848 september 2004 eitan bachmat tao kai lam effect configuration choice performance mirrored storage system journal parallel distributed computing v65 n3 p382395 march 2005 yifeng zhu hong jiang ceft costeffective faulttolerant parallel virtual file system journal parallel distributed computing v66 n2 p291306 february 2006 stergios v anastasiadis kenneth c sevcik michael stumm scalable faulttolerant support variable bitrate data exedra streaming server acm transactions storage tos v1 n4 p419456 november 2005