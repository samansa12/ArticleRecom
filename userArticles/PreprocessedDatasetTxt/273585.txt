locality reference lu decomposition partial pivoting paper presents new partitioned algorithm lu decomposition partial pivoting new algorithm called recursively partitioned algorithm based recursive partitioning matrix paper analyzes locality reference new algorithm locality reference known widely used partitioned algorithm lu decomposition called rightlooking algorithm analysis reveals new algorithm performs factor thetasqrtmn fewer io operations cache misses rightlooking algorithm n order matrix size primary memory analysis also determines optimal block size rightlooking algorithm experimental comparisons new algorithm rightlooking algorithm show implementation new algorithm outperforms similarly coded rightlooking algorithm six different risc architectures new algorithm performs fewer cache misses algorithm tested benefits strassens matrixmultiplication algorithm b introduction algorithms partition dense matrices blocks operate entire blocks much possible key obtaining high performance computers hierarchical memory systems partitioning matrix blocks creates temporal locality reference algorithm reduces number words must transferred primary secondary memories paper describes new partitioned algorithm lufactorization partial pivoting called recursivelypartitioned algorithm paper also analyzes number data transfers popular partitioned lufactorization algorithm socalled rightlooking algorithm used lapack 1 performance characteristics popular partitioned lufactorization algorithms particular crout leftlooking algorithm used nag library 4 similar rightlooking algorithm analyzed analysis two algorithms leads two interesting conclusions first simple systemindependent formula choosing block size right looking algorithm almost always optimal second recursivelypartitioned algorithm generates asymptotically less memory traffic memories rightlooking algorithm even block size right looking algorithm chosen optimally numerical experiments indicate recursivelypartitioned algorithm generates fewer cache misses runs faster rightlooking algorithm recursivelypartitioned algorithm computes lu decomposition partial pivoting nbym matrix transferring thetanm words primary secondary memories size primary memory rightlooking algorithm hand transfers least words number words actually transferred conventional algorithms depends parameter r chosen optimally parts research performed author postdoctoral fellow ibm tj watson research center postdoctoral associate mit laboratory computer science work mit supported part arpa grant n000149410985 xerox palo alto research center 3333 coyote hill road palo alto ca 94304 toledo lapack new algorithm optimal sense number words transfers asymptotically number transferred partitioned blocked algorithms matrix multiplication solution triangular systems least number columns small compared size primary memory right looking algorithm achieves performance matrix large rows fill primary memory recursivelypartitioned algorithm algorithm advantages conventional algorithms blocksize parameter must tuned order achieve high performance since recursive likely perform better memory system two levels example computer systems two levels cache cache virtual memory understand main idea behind new algorithm let us look first conventional rightlooking lu factorization algorithm algorithm decomposes input matrix dnre blocks r columns starting leftmost block columns algorithm iteratively factors block r columns using column oriented algorithm block factored algorithm updates entire trailing submatrix parameter r must carefully chosen minimize number words transferred memories r larger mn many words must transferred block columns factored r small many trailing submatrices must updated updates require entire trailing submatrix read secondary memory main insight behind recursivelypartitioned algorithm need update entire trailing submatrix block columns factored factoring first column matrix algorithm updates next column right enables proceed second column factored must apply updates first two columns proceed algorithm updates two columns proceeds four columns factored used update four words algorithm look way right every time columns factored shall see shortsighted approach pays another point view new algorithm recursive algorithm know larger r number columns block smaller number data transfers required updating trailing submatrices algorithm therefore chooses largest possible size m2 many columns fit within primary memory factored recursively using algorithm rather factored using naive column oriented algorithm left m2 columns factored used update right m2 columns subsequently rest paper organized follows section 2 describes analyzes recursivelypartitioned algorithm section 3 analyzes blockcolumn rightlooking algorithm actual performance lapacks rightlooking algorithm performance recursivelypartitioned algorithm compared section 4 several highend workstations section 5 concludes paper discussion results related research 2 recursivelypartitioned lu factorization recursivelypartitioned algorithm efficient conventional partitioned algorithms also simpler describe analyze section first describes algorithm analyzes complexity algorithm terms arithmetic operations terms amount data transferred memories execution locality reference lu decomposition 3 algorithm algorithm factors nbym matrix nbyn permutation matrix p nbym unit lower triangular matrix l ls upper triangle zeros mbym upper triangular matrix u treated block matrix 11 21 21 22 11 square matrix order m2bym2 1 perform pivoting scaling 11 21 u 11 return 2 else recursively factor 11 21 u 11 3 permute 0a 0 12 22 4 solve triangular system l 11 5 00 0gamma l 21 6 recursively factor p 2 00 l 22 u 22 7 permute l 0 p 2 8 return 11 12 21 22 u 11 u 12 complexity analysis hard see algorithm numerically equivalent conventional columnoriented algorithm therefore algorithm name numerical properties conventional algorithm performs number floating point operations nm fact variants lufactorization algorithm discussed paper essentially different schedules algorithm dataflow graph analyze number words must transferred primary secondary memories n size primary memory denoted ease exposition assume number columns power two denote number words algorithm must transfer memories iorp n denote number words must transferred solve nbyn triangular linear system right hand sides solution overwrites right hand side iots n denote number words must transferred multiply multiply nbygammam matrix mbyk matrix add result nbyk matrix iomm n k since factorization algorithm uses matrix multiplication solution triangular linear system subroutines number ios performs depends 4 toledo number ios performed subroutines partitioned algorithm solving triangular linear systems performs iots m3 m3 m3 ios actual number ios performed smaller since real crossover point m2 m3 incorporating improved bound analysis complicates analysis little effect final outcome number ios performed standard matrixmultiplication algorithm iomm n n m3 m3 bound matrix multiplication holds values n analysis assumes use conventional triangular solver matrix multiplication rather socalled fast strassenlike algorithms asymptotic bounds fast matrixmultiplication algorithms better 5 analyze recursivelypartitioned algorithm using induction initially analysis take account permutation rows algorithm performs shall return permutations later section recurrence governs total number words transferred algorithm first prove induction 12 m2 m3 iorp n 2nm1 lg base case true assuming claim true m2 25m 2 3nm prove induction iorp n 2nm mp m3 m2 m3 claim true base case m3 since m2 m3 since m2 1 assuming claim true m2 iorp n 2nm mp m3 locality reference lu decomposition 5 m3 m3 mp m3 m3 nm 2p m3 m3 mp m3 nm 2p m3 m3 mp m3 nm 2p m3 mp m3 bound number word transfers due permutations compute number permutations column undergoes algorithm column permuted either factorization step 2 permutation step 7 permutation step 3 factorization step 6 follows column permuted 1 times word brought secondary memory total number ios required permutations 2n bound achieved reading entire columns primary memory permuting primary memory following theorem summarizes main result section theorem 21 given matrix multiplication subroutine whose io performance satisfies equation 22 subroutine solving triangular linear systems whose io performance satisfies equation 21 recursivelypartitioned lu decomposition algorithm running computer words primary memory computes lu decomposition partial pivoting nbym matrix using iorp n 2nm mp m3 ios 3 analysis rightlooking lu factorization put performance recursivelypartitioned algorithm perspective analyze performance columnblock rightlooking algorithm first describe algorithm analyze number data transfers ios performs 6 toledo bounds obtain asymptotically tight focus lower bounds terms constants number ios required solution triangular linear systems smaller number ios required updates trailing submatrix rank r update matrix ignore triangular solves analysis rightlooking lu algorithm factors nbym matrix algorithm factors r columns every iteration kth iteration decompose pa 4 11 12 13 21 22 23 11 square matrix order square matrix order r kth iteration algorithm performs following steps 1 factor 22 2 permute 23 33 23 33 3 permute 4 solve triangular system l 22 u 5 update 33 u number ios required factor nbyr matrix using columnby column algorithm nr 2when nr2 nr simplify analysis ignore range half matrix fits within primary memory less entire matrix using one level recursion leads thetanr ios range use facts m3 m3 rs r m3 r 2ts rs m3 trs m3 2ts r m3 3 locality reference lu decomposition 7 bound 2ts rs underestimate rs ignore small slack analysis number ios algorithm performs depends relation r dimensions matrix size memory r small nr updates trailing submatrix dominate number ios algorithm performs updates trailing submatrix require least ios particular first m2r updates require least r larger factoring mr blocks r columns requires least r nr 2 nmrios number ios required rankr updates depends value r mn r m3 total number ios performed rankr updates least therefore number ios performed algorithm least nmr minimized optimal value r lies exact value might deviate slightly range since expression derived number ios lower bound substituting optimal value r find algorithm performs least nm 15 gamma4 nm 15 ios range value performance m3 value m3 yields better performance r yet larger r m3 rankr updates require ios particular first m2r updates require least m3 nm 2p m3 m3 nm 2p m3 8 toledo ios total number ios range including updates factoring blocks columns therefore least nm 2p m3 m3mn number ios minimized choosing smallest possible m3 matrix large compared size main memory n 2 3 also possible choose r mn case total number ios least nm 2p m3 analysis summarized follows value r close maxmn optimal almost cases exception truly huge matrices matrices m3 better combining results obtain following theorem theorem 31 given matrix multiplication subroutine whose io performance satisfies equation 31 subroutine solving triangular linear systems whose io performance satisfies equation 32 rightlooking lu decomposition algorithm running computer words primary memory computes lu decomposition partial pivoting nbym matrix using least iorl n m3 ios first case r mn leads better performance columns fit within primary memory although lower bounds asymptotically tight value 14 lower bound actual constant higher 4 experimental results implemented tested recursively partitioned algorithm 1 goal experiments determine whether recursively partitioned algorithm efficient rightlooking algorithm practice results experiments clearly show recursivelypartitioned algorithm performs less io faster least computer experiments conducted results experiments complement analysis two algorithms analysis shows recursivelypartitioned algorithm performs less io right looking algorithm values n analysis stops short demonstrating one algorithm faster another three respects first bounds analysis exact second analysis counts total number fortran 90 implementation available online anonymous ftp theorylcsmitedu pubpeoplesivandgetrf90f code compiled many fortran 77 compilers luding compilers ibm silicon graphics digital removing recursive keyword using compiler option enables recursion see 11 details locality reference lu decomposition 9 ios algorithm distribution io within algorithm significant finally analysis uses simplified model twolevel hierarchical memory capture subtleties actual memory systems experiments show even though analysis exact respects recursivelypartitioned algorithm indeed faster three sets experiments presented section first set presents analyzes detail experiments ibm rs6000 workstations goal set experiments establish recursivelypartitioned algorithm faster rightlooking algorithm second set experiments show less detail recursivelypartitioned algorithm outperforms lapacks rightlooking algorithm wide range architectures goal second set experiments establish robustness performance recursivelypartitioned algorithm third set experiments shows using strassens matrix multiplication algorithm speeds recursivelypartitioned algorithm seem speed right looking algorithm technical details experiments operating system ver sions compiler versions compiler options omitted paper details fully described technical report 11 detailed experimental analyzes first set experiments performed ibm rs6000 workstation 665 mhz power2 processor 14 128 kbytes 4way set associative level1 datacache 1 mbytes direct mapped level2 cache 128bitwide main memory bus power2 processor capable issuing two doubleprecision multiplyadd instructions per clock cycle lapacks right looking lufactorization subroutine dgetrf recursively partitioned algorithm compiled ibms xlf compiler version 32 algorithms used blas ibms engineering scientific subroutine library essl square matrices also measured performance lufactorization subroutine dgef essl interface subroutine allows factorization square matrices coding style data structures used recursivelypartitioned algorithm ones used lapack par ticular permutations represented algorithms sequence exchanges cases array contains matrix factored allocated statically aligned 16byte boundary leading dimension matrix equal number rows padding performance algorithms assessed using measurements running time cache misses time measured using machines realtime clock resolution one cycle number cache misses measured using power2 performance monitor 13 performance monitor hardware subsystem processor capable counting cache misses processor events realtime clock performance monitor oblivious time sharing minimize risk measurements influenced processes ran experiments users used machine connected network later verified measurements valid comparing real timeclock measurements user time reported aixs getrusage system call experiment experiment basis measurements reported based average 10 executions coded two variants recursivelypartitioned algorithm two versions differ way permutations applied submatrices one version permutations applied using lapacks auxiliary subroutine dlaswp sub table performance millions operations per second mflops number cache misses per thousand floating point operations cmkflop five lufactorization algorithms ibm rs6000 workstation square matrices figures lapacks dgetrf block size r best running time upright letters block size smallest number cache misses italics minimum number cache misses generally coincide minimum running time see text full description experiments subroutine mflops cmkflop mflops cmkflop lapacks dgetrf row exchanges 178 176 581 565 170 168 545 529 recursivelypartitioned row exchanges 201 376 186 414 lapacks dgetrf permuting columns 201 199 294 281 198 195 311 302 recursivelypartitioned permuting columns 222 161 223 159 essls dgef 228 215 221 342 routine also used lapacks rightlooking algorithm permutes rows submatrix exchanging rows using vector exchange subroutine dswap level1 blas second version permutes rows matrix applying entire sequence exchanges one column another difference amounts swapping inner outer loops change suggested fred gustavson first experiment whose results summarized table 41 designed determine effects complex hierarchical memory system partitioned algorithms four facts emerge table 1 recursively partitioned algorithm performs less cache misses delivers higher performance rightlooking algorithm essls subroutine performs less cache misses lapack recursively partitioned algorithm achieves best close best performance 2 permuting one column time leads less cache misses faster execution exchanging rows true rightlooking algorithm recursivelypartitioned algorithm probably result advantage stride1 access column column permuting large stride access rows row exchanges 3 performance term time cache misses algorithms except recursivelypartitioned column permuting worse leading dimension matrix power 2 performance recursivelypartitioned algorithm column permuting improves less half percent degradation performance power 2 probably caused fact caches fully associative 4 running time depends measured number cache misses completely seen fact essls dgef performs cache misses recursively partitioned algorithm faster fact block size leads minimum number cache misses dgetrf lead best running time discrepancy caused several factors mea sured including misses conflicts level2 cache tlb misses instruction scheduling four cases table minimum running time achieved value r higher number leads minimum number cache misses example row exchanges performed least number cache misses fastest running time achieved may mean locality reference lu decomposition 11 500 1000 1500 2000160200240 mflops order matrix rl optimal r r64 500 1000 1500 2000246cache reloads per kflop order matrix rl optimal r rl r64 fig 41 performance mflops left number cache misses per kflop right lu factorization algorithms ibm rs6000 workstation graphs depict performance recursivelypartitioned pr rightlooking rl algorithms square matrices optimal value r selected experimentally powers 2 2 256 dashed lines represent performance recursivelypartitioned algorithms column permuting cp cause discrepancy misses level2 cache larger level1 cache therefore may favor larger block size since columns fit summary experiment shows although implementation details memory system influence performance algorithms recursivelyparti tioned algorithm still emerges faster rightlooking one implemented similar way second set experiments designed assess performance algorithms wide range input sizes performance number cache misses algorithms presented figure 41 square matrices ranging order 200 2000 level1 cache large enough store matrix order 128 following points emerge experiment 1 beginning matrices order 300 recursivelypartitioned algorithm column permuting faster algorithm row exchanges still faster lapacks dgetrf row exchanges measure performance dgetrf column permuting experiment 2 performance dgetrf optimal block size r essentially except although optimal block size clearly leads smaller number cache misses 3 recursivelypartitioned algorithm performs less cache misses essls dgef input sizes faster first experiment experiment indicate causes phenomenon speculate caused better instruction scheduling fewer misses level2 cache next experiment designed determine sensitivity performance rightlooking algorithm block size r used column permuting strategy proved efficient previous experiments experiment consists running algorithm range block sizes square matrix order 1007 rectangular 62500by64 matrix factorization rectangular mflops block size r cache reloads per kflop block size r fig 42 performance mflops left number cache misses per kflop right rightlooking algorithm column permuting function block size r order square matrix used 1007 note yaxes start zero block size r reloads per kflop block size r fig 43 performance mflops left number cache misses per kflop right rightlooking algorithm column permuting function block size r dimensions matrix 62500 64 comparison performance recursively partitioned algorithm problem 118 mflops 1103 cmkflop matrix arises subproblem outofcore lu factorization algorithms factor blocks columns fit within core specific dimensions matrices chosen minimize effects conflicts memory system results results shown figures 42 show minimum number cache misses occurs higher best performance achieved even higher value r 55 performance sensitive choice r however values 50 70 yield essentially performance 201 mflops results matrices shown figures 43 show minimum number cache misses occur best performance occurs happens coincide exactly sensitivity r greater square case especially optimal value last experiment set presented figure 44 designed determine whether discrepancy optimal block size terms level1 cache misses locality reference lu decomposition 13 mflops block size r cache reloads per kflop block size r fig 44 performance mflops left number cache misses per kflop right rightlooking algorithm column permuting function block size r order square matrix used 1007 machine used bigger level1 cache level2 cache machine used experiments compare figure 42 comparison performance recursivelypartitioned algorithm problem machine 229 mflops 0650 cmkflop optimal block size terms running time caused level2 cache experiment repeats last experiment square matrices order 1007 except experiment conducted machine 256bitwide main memory bus 256 kbytes level1 cache level2 cache two machines identical respects discrepancy optimal block sizes figure 44 smaller discrepancy figure 42 experiment shows discrepancy caused solely level2 cache possible determine whether smaller discrepancy experiment due lack level2 cache larger level1 cache robustness experiments second set experiments show performance advantage recursively partitioned algorithm demonstrated first set experiments limited single computer architecture experiments accomplish goal showing recursively partitioned algorithm outperforms right looking algorithm wide range architectures experiments set compare performance recursively partitioned algorithm performance lapacks rightlooking two sizes square matrices larger matrices fit within main memory sizes chosen minimize impact cache associativity results measurement reported represents average best 5 10 runs minimize effect processes system block size rightlooking algorithm lapacks default used following machine configurations ffl 665 mhz ibm rs6000 workstation power2 processor 128 kbytes 4way set associative datacache 1 mbytes direct mapped level cache 128bitwide bus used blas ibms essl ffl 25 mhz ibm rs6000 workstation power processor 64 kbytes 4way set associative datacache 128bitwide bus used blas ibms essl ffl 100 mhz silicon graphics indy workstation mips r4600r4610 14 toledo table running time seconds lu factorization algorithms several machines machine matrix order table shows running times recursivelypartitioned rp algorithm rightlooking rl algorithm row exchanges column permutations measurements available marked na amount main memory insufficient factor larger matrix core see text full description experiments row column row column exchanges pivoting exchanges pivoting machine rl rp rl rp rl rp rl rp ibm power 2281 1907 1787 1686 1461 1434 1358 1297 cpufpu pair 16 kbytes direct mapped data cache 64bitwide bus used sgi blas machine 32 mbytes main memory experiment include matrices order ffl 250 mhz silicon graphics onyx workstation 4 mips r4400r4010 cpufpu pairs 16 kbytes direct mapped data cache per processor 4 mbytes level2 cache per processor 2way interleaved main memory system 256bitwide bus experiment used one processor used sgi blas ffl 150 mhz dec 3000 model 500 alpha 21064 processor 8 kbytes direct mapped cache 512 kbytes level2 cache used blas decs dxml ieee floating point limit amount physical memory allocated process prevented us running experiment matrices order ffl 300 mhz digital alphaserver 4 alpha 21164 processors 8 kbytes level1 data cache 96 kbytes onchip level2 cache 4 mbytes level2 cache experiment used one processor used blas decs dxml ieee floating point results reported table 42 show recursively partitioned algorithm consistently outperforms rightlooking algorithm results also show permuting columns almost always faster exchanging rows experiments using strassens algorithm performing updates trailing submatrix using variant strassens algorithm 10 improved performance recursively partitioned algorithm replaced call dgemm level3 bla subroutine matrix multiplyadd call dgemmb public domain implementation 2 variant strassen algorithm 3 replacing calls dgemm calls strassen matrixmultiplication subroutine ibms essl gave similar results dgemmb uses strassens algorithm dimensions input matrices greater machinedependent constant authors dgemmb set constant 192 ibm rs6000 workstations recursivelypartitioned algorithm column permuting replacement available online httpwwwnetliborglinalggemmw locality reference lu decomposition 15 dgemm dgemmb reduced factorization time power2 machine 299 seconds 1007 2218 seconds 2014 factorization times conventional matrix multiplication algorithm reported first line table 42 305 2345 seconds running time reduced 1827 1668 seconds matrix order 4028 change would effect rightlooking algorithm since matrices multiplies least one dimension r smaller 192 experiments similar experiment carried bailey lee simon 2 showed strassens algorithm accelerate lapacks rightlooking lu factorization cray ymp largest improvements performance however occured large values r used fastest factorization matrix order example obtained value likely cause poor performance machines caches cray ymp cache ibm power2 chine caches increasing r 64 512 causes factorization time conventional matrix multiplication algorithm increase 308 seconds 54 sec onds replacing matrix multiplication subroutine dgemmb reduces solution time less 2 seconds 5 conclusions recursivelypartitioned algorithm used instead rightlooking algorithm delivers similar better performance without parameters must tuned parameter choose means possibility poor choice hence new algorithm robust section 4 shows performance rightlooking algorithm sensitive r best performance always coincide block size causes smallest number cache misses choosing r especially difficult machines two levels memory recursive algorithm hand natural choice hierarchical memory systems two levels recursively partitioned algorithm provides good opportunity use fast matrix multiplication algorithm strassens algorithm since significant fraction work performed recursively partitioned algorithm used multiply large matrices benefit using strassens algorithm large rightlooking algorithm performs work several multiplications smaller matrices benefit strassens algorithm smaller analysis rightlooking algorithm section 3 shows block size r chosen value r optimal two exceptions single row large fit within primary memory value m3 leads better performance columns fit within primary memory r set mn minimize memory traffic extreme cases source difficulty choosing good value r hierarchical memory systems two levels experiments performance rightlooking algorithm matrices rows columns sensitive choice r sensitive large square matrices typical cases least one row fits within primary memory rightlooking algorithm optimal choice r performs factor theta data transfers recursively partitioned algorithm experiments factor led significant difference number cache misses running time conclusion value often close optimal shows systemindependent way choose r comparison model implementation ilaenv lapacks blocksizeselection subroutine uses fixed value 64 toledo lapacks users guide advises systemdependent tuning r could improve performance viewpoint lapack designers seems r systemdependent parameter whose role hide low bandwidth secondary memory system updates trailing submatrices analysis shows true role r balance number data transfers two components algorithm factorization blocks columns updates trailing submatrices designers outofcore lu decomposition codes often propose use block column row algorithms many propose choose r mn entire block columns fits within primary memory 4 6 7 15 approach works well columns short large number fits within primary memory performance algorithms would unacceptable columns fit within primary memory researchers 7 8 9 suggest algorithms use less primary memory necessary storing columns might difficulty implementing partial pivoting analysis paper shows possible achieve low number data transfers even single row column fit within primary memory womble et al 15 presented recursivelypartitioned lu decomposition algorithm without pivoting claimed without proof pivoting incorporated algorithm without asymptotically increasing number ios algorithm performs suggested recursive algorithm would difficult implement implemented instead partitioned leftlooking algorithm using toledo gustavson 12 describe recursivelypartitioned algorithm ofcore lu decomposition partial pivoting algorithm uses recursion large submatrices switches leftlooking variant smaller submatrices would still fit within main memory depending size main memory algorithm factor matrix 23 amount time used outofcore leftlooking algorithm fixed block size 6 acknowledgments thanks rob schreiber reading several early versions paper commenting thanks fred gustavson ramesh agarwal helpful suggestions thanks anonymous referees several helpful comments r using strassens algorithm accelerate solution linear systems gemmw portable level 3 blas winograd variant strassens matrixmatrix multiply algorithm note matrix multiplication paging environment solving systems large dense linear equations matrix computations fortran paging gaussian elimination optimal locality reference lu decomposition partial pivoting design implementation solar power2 performance monitor power2 next generation risc system6000 family beyond core making parallel computer io practical tr ctr bradley c kuszmaul cilk provides best overall productivity high performance computing hpc challenge award prove proceedings nineteenth annual acm symposium parallel algorithms architectures june 0911 2007 san diego california usa florin dobrian alex pothen design ioefficient sparse direct solvers proceedings 2001 acmieee conference supercomputing cdrom p3939 november 1016 2001 denver colorado sivan toledo eran rabani large electronic structure calculations using outofcore filterdiagonalization method journal computational physics v180 n1 p256269 july 20 2002 kang su gatlin larry carter architecturecognizant divide conquer algorithms proceedings 1999 acmieee conference supercomputing cdrom p25es november 1419 1999 portland oregon united states zizhong chen jack dongarra piotr luszczek kenneth roche selfadapting software numerical linear algebra lapack clusters parallel computing v29 n1112 p17231743 novemberdecember bjarne stig andersen jerzy waniewski fred g gustavson recursive formulation cholesky factorization matrix packed storage acm transactions mathematical software toms v27 n2 p214244 june 2001 rezaul alam chowdhury vijaya ramachandran cacheoblivious dynamic programming proceedings seventeenth annual acmsiam symposium discrete algorithm p591600 january 2226 2006 miami florida matteo frigo volker strumpen memory behavior cache oblivious stencil computations journal supercomputing v39 n2 p93112 february 2007 vladimir rotkin sivan toledo design implementation new outofcore sparse cholesky factorization method acm transactions mathematical software toms v30 n1 p1946 march 2004 alexander tiskin communicationefficient parallel generic pairwise elimination future generation computer systems v23 n2 p179188 february 2007 lars arge michael bender erik demaine bryan hollandminkley j ian munro cacheoblivious priority queue graph algorithm applications proceedings thiryfourth annual acm symposium theory computing may 1921 2002 montreal quebec canada jack dongarra victor eijkhout piotr uszczek recursive approach sparse matrix lu factorization scientific programming v9 n1 p5160 january 2001 siddhartha chatterjee alvin r lebeck praveen k patnala mithuna thottethodi recursive array layouts fast matrix multiplication ieee transactions parallel distributed systems v13 n11 p11051123 november 2002 michael bender ziyang duan john iacono jing wu localitypreserving cacheoblivious dynamic dictionary proceedings thirteenth annual acmsiam symposium discrete algorithms p2938 january 0608 2002 san francisco california dror irony gil shklarski sivan toledo parallel fully recursive multifrontal sparse cholesky future generation computer systems v20 n3 p425440 april 2004 isak jonsson bo kgstrm recursive blocked algorithms solving triangular systemspart onesided coupled sylvestertype matrix equations acm transactions mathematical software toms v28 n4 p392415 december 2002 richard vuduc james w demmel jeff bilmes statistical models empirical searchbased performance tuning international journal high performance computing applications v18 n1 p6594 february 2004