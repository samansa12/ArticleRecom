robust learning missing data paper introduces new method called robust bayesian estimator rbe learn conditional probability distributions incomplete data sets intuition behind rbe information pattern missing data available incomplete database constrains set possible estimates paper provides characterization constraints experimental comparison two popular methods estimate conditional probability distributions incomplete datagibbs sampling em algorithmshows gain robustness application rbe quantify naive bayesian classifier incomplete data set illustrates practical relevance b introduction probabilistic methods play central role development artificial intelligence ai applications deal degrees uncertainty often embedded realworld problems within sound mathematical framework unfortunately number constraints needed define probabilistic model called joint probability distribution grows exponentially number variables domain thus making infeasible straight use methods however assumption variables stochastically independent given subset remaining variables domain dramatically reduces number constraints needed specify probabilistic model order exploit property researchers developed new formalism capture graphical terms assumptions independence domain thus reducing number probabilities assessed formalism known bayesian belief networks bbns pearl 1988 bbns provide compact representation encoding probabilistic information may easily extended powerful decisiontheoretic formalism called influence diagrams technically bbn direct acyclic graph nodes represent stochastic variables links represent conditional dependencies among variables variable bears set possible values may regarded set mutually exclusive exhaustive states representing assignment particular value variable conditional dependency links child variable set parent variables defined set conditional probabilities state child variable given combination states parent variables dependency original development bbns domain experts supposed main source information independence assumptions coupled subjective assessment conditional dependencies among variables produces sound compact probabilistic representation domain knowledge however bbns strong statistical root past years root prompted development methods able learn bbns directly databases cases rather insight human domain experts cooper herskovitz 1992 buntine 1994 heckerman chickering 1995 choice extremely rewarding domain applications generates large amounts statistical information aspects domain knowledge still unknown controversial complex encoded subjective probabilities 2of domain experts given nature bbns two main tasks involved learning process bbn database induction graphical model conditional independence best fitting database hand extraction conditional probabilities defining dependencies given graphical model database certain constraints second task accomplished exactly efficiently database complete cooper herskovitz 1992 heckerman chickering 1995 call assumption database completeness assumption un fortunately databases rarely complete unreported lost corrupted data distinguished feature realworld databases order move applications methods learn bbns face challenge learning databases unreported data past years several methods proposed learn conditional probabilities bbns incomplete databases either deterministic sequential updating spiegelhalter lauritzen 1990 cowel et al 1996 emalgorithm dempster et al 1977 stochastic gibbs sampling neal 1993 methods share common assumption unreported data missing random unfortunately assumption unrealistic database completeness assumption real world often reason data missing paper introduces new method learn conditional probabilities bbns incomplete databases rely missing random assumption major feature method ability learn bbns robust respect pattern missing data return sets estimatesinstead point estimates whose width monotonic increasing function information available database reminder paper structured follows next section start establishing notation reviewing background motivation research section 3 outline theoretical framework robust approach learn probabilities bbns section 4 describe details algorithms used implement approach section 5 compare behavior system implemented using method implementation gibbs sampling finally draw conclusions figure 1 graphical structure bbn background bbn defined set variables network structure defining graph conditional dependencies among elements x limit attention variables take finite number values therefore variable x define set mutually exclusive exhaustive states x representing assignments x possible values figure 1 shows bbn use illustrate notation case assume variable may take two values true 1 false 0 network structure defines set conditional dependencies among variables x case reported figure 1 two depen dencies one linking variable x 3 parents x 1 x 2 marginally independent one linking variable x 4 parent x 3 since direct links separates x 4 nodes x 4 conditionally independent conditional dependencies defined set twelve conditional probabilities eight px four px 2 conditional probabilities marginal probabilities parent nodes x 1 x 2 px used write joint probability network configuration instance fx figure 2 graphical structure bbn associated parameters using generic structure joint probability particular set values variables x say decomposed parent nodes x x pax denotes combination states pax x x root node px following denote events x given database cases g case c set entries assume cases mutually independent given network structure task learn conditional probabilities defining dependencies bbn consider conditional probabilities defining bbn parameters joint probability case database say table 1 parameters defining conditional probabilities bbn displayed figure 1 parameterizes probability x ij given parent configuration pax instance network figure 1 need eight parameters describe dependencies two needed define probability distributions x 1 x 2 say six define probability distributions given figure 2 task assess set parameters induced database network structure classical statistical parameter estimation provides basis learn parameters database complete common approach maximum likelihood returns parameter values make database likely given values x database joint probability l function parameters usually called likelihood l maximum likelihood estimate value maximizes l discrete variables maximum likelihood estimates conditional probabilities observed relative frequencies relevant cases database let nx ij jpax observed frequency cases database x ij given parent configuration pax let npax observed frequency cases pax maximum likelihood estimate conditional probability x ij simply consider instance database given table 2 maximum case table 2 artificial database bbn displayed figure 1 likelihood estimate 3 13 bayesian approach extend classical parameter estimation technique two ways set parameters regarded random variables ii likelihood augmented prior say representing observers belief parameters observing data given information database prior density updated posterior density using bayes theorem hence z pdjd bayesian estimate expectation posterior distribution common assumptions bayesian approach learn bbn discrete variables parameters independent ii dirichlet distribution simplifies beta distribution binary variables assumption allows factorise joint prior density thus allowing local computations ii facilitates computations posterior density taking advantages conjugate analysis full details given instance spiegelhalter lauritzen 1990 outline standard conjugate analysis dirichlet priorconsider instance variable x let parameters associated conditional probabilities 1 dirichlet prior denoted dff continuous multivariate distribution density function proportional hyperparameters ff ij following interpretation ff regarded imaginary sample size needed formulate prior information mean ij ff ij ff note prior mean also probability x ij given parent configuration hence uniform prior would assign uniform probabilities x ij given parent configuration pax complete data posterior distribution parameters computed exactly using standard conjugate analysis yielding posterior density posterior means represent estimates thus bayes estimate conditional probability x ij g consider example table 2 prior distribution 3 priori conditional probability x 05 posterior distribution would updated d2 3 yielding bayesian estimate 25 unfortunately situation quite different entries database missing datum missing face set possible complete databases one possible value variable datum missing exact analysis would require compute joint posterior distribution parameters given possible completion database mix possible com pletions apparently infeasible deterministic method proposed spiegelhalter lauritzen 1990 improved cowel et al 1996 provides way approximate exact posterior distribution processing data sequentially however spiegelhalter cowel 1992 show method robust enough cope systematically missing data case estimates rely heavily prior distribution deterministic methods fail current practice resort estimate posterior means using stochastic methods describe popular stochastic method bayesian inference gibbs sampling basic idea gibbs sampling algorithm parameter sample value conditional distribution given parameters data database ii repeat parameters iii iterates steps several times proved broad conditions algorithm provides sample joint posterior distribution given information database sample used compute empirical estimates posterior means function parameters practical applications algorithm iterates number times stability seems reached final sample joint posterior distribution parameters taken buntine 1996 entries database missing gibbs sampling treats missing data unknown parameters missing entry values sampled conditional distribution corresponding variables given parameters available data algorithm iterated reach stability sample joint posterior distribution taken used provide empirical estimates posterior means thomas et al 1992 approach relies assumption unreported data missing random missing random assumption violated data systematically missing method suffers dramatic decrease ac curacy completion database using available information database leads learning system ascribe missing data known values database case systematically missing data twist estimates probabilities database apparent behavior prevent applicability learning method generate bbns general case produce unreliable estimates theory solution propose robust method learn parameters bbn method computes set possible posterior distributions consistent available information database proceeds refining set information becomes available set contains possible estimates parameters bbn efficiently computed calculating extreme posterior distributions consistent database section first define basic concepts convex sets probabilities probability intervals describe theoretical basis learning method 31 probability intervals traditionally probability function px assigns single real number probability x requirement called credal uniqueness assumption one controversial points bayesian theory difficulty assessing precise realvalued quantitative probability measures long standing challenge probability decision theory motivated development alternative formalisms able encode probability distributions intervals rather real numbers original interest notion belief functions proposed dempster dempster 1967 developed shafer shafer 1976 mainly due ability represent credal states intervals rather pointvalued probability measures grosof 1986 several efforts addressed interpret probability intervals within coherent bayesian framework thus preserving probabilistic soundness normative character traditional probability decision theory levi 1980 kyburg 1983 stiling morrel 1991 approach regards probability intervals convex sets standard probability distributions therefore referred convex bayesianism convex bayesianism subsumes standard bayesian probability decision theory special case credal states evaluated convex sets probability functions containing one element convex bayesianism relaxes credal uniqueness assumption replacing realvalued function px convex set functions convex set probability functions p x set two probability functions 0 ff 1 real number use p ffl p ffl denote minimum maximum probability set p respectively intuition behind convex bayesianism even agent cannot choose realvalued probability px information enough constrain px within set p x possible values appealing feature approach probability function p ff fulfills requirements standard probability function thus preserving probabilistic soundness normative character traditional bayesian theory conservative interpretation convex bayesianism called sensitivity analysis robust bayesian approach berger 1984 within set p x regarded set precise probability functions inference carried possible combinations probability functions good good 1962 proposes model process black box translates probability functions set constraints ideal pointvalued probability function review methods may found walley 1991 theoretical framework especially appealing faced problem assessing probabilities incomplete database indeed case safely assume existence ideal point valued probability probability value would assess database complete suitability convex bayesianism represent incomplete probabilistic information motivated development computational methods reason basis probability intervals white 1986 snow 1991 natural evolution approach leaded combination computational advantages provided conditional independence assumptions expressive power probability intervals van der gaag 1991 fore efforts addressed extend bbns realvalued probabilities interval probabilities breeze fertig 1990 ramoni 1995 thus combining advantages conditional independence assumptions explicit representation ignorance efforts provide different methods use bbns learn method 32 learning describe method propose using artificial database table 2 bbn given figure 1 table 3 reports database given table 2 entries denoted missing proceed analyzing database sequentially show derive results stress method require sequential updating start assuming total ignorance hence parameters given prior distribution d1 1 also assume independent consider first entry database complete case update distributions 1 2 3 8 1 distributions 3 changed since entries c 1 information processing first case thus bayes estimates parameters case table 3 incomplete database displayed table 2 consider case c 2 observation x 3 missing know either 1 0 thus possible completions case c 2 two possible completions would yield 1 would updated whichever com pletion since parent configuration 0 0 observed consider updating distributions 3 7 8 3 d1 2 completion would lead 7 1 instead c 2 c2 would lead 3 1 hence possible bayes estimates instead summarizing information somehow represent via intervals whose extreme points minimum maximum bayes estimates would possible completions database thus instance two possible posterior distributions 3 learn similarly posterior distributions 7 8 learn 34 clearly also consider third case database observation x 3 missing consider two possible completions case proceed updating relevant distributions however need update intervals done updating distributions corresponding extreme points interval would yield four distributions hence 4 bayes estimates relevant conditional probabilities extract extreme ones instance completion c 3 c1 would lead updating among others distribution 7 two distributions obtained processing first two cases database would become d2 1 8 dated consider completion c 3 c2 7 updated 1 sort corresponding estimates increasing order find minimum maximum obtain p ffl x consider 34 minimum probability achieved assuming completions database assign 1 x 3 maximum achieved assuming completions database assign 0 x 3 process fourth case instance distribution 7 would updated considering completions 0 leading extreme probabilities 45 thus minimum obtained assigning 0 x 4 maximum assigning 1 x 4 generalized follows let x binary variable bbn denote n ffl 1jpax frequency cases parent configuration pax obtained completing incomplete cases similarly let n ffl 0jpax denote frequency cases given parent configuration pax obtained completion incomplete cases suppose start total ig 3norance thus parameter associated p1jpax assigned d1 1 prior processing information database minimum maximum probability complementary event p ffl 1jpax result easily generalized discrete variables k states leading note case sum maximum probability x ij jpax minimum probabilities x ih jpax h 6 j one worth noting bounds depend frequencies complete entries database artificial frequencies completed entries computed batch mode section devoted description algorithms implementing method outlined section 3 first outline overall procedure extract conditional probabilities database given network structure describe procedure store observations database analyze computational complexity algorithms finally provide details current implementation 41 overview section 3 described process learning conditional probabilities sequential updating prior probability distribution nature method allows us implement method batch procedure first parses database stores observations variables final step computes conditional probabilities needed specify bbn observations procedure takes input database defined section 2 network structure network structure identified set conditional dependencies fd x 1 associated variable dependency dx ordered tuple x child variable x parent nodes pax learning procedure takes case database statistical unit parses using dependencies defining network structure therefore entry case procedure recalls dependency within entry variable appears child identifies states parent variables case way case database procedure detects configuration states dependency recall probabilities states child variables given states parent variables parameters want learn database configuration procedure maintains two coun ters say nx detected configuration contain missing datum first counter nx ij jpax increased one datum one variables combination missing procedure increases one second counter configuration states variable missing entry words procedure uses counter n ffl ascribe sort virtual observation possible state variable whose value missing case database parsed need collect counters configuration compute two extreme lower bound using formula 3 upper bound using formula 4 let database x set variables bbn denote statesx set states variable x set immediate predecessors bbn learning procedure defined follows procedure learndx k jp j l jx j procedure store stores counters configuration parent states described next subsection procedure collect simply collects counters state variable x 42 storing apparent procedure store plays crucial role efficiency procedure order develop efficient algorithm used discrimination trees store parameter counters following slightly modified version approach proposed ramoni et al 1995 along approach state variable network assigned discrimination tree level discrimination tree defined possible states parent variable path discrimination tree represents possible configuration parent variables state way path associated single parameter network leaf discrimination tree holds pair counters figure 3 discrimination tree associated state x 3 1 entry missing datum need follow path discrimination tree identify counters update order save memory storage discrimination trees incrementally built branch tree created first time procedure needs walk procedure store uses ordered tuples identify variable states interest state identified ordered tuple x x x variable x reported value value reported x procedure store takes input three arguments ordered tuple c representing current state child variable set ordered tuples state parent variables c current case flag dictating whether update induced missing datum procedure storec v c2 else p2 else counter ffl pcounter ffl p else return else functions counter counter ffl identify counters n respectively associated leaf node tree order illustrate procedures work lets turn back example described section 2 procedure learn starts parsing first line database reported table 3 uses network structure depicted figure 1 partition case four relevant elements corresponding parameters network structure suppose third entry stored discrimination tree associated state x 3 1 figure 3 displays discrimination tree associated state x 3 1 procedure walks along solid line figure 3 updates counter nx entry include missing datum suppose entry stored figure 3 identifies dashed line paths followed procedure case procedure hits state variable whose datum reported walks possible branches following updates n ffl counters end path 43 computational complexity learning procedure takes advantage modular nature bbns partions search space using dependencies network partitioning main task performed procedure learn learn starts scanning database elements row scans row identify patterns dependencies bbn suppose database contains n columns rows superior bound execution time part algorithm ogmn 2 g maximum number parents variable bbn note number columns equal number variables bbn procedure collect scans generated discrimination trees applied procedure number discrimination trees generated learning process discrimination tree number leaves parents variable state associated discrimination tree note number conditional distributions need learn define conditional dependency main job algorithm left procedure store using discrimination trees time required procedure store ascribe one entry appropriate counter linear number n parent variables dependency reported entry parameter contain missing datum data missing procedure worse case exponential number parent variables unreported data row worth noting main source complexity algorithm depend dimension database topology bbn previous example makes clear order variables tree plays crucial role performance algorithm positions states variable x 1 x 2 exchanged procedure walk whole tree rather half efficiency may gained careful sorting variable states tree using precompilation techniques estimate advance variables whose values missing often database task accomplished using common techniques machine learning community estimate information structure classification trees quinlan 1984 44 implementation method implemented common lisp machintosh performa 6300 machintosh common lisp porting clisp running sun sparc station development system implemented module general environment probabilistic inference called refinment architecture era ramoni 1995 era originally developed sun sparc 10 using lucid common lisp development environment careful use standard common lisp resources order develop code conforming new established ansi standard therefore code easily portable common lisp development environment implementation learning system deeply exploits modularity allowed common lisp object system clos protocol included ansi standard instance learning algorithm uses clos classes implemented architecture represent elements network structure strategy allows straightforward integration learning module within reasoning modules era way results learning process immediately available reasoning modules era draw inferences make decisions 5 experimental evaluation gibbs sampling currently popular stochastic method bayesian inference complex problems learning data missing although limitations wellknown convergence rate slow resource consuming however given popularity compared accuracy method one implementations section report results two sets experimental comparisons one using realworld problem one using artificial example aim experiments compare accuracy parameter estimates provided gibbs sampling method available information database decreases figure 4 network structure bbn used first set experiments 51 materials order experimentally compare method gibbs sampling choose program bugs thomas et al 1992 commonly regarded reliable implementation technique buntine 1996 following experiments used implementation bugs version 05 running sun sparc 5 sunos 55 era implementation method running machintosh powerbook 5300 machintosh common lisp version 39 experiments reported section share materials different materials used set experiments illustrated description experiment 52 experiment 1 child network first set experiments used wellknown medical problem problem already used bbns literature spiegelhalter et al 1993 concerns early diagnosis congenital hearth disease newborn babies name states 1 birth asphyxia yes disease pfc tga fallot paivs tapvd lung 3 age 03days 410days 1130days 4 lvh yes 5 duct flow lttort none rttolt 6 cardiac mixing none mild complete transp 7 lung parenchema normal congested abnormal 8 lung flow normal low high 9 sick yes 11 hypoxia o2 mild moderate severe chest xray normal oligaemic plethoric grdglass asypatchy 14 grunting yes 19 xray report normal oligaemic plethoric grdglass asypatchy grunting report yes table 4 definition variables child bbn first column reports numeric index used figure 4 second variable name third possible states 521 materials figure 4 displays network structure medical problem table 4 associates number bbn name variable reports possible values clinical problem underlying bbn depicted figure 4 described frankin et al 1989 task bbn diagnose occurrence congenital heart disease pediatric patients using clinical data reported phone referring pediatricians authors report referral process generates large amount clinical information information often incomplete includes missing unreported data features together reasonable realistic size bbn makes problem ideal testbed experiments bbn figure 4 defined 344 conditional probabilities although minimal set figure 5 plots estimates amount information parameters slightly 240 since variables bbn binary number cases reported original database 100 since aim experiment test accuracy competing methods cannot use original database need reliable measure probabilities want systems assess therefore still using original network generated complete random sample 100 cases known distribution used sample database learn using database bbn figure 4 run two different tests test 1 missing random goal first test characterize behavior method respect gibbs sampling missing random assumption holds method start complete database parameters independent uniformly distributed run learning algorithm gibbs sampling proceed randomly deleting 20 entries database running two methods incomplete database database empty incomplete database run 10000 iterations gibbs sampling appeared enough reach stability estimates returned based final sample 5000 values results figure 5 shows estimates conditional probabilities defining dependency linking variable n2 disease variable inferred two learning methods 5 different proportions completeness report dependency example overall behavior systems test stars report point estimates given gibbs sampling errorbars indicates 95 confidence interval estimates solid lines indicates lower upper bounds probability intervals inferred method choose represent outcomes gibbs sampling system two different ways basic semantic clash gibbs sampler confidence intervals intervals returned system estimates given gibbs sampling database incomplete based likely reconstruction missing entries relies prior belief parameters complete data 95 confidence intervals represent posterior uncertainty parameters depend inferred database prior uncertainty thus larger intervals less reliable estimates intervals returned method represent set posterior estimates parameters would obtain considering possible completions database particular estimates based original database estimates returned gibbs sampling one width intervals measure uncertainty considering possible completions database information database complete intervals systems degenerate single point point coincides exact estimates semantic difference intervals returned two systems accounts slightly tighter intervals returned gibbs sampling together fact assuming data missing random gibbs sampling exploit piece information figure plots estimates amount information parameters available system nonetheless width intervals returned two systems overall comparable case reported figure 5 well remaining parameters bbn main difference execution time worse case gibbs sampling took 37 minutes run completion sun sparc 5 system ran completion less 020 seconds machintosh 523 test 2 systematically missing happen data missing random therefore gibbs sampling using wrong guess aim second test compare behavior two systems data missing random value variable systematically removed database method procedure used test slight modified version test randomly missing data case iteratively deleted figure 7 plots estimates amount information parameters 1 database systematically removing entries reporting value normal variable n7 lung parenchema ran two learning algorithms run gibbs sampling based 10000 iterations final sample 5000 cases procedure iterated value normal reported variable n7 database results local independencies induced network structure modification values variable n7 database affect estimates conditional probabilities immediate predecessors successors therefore focus estimates parameters affected changes figure 6 shows estimates provided two systems conditional probabilities states variable n7 given state immediate predecessor bbn note information percentage reported xgammaaxis starts 98 meaning number entries state n7 normal account 2 complete database 98 database contains entry normal results report pointvalued estimates gibbs sampling always fall within intervals calculated sys tem however plots b display behavior gibbs sampling estimates jumping lower bound upper bound set calculated method missing data replaces entries n7 normal jump testifies 30 error point estimate provided gibbs sampling entries normal missing behavior systems change data missing parent variables rather child variable figure 7 plots estimates parameters dependency linking parent variables n7 lungparench n6 cardiac mixing child variable n11 hypoxia o2 case initial error gibbs sampling goes 45 shown plot b error even remarkable realize 98 overall information still available difference execution time comparable one previous test worse case gibbs sampling took 16 minutes run completion sun sparc 5 system ran completion less 020 seconds machintosh powerbook 5300 53 experiment 2 artificial network results experiments child network show bias point estimates given gibbs sampling although associated confidence intervals always large enough include estimates calculated complete database since width confidence intervals function sample size left doubt large database could give tighter intervals around biased estimates results prompt accurate investigation behavior size database increases thus section use artificial therefore controllable example order amplify bias rationale behind second experiment twofold display effect learning process bias induced missing random assumption fact data systematically missing size database large ii show effect bias predictive performance bbn figure 8 simple network used second set experiments 531 materials figure 8 shows simple bbn used second experiment two binary variables linked dependency generated database 1000 random cases following probability distribution parameters bbn assumed independent uniformly distributed 532 learning using bbn displayed figure 8 tried make clearer bias effect detected previous set experiments method followed procedure analogue used second test previous experiment iteratively deleted 10 entries value database run method gibbs sampling incomplete database run gibbs sampling based 1000 iterations reach stability final sample 2000 values results figure 9 shows parameter estimates given two sys tems bias gibbs sampling absolutely clear 75 information available database entries missing estimate given gibbs sampling lies lower extreme interval estimated method however figure 9 plots estimates agaist amount information parameters sample size tight confidence interval around estimate 00034 true estimate 04955 would computed complete database definitely excluded error overpassing 40 estimate px moves 08919 complete database 0662 entries left database ever wide interval associated estimate testifies low confidence gibbs sampler dramatic effect missing random assumption estimates conditional probability 01286 complete database 05059 narrow confidence interval 0475205367 narrow interval overestimates reliability inferred value excludes true value inferred complete database execution time 10 minutes gibbs sampling less seconds system 54 prediction goal learning bbns use perform different reasoning tasks prediction aim second test evaluate reliability predictions given learned bbn materials used bbn learned two systems previous test predict value x 2 given observation x 1 results effect strong bias estimates returned gibbs sampling remarkable predictive performance network suppose x observed want predict value x 2 since case px reduces px figure 9 c plots marginal probability px suppose use estimates learned gibbs sampling 75 complete data entries missing prediction gibbs sampling px value 01286 would inferred complete database instead method returns probability interval 012 054 thus including true value results experiments match expectations accuracy two systems overall comparable data missing random however estimates given gibbs sampling prone bias data systematically missing reason behavior easy identify consider instance artificial example figure 8 reconstruction original database gibbs sampling exploits available information ascribe missing entries mainly x hence high confidence estimate conditional distribution results show also method robust instead betting likely complete database could infer available information incomplete database hand method returns results make problem solver aware ignorance feature even important consider remarkable effect strong bias estimates returned gibbs sampling predictive performance bbn last word execution time computational cost gibbs sampling well known issue results experiments show computational advantages deterministic method respect stochastic simulator 6 conclusions incompleteness common feature realworld databases ability learning databases incomplete data basic challenge researchers face order move methods applications key issue method able learn incomplete database reliability knowledge bases generate results investigation shows common missing random assumption exploited current learning methods dramatically affect accuracy results paper introduced robust method learn conditional probabilities bbn rely assumption order drop assumption change overall learning strategy respect traditional bayesian methods rather guessing value missing data basis available information method bounds set posterior probabilities consistent database proceed refining set information becomes available main feature method robustness respect distribution missing data relies assumption data missing random try infer available information basic intuition behind method better rather trying complete database guessing value missing data regard available information set constraints possible distributions database reason basis set probability distributions consistent database hand experimental comparison method powerful stochastic method shows remarkable difference accuracy two methods computational advantages deterministic method respect stochastic one acknowledgments authors thank greg cooper pat langley zdenek zdrahal useful suggestions development research equipment provided generous donations apple computers sun microsystems r robust bayesian viewpoint decision making interval influence diagrams operations learning graphical mod els guide literature learning probabilistic networs data bayesian method induction probabilistic networks data comparison sequential learning methods incomplete data maximum likelihood incomplete data via em algorithm upper lower probabilities induced multivalued mapping combining clinical judgments clinical data expert systems subjective probability measure nonmeasurable set inequality paradigm probabilistic knowl edge learning bayesian networks combinations knowledge statistical data rational belief enterprise knowledge probabilistic inference using markov chain monte carlo methods probabilistic reasoning intelligent systems networks plausible inference learning efficient classification procedures application chess games ignorant belief network forecast glucose concentration clinical databases ignorant influence diagrams mathematical theory evidence improved posterior probability estimates prior linear constraint system learning probabilistic expert systems sequential updating conditional probabilities directed graphical structures bayesian analysis expert systems covex bayesian decision theory bugs program perform bayesian inference using gibbs sampling computing probability intervals independency constraints statistical reasoning imprecise probabilities posteriori representations based linear inequality descriptions priori conditional probabilities tr probabilistic reasoning intelligent systems networks plausible inference bayesian method induction probabilistic networks data c45 programs machine learning em algorithm graphical association models missing data learning bayesian networks irrelevance parameter learning bayesian networks bayesian classification autoclass optimality simple bayesian classifier zeroone loss bayesian network classifiers expert systems probabiistic network models probability intervals influence diagrams bayesian methods ctr marco zaffalon marcus hutter robust inference trees annals mathematics artificial intelligence v45 n12 p215239 october 2005 gert de cooman marco zaffalon updating beliefs incomplete observations artificial intelligence v159 n12 p75125 november 2004 ferat sahin etin yavuz ziya arnavut nder uluyol fault diagnosis airplane engines using bayesian networks distributed particle swarm optimization parallel computing v33 n2 p124143 march 2007