symbolic interpretation artificial neural networks abstracthybrid intelligent systems combine knowledgebased artificial neural network systems typically four phases involving domain knowledge representation mapping knowledge initial connectionist architecture network training rule extraction respectively final phase important provide trained connectionist architecture explanation power validate output decisions moreover used refine maintain initial knowledge acquired domain experts paper present three ruleextraction techniques first technique extracts set binary rules type neural network two techniques specific feedforward networks single hidden layer sigmoidal units technique 2 extracts partial rules represent important embedded knowledge adjustable level detail third technique provides comprehensive universal approach ruleevaluation technique orders extracted rules based three performance measures proposed three techniques area applied iris breast cancer data sets extracted rules evaluated qualitatively quantitatively compared obtained approaches b introduction several researchers investigated design hybrid systems combine expert connectionist subsystems 44 45 54 10 16 15 27 typical result knowledge based neural network kbnn system four phases rule base representation phase initial domain knowledge extracted represented symbolic format eg rulebased system ii mapping phase initial domain knowledge mapped initial connectionist architecture iii learning phase connectionist architecture trained set domain examples iv rule extraction phase trained thus modified connectionist architecture mapped back updated rulebased system provide explanation power kbnns attempt exploit complementary properties knowledge based neural network paradigms obtain powerful robust systems hia 44 kbann 53 34 rapture 27 kbcnn 10 11 examples kbnn hybrid systems figure 1 sketches typical components kbnn system combines rulebased connectionist paradigms researchers also combined connectionist systems fuzzy logic systems obtain fuzzy logic neural networks flnn neurofuzzy hybrid systems flnns neural network subsystem typically used adapt membership functions fuzzy variables 6 refine extract fuzzy rules 48 47 24 rules extracted updated rulebased system output hybrid rulebased decisions concepts learned architecture connectionist dec isions decisions integrated decision maker training examples rule extraction module extracted prior domain knowledge initial mapping mapping revised figure 1 typical components kbnn system integrates knowledge based connectionist paradigms extracting symbolic rules trained anns important feature comprehensive hybrid systems helps 1 alleviating knowledge acquisition problem refining initial domain knowledge 2 providing reasoning explanation capabilities 3 supporting crossreferencing verification capabilities 4 alleviating catastrophic interference problem anns since different set rules extracted network retrained using new environment examples one examine resulting rule bases find situation set suitable due capabilities extracting rules trained anns may essential obtaining powerful robust selfexplanatory selfmaintained hybrid systems paper proposes three rule extraction techniques kbnn hybrid systems also presents simple rule evaluation procedure orders rules obtained extraction approach according performance criteria qualitative evaluation three new techniques comparison approaches also provided next section illustrates key issues extracting rules trained neural networks summarizes existing rule extraction techniques section 3 describes proposed techniques rule evaluation procedure section 4 present implementation results three techniques using artificial problem well iris breast cancer data sets section 5 compares performance rule sets extracted techniques rule sets extracted approaches concluding section comment different rule extraction techniques summarize significance proposed techniques point future directions rule extraction 21 issues designing efficient rule extraction module fact difficult task several factors carefully considered designing rule extraction technique 1 transparency extracted rules transparent system self explanatory system capable attaching sufficient hypotheses evidence output decisions explain reaches 2 granularity explanation feature level detailed hypotheses evidence system provide output decisions 3 comprehensiveness extracted rules terms amount embedded knowledge captured 4 comprehensibility indicated number rules number premises extracted rule trained network 5 fidelity measure capability extracted rules mimic embedded knowledge trained network 6 accuracy accurate rulebased module one generalize well unseen examples 7 portability capability rule extraction algorithm extract rules different network architectures 8 modifiability ability extracted rules updated corresponding trained network architecture updated retrained different data sets issue depends trained network rule extraction modules interact 9 refinement capability capability extracted rules help resolving knowledge acquisition bottleneck ie incompleteness inconsistency andor inaccuracy initially acquired domain knowledge quality extracted rules improved increasing comprehensibility fidelity accuracy however extract comprehensive rule base trained ann embedded knowledge ann extracted case comprehensibility extracted rules degraded resulting rule base may many rules many premises 22 existing rule extraction techniques research work area extracting symbolic knowledge trained anns witnessed much activity recently subsection summarizes existing approaches emphasis extracting rules feedforward specifically mlp ann architectures rich source literature review different rule extraction approaches technical report written andrews et al 1 221 link rule extraction techniques methodology behind techniques rule extraction mlps summarized two main steps hidden output node network search different combinations input links whose weighted sum exceeds bias current node ii combination generate rule whose premises input nodes combination links premises rule conjuncted either 35 kt 9 subset 52 three notable rule extraction algorithms category main problems kt subset algorithms size search algorithm o2 l hiddenoutput node assuming network inputs binary ii algorithms extract large set rules fi p 1 numbers subsets positivelyweighted negativelyweighted links respectively iii generated rules may repetitive iv extracted rules tend hide significant structures trained network however rules extracted algorithms simple understand size extracted rules limited specifying number premises rules generally rules extracted kt subset algorithms tractable specially small application domains based shortcomings subset algorithm towell shavlik 52 developed another rule extraction algorithm called mofn name algorithm reflects rule format algorithm uses represent extracted rules least following n premises true concept designated unit true rationale behind mofn find group links form equivalence class class members effect ie similar weight values used interchangeably one another mofn extracts rules kbann trained network six main procedures rules extracted mofn significantly superior rules extracted symbolic approaches c45 37 either 35 linus 8 least problems like promoter recognition dna nucleotides natural fit 52 neurorule another rule extraction approach uses different combinations weighted links extract rules 43 main difference neurorule mofn former extracts rules networks pruning architectures discretizing hidden units activation values recently howes crook introduced another algorithm extracts rules feedforward neural networks 18 network architecture used algorithm restricted one hidden layer network trained binary sigmoid activation function rationale behind algorithm extract maximally general rules trained network using linear activation constraint function puts limits hidden nodes activation values satisfy activation output nodes least 09 step algorithm searches input combinations satisfy predetermined constrained hidden nodes activation values found algorithm extracts corresponding rule combination algorithm well previously mentioned approaches works binary networks howes crook proposed extended version algorithm continuous valued inputs currently far efficient reported algorithms authors categorize approaches mentioned subsection link rule extraction lre techniques first search weighted links cause node hidden output active combinations weighted links used generate symbolic rules heuristic methods commonly used lre category bound search space rules increase comprehensibility extracted rules researchers use term decompositional methods refer lre type techniques 1 11 several rule extraction approaches extract rules feedforward anns reported main difference approaches mentioned extract rules specialized anns rulenet 30 rulex 3 2 two examples class approaches rulex extracts rules constrained error backpropagation cebp mlp network similar radial basis function rbf networks hidden node cebp network localized disjoint region training examples distinctive feature rulex controls search space network approaches use heuristic measures rulenet hand uses idea adaptive mixture local expert 19 train localized ann extracts binary rules lre approach rulex rulenet classified localized lre techniques 222 blackbox rule extraction techniques another class rule extraction approaches extracts rules feedforward networks examining inputoutput mapping behavior example rule extraction approach algorithm developed saito nakano extract medical diagnostic rules trained network 39 brainne 40 ruleextractionaslearning 7 dedec 50 examples extracting rules investigating inputoutput mapping trained network paper refer class blackbox rule extraction bre category rules extracted regardless type structure neural network another given name class rule extraction techniques 00 pedagogical 00 approaches 3 example dedec extracts rules ranking inputs ann according importance contribution ann outputs 51 ranking process done examining weight vectors ann puts dedec border lre bre techniques next step dedec cluster ranked inputs use cluster generate set optimal binary rules describes functional dependencies attributes cluster outputs ann dedec implemented using standard feedforward mlp cascaded correlation cascor ann spite lre nature ranking procedure dedec classified bre since main theme extract rules based inputoutput mapping 223 extracting fuzzy rules anns research area fuzzy logic neural networks flnn neurofuzzy systems concerned combining neural networks fuzzy logic flnn systems include fuzzy rule extraction module refining fuzzy sets membership functions explaining trained neural network 17 48 47 24 224 extracting rules recurrent networks recurrent networks shown great success representing finite state languages 14 55 deterministic finite state automata 13 omlin giles 33 developed heuristic algorithm extract grammar rules form deterministic finitestate automata dfa discretetime neural networks specifically secondorder networks starting defined initial network state represents root search space dfa rule extraction algorithm searches equally partitioned output space n state neurons breadthfirst fash ion authors claim dfa rules extraction algorithm improves network generalization performance based stability internal dfa representation 3 proposed rule extraction approaches section introduce three different approaches extracting rule bases trained neural networks suitability approach depends network type inputs complexity nature application required quality extracted rules factors explained later first approach blackbox rule extraction technique second third approaches belong link rule extraction category also evaluation procedure rule ordering algorithm measure firing false alarm rates extracted rule order introduced applied existing rule extractors well three proposed methods 31 first approach biore first approach simple black box rule extraction technique surprisingly effective within relatively narrow domain applicability named binarized inputoutput rule extraction biore extracts binary rules neural network trained binary inputs based inputoutput mapping original inputs binary binarized using equation 1 1 x value original input x mean value x corresponding binarized input value x unique features biore 1 require information internal structure network 2 used extract rules kind neural network eg rbfs mlps recurrent networks 3 require specific training regime supervised unsupervised outline biore algorithm follows well trained neural network 1 obtain network output oy corresponding binary input pattern number input nodes n conceptually requires 2 n input patterns however problem specification may remove combinations occur 2 generate truth table concatenating input step 1 corresponding output decision oy trained network output set 1 corresponding output node active threshold otherwise 0 3 generate corresponding boolean function represented previously described binary rule format truth table step 2 available boolean simplification method used perform step 3 biore algorithm eg karnough map 22 algebraic manipulation tabulation method 29 used espresso 1 generate extracted rules 4 rules extracted biore represented inputvariable inputvariable gammaconsequent j delta optional term delta means term delta repeated 0 n times terms final rules extracted rule 1 2 1 rewritten binary input eg 1 represented negated binary input variable eg 2 represented section 43 examples biore approach suitable putoutput variables naturally binary binarization significantly degrade performance also input size n small given conditions satisfied biore advantages 1 allows use available logic minimization tools 2 extracted rules optimal cannot simplified hence rewriting procedure required 1 espresso software package logic design 38 3 extracted rules depend number layers trained network 4 set rules extracted biore comprehensive understandable 5 premises extracted rules conjuncted 6 limitation number premises extracted rules biore however maximum number premises rule equal number input nodes network biore algorithm tested three problems first representative binary problem used study biore soundness correctness two public domain examples used compare efficiency performance existing algorithms section 4 presents experimental results 32 second approach partialre idea underlying partialre algorithm first sorts positive negative incoming links hidden output node descending order two different sets based weight values starting highest positive weight say searches individual incoming links cause node j hiddenoutput active regardless input links node link exists generates rule node cf gamma node j cf represents measure belief extracted rule equal activation value node j current combination inputs values certainty factors computed equation 3 node found strong enough activate node j node marked cannot used combinations checking node j partialre continues checking subsequent weights positive set finds one cannot activate current node j important mention partialre assumes inputs range effect hidden layer simply determined weights therefore original input features may need scaled using equation 2 10 20oe 2 corresponding scaled input value original input value x oe standard deviation input feature x equation 2 oe multiplied 2 provide wider distribution input x range sigma 2oe contain approximately 95 percent normally distributed detailed rules required ie comprehensibility measure p 1 partial starts looking combinations two unmarked links starting first maximum element positive set process continues partialre reaches terminating criteria maximum number premises rule p also looks negative weights inputs active node higher layer network going active extracts rules format node g cf gamma node j link node g node j negative value moreover looks small combinations positive negative links cause hiddenoutput node active case extracted rules represented node node g cf gamma node j link node j positive g j negative extracting rules rewriting procedure takes place within rewriting procedure premise represents intermediate concept ie hidden unit replaced corresponding set conjuncted input features causes active final rules written format cf gamma consequent j see table 2 6 examples partialre used efficiently applications main objective extracting rules trained neural networks study main parameters cause specific output decisions taken moreover partialre used analyze correlations inputoutput parameters many applications use neural networks either function approximation classification tasks cases cost implementing partialre low compared mofn algorithm small number premises per rule enough extracting certain rules small number premises per rule reducing combinatorial nature rule extraction process one polynomial n partialre examines small subsets js incoming links hidden output node j extracts rule w ji x w ji weight value link input x hiddenoutput node j j threshold value node j delta small positive value 01 03 called certainty parameter value certainty parameter delta added previous equation make sure incoming links node j high enough cause node j active therefore extracted rules certain rules value delta chosen based certain extracted rules fact delta p determines number premises rule adjustable parameters increase efficiency partialre algorithm partialre easily parallelizeable experimental results show partialre algorithm suitable large size problems since extracting possible rules nphard extracting effective rules practical alternative 33 third approach like partialre approach fullre falls lre category notable 1 extracts rules certainty factors trained feedforward anns 2 extracts possible rules represent semantic interpretation internal structure trained neural network extracted 3 extract rules networks trained continuous normal binary inputs therefore restriction values input feature take capability makes fullre universal extractor 4 applicable neural network node unit monotonically increasing activation function examining different possible combinations incoming links feedforwarding effect output nodes fullre first generates intermediate rules format gamma consequent j c constant representing effect th input x consequent j j constant determined based activation value node j make active node j layer node eg node input node node j hidden node c represents weight value w ji link two nodes note range x values may satisfy intermediate rule one would want determine suitable extremum value range make tractable input range discretized small number values subsequently examined thus input feature discretized using k intervals ilgamma1 il lower upper boundary values interval l input x respec tively different discretization approaches exploited compute discretization boundaries input features x 46 5 23 26 56 fullre uses chi2 25 algorithm 2 powerful discretization tool compute discretization boundaries input features fullre finds 2 thankful liu setiono making chi2 source code available us one discretization value input x satisfy intermediate rule ie rule one feasible solution chooses minimum maximum values based sign corresponding effect parameter c c negative fullre chooses minimum discretization value x otherwise chooses maximum value however selected discretization values satisfy left hand side inequality intermediate rule boundary constraints input features inequality fullre method summarized following steps hidden node j well trained mlp 1 consider equation high enough make node j active activation function node j sigmoid 1 2 given discretization boundaries input feature consider linear programming lp problem minimizing w j1 fullre solves lp problem selecting discretization boundaries x determine feasible solutions intermediate rule satisfy given constraints values single input features satisfy lp problem regardless input features found easily substituting x positive weights node j minimum values negative weights maximum values b higher combinations found similarly examining different combinations discretized inputs finding edges feasible solution surface lp problem note linear programming tools also used solve standard lp problem 3 example assume feasible solution x x effect input x 1 x 2 node j positive negative respectively based extracted intermediate rule node j ie c 1 extracts following rule cf determined discretization process lp tool fullre 3 also used mathematica find feasible solutions computes certainty factors extracted rules based 4 equation 3 actj sigmoid linear threshold actj hard limiting actj hard limiting p n activation values hidden nodes bounded 01 fullre uses simplified version procedure extract rules hidden output nodes discretizing outputs hidden nodes longer required however extracted rules hidden output nodes represented format partialre eg h 1 h 2 cf gamma k fullre replaces hidden node h j previous rule left hand side rules whose right hand side h j general format final rules extracted fullre simplebooleanexpression simplebooleanexpression cf gamma consequent j simplebooleanexpression variable operator constant operator means term simplebooleanexpression repeated 0 n times stands alternation ie operator take four boolean operators cf represent certainty factor computed equation 3 extracted rule certainty factor cf rule represents measure confidencebelief rule consequent premises true final rules extracted fullre represented format partialre expect replaced one discretization boundaries say il selected fullre described earlier see table 3 7 examples note restriction number premises final rules extracted fullre limitation applied number premises rules nodes adjacent layers eg number premises intermediate rules input hidden nodes hidden output nodes note input features binary discretization step longer required 4 fullre generates rule cf computed eqn 3 05 34 rule evaluation evaluate performance rules extracted trained networks three presented techniques rule extraction approach developed simple rule evaluation procedure attaches three performance measures extracted rule partialre fullre approaches certainty factor attached extracted rule used along three performance measures evaluate extracted set rules main motivations developing rule evaluation procedure 1 find best order extracted rules maximizes performance available data set 2 test fidelity extracted rulebased system ie capability mimic embedded knowledge trained network objective achieved comparing performance extracted rules corresponding trained neural network performance 3 measure much knowledge left unextracted internal structure trained network 4 cases extracted rulebased system surpasses trained neural network vice versa analysis helps process integrating combining output decisions two subsystems values performance measures depend inference engine used fire extracted rules simple inference engine one examines rules predetermined sequential order decision thus determined first fireable rule predetermined order alternatively inference engine check possible rules fire provide one output decision time latter inference engine considered powerful former provides system user possible output decisions hence choices examined practice types inference engines predetermined order extracted rules plays important role determining rule going fired order fireable rules considered since embedded knowledge internal structure trained neural network directly help resolving problem ie ordering extracted rules rule evaluation procedure help order extracted rules crucial three performance measures 1 soundness measure measures many times rule correctly fired rule correctly fired premises satisfied consequent matches target decision soundness measure extracted rule represents ability rule correctly interpret output decisions trained network note soundness measure depend rule order 2 completeness measure completeness measure attached rule represents many distinct times rule correctly fired ie many unique patterns correctly identifiedclassified rule extracted rule inspected inference engine rule certainly resulting number case depends order extracted rules applied mechanism inference engine extracted set rules consequent sum completeness measures rules set equals total number input patterns corresponding output set extracted rules 100 complete respect consequent extracted rule zero completeness measure soundness measure 0 means preceding rules order rule application covers input patterns rule covers rule may removed 3 falsealarm measure measures many times rule misfired available data set considered application rule misfired premises satisfied consequent match target output value measure also depends order rule application mechanism inference engine 341 rule ordering algorithm finding optimal ordering extracted rules combinatorial problem developed following greedy algorithm order set extracted rules based three performance measures rule ordering algorithm first creates list l contains extracted rules assume list l divided two lists head list l h tail list l l h list ordered rules l list remaining unordered rules 5 initially l h empty l includes extracted rules performance criteria used select one rule 5 ie ordering rules l effect l moved end l h process continues till l null steps rule ordering algorithm follows 1 initialize l extracted rulesg 2 l 6 f g fire rules l h order b compute completeness falsealarm measures rule l using available data set c 9 rule zero falsealarm rule moved l end l h 6 else among rules l select one highest completeness falsealarm measure add rule end l h delete form l 9 rule l zero completeness measure remove rule l means rules l h cover rule 3 end paper rules extracted approaches ordered using rule ordering algorithm also measures attached extracted rules assume inference engine fires one rule per input namely first fireable rule used important issue needs addressed one discard rules low soundness completeness andor high falsealarm measures example rule r 10 table 5 small data sets might still retain rules bottom application ladder hope better generalization part overall characteristics corresponding trained network cases available data sets representative answer depends application nature example medical applications one interested 6 9 one rule zero falsealarm select one highest completeness measure rules moved l end lh high detection rates rules low soundness completeness andor high falsealarm measures may still kept applications like automatic target recognition atr one may retain rules low falsealarm rate reduce chances friendly fire three performance measures along rules certainty factor applicable used form composite measure application dependent manner specifies importance extracted rules 4 implementation performance evaluation 41 data sets applied three rule extraction techniques three problems 1 artificial rulebased system six rules relating four binary inputs four binary outputs 2 iris database simple classification problem contains 50 examples classes iris iris versicolor iris virginica 32 150 instances divides two subsets first subset used training size 89 second size 61 used testing input pattern four continuous input features sepalwidth petallength 4 petalwidth 3 breastcancer data set nine inputs two output classes 28 32 input features shape bland chromatin x mitoses 9 inputs continuous range 1 10 683 available instances labeled benign 444 instances malignant instances divided training set size 341 test set size 342 popular data sets used benchmarks rule extraction approaches monk 49 mushroom 21 dna promoter 54 data sets three data sets inputs symbolicdiscrete nature since want test general problems may include continuous valued variables iris breastcancer preferred initial experiments 42 methodology following points illustrate important procedures followed perform experimental work presented paper 1 training procedure experiments mlp network trained using backpropagation algorithm momentum well regularization term p adds gamma2w jk w weight update term backpropagation equation 12 cross validation used stopping criteria 2 network architectures data reduction iris problem mlp 4 input 6 hidden 3 output nodes used three experiments trained different data sets time described later breastcancer classification problem reduced dimensionality input space 9 6 inputs done removing inputs correspond lowest three eigenvalues covariance matrix original input space remaining 6 input features used training testing mlp 9 hidden 2 output nodes 3 network initialization artificial problem six initial rules used node link algorithm 44 initialize network 4 input 6 hidden 4 output nodes iris breastcancer data sets prior knowledge corresponding networks initialized randomly 4 input representation inputs artificial problem naturally binary required mapping since input features iris breastcancer problems con tinuous biore partialre extract rules networks binarybinarized normalized inputs respectively binarized normalized version two data sets computed used training testing corresponding network architectures binarizing input features input feature value x corresponding binarized value computed equation 1 b normalizing input features normalized value z input feature value x computed equation 2 5 extraction techniques networks labeling biore used extract rules networks trained binarybinarized input patterns iris breastcancer problems networks labeled irisbin cancerbin respectively partialre used extract rules networks trained normalized input patterns labeled irisnorm cancernorm fullre uses original data sets problems train corresponding networks two networks labeled iriscont cancercont 6 class rule comprehensive rule extraction approach one extracts rules cover inputoutput mapping cases cases achieving goal hard may convenient cover inputoutput mapping cases cannot covered extracted rulebase using default rules default rules make set extracted rules complete provide interpretation action done none extracted rules could fired default rule used output consequent chosen minimize false alarm rate maximize correct classification rate applications two goals may conflict cases criteria choosing default output decision depends application nature note default rule may fire none extracted rules fired 43 experimental results 431 artificial binary problem experiment designed test soundness completeness three rule extraction techniques original rules follows rule 1 b 08 gamma 1 rule 2 b c 07 gamma 2 rule 3 c 06 gamma 3 rule 4 07 gamma 4 rule 5 b 07 gamma 1 rule 6 08 gamma 1 binary inputs binary consequents using node links algorithm map six rules initial network 4 input 6 hidden 4 output nodes following two experiments performed table 1 rules extracted network irisbin biore technique rule rule iris soundness completeness falsealarm body class measure measure measure 4 12 versicolor 1050 1050 0150 4 12 setosa 5050 5050 5150 3 1 58 3 37 4 12 virginica 4750 4750 24150 4 1 58 2 30 4 12 versicolor 1550 1150 3150 performance 118150 32150 1 first experiment objective experiment check whether three approaches able extract original rules mapped network therefore network trained extraction procedures applied results applying three rule extraction techniques generated trained network follows ffl biore extracts set binary rules without certainty factors ffl partialre conditions per rule extracts 5 original rules two conditions less increasing p 3 rule2 also extracted certainty factors attached output decision approximately original rules ffl fullre extracts six original rules untrained network 2 second experiment based original rules 2 4 binary patterns generated training previous network applied three approaches final network architecture ie adapted one ffl biore fullre extract six original rules ffl partialre extracts six rules plus extra one rule 7 b 074 gamma 2 rule extracted table 2 rules extracted network irisnorm partialre technique rule rule iris certainty soundness completeness falsealarm body class factor measure measure measure 4 12 virginica 099 4750 4750 27150 3 1 58 2 30 3 37 versicolor 074 1850 1650 3150 4 1 58 2 30 4 12 versicolor 072 1550 150 0150 performance 118150 32150 table 3 rules extracted network iriscont fullre technique rule rule iris certainty soundness completeness falsealarm body class factorcf measure measure measure 3 3 48 virginica 098 4750 4750 1150 performance 146150 4150 432 iris classification table 1 2 3 present ordered rules extracted biore partialre fullre techniques respectively corresponding networks trained iris data set also present corresponding measures extracted rule generated rule evaluation procedure table 4 provides summary performance rule extraction technique compares performance corresponding trained network shows binarizing scaling input patterns iris problem degrades performance trained networks irisbin irisnorm well corresponding rules extracted two networks also shows remarkable performance rules extracted network iriscont fullre note numeric values compared input features rules extracted biore partialre represent mean input feature see rule bodies table 1 table 2 coarse thresholding largely responsible relatively poor performance table 4 performance comparison sets extracted rules corresponding trained networks iris problem neural network extracted rules ratio match ratio match binarized training network testing 4361 7049 5161 8361 normalized training network testing 5661 9180 4961 8033 continuous training network testing 5961 9672 6061 9836 two networks subsequently extracted rules ii table 3 numeric value compared input feature rule body represents one critical discretization boundaries feature selected fullre iii rules examined later eg rule 4 table 2 completeness may much less soundness instances rules would fire correctly already covered preceding rules iv fullre leads three simple rules classify iris data set well 433 breastcancer classification breastcancer classification problem table 5 6 7 present three sets ordered rules extracted three rule extraction techniques along corresponding performance measures table 8 provides overall comparison extracted rules corresponding trained networks shows three techniques successfully used approximately performance regardless nature training testing data sets used network also shows binarizing scaling breast cancer data set degrade performance trained networks well rules extracted biore partialre networks cancerbin cancernorm respectively since original input features breast cancer problem range 110 binarizing andor scaling change nature much table 5 rules extracted network cancerbin biore technique rule rule bcancer soundness completeness falsealarm body class measure measure measure table rules extracted network cancernorm partialre technique rule rule bcancer certainty soundness completeness falsealarm body class factorcf measure measure measure 9 total benign rules 427444 7683 total malignant rules 232239 17683 performance 659683 24683 table 7 rules extracted network cancercont fullre technique rule rule bcancer certainty soundness completeness falsealarm body class factor measure measure measure table 8 performance comparison sets extracted rules corresponding trained networks breastcancer problem neural network extracted rules ratio match ratio match binarized training network testing 317342 9269 329342 9620 normalized training network testing 325342 9503 328342 9591 continuous training network testing 331342 9678 327342 9561 44 discussion implementation results section 43 indicate 1 rules extracted three techniques sound 2 partialre sound complete completeness depends chosen degree comprehensibility p 3 rules extracted fullre much comprehensible extracted biore partialre likely due ffl fullre used extract rules neural networks trained original input features without binarization normalization ffl rules extracted fullre compare input features values discretization boundaries features two techniques compare mean 4 binarizing normalizing continuous features may degrade accuracy extracted rules well generalization capability corresponding trained neural network see first six rows table 4 5 fullre tested several times different networks initialized randomly time trained different sets training patterns time set extracted rules similar except values certainty factors indicates fullre accurate extract rules based combinations input features effective features see table 3 table 7 6 although biore partialre used extract rules networks trained binarized normalized input features still able extract certain rules may adequate application examples see table 5 table 6 7 although extracted rules low firing rate available data set extracted represent generalization capability trained network unseen data also extracted cover training testing data sets hence increase completeness extracted set rules examples rules r 1 r 4 table r 4 table 2 r 2 r 5 table 5 performance evaluation since iris breast cancer problems continuous input features fullre best technique used extract rules iriscont cancercont networks trained original continuous input features need prune trained network since fullre capable extracting rules mlps size section compare performance extracted rules iris breastcancer databases rules extracted neurorule c45rules algorithms 43 main reason choosing neurorule c45rules previously used extract rules two databases used fullre 42 moreover extract comprehensive rules relatively high correct classification rate reported authors neurorule 42 iris problem also compare set rules extracted fullre corresponding set rules extracted kt algorithm 11 analyzing extracted rules summarize computational complexity neu rorule c45rules ffl neurorule starting point neurorule algorithm 100 fully connected mlps generated training 100 networks input features binary discretized ie divided n intervals lower higher boundary thermometer coding used covert discretized values binary ones although discretization step helps simplify last step rule extraction process three major drawbacks increases number input nodes hence complexity required network 9architecture number input nodes generated network binary discretization step equal resulting number binary discretized intervals original input features network architecture generated neurorule 39 input nodes iris problem 91 input nodes breast cancer problem note corresponding networks used fullre 4 6 input nodes respectively iriscont cancercont increase complexity network architectures may degrade performance trained network increases training time complexity training algorithm increases complexity rule extraction procedure network pruned due complexity generated network architectures neurorule employs pruning procedure training phase pruning process continues network performance drops 95 original performance process applied 100 mlps rule extraction procedure starts choosing best one 100 pruned networks one highest performance neurorule extracts rules clustering remaining hidden nodes activation values checking input combination make hidden later output node active power neurorule lies pruning clustering techniques pruning phase neurorule removes input nodes example best pruned architecture iris problem network 4 input 2 hidden 3 output nodes breastcancer problem best pruned network 6 input 1 hidden 2 output nodes since resulting network architectures pruning step small rule extraction process easy could done visually iris breast cancer networks however pruning clustering processes lead substantial overheads ffl c45rules c45rules used authors neurorule extract rules iris breastcancer databases comparison reasons like id3 36 c45rules 20 generates decision tree rules based available input samples therefore complexity moderate performance rules generated c45rules highly affected noise level available data samples 11 51 comparison using iris data set rules extracted fullre techniques iris problem given table 3 rules extracted neurorule problem rule 1 3 19 iris setosa rule 2 3 49 4 16 iris versicolor rule 3 default rule iris virginica corresponding rules extracted c45rules rule 1 3 19 iris setosa rule 2 3 19 4 16 iris versicolor rule 3 4 16 iris virginica rule 4 default rule iris setosa corresponding rules extracted kt approach rule 1 3 27 iris setosa rule 2 3 50 3 27 4 16 4 07 iris versicolor rule 3 3 50 iris virginica rule 4 4 16 iris virginica rule 5 2 31 3 27 3 50 iris versicolor methodologies results note 1 completeness fullre kt extracted complete sets rules cover cases default rule required however default rule essential neurorule due pruning step c45 2 comprehensibility number rules fullre neurorule extract 3 rules kt extracts 5 rules c45 extracts 4 rules b number premises per rule except kt maximum number conditions per rule techniques 2 kt extracted rules maximum 4 conditions per rule 3 performance since iris simple classification problem techniques performed well fact able show setosa class linearly separable table 9 correct classification rate rule sets extracted different techniques fullre neurorule c45rules kt iris default rule 9733 9800 9600 9733 without default rule 9733 6467 9600 9700 breast cancer default rule 9619 9721 9721 na without default rule 9619 6310 9472 na two classes moreover rule extracted showed p etallength dominant input feature see row 1 2 table 9 4 certainty factors rules extracted fullre provide certainty factor attached extracted rule unlike approaches 52 comparison using breast cancer data set breast cancer database rules extracted fullre simple mlp architecture output nodes presented table 7 rules extracted neurorule best among pruned 100 mlp network architectures 6 inputs 1 hidden 2 output nodes 43 41 rule 1 rule 2 rule 3 rule 4 default rule malignant corresponding rules extracted c45rules 43 rule 1 rule 2 rule 3 x 2 50 malignant rule 4 x 6 90 malignant rule 5 x 1 70 malignant rule rule 7 default rule benign comparing three sets extracted rules observed 1 completeness neurorule extract rule class malignant neurorule dt c45 default rule rules extracted fullre 100 completeness measure hence need default rule default rules undesirable cannot provide symbolic interpretation decision none occurred 2 comprehensibility number rules number rules extracted fullre 5 dt c45 7 neurorule extracted 4 rules used default rule cover cases class malignant applied highly pruned network one hidden node b number premises per rule fullre maximum number conditions per extracted rule 2 rules extracted neurorule 4 conditions extracted dt maximum 4 conditions per rule thus rules extracted fullre comprehensible extracted two techniques 3 performance performance rules extracted three techniques high achieve low misclassification rate see row 3 table 9 default rules removed performance neurorule drops dramatically see row 4 table 9 authors neurorule reported choosing different trained network architectures extracted different rules one case two rule extracted one default rule another experiment neurorule extracts 3 rules one also default rule experiments achieved completeness measure approximately 95 however observe effect rules extracted fullre due changing initialization cancercont network used different input samples training testing indicates fullre rules quite stable long network trained reasonably well 4 certainty factors rules extracted fullre provide certainty factor attached extracted rule neurorule c45rules note kt used extract rules breast cancer problem table 9 compares classification rates obtained using rules extracted four technique neurorule c45rules kt iris breastcancer databases table presents qualitative comparison three techniques notable rule extraction techniques trained neural networks neurorule kt subset mofn note c45rules included comparative study presented table 10 conclusions 33 table 10 qualitative comparison different rule extraction techniques biore partialre fullre neurorule kt subset mofn provides cf may need default rule works 1binary inputs yes yes yes yes yes yes 2normalized inputs 3continuous inputs complexity low low med high med high additional overheads extracts rules based input samples decision trees trained networks like approaches 6 conclusions paper introduced three new rule extraction techniques suitability approach depends network type architecture complexity application nature inputs required transparency level three methods able extract meaningful rules well known iris database wisconsin breast cancer diagnosis database preexisting rules available extracted rules compare favorably reported implementation results proposed techniques less complex rules extracted efficient comprehensible powerful ordering extracted rules determined designing inference engine network provide direct information issue kbnn researchers reported aspect developed simple greedy rule evaluation procedure algorithm order rules extracted rule extraction algorithm goal maximizing performance minimizing error rates extracted rules available data also presented qualitative comparison key issues involved process extracting rules trained networks different approaches important mention obtaining possible combinations rules nphard feasible alternative often extract key rules cover concepts application conclusions 34 domain progress needed determining adequate set rules extracted another important issue needs investigated outputs rule extraction trained ann modules integrated provide robust decisions extracted rules used knowledge refinement truth maintenance domain knowledge r survey critique techniques extracting rules trained artificial neural networks rule extraction constrained error backpropagation mlp inserting extracting knowledge constrained error backpropagation networks logic minimization algorithm vlsi synthesis changing continuous attributes ordered discrete attributes fuzzy neural hybrid system using sampling queries extract rules trained neural networks learning relations noisy examples empirical comparison linus foil rule learning searching adapted nets neural networks computer intelligence structural adaptation generalization supervised feedforward networks learning class large finite state machines recurrent neural network learning extracting finite state automata secondorder recurrent neural networks hybrid neural network rulebased pattern recognition system capable selfmodification fuzzy modeling using fuzzy neural networks backpropagation algorithm rule extraction neural networks adaptive mixtures local experts 5 programs machine learning concept acquisition representational adjustment map method synthesis combinational logic circuits discretization numeric attributes chi2 feature selection discretization numeric attributes discretization ordinal attributes feature selection combining connectionist symbolic learning refine certainty factor rule bases cancer diagnosis via linear programming digital logic computer design connectionist scientist game rule extraction refinement neural network introduction probability statistics uci repository machine learning database extraction rules discretetime recurrent neural networks heuristically expanding knowledgebased neural network changing rules comprehensive approach theory refinement induction decision trees simplifying decision trees espressomv algorithms multiplevalued logic minimization medical diagnostic expert system based dpd model automated knowledge acquisition rules continuously valued attributes extracting rules pruned neural networks breast cancer diagnosis understanding neural networks via rule extraction symbolic representation neural networks controlling water reservoirs using hybrid intelligent architecture hybrid intelligent architecture application water reservoir control hybrid intelligent architecture refining input characterization domain knowledge generation methods fuzzy rules using neural networks planar lattice architecture dedec decision detection rule extraction neural networks dedec methodology extracting rules trained artificial neural networks extraction refined rules knowledgebased neural networks refinement approximate domain theories knowledgebased artificial neural network induction finitestate languages using secondorder recurrent networks system mathematics computer tr ctr marco muselli diego liberati binary rule generation via hamming clustering ieee transactions knowledge data engineering v14 n6 p12581268 november 2002 w wettayaprasitaffanb c lursinsap c h chu extracting linguistic quantitative rules supervised neural networks international journal knowledgebased intelligent engineering systems v8 n3 p161170 august 2004 sankar k pal sushmita mitra pabitra mitra roughfuzzy mlp modular evolution rule generation evaluation ieee transactions knowledge data engineering v15 n1 p1425 january zhihua zhou rule extraction using neural networks neural networks journal computer science technology v19 n2 p249253 march 2004 j l castro l floreshidalgo c j mantas j puche extraction fuzzy rules support vector machines fuzzy sets systems v158 n18 p20572077 september 2007 zan huang hsinchun chen chiajung hsu wunhwa chen soushan wu credit rating analysis support vector machines neural networks market comparative study decision support systems v37 n4 p543558 september 2004 alex freitas understanding crucial role attributeinteraction data mining artificial intelligence review v16 n3 p177199 november 2001