adaptive sampling methods scaling knowledge discovery algorithms scalability key requirement kdd data mining algorithm one biggest research challenges develop methods allow use large amounts data one possible approach dealing huge amounts data take random sample data mining since many data mining applications approximate answers acceptable however argued several researchers random sampling difficult use due difficulty determining appropriate sample size paper take sequential sampling approach solving difficulty propose adaptive sampling method solves general problem covering many actual problems arising applications discovery science algorithm following method obtains examples sequentially online fashion determines obtained examples whether already seen large enough number examples thus sample size fixed priorisemi instead iadaptively depends situation due adaptiveness worst case situation fortunately happens many practical applications solve problem number examples much smaller required worst case prove correctness method estimates efficiency theoretically illustrating usefulness consider one concrete task requiring sampling provide algorithm based method show efficiency experimentally b introduction scalability key requirement knowledge discovery data mining algorithm previously observed many well known machine learning algorithms scale well therefore one biggest research challenges develop new methods allow use machine learning techniques large amount data facing problem huge input data set typically two possible ways address one way could redesign known algorithms almost maintaining performance run eciently much larger input data sets second possible approach random sampling data mining applications approximate answers acceptable thus could take random sample instance space data mining however argued researchers see instance 17 approach less recommendable due diculty determining appropriate sample size needed paper advocate second approach reducing dimensionality data random sampling propose general problem covers many data mining problems general sampling algorithm solving typical task knowledge discovery data mining nd rule law explaining huge set examples well often case size possible candidates rules still manageable task simply select rule among candidates certain utility dataset problem discuss paper call general rule selection specically given input data set x examples set h rules utility function u measures usefulness rule x problem nd nearly best rule h precisely h satisfying uh 1 uh h best rule given accuracy parameter though simple problem covers several crucial topics knowledge discovery data mining shown section 4 would like solve general rule selection problem random sampling statistical point view problem solved taking rst random sample domain x selecting h 2 h largest uh choose enough number examples x randomly guarantee selected h nearly best within certain condence level refer simple method batch sampling approach one important issues random sampling choosing proper sample size ie number examples sampling method must take account problem parameters accuracy parameter condence parameter determine appropriate sample size needed solve desired problem non theoretically sound sampling methods like taking xed fraction data set widely used empirical machine learning research appropriate data mining amount data available huge moreover methods take account accuracy condence considerations therefore discuss widely used theoretically sound tools determine appropriate sample size given accuracy condence parameters called concentration bounds large deviation bounds like cherno hoeding bounds commonly used theoretical learning research see 7 examples well many branches computer science 13 examples sample size calculated concentration bounds data mining problems see eg8 15 18 bounds usually allow us calculate sample size needed many situations usually case resulting sample size immense obtain reasonable good accuracy condence moreover situations apply bounds need assume knowledge certain problem parameters unknown practical applications important notice batch sampling approach sample size calculated priori thus must big enough work well situations might encounter words sample size provided theoretical bounds batch sampling approach worst case sample size thus overestimated situations one main reasons researchers found practice bounds overestimating necessary sample size many non worstcase situations see eg discussion toivonen sampling association rules discovery 15 overcoming problem propose paper sampling online sequential fashion instead batch algorithm obtains examples sequentially one one determines obtained examples whether already received enough examples issuing currently best rule nearly best high condence thus x sample size priori instead sample size depend adaptively situation hand due adaptiveness worst case situation fortunately happens practical cases may able use signicantly much less examples worst case following approach propose general algorithm adaselect solving general rule selection problem provides us ecient tools many knowledge discovery applications general algorithm evolves preliminary works online adaptive sampling specic problems related model selection association rules done 3 4 idea adaptive sampling quite natural various methods implementing idea proposed literature statistics particular methods studied depth name sequential test sequential analysis 16 however main goal test statistical hypotheses thus even though methods applicable instances general rule selection problem far authors know method reliable ecient adaselect general rule selection problem using simple example explain next section dierences advantages adaselect related algorithms including one following batch sampling approach paper organized follows next section explain still intuitive level advantage algorithm random sampling methods section 3 present problem algorithm prove theorems concerning reliability complexity section 4 describe several applications algorithm particular data mining problems improvements might useful study particular situations described section 5 conclude section 6 highlighting future work related work idea determining number necessary examples adaptively thereby reducing average number examples quite natural methods implementing idea proposed literature statistics methods called sequential test sequential analysis since wald published pioneer 1 textbook sequential analysis 16 many researchers studied sequential analysis methods depth main goal however test statistical hypotheses thus even though methods applicable purpose evaluating andor comparing value given function given huge dataset method far authors know reliable ecient general algorithm adaselect illustrate dierence adaselect methods let us consider following simple problem need simple utility function problem test whether probability p given condition c holds 1 according walds textbook idea sequential test procedure goes back hf dodge hg romig 2 database x 12 c held exactly 12 transactions x would check database notice thus let us suppose intuitively condition holds many transactions instance 90 10 database sampling small number transactions enough gure answer positive negative hand closer 50 transactions condition holds dicult test sampling correct answer use adaselect solve problem follows keep two functions h 1 h 2 map transactions 01 values given one transaction x h 1 x outputs 1 transaction x satises condition c otherwise h 1 x outputs 0 hand h 2 negation h 1 utility function use uh dierence positive negative half proportion transactions purpose algorithm needs detect whether uh 1 positive case negative thus need x number less 1 run algorithm setting would able determine high condence whether condition c occurs 50 transactions x notice application adaselect always solves problem matter small far 0 hand sample complexity number necessary examples depends hence algorithm might require lot transactions small specically number transactions need sampled database o1 ln1 ignoring factor depending condence parameter therefore closer value 0 thus p 12 larger number examples needed batch sampling approach also applicable problem using appropriate large deviation bound eg cherno bound determine number examples sucient detect high condence whether condition holds x notice choose appropriate since high condence algorithm guaranteed hand number examples needed o1 therefore use smaller underestimate sample complexity becomes large words batch sampling approach always big enough cover possible values bigger thus depends since worst case might equal hand adaselect depends case hand thus sample size depends moreover practical applications might unrealistic assume assume knowledge lower bound quantity want estimate clearly see advantage algorithm simple batch sampling sophisticated way solve problem could following instead using cherno bound provides estimate within multiplicative error probability estimated thus adequate purposes cannot used unless lower bound known use hoeding bound also usually referred additive version cherno bound sample size provided bound independent probability estimated thus need assume knowledge lower bound hand estimate guaranteed within additive error value estimated therefore general impossible use one single sample guarantee problem solved instead following suppose run batch sampling calculating appropriate size hoeding bound obtain estimate p 1 p high probability p 1 satisfy 14 thus p 1 greater 34 determine high condence p greater 12 hand p 1 less 14 determine p less 12 however p 1 range 14 p 1 34 cannot conclude anything whether p 12 last situation occur choose small enough sample complexity becomes large underestimate one way solve problem iterate execution algorithm smaller accuracy parameter obtained estimate safe range case example run algorithm continue process obtained estimate ith iteration range 12 2 i1 122 i1 suppose algorithm always gives close estimate p routine calculations show algorithm terminates log1 iterations approach uses o1 2 examples obtaining close estimate ignoring factor depending condence parameter every step hence altogether need o1 gives order adaselect fact roughly speaking idea adaselect hand adaselect iteration phase naturally incorporated thus adaselect better sample complexity furthermore easy generalize technique estimating andor comparing general function values method sampling described referred classical statistics multiple sampling 1 one earlier works sequential sampling methods 16 recent work adaptive sampling comes database community due lipton etal 9 10 following problem discussed 2 given database query databases instance selection joint want estimate query size number transactions associated query database certain error condence level designed algorithms task refer algorithms adaptive estimator specically given database x condition c accuracy parameter condence parameter adaptive estimator estimates random sampling probability p transactions multiplicative error probability least 1 algorithm yields estimate p p 1 p p 1 achieving task adaptive estimator collects examples database sequentially random checking whether collected examples sucient terminating execution current estimate number examples used algorithm depends correctness algorithm proved using large deviation tools particular using central limit theorem thus words adaptive estimator sequential version multiplicative cherno bound however important notice adaptive estimator works estimating total probability example provides us multiplicative estimate p approach quite similar adaptive estimator algorithm adaselect propose also collects examples given database sequentially random checking whether collected examples sucient furthermore also use one large deviation tools order guarantee correctness ie reliability output algorithm thus one may think supercially adaselect diers adaptive estimator point enable us estimate general value ie value user dened utility function fact could estimate advantage instead p example note dierence essential example problem adaptive estimator exactly problem batch sampling method hoeding bound thus cannot used general must used iteratively described also remark choice utility function u important use uh problem execution adaselect becomes essentially adaptive estimator technically due generalization stopping condition sampling algorithm monotone requires careful probability analysis 2 order keep explanation intuitive level explain typical algorithm omit details dierences algorithms see papers cited details algorithm adaselect presented paper evolves previous work hypothesis selection 3 sampling association rules 4 fact two cases treated simple cases general utility function introduced paper algorithm present paper greatly generalizes adaptive algorithms provided use attack several problems 3 adaptive sampling algorithm section formally describe problem would like solve present algorithm investigate reliability complexity begin introducing notation let large set examples let nite large set n functions thought function evaluated example x producing real value result intuitively h 2 h corresponds rule law explaining examples call rule measures goodness rule x following identify h corresponding rule usually call h rule example task predict particular boolean feature example x terms features could set feature predicted correctly h predicted incorrectly also assume xed realvalued nonnegative utility function uh measuring global goodness rule corresponding h set x specically x uh dened f function ir 7 ir avg denotes taking arithmetic average ie i2i jij uh simply dened uh x section 4 describe several applications framework u instantiated specic functions meaning become clear ready state problem general rule selection given x h 0 1 goal find h 2 h uh 1 rule maximum value uh remark 1 accuracy parameter intuitively task nd h 2 h whose utility reasonably high compared maximum uh accuracy uh uh specied parameter certainly closer uh uh better however depending choice u accuracy essential cases may able use large advantage algorithm becomes clear cases recall example discussed section 2 see also discussion end section remark 2 condence parameter want achieve goal random sampling ie using examples randomly selected x must chance selecting bad examples make algorithm yield unsatisfactory h 2 h thus introduce one parameter specifying condence require probability error bounded remark 3 condition h order simplify discussion assume following value hx hx 0 constant 0 denote constant remark 4 condition u goal make sense uh negative thus assume uh positive also order sort random sampling work cannot happen single example changes drastically value u otherwise would forced look examples x even approximate value uh thus require function f denes u smooth formally f need clipschitz constant c 0 dened c denote lipschitz constant f denition 1 function f ir 7 ir clipschitz x holds jf x f yj c jx yj lipschitz constant f minimum c 0 f clipschitz observe lipschitz functions continuous dierentiable functions bounded derivative lipschitz fact f dierentiable mean value theorem lipschitz constant f max x jf 0 xj see section 4 natural functions used applications describe satisfy condition c also note conditions cd uh cd h 2 h remark 5 minimization problem situations primary goal might maximize utility function data minimize penalty function p want nd h p h 1 solve general hypothesis selection problem algorithm analysis similar one present end remarks one trivial way solve problem evaluating functions h h examples x x hence computing uh h nding h maximizes value obviously x large method might extremely inecient want solve task much eciently random sampling want look fairly small randomly drawn subset x nd h maximizes uh still sure probability 1 h output satises uh 1 uh one easily think following simple batch sampling approach obtain random sample x priori xed size output function h highest utility several statistical bounds calculate appropriate number examples paper choose hoeding bound widely used computer science see eg 7 13 one use reasonable bound choosing one use determined considering reliability eciency reason choose hoeding bound basically assumption necessary 3 using bound estimate error probability calculate sample size hand bounds example central limit theorem might appropriate practical situations since behaves better although slighlty less reliable easy modify algorithm analyze alternative bounds roughly sample size error value hoeding bound provides us upper bound probability estimate calculated randomly drawn sample apart real value thus using bound determine sample size guarantees batch sampling yields rule satisfying requirement problem probability least 1 batch sampling solves problem eciency satisfactory choose sample size worst case overcoming ineciency take sequential sampling approach instead statically decide sample size new algorithm obtains examples sequentially one one stops according condition based number examples seen values functions examples seen far algorithm adapts situation hand thus worst case algorithm would able realize stop 3 assumption obtain samples independently obtained distribution natural assumption holds problems considered algorithm repeat x randomly drawn example x 1 constant close 1 see proof theorem 3 output h 2 h largest uh figure 1 pseudocode online sampling adaselect figure 1 shows pseudocode algorithm propose called adaselect solving general rule selection problem provide two theorems discussing reliability complexity algorithm adaselect proofs make use following lemma lemma 2 let x set size obtained independently drawing elements x random h 2 h 0 proof let g value avgy hx x 2 x g random variable avgy observe using fact f clipschitz prf jg egj c g g average independent random variables range bounded hoeding bound probability less 2 exp claimed end proof rst prove reliability algorithm adaselect theorem 3 probability 1 adaselectx h outputs function h 2 h uh 1 uh proof xed 0 dene h g show function output adaselectx h h bad probability less want bound following error probability p error regard one repeatloop iteration basic step algorithm measure algorithms running time terms number repeatloop iterations use denote number executed repeatloop iterations particular let 0 integer following inequalities hold note strictly decreasing function hence 0 uniquely determined see algorithm terminates 0 th step ie 0 th repeatloop iteration high probability deriving bound consider following two cases case 1 h 2 h bad satises stopping condition repeatloop 0 th step case satisfy stopping condition rst 0 steps clearly whenever algorithm makes error one cases certainly occurs thus bounding probability either case 1 case 2 occurs bound error probability p error algorithm first bound probability case 1 let h bad rule h bad largest utility case 1 holds g prf h satises stopping condition adaselect tth step g stopping condition adaselect tth step g g 1 0 p 1 bounded follows used fact 0 uh 2 uh 1 uh bad lemma 2 thus estimate case 1 holds g 2n exp next consider case 2 clearly case 2 implies uh 2 1 thus probability case 2 bounded following bounding p 2 rst estimate 0 let b using assumption 0 1 uh 2 fact uh cd hence may consider b 1 therefore b p 2 bounded follows 2nt summary b probability either case 1 case holds bounded end proof next estimate running time algorithm regard one repeat loop iteration basic step algorithm measure algorithms running time terms number repeatloop iterations exactly number required examples proof already showed probability algorithm terminate within 0 steps case 2 occurs thus following theorem immediate proof theorem 4 probability 1 adaselectx h halts within 0 steps words adaselectx h needs 0 examples 0 largest integer 0 let us express 0 convenient form recall 1 since approximately approximate x ln x x ln cd let us discuss meaning formula since n within log func tion uence complexity small words handle relative large number rules require high condence without increasing much sample size needed main terms formula 1 cduh recall uh cd h 2 h hence 1 cduh least 1 depending choice u cases may assume cduh large cases may need small thus 1 large adaselect performs well latter case specically adaselect shows advantage u chosen 1 relatively large sucient 2 though cduh bounded general large lucky cases happen often bad cases see section 4 clever choice u might allow us choose large overall number examples might large 4 examples applications section describe two domains instance algorithm used solve particular problem two domains studied model hypothesis selection induction decision trees due space limitations cannot describe possible applications problems like association rules mining already studied 4 subgroup discovery batch sampling methods based large deviation methods already proposed speed computational process see instance 15and 18 algorithm also applied order instantiate framework particular problem need specify meaning functions class h utility function u conditions conditions u satised following problems model hypothesis selection typical application framework class functions h seen xed set hypotheses models set could obtained instance dierent runs rival learning algorithms learning algorithm dierent input parameters architecture could contain several dierent memorybased hypothesis xed certain restricted model space consequence design decision notice later case case algorithms try select hypothesis simple small class hypotheses instance decision stumps amplify precision using voting methods like boosting 5 thus rest discussion assume class h xed nite tractable size utility function capture criterion goodness hypothesis typical one prediction error order keep discussion simple level assume hypotheses binary goodness criteria prediction error case one might naively set utility function identity function uh notice however worst possible prediction error 12 answering ipping random coin hence precisely speaking goodness hypothesis h measure advantage random guessing thus want algorithm output hypothesis whose advantage random guessing close best possible advantage class fact setting ts particularly well voting methods like adaboost 5 typically one needs obtain hypothesis better random guessing every step purpose set uh particular selecting hypothesis voting methods choice may important set constant smaller 1 similar setting previously studied maron moore 11 proposed algorithm called hoeding races accelerate model selection idea discard hypotheses clearly going among best ones criteria discarding based inverting hoeding bound refer reader paper details clearly add feature algorithm without compromising reliability complexity possibly accelerating total running time notice combining hoeding races algorithm reduce much number examples needed since depends logarithmically number models n might greatly reduce computation time depends linearly n follow research moore lee 12 developed ecient version hoeding races based bayesian approach assumption models accuracies dataset normally distributed furthermore also introduce modication discarding models almost indistinguishable others thus allowing race output model best class notice also accuracy parameter framework allows one use complicated utility functions could incorporate size smoothness considerations together prediction error case realvalued functions could also consider instance mean square error decision tree induction algorithms decision tree induction typically work choosing test root certain node function class subsequently root subtree exploring training data choosing one best according certain splitting criteria like entropy measure gini index large dataset could possible reduce training time choosing split based subsample whole data musick catlett russell 14 described algorithm implements idea chooses sample based dicult decision node typically algorithm uses small sample root node enlarges progressively tree grows propose alternative way select appropriate sample size needed every node using instantiation algorithm describe following follow notation 6 simplicity assume want construct decision tree approximates unknown function using training data set x furthermore assume class node functions f xed priori nite small instance input variables negations 4 fact class commonly used standard software packages c45 cart finally denote g 0 0 1 splitting criteria used topdown decision tree induction algorithm typical example function binary entropy let decision tree whose internal nodes labeled functions f nodes labeled values f0 1g let l leaf want make new internal node substituting leaf l function h f goal choose function h value g original tree sum values g every leaf weighted probability reaching leaf decreases substituting l h 4 assumption makes sense attributes discrete discretized priori labeling new labels l 0 l 1 according majority class instances reaching l 0 l 1 respectively formally let set instances reach l x h f denote p 1 probability fx 1 p 1 probability hx 1 p 11 probability hx fx 1 p 01 probability hx 0 fx 1 notice probabilities taken distribution induced initial distribution training set typically assumed uniform distribution thus given leaf l given goal nd function h maximum value l h l denoted h function mentioned large dataset ecient way attack problem following given tree leaf l take sample x output function h highest value l h sample sample size chosen appropriately value l h h function output algorithm close l h apply algorithm use appropriate amount data follows use f h l h uh x 5 remains determine constant c lipschitz condition u notice u addition three g functions dierent inputs thus g lipschitz certain constant c u state obtain lipschitz constant g leave reader calculation appropriate constant whole function u gq gini index derivative g 0 4 thus mean value theorem lipschitz constant 4 gq binary entropy hq improved splitting criterion presented 6 derivatives bounded 0 1 range therefore cannot xed constant works possible values however suppose ignore input values close 0 1 consider instance interval 005 095 functions lipschitz constant 5 interval notice inputs function g estimated sample thus assumption makes sense since admit certain error values thus run algorithm inputs h u discussed desired accuracy condence level algorithm outputs probability larger 1 node function h l h 1 crucial point choice 5 precisely speaking l h denition uh x easy see algorithm also works situation u moreover according results 6 h l h suce reduce overall error whole tree adding new internal node thus x 12 informally described another possible application algorithm problem decision tree induction problem nding good cutting point continuous attribute order discretize case every function h represent possible cutting point even though total number cutting points innite notice need consider cutting points appears dataset set considered thus k k size x although x might big notice bound given theorem 4 depends logarithmically size h thus particular application size x gives us upper bound size h furthermore notice running algorithm need functions h priori every time new example reveals new possible cutting point add new function class calculate utility function examples seen far utility function case splitting criterion used compare splitting functions described thus algorithm problem would choose almost optimal respect splitting criterion cutting point particular continuous attribute done use attribute discrete attribute decide attribute used label node building 5 improvements section describe improvements incorporated main algorithm adaselect completely rigorous justication others seem intuitively clear harder capture form theorem use multiplicative cherno bounds instead additive hoeding bounds developed algorithm adaselect using hoeding bound deviation average value appears additively cases also possible use chernos bound deviation appears multiplicatively proof slightly involved applying cherno seems require previous knowledge expected value utility function precisely trying estimate careful also adaptive application lets us circumvent problem utility functions particular done case f identity function plus minus constant 0 1 problem model hypothesis selection case using cherno clear advantage case small probabilities precisely bound number examples o1 dependence 1uh linear instead quadratic although seems asymptotically better hidden constant bigger precisely 12 thus happens better uh 16 considering variance cases may true values hx always 0 moderate large probability versions hoeding called berstein bounds instead assumption incorporate variance hx bound estimates variances known plugging adaselect may give quite advantage using worstcase constant adapting local lipschitz constant bound depends constant c global lipschitz constant f however many regions much smaller constant may enough bound growth rate f consider example entropy style functions discussed decisiontree induction saw appropriate choice c around 4 5 probabilities near 0 1 however probabilities near 12 quite common case functions almost ie c much closer 0 suce practice one probably use step worstcase lipschitz constant within uncertainty interval uh ie uh dicult give rigorous analysis new version intuitively clear may result signicant improvement essentially aecting reliability 6 concluding remarks presented new methodology sampling keeping theoretical guarantees previous ones applicable wider setting moreover likely useful practice key point rather deciding priori sample size obtain batch algorithm performs sampling sequentially maintains stopping condition depends problem hand future work verify advantage approach experimentally although theorem provided paper suggests algorithm might eciently applicable several domains still plenty work test whether assertion true one hand algorithm take prot non worst case situation therefore likely outperform usual batch sampling approach hand asymptotics alone guarantee practicality since huge constant might spoil advantage sampling using data even ecient sampling algorithm used tested second point preliminary work 3 4 using synthetic data could test wide range values results promising sample size used method greatly outperform one used batch approach moreover sample size many cases reasonable small suggested reducing data size sampling algorithm might allows us improve overall running time experimental results 10 although dierent context also encouraging case remains experiments using real world data test real practicality algorithm furthermore pointed several improvements one might perform applying algorithm particular situation due generality approach every application deserves individual deeper study see possible enhancements one particular setting acknowledgments would like thank heikki mannila pointing us work sampling database query estimation encouraging us follow previous research adaptive sampling would also like thank chris watkins telling us hoeding races pedro domingos pointing us several related machine learning papers r multiple sampling constant probability method sampling inspection ricard gavald ricard gavald decisiontheoretic generalization online learning application boosting boosting ability topdown decision tree learning algorithms introduction computational learning theory power sampling knowledge discovery je randomized algorithms decision theoretic subsampling induction large databases sampling large databases association rules sequential analysis bala iyer je algorithm multirelational discovery subgroups tr ctr geoff hulten pedro domingos mining complex models arbitrarily large databases constant time proceedings eighth acm sigkdd international conference knowledge discovery data mining july 2326 2002 edmonton alberta canada szymon jaroszewicz tobias scheffer fast discovery unexpected patterns data relative bayesian network proceeding eleventh acm sigkdd international conference knowledge discovery data mining august 2124 2005 chicago illinois usa osamu watanabe sequential sampling techniques algorithmic learning theory theoretical computer science v348 n1 p314 2 december 2005 huan liu hiroshi motoda issues instance selection data mining knowledge discovery v6 n2 p115130 april 2002 jaekyung yang sigurdur olafsson optimizationbased feature selection adaptive instance sampling computers operations research v33 n11 p30883106 november 2006 pierrealain laur richard nock jeanemile symphor pascal poncelet mining evolving data streams frequent patterns pattern recognition v40 n2 p492503 february 2007