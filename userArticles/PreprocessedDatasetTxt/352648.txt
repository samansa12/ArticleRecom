comparison prediction accuracy complexity training time thirtythree old new classification algorithms twentytwo decision tree nine statistical two neural network algorithms compared thirtytwo datasets terms classification accuracy training time case trees number leaves classification accuracy measured mean error rate mean rank error rate criteria place statistical splinebased algorithm called polyclsss top although statistically significantly different twenty algorithms another statistical algorithm logistic regression second respect two accuracy criteria accurate decision tree algorithm quest linear splits ranks fourth fifth respectively although splinebased statistical algorithms tend good accuracy also require relatively long training times polyclass example third last terms median training time often requires hours training compared seconds algorithms quest logistic regression algorithms substantially faster among decision tree algorithms univariate splits c45 indcart quest best combinations error rate speed c45 tends produce trees twice many leaves indcart quest b introduction much current research machine learning statistics communities algorithms decision tree classifiers often emphasis accuracy algorithms one study called statlog project michie spiegelhalter taylor 1994 compares accuracy several decision tree algorithms nondecision tree algorithms large number datasets studies smaller scale include brodley utgoff 1992 brown corruble pittard 1993 curram mingers 1994 shavlik mooney towell 1991 recently comprehensibility tree structures received attention comprehensibility typically decreases increase tree size complexity two trees employ kind tests prediction accuracy one fewer leaves usually preferred breslow aha 1997 survey methods tree simplification improve comprehensibility third criterion largely ignored relative training time algorithms statlog project finds algorithm uniformly accurate datasets studied instead many algorithms possess comparable accuracy algorithms excessive training times may undesirable hand 1997 purpose paper extend results statlog project following ways 1 addition classification accuracy size trees compare training times algorithms although training time depends somewhat imple mentation turns large differences times seconds versus days differences cannot attributed implementation alone 2 include decision tree algorithms included statlog project namely splus tree clark pregibon 1993 holte maass 1995 oc1 murthy kasif salzberg 1994 lmdt brodley utgoff 1995 quest loh shih 1997 3 also include several newest splinebased statistical algorithms classification accuracy may used benchmarks comparison algorithms future 4 study effect adding independent noise attributes classification accuracy appropriate tree size algorithm turns except possibly three algorithms others adapt noise quite well 5 examine scalability promising algorithms sample size increased experiment compares twentytwo decision tree algorithms nine classical modern statistical algorithms two neural network algorithms many datasets taken university california irvine uci repository machine learning databases merz murphy 1996 fourteen datasets reallife domains two artificially constructed five datasets used statlog project increase probability finding statistically significant differences algorithms number datasets doubled addition noise attributes resulting total number datasets thirtytwo section 2 briefly describes algorithms section 3 gives background datasets section 4 explains experimental setup used study section 5 analyzes results issue scalability studied section 6 conclusions recommendations given section 7 2 algorithms short description algorithm given details may found cited references algorithm requires class prior probabilities made proportional training sample sizes 21 trees rules cart use version cart breiman friedman olshen stone 1984 implemented cart style ind package buntine caru ana 1992 gini index diversity splitting criterion trees based 0se 1se pruning rules denoted ic0 ic1 respec tively software obtained httpicwwwarcnasagovicprojectsbayesgroupindindprogramhtml splus tree variant cart algorithm written language becker chambers wilks 1988 described clark pregibon 1993 employs deviance splitting criterion best tree chosen tenfold crossvalidation pruning performed ptree function treefix library venables ripley 1997 statlib archive httplibstatcmuedus 0se 1se trees denoted st0 st1 respectively c45 use release 8 quinlan 1993 quinlan 1996 default settings including pruning httpwwwcseunsweduauquinlan tree constructed c45 rule induction program used produce set rules trees denoted c4t rules c4r fact fast classification tree algorithm described loh vanichse takul 1988 employs statistical tests select attribute splitting node uses discriminant analysis find split point size tree determined set stopping rules trees based univariate splits splits single attribute denoted ftu based linear combination splits splits linear functions attributes denoted ftl fortran 77 program obtained httpwwwstatwisceduloh quest new classification tree algorithm described loh shih 1997 quest used univariate linear combination splits unique feature attribute selection method negligible bias attributes uninformative respect class attribute approximately chance selected split node tenfold crossvalidation used prune trees univariate 0se 1se trees denoted qu0 qu1 respectively corresponding trees linear combination splits denoted ql0 ql1 respectively results paper based version 1710 program software obtained httpwwwstatwiscedulohquesthtml ind due buntine 1992 use version 21 default settings ind comes several standard predefined styles compare four bayesian styles paper bayes bayes opt mml mml opt denoted ib ibo im imo respectively opt methods extend nonopt methods growing several different trees storing compact graph struc ture although time memory intensive opt styles increase classification accuracy oc1 algorithm described murthy et al 1994 use version 3 httpwwwcsjhuedusalzbergannounceoc1html compare three styles first one denoted ocm default uses mixture univariate linear combination splits second one option denoted ocu uses univariate splits third one option denoted ocl uses linear combination splits options kept default values lmdt algorithm described brodley utgoff 1995 constructs decision tree based multivariate tests linear combinations attributes tree denoted lmt use default values software httpyakeecnpurdueedubrodleysoftwarelmdthtml cal5 fraunhofer society institute information data processing germany muller wysotzki 1994 muller wysotzki 1997 use version 2 cal5 designed specifically numericalvalued attributes however procedure handle categorical attributes mixed attributes numerical categorical included study optimize two parameters control tree construction predefined threshold significance level ff randomly split training set two parts stratified classes twothirds used construct tree onethird used validation set choose optimal parameter configura tion employ cshell program comes cal5 package choose best parameters varying ff 010 090 095 steps 005 best combination values minimize error rate validation set chosen tree constructed records training set using chosen parameter values denoted cal t1 onelevel decision tree classifies examples basis one split single attribute holte 1993 split categorical attribute 5with b categories produce b reserved missing attribute values hand split continuous attribute yield j leaves j number classes one leaf reserved missing values software obtained httpwwwcsiuottawacaholtelearningothersiteshtml 22 statistical algorithms lda linear discriminant analysis classical statistical method models instances within class normally distributed common covariance matrix yields linear discriminant functions qda quadratic discriminant analysis also models class distributions normal estimates covariance matrix corresponding sample covariance matrix result discriminant functions quadratic details lda qda found many statistics textbooks eg johnson 1992 use sas proc discrim sas institute inc 1990 implementation lda qda default settings nn sas proc discrim implementation nearest neighbor method pooled covariance matrix used compute mahalanobis distances log logistic discriminant analysis results obtained poly tomous logistic regression see eg agresti 1990 fortran 90 routine written first author httpwwwstatwiscedulimtlogdiscr fda flexible discriminant analysis hastie tibshirani buja 1994 generalization linear discriminant analysis casts classification problem one involving regression mars friedman 1991 nonparametric regression procedure studied use splus function fda mda library statlib archive two models used additive model degree1 denoted fm1 model containing firstorder interactions degree2 penalty3 denoted fm2 pda form penalized lda hastie buja tibshirani 1995 designed situations many highly correlated attributes classification problem cast penalized regression framework via optimal scoring pda implemented splus using function fda methodgenridge mda stands mixture discriminant analysis hastie tibshirani 1996 fits gaussian mixture density functions class produce classifier mda implemented splus using library mda pol polyclass algorithm kooperberg bose stone 1997 fits polytomous logistic regression model using linear splines tensor products provides estimates conditional class probabilities used predict class labels pol implemented splus using function polyfit polyclass library statlib archive model selection done tenfold crossvalidation 23 neural networks lvq use learning vector quantization algorithm splus class library venables ripley 1997 statlib archive details algorithm may found kohonen 1995 ten percent training set used initialize algorithm using function lvqinit training carried optimized learning rate function olvq1 fast robust lvq algorithm additional finetuning learning performed function lvq1 number iterations ten times size training set olvq1 lvq1 use default values 03 003 ff learning rate parameter olvq1 lvq1 respectively rbf radial basis function network implemented sas tnn3sas macro sarle 1994 feedforward neural networks httpwwwsascom network architecture specified archrbf argument study construct network one hidden layer number hidden units chosen 20 total number input output units 25 5 hidden units dna dna datasets 10 5 hidden units tae tae datasets memory storage limitations although macro perform model selection choose optimal number hidden units utilize capability would taken long datasets see table 6 therefore results reported algorithm regarded lower bounds performance hidden layer fully connected input output layers direct connection input output layers output layer class represented one unit taking value 1 particular category 0 otherwise except last one reference category avoid local optima ten preliminary trainings conducted best estimates used subsequent training details radial basis function network found bishop 1995 ripley 1996 3 datasets briefly describe sixteen datasets used study well modifications made experiment fourteen real domains two artificially created thirteen uci breast cancer bcw one breast cancer databases uci collected university wisconsin w h wolberg problem predict whether tissue sample taken patients breast malignant benign two classes nine numerical attributes 699 observations sixteen instances contain single missing attribute value removed analysis results therefore based 683 records error rates estimated using tenfold crossvalidation decision tree analysis subset data using fact algorithm reported wolberg tanner loh 1987 wolberg tanner loh 1988 wolberg tanner loh 1989 dataset also analyzed linear programming methods mangasarian wolberg 1990 contraceptive method choice cmc data taken 1987 national indonesia contraceptive prevalence survey samples married women either pregnant know pregnant time interview problem predict current contraceptive method choice use longterm methods shortterm methods woman based demographic socioeconomic characteristics lerman molyneaux pangemanan iswarati 1991 three classes two numerical attributes seven categorical attributes 1473 records error rates estimated using tenfold crossvalidation data obtained httpwwwstatwiscedupstatftppublohtreeprogsdatasets statlog dna dna uci dataset molecular biology used project splice junctions points dna sequence su perfluous dna removed process protein creation higher organisms problem recognize given sequence dna boundaries exons parts dna sequence retained splicing introns parts dna sequence spliced three classes sixty categorical attributes four categories sixty categorical attributes represent window sixty nucleotides one four categories middle point window classified one exonintron boundaries intronexon boundaries neither 3186 examples database divided randomly training set size 2000 test set size 1186 error rates estimated test set statlog heart disease hea uci dataset cleveland clinic foundation courtesy r detrano problem concerns prediction presence absence heart disease given results various medical tests carried patient two classes seven numerical attributes six categorical attributes 270 records statlog project employed unequal misclassification costs use equal costs algorithms allow unequal costs error rates estimated using tenfold crossvalidation boston housing bos uci dataset gives housing values boston suburbs harrison rubinfeld 1978 three classes twelve numerical attributes one binary attribute 506 records following loh vanichse takul 1988 classes created attribute median value owneroccupied homes follows class otherwise error rates estimated using tenfold crossvalidation led display led artificial domain described breiman et al 1984 contains seven boolean attributes representing seven lightemitting diodes ten classes set decimal digits attribute value either zero one according whether corresponding light digit attribute value ten percent probability value inverted class attribute integer zero nine inclusive c program uci used generate 2000 records training set 4000 records test set error rates estimated test set bupa liver disorders bld uci dataset contributed r forsyth problem predict whether male patient liver disorder based blood tests alcohol consumption two classes six numerical attributes 345 records error rates estimated using tenfold crossvalidation pima indian diabetes pid uci dataset contributed v sigillito patients dataset females least twentyone years old pima indian heritage living near phoenix arizona usa problem predict whether patient would test positive diabetes given number physiological measurements medical test results two classes seven numerical attributes 532 records original dataset consists 768 records eight numerical attributes however many attributes notably serum sulin contain zero values physically impossible remove serum insulin records impossible values attributes error rates estimated using tenfold cross validation statlog satellite image sat uci dataset gives multispectral values pixels within 3 theta 3 neighborhoods satellite image classification associated central pixel neighborhood aim predict classification given multispectral values six classes thirtysix numerical attributes training set consists 4435 records test set consists 2000 records error rates estimated test set image segmentation seg uci dataset used statlog project samples database seven outdoor images images handsegmented create classification every pixel one brickface sky foliage cement window path grass seven classes nineteen numerical attributes 2310 records dataset error rates estimated using tenfold crossvalidation algorithm t1 could handle dataset without modification program requires large amount memory therefore t1 algorithms discretize attribute except attributes 3 4 5 one hundred categories attitude towards smoking restrictions smo survey dataset bull 1994 obtained httplibstatcmuedudatasetscsb problem predict attitude toward restrictions smoking workplace prohibited restricted unrestricted based bylawrelated smokingrelated sociodemographic covariates three classes three numerical attributes five categorical attributes divide original dataset training set size 1855 test set size 1000 error rates estimated test set thyroid disease thy uci anntraindatacontributed r werner problem determine whether patient hyperthyroid three classes normal hyperfunction subnormal functioning six numerical attributes fifteen binary attributes training set consists 3772 records test set 3428 records error rates estimated test set statlog vehicle silhouette veh uci dataset originated turing institute glasgow scotland problem classify given silhouette one four types vehicle using set features extracted silhouette vehicle viewed many angles four model vehicles double decker bus chevrolet van saab 9000 opel manta 400 four classes eighteen numerical attributes 846 records error rates estimated using tenfold crossvalidation congressional voting records vot uci dataset gives votes member u house representatives 98th congress sixteen issues problem classify congressman democrat republican based sixteen votes two classes sixteen categorical attributes three categories yea nay neither 435 records rates estimated tenfold crossvalidation waveform wav artificial threeclass problem based three wave forms class consists random convex combination two waveforms sampled integers noise added description generating data given breiman et al 1984 c program available uci twentyone numerical attributes 600 records training set error rates estimated independent test set 3000 records evaluation tae data consist evaluations teaching performance three regular semesters two summer semesters 151 teaching assistant ta assignments statistics department university wisconsinmadison scores grouped three roughly equalsized categories low medium high form class attribute predictor attributes whether native english speaker bi nary ii course instructor 25 categories iii course 26 categories iv summer regular semester binary v class size numerical dataset first reported loh shih 1997 differs datasets two categorical attributes large numbers categories result decision tree algorithms cart employ exhaustive search usually take much longer train algorithms cart evaluate 2 splits categorical attribute c values error rates estimated using tenfold crossvalidation data obtained httpwwwstatwiscedupstatftppublohtreeprogsdatasets summary attribute features datasets given table 1 table 1 characteristics datasets last three columns give number type added noise attributes dataset notation n01 denotes standard normal distribution uimn denotes uniform distribution integers n inclusive u01 denotes uniform distribution unit interval training original attributes type noise attributes data sample num categorical total numerical categorical total set size classes 2 3 4 5 25 26 dna 2000 3 led 2000 bld pid smo thy veh 846 4 vot 435 2 tae 4 experimental setup algorithms designed categorical attributes cases categorical attribute converted vector 01 attributes categorical attribute x takes k values fc g replaced 1dimensional vector otherwise vector consists zeros affected algorithms statistical neural network algorithms well tree algorithms ftl ocu ocl ocm lmt order increase number datasets study effect noise attributes algorithm created sixteen new datasets adding independent noise attributes numbers types noise attributes added given right panel table 1 name new dataset original dataset except addition symbol example bcw dataset noise added denoted bcw dataset use one two different ways estimate error rate algorithm large datasets size much larger 1000 test set size least 1000 use test set estimate error rate classifier constructed using records training set tested test set twelve thirtytwo datasets analyzed way remaining twenty datasets use following tenfold crossvalidation procedure estimate error rate 1 dataset randomly divided ten disjoint subsets containing approximately number records sampling stratified class labels ensure subset class proportions roughly whole dataset 2 subset classifier constructed using records classifier tested withheld subset obtain crossvalidation estimate error rate 3 ten crossvalidation estimates averaged provide estimate classifier constructed data algorithms implemented different programming languages languages available platforms three types unix workstations used study workstation type implementation language algorithm given table 2 relative performance workstations according spec marks given table 3 floating point spec marks show task takes one second dec 3000 would take 14 08 seconds sparcstation 5 ss5 sparcstation 20 ss20 respectively therefore enable comparisons training times reported terms 3000equivalent secondsthe training times recorded ss5 ss20 divided 14 08 respectively 5 results error rates training times algorithms given separate table dataset appendix tables also report error rates naive plurality rule ignores information covariates classifies every record majority class training sample 51 exploratory analysis error rates present formal statistical analysis results helpful study summary table 4 mean error rate algorithm datasets given second row minimum maximum error rates table 2 hardware software platform algorithm workstations dec 3000 alpha model 300 dec sun sparcstation 20 model 61 ss20 sun sparcstation 5 ss5 algorithm platform algorithm platform tree rules st1 splus tree 1se decs qu0 quest univariate 0se decf90 lmt lmdt linear decc qu1 quest univariate 1se decf90 cal cal5 ss5c ql0 quest linear 0se decf90 single split decc ql1 quest linear 1se decf90 ftu fact univariate decf77 statistical linear decf77 lda linear discriminant anal decsas c4t c45 trees decc qda quadratic discriminant anal decsas c4r c45 rules decc nn nearestneighbor decsas ib ind bayes style ss5c log linear logistic regression decf90 ibo ind bayes opt style ss5c fm1 fda degree 1 ss20s im ind mml style ss5c fm2 fda degree 2 ss20s imo ind mml opt style ss5c pda penalized lda ss20s ic0 ind cart 0se ss5c mda mixture discriminant anal ss20s ic1 ind cart 1se ss5c pol polyclass ss20s ocu oc1 univariate ss5c ocl oc1 linear ss5c neural network ocm oc1 mixed ss5c lvq learning vector quantization ss20s st0 splus tree 0se decs rbf radial basis function network decsas table 3 spec benchmark summary workstation specfp92 specint92 source dec dec 3000 model 300 915 662 spec newsletter 150mhz vol 5 issue 2 june 1993 ss20 sun sparcstation 20 1028 889 spec newsletter model 61 60mhz vol 6 issue 2 june 1994 ss5 sun sparcstation 5 473 570 spec newsletter 70mhz vol 6 issue 2 june 1994 plurality rule given dataset last three columns let p denote smallest observed error rate row ie dataset algorithm error rate within one standard error p consider close best indicate p table standard error estimated follows p independent test set let n denote size test set otherwise p crossvalidation estimate let n denote size training set standard error p estimated formula pn algorithm largest error rate within row indicated x total numbers p xmarks algorithm given third fourth rows table following conclusions may drawn table table 4 minimum maximum naive plurality rule error rates dataset p mark indicates algorithm error rate within one standard error minimum dataset xmark indicates algorithm worst error rate dataset mean error rate algorithm given second row decision trees rules statistical algorithms nets error rates naive mean bcw cmc pp pp dna dna hea bos led bld pid seg smo thy ppp ppp pp ppp pp vot tae x tae pp 1 algorithm pol lowest mean error rate ordering algorithms terms mean error rate given upper half table 5 2 algorithms also ranked terms total number p xmarks criterion accurate algorithm pol fifteen p table 5 ordering algorithms mean error rate mean rank error rate mean pol log mda ql0 lda ql1 pda ic0 fm2 ibo imo error 195 204 207 207 208 211 213 215 218 219 219 rate c4r im lmt c4t qu0 qu1 ocu ic1 ib ocm st0 mean pol fm1 log fm2 ql0 lda qu0 c4r imo mda pda rank 83 122 122 122 124 137 139 140 140 143 145 c4t ql1 ibo im ic0 ftl qu1 ocu ic1 st0 st1 error 145 146 147 149 150 154 166 166 169 170 177 rate lmt ocm ib rbf ftu qda lvq ocl cal nn marks xmarks eleven algorithms one xmarks ranked increasing order number xmarks parentheses ftl1 ocm1 st11 fm21 mda1 fm12 ocl3 qda3 nn4 lvq4 t111 excluding remaining algorithms rank order decreasing number p marks parentheses pol15 log13 ql010 lda10 pda10 ql19 ocu9 1 qu08 qu18 c4r8 ibo8 rbf8 c4t7 imo6 im5 ic15 st05 ftu4 ic04 cal4 ib3 lmt1 top four algorithms 1 also rank among top five upper half table 5 3 last three columns table show algorithms sometimes less accurate plurality rule nn cmc cmc smo bld qda smo thy thy ftl tae st1 tae 4 easiest datasets classify bcw bcw vot vot error rates lie 003 009 5 difficult classify cmc cmc tae minimum error rates greater 04 6 two difficult datasets smo smo case smo t1 marginally lower error rate plurality rule algorithm lower error rate plurality rule smo 7 datasets largest range error rates thy thy rates range 0005 0890 however maximum 0890 due qda qda ignored maximum error rate drops 0096 8 six datasets one p mark bld pol sat lvq sat fm2 seg ibo veh veh qda times 9 overall addition noise attributes appear increase significantly error rates algorithms 52 statistical significance error rates 521 analysis variance statistical procedure called mixed effects analysis variance used test simultaneous statistical significance differences mean error rates algorithms controlling differences datasets neter wasserman kutner 1990 p 800 although makes assumption effects datasets act like random sample normal distribution quite robust violation assumption data procedure gives significance probability less 10 gamma4 hence hypothesis mean error rates equal strongly rejected simultaneous confidence intervals differences mean error rates obtained using tukey method miller 1981 p 71 according procedure difference mean error rates two algorithms statistically significant 10 level differ 0058 visualize result figure 1a plots mean error rate algorithm versus median training time seconds solid vertical line plot units right mean error rate pol therefore algorithm lying left line mean error rate statistically significantly different pol algorithms seen form four clusters respect training time clusters roughly delineated three horizontal dotted lines correspond training times one minute ten minutes one hour figure 1b shows magnified plot eighteen algorithms median training times less ten minutes mean error rate statistically significantly different pol 522 analysis ranks avoid normality assumption instead analyze ranks algorithms within datasets dataset algorithm lowest error rate assigned rank one second lowest rank two etc average ranks assigned case ties lower half table 5 gives ordering algorithms terms mean rank error rates pol first last note however mean rank pol 83 shows far uniformly accurate across datasets comparing two methods ordering table 5 seen pol log ql0 lda algorithms consistently good performance three algorithms perform well one criterion mda fm1 fm2 case mda low mean error rate due excellent performance four datasets veh veh wav wav many algorithms poorly domains concern shape identification datasets contain numerical mean error rate median sec ftu c4r ib ibo im imo ocu ocl ocm st0 st1 lda qda pda mda pol rbf 1hr 10min 1min thirtythree methods mean error rate median sec ftu c4r ib im ocu lda pda mda 1min 5min b less 10min accuracy sig different pol figure 1 plots median training time versus mean error rate vertical axis logscale solid vertical line plot divides algorithms two groups mean error rates algorithms left group differ significantly 10 simultaneous significance level pol minimum mean error rate plot b shows algorithms statistically significantly different pol terms mean error rate median training time less ten minutes attributes mda generally unspectacular rest datasets reason tenth place ranking terms mean rank situation fm1 fm2 quite different low mean rank indicates fm1 usually good performer however fails miserably seg seg datasets reporting error rates fifty percent algorithms error rates less ten percent thus fm1 seems less robust algorithms fm2 also appears lack robustness although lesser extent worst performance bos dataset error rate fortytwo percent compared less thirtyfive percent algorithms number xmarks algorithm table 4 good predictor erratic poor performance mda fm1 fm2 least one xmark friedman 1937 test standard procedure testing statistical significance differences mean ranks experiment gives significance probability less 10 gamma4 therefore null hypothesis algorithms equally accurate average rejected difference mean ranks greater 87 statistically significant 10 level hollander wolfe 1999 p 296 thus pol statistically significantly different twenty algorithms mean rank less equal 170 figure 2a shows plot median training time versus mean ranks algorithms algorithms lie left vertical line statistically significantly different pol magnified plot subset algorithms significantly different pol median training time less ten minutes given figure 2b algorithms differ statistically significantly pol terms mean error rate form subset differ pol terms mean ranks thus rank test appears powerful analysis variance test experiment fifteen algorithms figure 2b may recommended use applications good accuracy short training time desired 53 training time table 6 gives median dec 3000equivalent training time algorithm relative training time within datasets owing large range training times order relative fastest algorithm dataset reported fastest algorithm indicated 0 algorithm 10 xgamma1 times slow indicated value x example case dna dataset fastest algorithms c4t t1 requiring two seconds slowest algorithm fm2 takes three million seconds almost forty days hence 10 6 10 7 times slow last two columns table give fastest slowest times dataset table 7 gives ordering algorithms fastest slowest according median training time overall fastest algorithm c4t followed closely ftu ftl lda two reasons superior speed c4t compared decision tree algorithms first splits categorical attribute mean rank median sec ftu c4r ib ibo im imo ocu ocl ocm lda qda pda mda pol rbf 1hr 10min 1min thirtythree methods mean rank median sec c4r im ocu lda pda mda 1min 5min b less 10min accuracy sig different pol figure 2 plots median training time versus mean rank error rates vertical axis logscale solid vertical line plot divides algorithms two groups mean ranks algorithms left group differ significantly 10 simultaneous significance level pol plot b shows algorithms statistically significantly different pol terms mean rank median training time less ten minutes table 6 dec 3000equivalent training times relative times algorithms second third rows give median training time rank algorithm entry x subsequent rows indicates algorithm times slower fastest algorithm dataset fastest algorithm denoted entry 0 minimum maximum training times given last two columns h denote seconds minutes hours days respectively decision trees rules statistical algorithms nets cpu time median cpu 32m 32m 59m 59m 7s 8s 5s 20s 34s 275m 34s 339m 52s 47s 46s 149m 137m 151m 144m 57m 13h 36s 10s 15s 20s 4m 156m 38h 56s 3m 32h 11m 113h 5s rank table 7 ordering algorithms median training time 5s 7s 8s 10s 15s 20s 20s 34s 34s 36s 46s 47s 52s 56s 11m 3m 32m 32m 4m 57m 59m 59m ocm 137m 144m 149m 151m 156m 275m 339m 13h 32h 38h 113h many subnodes number categories therefore wastes time forming subsets categories second pruning method require crossvalidation increase training time several fold classical statistical algorithms qda nn also quite fast expected decision tree algorithms employ univariate splits faster use linear combination splits slowest algorithms pol fm2 rbf two splinebased one neural network although ic0 ic1 st0 st1 claim implement cart algorithm ind versions faster splus versions one reason ic0 ic1 written c whereas st0 st1 written language another reason ind versions use heuristics buntine personal communication instead greedy search number categories categorical attribute large apparent tae dataset categorical attributes twentysix categories case ic0 ic1 take around forty seconds versus two half hours st0 st1 results table 4 indicate inds classification accuracy adversely affected heuristics see aronis provost 1997 another possible heuristic since t1 onelevel tree may appear surprising faster algorithms c4t produce multilevel trees reason splits continuous attribute j number classes hand c4t always splits continuous attribute two intervals therefore j 2 t1 spend lot time search intervals 54 size trees table 8 gives number leaves tree algorithm dataset noise attributes added case error rate obtained tenfold cross validation entry mean number leaves ten crossvalidation trees table 9 shows much number leaves changes addition noise attributes mean median number leaves classifier given last columns two tables ibo imo clearly yield largest trees far apart t1 necessarily short design algorithm shortest trees average ql1 followed closely ftl ocl ranking algorithms univariate splits increasing median number leaves t1 ic1 st1 qu1 ftu ic0 st0 ocu qu0 c4t algorithm c4t tends produce trees many leaves algorithms one reason may due underpruning although error rates quite good another unlike binarytree algorithms c4t splits categorical attribute many nodes number categories addition noise attributes typically decreases size trees except c4t cal tend grow larger trees imo seems fluctuate rather wildly results complement oates jensen 1997 looked effect sample size number leaves decision tree algorithms found significant relationship tree size training sample size c4t observed tree algorithms employ costcomplexity pruning better able control tree growth 6 scalability algorithms although differences mean error rates pol many algorithms statistically significant clear error rate sole criterion pol would method choice unfortunately pol one computeintensive 1algorithms see training times increase sample size small scalability study carried algorithms qu0 ql0 ftl c4t c4r ic0 log fm1 pol training times measured algorithms training sets size four datasets used generate samplessat smo tae new large uci dataset called adult two classes six continuous seven categorical attributes since first three datasets large enough experiment bootstrap resampling employed generate training sets n samples randomly drawn replacement dataset avoid getting many replicate records value class attribute sampled case randomly changed another value probability 01 new value selected pool alternatives equal probability bootstrap sampling carried adult dataset 32000 records instead nested training sets obtained random sampling without replacement times required train algorithms plotted loglog scale figure 3 exception pol fm1 log logarithms training times seem increase linearly logn nonmonotonic behavior pol fm1 puzzling might due randomness use crossvalidation model selection erratic behavior log adult dataset caused convergence problems model fitting many lines figure 3 roughly parallel suggests relative computational speed algorithms fairly constant range sample sizes considered ql0 c4r two exceptions cohen 1995 observed c4r scale well 7 conclusions results show mean error rates many algorithms sufficiently similar differences statistically insignificant differences also probably insignificant practical terms example mean error rates top ranked algorithms pol log ql0 differ less 0012 small difference important real applications user may wish select algorithm based criteria training time interpretability classifier unlike error rates huge differences training times algorithms pol algorithm lowest mean error rate takes fifty times long train next accurate algorithm ratio times roughly equivalent hours versus minutes figure 3 shows maintained wide range sample sizes large applications time factor may advantageous use one quicker algorithms interesting old statistical algorithm lda mean error rate close best surprising designed binaryvalued attributes categorical attributes transformed 01 vectors prior application lda ii expected effective class densities cpu time cpu time tae cpu time adult cpu time figure 3 plots training time versus sample size loglog scale selected algorithms multimodal fast easy implement readily available statistical packages provides convenient benchmark comparison future algorithms low error rates log lda probably account much performance better algorithms example pol basically modern version log enhances flexibility log employing splinebased functions automatic model selection although strategy computationally costly produce slight reduction mean error rateenough bring top pack good performance ql0 may similarly attributable lda quest linearsplit algorithm designed overcome difficulties encountered lda multimodal situations applying modified form lda partitions data partition represented leaf decision tree strategy alone however enough higher mean error rate ftl shows latter based fact algorithm precursor quest one major difference quest fact algorithms former employs costcomplexity pruning method cart whereas latter results suggest form bottomup pruning may essential low error rates purpose constructing algorithm data interpretation perhaps decision rules trees univariate splits suffice exception cal t1 differences mean error rates decision rule tree algorithms statistically significant pol ic0 lowest mean error rate qu0 best terms mean ranks c4r c4t far behind four algorithms provide good classification accuracy c4t fastest far although tends yield trees twice many leaves ic0 qu0 c4r next fastest figure 3 shows scale well ic0 slightly faster trees slightly fewer leaves qu0 however loh shih 1997 show cartbased algorithms ic0 prone produce spurious splits situations acknowledgments indebted p auer c e brodley w buntine hastie r c holte c kooperberg k murthy j r quinlan w sarle b schulmeister w taylor help advice installation computer programs also grateful j w molyneaux providing 1987 national indonesia contraceptive prevalence survey data finally thank w cohen f provost reviewers many helpful comments suggestions r categorical data analysis increasing efficiency data mining algorithms breadthfirst marker propagation new language neural networks pattern recognition classification regression trees simplifying decision trees survey multivariate versus univariate decision trees multivariate decision trees comparison decision tree classifiers backpropagation neural networks multimodal classification problems analysis attitudes toward workplace smoking restrictions learning classification trees introduction ind version 21 recursive partitioning fast effective rule induction neural networks multivariate adaptive regression splines discussion use ranks avoid assumption normality implicit analysis variance construction assessment classification rules hedonic prices demand clean air discriminant analysis gaussian mixtures penalized discriminant analysis flexible discriminant analysis optimal scoring nonparametric statistical methods simple classification rules perform well commonly used datasets applied multivariate statistical analysis polychotomous regression cancer diagnosis via linear programming uci repository machine learning databases machine learning automatic construction decision trees classification decisiontree algorithm cal5 based statistical approach splitting algorithm system induction oblique decision trees applied linear statistical models effects training set size decision tree complexity improved use continuous attributes c4 pattern recognition neural networks neural networks statistical models sas institute symbolic neural learning algorithms empirical comparison modern applied statistics splus diagnostic schemes fine needle aspirates breast masses fine needle aspiration breast mass diagnosis statistical approach fine needle aspiration diagnosis breast masses tr applied multivariate statistical analysis symbolic neural learning algorithms c45 programs machine learning simple classification rules perform well commonly used datasets multivariate decision trees selforganizing maps sasets users guide version 6 neural networks pattern recognition effects training set size decision tree complexity multivariate versus univariate decision trees simplifying decision trees survey ctr ganesan velayathan seiji yamada behaviorbased web page evaluation proceedings 15th international conference world wide web may 2326 2006 edinburgh scotland ganesan velayathan seiji yamada behaviorbased web page evaluation proceedings 2006 ieeewicacm international conference web intelligence intelligent agent technology p409412 december 1822 2006 samuel e buttrey ciril karo using knearestneighbor classification leaves tree computational statistics data analysis v40 n1 p2737 28 july 2002 kwekumuata oseibryson evaluation decision trees multicriteria approach computers operations research v31 n11 p19331945 september 2004 richi nayak laurie buys jan loviekitchin data mining conceptualising active ageing proceedings fifth australasian conference data mining analystics p3945 november 2930 2006 sydney australia xiangyang li nong ye supervised clustering algorithm computer intrusion detection knowledge information systems v8 n4 p498509 november 2005 laura elena raileanu kilian stoffel theoretical comparison gini index information gain criteria annals mathematics artificial intelligence v41 n1 p7793 may 2004 nong ye xiangyang li scalable incremental learning algorithm classification problems computers industrial engineering v43 n4 p677692 september 2002 jonathan eckstein peter l hammer ying liu mikhail nediak bruno simeone maximum box problem application data analysis computational optimization applications v23 n3 p285298 december 2002 sattar hashemi mohammad r kangavari parallel learning using decision trees novel approach proceedings 4th wseas international conference applied mathematics computer science p18 april 2527 2005 rio de janeiro brazil khaled badran peter rockett roles diversity preservation mutation preventing population collapse multiobjective genetic programming proceedings 9th annual conference genetic evolutionary computation july 0711 2007 london england nigel williams sebastian zander grenville armitage preliminary performance comparison five machine learning algorithms practical ip traffic flow classification acm sigcomm computer communication review v36 n5 october 2006 sorin alexe peter l hammer accelerated algorithm pattern detection logical analysis data discrete applied mathematics v154 n7 p10501063 1 may 2006 ruggieri efficient c45 ieee transactions knowledge data engineering v14 n2 p438444 march 2002 md zahidul islam ljiljana brankovic framework privacy preserving classification data mining proceedings second workshop australasian information security data mining web intelligence software internationalisation p163168 january 01 2004 dunedin new zealand efstathios stamatatos gerhard widmer automatic identification music performers learning ensembles artificial intelligence v165 n1 p3756 june 2005 niels landwehr mark hall eibe frank logistic model trees machine learning v59 n12 p161205 may 2005 karann toh quoclong tran dipti srinivasan benchmarking reduced multivariate polynomial pattern classifier ieee transactions pattern analysis machine intelligence v26 n6 p740755 june 2004 abraham bernstein foster provost shawndra hill toward intelligent assistance data mining process ontologybased approach costsensitive classification ieee transactions knowledge data engineering v17 n4 p503518 april 2005 rich caruana alexandru niculescumizil empirical comparison supervised learning algorithms proceedings 23rd international conference machine learning p161168 june 2529 2006 pittsburgh pennsylvania nicolas baskiotis michle sebag c45 competence map phase transitioninspired approach proceedings twentyfirst international conference machine learning p10 july 0408 2004 banff alberta canada gabriela alexe peter l hammer spanned patterns logical analysis data discrete applied mathematics v154 n7 p10391049 1 may 2006 ingolf geist framework data mining kdd proceedings 2002 acm symposium applied computing march 1114 2002 madrid spain anthony j lee yaote wang efficient data mining calling path patterns gsm networks information systems v28 n8 p929948 december kwekumuata oseibryson postpruning decision tree induction using multiple performance measures computers operations research v34 n11 p33313345 november 2007 friedhelm schwenker hans kestler gther palm unsupervised supervised learning radialbasisfunction networks selforganizing neural networks recent advances applications springerverlag new york inc new york ny 2001 chenfu chien wenchih wang jenchieh cheng data mining yield enhancement semiconductor manufacturing empirical study expert systems applications international journal v33 n1 p192198 july 2007 irma becerrafernandez stelios h zanakis steven walczak knowledge discovery techniques predicting country investment risk computers industrial engineering v43 n4 p787800 september 2002 zhiwei fu bruce l golden shreevardhan lele raghavan edward wasil diversification better classification trees computers operations research v33 n11 p31853202 november 2006 rueyshiang guh hybrid learningbased model online detection analysis control chart patterns computers industrial engineering v49 n1 p3562 august 2005 krzysztof krawiec genetic programmingbased construction features machine learning knowledge discovery tasks genetic programming evolvable machines v3 n4 p329343 december 2002 huimin zhao sudha ram combining schema instance information integrating heterogeneous data sources data knowledge engineering v61 n2 p281303 may 2007 karann toh training reciprocalsigmoid classifier feature scalingspace machine learning v65 n1 p273308 october 2006 asparoukhov w j krzanowski nonparametric smoothing location model mixed variable discrimination statistics computing v10 n4 p289297 october 2000 r chandrasekaran young u ryu varghese jacob sungchul hong isotonic separation informs journal computing v17 n4 p462474 october 2005 chingpao chang chihping chu defect prevention software processes actionbased approach journal systems software v80 n4 p559570 april 2007 tony van gestel johan k suykens bart baesens stijn viaene jan vanthienen guido dedene bart de moor joos vandewalle benchmarking least squares support vector machine classifiers machine learning v54 n1 p532 january 2004 elena baralis silvia chiusano essential classification rule sets acm transactions database systems tods v29 n4 p635674 december 2004 siddharth pal david j miller extension iterative scaling decision data aggregation ensemble classification journal vlsi signal processing systems v48 n12 p2137 august 2007 man cheang kwong sak leung kin hong lee genetic parallel programming design implementation evolutionary computation v14 n2 p129156 june 2006 foster provost pedro domingos tree induction probabilitybased ranking machine learning v52 n3 p199215 september johannes gehrke wieyin loh raghu ramakrishnan classification regression money grow trees tutorial notes fifth acm sigkdd international conference knowledge discovery data mining p173 august 1518 1999 san diego california united states vasant dhar dashin chou foster provost discovering interesting patterns investment decision making glower xcirca genetic learner overlaid entropy reduction data mining knowledge discovery v4 n4 p251280 october 2000 perlich foster provost jeffrey simonoff tree induction vs logistic regression learningcurve analysis journal machine learning research 4 p211255 1212003 foster provost venkateswarlu kolluri data mining tasks methods scalability handbook data mining knowledge discovery oxford university press inc new york ny 2002 gabriela alexe sorin alexe tibrius bonates alexander kogan logical analysis data vision peter l hammer annals mathematics artificial intelligence v49 n14 p265312 april 2007 krzysztof j cios lukasz kurgan clip4 hybrid inductive machine learning algorithm generates inequality rules information sciences international journal v163 n13 p3783 14 june 2004 foster provost venkateswarlu kolluri survey methods scaling inductive algorithms data mining knowledge discovery v3 n2 p131169 june 1999 b kotsiantis zaharakis p e pintelas machine learning review classification combining techniques artificial intelligence review v26 n3 p159190 november 2006