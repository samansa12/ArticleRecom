choosing multiple parameters support vector machines problem automatically tuning multiple parameters pattern recognition support vector machines svms considered done minimizing estimates generalization error svms using gradient descent algorithm set parameters usual methods choosing parameters based exhaustive search become intractable soon number parameters exceeds two experimental results assess feasibility approach large number parameters 100 demonstrate improvement generalization performance b introduction problem supervised learning one takes set inputoutput pairs attempts construct classier function f maps input vectors x 2 x onto labels 2 interested pattern recognition classication case set labels simply 1g goal nd f 2 f minimizes error fx 6 future examples learning algorithms usually depend parameters control size class f way search conducted f several techniques exist performing selection parameters idea nd parameters minimize generalization error algorithm hand error estimated either via testing data used learning hold testing crossvalidation techniques via bound given theoretical analysis tuning multiple parameters usually multiple parameters tune time moreover estimates error explicit functions parameters applicable strategy exhaustive search parameter space becomes intractable since would correspond running algorithm every possible value parameter vector discretization propose methodology automatically tuning multiple parameters support vector machines svms takes advantage specicity algorithm svm algorithm support vector machines svms realize following idea map ndimensional input vector x 2 r n 1 high dimensional possibly innite dimensional feature space h construct optimal separating hyperplane space dierent mappings construct dierent svms training data separable optimal hyperplane one maximal distance h space hyperplane closest image vector x training data nonseparable training data generalization concept used suppose maximal distance equal images training vectors x within sphere radius r following theorem holds true 19 theorem 1 given training set size feature space h hyperplane w b margin mw b z radius rz dened kwk maximum margin algorithm takes input training set size return hyperplane feature space margin mw b z maximized note supposing training separable 1 rest article reference vectors matrices using bold notation means 0 assumption probability measures p underlying data z expectation misclassication probability bound expectation taken random draw training set z size left hand side size right hand side theorem justies idea constructing hyperplane separates data large margin larger margin better performance constructed hyperplane note however according theorem average performance depends ratio efr 2 2 g simply large margin multiple parameters svm algorithm usually depends several parameters one denoted c controls tradeo margin maximization error minimization parameters appear nonlinear mapping feature space called kernel parame ters simplicity use classical trick allows consider c kernel parameter parameters treated unied framework widely acknowledged key factor svms performance choice kernel however practice dierent types kernels used due diculty appropriately tuning parameters present technique allows deal large number parameters thus allows use complex kernels another potential advantage able tune large number parameters possibility rescaling attributes indeed priori knowledge available meaning attributes choice use spherical kernels ie give weight attribute one may expect better choice shape kernel since many realworld database contain attributes dierent natures may thus exist appropriate scaling factors give right weight right feature example see use radial basis function kernels rbf many dierent scaling factors input dimensions usual approach consider try pick best value however using proposed method choose automatically good values scaling factors indeed factors precisely parameters kernel moreover demonstrate problem feature selection addressed framework since corresponds nding attributes rescaled zero factor without harming generalization thus see tuning kernel parameters something extremely useful procedure allows would versatile tool various tasks nding right shape kernel feature selection nding right tradeo error margin etc gives rationale developing techniques approach thus see goal nd hyperplane maximizes margin also values mapping parameters yield best generalization error propose minimax ap maximize margin hyperplane coecients minimize estimate generalization error set kernel parameters last step performed using standard gradient descent approach kind error estimates consider several ways assessing generalization error validation error procedure requires reduce amount data used learning order save validation moreover estimates smoothed proper gradient descent leaveoneout error estimates procedure gives estimate expected generalization analytic function parameters examine accuracy estimates uences whole procedure nding optimal parameters particular show really matters variations estimate relate variations test error rather values related outline paper organized follows next section introduces basics svms dierent possible estimates generalization error described section 3 section 4 explains smooth theses estimates introduce section 5 framework minimizing estimates gradient descent section 6 deals computation gradients error estimates respect kernel parameters finally section 7 8 present experimental results method applied variety databases dierent contexts section 7 deals nding right penalization along right radius kernel nding right shape kernel section 8 presented results applying method feature selection learning introduce standard notations svms complete description see 18 let fx set training examples x 2 r n belong class labeled 1g svm methodology map vectors feature space using kernel function kx denes inner product feature space consider kernel k depending set parameters decision function given coecients 0 obtained maximizing following functional i2 constraints coecients 0 dene maximal margin hyperplane highdimensional feature space data mapped nonlinear function formulation svm optimization problem called hard margin formulation since training errors allowed every training point satises inequality corresponding equality satised points called support vectors notice one may require separating hyperplane pass origin choosing xed variant called hard margin svm without threshold case optimization problem remains except constraint disappears dealing nonseparability nonseparable case one needs allow training errors results called soft margin svm algorithm 4 shown soft margin svms quadratic penalization errors considered special case hard margin version modied kernel 4 16 identity matrix c constant penalizing training errors focus remainder hard margin svm use 3 whenever deal nonseparable data thus c considered another parameter kernel function 3 estimating performance svm ideally would like choose value kernel parameters minimize true risk svm classier unfortunately since quantity accessible one build estimates bounds section present several measures expected error rate svm 31 single validation estimate one enough data available possible estimate true error validation set estimate unbiased variance gets smaller size validation set increases validation set fx 0 estimate step function 32 leaveoneout bounds leaveoneout procedure consists removing training data one element constructing decision rule basis remaining training data testing removed element fashion one tests elements training data using dierent decision rules let us denote number errors leaveoneout procedure lx known 10 leaveoneout procedure gives almost unbiased estimate expected generalization error err probability test error machine trained sample size 1 expectations taken random choice sample although lemma makes leaveoneout estimator good choice estimating generalization error nevertheless costly actually compute since requires running training algorithm times strategy thus upper bound approximate estimator easy compute quantity possible analytical expression denote f 0 classier obtained training examples present f one obtained example removed also written thus u p upper bound p f 0 get following upper bound leaveoneout error since hard margin svms monotonically increasing 321 support vector count since removing nonsupport vector training set change solution computed machine ie u nonsupport vector restrict preceding sum support vectors upper bound term sum 1 gives following bound number errors made leaveoneout procedure 17 n sv denotes number support vectors 322 jaakkolahaussler bound svms without threshold analyzing optimization performed svm algorithm computing leaveoneout error jaakkola haussler 8 proved inequality leads following upper bound note wahba et al 20 proposed estimate number errors made leaveoneout procedure hard margin svm case turns seen upper bound jaakkolahaussler one since 323 opperwinther bound hard margin svms without threshold opper winther 12 used method inspired linear response theory prove following assumption set support vectors change removing example p k sv matrix dot products support vectors leading following estimate 324 radiusmargin bound svms without threshold training errors vapnik 18 proposed following upper bound number errors leaveoneout procedure r radius margin dened theorem 1 325 span bound vapnik chapelle 19 3 derived estimate using concept span support vectors assumption set support vectors remains leaveoneout procedure following equality true distance point set p gives exact number errors made leaveoneout procedure previous assumption span estimate related approximations link jaakkolahaussler bound consider svms without threshold constraint removed denition span easily upper bound value span 2 thus recover jaakkolahaussler bound link r 2 2 support vector number errors made leaveoneout procedure bounded x shown 19 span p bounded diameter smallest sphere enclosing training points since number errors made leaveoneout procedure bounded link opperwinther support vectors change hard margin case without threshold gives value opperwinther bound namely 4 smoothing test error estimates estimate performance svm validation error 4 leaveoneout error 5 requires use step function however would like use gradient descent approach minimize estimates test error unfortunately step function dierentiable already mentioned section 325 possible bound x 1 x x 0 bound r 2 2 derived leaveoneout error nevertheless large errors count one therefore might advantageous instead use contracting function form however choice constants b dicult small estimate accurate large resulting estimate smooth instead trying pick good constants b one try get directly smooth approximation test error estimating posterior probabilities recently platt proposed following estimate posterior distribution p svm output fx 13 40 30 20 10 00 10 2348 0306 figure 1 validation error dierent values width rbf kernel top left step function otherwise note bottom picture minimum right place fx output svm constants b found minimizing kullbackleibler divergence p empirical approximation p built validation set optimization carried using second order gradient descent algorithm 13 according estimate best threshold svm classier f pa b x 05 note b 6 0 obtained correction comparable usual svm threshold denition generalization error classier z z error empirically estimated min note labels validation set used directly last step indirectly estimation constants b appearing parametric form pa b better understanding estimate let us consider extreme case error validation set maximum likelihood algorithm going yield pa b x take binary values consequence estimate error probability zero 5 optimizing kernel parameters lets go back svm algorithm assume kernel k depends one several parameters encoded vector thus consider class decision functions parametrized b want choose values parameters w see equation 2 maximized maximum margin algorithm model selection criterion minimized best kernel parameters precisely xed want choose 0 one dimensional parameter one typically tries nite number values picks one gives lowest value criterion note abbreviation pa b x svm solution continuous respect better approach proposed cristianini et al 5 using incremental optimization algorithm one train svm little eort changed small amount however soon one component computing every possible value becomes intractable one rather looks way optimize along trajectory kernel parameter space using gradient model selection criterion optimize model parameters proposed 2 demonstrated case linear regression timeseries prediction also proposed 9 optimize regularization parameters neural network propose algorithm alternates svm optimization gradient step direction gradient parameter space achieved following iterative procedure 1 initialize value 2 using standard svm algorithm find maximum quadratic form w 3 update parameters minimized typically achieved gradient step see 4 go step 2 stop minimum reached solving step 3 requires estimating varies thus restrict case k dierentiated respect moreover consider cases gradient respect computed approximated note 0 depends implicitly since 0 dened maximum w n kernel parameters derivative respect p 0 xed computed gradient r way performing step 3 make gradient step small eventually decreasing convergence improved use second order derivatives newtons method laplacian operator dened formulation additional constraints imposed projection gradient 6 computing gradient section describe computation gradient respect kernel parameters dierent estimates generalization error first bound r 2 2 see theorem 1 obtain formulation derivative margin section 61 radius section 62 validation error see equation 4 show calculate derivative hyperplane parameters 0 b see section 63 finally computation derivative span bound 7 presented section 64 rst begin useful lemma suppose given n1 vector v nn matrix smoothly depending parameter consider function let x vector x maximum l attained x words possible dierentiate l respect x depend note also true one constraints denition f removed proof rst need express equality constraint lagrange multiplier inequality constraints lagrange multipliers maximum following conditions veried v p x last term written follows v p b x using derivatives optimality conditions namely fact either hence v p result follows 61 computing derivative margin note feature space separating hyperplane following expansion normalized min follows denition margin theorem 1 latter 1kwk thus write bound r 2 2 r 2 kwk 2 previous lemma enables us compute derivative kwk 2 deed shown 18 lemma applied standard svm optimization problem 2 giving 62 computing derivative radius computing radius smallest sphere enclosing training points achieved solving following quadratic problem 18 constraints use previous lemma compute derivative radius 0 maximizes previous quadratic form 63 computing derivative hyperplane parameters let us rst compute derivative 0 respect parameter kernel purpose need analytical formulation 0 first suppose points support vectors removed training set assumption done without loss generality since removing point support vector aect solution fact points lie margin z k n support vectors h n matrix parameters svms written able compute derivatives parameters respect kernel parameter p indeed since derivate inverse matrix depending parameter p written 3 follows nally easily use result calculation recover computation p indeed denote h turns 3 inequality easily proved dierentiating mm 64 computing derivative spanrule let us consider span value recall span support vector x p dened distance point set dened 6 value span written note introduced lagrange multiplier enforce constraint introducing extended vector extended matrix dot products support vectors value span written h submatrix k sv row column p removed v pth column k sv fact optimal value h 1 v follows last equality comes following block matrix identity known woodbury formula 11 2 closed form obtain particularly attractive since compute value span support vector inverting matrix k sv combining equation 12 11 get derivative span pp thus complexity computing derivative spanrule respect parameter p kernel requires computation inversion matrix k sv complexity operations larger quadratic optimization problem however problem approach value given span rule continuous changing smoothly value parameters coecients p change continuously span 2 actually discontinuity support vectors set support vectors changes easily understood equation 6 suppose changing value parameter point xm support vector anymore support vectors set p going smaller discontinuity likely appear value situation explained gure 2 plotted value span support vector x p versus width rbf kernel almost everywhere span decreasing hence negative derivative jumps appear corresponding change set support vectors moreover span globally increasing value derivate give us good indication global evolution span one way solve problem try smooth behavior span done imposing following additional constraint denition p equation constant given constraint point xm leave entered set support vectors large uence span support vectors since 0 small eect constraint make set p become continuous set support vectors changes however new constraint prevents us computing span eciently equation 12 possible solution replace constraint 74e376e378e380e3 figure 2 value p sum span training points dierent values width rbf kernel varying small vicinity regularization term computation span new denition span equation 12 becomes diagonal matrix elements shown gure 3 span much smoother minimum still right place experiments took note computing derivative new expression dicult previous span expression interesting look leaveoneout error svms without threshold case value span regularization writes already pointed section 325 value span 20 21 22 23 24 25 26 27 28 29 3077e385e393e3 figure 3 left minima span regularization dashed line without regularization solid line close right detailed behavior span dierent values regularizer recover opperwinther bound hand case span bound identical jaakkolahaussler one way span bound regularization bounds opperwinther jaakkolahaussler 7 experiments experiments dierent nature carried assess performance feasibility method rst set experiments consists nding automatically optimal value two parameters width rbf kernel constant c equation 3 second set experiments corresponds optimization large number scaling factors case handwritten digit recognition show optimizing scaling factors leads naturally feature selection demonstrate application method selection relevant features several databases 71 optimization details core technique present gradient descent algorithm used optimization toolbox matlab perform includes second order updates improve convergence speed crossvalidation r 2 2 spanbound breast cancer 2604 474 2684 471 2559 418 diabetis 2353 173 2325 17 2319 167 heart 1595 326 1592 318 1613 311 thyroid 480 219 462 203 456 197 table 1 test error found dierent algorithms selecting svm parameters c rst column reports results 14 second last column parameters found minimizing r 2 2 spanbound using gradient descent algorithm 72 benchmark databases rst set experiments tried select automatically width rbf kernel along constant c penalizing training error appearing equation 3 order avoid adding positivity constraints optimization problem constant c width rbf kernel use parameterization turns give stable optimization used benchmark databases described 14 databases long 100 splits training test sets available httpidafirstgmdderaetschdatabenchmarkshtm followed experimental setup 14 rst 5 training sets kernel parameters estimated using either 5fold crossvalidation minimization r 2 2 spanbound finally kernel parameters computed median 5 estimations results shown table 1 turns minimizing r 2 2 span estimates yields approximately performances pickingup parameters minimize crossvalidation error surprising since crossvalidation known accurate method choosing hyperparameters learning algorithm interesting compare computational cost meth crossvalidation r 2 2 spanbound breast cancer 500 142 7 diabetis 500 122 98 heart 500 9 62 thyroid 500 3 116 titanic 500 68 34 table 2 average number svm trainings one training set needed select parameters c using standard crossvalidation minimizing spanbound ods table shows many svm trainings average needed select kernel parameters split results crossvalidation ones reported 14 tried 10 dierent values c performed 5fold crossvalidation number svm trainings 5 training set needed method gain complexity impressive average 100 times less svm training required nd kernel parameters main reason gain two parameters optimize computational reasons exhaustive search crossvalidation handle selection 2 parameters whereas method highlighted next section discussion explained section 32 r 2 2 seem rough upper bound spanbound accurate estimate test error 3 however process choosing kernel parameters matters bound whose minimum close optimal kernel parameters even r 2 2 cannot used estimate test error previous experiments show minimization yields quite good results generalization error obtained minimizing spanbound cf gure slightly better since minimization latter dicult implement control local minima recommend practice minimize r 2 2 experiments following section relate experiments bound similar results obtained spanbound 73 automatic selection scaling factors experiment try choose scaling factors rbf polynomial kernel degree 2 precisely consider kernels following experiments carried usps handwritten digit recognition database database consists 7291 training examples 2007 test examples digit images size 16x16 pixels try classify digits 0 4 5 9 training set split 23 subsets 317 examples subset used experiments assess feasibility gradient descent approach nding kernel parameters rst used 16 parameters one corresponding scaling factor squared tile 16 pixels shown gure 4 figure 4 16 tiles scaling factors 16 pixels identical scaling parameters initialized 1 evolution test error bound r 2 2 plotted versus number iterations gradient descent procedure gures 5 polynomial kernel 6 rbf note polynomial kernel test error went 9 whereas best test error one scaling parameter 99 thus figure 5 evolution test error left bound r 2 2 right gradient descent optimization polynomial kernel figure evolution test error left bound r 2 2 right gradient descent optimization rbf kernel taking several scaling parameters managed make test error decrease might interesting see scaling coecients found purpose took 256 scaling parameters one per pixel minimized polynomial kernel map scaling coecient shown gure 7 result quite consistent one could expect situation coecients near border picture smaller middle picture coecients directly interpreted measures relevance corresponding feature figure 7 scaling factors found optimization procedure darker means smaller scaling factor discussion experiment considered sanity check experi ment indeed proves feasible choose multiple kernel parameters svm lead overtting however gain test error main motivation since expect signi cant improvement problem features play similar role taking scaling factors equal database seems reasonable choice however highlighted gure 7 method powerful tool perform feature selection 8 feature selection motivation feature selection threefold 1 improve generalization error 2 determine relevant features explanatory purposes 3 reduce dimensionality input space realtime applications finding optimal scaling parameters lead feature selection algo rithms indeed one input components useless classica tion problem scaling factor likely become small scaling becomes small enough means possible remove without aecting classication algorithm leads following idea feature selection keep features whose scaling factors largest also performed principal components space scale principal component scaling factor consider two dierent parametrization kernel rst one correspond rescaling data input space 2 r n second one corresponds rescaling principal components space matrix principal components compute using following iterative procedure 1 initialize 2 case principal component scaling perform principal component analysis compute matrix 3 solve svm optimization problem 4 minimize estimate error respect gradient step 5 discard dimensions corresponding small elements return step 2 demonstrate idea two toy problems show feature selection reduces generalization error apply feature selection algorithm dna microarray data important nd genes relevant performing classication also seems types algorithms feature selection improves performance lastly apply algorithm face detection show greatly reduce input dimension without sacricing performance 81 toy data compared several algorithms standard svm algorithm feature selection feature selection algorithm estimate r 2 2 span estimate standard svm applied feature selection via lter method three lter methods used choose largest features according pearson correlation coecients fisher criterion score 4 kolmogorovsmirnov test 5 note pearson coecients fisher criterion cannot model nonlinear dependencies two following articial datasets objective assess ability algorithm select small number target features presence irrelevant redundant features 21 rst example six dimensions 202 relevant probability equal rst three features drawn second three features fx 4 drawn probability 07 otherwise rst three drawn second three x 1 remaining features noise x second example two dimensions 52 relevant probability equal data drawn following drawn n ability drawn two normal distributions equal probability rest features noise x linear problem rst six features redundancy rest features irrelevant nonlinear problem rst two features irrelevant used linear kernel linear problem second order polynomial kernel nonlinear problem imposed feature selection algorithms keep best two features results shown gure 8 various training set sizes taking average test error 500 samples runs training set size fisher score shown graphs due space constraints performed almost identically correlation coecients problem clearly see method outperforms classical methods feature selection nonlinear problem among r r mean value rth feature positive negative classes ris standard deviation 5 ks tst fr denotes rth feature training example p corresponding empirical distribution lter methods kolmogorovsmirnov test improved performance standard svms rwbound gradient standard svms correlation coefficients kolmogorovsmirnov test rwbound gradient standard svms correlation coefficients kolmogorovsmirnov test b figure 8 comparison feature selection methods linear problem b nonlinear problem many irrelevant features xaxis number training points yaxis test error fraction test points 82 dna microarray data next tested idea two leukemia discrimination problems 6 problem predicting treatment outcome medulloblastoma 1 rst problem classify myeloid versus lymphoblastic leukemias based expression 7129 genes training set consists 38 examples test set 34 examples standard linear svms achieve 1 error test set using gradient descent r 2 achieved error 0 using error 1 using 1 gene using fisher score select features resulted 1 error 1 genes second leukemia classication problem discriminating b versus cells lymphoblastic cells 6 standard linear svms makes 1 error problem using either span bound gradient descent r 2 results 0 errors made using 5 genes using fisher score results 2 errors made using 5 genes nal problem one predicting treatment outcome patients medulloblastoma 60 examples 7129 expression values dataset use leaveoneout measure error rate standard svm gaussian kernel makes 24 errors selecting genes using gradient descent r 2 achieved error 15 83 face detection trainable system detecting frontal nearfrontal views faces gray images presented 7 gave good results terms detection rates system used gray values 1919 images inputs seconddegree polynomial kernel svm choice kernel lead 40000 features feature space searching image faces dierent scales took several minutes pc make system realtime reducing dimensionality input space feature space required feature selection principal components space used reduce dimensionality input space 15 method evaluated large cmu test set 1 consisting 479 faces 57000000 nonface patterns figure 9 compare roc curves obtained dierent numbers selected components results showed using 60 components improve performances system 15 figure 9 roc curves dierent number pca gray features 9 conclusion proposed approach automatically tuning kernel parameters svm based possibility computing gradient various bounds generalization error respect parame ters dierent techniques proposed smooth bounds preserving accuracy predicting location minimum test error using smoothed gradients able perform gradient descent search kernel parameter space leading improvement performance reduction complexity solution feature selection using method chose separable case appropriate scaling factors non separable case method allows us choose simultaneously scaling factors parameter c see equation 3 benets technique many first allows actually optimize large number parameters previous approaches could deal 2 parameters even case small number parameters improves running time large amount moreover experimental results demonstrated accurate estimate error required simple estimate like r 2 2 good behaviour terms allowing nd right parameters way renders technique even applicable since estimate simple compute derive finally approach avoids holding data validation thus makes full use training set optimization parameters contrary crossvalidation methods approach fact proven successful various situation opens new directions research theory practice support vector machines practical side approach makes possible use highly complex tunable kernels tuning scaling factors adapting shape kernel problem selection relevant features theoretical side demonstrates even large number parameter simultaneously tuned overtting eect remains low course lot work remains done order properly understand reasons another interesting phenomenon fact quantitative accuracy estimate used gradient descent marginally relevant raises question design good estimates parameter tuning rather accurate estimates future investigation focus trying understand phenomena obtain bounds generalization error overall algorithm along looking new problems approach could applied well new applications acknowledgments authors would like thank jason weston elodie nedelec helpful comments discussions r medulloblastoma diagnosis outcome prediction gene expression pro model selection support vector chines support vector networks dynamically adapting kernels support vector machines face detection still gray images probabilistic kernel regression models adaptive regularization neural network modeling estimation characters obtained statistical procedure recognition gaussian processes svm mean probabilities support vector machines feature selection face detection robust bounds generalization margin distribution nature statistical learning theory statistical learning theory bounds error expectation support vector machines generalized approximate crossvalidation support vector machines another way look margin like quantities feature selection support vector machines tr ctr alex holub max welling pietro perona hybrid generativediscriminative visual categorization international journal computer vision v77 n13 p239258 may 2008 dityan yeung hong chang guang dai learning kernel matrix maximizing kfdbased class separability criterion pattern recognition v40 n7 p20212028 july 2007 tobias glasmachers christian igel gradientbased adaptation general gaussian kernels neural computation v17 n10 p20992105 october 2005 kristin p bennett michinari momma mark j embrechts mark boosting algorithm heterogeneous kernel models proceedings eighth acm sigkdd international conference knowledge discovery data mining july 2326 2002 edmonton alberta canada yoram baram learning kernel polarization neural computation v17 n6 p12641275 june 2005 carl gold alex holub peter sollich bayesian approach feature selection parameter tuning support vector machine classifiers neural networks v18 n56 p693701 june 2005 koji tsuda shinsuke uda taishin kin kiyoshi asai minimizing cross validation error mix kernel matrices heterogeneous biological data neural processing letters v19 n1 p6372 february 2004 carlos soares pavel b brazdil selecting parameters svm using metalearning kernel matrixbased metafeatures proceedings 2006 acm symposium applied computing april 2327 2006 dijon france tristrom cooke two variations fishers linear discriminant pattern recognition ieee transactions pattern analysis machine intelligence v24 n2 p268273 february 2002 carlos soares pavel b brazdil petr kuba metalearning method select kernel width support vector regression machine learning v54 n3 p195209 march 2004 sayan mukherjee qiang wu estimation gradients coordinate covariation classification journal machine learning research 7 p24812514 1212006 alain rakotomamonjy variable selection using svm based criteria journal machine learning research 3 312003 malte kuss carl edward rasmussen assessing approximate inference binary gaussian process classification journal machine learning research 6 p16791704 1212005 keith sullivan sean luke evolving kernels support vector machine classification proceedings 9th annual conference genetic evolutionary computation july 0711 2007 london england mingrui wu bernhard schlkopf gkhan bakir building sparse large margin classifiers proceedings 22nd international conference machine learning p9961003 august 0711 2005 bonn germany andreas argyriou raphael hauser charles micchelli massimiliano pontil dcprogramming algorithm kernel selection proceedings 23rd international conference machine learning p4148 june 2529 2006 pittsburgh pennsylvania huseyin ince theodore b trafalis hybrid model exchange rate prediction decision support systems v42 n2 p10541062 november 2006 sayan mukherjee dingxuan zhou learning coordinate covariances via gradients journal machine learning research 7 p519549 1212006 mingrui wu bernhard schlkopf gkhan bakr direct method building sparse kernel learning algorithms journal machine learning research 7 p603624 1212006 liefeng bo ling wang licheng jiao feature scaling kernel fisher discriminant analysis using leaveoneout cross validation neural computation v18 n4 p961978 april 2006 alain rakotomamonjy francis bach stphane canu yves grandvalet efficiency multiple kernel learning proceedings 24th international conference machine learning p775782 june 2024 2007 corvalis oregon xuewen chen jong cheol jeong minimum reference set based feature selection small sample classifications proceedings 24th international conference machine learning p153160 june 2024 2007 corvalis oregon training algorithms fuzzy support vector machines noisy data pattern recognition letters v25 n14 p16471656 15 october 2004 francis r bach gert r g lanckriet michael jordan multiple kernel learning conic duality smo algorithm proceedings twentyfirst international conference machine learning p6 july 0408 2004 banff alberta canada r kumar kulkarni v k jayaraman b kulkarni symbolization assisted svm classifier noisy data pattern recognition letters v25 n4 p495504 march 2004 sren sonnenburg gunnar rtsch christin schfer bernhard schlkopf large scale multiple kernel learning journal machine learning research 7 p15311565 1212006 kaiquan shen chongjin ong xiaoping li einar p wildersmith feature selection via sensitivity analysis svm probabilistic outputs machine learning v70 n1 p120 january 2008 zhihua zhang james kwok dityan yeung modelbased transductive learning kernel matrix machine learning v63 n1 p69101 april 2006 kaimin chung weichun kao chialiang sun lilun wang chihjen lin radius margin bounds support vector machines rbf kernel neural computation v15 n11 p26432681 november mingwei chang chihjen lin leaveoneout bounds support vector regression model selection neural computation v17 n5 p11881222 may 2005 keem siah yap izham z abidin abdul rahim ahmad zahrul faizi hussien hooi loong pok fariq izwan ismail abdul malik mohamad abnormalities fraud electric meter detection using hybrid support vector machine genetic algorithm proceedings third conference iasted international conference advances computer science technology p388392 april 0204 2007 phuket thailand olivier chapelle training support vector machine primal neural computation v19 n5 p11551178 may 2007 guang dai dityan yeung kernel selection forl semisupervised kernel machines proceedings 24th international conference machine learning p185192 june 2024 2007 corvalis oregon yiming ying dingxuan zhou learnability gaussians flexible variances journal machine learning research 8 p249276 512007 christian igel tobias glasmachers britta mersch nico pfeifer peter meinicke gradientbased optimization kerneltarget alignment sequence kernels applied bacterial gene start detection ieeeacm transactions computational biology bioinformatics tcbb v4 n2 p216226 april 2007 lior wolf amnon shashua feature selection unsupervised supervised inference emergence sparsity weightbased approach journal machine learning research 6 p18551887 1212005 baback moghaddam minghsuan yang learning gender support faces ieee transactions pattern analysis machine intelligence v24 n5 p707711 may 2002 rakotomamonjy analysis svm regression bounds variable ranking neurocomputing v70 n79 p14891501 march 2007 yi wu edward chang distancefunction design fusion sequence data proceedings thirteenth acm international conference information knowledge management november 0813 2004 washington dc usa abdul majid asifullah khan anwar mirza combination support vector machines using genetic programming international journal hybrid intelligent systems v3 n2 p109125 january 2006 qiang wu yiming ying dingxuan zhou multikernel regularized classifiers journal complexity v23 n1 p108134 february 2007 shin ando hitoshi iba classification gene expression profile using combinatory method evolutionary computation machine learning genetic programming evolvable machines v5 n2 p145156 june 2004 giorgio valentini thomas g dietterich biasvariance analysis support vector machines development svmbased ensemble methods journal machine learning research 5 p725775 1212004 fabien lauer ching suen grard bloch trainable feature extractor handwritten digit recognition pattern recognition v40 n6 p18161824 june 2007 r brunelli verification finger matching comparison support vector machines gaussian basis functions classifiers pattern recognition letters v27 n16 p19051915 december 2006 r brunelli verification finger matching comparison support vector machines gaussian basis functions classifiers pattern recognition letters v27 n16 p19051915 december 2006 gavin c cawley nicola l c talbot constructing bayesian formulations sparse kernel learning methods neural networks v18 n56 p674683 june 2005 gavin c cawley nicola l c talbot fast exact leaveoneout crossvalidation sparse leastsquares support vector machines neural networks v17 n10 p14671475 december 2004 piyush kumar joseph b mitchell e alper yildirim approximate minimum enclosing balls high dimensions using coresets journal experimental algorithmics jea 8 saher esmeir shaul markovitch anytime learning decision trees journal machine learning research 8 p891933 512007 sbastien gadat laurent younes stochastic algorithm feature selection pattern recognition journal machine learning research 8 p509547 512007 k pelckmans j suykens b moor additive regularization tradeoff fusion training validation levels kernel methods machine learning v62 n3 p217252 march 2006 gavin c cawley nicola l c talbot preventing overfitting model selection via bayesian regularisation hyperparameters journal machine learning research 8 p841861 512007 mathias adankon mohamed cheriet optimizing resources model selection support vector machine pattern recognition v40 n3 p953963 march 2007 mingkun li ishwar k sethi confidencebased classifier design pattern recognition v39 n7 p12301240 july 2006 ataollah abrahamzadeh seyed alireza seyedin mehdi dehghan digitalsignaltype identification using efficient identifier eurasip journal applied signal processing v2007 n1 p6363 1 january 2007 ron meir gunnar rtsch introduction boosting leveraging advanced lectures machine learning springerverlag new york inc new york ny zexuan zhu yewsoon ong manoranjan dash markov blanketembedded genetic algorithm gene selection pattern recognition v40 n11 p32363248 november 2007