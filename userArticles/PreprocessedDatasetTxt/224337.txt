detecting coarsegrain parallelism using interprocedural parallelizing compiler paper presents extensive empirical evaluation interprocedural parallelizing compiler developed part stanford suif compiler system system incorporates comprehensive integrated collection analyses including privatization reduction recognition array scalar variables symbolic analysis array subscripts interprocedural analysis framework designed provide analysis results nearly precise full inlining without associated costs experimentation system shows capable detecting coarser granularity parallelism previously possible specifically parallelize loops span numerous procedures hundreds lines codes frequently requiring modifications array data structures privatization reduction transformations measurements several standard benchmark suites demonstrate integrated combination interprocedural analyses substantially advance capability automatic parallelization technology b introduction symmetric sharedmemory multiprocessors built latest mi croprocessors widely available class computationally pow research supported part air force material command arpa contract f3060295c0098 arpa contract dabt6394c0054 nsf cise postdoctoral fellowship jet propulsion laboratory fellowships intel corporation att bell laboratories nsf young investigator award erful machines hardware technology advances make pervasive parallel computing possibility ever important tools developed simplify parallel programming parallelizing compiler automatically locates parallel computations sequential programs particularly attractive programming tool frees programmers difficult task explicitly managing parallelism programs unfortunately todays commercially available parallelizing compilers effective getting good performance multiprocessors 3 23 parallelizers developed vectorizing compiler technology tend successful parallelizing innermost loops parallelizing inner loops adequate multiprocessors two reasons first inner loops may make significant portion sequential computation thus limiting parallel speedup limiting amount parallelism second synchronizing processors end inner loops leaves little computation occurring parallel synchronization points cost frequent synchronization load imbalance potentially overwhelm benefits parallelization multiprocessors powerful vector machines execute different threads control simultaneously thus exploit coarser granularity parallelism thus parallelizing compiler target multiprocessor effectively must identify outer parallelizable loops extract coarsegrain parallelism requires two major improvements standard parallelization techniques advanced array analyses loop often parallelizable unless compiler modifies data structures accesses example common iteration loop define use variable compiler must give processor private copy variable loop parallelizable another example compiler parallelize reduction eg computation sum product maximum data elements processor compute partial reduction locally update global result end compilers traditionally perform privatization reduction tranformations scalar variables find outer parallel loops compiler must able perform transformations array variables well 3 23 interprocedural analysis programs written modular style natural coarsegrain parallel loops span multiple pro cedures reason procedure boundaries must pose barrier analysis 3 one way eliminate procedure boundaries perform inline substitutionreplacing procedure call copy called procedureand perform program analysis usual way practical solution large programs inefficient time space interprocedural analysis applies dataflow analysis techniques across procedure boundaries much efficient analyzes single copy procedure although much research devoted interprocedural analysis parallelization 8 12 13 15 18 adopted practice primary obstacle progress area effective interprocedural compilers substantially harder build intraprocedural counterparts moreover inherent tradeoff performing analysis efficiently obtaining precise results successful interprocedural compiler must tackle complexity compilation process maintaining reasonable efficiency without sacrificing much precision developed automatic parallelization system fully interprocedural system incorporates standard analyses included todays automatic parallelizers data dependence analysis analyses scalar variables including constant propagation value numbering induction variable recognition scalar dependence reduction recognition addition system employs analyses array privatization array reduction recognition implementation techniques extends previous work meet demands parallelizing real programs interprocedural analysis designed practical providing nearly quality analysis program fully inlined paper presents comprehensive evaluation effectiveness system locating coarsegrain parallel computations large collection programs spec92fp nas perfect benchmark suites demonstrate techniques system significantly improve performance automatically parallelized codes increasing portion program executed parallel reducing synchronization overhead result parallelizing outer loops rather inner ones remainder paper organized seven sections sections 2 present types advanced analyses required parallelize full applications section 3 overview requirements precise efficient interprocedural analysis section 4 overviews parallelization analysis system section 5 compares work automatic parallelization systems section 6 devoted empirical evaluation system followed conclusion parallelization analysis techniques parallelizing coarsegrain outer loops requires compilers incorporate many techniques beyond standard analyses currently available commercial parallelizing compilers section briefly describe parallelization analysis techniques giving examples extracted real programs encountered experiments motivate need advanced analysis techniques 21 analysis scalar variables scalar parallelization analysis scalar parallelization analysis locates data dependences scalar variables data dependence occurs memory location written one iteration loop might accessed read written different iteration case say loop carries dependence cannot safely parallelized scalar dependences analysis determines whether parallelization may enabled privatization reduction transformations scalar symbolic analysis parallelizing compilers perform data dependence analysis arrays check loopcarried dependences individual array elements array data dependence analysis effective subscript expressions affine functions loop indices loop invariants within domain data dependence analysis shown equivalent integer programming integer programming potentially expensive data dependence problems found practice simple efficient algorithms developed usually solve problems exactly 7 20 reason parallelizing compilers incorporate host scalar symbolic analyses put array indices affine form including constant propagation value numbering induction variable recognition analyses provide integer coefficients subscript variables derive affine equality relationships among variables systems also propagate inequality relations relational constraints integer variables imposed surrounding code constructs ifs loops uses array subscripts 12 15 relations nonlinear variables propagating affine relations among scalars sufficient parallelize loops example scientific codes often linearize accesses conceptually multidimensional arrays resulting subscript expressions cannot expressed affine function enclosing loop indices loop nest figure 1a perfect benchmark trfd illustrates situation figure 1b shows information determined symbolic analysis loop access array xrsij induce dependence iterations outer loop since index mrsij never value two different iterations nonlinear induction variable trfd b symbolic information figure 1 nonlinear analysis example 22 analysis array variables scalar parallelization analyses described dependence privatization reduction must generalized apply array variables array dataflow analysis array privatization simple example motivating need array privatization k loop figure 2a 160line loop taken nas sample benchmark appbt concisely present examples use fortran 90 array notation place actual loops although array locations tm defined used different iterations outer loop value flows across iterations consequently safe parallelize loop private copy tm accessed process finding privatizable arrays requires dataflow analysis previously performed scalar variables applied individual array elements analysis called array dataflow analysis array privatization initialization array privatization usually applied case iteration first defines values array used however also applicable loops whose iterations use values computed outside loop private copies must initialized values parallel execution begins words array privatization illegal iterations refer values generated preceding iterations loop example array privatization initialization shown figure 2b figure shows portion 1002line loop perfect benchmark spec77 see section 632 part array ze second row modified referenced remainder array modified array privatization appbt uvglob reads entire kth column array ze b interprocedural array privatization initialization spec77 c recognizing regions across loops figure 2 array analysis examples loop array ze privatizable outer loop giving processor private copy second row initialized original values recognizing complicated regions data flow analysis arrays intrinsically difficult analysis scalar variables necessary keep track accesses individual array elements cases reads writes array may appear multiple loops read write operations may interleaved compiler must keep precise enough information analysis maintaining efficiency figure 2b also part 1002 line loop spec77 illustrates complexity problem statement array notation corresponds doubly nested loop compiler determine array w privatizable inferring collection write operations w completely defined read array reduction recognition compilers recognize simple reductions accumulation variable sum figure 3a reductions array variables also common scientific codes potential source significant improvements parallelization results sparse array reductions sparse computations pose usually considered insurmountable problem parallelizing compilers arrays part subscript expressions compiler cannot determine locations array read written cases scalar reduction b sparse reduction cgm jbeg le jend c multiple reduction statements interprocedural reduction mdljdp2 figure 3 reduction analysis examples loops containing sparse computations still parallelized computation recognized reduction example loop figure 3b constitutes main computation nas sample benchmark cgm observe accesses sparse vector commutative associative updates location thus safe transform reduction parallelizable form figure 3c excerpt 148line loop makes main computation benchmark mdljdp2 outer loop parallelizable sparse reduction performed interprocedurally example also demonstrates reductions loop may consist multiple updates array 3 interprocedural analysis issues interprocedural dataflow analysis support interprocedural optimization much spaceefficient manner inlining analyzing single copy procedure capture precise interprocedural information requires flowsensitive approach derives analysis results along possible control flow path program precise efficient flowsensitive interprocedural analysis difficult information flows procedure callers representing calling context procedure invoked callees representing side effects invocations example straightforward interprocedural adaptation traditional iterative analysis analysis might carried program representation called supergraph 22 individual control flow graphs procedures program linked together procedure call return points iterative analysis structure slow number control paths information flows large analysis also loses precision propagating information along unrealizable paths 17 analysis may propagate calling context information one caller procedure return sideeffect information different caller regionbased flowsensitive analysis system use regionbased analysis solves problems unrealizable paths slow convergence perform analysis efficiently two separate passes program proceeding bottomup call graph first pass analyzes procedure obtain description sideeffect behavior form transfer function transfer function turn inserted call sites computing transfer functions callers second topdown pass call graph dataflow information calling context applied transfer functions derive final analysis results procedure use similar approach within procedure summarizing code region eg loop behavior bottomup pass propagating context region topdown pass selective procedure cloning regionbased analysis approach described precision may still lost compared full inlining second pass deriving calling context pro cedure analysis must represent conservative approximation information contributed program paths procedure approximations affect precision analysis procedure invoked along paths contribute different information avoid loss precision incorporate calling context analysis technique called selective procedure cloning compiler replicates analysis results procedure determine dataflow information along paths program contribute significantly different calling contexts 5 replication done selectively according unique dataflow information exposes manage analysis costs usually obtain precision full inlining interprocedural framework different dataflow problems share many commonalities example support parameter passing useful interprocedural framework manage complexity implementation allow code reuse regionbased analysis selective procedure cloning techniques encapsulated common interprocedural framework part fiat tool developing interprocedural analysis systems 11 fiat facilitates adding new interprocedural analyses providing parameterized templates drive flowsensitive analysis cloning analysis problem implemented instantiating templates functions compute solutions dataflow equations interprocedural parallelization system suif extended fiat significantly support array dataflow analysis flowsensitive analysis parallelization analysis algorithms section overviews parallelization analysis algorithms describes different phases analysis fit together description found elsewhere 9 10 41 scalar analysis system interprocedural scalar analysis encompasses scalar parallelization analysis scalar symbolic analysis scalar parallelization analysis simple flowinsensitive analysis interprocedural analysis consider control flow within procedure provides information locate scalar dependences scalar reductions recognizing privatizable scalars com plex requiring bottomup flowsensitive regionbased analysis find upwardsexposed reads loop boundaries interprocedural symbolic analysis combines constant propaga tion value numbering induction variable recognition analysis performed regionbased analysis two passes call graph procedures regions summarized scalar value maps describe variable values exit arbitrary expression variable values entry topdown pass determines symbolic values integer variables terms loop indices loop invariants selectively clones based symbolic values used procedure analysis also provides support analyzing nonlinear array subscripts analysis recognizes higherorder induction variables mrsij mi loop figure 1a provides approximation information array analysis set additional linear inequality constraints simple approach captures required analyze many nonlinear subscript expressions without need adopt sophisticated nonlinear dependence tests 4 19 separate topdown interprocedural context analysis propagates contextual relations looprelative terms obtained symbolic analysis enclosing predicates loop bounds context relations represented set systems linear equalities similar array summary descriptors described 42 array analysis summaries traditional data dependence analysis solves integer programming problem every pair array accesses loop terest 2 analysis becomes prohibitively expensive large loops particularly interprocedural setting one way improve efficiency summarize array accesses region code data dependence analysis applied small number sum maries summaries also provide representation used array dataflow analysis described many different designs summaries trade efficiency precision 13 14 18 24 represent summary set array accesses list systems linear inequalities array indices equated affine expressions outer loop indices loopinvariant values constrained inequalities derived loop bounds representation data accesses set systems allows us trade precision efficiency necessary example union two summaries combines two systems one computationally inexpensive loss information results representing multiple regions array essential capture examples figure 2cd representing array summaries convex hulls would necessary precision create summary access outside enclosing loop projecting away loop index variable using fouriermotzkin projection enhanced integer domain similarly transform access summary across procedure call equating array subscript variables formal parameter subscript variables actual parameter constrained declared types formal actual projection eliminates formal parameter subscripts replaces actual parameter subscripts strategy transforming summaries across procedure boundaries provides general mechanism analyzing array reshapes number size array dimensions altered call similar approach array reshapes also recently adopted creussilet 6 array dataflow analysis single array dataflow analysis used determine arrays involved data dependences locate privatizable arrays recognize reductions array dataflow analysis bottomup interprocedural analysis loops procedures program using regionbased analysis framework described analysis computes following four sets summaries program region array sections definitely written mustwrite may written write may read read may read written exposedread data dependence testing loop comparison read write sets determine disjoint different iterations loop arrays involved data dependences may yield privatiza tion array privatized write set exposedread set different iterations disjoint array privatization analysis extension tu paduas approach 26 algorithm requires privatizable array read locations upwardsexposed beginning loop iteration approach general capturing cases one figure 2c array reductions array reduction recognition performed simple algorithm integrated array dataflow analysis recognizing reductions begins locating commutative associative min max operations memory location mark corresponding representations accesses summary reduction type according operator bottomup interprocedural propagation array dataflow analysis ensure accesses potential reduction array within current loop reduction operations type loop evaluate whether variables carry dependence cannot privatized involved reduction computation variables computation reduced every region described summary reduction type simple algorithm sufficiently powerful recognize parallelize reductions figure 3ac 43 putting together analysis techniques built using regionbased analysis frame work selective cloning figure 4 put together analysis phases demonstrating entire analysis system could execute four passes programs call graph scalar modifications references reductions performed initial flowinsensitive pass analyses could fold next pass flowinsensitive implementation performed efficiently 5 related work late 1980s series papers presented results interprocedural parallelization analysis 13 18 24 common approach determine sections arrays modified referenced procedure call enabling parallelization loops containing calls whenever invocation modifies array elements distinct referenced modified invocations techniques shown effective parallelizing linear algebra libraries 1 flowinsensitive pass ffl find modified referenced variables ffl find scalar reductions 2 bottomup pass scalar analysis ffl find privatizable scalars ffl summarize symbolic behaviors sideeffects 3 topdown pass scalar analysis apply calling context symbolic value maps symbolic analysis ffl extract propagate program controlflow constraints inequal ity relations ffl selectively clone based two analyses 4 bottomup pass array analysis ffl summarize mustwrite write read exposedread ffl find data dependences intersect write read ffl find privatizable arrays intersect write exposedread ffl recognize array reductions record reduction operator type figure 4 phases interprocedural parallelization analysis recently fida system developed ibm obtain precise array sections partial inlining array accesses 14 see section 6 irigoin et al developed pips system interprocedural analysis system part environment parallel programming 16 recently pips extended incorporate interprocedural array privatization 15 6 pips similar work lacks three important features 1 pathspecific interprocedural information obtained selective procedure cloning 2 interprocedural reductions 3 extensive interprocedural scalar dataflow analysis scalar privatization polaris system university illinois also currently developed advance state art parallelization technology 2 fundamental difference system polaris polaris performs interprocedural analysis instead relying full inlining programs obtain interprocedural information polaris group demonstrated good coverage results program parallelized obtained automatically although report full inlining feasible eight mediumsized programs approach difficulty parallelizing large loops containing thousands lines code commercial parallelizing compilers initial interprocedural analysis systems notably convex applications compiler performs flowinsensitive array analysis interprocedural constant propagation obtains pathspecific information inlining procedure cloning 21 applied parallel research demonstrated good speedup results programs presented programs parallelized programmer directives instruct compiler ignore dependences privatize certain vari ables know commercial system currently employs flowsensitive array analysis particularly interprocedural array privatization 6 empirical evaluation interprocedural parallelization analysis described previous sections implemented part stanford suif compiler section provides empirical evaluation results parallelization analysis collection benchmark programs previous evaluations interprocedural parallelization systems provided static measurements number additional loops parallelized result interprocedural analysis 13 14 18 24 compared results recent empirical stud ies examines spec89 perfect benchmark suites 14 considering loops containing calls set 16 programs suif system able parallelize greater five times loops 9 key difference two systems suif contains full interprocedural array analysis including array privatization reduction recognition see section 5 static loop counts however good indicators whether parallelization successful specifically parallelizing one outermost loop profound impact programs performance dynamic measurements provide much insight whether program may benefit parallelization thus addition static measurements benchmark suites also present series results gathered executing programs parallel machine present overall speedup results well measurements factors determine speedup also provide results identify contributions analysis components system focusing advanced array analyses 61 benchmark programs evaluate parallelization analysis measured success parallelizing three standard benchmark suites described table 1 fortran programs spec92fp sample nas benchmarks perfect spec92fp set 14 floatingpoint programs used benchmark uniprocessor architectures compilers omit four study parallelization analysis currently available tran omit alvinn ear two c programs spice program mixed fortran c code also omit fpppp contains type errors original fortran source program considered contain little looplevel parallelism programs presented alphabetical order program names nas suite eight programs used benchmarking parallel computers nasa provides sample sequential programs plus application information intention rewritten suit different machines use nasa sample programs except embar substitute embar version apr separates first call function initializes static data calls lastly perfect set originally sequential codes used benchmark parallelizing compilers present results 12 13 programs spice contains pervasive type conflicts parameter mismatches original fortran source violate fortran77 standard interprocedural analysis flags errors program considered little looplevel parallelism programs parallelized completely automatically system without relying user directives assist paral lelization made modifications original programs 1 programs produce valid results executed parallel 62 suif compiler system suif fully functional compiler takes fortran c input languages experiment consider fortran programs parallelized code output spmd single program multiple parallel c version program compiled native c compilers variety architectures resulting c program linked parallel runtime system currently runs several busbased shared memory architectures sgi challenge power challenge digital 8400 multiprocessors scalable sharedmemory architectures stanford dash kendall square ksr1 two major components automatic parallelization suif first analysis component locates available parallelism code component encompasses interprocedural parallelization analyses presented paper addition suif 1 except correct type declarations parameter passing arc2d bdna dyfesm mgrid mdg spec77 violated fortran 77 semantics program length description doduc 5334 lines monte carlo simulation lines equations motion wave5 7628 lines 2d particle simulation tomcatv 195 lines mesh generation ora 373 lines optical ray tracing lines equations motion single precision lines shallow water model su2cor 2514 lines quantum physics hydro2d 4461 lines navierstokes lines nasa ames fortran kernels nas appbt 4457 lines block tridiagonal pdes applu 3285 lines parabolicelliptic pdes appsp 3516 lines scalar pentadiagonal pdes buk 305 lines integer bucket sort cgm 855 lines sparse conjugate gradient embar 135 lines random number generator fftpde 773 lines 3d fft pde mgrid 676 lines multigrid solver perfect adm 6105 lines pseudospectral air pollution model arc2d 3965 lines 2d fluid flow solver bdna 3980 lines molecular dynamics dna dyfesm 7608 lines structural dynamics flo52 1986 lines transonic inviscid flow mdg 1238 lines moleclar dynamics water mg3d 2812 lines depth migration ocean 4343 lines 2d ocean simulation qcd 2327 lines quantum chromodynamics lines spectral analysis weather simulation track 3735 lines missile tracking trfd 485 lines 2electron integral transform table 1 benchmark programs cludes c pointer analysis support parallelization c programs outside scope paper second major component parallel code optimization generation specifically full suif system incorporates data loop transformations increase granularity parallelism improve memory behavior programs 1 27 optimizations eliminate unnecessary synchronization 25 paper however adopt simple parallel code generation strategy include optimizations order focus effects parallelization analysis compiler parallelizes outermost loop analysis proven parallelizable compiler suppresses parallelization array reductions overheads involved expected overwhelm benefits addition runtime system estimates amount computation parallelizable loop using knowledge iteration count run time runs loop sequentially considered finegrained parallelism benefit iterations parallel loop evenly divided processors time parallel loop spawned 63 applicability advanced analyses experimental framework currently support isolation contributions interprocedural scalar analyses know analyses important example performancecritical loops programs embar mdljdp2 ora spec77 would parallelized without one interprocedural scalar privatization scalar reduction recognition selective procedure cloning based interprocedural constants present static dynamic measurements assess impact array analysis components define baseline system serves basis comparison throughout section baseline refers system without advanced array analyses performs intraprocedural data dependence capability privatize arrays recognize reductions note baseline system much powerful existing parallelizing compilers contains interprocedural scalar analysis discussed section 41 631 static measurements table gives counts number loops suifparallelized program require particular technique parallelizable table count parallelizable loops including nested within parallel loops would consequently executed parallel parallelization strategy first column gives number loops parallelizable baseline system next three columns measure applicability intraprocedural versions advanced array analyses measure effect including reduction recognition privatization reduction recognition privatization respectively next set four columns interprocedural data dependence analysis similarly sixth eighth columns measure effect adding interprocedural reduction recog nition privatization reduction recognition privatization respectively see table advanced array analyses applicable majority programs benchmark suite several programs take advantage interprocedural array analyses although techniques apply uniformly programs frequency applicable relatively small set programs demonstrates techniques general useful observe many loops require new array techniques however loops parallelized advanced array analyses often involve computation shown make substantial difference overall performance 632 dynamic measurements also measure dynamic impact advanced array analyses contribution analysis component measured recording specific array analyses apply parallelized loop instrumenting sequential code determine execution time loops present execution times percentages total computation times figure 5c measurements taken running programs single processor 200mhz sgi challenge results reported relative terms applicable large class processors note even interprocedural analysis used parallelize say 100 computation mean noninterprocedural parallelizer find parallelism may parallelize inner loop term overall percentage time spent parallelized regions parallelism coverage overall observe rather good coverage 80 8 10 programs spec92fp 7 8 nas programs 6 12 perfect benchmarks third programs spend 50 execution time loops requiring advanced array analysis techniques graph also demonstrates important parallelizing single loop requiring one advanced analysis techniques example program mdljdp2 contains two loops requiring interprocedural reduction two loops program spends 78 time intraprocedural interprocedural array reduction p p p p array privatization p p p p doduc su2cor hydro2d 147 nas appbt 139 3 buk 4 fftpde mgrid 38 perfect arc2d 190 bdna 111 28 1 ocean track trfd table 2 static measurements number loops using technique suifparallelized loops execute long time also large largest loop suif parallelizes spec77 consisting 1002 lines code original loop invoked procedures loop contains 60 subroutine calls different procedures within loop 48 interprocedural privatizable arrays 5 interprocedural reduction arrays 27 arrays accessed independently loop illustrates advantage interprocedural analysis inlining parallelizing large programs instead loop fully inlined would contained nearly 11000 lines code 64 effectiveness advanced analyses section 63 establishes advanced techniques applicable many programs section addresses effectiveness tech niques provide quantitative data show techniques effective previous techniques parallelizing programs benchmark suites 641 metrics results parallel speedups measure overall effectiveness parallel system also highly machine dependent speedups depend number processors sensitive many aspects architecture cost synchronization interconnect bandwidth memory subsystem furthermore speedups measure effectiveness entire compiler system parallelization analysis focus paper exam ple techniques improve data locality minimize synchronization greatly improve speedups obtained thus precisely capture well parallelization analysis performs use two following metrics parallelism coverage coverage introduced section 632 important metric measuring effectiveness parallelization analysis amdahls law programs low coverage get good parallel speedup example even program 80 coverage ideal speedup 25 4 processors high coverage indicative compiler analysis locating significant amounts parallelism computation granularity parallelism program high coverage guaranteed achieve parallel speedup due number factors granularity parallelism extracted particularly important factor frequent synchronizations slow rather speed finegrain parallel computation quantify 10300408004080adm qcd mdg track bdna dyfesm arc2d trfd spec77 appbt applu appsp buk cgm embar fftpde mgrid tomcatv ora doduc mdljdp2 wave5 us us 100 us ms ms 100 ms 100 sec ocean parallelism coverage granularity parallelism applicable computation onprocessors 1 spec92fp 2 nas 3 perfect adm qcd mdg track bdna dyfesm arc2d trfd spec77 appbt applu appsp buk cgm embar fftpde mgrid tomcatv ora doduc mdljdp2 wave5 ocean adm qcd mdg track bdna dyfesm arc2d trfd spec77 appbt applu appsp buk cgm embar fftpde mgrid tomcatv ora doduc mdljdp2 wave5 ocean data dependence analysis array reduction array privatization array reduction array privatization procedural techniques baseline suif intraprocedural data dependence analysis array privatization arrayreduction interprocedural scalar analysis interprocedural scalar analysis data dependence analysis adm qcd mdg track bdna dyfesm arc2d trfd spec77 appbt applu appsp buk cgm embar fftpde mgrid tomcatv ora doduc mdljdp2 wave5 ocean figure 5 dynamic measurements suif baseline compiler property define programs granularity average execution time parallel regions figures 5b c show comparison parallelism coverage granularity achieved suif baseline compiler sake completeness also present set speedup mea surements programs benchmark suite relatively short execution times well fine granularities parallelism shown figure 5c programs cannot utilize large number processors effectively experiment run programs 4processor 200mhz sgi challenge speedups calculated ratios execution time original sequential program parallel execution time results shown figure 5d 642 discussion benchmarks figure 5b1 shows advanced array analyses dramatically increase parallelism coverage 3 10 programs words major loops require sophisticated array analyses contain loops parallelized using conventional techniques new parallel loops also rather coarse grained observed figure 5c1 overall compiler achieves good results parallelizing spec92fp coverage 80 8 10 programs speedup achieved 8 results also show coverage necessary sufficient high speedups programs fine granularity parallelism even high coverage su2cor tomcatv nasa7 tend lower speedups another important factor affects speedups data locality two programs tomcatv nasa7 poor memory behavior performance programs improved significantly via data loop transformations improve cache locality1 techniques minimize synchronization25 nas benchmarks advanced array analyses suif important successful parallelization nas benchmarks seen figure 5b2d2 comparing suif baseline system observe array analyses two important effects enable compiler locate significantly parallelism two programs cgm embar also increase granularity parallelism appbt appsp parallelizing outer loop instead inner loops nested inside observe seems like moderate improvement coverage appbtfrom 85 nearly 100is sig nificant difference corresponds change ideal speedup 275 4 4 processors improvements coverage granularity nas translate good speedup results six eight programs yield speedup two buks low coverage surprising implements bucket sort algorithm applu although high coverage finegrained yield speedup overall advanced array analyses important nas half benchmark suite would speed without techniques perfect benchmarks displayed figure 5b3d3 advanced array analyses significantly improve parallelism coverage bdna qcd bdna additional parallel loops provide reasonable granularity leads speedup granularity increased spec77 trfd speedup achieved case trfd although little parallel speedup observed spec77 improvement baseline system confirms validity preference outer loop parallelism whole suif doubles number programs achieve speedup 2 4 overall parallelization perfect successful two benchmark suites figure 5 indicates two basic problems half programs coverage 80 furthermore parallelism found rather finegrained parallelizable loops taking less 100 uniprocessor fact runtime system suppressed parallelization finegrained loops perfect results would much worse thus coverage low system exploit fraction parallelism extracted examine difficulties parallelizing perfect determine feasibility automatic parallelization identify possible future research directions found programs simply parallelizable implemented programs contain lot input output eg mg3d spec77 speedup depends success parallelizing io dusty deck features programs use equivalence constructs ocean obscure information analysis contrast spec92fp nas programs cleanly implemented thus amenable automatic parallelization many programs particularly ocean adm mdg key computational loops safe parallelize beyond scope techniques implemented suif ocean adm contain nonlinear array subscripts involving multiplicative induction variables beyond scope higherorder induction variable recognition always extensions automatic parallelization system improve effectiveness programs nonetheless fundamental limitation static parallelization programs cannot parallelized compiletime informa tion example main loop adm parallelizable problem size unknown compile time even promising solution program check loop parallelizable run time using dynamic information interprocedural analysis optimization play important part approach improving efficiency runtime tests derive highly optimized runtime tests hoist less frequently executed portions program possibly even across procedure boundaries interprocedural analysis system provides excellent starting point work area advanced analysis also form basis useful interactive parallelization system even analyses strong enough determine loop parallelizable results used isolate problematic areas focus users attention example compiler finds program qcd 617line interprocedural loop would parallelizable small procedure examination procedure reveals random number generator user potentially modify run parallel requesting little help user compiler parallelize loop perform tedious privatization reduction transformations automatically 643 summary table 3 summarizes impact improvements advanced array analyses coverage granularity speedup three benchmark suites first row contains number programs reported benchmark suite second row shows many programs coverage increased 80 adding advanced array analyses third row gives number programs increased granularity similar coverage result advanced array analyses fourth row shows significant improvements affect overall performance either improved coverage increased granularity 3 2fold speedup conclusions paper presented extensive experimental results using fully interprocedural automatic parallelization system demonstrated interprocedural array dataflow analysis array privatization reduction recognition key technologies greatly improve success automatic parallelization finding coarsegrain parallelism perfect number programs improved coverage increased granularity improved speedup 20 1 4 2 table 3 summary experimental results compiler increases parallelization coverage lowers synchronization costs improves speedups work discovered effectiveness interprocedural parallelization system depends strength individual analyses ability work together integrated fashion comprehensive approach parallelization analysis system much effective automatic parallelization previous interprocedural systems commercially available compilers programs analysis sufficient find available parallelism programs seems impossible unlikely purely static analysis could discover parallelismeither correct parallelization requires dynamic information available compile time difficult analyze cases benefit support runtime parallelization user inter action aggressive static parallelizer built provide good starting point investigate techniques acknowledgements authors wish thank patrick sathyanathan alex seibulescu contributions design implementation system rest suif group particularly jennifer anderson chris wilson providing support infrastructure upon system built r data computation transformations multiprocessors performance analysis parallelizing compilers perfect benchmarks programs range test dependence test symbolic methodology procedure cloning interprocedural array region analyses practical dependence testing symbolic analysis basis paral lelization interprocedural analysis parallelization interprocedural analysis parallelization design experience fiat framework interprocedural analysis transformation interprocedural symbolic analysis implementation interprocedural bounded regular section analysis empirical study precise interprocedural array analysis interprocedural analyses programming environments semantical interprocedural paralleliza tion overview pips project safe approximate algorithm interprocedural pointer aliasing efficient interprocedural analysis program restructuring parallel programs efficient way break multiloop dependence equations efficient exact data dependence analysis convex application compiler precise interprocedural data flow algorithm empirical investigation effectiveness limitations automatic parallelization direct parallelization call statements compiler optimizations eliminating barrier synchronization automatic array privatization improving locality parallelism nested loops tr direct parallelization call statements efficient interprocedural analysis program parallelization restructuring semantical interprocedural parallelization efficient exact data dependence analysis practical dependence testing delinearization safe approximate algorithm interprocedural aliasing improving locality parallelism nested loops range test empirical study precise interprocedural array analysis compiler optimizations eliminating barrier synchronization data computation transformations multiprocessors precise interprocedural data flow algorithm implementation interprocedural bounded regular section analysis performance analysis parallelizing compilers perfect benchmarks programs fiat automatic array privatization polaris symbolic analysis interprocedural array region analyses interprocedural analysis parallelization ctr michael g burke ron k cytron interprocedural dependence analysis parallelization acm sigplan notices v39 n4 april 2004 mary w hall jennifer anderson saman p amarasinghe brian r murphy shihwei liao edouard bugnion monica lam maximizing multiprocessor performance suif compiler computer v29 n12 p8489 december 1996 saman p amarasinghe jennifer anderson christopher wilson shihwei liao brian r murphy robert french monica lam mary w hall multiprocessors software perspective ieee micro v16 n3 p5261 june 1996 hwansoo han chauwen tseng pete keleher eliminating barrier synchronization compilerparallelized codes software dsms international journal parallel programming v26 n5 p591612 october 1998 e gutirrez plata e l zapata compiler method parallel execution irregular reductions scalable shared memory multiprocessors proceedings 14th international conference supercomputing p7887 may 0811 2000 santa fe new mexico united states manish gupta rahul nim techniques speculative runtime parallelization loops proceedings 1998 acmieee conference supercomputing cdrom p112 november 0713 1998 san jose ca byoungro sungdo moon mary w hall measuring effectiveness automatic parallelization suif proceedings 12th international conference supercomputing p212219 july 1998 melbourne australia manish gupta sayak mukhopadhyay navin sinha automatic parallelization recursive procedures international journal parallel programming v28 n6 p537562 december 2000 martin rinard pedro diniz eliminating synchronization bottlenecks objectbased programs using adaptive replication proceedings 13th international conference supercomputing p8392 june 2025 1999 rhodes greece shihwei liao amer diwan robert p bosch jr anwar ghuloum monica lam suif explorer interactive interprocedural parallelizer acm sigplan notices v34 n8 p3748 aug 1999 radu rugina martin rinard automatic parallelization divide conquer algorithms acm sigplan notices v34 n8 p7283 aug 1999 heidi e ziegler mary w hall pedro c diniz compilergenerated communication pipelined fpga applications proceedings 40th conference design automation june 0206 2003 anaheim ca usa edouard bugnion jennifer anderson todd c mowry mendel rosenblum monica lam compilerdirected page coloring multiprocessors acm sigplan notices v31 n9 p244255 sept 1996 sungdo moon byoungro mary w hall evaluating automatic parallelization suif ieee transactions parallel distributed systems v11 n1 p3649 january 2000 shihwei liao zhaohui du gansha wu gueiyuan lueh data computation transformations brook streaming applications multiprocessors proceedings international symposium code generation optimization p196207 march 2629 2006 pierre palatin yves lhuillier olivier temam capsule hardwareassisted parallel execution componentbased programs proceedings 39th annual ieeeacm international symposium microarchitecture p247258 december 0913 2006 yuanshin hwang parallelizing graph construction operations programs cyclic graphs parallel computing v28 n9 p13071328 september 2002 sungdo moon byoungro mary w hall combining compiletime runtime parallelization1 scientific programming v7 n34 p247260 august 1999 yuanshin hwang joel h saltz identifying parallelism programs cyclic graphs journal parallel distributed computing v63 n3 p337355 march martin c rinard pedro c diniz commutativity analysis new analysis framework parallelizing compilers acm sigplan notices v31 n5 p5467 may 1996 silvius rus guobin christophe alias lawrence rauchwerger region array ssa proceedings 15th international conference parallel architectures compilation techniques september 1620 2006 seattle washington usa yuan lin david padua compiler analysis irregular memory accesses acm sigplan notices v35 n5 p157168 may 2000 mahmut taylan kandemir compiler technique improving wholeprogram locality acm sigplan notices v36 n3 p179192 march 2001 sungdo moon mary w hall evaluation predicated array dataflow analysis automatic parallelization acm sigplan notices v34 n8 p8495 aug 1999 kathryn mckinley compiler optimization algorithm sharedmemory multiprocessors ieee transactions parallel distributed systems v9 n8 p769787 august 1998 heidi ziegler mary hall evaluating heuristics automatically mapping multiloop applications fpgas proceedings 2005 acmsigda 13th international symposium fieldprogrammable gate arrays february 2022 2005 monterey california usa mahmut taylan kandemir improving wholeprogram locality using intraprocedural interprocedural transformations journal parallel distributed computing v65 n5 p564582 may 2005 martin c rinard pedro c diniz eliminating synchronization bottlenecks using adaptive replication acm transactions programming languages systems toplas v25 n3 p316359 may martin c rinard pedro c diniz commutativity analysis new analysis technique parallelizing compilers acm transactions programming languages systems toplas v19 n6 p942991 nov 1997 g vranesic michael stumm leo budin analytical prediction performance cache coherence protocols ieee transactions computers v46 n11 p11551173 november 1997 michael beynon chialin chang umit catalyurek tahsin kurc alan sussman henrique andrade renato ferreira joel saltz processing largescale multidimensional data parallel distributed environments parallel computing v28 n5 p827859 may 2002 radu rugina martin c rinard symbolic bounds analysis pointers array indices accessed memory regions acm transactions programming languages systems toplas v27 n2 p185235 march 2005 mary w hall saman p amarasinghe brian r murphy shihwei liao monica lam interprocedural parallelization analysis suif acm transactions programming languages systems toplas v27 n4 p662731 july 2005