cut size statistics graph bisection heuristics investigate statistical properties cut sizes generated heuristic algorithms solve graph bisection problem approximately ensemble sparse random graphs find empirically distribution cut sizes found local algorithms becomes peaked number vertices graphs becomes large evidence given distribution tends toward gaussian whose mean variance scales linearly number vertices graphs given distribution cut sizes associated heuristic provide ranking procedure takes account quality solutions speed algorithms procedure demonstrated selection local graph bisection heuristics b introduction algorithms tackling combinatorial optimization problems 27 may divided two classes exact algorithms exhaustive search branchandbound branchandcut form first class determine exactly optimum cost function minimized however nphard problems require large computation ressources particular large computation times second class consists heuristic algorithms guaranteed find optimal lowest cost solution even solution close optimum practice find good approximate solutions fast problems science ones main interest optimal solution exact algorithm required however many engineering applications heuristic approach may preferable several reasons computational ressources simply insufficient solve instances interest exact methods ii cost function one wants minimize computationally demanding limited resources force one use approximate cost function instead rule rather exception complex systems vlsi true cost function cannot used little point finding true optimum wrong problem iii heuristic algorithms typically generate numerous good enough solutions thus providing information statistical properties low cost solutions information turn used generating better heuristics finding new criteria guiding branching exact algorithms branchandbound almost combinatorial optimization problem easy devise heuristic algorithms perform quite well probably many algorithms proposed date usually fall families popular local search simulated annealing tabu search evolutionary computation practitioner frequently confronted problem choosing method use thus would like rank algorithms determine one best instance set parameters completely specify cost function difficulty arises heuristic algorithms stochastic give many different solutions single instance general distributions solution costs generated different heuristics overlap winning algorithm varies one trial another furthermore necessary balance quality solutions found time necessary find since practice heuristics run different speeds final goal paper kind balancing section 8 shall introduce generally applicable ranking method based possibility performing multiple runs random starts algorithm allotted amount computer time exhausted ranking method determines whether better fast heuristic gives good solutions slower heuristic give better solutions establishing ranking single instance may needed real world problem useful prediction tool preferable consider effectiveness heuristic applied family instances since detailed knowledge distribution costs necessary ranking procedure major part paper depth study statistics costs found several classes heuristics nphard 9 combinatorial optimization problem chosen study graph bisection problem hereafter simply called graph partitioning problem gpp choice justified wide range practical applications gpp include host scheduling 3 memory paging program segmentation 17 load balancing 21 numerous aspects vlsidesign logic partitioning 12 placement 6 19 applications gpp used testing ground many heuristics work selection made view previous studies johnson et al 13 lang rao 20 berry goldberg 4 restricted study iterative improvement heuristics based local search simulated annealing made choice optimization problem algorithms remains define class instances testbeds ideally family instances reflect structure actual instances interest practitioner since particular application mind shall follow studies 13 20 4 consider ensemble sparse random graphs numerical study found heuristics tested share following properties random graphs become large algorithm characterized fixed percentage excess optimum cost ii partitions generated distribution costs becomes peaked within given graph across graphs iii distributions tend towards gaussians properties ranking heuristics large graphs largely determined mean variance costs found thus constant speedup factor small effect ranking expect property hold problems heuristics practical interest leading robust ranking paper organized follows section 2 define gpp well ensemble random graphs used testbed section 3 derives properties random partitions shows cut size statistics graph bisection heuristics 3 distribution cut sizes relative width goes zero instance size grows section 4 argue property hold also distribution costs found heuristic algorithms based local iterative processes section 5 discuss heuristic algorithms included tests section 6 gives mean standard deviation costs found function graph size distribution costs indeed found peaked leads first ranking however take account computation times implement speeddependent ranking must determine distribution cut sizes found different algorithms subject section 7 evidence given distribution typical graph tends towards gaussian limit large graphs section 8 present ranking method takes account quality solutions well speed heuristics section 9 finally discuss results conclude 2 minimum cuts graph partitioning graph bisection problem gpp defined follows consider graph e consists set n vertices set nonoriented edges connecting pairs vertices convenient introduce called connectivity matrix given connected v j since edges nonoriented discussed applies weighted graphs represent weight ij edge partition g given dividing vertices g two disjoint subsets v 1 v 2 number edges connecting v 1 v 2 called cut partition denoted c given gpp mincut problem consists finding partition cost 21 minimum subject given constraints sizes v 1 v 2 gpp nphard 9 standard formulation shall restrict work v 1 v 2 equal sizes study necessary fix ensemble graphs testbed chosen gn p ensemble random graphs n vertices edge present probability p choice gn p justified tractable mathematical properties fact many workers 13 20 4 used graphs ensemble test heuristics problem finding properties minimum cut size graphs belong ensemble sometimes called stochastic gpp let us review known results problem serve motivate conjectures behavior cuts obtained heuristics graph g call c 0 minimum cut size taking g ensemble gn p c 0 random variable following derivations standard number stochastic combinatorial optimization problems cop possible show using azumas inequality 1 distribution c 0 becomes peaked n 1 means n becomes large relative fluctuations mean tend zero property often referred selfaveraging typical processes many terms contribute certain stochastic cop possible show mean minimum cost satisfies power scaling law n c 0 n fl converges probability limiting value n 1 case stochastic gpp proof property hold nevertheless believed scaling holds within gn p ensemble p fixed calculations show c 0 n 2 p4 probability one 8 shown next section also limiting behavior random cuts ensemble p fixed challenging one heuristics reason uninteresting scaling high number edges connecting vertex thus consider work ensemble gn p ff mean connectivity number neighbors vertex graphs graphs sparse contrast dense graphs obtained taking p independent n consider optimal partition typical vertex v 1 finite fraction edges connect vertices v 2 vertex contributing o1 amount cut size c 0 expected grow linearly n since c 0 n known peaked large n natural conjecture stronger property c 0 n tends towards constant probability one n 1 major motivation work expectation identical scaling law hold replace c 0 cost found 4 gr schreiber oc martin heuristic algorithm albeit limiting constant depends heuristic motivate property next section analyzes cut sizes random partitions section 4 consider statistical physics gpp interpolate case minimum cuts random cuts 3 cuts random partitions show explicitly large n scaling law holds cut sizes random partitions asymptotically random cuts gaussian distribution relative variance proportional 1n consider graph gn p one always write cut size random partition x mean random cut size graph consideration hy average random partitions averaging explicitly balanced partitions fixed graph find 1 interpretation formula simple edge weight e ij probability cut ensemble gn p random graphs easy calculate first moments x particular find denotes average ensemble gn p also see x sum independent random implies kth cumulant connected moment distribution x statisfies c c large n c constant p ensemble c ffn p ffn ensemble random variable subtle sum correlated variables nevertheless graph possible compute moments done explicitly second third moments expressions long given average 2 random partitions gn p obtain calculations get significantly complicated higher moments order keep simple expressions limit ensemble 1 find furthermore graph graph fluctuations hy 2 become negligible relative magnitude ratio typical variance mean variance goes 1 large n however true higher moments instance find typical value hy 3 grows n 12 taking addition mean graphs leads n independent behavior finally one show hy k c hy 2 probability one shows n 1 gaussian distribution zero mean variance growing linearly n whose coefficient graph independent coming back cut size random partition find normalized correlation coefficients powers x tend zero large n thus x become independent random variables limit along results previously derived shows large n c gaussian distribution results deduce large n behavior relative deviations mean go zero thus distribution c becomes peaked probability one n 1 convergence distribution cn delta function referred selfaveraging c cut size statistics graph bisection heuristics 5 scaling variances summarized large n writing x independent gaussian random variables zero mean unit variance oe standard deviation rescaled 1 n x oe ff8 thus oe describes fluctuations cut sizes within graph oe x describes fluctuations mean cut size graph graph used analytical results test validity computer programs first two moments x allowed us test generation random graphs gn p similarly check random number generator obtained verifying several graphs second moment found numerics agreement formulae finally also checked random cut sizes limiting gaussian distribution third moment scales zero large n check performed random partitions 100 000 graphs 4 statistical physics gpp saw cut sizes random partitions gn p selfaveraging property conjectured property also holds minimum cut possible interpolate two kinds partitions random mincut following formalizm statistical physics given graph consider boltzmann probability distribution pb defined arbitrary partition p cut size cp e gammacp z z chosen pb normalized probability distribution arbitrary positive parameter called temperature 1 recover ensemble random partitions partitions equally probable 0 ensemble reduces partitions minimum cut size intermediate values temperature partitions weighted according exponential cut size boltzmann ensemble one define moments cut sizes done case random partitions statistical physics problems possible show quantity exponential eq 41 cut size selfaveraging random graphs however proofs inapplicable nevertheless evidence indicates cut size selfaveraging temperature 26 selfaveraging understood qualitatively low temperature follows number n c partitions cut size c sharply increasing function c whereas boltzmann factor sharply decreasing function c note probability distribution p c c given product two functions using naive standard statistical physics arguments n c one finds p c peak c grows linearly n width distribution n gives selfaveraging property c addition kind argument says becomes gaussian large n result usually correct statistical physics systems number statistical physics results obtained gpp ensemble dense random graphs ie gn p p fixed particular highly technical calculations 26 8 indicate cut sizes selfaveraging temperatures n 1 relative fluctuations within fixed graph become negligible well graph graph mean cut size given n 1 mean graphs performed formula remains valid almost sequences graphs n 1 equation ut function temperature dependence p long p independent n limit 0 gives expected typical value minimum cut 03816 although proof yet calculations exact general agreement statistical physics community results correct case sparse random graphs p 1n also studied within statistical physics approach 2 5 far however problem proven intractable plausible solution sight nevertheless expected cut sizes selfaveraging temperature mean distribution scales linearly n large n 6 gr schreiber oc martin property selfaveraging seems quite generic reason hold systems cut size partition sum large number random variables correlated plausible cut size selfaveraging whenever partitions generated iterative process involving vertices time local search methods modifications thereof simulated annealing fall category thus claim heuristic algorithm generates partitions iteratively according local vertex space criteria lead cut sizes selfaveraging thus distribution cut sizes found heuristic become peaked n 1 furthermore limit distribution converge towards gaussian way given central limit theorem see sections follow indeed born empirically heuristics investigated arguments presented specific graph partitioning problem expect apply stochastic cops many variables cost function surprisingly little research topic context nk model binary variables study kauffman levin 16 found costs local minima became peaked towards value random cost n grew peculiar property due structure energy landscape model however concerning behavior heuristic solutions research almost exclusively focused case euclidean traveling salesman problem points laid plane practitioners field know local search heuristics give rise costs whose relative variance decreases number points increases furthermore observed johnson mcgeoch 14 among others costs tend towards fixed percentage excess optimum purpose show convergence occurs albeit different combinatorial optimization problem provide theoretical framework understanding behavior comes also pay special attention distinction fluctuations within instance one instance another believe findings quite general particular ensemble instances considered need based points physical space 5 algorithms used testbed view previous arguments restricted local heuristics without trying complete representative studied statistics cut sizes three types local search four versions simulated annealing algorithms section sketch workings heuristics sections 6 7 show selfaveraging properties hold algorithms spite significant differences thus reason believe claims affected details algorithms rather properties likely generic dynamics local 51 kernighanlin kl simple local search one performs elementary transformations feasible solution cop long decrease cost procedure sometimes called opting 22 sophisticated version consists using variable depth search one builds sequence elementary transformations usually according greedy criterion p set ahead time depends sequence costs found elementary transformations imposed decrease cost sequence length p must applied current solution procedure first proposed kernighan lin 18 fact framework gpp hereafter refer algorithm kl elementary transformation use exchange pair vertices one vertex v 1 exchanged one v 2 sequence exchanges built greedy tabu fashion performing sweep vertices step sweep one finds best largest cost gain pair exchange among vertices yet moved sweep tabu condition sweep length n2 sweep finished one finds position along sequence exchanges generated cut size minimum minimum leads improved partition transformation p exchanges performed partition another sweep initiated otherwise search stopped partition klopt ie local minimum kl kl algorithm deterministic although possible introduce stochasticity break degeneracies selecting best pair exchange computational complexity easy estimate number sweeps known advance generic difficulty estimating speed iterative improvement heuristics however practice one finds kl finishes small number sweeps thus computational complexity estimated times performing last sweep known checkout sweep study used implementation kl 24 uses heaps find best pair exchange step sparse graphs leads operations per sweep nearly identical kl provided chaco software package cut size statistics graph bisection heuristics 7 gives sensibly identical results faster implementation algorithm given fiduccia mattheyses 7 whenever use radix sort possible time sweep terms quality solutions found kl quite good surprising although kernighan lin proposed method 20 years ago kl remains relatively unchallenged least general purpose method applicable kind graph regardless structure course special kinds graphs meshes heuristics eg spectral bisection perform better 4 11 13 15 52 multilevel klalgorithm chaco chaco software package includes number heuristics partitioning graphs information package see chaco users guide 10 purposes used multilevel generalization kl hereafter referred simply chaco chaco algorithm based coarse graining compactification graph partitioned level vertices paired using matching algorithm paired vertices considered vertices next higher level compactification process necessary weighted edges weights also propagated higher level compactification repeated sufficiently small graph obtained spectral bisection applied get first partition partition used starting partition kl graph level process recursive one obtains klopt partition original graph note construction deterministic require initial random partition multilevel strategy successful unstructured 2 3 dimensional meshes 11 15 terms solution quality much better kl alone terms speed much faster kl hierarchical nature however usefulness chaco random graphs priori obvious terms speed quality solutions 53 simulated annealing algorithms choosen third comparative algorithm simulated annealing sa sa based set elementary moves like local search moves increase cost accepted low probability sometimes appropriate consider sa noisy local search method simulated annealing really family algorithms include different bells whistles proposed algorithm considered four variations sa first introduced kirkpatrick et al 19 referred fsa initial final temperatures fixed ahead time user predetermined number trial moves performed temperature ii kirkpatrick et al also proposed determine initial final temperatures schedule dynamically set initial temperature beginning run using criterion 80 trial moves accepted tempera ture similarly stop cooling 5 cooling steps energy decrease refer method ksa iii johnson et al 13 improved speed algorithm allowing early exit next temperature schedule condition proposed exiting accepted minimum number moves also modified termination criterion acceptance rate less threshold value refer version jsa three sa methods use exponential cooling schedule cooling factor 095 iv last sa variation consists using adaptive schedule whereby next temperature value determined fly according energy fluctuations current temperature choosen variation implementation van laarhoven aarts 28 29 obtain good results one would spend long time freezing phase cooling since would increase computation times significantly choosen use finetuned adaptive schedule one provides cooling factor magnitude sa algorithms presented allows us similar computation times simulated annealing algorithms investigated sa one use elementary moves local search ie gpp pair exchanges however low cost partition obtained take long time lot luck find good exchanges finding good pair best done finding first vertex transfer second ie using sequential process suggests relaxing constraint balanced partitions replacing penalty function keeps sizes v 1 v 2 nearly equal small offbalance followed slightly different approach move destroying balance must followed move restoring balance markov chain explores partitions balanced offbalance sigma1 easy see method equivalent cost partitions equal infinity fixed temperature long chains one generates partitions cut sizes given boltzmann factor within constraint offbalance indeed succession acceptreject decisions makes global probability distribution boltzmannian enlarged space guarantee convergence properties standard case 8 gr schreiber oc martin remarks concerning implementations order first fixed temperature perform certain number sweeps sweep every vertex sequentially considered candidate changing sides partition move violate limit offbalance move rejected fact simply considered sweep thus requires operations sweeps use random permutations rather fixed random ordering vertices use random permutations according certain authors 13 28 29 result enhancement quality solutions found second maximum number sweeps temperature set ff implementations fsa ksa fact actual number sweeps computational complexity offn times number temperature steps used cases jsa asa difficult evaluate practice find jsa faster ksa constant factor asa hand spends quite lot time intermediate temperatures n increases empirically found 32 complexity terms quality aware systematic study sparse random graphs previous sa work gpp van laarhoven aarts used adaptive decrement rule 28 29 claim gain 13 simpler nonadaptive algorithms also compared results algorithm used johnson et al gpp claimed enhancement 5 jsa kernighanlin algorithm small gain found johnson 13 according van laarhoven aarts 28 29 due use nonadaptive choice temperature decrement rule however found sparse random graphs different variants simulated annealing nearly indistinguishable terms quality solutions may due using penalty term different nature graphs used present study 54 chainedlocaloptimization clo chainedlocaloptimization clo strategy synthesis local search simulated annealing 25 essential idea simulated annealing sample solutions locally optimal solutions strategy guaranteed least good local search successfully applied traveling salesman problem 23 partitioning unstructured meshes 24 work use kl local search engine given initial klopt partition p simplest implementation clo apply perturbation kick modify significantly partition practice means exchanging clusters vertices ii run kl modified partition reach new klopt partitionp f iii apply acceptreject procedure going initial partition final one p f defines analogue one move simulated annealing algorithm except many modifications partition occured single step temperature may modified according schedule desired simplicity set temperature zero runs discussed context simulated annealing inefficient exchange vertices clusters simultaneously better sequentially present clo algorithm thus proceeds follows given p initial balanced klopt partition choose connected cluster p vertices v 1 move v 2 respectively v 1 kloptimize partition generate intermediate balanced partition choose cluster p vertices v 2 modified partition generate p f final balanced partition whole procedure simulated annealing step apply acceptreject criterion going p p f runing clo irregular meshes 24 possible perform large kicks exchanging many vertices unfortunately sparse random graphs find acceptance becomes low thus used small kicks creating clusters sizes varying randomly 3 13 given small kicks kl usually terminates 2 sweeps speed clo per kick half kl consider limit large n using analogy simulated annealing fixed n independent number small kicks used expected clo perform better kl thus chosen use number kicks scales linearly n namely n choice course influences quality solutions generated larger value giving priori better results computational complexity algorithm order n 2 logn 6 selfaveraging cut size rest paper study statistical properties cut sizes generated algorithms described section 5 applied random initial partitions ensemble graphs used random graphs mean connectivity section 2 value chosen much larger connectivities ratio best worst cut size approaches 1 lower connectivities algorithms taking explicit advantage cut size statistics graph bisection heuristics 9 disconnected parts graph outperform general purpose heuristics order minimize effects associated finite sample graphs ensemble benchmarked algorithms graphs number graphs used production runs 10 000 values n ranging 50 200 however chaco algorithm fast also performed runs 100 000 graphs heuristic purpose section give numerical evidence distribution cut sizes becomes peaked limit large graphs heuristics considered properties distribution given section 7 find algorithm generates cut sizes mean variance scale linearly n behavior clear distribution cut sizes becomes peaked large n ie cut sizes selfaveraging also assuming cf section 2 minimum ie optimum cut size scales linearly n large n see heuristic algorithm leads fixed percentage excess true optimum note worst cut size also linear scaling n percentage excess provides first ranking algorithms however take account speed execution ci cut obtained heuristic graph g initial partition define mean cut per vertex averages initial partitions ensemble graphs studied cf section 3 notation compute ensemble averages numerically using standard estimator hereafter overlines refer numerical averages approximation due statistical error e associated fluctuations ci difficult see problem one need perform average using finite number r partitions graph g provides unbiassed estimator furthermore statistical error e sensitive r making numerically inefficient take large value r performed numerical averages leads simple expression e statistical error c ci figure 61 shows dependence c 1n error bars small visible also order avoid cluttering figure included among simulated annealing algorithms ksa implementations simulated annealing give nearly identical results algorithms figure suggests limiting large n value c convergence limit linear 1n thus fitted data linear function values b coefficients obtained fits given table 61 2 values show fits good identical analysis performed variance cuts found different algorithms figure 62 shows dependence n rescaled quantity scaling n apparent c summary data lead us conclude mean variance c scale linearly n large n relative width distribution c proportional 1 showing distribution cut sizes becomes peaked algorithms investigated one also say distribution ci mn tends towards delta function n 1 mean self averaging since fluctuations ci include graph graph fluctuations fluctuations gr schreiber oc martin chaco ksa kl clo fig 61 scaled mean cut sizes different algorithms algorithm b excess ksa 04485 495 000 asa 04499 496 032 jsa 04513 488 063 clo 04568 485 18 chaco 04802 581 71 kl table estimates large n value slope mean cut size per vertex percentage excess relative ksa heuristic within graph conclude relative fluctuations within fixed typical graph necessarily also go zero nb although runs use observable unbiassed estimator ci includes types fluctuations thus large n limit algorithm give fixed percentage excess minimum almost graphs almost random initial partitions speed independent ranking since algorithm characterized percentage excess introduce ranking different heuristics according excess large n limit course ranking take account speed algorithms graphs implementation different heuristics winners class simulated annealing best ksa using reference rather true min cut size unknown jsa excess 063 asa excess 032 fsa excess 008 next best heuristic cloalgorithm followed chaco finally kl results excesses given table 61 also included general interest excess obtained zero temperature simulated annealing 1821 note gives much less good results kl true simulated annealing gives much better results kl comment let us remark relative solution quality algorithms determined higher precision absolute quality simply put cut sizes obtain different algorithms correlated performed graphs statistical error instance 32 times smaller statistical error alone possible give reliable values excesses different simulated annealing algorithms even though solution quality similar nevertheless ranking simulated annealing algorithms without ambiguity fsa algorithm larger n within statistical error ksa algorithm hence strong evidence one better algorithms easily ranked kl chaco 96 71 worse ksa clo 18 worse comparison kl qualitatively though quantitatively similar cut size statistics graph bisection heuristics 11 ksa kl clo chaco fig 62 scaled variance cut sizes different algorithms given johnson et al 13 van laarhoven aarts 29 claimed gain saalgorithm klalgorithm 5 13 respectively differences results several origins first performed average ensemble graphs second graphs slightly different characteristics ones use third introduced penalty term implementation simulated annealing probably affects quality solutions found 7 distribution cut sizes section deepen statistical study c shown previous section distribution cn tends towards delta function natural ask limit reached understand nature intra intergraph fluctuations convenient use framework introduced section 3 random partitions replaced partitions found applying one heuristics random start graph g initial partition define hy xi average cut size found graph g gives fluctuation cut size mean graph heuristics study indicates large random graph g nearly gaussian distribution width distribution essentially independent study distribution large n show width selfaveraging relative asymmetry goes zero finally evidence x become independent variables large n properties lead fast robust ranking heuristics section 8 figure 71 shows distribution cut sizes found kl one graph chosen random gn p 1 superposed gaussian mean variance figure gives good evidence distribution graph close gaussian obvious question whether distribution similar across different graphs heuristics find answer yes indicated following study moments note chaco algorithm default parameter setting generates initial starting partition deterministically application coarse graining strategy spectral method applied since random initial partition fluctuations cut size function little section applies chaco parameter settings quantify oe 2 mi varies graph graph measured mean variance first measured ensemble averages n heuristic data extrapolates limiting value n becomes large comparing results mean cut size find algorithms lead best cut sizes also smallest widths distribution second studied variance oe 2 ie oe 2 delta study requires high statistics performed high accuracy kl fastest algorithms however algorithms show qualitatively behavior figure 72 displays kl 1n dependence relative variance oe 2 ie intergraph variance oe 2 divided square mean seen figure ratio goes zero large n showing oe 2 selfaveraging simply put gr schreiber oc martin frequency cut sizes fig 71 histogram kl cut sizes one graph overlaid gaussian means width distribution relative fluctuations graph graph dissapear n 1 lower statistics data heuristics consistent fig 72 relative variance intragraph cut size variance oe 2 following statistical physics analogy given section 4 reason believe distribution tends towards gaussian case random partitions test conjecture measured asymmetry distribution numerous graphs kl first find typical asymmetry small mean third moment satisfies n 1 second checked average squared asymmetry also small ie 0 properties give strong evidence distribution graph tends towards gaussian zero mean variance n 1 depends heurisitic actual graph distribution xi studied similarly previous section gave mean function n also showed selfaveraging interest quantify decrease n relative variance found distribution x roughly compatible gaussian distribution width proportional n algorithms unfortunately quantitive test requires high statistics however distribution xi essential ranking procedure clear next section studied greater depth cut size statistics graph bisection heuristics 13 finally completely specify statistics ci necessary describe correlations xi found numerically variables nearly uncorrelated particular correlation xi oe 2 tending towards zero n 1 assuming holds x gaussian distribution distribution ci also gaussian measurement asymetry jointly ci compatible property large n total variance given sum variances x summarized mathematically introducing two gaussian random variables x zero mean unit variance modeling rescaled cut size following sum oe oe equation exact analogue derived cut sizes random partitions see eq 35 8 speed dependent ranking heuristics section come back initial motivation work namely necessity comparing heuristics different speeds possibility relevant combinatorial optimization problems local search quite fast simulated annealing notoriously slow meaningful ranking must determine whether better fast heuristic gives good solutions slower heuristic giving better solutions show introduce ranking considering first one graph generalize ensemble graphs finally illustrate ranking gives case heuristics testbed applied sparse random graphs case one graph consider single graph g one provide ranking number heuristics give various cut sizes run different speeds take account speed algorithms quality solutions generate fix amount computation time allotted per algorithm call time measured instance cpu seconds given machine heuristic generates nonoptimal solutions time using multiple random initial starts suppose speed algorithm interest k independent starts performed allotted time shall assume execution time insensitive random initial start case practice heuristics knowledge speed algorithm gives value k used start output bestfound cost output end k starts best k costs hereafter called bestofk different algorithms ranked basis ensemble mean bestofk value k depending algorithm ensemble average average random numbers used random initial starts running algorithms establishes ranking particular graph given amount computation time inefficient perform average mentioned direct way ie extracting values bestofk many multiple runs far better compute average starting distribution bestfound cut sizes associated single random starts call p c probability finding bestfound cut size value c qc associated cumulative distribution ie probability finding cut size strictly smaller c since cut sizes integer valued qc introducing analogous probabilities bestofk values one distribution bestofk thus generated bestfound c mean bestofk easily extracted construction explains studied distribution single cut sizes section 7 note also possible extract c whole range values essentially extra work since affects k determination mean bestofk represents negligible amount work distribution bestfound known quantity c effect quantitative measure effectiveness algorithm course c depends amount computation ressources allotted ie increases k increases jumps unity c decreases broader distribution bestfound faster decrease c useful perform multiple runs establish ranking simply order algorithms according c general ranking may depend clearly sensitive lower tail distribution bestfound let us 14 gr schreiber oc martin illustrate considering instance two heuristics h 1 h 2 two overlapping distributions bestfound averages satisfying hc h1 mean h 1 seems better h 2 h 2 significantly faster tail distribution extends well domain ch1 one c h1 h 2 may effective algorithm assuming course large enough indeed h 2 run multiple times general properties may derived assuming instance ch1 ch2 described distribution shifted respect one another tail distribution falls exponential faster h 2 become effective h 1 1 ranking ensemble graphs extension ranking ensemble graphs straightforward assume c known graph g heuristic c real number measure effectiveness heuristic graph given amount computation time generalize measure one graph ensemble graphs considering mean c relevant ensemble final ranking simply given ordering algorithms according mean effectiveness expectation relatively homogeneous ensemble effectiveness thus ranking nearly essentially sufficiently large graphs average behavior also typical behavior expect happen whenever distribution cut sizes associated different heuristics overlap much pattern regardless graph occurs case ensemble random graphs indeed saw algorithm leads fixed percentage excess cost large n distribution costs peaked two algorithms non overlapping distributions give rise percentage excess clear large n mean ranking typical ranking also clear increasing amount computer resources thus speeding algorithm keeping quality solutions little improve ranking illustration value n follow procedure given obtain c different heuristics interest given graph g repeat many graphs gn p however number possible speedups case statistical properties derived previous sections first although principle bestofk construction repeated graph results section 7 provide shortcut since distribution bestfound high accuracy gaussian possible map mean bestfound bestofk mapping shift kdependent number standard deviations second noting fixed n variance gaussian well speed algorithm essentially constant graph graph calculate average graphs terms cpu time necessary find one bestfound ii mean cut size iii variance intragraph cut sizes mi graph independent large n quantities measured number values n fits performed interpolate arbitrary values n fits possible compute analytically values values n particular winning algorithm first ranking define regions n space given heuristic winner leading diagram figure 81 construction diagram included jsa ranking fsa ksa asa choice parameters simulated annealing algorithms tested give similar quality solutions jsa slightly faster although effectiveness sa algorithms nearly identical ranking depends n discrete jumps k whenever one algorithm increases k others may change ranking diagram figure 81 labeled different regions according associated winner indicated boundaries separating discrete nature k smoothed curves labeling sa fact corresponds jsa cpu time expressed multiples cpu cycles give units machine independent less technical meaning enough say lower boundary chaco region corresponds time chaco needs run diagram see large n given enough cpu time best algorithm simulated annealing simply mean excess cost lower algorithms limit distributions cut sizes overlap little ranking relatively insensitive algorithms speed using multiple random starts little improve quality solutions found fluctuations mean become negligible smaller values n fluctuations arising cut size statistics graph bisection heuristics 150 clo sachaco kl0 cpu time5000fig 81 ranking diagram different random starts negligible faster algorithms outperform simulated annealing using best k runs compare kl chaco clo see clo bit slower leads substantially better solutions winner amount cpu time enough run algorithms competitive neither clo simulated annealing terminate run explains kl region nearly invisible squeezed chaco region clo sa region note random graphs chaco slower kl ii initial partition set deterministically within default settings chaco bestfound bestofk values identical 9 discussion conclusions studied statistics cut sizes generated graph partitioning heuristics within given graph ensemble graphs motivated statistical physics analogy happens random partitions section 3 obtained strong numerical evidence cut sizes generated sparse random graphs selfaveraging ie distribution becomes peaked number vertices n becomes large quantitatively simply means relative fluctuations mean tend tend zero n 1 mean cut size found linear dependence n indicating heuristic leads fixed percentage excess cut size true minimum expect analogous properties hold local heuristics applied combinatorial optimization problem variable coupled others also investigated distribution cut sizes approaches limiting large n behavior gave evidence typical graphs distribution cut sizes generated becomes gaussian n 1 limit heuristic characterized mean cut size graphs variance describing fluctuations cut sizes typical graph variance seems scale linearly n large n limit selfaveraging also principal motivation work introduce method rank heuristics taking account quality solutions found speed algorithms knowledge distribution cut sizes allows one establish meaningful ranking heuristics assuming algorithms may applied k different random starts best k runs giving final cost although ranking done brute force used properties described demonstrate heuristics testbed large values n n 700 winner almost always simulated annealing fact large n distributions associated algorithms tested overlap significantly use multiple runs explore tail distributions effective smaller values n faster algorithms competitive find winner clo except allotted time short running even one run clo since graph graph fluctuations variance cut sizes found small ranking mean also almost cases ranking individual graphs thus robust gr schreiber oc martin number questions remain open one characterize distribution xi mean cut size graph extent similar properties hold heuristics manifestly local information found help generate better heuristics concerning last question worth pointing although simulated annealing general purpose method outperforms heuristics specifically developped graph partitioning problem suggests improvements methods might obtainable suitable modifications 10 acknowledgement indebted bruce hendrickson robert leland providing us software package chaco 20 also thanks w otto n sourlas stimulating discussions grs acknowledges support individual ec research grant contract number erbchbict941665 ocm acknowledges support institut universitaire de france fur thermore grs would like express gratitude professor jm gomez gomez generous hospitality department theoretical physics universidad complutense de madrid part work accomplished r weighted sums certain dependent random variables graph bipartitioning statistical mechanics partitioning strategy nonuniform problems multiprocessors path optimization graph partitioning problems replica symmetry breaking finite connectivity systems large connectivity expansion finite zero temperature procedure placement standardcell vlsi circuits lineartime heuristic improving network partitions application statistical mechanics npcomplete problems combinatorial optimization computers intractability guide theory npcompleteness chaco users guide version 20 partitioning vlsi circuits systems optimization simulated annealing experimental evaluation traveling salesman problem case study local optimization fast high quality multilevel scheme partitioning irregular graphs towards general theory adaptive walks rugged landscapes graph partitioning problems related program segmentation efficient heuristic procedure partitioning graphs optimization simulated annealing empirical evaluation empirical study static load balancing algorithms computer solutions traveling salesman problem partitioning unstructured meshes load balancing glass theory beyond algorithms complexity general approach combinatorial optimization problems tr ctr andrew e caldwell igor l markov toward cadip reuse web bookshelf fundamental algorithms ieee design test v19 n3 p7281 may 2002 angel vassilis zissimopoulos hardness quadratic assignment problem metaheuristics journal heuristics v8 n4 p399414 july 2002 andrew e caldwell andrew b kahng andrew kennings igor l markov hypergraph partitioning vlsi cad methodology heuristic development experimentation reporting proceedings 36th acmieee conference design automation p349354 june 2125 1999 new orleans louisiana united states andrew e caldwell andrew b kahng igor l markov design implementation movebased heuristics vlsi hypergraph partitioning journal experimental algorithmics jea 5 p5es 2000 olivier c martin rmi monasson riccardo zecchina statistical mechanics methods phase transitions optimizationproblems theoretical computer science v265 n12 p367 08282001