scalable stop broadcasting messagepassing mpps abstractin stop broadcasting processors pprocessor machine contain message broadcast processors 1 sp present number different broadcasting algorithms handle ranges show performance algorithm influenced distribution source processors relationships distribution characteristics interconnection network intel paragon show algorithm machine dimension exist ideal distributions distributions performance degrades cray t3d also demonstrate dependencies distributions machine sizes reduce dependence performance distribution sources propose repositioning approach approach initial distribution turned ideal distribution target broadcasting algorithm report experimental results intel paragon cray t3d discuss scalability performance b introduction broadcasting messages basic communication operation coarsegrained message passing massively parallel processors mpps standard broadcast operation one processor broadcasts message every processor various implementations operation architectures different machine characteristics proposed 5 9 12 13 14 another wellstudied broadcasting operation alltoall broadcast every processor broadcasts message every processor 3 7 8 15 let p number proces sors assume p processors call source processors contain message broadcast every processor 1 p paper present broadcasting algorithms handle ranges report experimental results sto research supported part arpa contract dabt6392c0022onr views conclusions contained paper authors interpreted representing official policies expressed implied us government broadcasting algorithms intel paragon discuss scalability performance general quantities influencing scalability thus choice algorithm gives best per formance include number processors message sizes number source processors 10 algorithms scalable respect p message sizes ie maintain speedup parameters change stop broadcast ing factors influence scalability well fixed particular algorithm exhibits different behavior depending source processors located algorithm ideal distribution patterns distribution patterns giving poor performance poor distribution patterns one algorithm ideal another thus location source processors relationship locations size dimensions architecture effect scalability algorithm order study relationships fullest extent assume every processor knows position source processors size messages implies synchronization occurs broadcasting paper describe number different broadcasting algorithms investigate algorithm good bad distribution patterns characterize features stop broadcast algorithms perform well wide variety source dis tributions algorithms tailored towards meshes others based architectureindependent approaches show algorithms ffl exhibit fast increase number processors actively involved broadcasting process ffl increase message length processors slowly possible give best performance show achieving two goals difficult regular machine sizes ie machines whose dimensions power 2 turn implies good bad input distributions cannot characterized pattern alone dimension machine plays crucial role well performance obtained ideal distributions vary greatly obtained poor distributions propose approach repositioning sources guarantee good performance basic idea perform permutation transform given distribution ideal distribution particular algorithm invoked perform actual broadcast paper organized follows section 2 describe algorithms reposition sources section 3 discuss different repositioning approaches section 4 describes different source distributions consider section 5 discuss performance scalability proposed algorithms intel paragon section 6 concludes algorithms without reposi tioning section describe stop broadcasting algorithms reposition sources first class broadcast algorithms generalizes efficient 1top broadcasting approach stop broadcasting could done one source processor initiate 1top broadcast ever broadcasting processes take place without interaction inefficient approach let processor initiate broadcast whenever messages different sources meet proces sor messages combined broadcasting steps proceed thus larger messages use binomial heap broadcasting tree 6 9 guide broadcasts algorithm br lin view processors mesh forming linear array using snakelike rowmajor indexing existence linear array required approach architectureindependent processors p contain message broadcast exchange messages form larger message consisting original received message one processors contains message sends one algorithm br lin proceeds recursively first p2 last p2 processors algorithms br lin behaves differently different machine sizes whether number processors actively involved broadcasting process increases depends source processors located example input distribution consists columns first log p2 iterations introduce new sources meshes odd number rows new sources introduced case column distribution order study use column links row links single iteration arbitrary mesh sizes introduce algorithm br xy algorithm br xy first select either rows columns assume rows selected view row linear array invoke algorithm br lin within row invoke algorithm br lin within column consider two versions algorithm br xy differ dimensions selected algorithm br xy source number sources rows columns determine order dimen sions recall every processor knows positions sources every processor determines maximum number sources row maximum number sources col umn rows selected algorithm br lin invoked rows otherwise columns selected first reason choosing dimensions order following rows contain fewer elements broadcasting done within rows likely generate messages smaller size broadcast within columns assume sources located say ff columns r number rows mesh first broadcasting rows results every processors containing ff messages time column broadcast starts sake comparison also consider version algorithm br xy compares dimensions broadcasts first along longer dimension assume mesh consists r rows c columns algorithm br xy dim selects rows r c columns r c algorithms described far processors issue sends receives facilitate communication make use existing communication operations generally available communication libraries 1 2 7 stop broadcasting easily stated terms known communication operations considered two approaches first one algorithm xor invokes alltoall personalized exchange communication 7 second approach results algorithm 2step algorithm performs broadcast invoking two regular communication operations one stoone followed onetoall operation stoone communication processor receives messages source processors combines messages initiates onetoall broadcast 3 algorithms reposi tioning coarsegrained machines like paragon sending relatively short messages cheap compared cost entire stop operation time experimental results show performance stop algorithms differ factor 2 number sources depending sources positioned algorithm ideal source distribution section consider approach repositioning sources invoking stop algorithm ideal input distribution algorithm repos invoked one algorithms described previous section sake example assume algorithm br lin first step generates br lins ideal input distribution sources given machine size machine dimension achieved source processor sending message processor determined ideal distribution refer next section discussion ideal distributions whether pays perform redistribution depends quality initial distribution sources point current implementation algorithm repos check whether initial distribution actually close enough ideal distribution simply perform repositioning second class repositioning algorithms repositions sources also makes use observation time broadcasting s2 sources p2processor machine less half time broadcasting sources pprocessor machine assume partition p processors group g 1 consisting p 1 processors group g 2 consisting p 2 processors partition processors two groups independent position sources may depend choice broadcasting algorithm invoked processor group broadcasting within g 1 g 2 completed every processor g 1 resp g 2 exchanges data processor g 2 resp g 1 communication step corresponds permutation processors g 1 g 2 refer algorithm part lin algorithm based principle using algorithm br lin within submachines refer algorithm part xy source algorithm based principle using algorithm br xysource within submachines 4 source distributions section discuss different patterns source distribution used experiments distributions exploit strengths highlight weaknesses proposed algorithms chosen expect difficult distributions algorithms sake brevity distributions may explained intuitive level assume machine mesh size r c processors indexed rowmajor order let c e ffl row column distributions row distribution source processors rows spaced evenly every row exception last one contains c source processors mesh r30 source processors positioned shown figure 1 column distribution cs defined analogously ffl equal distribution equal distribution es processor 1 1 source processor every dpse th bpscth processor source processor particular values r c es turn row column diagonal distribution exhibit rather irregular position sources ffl right left diagonal distributions right diagonal distribution drs source processors positioned diagonals include diagonal 1 1 r r remaining spaced evenly using modulo arithmetic last diagonal necessarily filled sources left diagonal distribution dls source processors diagonal 1 c r 1 spaces remaining accordingly ffl band distribution band distribution bs similar right diagonal distribution right diagonal distribution contains diagonals width 1 band distribution bs contains r e evenly distributed bands width br e ffl cross distribution cross distribution cs corresponds union row column dis tribution number source processors row distribution roughly equal number processors column distribution ffl square block distribution square block distribution sbs source processors contained square mesh size se theta se figure shows three distributions 30 remainder section describes algorithms handle different distributions row l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l cross l l l l l l l l l l l l l l l l l l l l l l l l l l l l diagonal l l l l l l l l l l l l figure 1 placement source processors row cross right diagonal distributions machine performance algorithms distributions discussed next section consider first algorithm br xy source one expects row column distributions ideal source distributions algorithm br xy source choose first dimension number source processors increased fast possible message length increases slowly possible ever row column distributions equally good example r20 mesh size 10theta10 first sixth row contain source processors thus first iteration increase number source processors 20 sources positioned first seventh row eliminates important observation algorithms generating ideal distributions shows machine dimension effects ideal distribution sources diagonal distribution places number sources row col umn one would expect algorithm br xy source perform quite well diagonal distributions performance algorithm br xy source equal distribution vary cross square block band distributions considerably expensive since source positions may allow fast increase number sources behavior algorithm br lin source distributions quite different first neither row column distribution ideal distributions br lin even number rows iteration achieves increase number sources column distribution consider row distri bution number rows power 2 algorithm br lin actually identical algorithm br xy source number rows odd communication iteration occurs processors column congestion increase equal distribution turn row column distribution thus ideal either behavior algorithm left right diagonal distribution differ difference exists algorithm br xy source machine size 10 theta 10 dr10 experiences increase number sources first iteration since processor p 50 lies diagonal machine sizes right diagonal distribution may experience disadvantage left diagonal distribution least sensitive towards size machine achieves desired properties efficient broadcasting algorithm remaining distributions appear difficult distributions finally algorithm br xy dim suffers obvious drawbacks selection dimension done according size dimensions according number sources ideal distribution algorithm br xy dim either row column distribution depending dimensions 5 experimental results section report performance results broadcasting algorithms intel paragon consider machine sizes 4 256 processors message sizes 32 bytes 16k bytes study performance whole spectrum source numbers ranging 1 p representative selection source distributions paper report performance case source processors broadcast messages length experiments using different length messages influence performance algorithms particular given algorithm good distribution remains good distribution length messages varies throughout sec tion use l denote size messages source processors implementation issues follow straightforward way descriptions given previous sections point synchronize globally iteration one dimension handled algorithms soon processor relevant data continues 51 performance algorithms without repositioning following first study scalability five algorithms described section 2 standard scalability parameters machine size number source processors message length consider relevant parameters including distribution source processors dimension machine interaction dimension machine source processor distribution respect particular algorithm show parameters significant impact performance xor brlin brxysource 100103050number sources time msec figure 2 performance algorithms number sources varies 1 100 assuming equal distribution communication operations invoked algorithms xor 2step use implementations described 7 particular alltoall exchange algorithm views exchanges consisting p permutations uses exclusiveor processor indices generate permutations efficient paragon implementation onetoall communication views mesh linear array applies communication pattern used algorithm br lin ie processor p exchanges message onetoall communication performed within machine half expect algorithms xor 2step give good per formance xor simply exchanges many messages algorithm 2step creates unnecessary communication bottlenecks however want see performance proposed algorithms show disadvantage using existing communication routines bruteforce way figure 2 shows performance five algo rithms figure apparent algorithms 2step xor efficient particu lar 4 sources algorithm 2step suffers congestion node receives messages algorithm xor inherently inefficient large number sends issued source processors algorithm 2step rate increase execution time steeper increase number sources due fact number source processors increases bottleneck processor algorithm 2step receives messages first step sends data second step however case algorithm xor increase number source processors increase number sources distributed among processors xor brlin brxysource message length bytes time msec figure 3 performance algorithms l varies 32 bytes 16k keeping 30 machine right diagonal distribution xor brlin brxysource machine size time msec figure 4 performance algorithms machine size varies assuming approximately sources right diagonal distribution bandwidth network high enough handle type increased communication volume better performance three algo rithms br lin br xy source br xy dim scales linearly increase number sources depending number sources equal distribution places sources machine performance algorithms differs slightly figure 3 shows performance right diagonal distribution message size changes already stated diagonal distributions place number sources rows columns regardless small message size algorithms 2step xor perform poorly almost flat curve message size 1k algorithm xor supports observation related figure 2 three algorithms experience little increase time bytes see linear increase figure 4 shows behavior five algorithms machine size varies 4 256 processor algorithm xor good algorithm small machine sizes 4 16 processors feature also observed number sources close p small machine sizes first three figures give impression algorithms br lin br xy source br xy dim give performance however true following show different distributions different machine sizes effect algorithms different ways brlin brxysource row dia blk cro6789 distributions time msec figure 5 performance three algorithms 10x10 machine assuming different source distributions figure 5 shows performance using different distribution patterns figure confirms discussion given section 4 respect ideal difficult distributions algorithm br xy source gives roughly performance first 4 distributions square block cross distribution see considerable increase time point performance first 4 distributions br xy source true general however row column distribution show ideal distributions square block cross distributions require time three al gorithms expected algorithm br lin performs best due fact algorithm br lin sources spread different rows columns first iterations thus utilizing links efficiently hand square block distribution algorithms br xy source br xy dim columns rows available generate new sources big increase algorithm br xy dim row distribution indicates importance choosing right dimension first brlin brxysource 40758595105115number sources time msec figure performance three algorithms 10x10 machine right diagonal distribution total message size kept 80k number sources varies figure 6 shows performance three algorithms total message size ie sum message sizes source processors fixed interesting aspect performance curves data spread among larger number sources broadcast operation accomplished faster example 80k size data spread among 5 sources takes approximately 114 msec using algorithm br xy source however amount data spread among 40 sources begin takes 73 msec plot highlights claim made earlier given amount data number sources involved broadcasting yield faster execution times time figure 7 performance algorithm br lin dimensions vary assuming equal distribution three source sizes shown 4k cases figure 7 shows performance three algorithms dimensions machine vary demonstrates performance related size dimensions number sources message size number pro cessors distribution gives different performance hence considered good bad depending dimension machine small number sources example machine dimensions may affect performance large number sources machine dimensions impact performance considerably seems like anomaly faster performance reason lies distribution number rows equal distribution tends place source processors within columns allow fast increase number sources hand source processors exception size 4 theta 30 positioned along diagonals 52 performance algorithms repositioning algorithms br xy source br lin exhibit good performance variety source distributions machine dimensions however algorithm source distributions exhibit algorithms weaknesses addition relationship source distribution machine dimension result performance loss algorithm cannot change machine dimension problems arising source distribution avoided performing repositioning section 3 described repositioning partitioning approach next discuss performance repositioning approach using algorithm br xy source use row distribution ideal source distribution br xy source similar results hold repositioning algorithm using br lin left diagonal distribution ideal source distribution let algorithm repos xy source repositioning algorithm invoking br xy source algorithm first perform permutation redistribute source processors according row distribution generate row distribution positions rows number new sources increases fast possible exact position rows depends number rows mesh cost permutation depends source processors located message length figure 8 show percentage difference algorithms repos xy source br xy source four input distributions number sources increases 16 192 machine size message size 6k bytes repositioning pays distributions except band distribution repositioning band distribution costs 65 translating actual time less 150 repos xy source 10103050number sources percentage difference equal cross band x blk figure 8 difference repos xy source br xy source different input distributions machine varying number sources costs 1 2 msec repositioning costs 75 msec indicating repositioning expensive large source numbers repositioning approach results significant gain cross square block distributions terms actual time gain repositioning cross distributions lies 13 31 msec gain 13 msec observed source numbers gain lies 20 31 msec conclusion experimental study repositioning pays ie cost initial permutation less gain resulting working ideal source distribution unless ffl number sources large p2 appears breakpoint ffl number processors small p 16 appears little difference algorithms different source distributions ffl message length small ie less 1k clearly input distribution close ideal distribution pay reposition however none algorithms tries analyze input dis tribution effect message length repositioning illustrated figure 9 figure shows percentage difference four distributions 16 theta 16 machine 75 sources message length increases message size less 1k repositioning pays cross distribution message size increases benefit repositioning increases distributions surprisingly large message length gain tapers decreases distributions section 3 also proposed combine repositioning partitioning approach first generate ideal source distribution create two message length percentage difference equal cross band x blk figure 9 difference repos xy source br xy source different input distributions machine varying message length broadcasting problems one half chine let algorithm part xy source partitioning algorithm using br xy source within machine half compared part xy source performance repos xy source br xy source results showed intel paragon partitioning approach hardly ever gives better performance repositioning alone reason lies cost final permutation exchange long messages done final step dominates performance eliminates gain obtained broadcasting smaller machines performance partitioning algorithms could different machines characteristics conjecture currently available mpps outcome 6 conclusions described different stop broadcasting algorithms analyzed scalability performance intel paragon showed performance algorithm influenced distribution source processors relationship distribution dimension machine algorithm ideal distributions distributions performance degrades reduce dependence performance input distribution proposed repositioning approach approach given distribution turned ideal distribution target broadcasting algorithm also compiled run programs mpi environment using mpi pointtopoint message passing primitives observed performance loss 2 5 every implementation r ccl portable tunable collective communication library scalable parallel computers interprocessor collective communication library multiphase complete exchange circuit switched hypercube benchmarking cm5 multicomputer design implementation broadcast global combine operations using postal model introduction algorithms communication operations coarsegrained mesh architectures architecture optimal alltoall personalized communication opti mal broadcast summation logp model introduction parallel computing multicast hypercube multiprocessors performance evaluation multicast wormhole routing 2d mesh multicomputers optimum broadcasting personalized communication hypercubes manyto many communication bounded traffic alltoall communication meshes wormhole routing tr ctr yuhshyan chen chaoyu chiang cheyi chen multinode broadcasting allported 3d wormholerouted torus using aggregationthendistribution strategy journal systems architecture euromicro journal v50 n9 p575589 september 2004