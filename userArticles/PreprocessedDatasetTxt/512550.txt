compiler approach fast hardware design space exploration fpgabased systems current practice mapping computations custom hardware implementations requires programmers assume role hardware designers tuning performance hardware implementation designers manually apply loop transformations loop unrolling designers manually apply loop transformations example loop unrolling used expose instructionlevel parallelism expense hardware resources concurrent operator evaluation unrolling also increases amount data computation requires much unrolling lead memory bound implementation resources idle negotiate inherent hardware spacetime tradeoffs designers must engage iterative refinement cycle step manually applying transformations evaluating impact process errorprone tedious also prohibitively expensive given large search spaces long synthesis times paper describes automated approach hardware design space exploration collaboration parallelizing compiler technology highlevel synthesis tools present compiler algorithm automatically explores large design spaces resulting application several program transformations commonly used applicationspecific hardware designs approach uses synthesis estimation techniques quantitatively evaluate alternate designs loop nest computation implemented design space exploration algorithm context compilation synthesis system called defacto present results implementation five multimedia kernels algorithm derives implementation closely matches performance fastest design design space among implementations comparable performance selects smallest design search average 03 design space technology thus significantly raises level abstraction hardware design explores design space much larger feasible human designer b introduction extreme flexibility field programmable gate arrays made medium choice fast hardware prototyping popular vehicle realization custom computing machines fpgas composed thousands small programmable logic cells dynamically interconnected allow implementation logic function tremendous growth device capacity made possible implementation complex functions fp gas example fpga implementations sometimes yield even faster solutions conventional hardware 2 orders magnitude encryption 18 addition fpgas oer much faster time market timecritical applications allow postsilicon infield modification prototypical lowvolume designs application specific integrated circuit asic justified despite growing importance applicationspecific fpga designs devices still dicult program making inaccessible average developer standard practice requires developers express application hardwareoriented language verilog vhdl synthesize design hardware using wide variety synthesis tools optimizations performed synthesis tools limited developers must perform highlevel global optimizations hand example commerciallyavailable highlevel synthesis tool handles multidimensional array variables 1 automatic selection loop unroll factors complexity synthesis dicult predict priori performance space characteristics resulting design reason developers engage iterative refinement cycle step manually applying transformations synthesizing design examining results modifying design trade performance space throughout process called design space exploration developer carries responsibility correctness application mapping believe way make programming fpgabased systems accessible oer highlevel imperative programming paradigm c coupled compiler technology oriented towards fpga designs way developers retain advantages simple programming several claim support feature simulation purposes actual hardware synthesis model via highlevel language rely powerful compiler analyses transformations optimize design well automate tedious errorprone mapping tasks make observation class fpga applications characterized highly parallel arraybased computations eg multimedia codes many hand optimizations performed developers similar transformations used parallelizing compilers example developers parallelize computations optimize external memory accesses explicitly manage storage perform loop transformations reason argue parallelizing compiler technology used optimize fpga designs paper describe automated approach design space exploration based collaboration parallelizing compiler highlevel synthesis tools completely synthesizing design prohibitively slow hours days compiler must try several designs arrive good solution reasons exploit estimation behavioral synthesis determine specific hardware parameters eg size speed compiler quantitatively evaluate application transformation derive optimized feasible implementation loop nest computation since hardware implementation bounded terms capacity compiler transformations must also consider space constraints compiler algorithm eectively enables developers explore potentially large design space without automation would feasible previous work presented overview defacto system upon work based combines parallelizing compiler technology stanford suif compiler hardware synthesis tools 9 paper present detailed algorithm design space exploration results demonstrating eectiveness systems automatically synthesize hardware designs c specifications 24 knowledge system automatically explores design space collaboration behavioral synthesis estimation features current infrastructure largely supports direct mapping computations multiple fpgas 26 however work paper describes implementation experimental results designs mapped single fpga multiple memories thus focus algorithmic aspects design space exploration simpler data computation partitioning strategies paper makes following specific contributions describes integration behavioral synthesis tools parallelizing compiler technology map computations fpgabased architectures present compiler algorithm design space exploration relies behavioral synthesis estimates algorithm applies loop transformations explore spacetime tradeo realization hardware designs defines balance metric guiding design space explo ration suggests profitable devote resources storage computation design space exploration algorithm exploits monotonicity properties balance metric eectively prune large regions search space thereby allowing compiler consider wider range transformations otherwise would feasible presents experimental results five multimedia ker nels algorithm derives implementation closely matches performance fastest design design space among implementations comparable performance selects smallest design search average 03 design space technology advances increase density fpga devices tracking moores law conventional logic doubling every months devices able support sophisticated functions future trend towards onchip integration internal memories fpgas specialpurpose functional units becoming attractive replacement asics custom embedded computing architec tures foresee growing need combine strengths highlevel program analysis techniques complement capabilities current future synthesis tools devices consequently designs become complex demanding ecient solution exploring even larger design spaces remainder paper organized follows next section present background fpgas behavioral synthesis section 3 describes optimization goal design space exploration algorithm mapping loop nest computations hardware section 4 discuss analyses transformation algorithm uses section 5 present design space exploration algorithm section 6 present experimental results application algorithm 5 image processing computations survey related work section 7 conclude section 8 2 background describe fpgas synthesis compare optimizations performed parallelizing compilers also discuss features target application domain 21 fieldprogrammablegatearrays fpgas popular vehicle rapid prototyping way implement simple logic interfaces fpgas implemented reprogrammable seasofgates distinct internal architectures example xilinx virtex family devices consists 12 288 device slices slice turn composed 2 lookup tables luts implement arbitrary logic function 11 boolean inputs 6 outputs 15 two slices form configurable logic block clbs blocks interconnected 2 dimensional mesh via programmable static routing switches configure fpga designers download bitstream file configuration slices fpga well routing programmable devices example apex ii devices altera hierarchical routing approach connecting clbs fpgas overall functionality similar 6 traditional architectures bandwidth external memory key performance bottleneck fpgas since possible compute orders magnitude data cycle fetched stored memory however unlike traditional architectures fpga flexibility devote internal configurable resources either storage computation 22 fpga synthesis flow synthesis flow fpgas term given process translating functional logic specifications bitstream description configures device functional specification done multiple levels using hardware description languages vhdl verilog designers specify functionality datapath circuits eg adders multipliers etc diagram design blocks structural specification defines inputoutput interface block allows designers describe finite state machines fsms control temporal behavior blocks using approach designers control every single aspect operations datapaths preferred approach maximum performance sought requires extremely high design times process takes structural specification targets particular architectures programmable units luts case xilinx devices called rtllevel synthesis rtllevel synthesis generates netlist representation intended design used input lowlevel synthesis steps mapping placeandroute pr ultimately generate device bitstream configuration file 23 behavioral synthesis vs compilers behavioral specifications vhdl verilog opposed lower level structural specifications express computations without committing particular hardware implementation structure process taking behavioral specification generating hardware implementation called behavioral synthesis behavioral synthesis performs three core functions binding operators registers specification hardware implementations eg selecting ripplecarry adder implement addition resource allocation eg deciding many ripplecarry adders needed scheduling operations particular clock cycles generate particular implementation behavioral synthesis requires programmer specify target design requirements terms area clock rate number clock cycles number operators combination ex ample designer might request design uses two multipliers takes 10 clock cycles behavioral synthesis tools use information generate particular implementation satisfies constraints addition behavioral synthesis supports optimiza tions relies heavily developer direct mapping steps example current behavioral synthesis tools allow specification loops unroll loop unrolling tool perform extensive optimizations resulting inner loop body parallelizing pipelining operations minimizing registers operators save space however deciding unroll factor left programmer behavioral synthesis parallelizing compilers optimizations scalar variables optimizations scalars arrays optimizations inside loop body optimizations inside loop body across loop iterations supports usercontrolled analyses guide automatic loop unrolling loop transformations manages registers optimizes memory accesses interoperator communication evaluates tradeoffs different storage offchip considers single fpga systemlevel view multiple fpgas multiple memories performs allocation binding knowledge hardware scheduling hardware resources implementation computation table 1 comparison behavioral synthesis parallelizing compiler technologies similarities optimizations performed synthesis tools parallelizing com pilers many ways oer complementary capabilities shown table 1 key advantage parallelizing compiler technology behavioral synthesis ability perform data dependence analysis array variables used basis parallelization loop transformations optimizing memory accesses technology permits optimization designs array variables data resides ochip memories enables reasoning benefits code transformations loop unrolling without explicitly applying addition parallelizing compilers capable performing global program analysis permits optimization across entire system 24 target application domain customizability fpgas commonly used applications significant amounts finegrain parallelism possibly benefit nonstandard numeric formats eg reduced data widths specifically multimedia applications including image signal processing 8bit 16bit data respectively oer wide variety popular applications map well fpgas example typical image processing algorithm scans multidimensional image operates given pixel value neighbors images often represented multidimensional array variables computation expressed loop nest applications exhibit abundant concurrency well temporal reuse data examples computations fall category include image correlation laplacian image operators erosiondilation operators edge detection fortunately applications good match capabilities current parallelizing compiler analyses eective ane domain array subscript expressions linear functions loop index variables constants 25 paper restrict input programs loop nest computations array scalar variables pointers subscript expressions ane fixed stride loop bounds must constant 2 support loops control flow simplify control scheduling generated code always performs conditional memory accesses 3 optimization goal balance simply stated optimization criteria mapping single loop nest fpgabased systems follows 1 design must exceed capacity constraints sys tem 2 execution time minimized 3 given level performance fpga space usage minimized motivation first two criteria obvious third criterion also needed several reasons first two designs equivalent perfor mance smaller design desirable frees space uses fpga logic map loop nests addition smaller design usually less routing complexity result may achieve faster nonconstant bounds could potentially supported algorithm generated code resulting fpga designs would much complex example behavioral synthesis would transform loop nonconstant bound loop hardware implementation target clock rate moreover third criterion suggests strategy selecting among set candidate designs meet first two criteria respect particular set transformations described next section algorithm attempts select best design meets criteria algorithm uses two metrics guide selection design first results estimation provide space usage design related criterion 1 another important metric used guide selection design related criteria 2 3 balance defined equation f refers data fetch rate total data bits memory provide per cycle c refers data consumption rate total data bits computation consume computational delay balance close one memories fpgas busy balance less one design memory bound greater one compute bound design balanced metric suggests whether resources devoted improving computation time memory time borrow notion balance previous work mapping array variables scalar registers balance floating point operations memory accesses 5 flexibility fpgas adjust time spent either computation memory accesses use data fetch rate data consumption rate compare dierent optimization assumptions 4 analyses transformations section describes high level code transformations performed system illustrated fir filter example figure 1 unrollandjam first code transformation unroll andjam involves unrolling one loops iteration space fusing inner loop bodies together shown figure 1b unrolling exposes operator parallelism highlevel synthesis example multiplies performed parallel two additions subsequently performed parallel followed two additions unrollandjam also decrease dependence distances reused data accesses combined scalar replacement discussed used expose opportunities parallel memory accesses scalar replacement scalar replacement replaces array references accesses temporary scalar variables highlevel synthesis exploit reuse registers 5 approach scalar replacement closely matches previous work eliminates true dependences reuse carried innermost loop accesses ane domain consistent dependences ie constant dependence distances 5 however two dierences 1 also eliminate unnecessary memory writes output dependences 2 exploit reuse across loops nest innermost loop latter dier ence stems observation many though algorithms mapped fpgas suciently small loop bounds small reuse distances number registers configured fpga suciently large detailed description scalar replacement register reuse analysis found 9 example figure 1c see results scalar replacement illustrates dierences int s96 int c32 int d64 j0 j64 j original code j0 j64 j2 b unrolling j loop loop 1 unroll factor 2 jamming copies loop together j0 j64 j2 initialize registers i0 i32 i2 j0 initialize c registers rotate registersc 0 0 c 0 15 rotate registersc 1 0 c 1 15 c scalar replacement accesses c across j loop j0 j32 j initialize registers i0 i16 j0 initialize c registers rotate registersc 0 0 c 0 15 rotate registersc 1 0 c 1 15 final code generated fir including loop normalization data layout optimization figure 1 optimization example fir previous work accesses arrays c replaced array written back memory end iteration j loop redundant writes eliminated loopindependent accesses array replaced accesses array consistent dependence distance reuse array c carried outer loop exploit full reuse data c involves introducing extra registers hold values c across iterations inner loop rotate operation shifts registers rotates last one first position operation performed parallel hardware loop peeling loopinvariant code motion see figure 1c values c registers loaded first iteration j loop clarity shown code generated compiler actually peels first iteration j loop instead including conditional loads iterations j loop number memory loads optimized scheduled highlevel synthesis ac cordingly although first glance code size appears doubled peeling highlevel synthesis usually reuse operators peeled original loop body code growth correspond growth design memory accesses array invariant respect loop moved outside loop using loopinvariant code motion within main unrolled loop body memory accesses array remain data layout array renaming another code transformation lays data fpgas external memory maximize memory parallelism custom data layout separated two distinct phases first phase call array renaming performs 1 mapping array access expressions virtual memory ids customize accesses array according access patterns array renaming performed accesses array within loop nest uniformly generated two ane array references b1 b1 bn c1 cn d1 dn constants loop index variables uniformly generated arrays accesses uniformly generated mapped single memory result array renaming even distribution data across virtual memories second phase called memory mapping binds virtual memory ids physical ids taking consideration accesses arrays loop nest avoid scheduling conflicts shown figure 1d eect data layout even elements c mapped memory 0 odd elements mapped memory 1 accesses renamed reflect layout similarly distributed memories 2 3 approach similar spirit modulo unrolling used raw compiler 3 however compared modulo unrolling loop transformation assumes fixed data layout approach data trans formation current implementation supports varied set custom data layouts typical layout cyclic least one dimension array possibly customized data layouts arise packing small data types strided accesses subscript expressions multiple induction variables ie subscripts form one nonzero full discussion data layout algorithm beyond scope paper discussion found 9 summary summarize algorithm evaluates focused set possible unroll factors multiple loops loop nest data reuse exploited within across loops nest result scalar replacement compiler eliminating unnecessary memory accesses operator parallelism exposed highlevel synthesis unrolling one loops nest independent operations performed parallel highlevel synthesis deems beneficial thus defined set transformations widely used conventional computing permit us adjust parallelism data reuse fpgabased systems collaboration parallelizing compiler technology highlevel synthesis meet optimization criteria set forth previous section reduced optimization process tractable problem selecting unroll factors loop nest leads highperformance balanced ecient design next section present algorithm detail although algorithm focuses fixed set compiler transformations notion using balance guide performancespace tradeo design space exploration used optimizations well 5 optimization algorithm discussion section defines terms uses describe design space exploration algorithm algorithm presented assuming scalar replacement exploit reuse loop nest ane array accesses resulting design store reused data internally fpga feasible many applications short reuse distances may require many onchip registers general case address problem limiting number registers section 54 51 definitions define saturation point vector unroll factors memory parallelism reaches bandwidth architecture following property holds resulting unrolled loop body ireads lnummemories width l jwrites lnummemories width l c1 c2 integer constants simplify discussion let us assume access widths match memory width simply looking unroll factor results multiple nummemories read write accesses smallest values c1 c2 saturation set sat determined function number read write accesses r w single iteration loop nest unroll factor loop nest consider reads writes separately scheduled separately interested determining saturation point scalar replacement redundant write elimination purposes discussion assume array accessed main loop body accesses uniformly generated thus customized data layout ob tained modifications algorithm hold straightforward complicate calculation saturation point r defined number uniformly generated read sets w number uniformly generated write sets single memory read single write access set uniformly generated references others removed scalar replacement redundant write elimination define unroll factor vector u corresponds unroll factor loop function 1in product unroll factors let saturation set sat defined vector whose product psat u 1 array subscript expressions memory accesses varying respect loop saturation point considers unrolling loops introduce additional memory parallelism since loop peeling loopinvariant code motion eliminated memory accesses main loop body invariant respect loop nest perspective memory parallelism unroll factor vectors equiv alent particular saturation point sat refers unrolling loop factor psat using unroll factor 1 loops 52 search space properties optimization involves selecting unroll factors loops nest search guided following observations impact unrolling single loop nest depend upon assumptions target applications section 24 observation 1 data fetch rate monotonically nondecreasing unroll factor increases multiples psat also nonincreasing beyond saturation point intuitively data fetch rate increases memory accesses available loop body scheduling parallel observation requires data laid memory accesses scheduled number independent memory accesses memory cycle monotonically nondecreasing unroll factor increases unroll factor must increase multiples psat time memory operation per formed nummemories accesses main loop body available schedule parallel true whenever data layout successfully mapped array multiple memories data layout successful case accesses array uniformly generated steady state mapping data memories guarantee monotonicity even less nummemories parallel accesses ignore possibility subsequent discussion data layout mapping data specific memories controlled compiler given property array renaming section 4 accessed data evenly distributed across virtual memory ids mapping derives solution absence conflicting accesses arrays exposes fully parallelizable accesses prevent conflicting read write accesses mapping virtual memory ids physical ones must first consider accesses scheduled compiler component system directly responsible scheduling scheduling memory accesses well computation performed behavioral synthesis tools monet scheduling algorithm used monet called soon possible first considers memory accesses occur parallel based comparing subscript expressions physical memory ids rules writes whose results yet available due dependences 10 performing physical memory id mapping first consider read ac cesses maximize number read operations occur parallel physical id mapping matches read access order total number memory reads loop evenly distributed across memories arrays added benefit operands individual writes fetched parallel physical mapping write operations also performed order evenly distributing write operations across memories properties data layout scheduling saturation point guaranteed choice unroll factor data fetch rate increases saturation point beyond observation 2 consumption rate monotonically nondecreasing unroll factor increases multiples psat even beyond saturation point intuitively unroll factor increases operator parallelism enabled thus reducing computation time increasing frequency data con sumed based observation 1 increase data fetch rate eliminate idle cycles waiting memory thus increase consumption rate although parallelism exploited result unrolling loop may reach threshold performance continues improve slightly due simpler loop control observation 3 balance monotonically nondecreasing saturation point monotonically nonincreasing beyond saturation point unroll factor increases multiples psat balance nondecreasing saturation point relies observation 1 data fetch rate increasing fast faster data consumption rate memory accesses completely independent whereas operator parallelism may restricted beyond saturation point data fetch rate increasing consumption rate increasing least slightly 53 algorithm description algorithm presented figure 2 given described monotonicity search space loop nest start design saturation point search larger unroll factors multiples psat looking two points balance crosses compute bound memory bound vice versa fact ignoring space constraints could search loop nest independently converge nearoptimal design rapidly select unroll factors based data dependences described algorithm first selects u init starting point search sat select promising unroll factors based dependence distance vectors dependence distance vector vector represents vector dierence two accesses array terms loop indices nest 25 since starting design maximizes memory parallelism either design memory bound stop search compute bound continue compute bound consider unroll factors provide increased operator parallelism addition memory parallelism thus first look loop carries dependence ie ddd unrolled iterations loop executed parallel loop found set unroll factor sat assuming unroll factor sat loop exists instead select unroll factor favors loops largest dependence distances loops perform parallel computations dependences details algorithm selects initial unroll factor case beyond scope paper key insight unroll loops nest larger unroll factors loops carrying larger minimum nonzero dependence distances monotonicity property also applies considering simultaneous unrolling multiple loops long unroll factors loops either increasing decreasing initial design space constrained must reduce unroll factor design size less size constraint capacity resulting suboptimal design function findlargestfit simply selects largest unroll factor baseline design corresponding unrolling called u base u init regardless balance maximize available parallelism assuming initial design compute bound algorithm increases unroll factors reaches design 1 memory bound 2 larger capacity 3 represents full unrolling loops nest ie function increaseuin returns unroll factor vector uout remaining unroll factor vectors increase returns uin either spaceconstrained memory bound design found algorithm select unroll factor vector last compute bound design fit current design approximating binary search follows function selectbetweenusmall u large returns unroll factor vector uout remaining unroll factor vectors selectbetween returns usmall compute bound design 54 adjusting number onchip registers designs reuse distance large many registers required may become necessary reduce number data items stored fpga using fewer onchip registers means less reuse exploited turn slows fetch rate lesser extent consumption rate net eect search algorithm input code ndeep loop nest output u1 un vector unroll factors ok first deal spaceconstrained designs estimatespace capacity else else else b 1 memory bound else balanced solution earlier size else b 1 compute bound u seen compute bound far else balanced solution earlier size check points search return ucurr figure 2 algorithm design space exploration first place design smaller likely fit chip secondly space freed used increase operator parallelism designs compute bound adjust number onchip registers use loop tiling tile loop nest localized iteration space within tile matches desired number registers exploit full register reuse within tile 6 experimental results section presents experimental results characterize eectiveness previously described design space exploration algorithm set kernel applications describe applications experimental methodology discuss results 61 application kernels demonstrate design exploration algorithm five multimedia kernels namely finite impulse response fir filter integer multiplyaccumulate consecutive elements 64 element array matrix multiply mm integer dense matrix multiplication 32by16 matrix 16by4 matrix string pattern matching pat character matching operator string length 16 input string length 64 metrics area number clock cycles behavioral vhdl transformed suif application yes balanced design balance calculation monet behavioral synthesis compiler analyses scalar replacement data layout array renaming data reuse determination unroll factor tiling unroll jam figure 3 compilation synthesis flow jacobi iteration jac 4point stencil averaging computation elements array sobel sobel edge detection see eg sobel 22 3by3 window laplacian operator integer image application written standard c program computation single loop nest pragmas annotations language extensions describing hardware implementation 62 methodology applied prototype compilation synthesis system analyze determine best unrolling factor balanced hardware implementation figure 3 depicts design flow used experiments first code compiled suif format along application standard compiler optimizations next design space exploration algorithm iteratively determines loops loop nest unrolled much make determination compiler starts given unrolling factor applies sequence transformations described sections 4 5 next compiler translates suif code resulting application selected set transformations behavioral vhdl using tool called suif2vhdl compiler next invokes mentor graphics monet tm behavioral synthesis tool obtain space performance estimates implementation behavioral specification process compiler currently fixes clock period 40ns monet tm synthesis estimation yields amount area used implementation number clock cycles required execute completion computation behavioral specification given data compiler next computes balance metric system fully automated implementation compiler passes specific experiment namely data reuse analysis scalar replacement unrolljam loop peel ing customized data layout constitutes approximately 14 500 lines c source code algorithm executed less 5 minutes application fully synthesize design would require additional couple hours 63 results section present results five previously described kernels figures 4 10 graphs show large number points design space substantially searched algorithm highlight relationship unroll factors metrics interest first set results figures 4 7 plots bal ance execution cycles design area target fpga function unroll factors inner outer loops fir mm although mm 3deep loop nest consider unroll factors two outermost loops since loopinvariant code motion compiler eliminated memory accesses innermost loop graphs first two columns xaxis unroll factors inner loop curve represents specific unroll factor outer loop fir mm plotted results pipelined nonpipelined memory accesses observe impact memory access costs balance metric consequently selected designs plots squared box indicates design selected search algorithm pipelined memory accesses assume read write latency 1 cycle nonpipelined memory accesses assume read latency 7 cycles write latency 3 cycles latencies annapolis wildstar tm 13 board target platform work practice memory latency somewhere two memory accesses fully pipelined results assuming 4 memories number external memories connected fpgas annapolis wildstar tm board plots design balanced unrolling factor yaxis value 10 data points yaxis value 10 indicate computebound designs whereas points yaxis value 10 indicate memorybound de signs computebound design suggests resources devoted speeding computation component design typically unrolling consuming resources computation memorybound design suggests less resources devoted computation functional units implement computation idle waiting data design area graphs represent space consumed using log scale target xilinx virtex 1000 fpgas unrolling factors vertical line indicates maximum device capacity designs right side line therefore unrealizable pipelined memory accesses trend towards computebound designs due low memory latency without pipelining memory latency becomes bottleneck leading case fir designs always memory bound nonpipelined mm exhibits computebound balanced designs second set results figures 8 10 show performance remaining three applications jac pat sobel figures present balance cycles area function unroll factors pipelined memory accesses due space limitations make several observations full results first see balance follows monotonicity properties described observation 3 increasing reaches sat inner loop unroll factor015025035balance outer loop unroll factor 1 outer loop unroll factor 2 outer loop unroll factor 4 outer loop unroll factor 8 outer loop unroll factor outer loop unroll factor outer loop unroll factor 64 selected design inner loop unroll factor200060001000014000execution cycles outer loop unroll factor 1 outer loop unroll factor 2 outer loop unroll factor 4 outer loop unroll factor 8 outer loop unroll factor outer loop unroll factor outer loop unroll factor 64 selected design space logscaled10execution cycles logscaled selected design space balance b execution time c area figure 4 balance execution time area nonpipelined fir inner loop unroll factor12228balance outer loop unroll factor 1 outer loop unroll factor 2 outer loop unroll factor 4 outer loop unroll factor 8 outer loop unroll factor outer loop unroll factor outer loop unroll factor 64 selected design inner loop unroll factor200040006000 execution cycles outer loop unroll factor 1 outer loop unroll factor 2 outer loop unroll factor 4 outer loop unroll factor 8 outer loop unroll factor outer loop unroll factor outer loop unroll factor 64 selected design space logscaled execution cycles logscaled selected design space balance b execution time c area figure 5 balance execution cycles area pipelined fir inner loop unroll factor0406081 balance outer loop unroll factor 1 outer loop unroll factor 2 outer loop unroll factor 4 outer loop unroll factor 8 outer loop unroll factor outer loop unroll factor selected design inner loop unroll factor200040006000execution cycles outer loop unroll factor 1 outer loop unroll factor 2 outer loop unroll factor 4 outer loop unroll factor 8 outer loop unroll factor outer loop unroll factor selected design space logscaled10 execution cycles logscaled selected design space balance b execution time c area figure balance execution cycles area nonpipelined mm inner loop unroll factor123 balance outer loop unroll factor 1 outer loop unroll factor 2 outer loop unroll factor 4 outer loop unroll factor 8 outer loop unroll factor outer loop unroll factor selected design inner loop unroll factor20004000execution cycles outer loop unroll factor 1 outer loop unroll factor 2 outer loop unroll factor 4 outer loop unroll factor 8 outer loop unroll factor outer loop unroll factor selected design space logscaled10execution cycles logscaled selected design space balance b execution time c area figure 7 balance execution cycles area pipelined mm inner loop unroll factor1141822 balance outer loop unroll factor 1 outer loop unroll factor 2 outer loop unroll factor 4 outer loop unroll factor 8 outer loop unroll factor outer loop unroll factor selected design inner loop unroll factor40080012001600 execution cycles outer loop unroll factor 1 outer loop unroll factor 2 outer loop unroll factor 4 outer loop unroll factor 8 outer loop unroll factor outer loop unroll factor selected design space logscaled 2execution cycles logscaled selected design space balance b execution time c area figure 8 balance execution time area pipelined jac inner loop unroll factor1525balance outer loop unroll factor 1 outer loop unroll factor 2 outer loop unroll factor 3 outer loop unroll factor 4 outer loop unroll factor 6 outer loop unroll factor 8 outer loop unroll factor 12 outer loop unroll factor outer loop unroll factor 24 outer loop unroll factor 48 selected design inner loop unroll factor10002000execution cycles outer loop unroll factor outer loop unroll factor outer loop unroll factor outer loop unroll factor outer loop unroll factor outer loop unroll factor outer loop unroll factor 12 outer loop unroll factor outer loop unroll factor 24 outer loop unroll factor 48 selected design space logscaled1010execution cycles logscaled selected design space balance b execution time c area figure 9 balance execution cycles area pipelined pat inner loop unroll factor15171921 balance outer loop unroll factor 1 outer loop unroll factor 2 outer loop unroll factor 4 outer loop unroll factor 8 outer loop unroll factor outer loop unroll factor outer loop unroll factor 64 selected design inner loop unroll factor2000400060008000 execution cycles outer loop unroll factor 1 outer loop unroll factor 2 outer loop unroll factor 4 outer loop unroll factor 8 outer loop unroll factor outer loop unroll factor outer loop unroll factor 64 selected design space logscaled10 execution cycles logscaled selected design space balance b execution time c area figure 10 balance execution time area pipelined sobel uration point decreasing execution time also monotonically nonincreasing related observation 2 programs algorithm selects design close best terms performance uses relatively small unroll factors among designs comparable perfor mance cases algorithm selected design consumes smallest amount space result shown approach meets optimization goals set forth section 3 cases balanced design selected algorithm less balanced design selected either balanced design saturation point nonpipelined fir large fit fpga pipelined mm table 2 presents speedup results selected design kernel compared baseline pipelined nonpipelined designs baseline loop nest unrolling unroll factor 1 loops including applicable code transformations described section 4 although graphs present large number design points algorithm searches tiny fraction displayed instead algorithm uses prun program nonpipelined pipelined fir 767 1726 mm 455 1336 jac 387 556 pat 753 3461 table 2 speedup single fpga ing heuristics based saturation point balance described section 5 reveals eectiveness algorithm finds best design point explored small fraction 03 design space consisting possible unroll factors loop larger design spaces expect number points searched relative size even smaller 64 accuracy estimates speed design space exploration approach relies estimates behavioral synthesis rather going lengthy process fully synthesizing design anywhere 10 10 000 times slower set designs determine gap behavioral synthesis estimates fully synthesized designs ran logic synthesis placeandroute derive implementations selected design points design space applications synthesized baseline design selected designs pipelined nonpipelined versions additional unroll factors beyond selected design cases number clock cycles remains behavioral synthesis implemented design however target clock rate degrade larger unroll factors due increased routing complexity similarly space also increase slightly linearly unroll factors factors present output logic synthesis placeandroute negligible designs selected algorithm clock rates degraded less 10 almost selected designs compared baseline speedups terms reduction clock cycles made case fir pipelining clock degraded 30 met target clock 40ns speedup 17x performance improvement still significant space increases sublinear compared unroll factors tended space constrained large designs suggested output behavioral synthesis large designs appear highest performance according behavioral synthesis estimates show much significant degradations clock increases space cases performance would worse designs smaller unroll factors approach suer potential problem favor small unroll factors increase unrolling factor significant reduction execution cycles due memory parallelism instructionlevel parallelism set applications estimation discrepan cies negligible never influenced selected de sign accuracy issue clearly orthogonal design space algorithm described paper believe estimation tools improve ability deliver accurate estimates given growing pressures accuracy simulation increasingly larger designs 7 related work section discuss related work areas automatic synthesis hardware circuits highlevel language constructs design space exploration using highlevel loop transformations 71 synthesizing highlevel constructs gap hardware description languages vhdl verilog applications highlevel imperative programming languages prompted researchers develop hardwareoriented highlevel languages new languages would allow programmers migrate configurable architectures without learn radically new programming paradigm retaining level control hardware mapping synthesis process one first eorts direction handel parallel programming language handelc heavily influenced occam csplike parallel language clike syntax mapping handelc hardware compositional constructs loops directly mapped predefined template hardware structures 20 researchers developed approaches mapping applications reconfigurable architectures fpgas eorts eg rapid 7 reconfigurable architecture piperench 12 developed explicitly parallel programming language andor developed compilation synthesis flow tailored features architecture cameron research project system compiles programs written singleassignment subset c called sac dataflow graphs synthesizable vhdl 23 sac language includes reduction windowing operators twodimensional array variables combined doall constructs explicitly expose parallel operations computation like approach sac compiler includes looplevel transformations loop unrolling tiling particularly windowing operators present loop however application transformations controlled pragmas automatic camerons estimation approach builds internal dataflow representation using curve fitting techniques 17 several researchers developed tools map computations expressed sequential imperative programming language c reconfigurable custom computing architectures weinhardt 24 describes set program transformations pipelined execution loops loopcarried dependences onto custom machines using pipeline control unit approach similar also recognizes benefit data reuse present compiler algorithm two projects closely related nimble compiler 19 work babb et al 2 map applications c fpgas perform design space exploration also rely behavioral synthesis fact compiler replaces function synthesis tools 72 design space exploration discussion focus related work attempted use loop transformations explore wide design space work addressed general issues finding suitable architecture either reconfigurable particular set applications 1 context behavioral vhdl 16 current tools monet tm 14 allow programmer control application loop unrolling loops constant bounds programmer must first specify application behavioral vhdl linearize multidimensional arrays select order loops must execute next programmer must manually determine exact unroll factor loops determine unrolling going aect required bandwidth computation given eort interaction transformations data layout options available approach design space exploration extremely awkward errorprone researchers also recognized value exploiting looplevel transformations mapping regular loop computations fpgabased architectures der rienrajopadhye 8 describe tiling strategy doubly nested loops model performance analytically select tile size minimizes iterations execution time 73 discussion research presented paper diers efforts mentioned several respects first focus research developing algorithm explore wide number design points rather selecting single implementation second proposed algorithm takes input sequential application description require programmer control compilers transfor mations third proposed algorithm uses highlevel compiler analysis estimation techniques guide application transformations well evaluate various design points algorithm supports multidimensional array variables absent previous analyses mapping loop computations fpgas finally use commercially available behavioral synthesis tool complement parallelizing compiler techniques rather creating architecturespecific synthesis flow partially replicates functionality existing commercial tools behavioral synthesis allows design space exploration extract accurate performance metrics time area used rather relying compilerderived performance model approach greatly expands capability behavioral synthesis tools precise program analysis 8 conclusion described compiler algorithm balances computation memory access rates guide hardware design space exploration fpgabased systems experimental results five multimedia kernels reveal algorithm quickly less five minutes searching less 03 search space derives design closely matches best performance within design space smaller designs comparable performance work addresses growing need raising level abstraction hardware design simplify design pro cess combining strengths parallelizing compiler behavioral synthesis system automatically performs transformations typically applied manually hardware de signers rapidly explores large design space technology increases complexity devices consequently designs become complex furthering automation design process become crucial acknowledgements research supported darpa contract f306029820113 authors wish thank contributors defacto project upon work based particular joonseok park heidi ziegler yoonju lee brian richards 9 r parallelizing applications silicon compilermanaged memory system raw machines improving register allocation subscripted variables improving ratio memory operations floatingpoint operations loops altera corp specifying compiling applications rapid loop tiling reconfigurable accelerators bridging gap compilation synthesis defacto system understanding behavioral synthesis practical guide highlevel design evaluation streamsc ctofpga compiler applications perspective coprocessor streaming multimedia acceleration annapolis microsystems wildstar tm manual mentor graphics monet tm users manual release r42 xilinx virtexii 15v fpga data sheet behavioral synthesis fast area estimation support compiler optimizations fpgabased reconfigurable systems structured hardware compilation parallel programs compiling occam fpgas digital signal processing principles automated process compiling dataflow graphs reconfigurable hardware compilation pipeline synthesis reconfigurable architectures optimizing supercompilers supercomputers tr improving register allocation subscripted variables digital signal processing 3rd ed maps piperench hardwaresoftware codesign embedded reconfigurable architectures evaluation streamsc ctofpga compiler automated process compiling dataflow graphs reconfigurable hardware understanding behavioral synthesis optimizing supercompilers supercomputers loop tiling reconfigurable accelerators specifying compiling applications rapid parallelizing applications silicon bitserial implementation international data encryption algorithm idea fast area estimation support compiler optimizations fpgabased reconfigurable systems coarsegrain pipelining multiple fpga architectures ctr rui rodrigues joao p cardoso infrastructure functionally test designs generated compilers targeting fpgas proceedings conference design automation test europe p3031 march 0711 2005 byoungro pedro c diniz mary w hall using estimates behavioral synthesis tools compilerdirected design space exploration proceedings 40th conference design automation june 0206 2003 anaheim ca usa gang quan james p davis siddhaveerasharan devarkal duncan buell highlevel synthesis large bitwidth multipliers fpgas case study proceedings 3rd ieeeacmifip international conference hardwaresoftware codesign system synthesis september 1921 2005 jersey city nj usa sharing synthesis control data flow graphs fpgas proceedings 40th conference design automation june 0206 2003 anaheim ca usa david andrews ron sass erik anderson jason agron wesley peck jim stevens fabrice baijot ed komp achieving programming model abstractions reconfigurable computing ieee transactions large scale integration vlsi systems v16 n1 p3444 january 2008 joonseok park pedro c diniz k r shesha shayee performance area modeling complete fpga designs presence loop transformations ieee transactions computers v53 n11 p14201435 november 2004 heidi e ziegler mary w hall pedro c diniz compilergenerated communication pipelined fpga applications proceedings 40th conference design automation june 0206 2003 anaheim ca usa r gonalves p moraes j p cardoso f wolf fernandes r f romero e marques architectr system reconfigurable robots design proceedings acm symposium applied computing march 0912 2003 melbourne florida byoungro mary w hall heidi e ziegler custom data layout memory parallelism proceedings international symposium code generation optimization feedbackdirected runtime optimization p291 march 2024 2004 palo alto california uday bondhugula j ramanujam p sadayappan automatic mapping nested loops fpgas proceedings 12th acm sigplan symposium principles practice parallel programming march 1417 2007 san jose california usa heidi ziegler mary hall evaluating heuristics automatically mapping multiloop applications fpgas proceedings 2005 acmsigda 13th international symposium fieldprogrammable gate arrays february 2022 2005 monterey california usa yongkang zhu grigorios magklis michael l scott chen ding david h albonesi energy impact aggressive loop fusion proceedings 13th international conference parallel architectures compilation techniques p153164 september 29october 03 2004 zhi guo betul buyukkurt walid najjar input data reuse compiling window operations onto reconfigurable hardware acm sigplan notices v39 n7 july 2004 scheduling algorithm optimization early planning highlevel synthesis acm transactions design automation electronic systems todaes v10 n1 p3357 january 2005 charles r hardnett krishna v palem yogesh chobe compiler optimization embedded applications adaptive soc architecture proceedings 2006 international conference compilers architecture synthesis embedded systems october 2225 2006 seoul korea chen ding yutao zhong predicting wholeprogram locality reuse distance analysis acm sigplan notices v38 n5 may patrick carribault albert cohen applications storage mapping optimization register promotion proceedings 18th annual international conference supercomputing june 26july 01 2004 malo france