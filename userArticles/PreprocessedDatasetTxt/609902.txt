pac learning nasty noise introduce new model learning presence noise call nasty noise model model generalizes previously considered models learning noise learning process model variant pac model proceeds follows suppose learning algorithm execution asks examples examples algorithm gets generated nasty adversary works according following steps first adversary chooses examples independently according fixed unknown learning algorithm distribution pacmodel powerful adversary upon seeing specific examples chosen using knowledge target function distribution learning algorithm allowed remove fraction examples choice replace examples number arbitrary examples choice modified examples given learning algorithm restriction adversary number examples adversary allowed modify distributed according binomial distribution parameters noise rate mon negative side prove algorithm achieve accuracy 2 learning nontrivial class functions also give lower bounds sample complexity required achieve accuracy positive side show polynomial usual parameters 12 number examples suffice learning class finite vcdimension accuracy 2 algorithm may efficient however also show fairly wide family concept classes efficiently learned presence nasty noise b introduction valiants pac model learning 22 one important models learning examples although extremely elegant model pac model drawbacks particular assumes learning algorithm access perfect source random examples namely upon request learning algorithm ask random examples return gets pairs x c x xs points input space distributed identically independently according fixed probability distribution c x correct classification x according target function c algorithm tries learn since valiants seminal work several attempts relax assumptions introducing models noise first noise model called random classification noise model introduced 2 extensively studied eg 1 6 9 12 13 16 model adversary providing example x c x learning algorithm tosses biased coin whenever coin shows h happens probability j classification example flipped algorithm provided wrongly classified example stronger model called malicious noise model introduced 23 revisited 17 studied 8 10 11 20 model adversary whenever jbiased coin shows h replace example x c x arbitrary pair point input space b boolean value note particular gives adversary power distort distribution work present new model call nasty sample noise model model adversary gets see whole sample examples requested learning algorithm giving algorithm modify e examples choice e random variable distributed binomial distribution parameters j size sample distribution makes number examples modified determined independent tosses jbiased coin however allow adversarys choice dependent sample drawn modification applied adversary arbitrary malicious noise model 1 intuitively speaking new adversary powerful previous ones examine whole sample remove informative examples replace less useful even misleading examples whereas malicious noise model instance adversary also may insert sample misleading examples freedom choose examples remove relationships various models shown table 1 random noiselocation adversarial noiselocation label noise random classification noise nasty classification noise point label noise malicious noise nasty sample noise table 1 summary models paclearning noisy data argue newly introduced model generalizes previous noise models including variants decaturs cam model 11 cpcn model 12 also many realworld situations assumptions previous models made noise seem unjustified example training data result physical experiment noise may tend stronger boundary areas rather uniformly distributed inputs special models also consider weaker variant model called nasty classification noise model adversary may modify classification chosen points random classification noise model devised describe situation exactlearning setting example incomplete boundary query model blum et al 5 may regarded special case nasty noise adversary chooses provide unreliable answers sample points near boundary target concept remove points sample another situation model related setting agnostic learning model concept class given instead learning algorithm needs minimize empirical error using hypothesis predefined hypotheses class see example 18 definition model assuming best hypothesis classifies input j fraction may alternatively see problem learning hypotheses class nasty noise rate j however note success criterion agnostic learning literature different one used pacbased setting show two types results sections 3 4 show information theoretic results sect 5 shows algorithmic results first result presented section 3 lower bound quality learning possible nasty adversary result shows learning algorithm cannot learn nontrivial concept class accuracy better 2j sample contains nasty noise rate j show learning concept class vc dimension accuracy requires examples complemented matching positive result section 4 shows class finite vcdimension learned using sample polynomial size accuracy ffl 2j size sample required polynomial usual pac parameters 1delta margin requested accuracy ffl mentioned lower bound main quite surprising result presented section 5 another positive result showing efficient learning algorithms still possible spite powerful adversary specifically present composition theorem analogous 3 8 nastynoise learning model shows concept class constructed composing concept classes paclearnable hypothesis class fixed vcdimension efficiently learnable using sample subject nasty noise includes instance class concepts formed boolean combination halfspaces constant dimension euclidean space complexity polynomial usual parameters 1delta algorithm used proof result adaptation model pac algorithm presented 8 results may compared similar results available malicious noise model model cesabianchi et al 10 show accuracy learning malicious noise lower bounded matching algorithm learning classes similar presented malicious noise presented 8 random classification noise model learning arbitrary small accuracy even noise rate close half possible techniques presented 8 may used learn type classes examine work random classification noise preliminaries section provide basic definitions related learning pac model without noise learning task specified using concept class denoted c boolean concepts defined instance space denoted x boolean concept c function c x 7 f0 1g concept class c set boolean concepts c f0 1g x throughout paper sometimes treat concept set points instead boolean function set corresponds concept c simply 1g use c denote function corresponding set interchangeably specifically probability distribution defined x use notation dc refer probability point x drawn x according 21 classical pac model probably approximately correct pac model originally presented valiant 22 model learning algorithm access oracle pac returns call labeled example according fixed distribution x unknown learning algorithm c 2 c target function learning algorithm learn definition 1 class c boolean functions paclearnable using hypothesis class h polynomial time exists algorithm c 2 c input parameters distribution x given access pac oracle runs time polynomial log jx j 1ffi 1ffl probability least outputs function h 2 h pr 22 models learning presence noise next define model paclearning presence nasty sample noise nsn short model learning algorithm concept class c given access adversarial oracle nsn cj learning algorithm allowed call oracle single run learning algorithm passes single natural number oracle specifying size sample needs gets return labeled sample 2 x theta f0 1g assumed simplicity algorithm knows advance number examples needs extension model scenarios bound available advance given section 6 sample required learning algorithm constructed follows pac model distribution instance space x defined target concept c 2 c chosen adversary draws sample g points x according distribution full knowledge learning algorithm target function c distribution sample drawn adversary chooses points sample es g random variable e points chosen adversary removed sample replaced pointandlabel pairs adversary chosen adversary remain unchanged labeled correct labels according c modified sample points denoted given learning algorithm limitation adversary number examples may modify distributed according binomial distribution parameters j namely probability taken first choosing g 2 choosing e according corresponding random variable es g definition 2 algorithm said learn class c nasty sample noise rate j 0 accuracy parameter ffl 0 confidence parameter access oracle nsn cj distribution target c 2 c outputs hypothesis h x 7 f0 1g probability least pr also interested restriction model call nasty classification noise learning model ncn short difference ncn nsn models ncn adversary allowed modify labels e chosen samplepoints cannot modify e points previous models learning presence noise also readily shown restrictions nasty sample noise model malicious noise model corresponds nasty noise model adversary restricted introducing noise points chosen uniformly random probability j original sample random classification noise model corresponds nasty classification noise model adversary restricted noise introduced points chosen uniformly random probability j original sample point chosen gets label flipped 23 vc theory basics vcdimension 24 widely used learning theory measure complexity concept classes vcdimension class c denoted vcdimc maximal integer exists subset x size 2 possible behaviors present class c subset exists natural well known eg 4 two classes c h x class negations fcjx n c 2 cg vcdimension class c class unions fc hjc 2 c h 2 hg vcdimension vcdimcvcdimh1 following 3 define dual concept class definition 3 dual h f0 1g h class h f0 1g x defined set defined x view concept class h boolean matrix row represents concept column point instance space x matrix corresponding h transpose matrix corresponding h following claim 3 gives tight bound vc dimension dual class 1 every class h log following discussion limit instance spaces x finite cardinality main use make vcdimension constructing ffnets following definition theorem 7 definition 4 set points x ffnet concept class h f0 1g x distribution x every h 2 h dh ff h 6 theorem 1 class h f0 1g x vcdimension distribution x ae 4 ff log 2 ff log 13 ff oe examples drawn iid x according distribution constitute ffnet h probability least 1 gamma ffi 21 talagrand proved similar result definition 5 set points x ffsample concept class h f0 1g x distribution x holds every h 2 h theorem 2 constant c 1 class h f0 1g x vcdimension distribution x ff 0 examples drawn iid x according distribution constitute ffsample h probability least 24 consistency algorithms let p n subsets points x say function h x 7 f0 1g consistent positive point x 2 p negative point x 2 n consistency algorithm see 8 pair classes c h instance space x c h receives input two subsets instance space runs time tjp n j satisfies following function c consistent p n algorithm outputs yes h 2 h consistent p n algorithm outputs consistent exist restriction output case consistent function h c given subset points instance space q x interested set possible partitions q positive negative examples function h 2 h function c 2 c consistent partition may formulated con consistency algorithm c h following based sauers lemma 19 lemma 1 set points q furthermore efficient algorithm generating set partitions along corresponding functions h presented assuming c paclearnable h constant vc dimension algorithms output denoted h consistent information theoretic lower bound section show learning algorithm even inefficient ones learn non trivial concept class accuracy ffl better 2j nsn model fact prove impossibility result holds even ncn model also give results size samples required learn nsn model accuracy ffl 2j definition class c instance space x called nontrivial exist two points theorem 3 let c nontrivial concept class j noise rate ffl 2j accuracy parameter algorithm learns concept class c accuracy ffl ncn model rate j proof base proof method induced distributions introduced 17 theorem 1 show two concepts distribution adversary force labeled examples shown learning algorithm distributed identically c 1 target c 2 target let c 1 c 2 two concepts whose existence guaranteed fact c nontrivial class let x two points satisfy c 1 define probability distribution dx 1 g clearly indeed prd c 1 define nasty adversary strategy respect probability distribution let size sample asked learning algorithm adversary starts drawing sample g points according distribution occurrence x 1 sample adversary labels correctly according c occurrence x 2 adversary tosses coin probability 12 labels point correctly ie c flips label resulted sample examples given adversary learning algorithm first argue number examples modified adversary indeed distributed according binomial distribution parameters j view adversary picks independently points decides whether flip label hence suffices show example labeled incorrectly probability j independently examples indeed example independently probability labeled incorrectly equals probability choosing x 2 according times probability adversary chooses flip label x 2 example ie 2j delta needed emphasize binomial distribution obtained known adversary next observe matter whether target c 1 c 2 examples given learning algorithm modified nasty adversary distributed according following probability distribution therefore according sample learning algorithm sees impossible differentiate case target function c 1 case target function c 2 note proof indeed take advantage nastiness adversary unlike malicious adversary adversary focus power point x 2 causing suffer relatively high error rate examples point x 1 suffer noise also took advantage fact e number modified examples allowed depend sample case depends number times x 2 appears original sample allows adversary focus destructive power samples otherwise good learning algorithm finally since ncn adversary also nsn adversary theorem 3 implies following corollary 4 let c nontrivial concept class j 0 noise rate ffl 2j accuracy parameter algorithm learns concept class c accuracy ffl nsn model noise rate j settled 2j lower bound accuracy possible nasty adversary error rate j turn question number examples necessary learn concept class accuracy section considering informationtheoretic issues results similar presented cesabianchi et al 10 malicious noise model note however definition margin delta used relative lower bound different one used 10 proofs results use following claim see 10 provides lower bound probability random variable binomial distribution deviates expectation standard deviation 2 10 fact 32 let snp random variable distributed binomial distribution parameters n p let p n 37pq pr ki 19 1 pr ki 19 2 theorem 5 nontrivial concept class c noise rate j 0 confidence parameter sample size needed pac learning c accuracy confidence tolerating nasty classification noise rate j least omega proof let c 1 c 2 two concepts whose existence guaranteed fact c nontrivial class let x two points satisfy c 1 let us define distribution gives weight ffl point x 2 weight 1 gamma ffl x 1 making f target function either c 1 c 2 nasty classification adversary use following strategy pair form sample probability jffl reverse label ie present learning algorithm pair instead rest sample pairs form unmodified note examples probability classification changed therefore exactly ffl delta j number points suffer noise indeed distributed according binomial distribution parameters j induced probability distribution sample learning algorithm sees contradiction let possibly randomized algorithm learns c accuracy ffl using sample generated oracle whose size p error hypothesis h outputs using examples let b bayes strategy outputting c 1 majority instances x 2 labeled c 1 clearly strategy minimizes probability choosing wrong hypothesis implies define following two events runs b samples size let n denote number examples showing point x 2 bad 1 event least dn2e corrupted bad 2 event n 36jj answer incorrectly examples showing x 2 wrong label examples showing x 2 correct label examine probability bad 2 occur note n random variable distributed binomial distribution parameters ffl recall delta interested since probability n large higher larger upper bounded 17j1 gamma may bounded using hoeffdings inequality least therefore hand assume bad 2 holds namely n 36jj additionally assume n 372j delta 2 jj delta claim 2 using following inequality follows prbad 1 see inequality equation 3 indeed holds bad 2 holds note 3 implied turn implied two conditions2 verified two conditions hold take n range assume 2 optimal strategy hence worse strategy ignores sample points error decrease points shown x 2 therefore results hold remove lower bound n thus prbad 1 second type lower bound number required examples based vc dimension class learned similar results proof techniques 7 standard pac model 2 conditions delta must least one integer range assume n theorem 6 concept class c vcdimension 3 0 ffl 18 112 sample size required learn c accuracy ffl confidence ffi using samples generated nasty classification adversary error rate delta greater omega set points shattered c define probability distribution follows assume contradiction gamma 232delta examples used learning algorithm let nasty adversary behave follows reverses label example x dgamma1 probability 12 independent sample points making labels x dgamma1 appear random noise also note probability example corrupted adversary exactly 2j delta j thus probability 12 point x dgamma1 misclassified learners hypothesis rest sample left unmodified denote bad 1 event least half points x seen learning algorithm given bad 1 denote set gamma 22 unseen points lowest indices define bad 2 event algorithms hypothesis misclassifies least gamma 28 points finally let bad 3 denote event x dgamma1 misclassified easy see bad 1 bad 2 bad 3 imply hypothesis error least ffl implies hypothesis errs gamma 28 points points weight 8deltad gamma 2 point x whose weight 2j making total error least delta therefore algorithm learn class confidence ffi must hold prbad 1 bad 2 bad 3 noted x dgamma1 appears labeled random noise hence prbad 3 independent bad 1 bad 2 thus events since gamma 232delta examples seen expected number points x learning algorithm sees gamma 24 markov inequality follows probability least 1 2 gamma 22 points seen hence 2 every unseen point misclassified learning algorithm probability least half since point adversary may set target label point label lower probability given algorithm thus prbad 2 jbad 1 probability fair coin flipped gamma 22 times shows heads least gamma 28 times using 10 fact 33 probability shown least 13 thus completes proof since learning nasty sample noise easier learning nasty classification noise results theorems 5 6 also hold learning nasty sample noise oracle information theoretic upper bound section provide positive result complements negative result section 3 result shows given sufficiently large sample hypothesis performs sufficiently well sample even sample subject nasty noise satisfies pac learning condition formally analyze following generic algorithm learning class c vcdimension whose inputs certainty parameter ffi 0 nasty error rate parameter 2 required algorithm nastyconsistent 1 request sample 2 output h 2 c h exists choose h 2 c arbitrarily theorem 7 let c class vcdimension constant c algorithm nasty consistent pac learning algorithm nasty sample noise rate j proof theorem well analysis algorithm next section use convenience slightly weaker definition pac learnability one used definition 1 require algorithm output probability least pr rather strict inequality however use algorithm give slightly smaller accuracy parameter eg ffl ffl get algorithm learns using original criterion definition 1 proof first argue high probability number sample points modified adversary mj delta4 random variable e distributed according binomial distribution expectation jm may use hoeffdings inequality 14 get pr choice c event happens probability ffi2 note target function c errs e points sample shown learning algorithm completely accurate nonmodified sample g thus probability least nastyconsistent able choose function h 2 c errs j delta4m points sample shown however worst case errors function h occur points modified adversary addition h may erroneous points adversary modify therefore guaranteed case hypothesis h errs 2e points original sample g theorem 2 exists constant c probability taking g size least c resulting sample g delta sample class symmetric differences functions c union bound therefore probability least delta4m meaning js delta2m g delta2sample class symmetric differences pr required 5 composition theorem learning nasty noise following 3 8 define notion composition class let c class boolean functions define class c set boolean functions f x represented fg 1 boolean function g 2 c define size fg k given vector hypotheses following 8 set wh set subdomains w ag possible vectors 2 f0 1g show variation algorithm presented 8 learn class c nasty sample adversary assuming class c paclearnable class h constant vc dimension algorithm builds fact consistency algorithm con c h constructed given algorithm pac learns c h 8 algorithm learn concept class c confidence parameter ffi accuracy ffl arbitrarily close lower bound 2j proved previous section sample complexity computational complexity polynomial k 1ffi 1delta algorithm based following idea request large sample oracle randomly pick smaller subsample sample retrieved randomly picking subsample algorithm neutralize power adversary since adversary cannot know examples ones informative us use consistency algorithm c h find one representative h possible behavior smaller subsample hypotheses h define division instance space cells cell characterized specific behavior hypotheses picked final hypotheses simply based taking majority vote among complete sample inside cell demonstrate algorithm let us consider informally specific relatively simple case class learned class k intervals straight line see figure 1 algorithm given sample input proceeds follows 1 algorithm uses relatively small random subsample divide line subintervals two adjacent points subsample define subinterval 2 subinterval algorithm calculates majority vote complete sample result hypothesis number points specific case number subintervals algorithm chooses first step depends k intuitively want total weight subintervals containing targets endpoints relatively small called bad part formal analysis follows naturally 2k bad subintervals larger k target concept subsample intervals bad bad bad bad algorithms hypothesis figure 1 example nastylearn intervals larger subsample needed except bad subintervals subintervals algorithm errs least half points modified adversary thus total error roughly 2j plus weight bad subintervals proceed formal description learning algorithm given constant size k target function bound error rate j parameters ffi delta two additional parameters mn specified algorithm proceeds follows algorithm nastylearn 1 request sample size n 2 choose uniformly random subsample r size 3 use consistency algorithm c h compute 4 output hypotheses hh computed follows w 2 wh empty set h majority labels w w empty set h 0 x 2 w theorem 8 let log 8 log 78k constants algorithm nastylearn learns class c accuracy confidence ffi time polynomial k 1 theorem 7 theorem refers modified pac criterion require algorithm output probability least 1 gamma ffi function h pr technique mentioned algorithm nastyconsistent may used modify algorithm pac learning algorithm sense definition 1 commencing actual proof present technical lemma lemma 2 assuming n set statement theorem 8 probability least 4 number points errors introduced e j delta12n proof lemma 2 note definition model e distributed according binomial distribution parameters j n thus e behaves number successes independent bernoulli experiments hoeffding inequality 14 may used bound value pr therefore take n 72 ln4ffi probability least 4 e j delta12n note value chosen n statement theorem 8 clearly large enough ready present proof theorem 8 proof analyze error made hypothesis algorithm generates let us denote adversarys strategy follows 1 generate sample requested size n according distribution label target concept f denote sample g 2 choose subset g size random variable defined section 22 3 choose maliciously set points x theta f0 1g size e 4 hand learning algorithm sample assume target function f form hypothesis algorithm chosen step 3 exhibits behavior g points r definition scon guaranteed hypothesis exists definition points r h j vcdimension class c g class h h class possible symmetric differences also vcdimension od see section 23 applying theorem 1 viewing r sample taken according uniform distribution choosing statement theorem r ffnet respect uniform distribution class symmetric differences least 1 gamma ffi4 note may still points h j 4g hence let using 4 get probability least 1 gamma ffi4 simultaneously every subdomain b 2 wh n b js bj words nb n simply stand size restriction original noisefree sample g noisy examples introduced adversary subdomain b rest definitions based distinction good part b g h j behave bad part present due fact g h j exhibit behavior smaller subsample r rather complete sample use n ff b denote number sample points bad part b n outg b denote number sample points removed adversary good bad parts b respectively since learning algorithm decides classification subdomain majority vote hypothesis err domain b number examples left untouched b less number examples b modified adversary plus misclassified h j respect g may formulated following condition n therefore total error algorithm may experience b nbn calculate bound two terms separately bound second term note theorem 2 choice n guarantees g delta sample domain probability least 1 gamma ffi4 note definition wh sauer lemma 19 jwh choice n indeed guarantees probability least b nbn b nbn n choice n follows g also delta sample class symmetric differences form h j 4g thus probability least 1 gamma ffi4 total error made hypothesis assuming none four bad events happen therefore bounded pr n required bound holds certainty least 1 gamma ffi 6 conclusion presented model pac learning nasty noise generalizing previous models proved negative informationtheoretic result showing learning algorithm learn nontrivial class accuracy better 2j paired positive result showing bound tight complemented results lower bounds sample size required learning accuracy 2j delta also shown wide variety interesting concept classes efficient learning algorithm model exists negative result generalized case learning algorithm uses randomized hypotheses coin rules defined 10 case get information theoretic lower bound j achievable accuracy compared lower bound j21 proved 10 learning malicious noise rate j partition two separate variants nsn ncn models seem intuitive wellmotivated remains open problem come results actually separate two models negative positive results presented work apply equally nsn ncn models finally note definition nasty noise model requires learning algorithm know advance sample size upper bound model however extended deal scenarios bound known learning algorithm several scenarios kind example sample complexity may depend certain parameters size target function known algorithm 3 adversary knows learning algorithm knows target function general know parameters hidden learning algorithm thus plan ahead draw advance sample g size sufficiently large satisfy high probability requests learning algorithm make modifies g defined reorders resulted sample randomly 4 learning algorithm simply asks one example time pac model adversary supplies next example randomlyordered set sample exhausted may happen cases expected samplecomplexity guarantee say learning algorithm failed however using large enough sample respect 1ffi happen sufficiently small probability r general bounds statistical query learning pac learning noise via hypothesis boosting learning noisy examples composition theorem learning algorithms applications geometric concept classes combinatorial variability vapnikchervonenkis classes applications sample compression schemes learning unreliable boundary queries weakly learning dnf characterizing statistical query learning using fourier analysis learnability vapnikchervonenkis dimension new composition theorem learning algorithms noisetolerant distributionfree learning general geometric concepts sampleefficient strategies learning presence noise learning hybrid noise environments using statistical queries pac learning constantpartition classification noise applications decision tree induction learning noisy incomplete examples probability inequalities sums bounded random variables efficient noisetolerant learning statistical queries learning presence malicious toward efficient agnostic learning density families sets design analysis efficient learning algorithms sharper bounds gaussian empirical processes theory learnable learning disjunctions conjunctions uniform convergence relative frequencies events probabilities tr theory learnable learnability vapnikchervonenkis dimension design analysis efficient learning algorithms learning presence malicious errors efficient noisetolerant learning statistical queries weakly learning dnf characterizing statistical query learning using fourier analysis toward efficient agnostic learning learning unreliable boundary queries learning noisy incomplete examples noisetolerant distributionfree learning general geometric concepts composition theorem learning algorithms applications geometric concept classes new composition theorem learning algorithms combinatorial variability vapnikchervonenkis classes applications sample compression schemes sampleefficient strategies learning presence noise learning noisy examples ctr marco barreno blaine nelson russell sears anthony joseph j tygar machine learning secure proceedings 2006 acm symposium information computer communications security march 2124 2006 taipei taiwan