spacetimeefficient scheduling execution parallel irregular computations article investigate tradeoff time space efficiency scheduling executing parallel irregular computations distributedmemory machines employ acyclic task dependence graphs model irregular parallelism mixed granularity use direct remote memory access support fast communication propose new scheduling techniques runtime active memory management scheme improve memory utilization retaining good time efficiency provide theoretical analysis correctness performance work implemented context rapid system uses inspectorexecutor approach parallelize irregular computations runti demostrate effectiveness proposed techniques several irregular applications sparse matrix code fast multipole method particle simulation experimental results crayt3e show problems large sizes solved limited space capacity loss execution efficiency caused extra memory management overhead reasonable b introduction considerable effort parallel system research spent timeefficient par allelizations article investigates tradeoff time space efficiency executing irregular computations memory capacity processor limited since timeefficient parallelization may lead extra space requirements definition irregular computation literature actually clear normally scientific computing code chaotic adaptive computation communication patterns considered irregular providing effective system support low overhead irregular problems difficult identified one key issues parallel system research kennedy 1996 number projects addressed software techniques parallelizing different classes irregular code wen et al 1995 das et al 1994 lain banerjee 1994 gerasoulis authors addresses yang department computer science university california santa barbara ca 93106 c fu siemens pyramid mailstop sj1210 san jose ca 95134 work supported part nsf ccr9409695 cda9529418 int9513361 ccr9702640 darpa subcontract umd z883603 preliminary version article appeared 6th acm symposium principles practice parallel programming ppopp97 yang c fu et al 1995 fink et al 1996 article addresses scheduling issues parallelism modeled directed acyclic dependence graphs dags sarkar 1989 mixed granularity model found useful performance prediction code optimization parallel applications static slowly changing dependence patterns sparse matrix problems particle simulations chong et al 1995 fu yang 1996b gerasoulis et al 1995 jiao 1996 problems communication computation irregularly interleaved varying granularity asynchronous scheduling fast communication techniques needed exploiting data locality balancing loads low synchronization costs however techniques may impose extra space requirements example direct memory access normally much lower software overhead highlevel messagepassing primitives remote addresses however need known time performing data accesses thus accessible remote space must allocated advance scheduling optimization technique may prefetch presend data objects overlap computation communication may require extra temporary space hold data objects therefore using advanced optimization techniques adds difficulties designing software support achieve high utilization processor memory resources article present two dag scheduling algorithms minimize space usage retaining good parallel time efficiency basic idea use volatile objects early possible space released reuse designed active memory management scheme incrementally allocates necessary space processor efficiently executes dag schedule using direct remote memory access provide analysis performance correctness techniques proposed techniques implemented rapid programming tool fu yang 1996a parallelizes irregular applications runtime original schedule execution scheme rapid support incremental memory allo cation sufficient space rapid delivers good performance several tested irregular programs sparse cholesky factorization sparse triangular solvers fu yang 1997 fast multipole method nbody simulation fu 1997 particular show rapid used parallelize sparse lu gaussian elimination dynamic partial pivoting important open parallelization problem literature deliver high megaflops crayt3dt3e fu yang 1996b another usage rapid performance prediction since static scheduler rapid predict potential speedup given dag reasonable accuracy example rapid sparse lu code achieves 70 predicted speedup fu yang 1996b found size problems rapid solve restricted available amount memory motivates us study space optimization noted forms space overhead exist space operating system kernel hash tables indexing irregular objects task graphs article focuses optimization space usage dedicated storing content data objects rest article organized follows section 2 summarizes related work section 3 describes computation memory model section 4 presents spaceefficient scheduling algorithms section 5 discusses use yang c fu delta 3 scheduling algorithms rapid active memory management scheme executing schedules section 6 gives experimental results 2 related work previous research static dag scheduling sarkar 1989 wolski feo 1993 yang gerasoulis 1992 1994 address memory issues scheduling algorithm dynamic dags proposed blelloch et al 1995 requires space usage processor 1 sequential space quirement p total number processors depth dag work provides solid theoretical ground spaceefficient scheduling space model different since assumes globally shared memory pool model space upper bound imposed individual processor stronger constraint cilk runtime system blumofe et al 1995 addresses space efficiency issues space complexity os 1 per processor good general high solving large problems space limited memoryoptimizing techniques proposed article space usage close 1 p per processor another distinct difference work blelloch et al 1995 blumofe et al 1995 rapid dag obtained runtime inspector stage execution thus making scheduling scheme static models dags grow onthefly computation proceeds dynamic scheduling preferred two reasons use static scheduling 1 practice difficult minimize runtime control overhead dynamic scheduling parallelizing sparse code mixed granularity distributed memory machines 2 application problems consider iterative nature optimized static schedules used many iterations work uses hardware support directly accessing remote memory available several modern parallel architectures workstation clusters ibel et al 1996 stricker et al 1995 schauser scheiman 1995 advantages direct remote memory access identified fast communication research active messages von eicken et al 1992 thus expect researchers benefit results using fast communication support design software layers 3 computation memory model computation model consists set tasks set distinct data objects task reads writes subset data objects data dependence graphs ddg derived partitioned code normally three types dependence tasks true dependence antidependence output dependence poly chronopoulos 1988 ddg anti output dependence edges may redundant subsumed true data dependence edges antioutput dependence edges eliminated program transformation article deals transformed dependence graph contains acyclic true dependencies extension classical task graph model commuting tasks marked task graph capture parallelism arising commutative operations details parallelism model described fu yang 1996a1997 define terms used task model follows yang c fu dag contains set tasks directed dependence edges tasks data objects accessed task let rt x set objects task x reads w x set objects task x writes data object assigned unique owner processor processor may allocate temporary space objects processor p x owns called permanent object p x called volatile object p x define permp x set permanent data objects processor p x v olat ilep x set volatile data objects processor p x permanent object stay allocated execution owner processor static schedule gives processor assignment tasks execution order tasks processor define tap x set tasks executed processor p x let term x executed processor schedule let x denote task x executed immediately given dag g static schedule g scheduled graph derived marking execution edges g eg add edge x g x schedule legal corresponding scheduled graph acyclic x dependence edge g task x weight denoting predicted computation time called x complete task edge x weight denoting predicted communication delay called c xy sending corresponding message two processors using weight information static scheduling algorithm optimize expected parallel time execution3711158t 1 b c fig 1 dag b schedule dag 2 processors c another schedule yang c fu delta 5 figure 1a shows dag 20 tasks access 11 data objects task represented either ij reads j writes j j reads writes j table lists read write sets tasks parts b c figure two schedules dag permanent objects evenly distributed two processors g g owner computes rule used assign tasks processors example assume task message cost one unit time messages sent asynchronously processor overhead sending receiving messages ignored table read write sets tasks figure 1a notice task graph model use ssa form static single assignment cytron et al 1991 notice different tasks modify data object main reason targeted applications task graph derived runtime normally data dependent ssa form used would many objects manage spacetimeefficient scheduling algorithm needs balance two conflicting goals shortening length schedule minimizing space requirement memory sufficient hold objects execution stage space recycling volatile data necessary following two strategies used release space volatile data object certain execution point 1 value object longer used 2 object longer accessed discuss section 5 second strategy reduces overhead data management therefore choose approach strategy space requirement estimated follows definition 1 given task execution order processor volatile object processor called live task accessed task accessed still accessed future formally following holds live 9t x 9t z x otherwise called dead definition 2 let sizem size data object task tw processor p x ie tw 2 tap x compute volatile space requirement tw p x live volatile object tw yang c fu memory requirement schedule 8px f schedule figure 1b volatile object 3 processor p 1 dead task 310 5 dead 510 assume data object unit size easy calculate mem 9 however schedule figure 1c mem req 8 lifetime volatile objects 7 3 disjoint p 1 space sharing possible 4 space time efficient scheduling given dag p processors scheduling approach contains two stages owner computes rule used assign tasks processors tasks modify data objects mapped cluster clusters evenly assigned processors example given dag figure 1a two processors set tasks assigned processor p 0 g set tasks processor p 1 ft g based task assign ment determine permanent volatile data objects processor tasks processor ordered following dag dependence edges focus optimization task ordering previously ordering algorithm called yang gerasoulis 1992 proposed time efficient may require extra space hold volatile objects order aggressively execute timecritical tasks section propose two new ordering algorithms called mpo dts idea volatile objects referenced early possible available local memory shortens lifetime volatile objects potentially reduces memory requirement processor 41 rcp algorithm briefly discuss algorithm detailed description found yang gerasoulis 1992 heuristic orders tasks simulating execution tasks let exit task task children given dag time priority task x called tp x length longest path task exit task namely tp x x exit task otherwise ty child simulated execution task called ready parents selected execution needed data objects received point scheduling cycle processor selects ready task highest time priority yang c fu delta 7 1 least one unscheduled task 2 find processor px earliest idle time 3 schedule ready task tx highest priority processor px update ready task list processor endwhile fig 2 rcp algorithm rcp algorithm summarized figure 2 lines 4 5 exist description want illustrate difference rcp mpo 42 mpo memorypriorityguided ordering mpo ordering algorithm listed figure 3 difference mpo rcp 1 priority used mpo selecting ready task size total object space allocated divided size total object space needed execute task tie rcp time priority tp used break tie 2 mpo needs estimate total object space allocated scheduling cycle update memory priority tasks lines 4 5 1 least one unscheduled task 2 find processor px earliest idle time 3 schedule ready task tx highest priority processor px allocate volatile objects tx uses allocated yet processor px 5 update priorities txs children siblings processor px update ready task list processor endwhile fig 3 mpo algorithm example figure 1c shows schedule produced mpo b produced rcp ordering difference figure 1b c starts time 6 processor 1 time 78 selected rcp 310 chosen mpo illustrated figure 4 three ready tasks processor 1 time selects 78 longest path exit task path length 4 mpo 310 highest space priority 1 3 10 available locally time 6 78 space priority 05 space 7 allocated time 6 assume object unit size result mpo schedule requires less memory longer parallel time algorithm complexity potentially time consuming part update space priorities unscheduled tasks line 5 sufficient update space priorities children siblings newly scheduled task tasks possible candidates ready tasks next round space priorities children candidate tasks updated later candidate tasks scheduled use e x e x denote number yang c fu 05 ready task list proc 1 b fig 4 scheduling scenario time 6 numbers parentheses next ready tasks mpo space priority rcp time priority time remaining unscheduled tasks b partial schedule time 6 incoming edges outgoing edges task x respectively complexity line 5 number tasks e number dependence edges term e x v used v bound number children parent x may additionally complexities lines 2 3 4 6 v log p v log v vp log v respectively p total number processors total number data objects since p usually small compared v total time complexity mpo ove 43 dts dataaccessdirected time slicing dts aggressive space optimization thus intended cases memory usage primary importance design based fact memory usage processor improved volatile object short life span words time period allocation deallocation short basic idea dts slice computation based data access patterns tasks tasks within slice access small group volatile objects tasks scheduled physical processors slice slice tasks within slice ordered using dependence criticalpath information algorithm given dag set tasks operates set data objects describe steps dts algorithm follows 1 construct data connection graph dcg node dcg represents distinct data object edge represents temporal order data access computation cycle may occur accesses two data objects interleaved simplicity use name data object corresponding data node unless cause confusion construct dcg following rules applied based original dag task x 2 v uses modify data object x associated data object node yang c fu delta 9 x modifies one objects may use objects compu tation use objects x associated modified objects possible task associated multiple data nodes case doubly directed edges added among data nodes make strongly connected directed edge added data node data node j exists task dependence edge x x associated data node associated data node j last two rules reflect temporal order data access computation step 2 derive strongly connected components dcg edges components constitute dag task appears one component component associated set tasks usemodify data objects component defined one slice tasks slice considered scheduling together runtime processor execute tasks slice slice following topological order slices imposed dependencies among corresponding strongly connected components noted topological order slices imposes constraint task ordering processor assignment tasks following owner computes rule must supplied using dts produce actual schedule step 3 use priority based precedence scheduling approach generate dts schedule slices derived step 2 priorities assigned tasks based slices belong two ready tasks slice task higher criticalpath priority scheduled first ready task slice priority lower unscheduled tasks processor task scheduled tasks higher slice priorities processor scheduled using priority guarantee tasks executed according derived slice order example figure 5 shows example dts ordering dag figure 1a part dcg mark node corresponding data name tasks within rectangle associated corresponding data object since dcg acyclic data node maximal strongly connected component treated one slice topological ordering nodes produces slice processor execute tasks following slice order shown part b slices marked numerically illustrate execution order memory requirement mem req 7 compared 9 figure 1b produced rcp 8 figure 1c mpo hand schedule length increases rcp mpo dts less less critical path information used algorithm complexity step 1 complexity deriving access pattern task ie read andor write access data object oe log v complexity mapping tasks data nodes ov complexity generating edges data nodes oe log check needed prevent duplicate edges added thus total complexity step 1 v step 2 deriving strongly connected components costs yang c fu3711158t1416 d357 slice 1 slice 2 slice 4 slice 5 slice 6 slice 1 slice 2 slice 3 slice 4 slice 5 slice 6 slice 7 b fig 5 sample dcg derived dag b dts schedule dag 2 processors generating precedence edges among slices costs om log topological sorting slices costs om e therefore total complexity step 2 om cost step 3 ov log v e gives overall complexity dts oelog v number tasks e number edges number data objects original dag space efficiency dts lead good memory utilization following theorem gives memory bound dts first definition introduced definition 3 given processor assignment r tasks volatile space requirement slice l processor p x denoted vpx r l defined amount space needed allocate volatile objects used executing tasks l p x maximum volatile space requirement l r defined assuming processor assignment r tasks using owner computes rule produces even distribution data space permanent data objects among processors show following results theorem 1 given processor assignment r tasks dts schedule processors slices ordered l 1 schedule executable space usage per processor 1 sequential space complexity proof first since r leads even distribution permanent objects permanent data space needed processor 1 p suppose task yang c fu delta 11 x slice l needs allocate space volatile object enough space according definition h 1 claim space allocated volatile data objects associated slices l 1 freed therefore extra h space processor enough execute tasks l need show claim correct suppose volatile data object 0 cannot deallocated slice l igamma1 0 associated l least one task 2 l j j uses 0 modifies 0 0 permanent data object modify 0 according dts algorithm belong slice l instead l j thus contradiction 2 dcg acyclic data node dcg constitutes strongly connected component therefore slice associated one data object implies h defined theorem 1 size largest data object thus following corollary corollary 1 dcg task graph acyclic maximum size object unit 1 dts produces schedule executed processors using 1 per processor 1 sequential space complexity apply theorem 1 corollary 1 important application graphs 1d columnblockbased sparse lu dags fu yang 1996b matrix partitioned set sparse column blocks task kj uses one sparse column block k modify column block j figure 1 actually sparse lu graph dts produces acyclic dcg 1d columnblockbased sparse lu task graph illustrated figure 5a let w maximum space needed storing column block according corollary 1 processor need w volatile space execute dts schedule sparse lu 2d blockbased sparse cholesky approach described fu yang 1997 matrix divided n sparse column blocks column block divided n submatrices submatrix index j marked ij cholesky task graph structured n layers layer represents elimination process uses column block k modify column blocks k 1 n specifically cholesky factor computed diagonal block kk used scale nonzero submatrices within kth column block ie ik nonzero submatrices used update rest matrix ie ij update tasks step k belong slice associated data objects ik hence extra space needed execute slice summation submatrices column block k according theorem 1 dts schedule cholesky execute 1 p w space processor w maximum space needed store column block results summarized following corollary normally w dts schedule space efficient two problems corollary 2 1d columnblockbased sparse lu task graphs 2d blockbased sparse cholesky graphs dts schedule executable using 1 yang c fu space 1 i2 k merge l l 0 space else space fig 6 dts slicemerging algorithm per processor w size largest column block partitioned input matrix optimization available memory space processor known say av ail mem time efficiency dts algorithm optimized merging several consecutive slices memory sufficient slices applying prioritybased scheduling algorithm merged slices assuming k slices valid slice order given task assignment r merging strategy summarized figure 6 set new slices generated since calculating memory requirements slices takes oe log time complexity merging process ove log shown merging algorithm produces optimal solution given slice ordering theorem 2 given ordered slice sequence slicemerging algorithm figure 6 produces solution minimum number slices proof theorem proven contradiction let new slice sequence produced algorithm figure 6 optimal sequence u e f slice contains set consecutive l slices l 1 merging algorithm figure 6 groups many first l slices possible hre 1 take original l slices slice f 2 add f 1 new f 1 identical e 1 let new f 2 called 2 thus produce optimal sequence apply transformation f 0 2 comparing finally transform sequence f 1 another optimal sequence contradiction since new sequence cannot completely cover l slices unless slices empty 2 noted optimality merging algorithm restricted given slice ordering interesting topic study exists slice sequencing algorithm follows partial order implied given dcg leads minimum number slices minimum parallel time shown tang 1998 heuristic using binpacking techniques developed number slices within factor two optimum yang c fu delta 13 5 memory management schedule execution section first briefly describe rapid runtime system scheduling techniques applied discuss necessary runtime support efficiently executing dag schedules derived proposed scheduling algorithms 51 rapid system dependence transformation analysis dependence task scheduling clustering user specification tasks data objects data access patterns data dependence graph ddg complete task graph iterative asynchronous task assignments data object owners schedules execution fig 7 process runtime parallelization rapid figure 7 shows runtime parallelization process rapid circle action performed system boxes either side circle represent input output action api rapid includes set library functions specifying irregular data objects tasks access objects inspector stage depicted left three circles figure 7 rapid extracts dag data access patterns produces execution schedule executor stage rightmost circle schedule computation executed eratively since targeted applications iterative nature example sparse matrix factorization used extensively solving set nonlinear differential equations numerical method newtonraphson iterates sparse dependence graph derived jacobian matrix sparse pattern jacobian matrix remains one iteration another nine applications studied karmarkar 1991 typical number iterations executing computation graphs ranges 708 16069 average 5973 thus optimization cost spent inspector stage pays long simulation problems shown fu 1997 rapid inspector stage tested sparse matrix factorization triangular solvers fast multipole method fmm relatively large problem sizes takes 12 total time schedules reused 100 iterations inspector idea found previous scientific computing research george liu 1981 inspector optimization called preprocessing compared previous spectorexecutor systems irregular computations das et al 1994 executor phase rapid deals complicated dependence structures executor stage rapid uses direct remote memory access rma execute schedule derived inspector stage rma available modern multiprocessor architectures crayt3e shmem meiko cs2 dma sci clusters memory mapping rma processor write memory processor given remote address rma allows data transfer directly source destination location without buffering imposes much lower overhead higherlevel communication layer mpi use rma complicates design runtime execution control data consistency yang c fu however find dag generated rapid satisfies following properties simplifies design distinct data objects task x receive data objects identification different parents readwrite dependence path either x x dependence path either x x d4 dag sequentialization tasks executed consistently sequential execution following topological sort dag namely 2 rt x value x reads memory execution produced one x parents general dag satisfies d1 d2 d3 may always sequentializable yang 1993 dag properties called dependencecomplete example dags discussed figure 1 section 6 dependencecomplete ddg derived sequential code transformed dependencecomplete dag fu yang 1997 52 execution scheme active memory management maintaining reusing data space execution new research topic complicated using lowoverhead rmabased communication since remote data addresses must known advance discuss two issues related executing dag schedule rma 1 address consistency address data object processor become stale value data object longer used space object released using classical cache coherence protocol maintain address consistency introduce substantial amount overhead taken simple approach volatile object considered dead object name accessed processor way volatile object name allocated processor strategy lead slightly larger memory requirement reduces complexity maintaining address consistency memory requirement estimated section 3 follows design strategy 2 address buffering also use rma transfer addresses since address packages sent infrequently use address buffering processor cannot send new address information unless destination processor read previous address package reduces management overhead execution model using active memory management scheme presented map memory allocation point inserted dynamically two consecutive tasks executed processor first map always beginning execution processor map following deallocate space dead volatile objects dead information statically calculated performing data flow analysis given dag complexity proportional size graph yang c fu delta 15 map allocate d8 map allocate d1 d3 d5 addrs d1 d3 d5 d8 addr d3 suspended send d3 d5 suspended d7 map allocate d7 addr d7 send d7 send d8 map stop data objects ready map yes rec end ra cq next task send addrs complete computation b fig 8 maps executing schedule figure 2c bthe control flow processor allocate volatile space tasks executed current point execution chain assuming remaining tasks processor allocation stop k enough space executing k1 next map right k1 assemble address packages processors address packages may differ depending objects accessed processors figure 8a illustrates maps address notification executing schedule figure 1c available amount memory 8 processor 2 units memory volatile objects p 1 addition maps beginning task chain another map right task 510 1 space 3 5 freed space 7 allocated address 7 p 1 sent p 0 p 0 send content 7 p 1 receives address 7 figure 8b shows control flow execution scheme system five different states execution 1 rec waiting receive desired data objects processor rec state cannot proceed objects current task needs available locally 2 exe executing task 3 snd sending messages remote address message available message enqueued map processor could blocked map state attempts send address packages processors previous address package consumed destination processor end state processor executed tasks still needs clear send queue might blocked addresses suspended yang c fu messages still unavailable three blocking states ie states selfcycles figure 8b following two operations must conducted frequently order avoid deadlock make execution evolve quickly ra read new address package deliver suspended messages addresses available 53 analysis deadlock consistency theorem 3 given dag g legal schedule execution active memory management deadlock free namely system eventually executes tasks proof assume communication processors reliable prove theorem induction use following two facts proof fact 1 deadlock situation happens processors blocked waiting cycle eg circular chain state rec map end eventually processors circular chain two things ra cq space allocation releasing activities complete fact 2 processor waiting receive data object local address data object must already notified processors processor always allocates space sends addresses objects using objects let g 0 scheduled graph g g 0 acyclic given schedule legal without loss generality assume topological sort g 0 produces linear task order induction follows order induction base 1 must entry task g 0 ie task without parents execution 1 processor called map executed completing space allocation p x send newly created addresses deadlock occurs processors involved circular waiting chain p x blocked state map awaiting availability address buffers destination processors ra cq since destination processors must circular chain also ra cq according fact 1 address buffers eventually free newly created p x sent p x able leave map state parent data 1 needs available locally hence 1 complete successfully induction assumption assume tasks x 1 x execution k parents g 0 completed execution show k executed successfully suppose ie deadlock occurs let p x k processor state p x either map rec cannot state end discuss following two cases case 1 p x state rec induction assumption reason p x cannot receive data object k object sent yang c fu delta 17 remote processor p since k parents finished cause p send data object unavailability remote address p x according fact 2 address must already sent p p x waiting receive object hence p eventually read address operation ra fact 1 deliver message p x therefore p x execute k case 2 p x state map situation one discussed induction base processor able leave map state 2 theorem 4 given dependencecomplete dag g legal schedule execution active memory management consistent namely task reads data objects produced parents specified g proof theorem 3 tasks executed runtime scheme dependence edge x check indeed reads object produced x execution illustrated figure 9 two cases could read inconsistent copy prove contradiction impossible assume scheduled processor p x scheduled p x time time dependence pathedge writesend object case 1 case 2 fig 9 illustration two cases proving theorem 4 case 1 senderside inconsistency p 6 p x execution x p x tries send p message may suspended destination address may available since buffering used content p x may modified actually sent p time assume case true let u task overwrites processor sent p u must intend produce another task v p x since execution u x happens v according property d2 must exist dependence path x v u according property d3 must exist dependence path u x yang c fu exists dependence path x u order among x u sequential execution must x would able read copy produced x contradicts property d4 exists dependence path u x similarly show v would able read produced u sequential execution contradicts property d4 case 2 receiversite inconsistency object produced x successfully delivered local memory p content p may overwritten another task called u time assume case true let v task assigned p task supposed read produced u according property d1 v 6 illustrated figure 9 according properties d2 d3 dependence structure among case 1 similarly find contradiction 2 6 experimental results implemented proposed scheduling heuristics active memory management scheme rapid crayt3dt3e meiko cs2 section report performance approach t3e three irregular programs sparse cholesky factorization 2d block data mapping rothberg 1992 rothberg schreiber 1994 fu yang 1997 task graph static dependence structure long nonzero pattern sparse matrix given runtime preprocessing stage sparse gaussian elimination lu factorization partial pivoting problem unpredictable dependence storage structures due dynamic piv oting parallelization sharedmemory platforms addressed li 1996 however efficient parallelization distributedmemory machines still remains open problem scientific computing literature used static factorization approach estimate worstcase dependence structure storage need fu yang 1996b show approach overestimate space much tested matrices rapid code deliver breakthrough performance t3dt3e 1 fast multipole method fmm simulating movement nonuniformly distributed particles given irregular particle distribution spatial domain divided boxes different levels represented hierarchical tree leaf contains number particles iteration particle simulation fmm computation consists upward downward passes tree end iteration particle may move one leaf another computation communication weights dag represents fmm computation may change slightly since particle movement normally slow dag representing fmm computation reused many iterations found jiao 1996 static scheduling reused approximately 100 iterations without much performance degradation recently optimized code using special scheduling mechanism eliminating rapid control overhead set new performance record shen et al 1998 yang c fu delta 19 detailed description fmm parallelization using rapid found fu 1997 first examine memorymanaging scheme impacts parallel performance space limited study effectiveness scheduling heuristics reducing memory requirements reason presentation order proposed scheduling algorithms effective without proper runtime memory management presentation order also allows us separate impact runtime memory management new scheduling algorithms t3e machine use 128mb memory per node blas3 gemm routine dongarra et al 1988 achieve 388 megaflops rma primitive shmem put achieve 052s overhead 500mbsec peak bandwidth test matrices used article harwellboeing matrix bcsstk29 arising structural engineering analysis sparse cholesky goodwin matrix fluid mechanics problem sparse lu matrices medium size 2 solvable one three scheduling heuristics compare performance fmm used distribution 64k particles experiments test cases reach similar conclusions reporting parallel time different memory constraints manually control available memory space processor 75 50 40 25 tot tot total memory space needed given task schedule without space recycling obtain tot calculate sum space permanent volatile objects accessed processor let tot maximum value among processors 61 rapid without active memory management 7051525performance sparse cholesky factorization t3e processors 70103050performance fast multipole method t3e processors b fig 10 speedups without memory optimization sparse cholesky b fmm 2 bcsstk29 dimension 13992 18 million nonzeros including fillins goodwin dimension 7320 35 million nonzeros including fillins yang c fu table ii absolute performance megaflops sparse lu partial pivoting matrix p2 p4 p8 p16 p32 p64 goodwin 736 1357 2380 3737 5226 6558 rapid without memory management figure 10 table ii show overall performance rapid t3e without using memory optimization three test programs version rapid recycle space executor stage results serve comparison basis assessing performance memory management scheme note speedups cholesky fmm compared highquality sequential code results consistent previous work rothberg 1992 jiao 1996 speedup cholesky reasonable since deal sparse matrices speedup fmm high leaf nodes fmm hierarchical tree normally computationintensive sufficient parallelism sparse lu since approach uses static symbolic factorization overestimates computation list megaflops performance calculating megaflops use accurate operation counts superlu li 1996 divide corresponding numerical factorization time rapid active memory management table iii examines performance degradation using active memory management rcp still used task ordering show later much improvement space efficiency obtained using dts mpo results table sparse cholesky bcsstk29 sparse lu goodwin fmm different space constraints column pt inc ratio parallel time increase using memory management scheme comparison base parallel time rcp schedule 100 memory available without memory management overhead entries marked 1 imply corresponding schedule nonexecutable memory constraint results basically show trend performance degradation increases number processors increases available memory space decreases overhead contributed address notification space recycling however degradation reasonable considering amount memory saved example memory management scheme save 60 space cholesky parallel time degraded 6493 observe schedule likely executable reduced memory capacity number processors increases processors lead volatile objects processor gives memory management scheme flexibility allocate deallocate space even 40 maximum memory requirement schedules active memory management still executable 16 32 processors rapid without support fails execute table iii also list average number maps required one processor processors used fewer maps required since less space needed store permanent objects processor note fmm execution time active memory management sometimes even shorter without memory management explanation yang c fu delta 21 table iii performance degradation using active memory management map pt inc map pt inc map pt inc p8 200 381 300 421 500 641 p32 200 492 294 727 322 943 lu 75 50 40 map pt inc map pt inc map pt inc fmm 75 50 40 map pt inc map pt inc map pt inc p32 200 115 300 115 500 185 although computation associated leaf nodes particle partitioning tree intensive mix much intensive communication incurred downward upward passes compared cholesky lu interprocessor messages fmm downward upward passes insertion memorymanaging activities enlarges gaps consecutive communication messages leads less network contention overhead map three types memory management activities result time increase ra cq map experiments found delivery address packages map never hindered waiting previous content address buffers consumed table iv reports overhead imposed maps clear overhead insignificant compared total time increase studied table iii however activity frequent address checkingdelivering operations prolong message sending cause execution delay tasks critical paths 62 effectiveness comparisons memoryscheduling heuristics subsection compare memory time efficiency rcp mpo dts memory scalability first examine much memory saved using mpo dts define memory scalability memory reduction ratio 1 sequential space requirement p space requirement per processor schedule produced algorithm p processors 22 delta yang c fu table iv map overhead terms percentage total execution time 75 50 40 75 50 40 75 50 40 p32 154 120 100 102 68 50 43 32 33 comparison memory requirements sparse cholesky processors memory requirement reduction ratio x mpo comparison memory requirements sparse lu processors memory requirement reduction ratio x mpo b comparison memory requirements fmm processors memory requirement reduction ratio x mpo c fig 11 memory scalability comparison three scheduling heuristics sparse cholesky b sparse lu c fmm figure 11 shows memory reduction ratios three scheduling algorithms cholesky lu fmm uppermost curve graph 1 p perfect memory scalability figure shows mpo dts significantly reduce memory requirement dts memory requirement close optimum cholesky lu cases consistent corollaries 1 2 hand rcp time efficient memory scalable particularly sparse lu fmm find dts algorithm results single slice ie tasks belong slice reason lot yang c fu delta 23 dependencies among tasks dts actually reduced rcp thus experiment shows allow complexity increase oelogvmv log v oev mpo applied scheduling tasks within slice instead rcp improves space efficiency time difference rcp mpo dts also compared parallel time difference among three heuristics tables v vi different memory constraints two tables algorithm compared b ie vs b entry marked indicates corresponding b schedule executable memory constraint schedule mark indicates b schedules nonexecutable table v increase parallel time rcp mpo rcp vs mpo ratio p4 96 110 111 lu 100 75 50 40 25 fmm 100 75 50 40 25 table shows actual parallel time increase switching rcp mpo average increase reasonable sometimes mpo schedules outperform rcp schedules even though predicted parallel time rcp shorter although mpo use much criticalpath information rcp reduces number maps needed improve execution efficiency furthermore reusing object soon possible potentially improves caching performance factors mixed together making actual execution time mpo schedules competitive rcp dts aggressive memory saving utilize criticalpath information computation slicing table vi shows time slowdown using dts instead mpo clear mpo substantially outperforms dts terms execution time even though dts efficient memory usage difference especially significant large number processors mpo optimizes memory usage parallel time however times need dts instance lu case 25 available memory dts schedule yang c fu table vi increase parallel time mpo dts mpo vs dts ratio lu 100 75 50 40 25 p8 435 374 322 55 executable 16 processors mpo schedule space costly run note dts space efficiency improved using mpo schedule slice slice merging dts available amount memory space known dts schedules optimized slicemerging process called dtsm discussed section 43 list time reduction ratio using slice merging table vii dts vs dtsm results encouraging cases substantial improvement obtained result parallel time dts schedules slice merging get close rcp schedules merged slices give scheduler flexibility utilizing criticalpath information dts also effectively improving cache performance thus dts algorithm slice merging valuable problem size big available amount space known table vii reduction parallel time dts dtsm ratio p4 613 485 290 729 lu 100 75 50 40 25 p32 5055 3996 3885 3456 2395 impact solvable problem sizes new scheduling algorithms help solve problems unsolvable original rapid system optimize space usage example previously biggest matrix could solved yang c fu delta 25 using rapid lu code e40r0100 contains 958 million nonzeros fillins using runtime active memory management dts scheduling algorithm rapid able solve larger matrix called ex11 268 million nonzeros achieves 9785 megaflops 64 t3e nodes terms singlenode performance get 387 megaflops per node 16 nodes 137 megaflops per node 64 nodes considering code parallelized software tool numbers good t3e 7 conclusions optimizing memory usage important solve large parallel scientific problems software support becomes complex applications irregular computation data access patterns main contribution work development scheduling optimization techniques efficient memory managing scheme supports use fast communication primitives available modern processor architectures proposed techniques integrated rapid runtime system achieve good time space efficiency theoretical analysis correctness memory performance corroborates design techniques experiments sparse matrix fmm code show overhead introduced memory management activities reasonable mpo heuristic competitive criticalpath scheduling algorithm delivers good memory time efficiency dts aggressive memory saving achieves competitive time efficiency slice merging conducted space efficiency improved incorporating mpo slice scheduling noted proposed techniques useful semiautomatic programming tools rapid still challenging develop fully automatic system future interesting study automatic generation coarsegrained dags sequential code cosnard loi 1995 extend results complicated dependence structures chakrabarti et al 1995 girkar polychronopoulos 1992 ramaswamy et al 1994 investigate use proposed techniques performance engineered parallel systems darpa 1998 massively parallel distributedmemory machines still valuable highend largescale application problems future eg doe asci program extension smp clusters useful dts scheduling actually also improves caching performance use result data placement smps memory hierarchies needs study acknowledgements would like thank apostolos gerasoulis keshav pingali ed rothberg vivek sarkar rob schreiber kathy yelick comments work anonymous referees siddhartha chatterjee vegard holmedahl valuable feedbacks improve presentation theorem 2 pointed one referees also thank xiangmin jiao help implementing rapid jia jiao providing us fmm code test cases xiaoye li providing lu test matrices r provably efficient scheduling cilk efficient multithreaded runtime system modeling benefits mixed data task parallelism multiprocessor runtime support finegrained irregular dags automatic task graph generation techniques efficiently computing static single assignment form control dependence graph httpwww communication optimizations irregular scientific computations distributed memory architectures extended set basic linear algebra subroutines flexible communication mechanismsfor dynamic structured applications scheduling runtime support parallel irregular computations sparse lu factorization partial pivoting distributed memory machines also ucsb technical report trcs9703 computer solution large sparse positive definite systems scheduling structured unstructured computation automatic extraction functinal parallelism ordinary programs implementing active messages splitc sci clusters architectural implications software support parallel processing irregular dynamic computations new parallel architecture sparse matrix computation based finite project geometries high performance fortran problems progress sparse gaussian elimination high performance computers parallel programming compilers exploiting memory hierarchy sequential parallel sparse cholesky factorization improved load distribution parallel sparse cholesky factorization partitioning scheduling parallel programs execution multiproces sors experience active messages meiko cs2 elimination forest guided 2d sparse lu factorization decoupling synchronization data transfer message passing systems parallel computers personal communication active messages mechanism integrated communication computation runtime support portable distributed data structures program parititoning numa multiprocessor computer sys tems scheduling code generation parallel architectures computer science list scheduling without communication delays parallel computing dsc scheduling parallel tasks unbounded number processors revised july tr algorithm 656 extended set basic linear algebra subprograms model implementation test programs efficiently computing static single assignment form control dependence graph new parallel architecture sparse matrix computation based finite projective geometries active messages program partitioning numa multiprocessor computer systems list scheduling without communication delays techniques overlap computation communication irregular iterative applications communication optimizations irregular scientific computations distributed memory architectures scheduling code generation parallel architectures improved load distribution parallel sparse cholesky factorization provably efficient scheduling languages finegrained parallelism modeling benefits mixed data task parallelism decoupling synchronization data transfer message passing systems parallel computers runtime compilation parallel sparse matrix computations runtime techniques exploiting irregular task parallelism distributed memory architectures elimination forest guided 2d sparse lu factorization sparse lu factorization partial pivoting distributed memory machines partitioning scheduling parallel programs multiprocessors parallel programming compilers computer solution large sparse positive definite automatic extraction functional parallelism ordinary programs experience active messages meiko cs2 flexible communication mechanisms dynamic structured applications software support parallel processing irregular dynamic computations sparse gaussian elimination highperformance computers scheduling runtime support parallel irregular computations ctr roxane adle marc aiguier franck delaplace toward automatic parallelization sparse matrix computations journal parallel distributed computing v65 n3 p313330 march 2005 heejo lee jong kim sung je hong sunggu lee task scheduling using block dependency dag blockoriented sparse cholesky factorization proceedings 2000 acm symposium applied computing p641648 march 2000 como italy heejo lee jong kim sung je hong sunggu lee task scheduling using block dependency dag blockoriented sparse cholesky factorization parallel computing v29 n1 p135159 january