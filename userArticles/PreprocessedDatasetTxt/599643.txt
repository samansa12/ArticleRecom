relative loss bounds online density estimation exponential family distributions consider online density estimation parameterized density exponential family online algorithm receives one example time maintains parameter essentially average past examples receiving example algorithm incurs loss negative loglikelihood example respect current parameter algorithm offline algorithm choose best parameter based examples prove bounds additional total loss online algorithm total loss best offline parameter relative loss bounds hold arbitrary sequence examples goal design algorithms best possible relative loss bounds use bregman divergence derive analyze algorithm divergences relative entropies two exponential distributions also use methods prove relative loss bounds linear regression b introduction main focus statistical decision theory consists following receiving statistical information form sampling data goal make decisions minimize loss expected loss respect underlying distribution assumed model data distribution often dened terms certain parameters statistical decisions depend specic values chosen parameters thus statistical decision theory three important elements parameters values take decisions loss functions evaluate decisions extended abstract appeared uai 99 aw99 z supported nsf grant ccr 9700201 ccr9821087 c 2000 kluwer academic publishers printed netherlands k azoury k warmuth bayesian statistical decision theory prior distribution parameters data distribution additional important element information needed assess decision performance simple case suppose given sample data points fx 1 also referred examples assume examples independently generated gaussian unknown mean known variance one wants nd parameter setting mean minimizes expected loss new example drawn data distribution bayesian framework prior distribution mean would also enter decision making process context learning theory setup bayesian would described batch oline learning model since examples given learner ahead time decisions made based information entire data set paper focus online learning models oline learning fundamental elements parameters deci sions loss functions dierence however examples given learner one time thus online learning naturally partitioned trials trial one example processed trial proceeds follows begins current parameter setting hypothesis next example presented learner online algorithm incurs loss loss function recent example current parameter setting finally algorithm updates parameter setting new trial begins context online learning decisions parameter updates learner goal design online learning algorithms good bounds total loss clearly cannot meaningful bounds total loss online algorithm stand alone also hold arbitrary sequence examples however online learning literature certain type relative loss bound desirable used successfully term relative means comparison best parameter chosen oline seeing whole batch examples parameter space called comparison class relative loss bounds quantify additional total loss online algorithm total loss best line parameter comparator since online learner see sequence examples advance additional loss sometimes called regret price hiding future examples learner paper design motivate online algorithms parameter updates utilize available information best possible manner lead good relative loss bounds focus relative loss bounds 3 two types learning problems online density estimation online regression density estimation online algorithm receives sequence unlabeled examples data vectors fx 1 g start learner current parameter setting used predict next example x making predic tion algorithm receives example x incurs loss l algorithm updates parameter setting t1 con trast online regression problems receive labeled sequence examples called instances labels trial learner starts current parameter receives instance x learner makes prediction label prediction depends x loss l incurred parameter updated t1 types problems density estimation regression use abbreviated notation l denote loss incurred trial subscript indicates dependence example presented trial total loss online algorithm total loss best oline batch parameter b charge size b relative loss bounds give upper bounds dierence two total losses prove relative loss bounds density estimation underlying model member exponential family distributions also online linear regression consider two algorithms rst algorithm called incremental oline algorithm predicts ie chooses parameter best oline algorithm would predicted based examples seen far second algorithm called forward algorithm algorithm called forward uses guess future example corresponding future loss forming prediction motivated vovks work linear regression relative loss bounds algorithms grow logarithmically number trials motivation parameter updates bayesian probabilistic interpretation however relative loss bounds prove hold arbitrary worstcase sequence ex amples key element design analysis online learning algorithms generalized notion distance called bregman diver gence divergence interpreted relative entropy two exponential distributions 4 k azoury k warmuth outline rest paper organized follows section 2 present brief overview previous work section 3 dene bregman divergences give relevant background exponential family distributions show relative entropies two exponential distributions ama85 special bregman divergences conclude section listing basic properties bregman divergences section 4 introduce incremental oline algorithm general setting apply algorithm problem density estimation exponential family linear regression give number relative loss bounds specic examples section 5 dene motivate forward algorithm seen generalization incremental oline algorithm apply algorithm density estimation exponential family case linear regression reprove relative loss bounds obtained vovk vov97 proofs concise alternate simple proof forward linear regression algorithm given for99 section 6 brie discuss alternate method developed vovk proving relative loss bounds uses integration generalized posterior discuss advantages methods finally conclude section 7 discussion number open problems 2 overview previous work method proving bounds additional total loss online algorithm total loss best parameter comparison class essentially goes back work blackwell bla56 hannnan han57 investigated bounds context game theory comparison class consists mixture strategies later cover cov91 proved bounds context mathematical nance used comparison class constant rebalanced portfolios research paper rooted study relative loss bounds online learning algorithms computational learning theory community even though bounds may underestimate performance natural data used powerful yardstick analyzing comparing online algorithms computational learning theory community line research initiated littlestone discovery winnow algorithm lit88 littlestone also pioneered style amortized analysis relative loss bounds 5 proving relative loss bounds use certain divergence functions potential functions winnow designed disjunctions comparison class total number mistakes used loss next wave online algorithms designed nite set experts comparison class wide range loss functions algorithms developed online linear least squares regression ie comparison class consists linear neurons linear combination ex perts llw95 cblw96 kw97 work generalized case comparison class set sigmoided linear neurons hkw95 kw98 also starting littlestones work relative loss bounds comparison class linear threshold functions investigated lit88 gls97 online algorithms cited previous paragraph use xed learning rates simple settings relative loss bounds grow number trials however already linear regression square loss relative loss bounds algorithms xed learning rates grow square root loss best linear predictor cblw96 kw97 best loss often linear number trials contrast algorithms use variable learning rate relative loss bounds grow logarithmically bounds proven generalization bayes algorithm xb97 yam98 maintains posterior parameters comparison class outline method proving relative loss bounds section 6 bounds grow logarithmically also proven previously online linear regression important insight gained research olog relative loss bounds seem require use variable learning rates paper learning rate applied trial o1t use o1t learning rates exponential family also suggested gordon possible strategy leading better bounds however specic examples worked case linear regression o1t learning rates become inverses covariance matrix past examples general frameworks online learning algorithms developed gls97 kw97 kw98 gor99 follow philosophy kivinen warmuth kw97 starting divergence function divergence function derive online update use divergence potential amortized analysis similar method developed gls97 case comparison class consists linear threshold functions start update construct appropriate divergence used analysis 6 k azoury k warmuth recently learned divergences used online learning employed extensively convex optimization called bregmandistances bre67 cl81 csi91 jb90 bregmans method pick set allowable models one minimal distance current model words current hypothesis projected onto convex set allowable models mild additional assumptions assure uniqueness projections assumptions generalized pythagorean theorem proven bregman divergences hw98 latter theorem often contradicts triangular inequality reason use term divergence instead distance projections respect bregman divergences recently applied hw98 case oline comparator allowed shift time projections used keep parameters algorithm reasonable regions aids recovery process underlying model shifts 3 bregman divergences exponential family paper use notion divergence due bregman bre67 deriving analyzing online learning algorithms divergences interpreted relative entropies distributions exponential family begin section dening bregman divergences review important features exponential family distributions relevant paper conclude section properties divergences arbitrary realvalued convex dierentiable function g parameter space r bregman divergence two parameters e dened r denotes gradient respect throughout paper vectors column vectors use denote dot product vectors note bregman divergence g e minus rst two terms taylor expansion g e around words g e tail taylor expansion g e beyond linear term since g convex g e properties listed section 34 example let parameter space 2 case bregman divergence becomes squared relative loss bounds 7 euclidean distance ie also e e e e 31 exponential family features exponential family used throughout paper include measure divergence two members family intrinsic duality relationship see bn78 ama85 comprehensive treatment exponential family multivariate parametric family fg distributions said exponential family members density function x vectors r p 0 x represents factor density depend ddimensional parameter usually called natural canonical parameter many common parametric distributions members family including gaussian function g normalization factor dened z space r integral nite called natural parameter space exponential family called regular open subset r well known bn78 ama85 convex set g strictly convex function function g called cumulant function plays fundamental role characterizing members family distributions use g denote gradient r g r 2 g denote hessian g let represent loglikelihood viewed function standard regularity conditions loglikelihood functions satisfy wellknown moment identities mn89 applying identities exponential family reveals special role played cumulant function g 8 k azoury k warmuth expectation respect distribution pg xj rst moment identity loglikelihood functions gradient loglikelihood 32 linear x r g applying 33 get shows mean x equal gradient g let call expectation parameter since cumulant function g strictly convex map inverse denote image map g inverse map g 1 set called expectation space may necessarily convex set second moment identity loglikelihood functions 0 denotes transpose exponential family r 2 thus second moment identity variancecovariance matrix x hessian cumulant function g also called fisher information matrix since g strictly convex hessian symmetric positive denite 32 duality natural parameters expectation parameters sometimes convenient parameterize distribution exponential family using expectation parameter instead natural parameter pair parameterizations dual relationship provide aspects duality relevant paper first dene second function range g follows let f rf denote gradient f relative loss bounds 9 note taking gradient f 36 respect treating function get r jacobian respect thus f inverse map g 1 two parameterizations related following transformations since g positive denite jacobian 2 g 1 positive denite jacobian 2 thus second function f strictly convex well function called dual g ama85 furthermore f negative entropy pg xj respect reference measure p 0 x ie follows 38 hessian f inverse fisher information matrix ie r 2 consider function v gf fisher information matrix expressed terms expectation parameter function dened expectation space takes values space symmetric matrices called variance function variance function plays important role characterizing members exponential family mor82 gps95 matrix v positive denite 2 v thus context exponential families functions f g arbitrary convex functions must positive denite hessians 33 divergence two exponential distributions consider two distributions pg xj e old parameter setting e pg xj new parameter setting following amari ama85 one may see exponential family fg manifold parameters e represent two points manifold several measures distance divergence two points proposed literature amari introduced divergences ama85 related distances introduced csiszar csi91 known f divergences use letter h also cherno distances renyis information related ama85 divergences k azoury k warmuth following general form h continuous convex function main choice h gives relative entropy another interesting choice gives opposite two entropies respectively called 1 1 divergences amari ama85 34 properties divergences section give simple properties divergences properties need g positive denite hessian hence properties hold general denition bregman divergence 31 allow g arbitrary realvalued dierentiable convex function g parameter space throughout paper use represent r g 1 g e convex rst argument since g e convex 2 g strictly convex equality holds e 3 gradient divergence respect rst argument following simple form r e 4 divergences usually symmetric ie g e 5 divergence linear operator ie ag 0 relative loss bounds 11 6 divergence aected adding linear term g g g 7 1 2 3 dot product usually sign negative contradicts triangular inequality case dot product zero exploited proof generalization pythagorean theorem bregman divergences see example 8 g strictly convex denition dual convex function f parameter transformations still hold 36 38 bregman divergences following duality property rst six properties immediate property 7 proven appendix property rst used wj98 proving relative loss bounds last property follows denition dual function f also called convex conjugate roc70 note order arguments g e switched paper need property 8 case g strictly con vex however realvalued dierentiable convex function g one dene dual function f parameter r roc70 denition property 8 still holds note gordon gor99 gives elegant generalization bregman divergences case convex function g necessarily dierentiable sake simplicity restrict dierentiable case paper finally g dierentiable bregman divergence also written path integral integral version divergence used dene notion convex loss matching increasing transfer function g articial neuron ahw95 hkw95 kw98 12 k azoury k warmuth 4 incremental oline algorithm section give basic algorithm show prove relative loss bounds general setting learning proceeds trial example processed density estimation examples data vectors x domain x regression setting tth example consists instance x instance domain x label label domain setup learning problem dened three parts parameter space r realvalued loss function divergence function measure distance initial parameter setting parameter space represents models algorithms compared loss parameter vector th example denoted l l 1t shorthand usually losses nonnegative third component setup initial parmeter 0 bregman divergence u0 0 initial pa rameter initial parameter 0 may interpreted summary prior learning divergence u0 0 represents measure distance initial parameter oline batch algorithm sees examples sets parameter u t1 assumptions losses l 1 u 0 dier entiable convex functions parameter space reals furthermore assume argmin u t1 always solution note oline algorithm trades total loss examples closeness original parameter alternatively divergence u 0 may interpreted size parameter interpretation oline algorithm nds parameter minimizes sum size total loss online algorithm sees one example time according following protocol online protocol incremental oline algorithm initial hypothesis 0 predict get tth example incur loss l update hypothesis t1 relative loss bounds 13 goal online algorithm incur loss never much larger loss oline algorithm sees examples end trial online algorithm knows rst examples expects see next example one reasonable desirable setup parameter update point make online algorithm exactly oline algorithm would done seeing examples use name incremental oline online algorithm property incremental oline algorithm u t1 additional assumptions assume argmin u t1 1 always solution one solution interpreted t1 2 argmin u t1 learning problems use examples paper u t1 typically strictly convex one solution note nal parameter t1 incremental oline algorithm coincides parameter b chosen batch algorithm consistent protocol given necessary begin indexing parallel indexing second online algorithm given next section second algorithm called forward algorithm uses guess next loss updating parameter setup update 41 seem truly online since needs previous examples truly online yet equivalent setup given following lemma lemma 41 incremental oline algorithm 1 proof note since ru thus since u constant argmin t1 used denition 41 argmin lemma qed ready show key lemma incremental line algorithm lemma compare total loss online algorithm total loss comparator total loss comparator includes divergence term u 0 0 14 k azoury k warmuth lemma 42 incremental oline algorithm sequence examples 2 u t1 proof 0 expand divergence u t1 t1 use ru t1 t1 gives us u t1 t1 since u t1 special case subtracting 43 44 applying u u version 42 gives summing trials obtain u lemma follows equality u 1 equality follows property 6 linear qed obtain relative loss bounds choose best oline parameter b comparator bound righthandside equation lemma note case incremental oline algorithm thus last divergence righthandside zero divergence u t1 represents cost update t1 incurred online algorithm relative loss bounds bounds total cost online updates relative loss bounds 15 41 incremental offline algorithm exponential family apply incremental oline algorithm problem density estimation exponential family distributions rst give general treatment prove relative loss bounds specic members family subsections follow make obvious choice loss function namely negative loglikelihood using general form loglikelihood 32 loss parameter example x purpose relative loss bounds see lemma 42 changing loss constant depend inconsequential thus form reference measure p 0 x immaterial allow algorithm initial parameter value 0 choose u 0 multiple cumulant function ie thus context density estimation exponential family incremental oline algorithm becomes u t1 throughout paper use notation 1 denote tradeo parameters two reasons first inverse tradeo parameters become learning rates algorithms learning rates commonly denoted also use 1 instead 1 linear regression parameters generalized matrices setup 45 interpreted nding maximum aposteriori map parameter divergence term corresponds conjugate prior 1 hyper parameter 1 divergence term disappears maximum likelihood esti mation alternatively one think 0 initial parameter estimate based hypothetical examples seen rst real example 0 number examples also one interpret parameter 1 0 tradeo parameter staying close initial parameter 0 minimizing loss examples seen end trial yet another interpretation 45 follows rewriting u t1 k azoury k warmuth thus u t1 corresponds negative loglikelihood exponential density cumulant function 1 example 1 develop alternate online motivation given lemma 41 let 1 linear follows properties divergences u thus online motivation lemma 41 becomes divergence measures distance last parameter tradeo parameter 1 updated parameter t1 obtained minimizing u t1 dened 45 strictly convex function gradient function terms expectation parameters setting zero 0 gives update expectation parameter incremental oline algorithm 1 also express t1 convex combination last instance x note 1 alternate recursive forms update used later 1 thus online update may seen gradient descent different learning rates update 49 uses gradient loss t1 410 uses gradient loss evaluated special case 1 valid consistent updates 48 410 relative loss bounds 17 relative loss bounds proven using lemma 42 thus density estimation equality simplies following lemma gives concise expression minimum u t1 see 45 terms dual cumulant function lemma following discussion interesting right although essential main development paper use bernoulli example discussed later combining 411 lemma one also get expression total loss online algorithm lemma 31 aw99 lemma 43 min proof rewrite righthandside equality lemma using denition dual function f 36 expression rewrite using 1 denitions loss divergence equal u t1 t1 qed 1 case maximum likelihood rewritten k azoury k warmuth thus essentially inmum average loss data equals expected loss parameter minimizes average loss ie maximum likelihood parameter relationship used gru98 remaining subsections discuss specic examples give relative loss bounds 411 density estimation gaussian derive relative loss bounds gaussian density estimation problem consider gaussian density r known xed without loss generality develop bounds special case identity matrix similar bounds immediately follow general case xed arbitrary variancecovariance matrix linear transformation argument gaussian density identity matrix variancecovariance matrix x 2 shorthand x x density member exponential family natural parameter cumulant parameter transformations functions dual convex function f loss l const note constant loss immaterial bounds therefore set zero sake simplicity set 2 recall incremental oline update dierence total loss incremental oline algorithm oline algorithm relative loss bounds 19 use online updates eg 49 410 rewrite divergences righthandside want allow 1 case update 49 cannot applied 1 however update 48 1 1 1 412 rewritten follows easy nd two examples x 1 x 2 dierence 414 depends order two examples presented develop upper bound permutation invariant ie depend order examples presented drop negative terms 414 use since obtain following relative loss bound k azoury k warmuth theorem 44 gaussian density estimation incremental oline algorithm note special case 1 oline algorithm chooses maximum likelihood parameter bound simplies 1 412 density estimation gamma give relative loss bounds gamma distribution density shape parameter inverse scale parameter member exponential family natural parameter density terms written assume known xed parameter scales loss divergences inverse called dispersion parameter sake simplicity drop ignored xed variance case gaussian density estimation cumulant parameter transformations dual convex function f loss l bound divergence t1 leads relative loss bound incremental oline algorithm see 411 4 relative loss bounds 21 using update 410 notation r convex combinations elements fx g 1 sum divergences righthandside 411 bounded summary following relative loss bound theorem 45 density estimation gamma distribution using incremental oline algorithm 1 better relative loss bounds include case 1 possible bounding 415 carefully 413 density estimation general exponential family give brief discussion form bounds take member exponential family rewrite divergence second order taylor expansion f t1 last e e 22 k azoury k warmuth e convex combination t1 r 2 f constant essentially gaussian general case may seen local gaussian timevarying curvature reasonable methods proceed casebycase basis mor82 gps95 based form r 2 f inverse variance function recall summing last term always give logt style bound sometimes range x needs restricted done previous subsections density estimation gaussian gamma distributions 414 linear regression subsection bounds linear regression developed instance domain x r label domain r parameter domain also r components parameter vectors 2 linear weights given example parameter vector linear model predicts x square loss used measure discrepancy prediction label example note l strictly convex thus make u 0 strictly convex initial divergence u 0 strictly convex updates always unique solution use u positive denite matrix divergence initial parameter becomes u 0 0 thus linear regression update 41 incremental oline update becomes u t1 note use transpose notation x 0 q instead dot product subsequent derivations use matrix algebra setup linear regression usually interpreted conditional density estimation problem gaussian label given x cumulant function trial 1 divergence corresponds gaussian prior develop alternate online version 416 done general lemma 41 since u equals 1 linear terms online version becomes relative loss bounds 23 dierentiating 416 0 obtain incremental oline update linear regression standard linear least squares update easy derive following recursive versions 1 note correspondence updates updates 47 410 density estimation also lemma 42 becomes following quadratic equation see 411 corresponding equation density reprove bound obtained vovk vov97 incremental oline algorithm sake simplicity choose 1 0 multiple identity matrix similar bound proven foster algorithm however assumes comparator probability vector fos91 theorem 46 linear regression incremental oline algorithm 1 tg note theorem assumes predictions x 0 labels trial lie assumption satised might use clipping ie algorithm predicts number yy closest x 0 clipping requires algorithm know k azoury k warmuth little incentive work details incremental line algorithm algorithm next section prove better relative loss bound predictions dont need lie proof apply update 419 twice divergence sum righthandside 420 give rst two equalities third equality follows lemma a1 appendix2 t1 last inequality used assumption x 0 fact z 1 ln z note last inequality may hold without assumption x 0 theorem follows applying crude approximations equality lemma 42 last inequality follows assumption x qi x 2 second last inequality follows fact 1 ai determinant symmetric matrix product diagonal elements see bb65 chapter 2 theorem 7 qed ideally dont want use crude bounding method goal rewrite sum divergences telescoping occurs incremental online update able partial attempt follows gaussian density relative loss bounds 25 estimation 4132 t1 last equality use fact 1 symmetric note last two terms nal expression telescope gaussian case 413 surprisingly forward algorithm introduced next section corresponding two terms telescope thus forward algorithm one prove bound one given theorem 46 except last term bound 1 1 quarter theorem 46 related problem density estimation gaussian corresponding improved bound factor 1 holds incremental oline algorithm 5 estimating future loss forward algorithm section present second algorithm called forward algorithm give lemmas used proving relative loss bounds trial forward algorithm expects see next example allow incorporate estimate loss next example choosing parameter regression part example namely instance x available trial algorithm must commit parameter algorithm use instance x form estimate loss trial shall see linear regression incorporating estimate motivation used include current instance learning rate algorithm leads better relative loss bounds density estimation however instances yet algorithm still uses estimate future loss 26 k azoury k warmuth online protocol forward algorithm regression density estimation initial hypothesis 0 initial hypothesis 0 get instance x guess loss tth example guess loss tth example update hypothesis 1 update hypothesis 1 predict predict get label tth example get example x incur loss l incur loss l dene update analogous previous section minimizing sum divergence plus losses past trials estimate loss next trial forward algorithm u t1 assumption losses l 1 estimated losses dierentiable convex functions parameter space reals furthermore assume argmin u t1 1 always solution note incremental oline algorithm special case forward algorithm estimated losses zero alternate online motivation update using divergence last parameter vector lemma 51 forward algorithm 1 u rewrite argument argmin relative loss bounds 27 thus since u constant argmin t1 used denition 51 argmin lemma qed following key lemma generalization lemma 42 incremental oline algorithm lemma 52 forward algorithm sequence examples 2 u t1 proof 0 expand divergence u t1 t1 use ru t1 t1 proof lemma 42 gives us u t1 t1 since u t1 obtain special case subtracting 53 54 applying u u version 52 obtain u t1 equation lemma follows summing trials subtracting u 0 sides qed relative loss bound forward algorithm must based bounding righthandside lemma 51 density estimation exponential family apply forward algorithm problem density estimation exponential family distributions choose u 0 28 k azoury k warmuth 0 g done incremental oline algorithm thus initial divergence becomes u 0 estimated future loss use const may seen average loss number examples lies instance domain 0 seen guess future instance corresponding loss estimated loss rewritten g const thus choices forward algorithm 51 becomes following u t1 incremental oline algorithm 45 except tradeo parameter 1 0 1 online motivation becomes online motivation incremental line algorithm 46 except 1 1 increased one 1 updates 47410 lemma 43 remain learning rates shifted updates hold 1 rst one holds well shows choice estimate since estimated loss independent trial estimated losses last equality lemma 52 cancel get relative loss bounds 29 52 density estimation gaussian section give bound forward algorithm better corresponding bound incremental oline al gorithm following steps 413 simplify following divergence using 59 becomes set 0 thus 1 zero choose t1 thus last three terms equation rewritten as2 2 0 511 equation 510 bounded by2 x 2 theorem 53 gaussian density estimation forward algorithm k azoury k warmuth note bound forward algorithm better bound incremental oline algorithm see theorem 44 improvement essentially 2x 2 53 density estimation bernoulli subsection give relative loss bounds bernoulli distribution examples x coin ips f0 1g distribution typically expressed p probability 1 let 1 distribution terms member exponential family natural parameter cumulant parameter transformations 1e dual function f loss l consider forward algorithm 1 case maximum likelihood forward algorithm uses t1 2 t1 rst develop concise expression total loss algorithm lemma 54 bernoulli density estimation forward algorithm proof rst rewrite loss trial various ways let abbreviate develop formula note trials x increases one loss l contains relative loss bounds 31 term lns t2 trials terms contribute similarly trials x one loss l contains term lnt t2 trials terms contribute fact lemma follows qed note righthandside expression lemma independent order examples seen thus bernoulli distribution total loss forward algorithm permutation invariant lemma 43 l 1t thus equivalent expression using gamma function rst derived freund fre96 based laplace method integration using standard approximations gamma function one bound righthandside 1 theorem 55 fre96 bernoulli density estimation forward algorithm 54 linear regression subsection derive relative loss bounds forward algorithm applied linear regression incremental oline algorithm let u 0 positive denite divergence initial 0 u use estimated future loss ie next label t1 guessed x 0 forward update 51 linear regression becomes k azoury k warmuth u t1 denition u 1 0 minimized 0 thus dierentiating u t1 obtain forward update linear regression 0 note t1 depends x t1 thus forward algorithm different incremental oline algorithm 417 uses current instance form prediction sake simplicity assume rest section recursive versions update following rewrite equality lemma 52 linear regression following steps used incremental oline algorithm 422 relative loss bounds 33 using argument sum righthandside equation 515 simplied note last two terms telescope corresponding derivation incremental oline update last two terms telescope 422 515 gaussian density estimation 510 show last three terms equation zero first rewrite b identity matrix last term becomes2 second last term simplies to2 x t1 x 0 sum last three terms pulling factor 1 34 k azoury k warmuth thus last three terms zero gaussian density estimation forward algorithm 511 finally use upper bound see theorem 462 2 sums righthandside 516 bounded 1 summarize relative loss bound proved forward algorithm theorem 56 linear regression forward algorithm note bound forward algorithm better corresponding bound incremental oline algorithm see theorem 46 improvement factor four bound rst proven vovk vov97 using integration alternate proof exact expression 517 given forster for99 6 relationship bayes algorithm alternate method pioneered vovk vov97 freund yamanishi yam98 proving relative loss bounds section sketch method compare distribution maintained set parameters parameters names algorithms called experts online learning literature 97 initial work considered case nite however method also applied continuous yam98 beginning trial relative loss bounds 35 distribution form r e p 1 prior 0 learning rate following type inequality main part method z z z la denotes loss algorithm trial important special case occurs loss l q negative loglikelihood respect parameterized density ie call bayes algorithm case p given 61 posterior distribution seeing rst examples algorithm trial predicts predictive distribution 62 equality dmw88 general setting 6 1 losses necessarily negative loglikelihoods prediction algorithm chosen inequality 62 holds matter tth example also larger learning rate better resulting relative loss bounds learning rate chosen large possible prediction always guaranteed exist inequality 62 holds learning rate used trials inequality often tight examples lie boundary set possible examples summing inequality 62 trials one gets bound z integral cannot computed exactly bounded using laplaces method integration around best oline parameter would like point following distinction method algorithms presented paper prediction bayes algorithm ie predictive distribution usually represented parameter expert instead algorithms analyzed paper chooses map parameter trial case bernoulli distribution bayes algorithm based jereys prior coincides forward 36 k azoury k warmuth algorithm 1 section 53 similarly case linear regression gaussian prior vovks algorithm vov97 coincides forward algorithm linear regression section 54 however clear density estimation problems exponential family predictions bayes algorithm algorithms produced general case 6 1 represented parameters parameter space contrast method proving relative loss bounds avoids use involved integration methods needed 63 co96 parameter maintain based simple average past examples sucient statistic exponential family 7 conclusion paper presented techniques proving relative loss bound density estimation exponential family gave number examples apply methods including case linear regression exponential density cumulant function g use bregman divergence g e measure distances parameters e thus loss l divergence based function g however lemmas 42 52 methodology proving relative loss bounds general initial divergence loss need related lemmas might extended nondierentiable convex functions using generalized notion bregman divergences introduced gordon gor99 parameters maintained algorithms invariant permuting past examples however total online loss algorithms permutation invariant therefore adversary could use fact present examples order disadvantageous learning algorithm suggests algorithms better relative loss bounds incremental oline algorithm forward algorithm incomparable sense either one may larger total loss however believe sense forward algorithm better needs formalized belief inspired phenomenon stein estimator statistics ste56 since like stein estimator forward update uses shrinkage factor compared incremental oline update still need explore bounds obtained paper relate large body research minimum description length relative loss bounds 37 literature ris89 ris96 literature lower upper bounds relative loss form explored extensively number parameters however contrast setup used paper work minimum description length literature require algorithm predicts parameter parameter space methodology developed paper proving relative loss bounds still needs worked learning problems instance always logt style relative loss bound member exponential family see section 413 another open problem prove logt style bounds logistic regression finally lower bounds relative loss need explored case algorithm restricted predict parameter parameter space bounds shown linear regression vov97 particular proven constant lnt relative loss bound forward algorithm theorem 56 tight however general logt style lower bounds known arbitrary member exponential family acknowledgments thanks leonid gurvitz introducing us bregman divergences claudio gentile peter grunwald georey gordon eiji taki moto two anonymous referees many valuable comments r exponentially many local minima single neurons relative loss bounds online density estimation exponential family distributions analog minimax theorem vector payo relaxation method iterative rowaction method interval convex pro gramming universal portfolios side information universal portfolios least squares maximum entropy learning probabilistic prediction functions prediction worst case predicting binary sequence almost well optimal biased coin general convergence results linear discriminant updates technical report cmucs99143 minimum discription length principle reasoning uncertainty approximation bayes risk repeated play learning algorithms tracking changing concepts investigation error surfaces single arti sequential prediction individual sequences general loss functions tracking best regressor conf comput additive versus exponentiated gradient updates linear prediction relative loss bounds multidimensional regression problems learning irrelevant attributes abound new linearthreshold algorithm journal computational complexity weighted majority algorithm generalized linear models natural exponential families quadratic variance functions stochastic complexity statistical inquiry fisher information stochastic complexity convex analysis inadmissibility usual estimator mean multivariate normal distribution asymptotically minimax regret exponential families asymptotically minimax regret bayes mixtures aggregating strategies competitive online linear regression continuous discrete time nonlinear gradient descent relative loss bounds convergence minimax redundancy class memoryless sources ieee transactions information theory decisiontheoretic extension stochastic complexity applications learning tr ctr arindam banerjee inderjit dhillon joydeep ghosh srujana merugu information theoretic analysis maximum likelihood mixture estimation exponential families proceedings twentyfirst international conference machine learning p8 july 0408 2004 banff alberta canada shai shalevshwartz yoram singer primaldual perspective online learning algorithms machine learning v69 n23 p115142 december 2007 jrgen forster manfred k warmuth relative loss bounds temporaldifference learning machine learning v51 n1 p2350 april srujana merugu joydeep ghosh privacysensitive approach distributed clustering pattern recognition letters v26 n4 p399410 march 2005 vn vishwanathan nicol n schraudolph alex j smola step size adaptation reproducing kernel hilbert space journal machine learning research 7 p11071133 1212006 nicol cesabianchi claudio gentile luca zaniboni worstcase analysis selective sampling linear classification journal machine learning research 7 p12051230 1212006 claudio gentile robustness pnorm algorithms machine learning v53 n3 p265299 december arindam banerjee srujana merugu inderjit dhillon joydeep ghosh clustering bregman divergences journal machine learning research 6 p17051749 1212005 mark herbster manfred k warmuth tracking best linear predictor journal machine learning research 1 p281309 912001