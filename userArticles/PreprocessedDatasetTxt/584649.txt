maximum likelihood estimation mixture densities binned truncated multivariate data binning truncation data common data analysis machine learning paper addresses problem fitting mixture densities multivariate binned truncated data em approach proposed mclachlan jones biometrics 44 2 571578 1988 univariate case generalized multivariate measurements multivariate solution requires evaluation multidimensional integrals bin iteration em procedure naive implementation procedure lead computationally inefficient results reduce computational cost number straightforward numerical techniques proposed results simulated data indicate proposed methods achieve significant computational gains loss accuracy final parameter estimates furthermore experimental results suggest sufficient number bins data points possible estimate true underlying density almost well data binned paper concludes brief description application approach diagnosis iron deficiency anemia context binned truncated bivariate measurements volume hemoglobin concentration individuals red blood cells b introduction paper address problem fitting mixture densities multivariate binned truncated data problem motivated application medical diagnosis blood samples taken subjects typically sample contains 40000 different red blood cells volume hemoglobin concentration red blood cells measured cytometric blood cell counter produces output bivariate histogram 100 theta 100 grid volume hemoglobin concentration space eg figure 1 bin contains count number red blood cells whose volume hemoglobin concentration fall bin known data truncated ie range machine measurement less actual possible range volume hemoglobin concentration values present general solution problem fitting multivariate mixture density model binned truncated data binned truncated data arise frequently variety application settings binning occur systematically measuring instrument finite resolution eg digital camera finite precision pixel intensity binning also may occur intentionally realvalued variables quantized simplify data collection eg binning persons age ranges 0 10 1020 forth truncation also easily occur practical data collection context whether due fundamental limitations range measurement process intentionally reasons binning truncation one think original raw measurements masked binning truncation processes ie know exact location data points within bins many data points fall outside measuring range natural think problem one involving missing data expectationmaximization em algorithm obvious candidate model fitting probabilistic context theory fitting finite mixture models univariate binned truncated data maximum likelihood via em algorithm developed mclachlan jones 1988 problem somewhat simpler form addressed earlier demp ster laird rubin 1977 em algorithm originally introduced univariate theory mclachlan jones 1988 extended straightforward manner cover multivariate data however implementation subject exponential time complexity numerical instability requires careful consideration focus present paper section 2 extend mclachlan jones results univariate mixture estimation multivariate case section 3 present detailed discussion computational numerical considerations necessary make algorithm work practice section 4 discusses experimental results simulation data aforementioned red blood cell data basic theory em bins truncation begin brief review em algorithm general form em algorithm general procedure finding maximum likelihood model parameters part data missing finite mixture model underlying assumption generative model data point comes one g component distributions however information hidden identity component generated point unknown knew information estimation parameters maximum likelihood would direct least single normal population estimate mean covariance parameters component separately using data points identified component relative count data points population would maximum likelihood estimate weight components mixture model regardless component distributions think two types data observed data missing data ac cordingly observed likelihood one want maximize full likelihood one includes missing data typically easier maximize em algorithm provides theoretical framework enables us iteratively maximize observed likelihood maximizing expected value full likeli hood fitting gaussian mixtures em iterations quite straightforward wellknown see mclachlan basford 1988 bishop 1995 tutorial treatments em gaussian mixtures see little rubin 1987 mclachlan krishnan 1997 discussion em general context binning truncation two additional sources hidden information addition hidden component identities data point mclachlan jones 1988 show use em algorithm type problem underlying finite mixture model written weights individual components f component density functions mixture model parametrized phi set mixture model parameters g overall sample space h divided v disjoint subspaces h bins counts first r bins observed counts last bins missing observed likelihood associated model irrelevant constant terms given jones mclachlan r n total observed count r p represent integrals probability density function pdf bins z z r form likelihood function corresponds multinomial distributional assumption bin occupancy invoke em machinery first define several quantities pth iteration phi p p represent current estimates model parameters e p denotes conditional expectation given random variable belongs jth bin using current value unknown parameter vector eg expected value respect normalized current pdf fx phi p p j phi p specifically function gx z also define c p p quantities lefthand side superscript p depend current parameter estimates phi p andor p term intuitive interpreta tion example j represent generalization bin counts unobserved data either equal actual count observed bins ie j r represent conditional expected counts unobserved bins ie j r conditional expected count formalizes notion say 1 pdf mass unobserved bins assign 1 total data points x posterior probability membership ith component mixture model given x observed individual represents relative weight mixture component point x intuitively probability data point x belonging component c measure overall relative weight component note order calculate c local relative weight x averaged bin weighted count bin summed bins way data point within bin contributes c average local weight bin ie compare nonbinned data data point contributes c actual local weight evaluated data point value data point next use quantities defined last equation define estep express closed form solution mstep iteration p x p c p oe p1 c p equations specify component weights ie component means ie component standard deviations ie oes updated em step note main difference standard version em nonbinned data comes fact taking expected values bins ie e p data point within bin contributes corresponding value averaged bin whereas nonbinned case point contributes value evaluated data point generalize multivariate case theory need generalize equations 68 vectorcovariance cases c p x p c p c p multivariate theory straightforward extension univariate case practical implementation theory considerably complex due fact approximation multidimensional integrals considerably complex univariate case note approach guaranteed maximize likelihood defined equation 1 irrespective form selected conditional probability model missing data given observed data different choices conditional probability model lead different paths parameter space overall maximum likelihood parameters makes approach quite general additional assumptions distribution data required 3 computational numerical issues section discuss approach two separate problems arise multivariate case 1 perform single iteration em algorithm 2 setup full algorithm exact time efficient main difficulty handling binned data opposed standard nonbinned data evaluation different expected values ie e p em iteration defined equation 2 expected value equations 911 requires integration function v bins integrals cannot evaluated analytically mixture models even gaussian mixture models thus evaluated numerically em iteration considerably complicating implementation em procedure especially multivariate data summarize present difficulties ffl bins univariate space om bins ddimensional space consider dimension om bins represents exponential growth number bins ffl univariate space numerical integration requires oi function eval uations multivariate space require least oi function evaluations comparable accuracy integral combined exponential growth number bins leads exponential growth number function evaluations underlying exponential complexity cannot avoided overall execution time greatly benefit carefully optimized integration schemes ffl geometry multivariate space complex geometry univariate space univariate histograms natural endpoints truncation occurs unobserved regions simple shape multivariate histograms typically represent hypercubes unobserved regions still rectangular simple shape example 2 dimensional histogram four sides unobserved regions extend infinity also four wedges regions ffl fixed sample size multivariate histograms much sparser univariate counterparts terms counts per bin ie marginals sparseness leveraged purposes efficient numerical integration 31 numerical integration em iteration e step em algorithm consists finding expected value completedata log likelihood respect distribution missing data step consists maximizing expected value respect model parameters phi equations steps single iteration em algorithm expected values equations ie e p terms would represent closed form solution parameter updates binned truncated data almost closed form solution additional integration still quired one could use variety monte carlo integration techniques integration problem however slow convergence monte carlo undesirable problem since functions integrating typically quite smooth across bins relatively straightforward numerical integration techniques expected give solutions high degree accuracy multidimensional numerical integration consists repeated 1dimensional inte grations results paper use romberg integration see thisted 1988 press et al 1992 details important aspect romberg integration selection order integration lowerorder schemes use relatively function evaluations initialization phase may converge slowly higherorder schemes may take longer initialization phase converge faster thus order selection substantially affect computation time numerical integration return point later note order affects path convergence integration final solution order given prespecified degree accuracy 32 handling truncated regions next problem arises practice concerns truncated regions ie regions outside measured grid want use mixture model naturally defined whole space must define bins cover regions extending grid boundaries 1 1dimensional case suffices define 2 additional bins one extending last bin 1 extending gamma1 first bin multivariate case natural define single bin r covers everything data grid explicitly describe outofgrid regions reason calculate expected values whole space h without actually integration mind readily write integrals truncated regions z r z r z z r z note extra work required obtain integrals righthand side equations basic em equations 911 require calculation expected values similar defined equation 2 bin note however difference expected values integrals righthand side equations 1214 normalizing constant 1p j phi normalizing constant affect integration suffices separately record normalized unnormalized values integrals bin normalized values later used equations 911 unnormalized values used equations 1214 computational efficiency take advantage sparseness bin counts assume want integrate function ie pdf whole grid assume require prespecified accuracy integration ffi means relative change value integral two consecutive iterations falls ffi consider integral converged ffi small number typically order 10 gamma5 less assume perform integration integrating bin grid adding results intuitively contribution bins large ie bins significant pdf mass contribution others negligible ie bins contain near zero pdf mass data sparse many bins negligible contributions goal optimize time spent integrating numerous empty bins significantly contribute integral accuracy integration see influences overall accuracy consider following simplified analysis let size bins proportional h let mean height pdf approximately f let order pn bins relevant pdf mass total number bins rough estimate integral bins given fhpn since accuracy integration order ffi tolerating absolute error integration order ffii hand assume irrelevant bins value pdf height order fflf ffl small number estimated contribution irrelevant bins value integral 0 fflf h1 gamma pn approximately 0 fflpi sparse data ie p small compared 1 estimated contribution irrelevant bins absolute error integration integration within irrelevant bins since integration accurate least accurate part optimal scheme contribution error integration irrelevant relevant bins comparable words suboptimal also confirm experimentally result section choose ffi 0 smaller required ffi 0 fflp ffi means integration within bin low probability mass ie fflf need carried accurately note ffl 0 integrate less less accurately within bin without hurting overall integral full grid note also ffl 0 treshold e figure 2 execution time single em step function threshold ffl several different values k romberg integration order 2 values scale eg etc results based fitting twocomponent mixture 40000 red blood cell measurements two dimensions 100 theta 100 bins time seconds loglikelihood within multiplicative kmeans final loglikelihood k 3 figure 3 quality solution measured loglikelihood function time different variations algorithm becomes o1 start using single iteration simplest possible integration scheme still stay within allowed limit ffi 0 summarize given value ffl algorithm estimates average height f pdf bins pdf values less fflf uses single iteration simple fast integrator original behavior recovered setting bins integrated quickly general idea provides large computational gain virtually loss accuracy note ffi controls overall accuracy ffl adds small correction ffi example found variability parameter estimates using different small values ffl much smaller bin size andor variability parameter estimates different random initial conditions figure 2 shows time required complete single em step different values k romberg integration order ffl time minimized different values ffl using 4 greatest choosing either low high integration order quite computationally inefficient 33 full em algorithm fine tuning single em iteration step able significantly cut execution time however since step still computationally intensive desirable em converge quickly possible ie iterations possible mind use following additional heuristic take random sample binned points randomize coordinates point around corresponding bin center use uniform distribution within bin em algorithm nonbinned nontruncated data relatively fast closed form solution exists em step without integration em algorithm converges solution parameter space initial data set use parameters initial starting points em algorithm full set binned truncated data second application em using methodology described earlier paper refines initial guesses final solution typically taking iterations note initialization scheme cannot affect accuracy results loglikelihood full set binned truncated data used final criterion convergence figure 3 illustrates various computational gains axis loglikelihood within multiplicative constant data x axis computation time fitting twocomponent mixture twodimensional grid 100 theta 100 bins red blood cell counts k order romberg integration ffl threshold declaring bin small enough fast integration described earlier parameter choices k ffl result quality final solution ie asymmptote loglikelihood eventually using approximation two orders magnitude slower using nonzero ffl values increasing ffl 0001 01 results loss likelihood results faster convergence comparing curves kmeans used initialize binned algorithm versus randomized initialization method described earlier shows factor two gain convergence time randomized initialization summarize overall algorithm fitting mixture models multivariate binned truncated data consist following steps 1 treat multivariate histogram pdf draw small number data points add counts bins prevent 0 probabilities empty bins 2 fit standard mixture model sample using usual em algorithm nonbinned nontruncated data 3 use parameter estimates step 2 refine using em algorithm full set binned truncated data consists iteratively applying equations 911 bins within grid applying equations single bin outside grid convergence measured equation 1 4 experimental results 41 em methodology experiments use following methods parameters implementation em ffl standard em algorithm initialized running kmeans 10 different initial starting points choosing em solution highest likelihood avoid poor local maxima ffl k points randomly drawn binned histogram k chosen 10 number total data points 100 points whichever greater points drawn using uniform sampling distribution ffl binned em algorithm initialized running standard em algorithm 5 random restarts k randomly drawn data points ffl avoid poor local maxima binned em algorithm chooses solution highest likelihood solutions 10 different random initializations ffl convergence standard binnedtruncated em judged change less 001 loglikelihood maximum 20 em iterations whichever comes first ffl order romberg integration set 3 ffl set 10 gamma4 ffl default accuracy integration set 42 simulation experiments simulated data twodimensional mixture two gaussians centered 150 150 unit covariance matrices varied number data points per dimension steps 10 1000 drew 10 random samples size n bivariate mixture addition varied number bins per dimension steps 5 original unbinned samples quantized b 2 bins range grid extended 55 55 truncation relatively rare original unbinned samples ran standard em algorithm binned data ran binned version em using versions em parameters settings described earlier purpose simulation observe effect binning sample size quality solution note standard algorithm typically given much information data ie exact locations data points thus average expect perform better algorithm binned data learn measure solution quality calculated kullbackleibler kl cross entropy distance estimated density true known density figure 4 average kl distance estimated density estimated using procedure described paper true density function number bins number data points bins per dimension different random samples kl distance binned 100 data points per component standard 100 data points per component binned 300 data points per component standard 300 data points per component binned 1000 data points per component standard 1000 data points per component figure 5 average kl distance logscale estimated densities true density function number bins different sample sizes compared standard em unbinned data kl distance nonnegative zero two densities identical calculated average kl distance 10 samples value n b binned standard em algorithms total standard binned algorithms run 20000 different times generate reported results figure 4 shows plot average kldistance binned em algorithm function number bins number data points one clearly see plateau effect kldistance solution generating true density measure quality solution relatively close zero number bins 20 number data points 500 function n number data points one sees typical exponentially decreasing learning curve ie solution quality increases roughly proportion n gammaff constant ff function bin size b appears threshold effect 20 bins solution quality relatively flat function number bins solutions rapidely decrease quality eg significant degradation figure 5 plot kl distance logscale function bin size specific values n comparing standard binned versions em 3 values n curves qualitative shape rapid improvement quality move relatively flat performance ie sensitivity b 20 3 values n binned em tracks performance standard em quite closely number data points kl distance 5 bins per dimension bins per dimension 100 bins per dimension standard algorithm figure average kl distance logscale estimated densities true density function sample size different numbers bins compared standard em unbinned data difference two becomes less n increases variability curves due variability 10 randomly sampled data sets particular value b n note b 20 difference binned standard versions em smaller natural variability due random sampling effects figure 6 plots average kl distance logscale function n number data points per dimension specific numbers bins b compare binned algorithm various b values standard unbinned algorithm overall see characteristic exponential decay linear loglog plot learning curves function sample size b 20 binned em tracks standard em quite closely results suggest particular problem least em algorithm binned data sensitive number bins number data points terms comparative performance em unbinned data certain threshold number bins 20 binned version em appears able recover true shape densities almost well version em sees original unbinned data volume hemoglobin concentration control 1 hemoglobin concentration volume control 2 hemoglobin concentration volume control 3 hemoglobin concentration volume iron deficient 1 hemoglobin concentration volume iron deficient 2 hemoglobin concentration volume iron deficient 3 figure 7 contour plots estimated density estimates three typical control patients three typical iron deficient anemia patients lowest 10 probability contours plotted emphasize systematic difference two groups 43 application red blood cell data mentioned beginning paper work motivated realworld application medical diagnosis based twodimensional histograms characterizing red blood cell volume hemoglobin measurements see figure 1 mclaren 1996 summarizes prior work problem onedimensional mixturefitting algorithm mclachlan jones 1988 used fit mixture models onedimensional red blood cell volume histograms mixture models particularly useful context generative model since plausible different components model correspond blood cells different states cadez et al 1999 generalized earlier work mclaren et al 1991 mclaren 1996 onedimensional volume data analysis twodimensional volume hemoglobin histograms mixture densities fit histograms 97 control subjects 83 subjects iron deficient anemia using binnedtruncated em procedure described present paper figure 3 demonstrated improvement computation time achievable data figure 3 2component mixture model fit control subject 2dimensional histogram 40000 red blood cells figure 7 shows contour probability plots fitted mixture densities 3 control 3 iron deficient subjects plot lower 10 probability density function since differences two populations obvious tails one clearly see systematic variability within control iron deficient groups well two groups since number bins relatively large dimension number data points 40000 simulation results previous section would tend suggest density estimates likely relatively accurate compared running em unbinned data cadez et al 1999 used parameters estimated mixture densities basis supervised classification subjects two groups resulting error rate 15 crossvalidated experiments compares crossvalidated error rate 4 subjects using algorithms cart c50 directly features histogram univariate means standard deviations ie using mixture modeling thus ability fit mixture densities binned truncated data played significant role improved classification performance particular problem conclusions problem fitting mixture densities multivariate binned truncated data addressed using generalization mclachlan jones 1988 em procedure onedimensional problem multivariate em algorithm requires multivariate numerical integration em iteration described variety computational numerical implementation issues need careful consideration context simulation results indicate high quality solutions obtained compared running em raw unbinned data unless number bins relatively small acknowledgements contributions ic ps paper supported part national science foundation grant iri9703120 contribution cmcl supported part grants national institutes health r43 hl46037 r15hl48349 wellcome research travel grant awarded burroughs wellcome fund thank thomas h cavanagh providing laboratory facilities grateful dr albert greenbaum technical assistance r neural networks pattern recognition hierarchical models screening iron deficiency anemia submitted icml99 maximum likelihood incomplete data via em algorithm j statistical analysis missing data fitting mixture models grouped truncated data via em algorithm biometrics em algorithm extensions mixture models haematology series case studies statistical methods medical research numerical recipes c art scientific computing elements statistical computing tr statistical analysis missing data elements statistical computing color indexing numerical recipes c 2nd ed intelligent multimedia information retrieval histogrambased estimation techniques database systems waveletbased histograms selectivity estimation multidimensional selectivity estimation using compressed histogram information neural networks pattern recognition query image video content hierarchical models screening iron deficiency anemia ctr nizar bouguila djemel ziou unsupervised learning finite discrete mixture applications texture modeling image databases summarization journal visual communication image representation v18 n4 p295309 august 2007