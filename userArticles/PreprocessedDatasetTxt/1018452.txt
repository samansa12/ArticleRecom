complexity synchronous iterative doall crashes ability cooperate common tasks distributed setting key solving broad range computation problems ranging distributed search seti distributed simulation multiagent collaboration doall abstraction cooperative activity problem performing n tasks distributed system p failureprone processors many distributed parallel algorithms developed problem several algorithm simulations developed iterating doall algorithms efficiency solutions doall measured terms work complexity processing steps taken processors counted work ideally expressed function n p f number processor crashes however known lower bounds upper bounds extant algorithms adequately show work depends f present first nontrivial lower bounds doall capture dependence work n p f model computation processors able make perfect loadbalancing decisions locally also present matching upper bounds define riterative doall problem abstracts facts repeated use doall found typical algorithm simulations fsensitive analysis enables us derive tight bounds riterative doall work stronger rfold work complexity single doall approach models perfect loadbalancing allows analysis specific algorithms divided two parts analysis cost tolerating failures performing work free loadbalancing ii analysis cost implementing loadbalancing demonstrate utility generality approach improving analysis two known efficient algorithms give improved analysis efficient messagepassing algorithm also derive tight complete analysis best known doall algorithm synchronous sharedmemory model finally present new upper bound simulations synchronous sharedmemory algorithms crashprone processors b introduction performing set tasks decentralized setting fundamental problem distributed comput ing often challenging set processors available computation ability communicate may dynamically change due perturbations computation medium abstract statement problem referred doall problem p faultprone processors perform n independent tasks one standard problems research complexity faulttolerant distributed computation 9 18 problem studied variety set tings eg sharedmemory models writeall 19 20 26 messagepassing models 7 9 11 partitionable networks omnido 8 15 25 solutions doall must perform tasks eciently presence specic failure patterns eciency assessed terms work time communication complexity depending specic model computation design practical distributedparallel programs one needs ensure good performance dependability unpredictable load patterns caused example deviations synchrony failures processors complete tasks time common challenge perform n independent tasks p processors 14 tasks could copying large array searching collection data applying function elements matrix 12 13 examples include checking points large solution space trying generate witness refute existence simply performing number similar independent calculations paper focus work complexity doall problem presence arbitrary failure patterns imposed adversary processors synchronous assumed failstop 31 work complexity ects amount processing steps expended algorithm solving problem incorporates total number tasks including multiplicities performed algorithm distinguishing feature new results complexity expressed terms number processor crashes f addition number processors p size problem n approach motivated part analyses consensus problems venerable flp impossibility result 10 algorithms solve consensus models allow faulttolerant solutions teach following asynchronous models weak faulttolerance 23 ii maximum number processor failures needs included upperlower bounds impossibility results eg tolerate f failures models consensus algorithms require processors 24 29 work consider crash failures ensure solutions exist long number failures f inferior number processors p aim express work synchronous processors function n p f recently unsatisfactory landscape existed respect understanding upper lower bounds work depend f number failures work typically given function n p either elucidated f impacts work f part equation primarily due nature specic algorithm due inherent properties doall problem example work best known synchronous sharedmemory algorithm 18 given solely function n p also case best known asynchronous sharedmemory algorithm 3 similarly best known sharedmemory lower bound work doall parameterized terms f 20 however shown sharedmemory algorithm take least log p time steps likewise best known lower bound applicable messagepassing models involve f 4 work messagepassing algorithms eg 7 11 typically includes f due use single coordinators means f coordinator failures work necessarily includes factor f p messagepassing algorithm using multiple coordinators 5 avoids ineciency includes factor depends log f show paper analysis involves f somewhat supercial way thus prior lowerupper bound results doall teach adequately work complexity depends number failures f considering synchronous sharedmemory computing failureprone processors impact imprecise analysis work complexity especially signicant approaches 22 30 use iterative doall approach execute synchronous parallel pram algorithms failureprone processors simulating parallel steps ideal processors help chosen doall algorithm see also related work particular shown execution single nprocessor step p failureprone processors exceed asymptotic complexity solving nsize instance doall problem using p failureprone processors thus wnp complexity solving doall instance size n using p processors parallel timeprocessor product given nprocessor algorithm n algorithm deterministically simulated work wnp analysis accurately ect impact number failures f resulting upper bound needlessly ated contributions work study work complexity deterministic doall presence arbitrary dynamic patterns stopfailures let n size doall problem p number processors f number crashes 0 f p n present rst complete analysis doall work complexity perfect load balancing assumption proving matching upper lower bounds functions n p f model computation computation fully abstracted away lowlevel sharedmemory messagepassing issues worstcase omniscient dynamic adversary cause f crashes also establishes rst nontrivial lower bound doall moderate number failures f p log p important contribution work denition analysis riterative doall problem models repetitive use doall algorithms found algorithm simulations demonstrate utility generality results showing new bounds work faulttolerant simulations arbitrary pram algorithms crashprone processors improving analysis two known ecient algorithms derive new complete failure sensitivity analysis best known algorithm synchronous sharedmemory model algorithm w 18 also give improved analysis ecient messagepassing algorithm algorithm 5 let doalln stand doall problem n tasks p processors f failures let doall n f denote doalln problem solved use omniscient oracle assists processors unlike oracles delphian colleague cannot predict future oracle assumption used tool studying work complexity patterns faulttolerant algorithm implements perfect workload balancing allows complexity analysis specic algorithms divided two parts analysis cost tolerating failures performing work assuming perfect loadbalancing ii analysis cost implementing perfect loadbalancing use exactly approach derive new fsensitive upper bounds messagepassing sharedmemory models recently shown 16 building prior results 19 doall n f solved work onp log p log log p f p gave matching lower bound specic case log log p log log p meant long adversary cause least p log log p failures doall n p f matching upper lower bounds n p log p log log p also showed log p doall n f solved work thus prior newest results nontrivial lower bounds known f p log p ii fsensitive analysis available upper bounds f p log p p log log p therefore iii existed gap upperlower bounds analysis range 1 f p log log p practical concerns would well served knowledge happens doall number failures moderate particular important understand behavior best known algorithms entire range f detailed contributions work follows provide upper bounds section 31 matching lower bounds section 32 address remaining gaps hence give complete analysis doall n f entire range f bounds work w 1 log log p log c 0 f f c p log c 0 1 lower bounds course apply algorithms weaker models turns quantity q pf dened extracted bounds plays important role analysis complexity several algorithms log p log log p f c p log c 0 f log c 0 2 use bounds 1 derive new bounds algorithms extant analyses integrate f adequately done analyzing workload balancing implemented algorithms eg using coordinators global datastructures show following ii section 41 provide new analysis algorithm chlebus et al 5 doall messagepassing model crashes algorithm best known work moderate number failures show complete analysis work w message complexity iii section 51 give complete analysis work complexity w algorithm kanellakis shvartsman 18 solves doall writeall problem synchronous sharedmemory systems processor crashes note two algorithms 5 18 designed dierent models use dissimilar data control structures however algorithms make loadbalancing decisions gathering global knowledge understanding work expended load balancing vs inherent work overhead due lower bounds 1 able obtain new results demonstrating utility generality approach doall algorithms used developing simulations failurefree algorithms failureprone processors eg 22 30 done iteratively using doall algorithm simulate steps failurefree processors paper abstract idea iterative doall problem follows riterative doalln problem using p processors solve r instances ntask doall one set tasks time let g stand describing upper bound describing lower bound logarithms base 2 unless explicitly specied otherwise expression log x stands maxf1 log 2 xg given x description complexity results oracle rdoall n f dened similarly obvious solution problem run doall algorithm r times work complexity doall given model wnpf work rdoall clearly r wnpf present substantially better analysis iv section 33 show matching upper lower bounds work w rdoall n f p n specic ranges failures r log log p log r log pr f f p r log extract quantity r rpf dened bounds 3ab plays important role analysis complexity iterative doall algorithms r rpf log p log log p log log p log pr f f p r log note r q pf r rpf specic range f 4b respect r xed p f thus bounds 3 asymptotically better obtained computing product r noniterated doall bounds 1 v section 42 show rdoalln f solved synchronous messagepassing processors following work complexity w message complexity r log f r n r rpf vi section 52 use rdoalln f show p processors crashes simulate synchronous nprocessor rtime sharedmemory algorithm pram work last result strictly better previous deterministic bounds parallel algorithm simulations using doall algorithm 18 best known date simulation techniques 22 30 due relationship q pf r rpf pointed related workalgorithm simulations doall algorithms used iteratively simulate parallel algorithms formulated synchronous failurefree processors deterministic probabilistic settings 22 20 27 28 30 commonly requires individual processor steps made idempotent since may performed multiple times due failures asynchrony ii linear number processors auxiliary memory made available used scratchpad store intermediate results former solved help automated tool eg compiler latter requires sophisticated solutions diculty reusing auxiliary memory due late writers ie processors slow unknowingly write stale values memory examples randomized solutions addressing problems include 2 1 21 another important aspect algorithm simulations use optimistic approach computation may proceed several steps assuming tasks assigned active processors successfully completed example series potentially incorrect tentative steps combined complete denitive step detects rolls back incorrect computation steps 20 overall computation ecient high probability note deterministic models optimal simulations possible cf 30 however randomized solutions able achieve optimality whp broader ranges models algorithms practical implementations discussed 6 also observed parallel computation made faster essentially ignoring processors slower others rest paper structured follows section 2 present models denitions section 3 present bounds perfect loadbalancing assumption section 4 give new upper bounds messagepassing model section 5 give upper bounds sharedmemory model pram simulations conclude section 6 models denitions dene models abstract problem performing n tasks distributed environment consisting p processors subject stopfailures work complexity measure distributed setting consider distributed system consisting p synchronous processors assume p xed known processor unique identier pid set pids totally ordered processors activity governed local clock nonfaulty synchronous systems processor clocks identical model failures introduce delays local clock ticks tasks dene task computation performed processor one time step execution dependent task tasks also idempotent ie executing task many times andor concurrently eect executing task tasks uniquely identied task identiers tids set tids totally ordered denote set n tasks assume known processors model failures assume failstop processor model 31 processor may crash moment computation crashed restart let omniscient adversary impose failures system use term failure pattern denote set events ie crashes caused adversary failure model set failure patterns given adversary failure pattern f dene size f failure pattern number failures oracle model section 3 consider computation processors assisted deterministic omniscient oracle processor may contact oracle per step introduction oracle serves two purposes 1 oracle strengthens model providing processors information progress computation oracle cannot predict future thus lower bounds established oracle model also apply weaker model eg without oracle 2 oracle abstracts away concerns communication normally dominate specic messagepassing sharedmemory models allows general results established enables us use results context specic models understanding information provided oracle simulated specic algorithms communication sections 4 5 deal messagepassing sharedmemory models computation messagepassing model assume known upper bound message delays communication complexity dened section 4 considering computation sharedmemory model assume reading writing memory cell takes one time unit reads writes concurrent doall problems dene doall problem follows doall given set n tasks p processors perform tasks failure pattern failure model f let doalln stand doall problem n tasks p processors p n pattern crashes f jf j f p let doall n stand doalln problem oracle dene iterative doall problem follows iterative doall given r sets r n tasks p processors perform r n tasks one set time failure pattern failure model f denote riterative doall rdoalln oracle version rdoall n f dened similarly measuring eciency interested studying complexity doall measured work cf 18 9 7 assume takes unit time processor perform unit work single task corresponds unit work denition work complexity based available processor steps measure 19 let f adversary model computation subject failure pattern f f 2 f denote p number processors completing unit work step computation denition 21 given problem size n p processor algorithm solves problem failure model f algorithm solves problem pattern f f jf j f time step work complexity w algorithm note idling processors still consume unit work per step even though contribute computation denition 21 depend specics lowlevel target model computation eg whether messagepassing sharedmemory give similar denition communication complexity section 4 3 bounds perfect load balancing section give complete analysis upper lower bounds doall n rdoall n problems entire range f crashes f p n note use quantities q pf r rpf dened section 1 equations 2 4 respectively 31 doall upper bounds study upper bounds doall give oraclebased algorithm figure 1 oracle tells processor whether tasks performed oraclesays task perform next oracletask correctness algorithm trivial thus oracle performs termination loadbalancing computation behalf processors processor global 1n done perform task oracletaskpid od end figure 1 oraclebased algorithm lemma 31 19 16 doall n problem f p n solved using work log log p note lemma 31 teach work depends f number crashes lemma 32 c 0 doall n f solved stopfailure pattern f c p log p using work proof proof based proof theorem 36 16 let f denote number processor stopfailures within single iteration computation f dierent iteration though sum iterations cannot exceed f set 2f dene w n f work required solve doall n goal show u p f work w u f 3p u p log p u op u p log p f u u n denotes number undone tasks proof proceeds induction u base case observe u 3 w u desired induction case assume proved theorem u u n p f consider u investigate two cases case 1 u case processor assigned unique task hence u p fp f f f u u induction hypothesis u u p f desired case 2 u case assumption get f ratio number remaining tasks u 0 1 fraction processors fail iteration 2 see observe uc u u let u c 1 u u bcc observe 1 c bcc 2 u may apply induction hypothesis complete proof suces show 2 0 fp u 1 p log bpf upper bounding 31 dividing p sucient show equivalently log bpf 3 focus left hand side equation log bpf log bpf log bpf u log p 8p 2 2f 2 particular log bpf note f tasks completed iteration recall p therefore desired induction proof denition give main upperbound result theorem 33 doall n f solved failure pattern using work proof follows directly lemmas 31 32 2 32 doall lower bounds show matching lower bounds doall n note results section hold also doalln problem without oracle lemma 34 18 16 algorithm alg solves doall n exists pattern f stopfailures f p results work w log log p dene specic adversarial strategy used derive lower bounds let alg algorithm solves doall problem let p number processors remaining end th step alg let u denote number tasks remain done end step initially log p 0 1 adversary adv step 1 alg adversary stops processors follows among u 1 tasks remaining step 1 adversary chooses u tasks least number processors assigned crashes processors adversary continues long u 1 soon u adversary allows remaining processors perform single remaining task alg terminates following two lemmas used proof lemma 37 lemma 35 18 1 sorted list nonnegative integers lemma 36 18 given g 1 n g integer log n log g 1 following inequality holds z times lemma 37 given c 0 algorithm alg solves doall n adversary adv causes f stopfailures f c p log p f n denition adv follows u z times using lemma 36 get log n 1 recall log p f log log f 1 z times 0 tells us adversary adv cause algorithm alg cycle least log n log p f log log fiterations let denote number iterations needed alg terminate note algorithm cycle least one iteration hence log n log p f log log f need compute lower bound p quantities processors assigned task sorted ascending order let also include quantity unassigned processors ie 1 least number processors assigned task 2 next least quantity processors etc let thus adversary stops exactly processors beginning iteration number processors therefore number surviving processors ij1 using lemma 35 get p 1 u substituting u using properties observe work must least p p surviving processors alg ter minates consider two cases case 1 log p log p let f denote total number failstops alg terminates log p log p f since log log p log p f log p therefore adversary adv cause processor failstops allowed work caused adv case f log p case 2 log p f log log f log p log n log p f log log f p log p log p log p log p log p log p f hence adv cause failstops allowed work caused adv case log n log p log n log n f log n note w 2 0 since p log n p recall f c p log p hence 8p 2 2c p f 2 particular suces p 4 two cases denition get wnpf log n log p f lemma 38 given c 0 algorithm alg solves doall n exists adversary causes f stopfailures f c p log p f follows lemma 37 slightly modifying adversary adv 2 give main lowerbound result theorem 39 given algorithm alg solves doall n exists adversary causes proof range failures 0 f c p log establishes bound lemma 38 also obtain fact log p work must log log p larger f adversary establishes worst case work using initial c p log p failures 2 33 iterative doall doall algorithms used developing simulations failurefree algorithms failureprone processors done iteratively using doall algorithm simulate steps failurefree processors study iterative doall problems understand complexity implications iterative use doall algorithms obvious rdoalln f solved running doalln times work doall solution w work riterative doall r w however show possible obtain ner result refer doall iteration round rdoall n theorem 310 rdoall n solved denote th round rdoall n number active processors beginning r f number crashes r note p rst round rdoall n f p consider two cases case 1 f p r log consider round r theorem 33 get work round log log log log p however case f log p r without running processors thus work case log log p case 2 f p r log first observe reasonable adversary would kill p log processors round r since would cause work log log log p achieved f log therefore consider f log rounds r hence work every round r per theorem 33 log log log p let w n f oneround upper bound upper bound rdo n f given maximizing failure patterns may assume p purposes upper bound show maximum attained f simplicity treat f continuous parameter consider factor single round work expression given depends f c constant hidden notation rst derivative f clog p second derivative 2 clog p observe second derivative negative domain considered long p 16 hence rst derivative decreasing f case given two f failure pattern obtained replacing f results increased work implies sum maximized f equal specically f upper bound sum range holds particular choices made adversary must course cause integer number faults round therefore log pr f result follows denition r rpf combining two cases 2 theorem 311 given algorithm solves rdoall n exists stop failure adversary causes r n r rpf consider two cases case 1 f p r log case adversary may failstop p log p processors every round r doall n note adversary processors remain alive rst dr2e rounds per theorem 39 results dr2e log log p log log p work case 2 f p r log case adversary ideally would kill f r processors every round case f divides r case adversary kills f r e processors r rounds b f r c r b rounds way considering rst half rounds appealing theorem 39 results rn p log rp f lower bound work note consider case r f otherwise work trivially 4 new bounds messagepassing model section demonstrate utility complexity results perfect loadbalancing assumption giving tight complete analysis algorithm 5 establish new complexity results iterative doall messagepassing model 41 analysis algorithm algorithm presented chlebus et al 5 uses multiplecoordinator approach solve alln f crashprone synchronous messagepassing processors model assumes messages incur known bounded delay reliable multicast 17 available however messages tofrom faulty processors may lost eciency algorithm characterized terms work message complexity dene message complexity similarly denition 21 work computation subject failure pattern f f 2 f denote number pointtopoint messages sent step computation given problem size n computation solves problem step presence failure pattern f jf j f message complexity description algorithm due space limitation give brief description algorithm additional details given appendix avoid complete restatement refer reader 5 algorithm proceeds loop iterated tasks executed single iteration loop called phase phase consists three consecutive stages stage consists three steps stage processors use rst step receive messages sent previous stage second step perform local computation third step send messages processor coordinator worker phase may multiple coordinators number processors assume coordinator role determined martingale principle none expected coordinators survive entire phase number coordinators next phase doubled least one coordinator survives given phase next phase one coordinator phase completed least one coordinator alive called attended otherwise called unattended processors become coordinators balance loads according processors local view processors local view contains set processors ids assumes alive local view partitioned layers rst layer contains one processor second two processors third four processors given phase rst stage processors perform task according load balancing rule derived local views report completion task coordinators phase determined local views second stage coordinators gather reports update knowledge done tasks multicast information processors according local views alive last stage processors receive information sent coordinators update knowledge done tasks local views given full details algorithm dicult see combination coordinators local views allows processors obtain information would available oracle algorithm figure 1 shown 5 work algorithm log n log log n log f message complexity new analysis work complexity assess work w consider separately attended phases unattended phases execution let w part w spent attended phases w u part w spent unattended phases hence note p n lemma 41 5 execution algorithm f p w log log p w give new analysis algorithm lemma 42 execution algorithm w p log p f log p c 0 given phase execution algorithm dene p number live processors u number undone tasks beginning phase attended phases execution last phase execution observe holds u u i1 follows construction algorithm since phase attended least one coordinator call c alive phase c executes one task hence least one task executed consequently least one task taken u number processors decrease since allow restarts therefore focusing attended phases proof lemma induction size undone tasks u note proof proceeds proof lemma 32 p n 2 theorem 43 execution algorithm follows lemmas 41 42 fact analysis message complexity assess message complexity consider separately attended phases unattended phases execution let number messages sent attended phases u number messages sent unattended phases hence lemma 44 5 execution algorithm ow theorem 45 execution algorithm proof proof follows lemmas 41 42 44 fact 42 analysis messagepassing iterative doall consider rdoalln f problem p n messagepassing model theorem 46 rdoalln f problem solved synchronous messagepassing crash prone processors work r n n r rpf sketch iterative doall solved running algorithm r instances size n sequence call algorithm analyze eciency use approach proof theorem 310 current context base work complexity arguments result theorem 43 base message complexity arguments result theorem 45 2 5 new bounds sharedmemory model give new rened analysis workecient known doall algorithm sharedmemory model algorithm w 18 also establish complexity results iterative doall simulations synchronous parallel algorithms crashprone processors 51 analysis algorithm w algorithm w solves doalln f sharedmemory model doall better known writeall work pattern crashes note bound conservative since include f number crashes description algorithm give brief description algorithm additional details found appendix avoid complete restatement refer reader 19 algorithm w structured parallel loop four phases w1 failure detecting phase w2 load rescheduling phase w3 work phase w4 phase estimates progress computation remaining work controls parallel loop phases use full binary trees leaves processors traverse binary trees topdown bottomup according phase traversal takes olog n time height tree single processor iteration loop called blockstep since four phases one tree traversal per phase block step takes olog n time algorithm w trees stored shared memory serve gathering places global information number active processors remaining tasks load balancing dicult see binary trees indeed provide information processors would available oracle oracle model binary tree used phase w2 implement load balancing phase w3 assess remaining work called progress tree use parameterized version algorithm p n progress tree leaves tasks associated leaves tree nu tasks per leaf note blockstep still takes time olog n new complexity analysis give work analysis charge processor block step starts regardless whether processor completes crashes lemma 51 19 failure pattern f p number blocksteps required processor algorithm w u leaves progress tree log log p lemma 52 failure pattern f c p log p c 0 number blocksteps required p processor algorithm w u leaves progress tree f processor blocksteps shown equivalent processor steps perfect loadbalancing assumption hence proof proof lemma 32 2 theorem 53 algorithm w solves doalln consider following two cases case 1 p n log number leaves progress tree log n work phase w3 processor performs tasks cost single blockstep c since four phases takes log n time consider two subcases f p log p lemma 52 gives number blockssteps b 1a case ou log p f n log log p f therefore work w 1a case b 1a c log p f log p lemma 51 gives number blocksteps b 1b case ou p log p log log p n log n p log p log log p therefore work w 1b case b 1b c log log p two subcases together denition q pf yield w case 2 n log n p n number leaves progress tree work phase w3 processor performs n tasks thus cost single blockstep consider two subcases f p log p lemma 52 gives number blocksteps b 2a case ou log p f opp log p log p f log p f therefore work w 2a case b 2a c log p f log p lemma 51 gives number blocksteps b 2b case op p log p log log p op log p log log p therefore work w 2b b 2b c log log p last two subcases denition q pf yield w combining case 1 case 2 results get 52 iterative doall parallel algorithm simulations consider complexity sharedmemory rdoalln f pram simulations theorem 54 rdoalln f problem solved p crashprone processors p n using shared memory work sketch iterative doall solved running algorithm w r instances size n sequence call algorithm w analyze eciency w use approach proof theorem 310 current context base work complexity arguments result theorem 53 2 state another main result paper theorem 55 synchronous nprocessor rtime sharedmemory parallel algorithm pram simulated p crashprone synchronous processors work n r rpf log n proof complexity simulating single parallel step n ideal processors p crashprone processors exceed complexity solving single doalln instance 22 30 result follows theorem 54 2 6 conclusions paper gave rst complete analysis doall problem perfect loadbalancing assumption introduced analyzed iterative doall problem models repeated use doall algorithms found algorithm simulations transformations unique contribution analyses precisely describe eect crash failures work computation analyses obtained perfect loadbalancing assumption immediately useful used analyze algorithms simulations attempt balance loads among processors finally provided rst failuresensitive analysis work iterative doall problem messagepassing sharedmemory models r tr efficient parallel algorithms made robust efficient robust parallel computations combining tentative definite executions fast dependable parallel computing achieving optimal crcw pram faulttolerance efficient program transformations resilient parallel computation via randomization preliminary version performing work efficiently presence faults workoptimal asynchronous algorithms shared memory parallel computers complexity certified writeall algorithms timeoptimal messageefficient work performance presence faults parallel algorithms processor failures delays algorithms certified writeall problem faulttolerant broadcasts related problems failstop processors setihomemyampersandmdashmassively distributed computing seti faulttolerant parallel computation distributed cooperation absence communication complexity synchronous iterative doall crashes optimal freliable protocols doall problem singlehop wireless networks resolving message complexity byzantine agreement beyond parallelism random access machines parallel processing networks workstations cooperative computing fragmentable mergeable groups ctr chryssis georgiou dariusz r kowalski alexander shvartsman efficient gossip robust distributed computation theoretical computer science v347 n12 p130166 november 2005 antonio fernndez chryssis georgiou alexander russell alex shvartsman doall problem byzantine processor failures theoretical computer science v333 n3 p433454 3 march 2005