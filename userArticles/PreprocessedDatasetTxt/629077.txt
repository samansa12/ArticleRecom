demonstration automatic data partitioning techniques parallelizing compilers multicomputers approach problem automatic data partitioning introduced notion constraints data distribution presented shown based performance considerations compiler identifies constraints imposed distribution various data structures constraints combined compiler obtain complete consistent picture data distribution scheme one offers good performance terms overall execution time results study performed fortran programs taken linpack eispack libraries perfect benchmarks determine applicability approach real programs presented results encouraging demonstrate feasibility automatic data partitioning programs regular computations may statically analyzed covers extremely significant class scientific application programs b introduction distributed memory multiprocessors multicomputers increasingly used providing high levels performance scientific applications distributed memory machines offer significant advantages shared memory counterparts terms cost scalability widely accepted fact much difficult program shared memory machines one major reason difficulty absence single global address space result programmer distribute code data processors manage communication among tasks explicitly clearly need parallelizing compilers relieve programmer burden area parallelizing compilers multicomputers seen considerable research activity last years number researchers developing compilers take program written sequential sharedmemory parallel language based userspecified partitioning data generate target parallel program multicomputer research efforts include fortran compiler project rice university 9 superb project bonn university 22 kali project purdue university 13 dino project colorado university 20 dealing imperative languages extensions fortran c crystal project yale university 5 compiler 19 also based idea targeted functional languages parallel program generated systems corresponds spmd single program multipledata 12 model processor executes program operates distinct data items current work parallelizing compilers multicomputers large concentrated automating generation messages communication among processors use term paralleliz ing compiler somewhat misleading context since parallelization decisions really left programmer specifies data partitioning method data partitioning determines interprocessor communication takes place independent computations actually get executed different processors distribution data across processors critical importance efficiency parallel program distributed memory system since interprocessor communication much expensive computation processors essential processor able much computation possible using local data excessive communication among processors easily offset gains made use parallelism another important consideration good data distribution pattern allow workload evenly distributed among processors full use made parallelism inherent computation often tradeoff involved minimizing interprocessor communication balancing load processors good scheme data partitioning must take account communication computation costs governed underlying architecture machine goal automatic parallelization sequential code remains incomplete long programmer forced think issues come right data partitioning scheme program task determining good partitioning scheme manually extremely difficult tedious however existing projects parallelization systems multicomputers far chosen tackle problem compiler level known difficult problem mace 16 shown problem finding optimal data storage patterns parallel processing even 1d 2d arrays npcomplete another related problem component alignment problem discussed li chen 15 shown npcomplete recently several researchers addressed problem automatically determining data partitioning scheme providing help user task ramanujan sadayappan 18 worked deriving data partitions restricted class programs however concentrate individual loops strongly connected components rather considering program whole hudak abraham 11 socha 21 present techniques data partitioning programs may modeled sequentially iterated parallel loops balasundaram et al 1 discuss interactive tool provides assistance user data distribution key element tool performance estimation module used evaluate various alternatives regarding distribution scheme li chen 15 address issue data movement processors due crossreferences multiple distributed arrays also describe explicit communication synthesized communication costs estimated analyzing reference patterns source program 14 estimates used evaluate different partitioning schemes approaches serious drawbacks associated problem restricted applicability apply programs may modeled single multiply nested loops others require fairly exhaustive enumeration possible data partitioning schemes may render method ineffective reasonably large problems clearly strategy automatic data partitioning expected work well applications regular computational structure static dependence patterns determined compile time however even though exists significant class scientific applications properties data show effectiveness methods real programs paper present novel approach call constraintbased approach 7 problem automatic data partitioning multicomputers approach compiler analyzes loop program based performance considerations identifies constraints distribution various data structures referenced loop quality measure associated constraint captures importance respect performance program finally compiler tries combine constraints data structure consistent manner overall execution time parallel program minimized restrict partitioning arrays ideas underlying approach applied distributed memory machines intel ipsc2 ncube warp systolic machine examples written fortranlike language present results fortran programs however ideas developed partitioning arrays equally applicable similar programming language rest paper organized follows section 2 describes abstract machine kind distributions arrays may scheme section 3 introduces notion constraints describes different kinds constraints may imposed array distributions section 4 describes compiler analyzes program references record constraints determine quality measures associated section 5 presents strategy determining data partitioning scheme section 6 presents results study fortran programs performed determine applicability approach real programs finally conclusions presented section 7 2 data distribution abstract target machine assume ddimensional maximum dimensionality array used program grid n 1 n 2 processors topology easily embedded almost distributed memory machine processor topology represented tuple correspondence tuple p processor number range 0 established scheme embeds virtual processor grid topology real target machine make notation describing replication data simpler extend representation processor tuple following manner processor tuple x appearing ith position denotes processors along ith grid dimension thus 2 2 grid processors tuple 0 x represents processors 0 0 0 1 tuple x x represents four processors scalar variables small arrays used program assumed replicated processors arrays use separate distribution function dimension indicate array distributed across processors turns convenient single distribution function associated multidimensional array refer kth dimension array k array dimension k gets mapped unique dimension processor grid number processors along grid dimension one say array dimension k sequentialized sequentialization array dimension implies elements whose subscripts differ dimension allocated processor distribution function k takes argument index returns component tuple representing processor owns element agamma denotes arbitrary value index appearing k th dimension array dimension k may either partitioned replicated corresponding grid dimension distribution function form ae b igammaoffset block cmodn replicated square parentheses surrounding modn indicate appearance part expression optional higher level given formulation distribution function thought specifying following parameters 1 whether array dimension partitioned across processors replicated 2 method partitioning contiguous cyclic 3 grid dimension kth array dimension gets mapped 4 block size distribution ie number elements residing together block processor 5 displacement applied subscript value mapping examples data distribution schemes possible array 4processor machine shown figure 1 numbers shown figure indicate processors part array allocated machine considered n 1 n 2 mesh processor number corresponding tuple p distribution functions corresponding different figures given array subscripts assumed start value 1 fortran c f last example illustrates notation allows us specify partial replication data ie replication array dimension along specific dimension processor grid array replicated completely processors distribution function dimensions takes value x dimensionality processor topology greater dimensionality array need gamma distribution functions order completely specify processors owning given element array functions provide remaining gamma numbers processor tuple restrict functions take constant values value x array replicated along corresponding grid dimension arrays used real scientific programs routines linpack eispack libraries perfect benchmark programs 6 fewer three dimensions believe even programs higher dimensional arrays restricting number dimensions distributed across processors two usually lead loss effective parallelism consider completely parallel loop nest shown even though loop parallelism three levels twodimensional grid topology z 1 z 2 distributed z 3 sequentialized would give performance threedimensional topology number processors z distributed order simplify strategy observation providing justification shall assume underlying target topology twodimensional mesh sake notation describing distribution array dimension grid dimension shall continue regard target topology conceptually ddimensional grid restriction values n later set one 3 constraints data distribution data references associated loop program indicate desirable properties final distribution various arrays formulate desirable characteristics constraints data distribution functions use term differs slightly common usage sense constraints data distribution represent requirements met requirements necessarily met corresponding statement assigning values array parallelizable loop two kinds constraints parallelization constraints communication constraints former kind gives constraints distribution array appearing left hand side assignment statement distribution array elements assigned values parallelizable loop distributed evenly many processors possible get good performance due exploitation parallelism communication constraints try ensure data elements read statement reside processor one owns data element written motivation owner computes rule 9 followed almost parallelization systems multicomputers according rule processor responsible computation one owns data item assigned value computation whenever computation involves use value available locally processor need interprocessor communication communication constraints try eliminate need interprocessor communication whenever possible general depending kind loop single loop may correspond one category rules imposing following kinds constraints 1 parallelizable loop array gets assigned values parallelization constraints distribution 2 loop assignments array use values array b communication constraints specifying relationship distributions b 3 loop assignments certain elements use values different elements communication constraints distribution 4 loop single assignment statement uses values multiple elements array b communication constraints distribution b constraints distribution array may specify relevant parameters number processors array dimension distributed whether distribution contiguous cyclic block size distribution two kinds constraints relationship distribution arrays one kind specifies alignment dimensions different arrays two array dimensions said aligned get distributed processor grid dimension kind constraint relationships formulates one distribution function terms aligned dimensions example consider loop shown data references loop suggest 1 aligned b 1 2 sequentialized secondly suggest following distribution function b 1 terms 1 bic 2 c 1 thus given parameters regarding distribution like block size offset number processors determine corresponding parameters regarding distribution b looking relationship two distributions intuitively notion constraints provides abstraction significance loop respect data distribution distribution array involves taking decisions regarding number parameters described earlier constraint specifies basic minimal requirements distribution hence parameters related distribution array left unspecified constraint may selected combining constraint others specifying parameters combination leads improvement distribution scheme program whole however different parts program may also impose conflicting requirements distribution various arrays form constraints inconsistent order resolve conflicts associate measure quality constraint depending kind constraint use one following two quality measures penalty execution time actual execution time constraints finally either satisfied satisfied data distribution scheme refer boolean constraints example constraint one specifying alignment two array dimensions use first measure estimates penalty paid execution time constraint honored constraints specifying distribution array dimension number processors use second measure expresses execution time simple function number proces sors depending whether constraint affects amount parallelism exploited interprocessor communication requirement expression quality measure terms computation time communication time one problem estimating quality measures constraints may depend certain parameters final distribution known beforehand express quality measures functions parameters known stage instance quality measure constraint alignment two array dimensions depends numbers processors two dimensions otherwise distributed expressed function numbers determining constraints quality measures success strategy data partitioning depends greatly compilers ability recognize data reference patterns various loops program record constraints indicated references along quality measures limit attention statements involve assignment arrays since scalar variables replicated processors computation time component quality measure constraint determined estimating time sequential execution based count various operations estimating speedup determining communication time component relatively tougher problem developed methodology compiletime estimation communication costs incurred program 8 based identifying primitives needed carry interprocessor communication determining message sizes communication costs obtained functions numbers processors various arrays distributed method partitioning namely contiguous cyclic quality measures various communication constraints based estimates obtained following methodology shall briefly describe details found 8 communication primitives use array reference patterns determine communication routines given library best realize required communication various loops idea first presented li chen 14 show explicit communication synthesized analyzing data reference patterns extended work several ways able handle much comprehensive set patterns described 14 assume following communication routines supported operating system runtime library sending message single source single destination processor onetomanymulticast multicasting message processors along specified dimensions processor grid reducing sense apl reduction operator data using simple associative operator processors lying specified grid dimensions manytomanymulticast replicating data processors given grid dimensions table 1 shows cost complexities functions corresponding primitives hypercube architecture parameter denotes message size words seq sequence numbers representing numbers processors various dimensions aggregate communication primitive carried function num applied sequence simply returns total number processors represented sequence namely product numbers sequence general parallelization system written given machine must knowledge actual timing figures associated primitives machine one possible approach obtaining timing figures training set method recently proposed balasundaram et al 2 subscript types array reference pattern characterized loops statement appears kind subscript expressions used index various dimensions array subscript expression assigned one following categories subscript expression evaluates constant compile time subscript expression reduces form c 1 constants loop index note induction variables corresponding single loop index also fall category ffl variable default case signifies compiler knowledge subscript expression varies different iterations loop subscripts type index variable define parameter called changelevel level innermost loop subscript expression changes value subscript type index simply level loop corresponds index appearing expression method statement loop assignment array uses values different array shall refer arrays appearing left hand side right hand side assignment statement lhs rhs arrays express estimates communication costs functions numbers processors various dimensions arrays distributed assignment statement references multiple arrays procedure repeated rhs array sake brevity shall give brief outline steps procedure details algorithm associated step given 8 1 loop enclosing statement loops need perfectly nested inside one another determine whether communication required taken loop step ensures whenever different messages sent different iterations loop combined recognize opportunity use cost functions associated aggregate communication primitives rather associated repeated transfer operations algorithm taking decision also identifies program transformations loop distribution loop permutations expose opportunities combining messages 2 rhs reference identify pairs dimensions arrays rhs lhs aligned communication costs determined assuming alignment array dimensions determine quality measures alignment constraints simply obtain difference costs cases given dimensions aligned 3 pair subscripts lhs rhs references corresponding aligned dimensions identify communication terms representing contribution pair overall communication costs whenever least one subscript pair type index variable term represents contribution enclosing loop identified value changelevel kind contribution loop depends whether loop identified step 1 one communication taken outside communication taken outside term contributed loop corresponds aggregate communication primitive otherwise corresponds repeated transfer 4 multiple references statement rhs array identify isomorphic references namely references subscripts corresponding dimension type communication costs pertaining isomorphic references obtained looking costs corresponding one references step 3 determining get modified adjustment terms remaining references 5 communication terms representing contributions various loops various loop independent subscript pairs obtained compose together using appropriate ordering determine overall communication costs involved executing given assignment statement program examples present example program segments show kind constraints inferred data references associated quality measures obtained applying methodology along example provide explanation justify quality measure derived constraint expressions quality measures however obtained automatically following methodology example 1 parallelization constraints distribute 1 2 cyclic manner example shows multiply nested parallel loop extent variation index inner loop varies value index outer loop simplified analysis indicates 1 2 distributed cyclic manner would obtain speedup nearly n otherwise imbalance caused contiguous distribution would lead effective speedup decreasing factor two c p estimated time sequential execution given program segment quality measure example 2 communication constraints align 1 ensure distributions related following manner j 2 b c 3 dimension pairs mentioned aligned relationships hold elements b residing processor may needed processor hence n 1 n 2 n n j elements held processor replicated processors communication constraints ffl align 1 seen previous example values b held n n j processors replicated indicated dimensions aligned distributed n 1 processors processor needs get elements boundary rows two neighboring processors given term indicates transfer operation takes place condition n 1 analysis similar previous case contiguous manner distributed cyclically processor needs communicate b elements two neighboring processors contiguous manner analysis similar previous case note loop also parallelization constraints associated c p indicates estimated sequential execution time loop combining computation time estimate given parallelization constraint communication time estimates given get following expression execution time n j interesting see expression captures relative advantages distribution arrays b rows columns blocks different cases corresponding different values n 1 n 2 5 strategy data partitioning basic idea strategy consider constraints distribution various arrays indicated important segments program combine consistent manner obtain overall data distribution resolve conflicts mutually inconsistent constraints basis quality measures quality measures constraints often expressed terms n number elements along arry dimension n number processors dimension distributed compare numerically need estimate values n n value n may supplied user assertion specified interactive environment may estimated compiler basis array declarations seen program need values variables form poses circular problem values become known final distribution scheme determined needed stage decisions data distribution taken break circularity assuming initially array dimensions distributed equal number processors enough decisions data distribution taken boolean constraint know whether satisfied start using expressions execution time functions various n determine actual values execution time minimized strategy determining data distribution scheme given information constraints consists steps given step involves taking decisions aspect data distribu tion manner keep building upon partial information describing data partitioning scheme complete picture emerges approach fits naturally idea using constraints distributions since constraint looked upon partial specification data distribu tion steps presented simple enough automated hence discussion really refers parallelizing compiler 1 determine alignment dimensions various arrays problem referred component alignment problem li chen 15 prove problem npcomplete give efficient heuristic algorithm adapt approach problem use algorithm determine alignment array dimensions undirected weighted graph called component affinity graph cag constructed source program nodes graph represent dimensions arrays every constraint alignment two dimensions edge weight equal quality measure constraint generated corresponding two nodes component alignment problem defined partitioning node set cag maximum dimension arrays disjoint subsets total weight edges across nodes different subsets minimized restriction two nodes corresponding array subset thus approximate solution component alignment problem indicates dimensions various arrays aligned establish onetoone correspondence class aligned array dimensions virtual dimension processor grid topology thus mapping array dimension virtual grid dimension becomes known end step 2 sequentialize array dimensions need partitioned given class aligned array dimen sions dimension necessarily distributed across one processor get speedup determined looking parallelization constraints sequentialize dimensions class lead significant savings communication costs without loss effective parallelism 3 determine following parameters distribution along dimension contiguouscyclic relative block sizes class dimensions sequentialized array dimensions number elements given kind distribution contiguous cyclic array dimensions compare sum total quality measures constraints advocating contiguous distribution favoring cyclic distribution choose one higher total quality measure thus collective decision taken dimensions class maximize overall gains array dimension distributed certain number processors contiguous manner block size determined number elements along dimension however distribution cyclic flexibility choosing size blocks get cyclically distributed hence cyclic distribution chosen class aligned dimensions look constraints relative block sizes pertaining distribution various dimensions class constraints may mutually consistent hence strategy adopt partition given class aligned dimensions equivalence subclasses member subclass block size assignment dimensions subclasses done following greedy approach constraints implying relationships two distributions considered nonincreasing order quality measures two concerned array dimensions yet assigned subclass assignment done basis relative block sizes implied constraint dimensions already assigned respective subclasses present constraint ignored since assignment must done using constraint higher quality measure relative block sizes determined using heuristic smallest block size fixed one related block sizes determined accordingly 4 determine number processors along dimension point boolean constraint know whether satisfied adding together terms computation time communication time quality measures constraints satisfied obtain expression estimated execution time let 0 denote number virtual grid dimensions yet sequentialized point expression obtained execution time function variables representing numbers processors along corresponding grid dimensions real programs expect value 0 two one 0 2 first sequentialize except two given dimensions based following heuristic evaluate execution time expression program c 0cases case corresponding 2 different n variables set n set 1 n total number processors system case gives smallest value execution time chosen corresponding dimensions sequentialized get two dimensions execution time expression function one variable 1 since n 2 given nn 1 evaluate execution time expression different values various factors n ranging 1 n select one leads smallest execution time 5 take decisions replication arrays array dimensions take two kinds decisions step first kind consists determining additional distribution function onedimensional array finally chosen grid topology two real dimensions kind involves deciding whether override given distribution function array dimension ensure replicated rather partitioned processors corresponding grid dimension assume enough memory processor support replication array deemed necessary assumption hold strategy simply modified become selective choosing arrays array dimensions replication second distribution function onedimensional array may integer constant case array element gets mapped unique processor may take value x signifying elements get replicated along dimension array look constraints corresponding loops array used array candidate replication along second grid dimension quality measure constraint satisfied shows array multicast dimension example array array b example loop shown section 3 2 sequentialized decision favoring replication taken time array written cost processors second grid dimension carrying computation less sum costs performing computation single processor multicasting result note cost performing computation processors turn less values needed computation replicated every onedimensional array replicated second distribution function given constant value zero decision override distribution function array dimension partitioning replication grid dimension taken sparingly replication done array element written program loops involve sending values elements array processors along grid dimension simple example illustrating strategy combines constraints across loops shown first loop imposes constraints alignment 1 since variable used subscript dimensions also suggests sequentialization 2 regardless values c 1 c 2 elements may reside processor second loop imposes requirement distribution cyclic compiler recognizes range inner loop fixed directly value outer loop index hence would serious imbalance load processors carrying partial summation unless array distributed cyclically constraints consistent get accepted steps 1 4 3 respectively strategy hence finally combination constraints leads following distributions rowwise cyclic columnwise cyclic b general conflicts step strategy different constraints implied various loops consistent conflicts get resolved basis quality measures 6 study numeric programs approach automatic data partitioning presupposes compilers ability identify various dependences program currently process implementing approach using parafrase2 17 sourcetosource restructurer developed university illinois underlying tool analyzing programs prior performed extensive study using well known scientific application programs determine applicability proposed ideas real programs one objectives determine extent stateoftheart parallelizing compiler provide information data references program system may infer appropriate constraints distribution arrays however even complete information programs computation structure available problem determining optimal data decomposition scheme nphard hence second objective find strategy leads good decisions data partitioning given enough information data references program application programs five different fortran programs varying complexity used study simplest program chosen uses routine dgefa linpack library routine factors real matrix using gaussian elimination partial pivoting next program uses eispack library routine tred2 reduces real symmetric matrix symmetric tridiagonal matrix using accumulating orthogonal similarity transformations remaining three programs perfect club benchmark suite 6 program trfd simulates computational aspects twoelectron integral transformation code mdg provides molecular dynamics model water molecules liquid state room temperature pressure flo52 twodimensional code providing analysis transonic inviscid flow past airfoil solving unsteady euler equations methodology testbed implementation evaluation scheme intel ipsc2 hypercube system objective obtain good data partitionings programs running 16processor configuration obtaining actual values quality measures various constraints requires us knowledge costs various communication primitives arithmetic operations machine use following approximate function 10 estimate time taken microseconds complete ransfer operation l bytes ae parallel code implement manytomanymulticast primitive repeated calls onetomany multicast primitive hence estimates quality measures former functions expressed terms latter one onetomanymulticast operation sending message p processors assumed pe times expensive transfer operation message size time taken execute double precision floating point add multiply operation taken approximately 5 microseconds floating point division assumed twice expensive simple assignment load store onetenth expensive overhead making arithmetic function call five times much timing overhead associated various control instructions ignored study apart use parafrase2 indicate loops parallelizable steps approach simulated hand used study opportunity gain insight data decomposition problem examine feasibility approach results large majority loops parafrase2 able generate enough information enable appropriate formulation constraints determination quality measures approach loops information parallelization adequate based examination programs identified following techniques underlying parallelizing compiler used implementing approach needs augmented ffl sophisticated interprocedural analysis need constant propagation across procedures 3 cases need additional reference information variables procedure inline expansion procedure ffl extension idea scalar expansion namely expansion small arrays essential get benefits approach like scalar variables also treat small arrays replicated processors helps removal antidependence outputdependence loops number cases often saves compiler getting fooled parallelizing smaller loops involving arrays expense leaving bigger parallelizable loops sequential ffl recognition reduction operators loop operator may get parallelized appro priately examples addition min max operators since none features beyond capabilities parafrase2 gets fully developed matter stateoftheart parallelizing compiler assume remaining part study capabilities present parallelizing compiler supporting implementation present distribution schemes various arrays arrive applying strategy programs various constraints associated quality measures recorded table 2 summarizes final distribution significant arrays programs use following informal notation description array indicate number elements along dimension specify dimension distributed cycliccontiguousreplicated also indicate number processors dimension distributed special case number one indicate dimension sequentialized example first entry table shows 2d arrays z consisting 512 x 512 elements distributed cyclically rows processors choose tred2 routine illustrate steps strategy greater detail since small yet reasonably complex program defies easy determination right data partitioning scheme simple inspection remaining programs explain basis certain important decisions related formulation constraints array distributions taken sometimes help sample program segments tred2 program show effectiveness strategy actual results performance different versions parallel program implemented ipsc2 using different data partitioning strategies tred2 source code tred2 listed figure 2 along code listing shown probabilities taking branch various conditional go statements probabilities assumed supplied compiler also corresponding statement loop imposes constraints indicated four categories described section 3 constraint belongs based alignment constraints component affinity graph cag shown figure 3 constructed program node cag 15 represents array dimension weight edge denotes communication cost incurred array dimensions represented two nodes corresponding edge aligned edge weights cag follows onetomanymulticastnn hn j line 83 along term indicated line number program constraint corresponding quality measure may traced total number processors denoted n n n j refer number processors along various array dimensions initially assumed distributed applying algorithm component alignment 15 graph get following classes dimensions class 1 consisting 1 consisting 2 z 2 classes get mapped dimensions 1 2 respectively processor grid step 2 strategy none array dimensions sequentialized parallelization constraints favoring distribution dimensions z 1 z 2 step 3 distribution functions array dimensions two classes determined cyclic numerous constraints dimension arrays z e favoring cyclic distribution block size distribution aligned array dimensions set one hence end step distributions various array dimensions moving step 4 determine value n 1 value n 2 simply gets fixed nn 1 adding together actual time measures given various constraints penalty measures various constraints getting satisfied get following expression execution time program part dependent n 1 c fact values n ranging 4 16 see expression execution time gets minimized value n 1 set n easy see since first term appearing boldface dominates expression vanishes n incidentally term comes quality measures various constraints sequentialize z 2 real processor grid therefore one dimension array dimensions second class get sequentialized hence distribution functions array dimensions end step since using processor grid one single dimension need second distribution function arrays e uniquely specify processors various elements arrays none array dimensions chosen replication step 5 specified formal definitions distribution functions data distribution scheme finally emerges distribute arrays z rows cyclic fashion distribute arrays e also cyclically n processors dgefa dgefa routine operates single n x n array factorized using gaussian elimination partial pivoting let n 1 n 2 denote respectively number processors rows columns array distributed loop computing maximum elements column pivot element one scaling elements column yield execution time terms show communication time part increasing computation time part decreasing due increased parallelism increase n 1 loop involving exchange rows due pivoting suggests constraint sequentialize 1 ie setting n 1 1 internalize communication corresponding exchange rows doublynested loop involving update array elements corresponding triangularization column shows parallelism potential communication parallelization done level nesting parallelizable loops nested inside single inherently sequential loop program number iterations execute varies directly value outer loop index hence loops impose constraints distribution along dimensions cyclic better load balance compiler needs know value n evaluate expression execution time n 1 unknown present results two cases 256 analysis shows first case compiler would come n 1 16 second case would 8 thus given information value n compiler would favor columncyclic distribution smaller values n gridcyclic distribution larger values n trfd trfd benchmark program goes series passes pass essentially involves setting data arrays making repeated calls loop particular subroutine apply approach get distribution arrays used subroutine nine arrays get used shown table 3 actually aliases give flavor distributions get arrived show program segments 70 70 continue first loop leads following constraints alignment xrsiq 1 v 2 sequentialization xrsiq 2 v 1 second loop advocates alignment xij 1 v 2 sequentialization v 1 cyclic distribution xij 1 since number iterations inner loop modifying xij varies value outer loop index constraint cyclic distribution gets passed xij 1 v 2 v 2 xrsiq 1 constraints together imply columncyclic distribution v rowcyclic distribution xrsiq cyclic distribution xij similarly appropriate distribution schemes determined arrays xijks xkl references involving arrays xrspq xijrs xrsij xijkl somewhat complex variation subscripts many references cannot analyzed compiler distribution arrays specified contiguous reduce certain communication costs involving broadcast values arrays mdg program uses two important arrays var vm number small arrays replicated according strategy array vm divided three parts get used different arrays named xm ym zm various subroutines similarly array var gets partitioned twelve parts appear different subroutines different names table 3 show distributions individual smaller arrays arrays fx fy fz correspond two different parts var different invocations subroutine interf program numerous parallelizable loops three distinct contiguous elements array corresponding var get accessed together iteration lead constraints distributions arrays use block size multiple three doubly nested loops subroutines interf poteng operating arrays number iterations inner loop varying directly value outer loop index seen earlier programs leads constraints arrays cyclically distributed combined previous constraints get distribution scheme arrays partitioned blocks three elements distributed cyclically show parts program segment using slightly changed syntax make code concise illustrates relationship distributions parts var x parts vm 1000 1000 continue loop variables iwo iw1 iw2 get recognized induction variables expressed terms loop index references loop establish correspondence element xm threeelement block x yield similar relationships involving arrays ym zm hence arrays xm ym zm given cyclic distribution completely cyclic distribution block size one flo52 program computations involving number significant arrays shown table 3 many arrays declared main program really represent collection smaller arrays different sizes referenced different steps program name instance array w declared big 1d array main program different parts get supplied subroutines euler parameter formal parameter 3d array w different steps program cases always refer smaller arrays passed various subroutines describing distribution arrays also size array w varies different steps program entry size array table indicates largest array number parallelizable loops referencing 3d arrays w x access elements varying along third dimension array together single assignment statement hence third dimension arrays sequentialized numerous parallelizable loops establish constraints 2d arrays listed table identical distributions moreover two dimensions array aligned first two dimensions listed 3d arrays dictated reference patterns several loops interesting issues illustrated following program segments 38 continue loops impose constraints first two dimensions arrays contiguous rather cyclic distributions communications involving p values occur across boundaries regions arrays allocated various processors let n 1 n 2 denote number processors first second dimensions distributed first part program segment specifies constraint sequentialize p 1 second one gives constraint sequentialize p 2 quality measures constraints give terms communication costs vanish n 1 n 2 respectively set one order choose actual values n 1 n 2 given n 1 compiler evaluate expression execution time different values n 1 n 2 requires know values array bounds specified program segment variables il jl since values actually depend user input compiler would assume real array bounds given array declarations based simplified analysis expect compiler finally come given accurate information different assumptions array bounds values chosen n 1 n 2 may different distributions 1d arrays indicated table get determined appropriately cases based considerations alignment array dimensions others due contiguous distribution processors default mode distribution experimental results tred2 program implementation show results performance different versions parallel tred2 program implemented ipsc2 using different data partitioning strategies data distribution scheme selected strategy shown table 2 distribute arrays z rows cyclic fashion distribute array e also cyclic manner n processors starting sequential program wrote target host node programs ipsc2 hand using scheme suggested parallelizing compiler 4 22 handoptimized code first implemented version uses data distribution scheme suggested strategy ie row cyclic alternate scheme also looks reasonable looking various constraints one distributes arrays z columns instead rows get idea gains made performance sequentializing class dimensions ie distributing z blocked gridlike manner also gains made choosing cyclic rather contiguous distribution arrays implemented two versions program versions correspond bad choices data distribution user might make careful enough programs run two different data sizes corresponding values 256 512 n plots performance various versions program shown figure 4 sequential time program shown case since program could run single node due memory limitations data partitioning scheme suggested strategy performs much better schemes data size shown figure 7 b smaller data size figure 7 scheme using column distribution arrays works slightly better fewer processors used approach identify number constraints favor column distribution scheme get outweighed constraints favor rowwise distribution arrays regarding issues strategy clearly advocates use cyclic distribution rather contiguous also sequentialization class dimensions suggested numerous constraints sequentialize various array dimensions fact observations indeed crucial seen poor performance program corresponding contiguous rowwise z distribution arrays also one corresponding blocked gridlike distribution arrays z results show program approach able take right decisions regarding certain key parameters data distribution suggest data partitioning scheme leads good performance conclusions presented new approach constraintbased approach problem determining suitable data partitions program approach quite general applied large class programs references analyzed compile time demonstrated effectiveness approach reallife scientific application programs feel major contributions problem automatic data partitioning ffl analysis entire program look data distribution perspective performance entire program individual program segments notion constraints makes easier capture requirements imposed different parts program overall data distribution since constraints associated loop specify basic minimal requirements data distribution often able combine constraints affecting different parameters relating distribution array studies numeric programs confirm situations combining possible arise frequently real programs ffl balance parallelization communication considerations take account communication costs parallelization considerations overall execution time reduced ffl variety distribution functions relationships distributions formulation distribution functions allows rich variety possible distribution schemes array idea relationship array distributions allows constraints formulated one array influence distribution arrays desirable manner approach data partitioning limitations guarantee optimality results obtained following strategy given problem nphard procedure compiletime determination quality measures constraints based number simplifying assumptions instance assume loop bounds probabilities executing various branches conditional statement known compiler expect user supply information interactively form assertions future plan use profiling supply compiler information regarding frequently various basic blocks code executed mentioned earlier process implementing approach intel ipsc2 hypercube using parafrase2 restructurer underlying system also exploring number possible extensions approach important issue looked data reorganization programs might desirable partition data one way particular program segment repartition moving next program segment also plan look problem interprocedural analysis formulation constraints may done across procedure calls finally examining better estimates could obtained quality measures various constraints presence compiler optimizations like overlap communication computation elimination redundant messages via liveness analysis array variables 9 importance problem data partitioning bound continue growing machines larger number processors keep getting built number issues need resolved research truly automated high quality system built data partitioning multicomputers however believe ideas presented paper lay effective framework solving problem acknowledgements wish express sincere thanks prof constantine polychronopoulos giving us access source code parafrase2 system allowing us build system top also wish thank referees valuable suggestions r interactive environment data partitioning distribution static performance estimator guide data partitioning decisions interprocedural constant propagation compiling programs distributedmemory multiprocessors compiling parallel programs optimizing performance perfect club automatic data partitioning distributed memory multiprocessors compiler support machineindependent parallel programming fortran message passing coprocessor distributed memory multicomputers compiler techniques data partitioning sequentially iterated parallel loops programming parallelism compiler transformations nonshared memory machines generating explicit communication sharedmemory program references index domain alignment minimizing cost crossreferencing distributed arrays memory storage patterns parallel processing methodology parallelizing programs multicomputers complex memory multiprocessors process decomposition locality reference dino parallel programming language approach compiling singlepoint iterative programs distributed memory com puters superb tool semiautomatic mimdsimd parallelization tr programming parallelism memory storage patterns parallel processing process decomposition locality reference methodology parallelizing programs multicomputers complex memory multiprocessors static performance estimator guide data partitioning decisions message passing coprocessor distributed memory multicomputers generating explicit communication sharedmemory program references compiler techniques data partitioning sequentially iterated parallel loops ctr kueiping shih jangping sheu chuahuang huang statementlevel communicationfree partitioning techniques parallelizing compilers journal supercomputing v15 n3 p243269 mar12000 rohit chandra dingkai chen robert cox dror e maydan nenad nedeljkovic jennifer anderson data distribution support distributed shared memory multiprocessors acm sigplan notices v32 n5 p334345 may 1997 tom bennet distributed message routing runtime support messagepassing parallel programs derived ordinary programs proceedings 1994 acm symposium applied computing p510514 march 0608 1994 phoenix arizona united states manish gupta edith schonberg static analysis reduce synchronization costs dataparallel programs proceedings 23rd acm sigplansigact symposium principles programming languages p322332 january 2124 1996 st petersburg beach florida united states zaafrani r ito partitioning global space distributed memory systems proceedings 1993 acmieee conference supercomputing p327336 december 1993 portland oregon united states zaafrani mabo robert ito efficient execution doacross loops distributed memory systems proceedings ifip wg103 working conference architectures compilation techniques fine medium grain parallelism p2738 january 2022 1993 chen j p sheu communicationfree data allocation techniques parallelizing compilers multicomputers ieee transactions parallel distributed systems v5 n9 p924938 september 1994 ernesto su daniel j palermo prithviraj banerjee processor tagged descriptors data structure compiling distributedmemory multicomputers proceedings ifip wg103 working conference parallel architectures compilation techniques p123132 august 2426 1994 ram subramanian santosh pande framework performancebased program partitioning progress computer research nova science publishers inc commack ny 2001 ram subramanian santosh pande framework performancebased program partitioning progress computer research nova science publishers inc commack ny 2001 manish gupta prithviraj banerjee paradigm compiler automatic data distribution multicomputers proceedings 7th international conference supercomputing p8796 july 1923 1993 tokyo japan manish gupta prithviraj banerjee methodology highlevel synthesis communication multicomputers proceedings 6th international conference supercomputing p357367 july 1924 1992 washington c united states tatsuya shindo hidetoshi iwashita shaun kaneshiro tsunehisa doi junichi hagiwara twisted data layout proceedings 8th international conference supercomputing p374381 july 1115 1994 manchester england chihzong lin chienchao tseng yilin chen tsowei kuo systematic approach synthesize data alignment directives distributed memory machines nordic journal computing v3 n2 p89110 summer 1996 skewed data partition alignment techniques compiling programs distributed memory multicomputers journal supercomputing v21 n2 p191211 february 2002 data parallel programming numa multiprocessors usenix systems usenix experiences distributed multiprocessor systems p1313 september 2223 1993 san diego california zaafrani r ito expressing crossloop dependencies hyperplane data dependence analysis proceedings 1994 acmieee conference supercomputing november 1418 1994 washington dc kandemir 2d data locality definition abstraction application proceedings 2005 ieeeacm international conference computeraided design p275278 november 0610 2005 san jose ca zaafrani r ito expressing crossloop dependencies hyperplane data dependence analysis proceedings 1994 conference supercomputing p508517 december 1994 washington dc united states paul feautrier toward automatic partitioning arrays distributed memory computers proceedings 7th international conference supercomputing p175184 july 1923 1993 tokyo japan kandemir j ramanujam choudhary p banerjee layoutconscious iteration space transformation technique ieee transactions computers v50 n12 p13211336 december 2001 niclas andersson peter fritzson generating parallel code object oriented mathematical models acm sigplan notices v30 n8 p4857 aug 1995 gwanhwan hwang chengwei chen jenq kuen lee roy dzching ju segmented alignment enhanced model align data parallel programs hpf journal supercomputing v25 n1 p1741 may anant agarwal david kranz venkat natarajan automatic partitioning parallel loops data arrays distributed sharedmemory multiprocessors ieee transactions parallel distributed systems v6 n9 p943962 september 1995 dean engelhardt andrew wendelborn partitioningindependent paradigm nested data parallelism proceedings ifip wg103 working conference parallel architectures compilation techniques p224233 june 2729 1995 limassol cyprus byoungro mary w hall heidi e ziegler custom data layout memory parallelism proceedings international symposium code generation optimization feedbackdirected runtime optimization p291 march 2024 2004 palo alto california chauwen tseng jennifer anderson saman p amarasinghe monica lam unified compilation techniques shared distributed address space machines proceedings 9th international conference supercomputing p6776 july 0307 1995 barcelona spain edouard bugnion jennifer anderson todd c mowry mendel rosenblum monica lam compilerdirected page coloring multiprocessors acm sigplan notices v31 n9 p244255 sept 1996 waimee ching alex katz experimental apl compiler distributed memory parallel machine proceedings 1994 conference supercomputing p5968 december 1994 washington dc united states wai mee ching alex katz experimental apl compiler distributed memory parallel machine proceedings 1994 acmieee conference supercomputing november 1418 1994 washington dc peizong lee efficient algorithms data distribution distributed memory parallel computers ieee transactions parallel distributed systems v8 n8 p825839 august 1997 jennifer anderson monica lam global optimizations parallelism locality scalable parallel machines acm sigplan notices v28 n6 p112125 june 1993 vikram adve alan carle elana granston seema hiranandani ken kennedy charles koelbel ulrich kremer john mellorcrummey scott warren chauwen tseng requirements dataparallel programming environments ieee parallel distributed technology systems technology v2 n3 p4858 september 1994 david garzasalazar wim bhm reducing communication honoring multiple alignments proceedings 9th international conference supercomputing p8796 july 0307 1995 barcelona spain mahmut kandemir alok choudhary nagaraj shenoy prithviraj banerjee j ramanujam linear algebra framework automatic determination optimal data layouts ieee transactions parallel distributed systems v10 n2 p115135 february 1999 chauwen tseng compiler optimizations eliminating barrier synchronization acm sigplan notices v30 n8 p144155 aug 1995 mario nakazawa david k lowenthal wendou zhou execution model heterogeneous clusters proceedings 2005 acmieee conference supercomputing p7 november 1218 2005 john plevyak vijay karamcheti xingbin zhang andrew chien hybrid execution model finegrained languages distributed memory multicomputers proceedings 1995 acmieee conference supercomputing cdrom p41es december 0408 1995 san diego california united states mahmut kandemir alok choudhary j ramanujam meenakshi kandaswamy unified framework optimizing locality parallelism communication outofcore computations ieee transactions parallel distributed systems v11 n7 p648668 july 2000 mahmut taylan kandemir compiler technique improving wholeprogram locality acm sigplan notices v36 n3 p179192 march 2001 jennifer anderson saman p amarasinghe monica lam data computation transformations multiprocessors acm sigplan notices v30 n8 p166178 aug 1995 micha cierniak wei li unifying data control transformations distributed sharedmemory machines acm sigplan notices v30 n6 p205217 june 1995 ismail kadayif mahmut kandemir quasidynamic layout optimizations improving data locality ieee transactions parallel distributed systems v15 n11 p9961011 november 2004 mahmut taylan kandemir improving wholeprogram locality using intraprocedural interprocedural transformations journal parallel distributed computing v65 n5 p564582 may 2005 pedro c diniz martin c rinard dynamic feedback effective technique adaptive computing acm sigplan notices v32 n5 p7184 may 1997 peizong lee zvi meir kedem automatic data computation decomposition distributed memory parallel computers acm transactions programming languages systems toplas v24 n1 p150 january 2002 mahmut kandemir alok choudhary prithviraj banerjee j ramanujam nagaraj shenoy minimizing data synchronization costs oneway communication ieee transactions parallel distributed systems v11 n12 p12321251 december 2000 mahmut kandemir prithviraj banerjee alok choudhary j ramanujam eduard ayguad static dynamic locality optimizations using integer linear programming ieee transactions parallel distributed systems v12 n9 p922941 september 2001 pascal fradet julien mallet compilation specialized functional language massively parallel computers journal functional programming v10 n6 p561605 november 2000 kandemir p banerjee choudhary j ramanujam n shenoy global communication optimization technique based dataflow analysis linear algebra acm transactions programming languages systems toplas v21 n6 p12511297 nov 1999 mahmut kandemir compilerdirected collectiveio ieee transactions parallel distributed systems v12 n12 p13181331 december 2001 mahmut kendemir j ramanujam data relation vectors new abstraction data optimizations ieee transactions computers v50 n8 p798810 august 2001 pedro c diniz martin c rinard eliminating synchronization overhead automatically parallelized programs using dynamic feedback acm transactions computer systems tocs v17 n2 p89132 may 1999 akimasa yoshida kenichi koshizuka hironori kasahara datalocalization fortran macrodataflow computation using partial static task assignment proceedings 10th international conference supercomputing p6168 may 2528 1996 philadelphia pennsylvania united states p banerjee peercy design evaluation hardware strategies reconfiguring hypercubes meshes faults ieee transactions computers v43 n7 p841848 july 1994 henri e bal frans kaashoek object distribution orca using compiletime runtime techniques acm sigplan notices v28 n10 p162177 oct 1 1993 ken kennedy ulrich kremer automatic data layout distributedmemory machines acm transactions programming languages systems toplas v20 n4 p869916 july 1998