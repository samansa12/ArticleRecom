overcoming myopia inductive learning algorithms relieff current inductive machine learning algorithms typically use greedy search limited lookahead prevents detect significant conditional dependencies attributes describe training objects instead myopic impurity functions lookahead propose use relieff extension relief developed kira rendell lsqb10 11rsqb heuristic guidance inductive learning algorithms reimplemented assistant system top induction decision trees using relieff estimator attributes selection step algorithm tested several artificial several real world problems results compared well known machine learning algorithms excellent results artificial data sets two real world problems show advantage presented approach inductive learning b introduction inductive learning algorithms typically use greedy search strategy overcome combinatorial explosion search good hy potheses heuristic function estimates potential successors current state search space major role greedy search current inductive learning algorithms use variants impurity functions like information gain gain ratio25 giniindex1 distance ever measures assume attributes conditionally independent given class therefore domains strong conditional dependencies attributes greedy search poor chances revealing good hypothesis kira rendell 10 11 developed algorithm called seems powerful estimating quality attributes example parity problems various degrees significant number irrelevant ran dom additional attributes relief able correctly estimate relevance attributes time proportional number attributes square number training instances reduced limiting number iterations relief original relief deal discrete continuous tributes deal incomplete data limited twoclass problems developed extension relief called relieff improves original algorithm estimating probabilities reliably extends handle incomplete multiclass data sets complexity remains relieff seems promising heuristic function may overcome myopia current inductive learning algorithms kira rendell used relief preprocessor eliminate irrelevant attributes data description learn ing relieff general relatively efficient reliable enough guide search learning process paper reimplementation assistant learning algorithm top induction decision trees 4 described named assistant r instead information gain assistantr uses relieff heuristic function estimating attributes quality step tree generation experiments series artificial realworld data sets described results obtained using relieff selection criterion compared results ap proaches following approaches compared ffl use information gain selection criterion ffl lfc 27 28 tries overcome myopia information gain limited lookahead ffl naive bayesian classifier assumes conditional independence attributes ffl knearest neighbors algorithm paper organized follows next section original relief briefly described along interpretation extended version relieff section 3 present reimplementation assistant called assistantr section 41 briefly describe algorithms used experiments section 42 describe experimental methodology section 5 describes experiments compare results different algorithms show assistantr performs least well assistant sometimes much better conclusion potential breakthroughs discussed basis excellent results artificial data sets finally integration compared algorithms proposed 2 relieff 21 relief key idea relief estimate attributes according well values distinguish among instances near purpose given instance relief 1 set weights wa 00 2 1 n 3 begin 4 randomly select instance r 5 find nearest hit h nearest miss 6 1 attributes 7 wa wa diffarhn 8 9 end figure 1 basic algorithm relief searches two nearest neighbors one class called nearest hit different class called nearest miss original algorithm relief 10 11 randomly selects n training instances n userdefined parameter algorithm given figure 1 function diffattributeinstance1instance2 calculates difference values attribute two instances discrete attributes difference either 1 values different 0 values equal continuous attributes difference actual difference normalized interval 0 1 normalization n guarantees weights w interval gamma1 1 however normalization n unnecessary step w used relative comparison among attributes weights estimates quality attributes rationale formula updating weights good attribute value instances class subtracting difference differentiate instances different classes adding difference function diff used also calculating distance instances find nearest neighbors total distance simply sum differences attributes fact original relief uses squared difference discrete attributes equivalent diff experiments significant difference results using diff squared differ ence n number training instances complexity algorithm theta n theta attributes 22 interpretation reliefs estimates following derivation shows reliefs estimates strongly related impurity functions obvious reliefs estimate w attribute approximation following difference probabilities different value aj nearest instance different class gammap different value aj nearest instance class 1 eliminate 1 requirement selected instance nearest formula becomes different value ajdifferent class gammap different value ajsame class 2 rewrite equal value classjequal value obtain using bayes rule sampling replacement strict sense following equalities hold using equalities obtain const theta highly correlated giniindex gain 1 classes c values v attribute difference instead factor giniindex gain uses equation 3 shows strong relation liefs weights giniindex gain probability two instances value attribute eq 3 kind normalization factor multivalued tributes impurity functions tend overestimate multivalued attributes various normalization heuristics needed avoid tendency eg gain ratio 25 distance measure 16 binarization attributes 4 equation 3 shows relief exhibits implicit normalization effect another deficiency giniindex gain values tend decrease increasing number classes 14 denominator constant factor equation 3 given attribute serves kind normalization therefore reliefs estimates exhibit strange behavior giniindex gain derivation eliminated nearest instance condition probabilities put back interpret reliefs estimates average local estimates smaller parts instance space enables relief take account context attributes ie conditional dependencies attributes given class value detected context locality global point view dependencies hidden due effect averaging training instances exactly makes impurity functions myopic impurity functions use correlation attribute class disregarding context attributes using global point view disregarding local peculiarities example data set given table 1 illustrates difference myopic estimation functions relief three attributes eight training instances class value determined xor function attributes a1 a2 third attribute a3 randomly generated relief equation 1 correctly estimates attributes a1 a2 important contribution attribute a3 poor hand wa equation 3 table example data set estimated quality attributes function a1 a2 a3 class information gain 9 0000 0000 0049 gainratio 25 0000 0000 0051 distance 16 0000 0000 0026 ginigain equation 4 original giniindex gain 1 information gain 9 gain ratio 25 distance measure 16 estimate contribution a3 highest attributes a1 a2 estimated completely irrelevant hong 8 developed procedure similar relief estimating quality attributes directly emphasizes use contextual information difference relief approach uses information nearest misses ignores nearest hits besides hong uses normalization penalize contribution nearest misses far away given instance 23 extensions relief original relief deal discrete continuous attributes however deal incomplete data limited twoclass problems equation 1 crucial importance extensions relief turned extensions relief straightforward unless realized relief fact approximates probabilities extensions designed way probabilities reliably approximated developed extension relief called relieff improves original algorithm estimating probabilities reliably extends deal incomplete multiclass data sets brief description extensions follows reliable probability approxima tion parameter n algorithm lief described section 21 represents number instances approximating probabilities eq 1 larger n implies reliable approximation obvious choice adopted relieff relatively small number training instances one thousand run outer loop relief available training instances selection nearest neighbors crucial importance relief purpose find nearest neighbors respect important attributes redundant noisy attributes may strongly affect selection nearest neighbors therefore estimation probabilities noisy data becomes unreliable increase reliability probability approximation relieff searches k nearest hitsmisses instead one near hitmiss averages contribution k nearest hitsmisses shown extension significantly improves reliability estimates attributes qualities13 overcome problem parameter tuning experiments k set 10 empirically gives satisfactory results problems significantly better results obtained tuning typical majority machine learning algorithms incomplete data enable relief deal incomplete data sets function diffattributeinstance1 instance2 relieff extended missing values attributes calculating probability two given instances different values given attribute ffl one instance eg i1 unknown value ffl instances unknown value conditional probabilities approximated relative frequencies training set nearest neighbors correlation coefficient independent atts parity problems figure 2 correlation relieffs estimates intended quality attributes data sets conditionally independent strongly dependent attributes approach assumes conditional probabilities attributevalues given class applicable without context tribute may cases naive however including context atributes far inefficient multiclass problems kira rendell relief used estimate attributes qualities data sets two classes splitting problem series 2class problems solution seems unsatisfactory section 41 discuss performance approach compare extension described use prac tice relief able deal multiclass problems without prior changes knowledge representation could affect final outcomes instead finding one near miss different class relieff searches k near misses different class c averages contribution updating estimate w average weighted prior probability class idea algorithm estimate ability attributes separate pair classes regardless two classes closest normalization prior probabilities classes necessary k near misses different class would tend exaggerate influence classes small number cases note time complexity relieff 2 theta attributes n number training instances 24 relieffs estimates attributes qualit estimate contribution parameter k nearest hitsmisses relieffs estimates attributes quality kononenko 13 compared intended information gain attributes estimates generated relieff calculating standard linear correlation coefficient correlation coefficient show intended quality estimated quality attributes related typical graph data sets conditionally independent attributes strongly dependent attributes parity problems various de grees shown figure 2 conditionally independent attributes quality estimate monotonically increases number nearest neighbors conditionaly dependent tributes quality increases maximum later decreases number nearest neighbors exceeds number instances belong peak distribution space given class note attributes evaluated myopic impurity functions like giniindex information gain quality estimates would high conditionally independent attributes poor strongly dependent attributes corresponds estimates relieff large number nearest hitsmisses test effect normalization factor eq 3 run relieff also one well known medical data set primary tumor described 6 authors section 53 major difference estimates impurity functions estimates relieff primary tumor problem estimates two significant attributes information gain giniindex overestimate one attribute 3 values opinion physicians specialists relieff normalized versions impurity functions correctly estimate attribute less important 3 assistantr assistantr reimplementation assistant learning system top induction decision trees4 basic algorithm goes back cls concept learning system developed hunt et al 9 reimplemented several authors see 25 overview following describe main features assistant binarization attributes algorithm generates binary decision trees decision step binarized version attribute selected maximizes information gain attribute continuous attributes decision point selected maximizes tributes information gain discrete attributes heuristic greedy algorithm used find locally best split attributes values two subsets purpose binarization reduce replication problem strengthen statistical support generated rules decision tree pruning prepruning postpruning techniques used pruning unreliable parts decision trees preprun ing three userdefined thresholds provided minimal number training instances minimal attributes information gain maximal probability majority class current node postpruning method developed niblett bratko 22 used uses laplaces law succession estimating expected classification error current node commited pruningnot pruning subtree incomplete data handling learning training instances missing value selected attribute weighted probabilities attributes value conditioned class label classification instances missing values weighted unconditional probabilities attributes values naive bayesian classifier internal node decision tree eventually third successor appears labeled attributes values training instances available null leaves naive bayesian formula used calculate probability distribution leaf using attributes appear path root leaf note calculation done offline ie learning phase classification null leaves already labeled calculated class probability distribution used classification manner ordinary leaves main difference assistant reimplementation assistantr reli eff used attribute selection addi tion wherever appropriate instead relative frequency assistantr uses mestimate probabilities shown often significantly increase performance machine learning algorithms2 3 prior probabilities laplaces law succession used possible outcomes n number trials n x number trials outcome x prior probabilities used mestimate conditional probabilities parameter trades contributions relative frequency prior probability experiments parameter set setting usually used default em pirically gives satisfactory results 2 3 although tuning problem domains better results may expected mestimate used naive bayesian formula 5 postpruning instead laplaces law succession proposed cestnik bratko3 relieffs estimates probabilities eq 1 use probabilities root tree estimate prior probabilities lower internal node nt corresponding training instances ajnearest miss ajnearest miss root ajnearest hit ajnearest hit root 4 experimental environment 41 algorithms comparison performed series experiments assistantr compared performance following algorithms assistanti variant assistantr instead relieff uses information gain selection criterion assistant ever differences assistant remain mestimate probabilities algorithm enables us evaluate contribution lieff parameters assistanti assistantr fixed throughout experiments prepruning postpruning 2 lfc ragavan et al 27 28 use limited lookahead lfc lookahead feature con struction algorithm top induction decision trees detect significant conditional dependencies attributes constructive induction show interesting results data sets reimplemented algorithm 29 tested performance results presented paper show drawbacks experimental comparison described ragavan rendell confirm advantage limited lookahead constructive induction lfc generates binary decision trees node algorithm constructs new binary attributes original attributes using logical operators conjunction disjunction negation constructed binary tributes best attribute selected process recursively repeated two subsets training instances corresponding two values selected attribute constructive induction limited lookahead used space possible useful constructs stricted due geometrical representation conditional entropy estimator attributes quality reduce search space algorithm also limits breadth depth search lfc uses lookahead less myopic greedy algorithm assistant comparison results may show performance greedy search combination reli eff versus lookahead strategy make results comparable assistantr equipped lfc pruning probability estimation facilities described section 3 tests performed default set parameters depth lookahead 3 beam size 20 although domains better results may obtained parameter tuning however higher values parameters may combinatorially increase search space lfc makes algorithm impractical naive bayesian classifier classifier uses naive bayesian formula 5 calculate probability class given values attributes assuming conditional independence attributes new instance classified class maximal calculated probability mestimate probabilities used parameter set 2 experiments performance naive bayesian classifier serve estimate conditional independence attributes knn knearest neighbor algorithm given new instance algorithm searches nearest training instances classifies instance frequent class k instances knn algorithm distance measure used relieff see section 21 presented results obtained manhattandistance results using euclidian distance practically best results respect parameter k pre sented although fair comparison parameter tuning allowed training testing sets selected naive bayesian classifier knn algorithm comparison well known simple perform well many realworld problems performance two algorithms may show nature classification problems 42 experimental methodology experiment data set performed times randomly selecting 70 instances learning 30 testing results averaged system used subsets instances learning testing order provide experimental conditions verify significance differences used onetailed ttest confidence level null hypothesis stating difference zero5 differences results value statistic threshold considered significant exception methodology experiments finite element mesh design problem experimental methodology dictated previous published results described section 54 besides classification accuracy measured also average information score15 measure eliminates influence prior probabilities appropriately treats probabilistic answers classifier average information score defined testing instances testing instances information score classification ith testing instance defined class ith testing instance p cl prior probability class cl probability returned classifier returned probability correct class greater prior probability information score positive obtained information correct interpreted prior information necessary correct classification minus posterior information necessary correct classi fication returned probability correct class lower prior probability information score negative obtained information wrong interpreted prior information necessary incorrect classification minus posterior information necessary incorrect classification main difference classification accuracy information score illustrated following example let prior distribution classes p let posterior distribution returned classifier p correct class c 1 information score positive classification accuracy treats given posterior distribution wrong answer correct class c 2 information score negative classification accuracy treats given posterior distribution correct answer classification accuracy may special cases exhibit high variance information score much stable special case data set irrelevant attributes exactly 50 instances one class 50 instances class leave oneout testing probabilistic classifier would give approximate accuracy 50 thedefault classifier classifies every instance majority class accuracy would 0 slight modification distribution training instances would drastically change latter accuracy approximately 50 drastic modification distribution say 80 cases one class 20 would increase accuracy default classifier 80 accuracy probabilistic classifier would approximately 08 theta 0802 theta 02 68 however classifiers information score would scenarios remain approximately 0 bits would indicate classifiers unable extract useful information attributes 5 experimental results section give results several artificial realworld data sets presentation experiments divided four parts according four groups data sets artificial data sets controlled conditional dependency attributes benchmark artificial data sets medical data sets realworld data sets group give brief description data sets followed results results tables include averages several runs standard errors 51 artificial data sets generated several data sets order compare performance various algorithms inf1 domain three conditionally independent informative binary attributes three classes three random binary attributes learner detect three attributes informative relatively easy task five algorithms able solve problem inf2 domain obtained inf1 replacing informative attribute two attributes whose values define value original attribute xor relation prob lem learner detect six important attributes fact attributes pairwise strongly conditionally dependent fairly complex problem cannot solved myopic heuristics data set show advantage lfc assistantr tree domain whose instances generated decision tree 6 internal nodes containing different binary attribute 5 random binary attributes added description instances problem easy greedy decision tree learning algorithms approaches may difficulties due inappropriate knowledge representation target concept par2 parity problem two significant binary attributes 10 random binary tributes 5 randomly selected instances labeled wrong class problem hard lot attributes equal score evaluated myopic evaluation function information gain par3 par2 except three significant attributes parity relation makes problem harder par4 par2 except four significant attributes parity relation makes problem hardest among parity problems used experiments basic characteristics artificial data sets listed table 2 characteristics include percentage majority class interpreted default accuracy class entropy gives impression complexity classification problem results learning algorithms lfc assistanti assistantr well naive bayesian classifier knn algorithm given table 3 classification accuracy table information score results expected show ffl classifiers perform well relatively sim ple domain conditionally independent attributes ffl versions assistant perform well problem reconstruction decision tree tree classifiers significantly worse assistantr lfc able successfully solve problems strong conditional dependencies attributes inf2 par2 4 however two assistantr performs better especially case hardest problem par4 note lfc solve par4 depth lookahead increased table basic description artificial data sets domain class atts valatt instances majclass entropybit inf2 3 21 20 200 36 158 table 3 classification accuracy learning systems artificial data sets domain lfc assistanti assistantr naive bayes knn inf1 860sigma51 901sigma35 888sigma38 916sigma31 890sigma36 inf2 671sigma63 554sigma98 687sigma78 321sigma45 568sigma63 tree 758sigma54 792sigma57 788sigma62 690sigma59 682sigma53 936sigma33 749sigma79 957sigma28 567sigma57 794sigma43 however time complexity lookahead increases exponentially depth hand assistantr solves parity problems equally quickly ffl information score naive bayesian classifier problems strong conditional dependencies attributes poor indicates classifier failed find regularity data sets 52 benchmark artificial data sets besides artificial data sets previous subsection used also following benchmark artificial data sets used authors note results authors directly compared results experimental conditions trainingtesting splits bool boolean function defined 6 attributes 10 class noise optimal recognition rate 90 target function data set used smyth et al 31and report 672sigma17 classification accuracy naive bayes 825sigma11 back propagation 859sigma09 rulebased classifier led leddigits problem 10 noise attribute values optimal recognition rate estimated 74 smyth et al 31 report 681sigma17 classification accuracy naive bayes 646sigma35 backpropa gation 727sigma13 rulebased classi fier data set obtained irvine database21 krk1 problem legality kingrook king chess endgame positions attributes describe relevant relations pieces rank adjacent file originally data included five sets 1000 examples 1000 learning 4000 testing used test inductive logic programming algorithms7 reported classification accuracy 997sigma01 used one set 1000 examples ie 700 instances training krk2 krk1 except available attributes coordinates pieces data set used mladenic19 reported results 69 accuracy atris system 64 assistant basic description data sets provided table 5 results given tables 6 7 interesting led domain naive bayesian classifier knn algorithm reach estimated upper bound classification accuracy suggests attributes considered optimal classification domain problem attributes conditionally independent given class therefore good performance naive bayesian classifier surprising however three domains performance naive bayesian classifier poor due strong table 4 average information score learning systems artificial data sets domain lfc assistanti assistantr naive bayes knn table 5 basic description benchmark artificial data sets domain class atts valatt instances majclass entropybit table 6 classification accuracy learning systems artificial data sets domain lfc assistanti assistantr naive bayes knn led 708sigma23 711sigma24 717sigma22 739sigma21 739sigma21 krk1 987sigma12 986sigma12 986sigma12 916sigma14 922sigma19 krk2 860sigma21 666sigma31 701sigma33 648sigma21 707sigma17 table 7 average information score learning systems artificial data sets domain lfc assistanti assistantr naive bayes knn led 213sigma007 211sigma006 212sigma007 233sigma005 222sigma005 conditional dependencies attributes information score see table 7 shows naive bayesian classifier provides average information bool krk2 domains performance different variants assistant almost except krk2 domain performance assistanti poor note default accuracy krk2 67 performance assistantr knn algorithm significantly better 9995 confidence level however information score shows assistantr knn successful problem expected without constructive induction possible reveal regularities chess positions described coordinates pieces lfc able construct important attributes domain enables achieve significantly better results algorithms 53 medical data sets compared performance algorithms several medical data sets ffl data sets obtained university medical center ljubljana slovenia problem locating primary tumour patients metastases prim problem predicting recurrence breast cancer five years removal tumour brea problem determining type cancer lymphography lymp diagnosis rheumatology rheu ffl hepa prognostics survival patients suffering hepatitis data provided gail gong carnegiemellon university ffl data sets obtained statlog database18 diagnosis diabetes diab diagnosis heart diseases heart diab data set ragavan rendell 27report 788 classification accuracy lfc al gorithm also report poor performance table 8 basic description medical data sets domain class atts valatt instances majclass entropybit prim 22 17 22 339 25 389 table 9 classification accuracy learning systems medical data sets domain lfc assistanti assistantr naive bayes knn brea 761sigma43 768sigma46 785sigma39 787sigma45 795sigma27 lymp 824sigma52 770sigma55 770sigma59 847sigma42 826sigma57 hepa 790sigma53 772sigma53 823sigma54 861sigma39 826sigma49 heart 773sigma52 754sigma40 776sigma45 845sigma30 829sigma37 table average information score learning systems medical data sets domain lfc assistanti assistantr naive bayes knn several algorithms without constructive induction 58 however results see results statlog project 18 show poor results algorithms domain due lack constructive induction experiments diab dataset classifiers perform equally well exception naive bayesian classifier significantly better basic characteristics medical data sets given table 8 results experiments data sets provided tables 9 10 medical data sets attributes typically conditionally independent given class fore surprising naive bayesian classifier shows clear advantage data sets12 interesting performance knn algorithm good domains although worse performance naive bayesian classifier information score table 10 brea data set indicates learning algorithm able solve problem suggests attributes relevant versions assistant similar per formance except hepa domain assistantr significantly better performance confidence level detailed analysis showed problem relieff discovered significant conditional interdependency two attributes given class two attributes score poorly considered indepen dently assistanti able discover regularity data hand attributes available contain similar information two attributes together reason naive bayesian classifier performs better tried provide naive bayesian classifier additional attribute joining two conditionally dependent attributes ever performance remained achieved significantly better results two inductive algorithms lymp domain constructive induction seems useful however lfc performed significantly worse rheu domain domains three inductive algorithms perform equally well 54 nonmedical realworld data sets compared performance algorithms also following nonmedical real world data sets soyb iris vote obtained irvine database21 sat obtained statlog database 18 soyb famous soybean data set used iris well known fishers problem determining type iris flower mesh3mesh15 problem determining number elements edges object finite element mesh design problem6 five objects experts constructed appropriate meshes five experiments one object used testing four learning results averaged results reported dzeroski 7 various ilp systems 12 classification accuracy foil 22 mfoil 29 golem result reported pompe et al 23 28 sfoil description mesh problem appropriate ilp systems attribute learners relations arity 1 ie attributes used describe problem note domain trainingtesting splits algorithms testing methodology special case leaveoneout therefore results tables problem standard deviations quinlan 26 reports results ilp systems achieved 90 domain testing positive negative instances however results misleading positive instance ten negative instances average therefore 11 copies instance classification instance correct least 9 11 copies gives 82 classification accuracy classifier always classifies wrong class mesh3 contains three basic attributes original database ignores relational description objects therefore domain attribute learners given less information ilp learners contains besides 3 original tributes 12 attributes derived relational background knowledge prob lem attribute learners advantage already provided additional attributes provided description objects ilp learners actually informative princi ple attributes number additional attributes could derived extremely cleaver ilp learners relational description background knowledge ever fairly complex task therefore attribute learners mesh15 data set better chances ilp learners reveal good hypothesis sat database consists multiclass spectral values pixels 3 theta 3 neighborhoods satellite image classification central pixel neighborhood results statlog project18 906 classification accuracy knn algorithm 861 backpropagation 850 c45 848 cn2 693 naive bayesian classifier using relative frequencies mestimate probabilities vote voting records session 1984 united states congress smyth et al 31 report 889 classification accuracy naive bayesian classifier 930 backpropagation 949 rulebased classifier basic characteristics nonmedical real world data sets presented table 11 tables 12 13 give results soyb iris data sets classifiers perform equally well results naive bayesian classifier indicate attributes conditionally relatively independent data sets agreement previously published results sat data set knn significantly outperforms algorithms agreement results statlog project 18 ever naive bayesian classifier mestimate probabilities reaches classification accuracy inductive learning algorithms results naive bayesian classifier used 14 authors table basic description nonmedical realworld data sets domain class atts valatt instances majclass entropybit table classification accuracy learning systems nonmedical realworld data sets domain lfc assistanti assistantr naive bayes knn iris 950sigma38 957sigma37 952sigma26 966sigma26 970sigma21 table average information score learning systems nonmedical realworld data sets domain lfc assistanti assistantr naive bayes knn statlog project much worse cestnik 2 shown mestimate significantly increases performance naive bayesian classifier also confirmed experiments versions assistant perform data sets except sat data set assistantr lfc achieve significantly better result 9995 confidence level result confirms relieff estimates quality attributes better information gain vote data set naive bayesian classifier worst versions assistant comparable rule based classifier smyth et al 31 interesting results appear domains although attribute learners mesh3 less information ilp systems outperform results ilp systems reported dzeroski 7 pompe et al 23 12 additional attributes mesh15 results inductive learners significantly improved inductive learning systems significantly outperform naive bayesian classifier knn algorithm detailed analysis showed excellent result versions assistant due use naive bayesian formula calculate class probability distribution null leaves see section 3 namely problem often happens testing instances fall null leaf training instances values significant attributes testing instances naive bayesian classifier efficiently solves problem lfc generates null leaves constructed attributes strictly binary values true false therefore classification objects different value original attribute training instances always proceeds branch labeled false effect strategy given testing instance corresponding leaf contains training instances similar values attributes appear path root leaf strategy also works well mesh problems 6 discussion note null leaves versions assistant influence performance arti ficial data sets missing values data also mesh15 problem performance lfc good although generate null leaves therefore use null leaves crucial difference assistant lfc equation 3 shows interesting relation reliefs estimates impurity func tion relief efficiently estimate continuous discrete attributes implicit normalization eq 3 enables relief appropriately deal multivalued attributes however assistanti would use eq 3 instead information gain would still myopic ex ample par24 problems eq 3 would estimate attributes equally nonimportant therefore reason success assistantr nearest instances heuristic influences estimation probabil ities heuristic enables relief detect strong conditional dependencies attributes would overlooked estimates probabilities would done randomly selected instances instead nearest instances relieff efficient heuristic estimator attribute quality able deal data sets conditionally dependent independent tributes extensions relieff enable deal noisy incomplete multiclass data sets increasing number k nearest hitsmisses correlation relieffs estimates impurity functions also increases unless k greater number instances peak instance space study reported 14 showed relieff acceptable bias respect measures estimating attributes different number values myopia current inductive learning systems partially overcome replacing existing heuristic functions relieff assistantr variant top induction decision trees algorithms uses relieff estimating quality attributes significantly outperforms classifiers domains strong conditional dependencies tributes myopia inductive learners may cause overlook significant relations easily demonstrated artificial data sets also shown two real world problems hepa sat data sets relieff detected significant conditional interdependencies attributes resulted significantly better result assistantr result assistanti one feature relief addressed paper attribute replicated data set replications get estimate increasing number replications quality estimates descrease replicated attribute affects distances instances constructive induction lfc uses limited lookahead detect significant conditional dependencies attributes lfc shows similar advantage algorithms assistant r one artificial problem krk2 one real world problem lymp lfc performs significantly better due constructive induction however cases constructive induction may spoil results case rheu data set lfc performs well prob lems suggests limited lookahead good search strategy realworld prob lems lookahead however reasonable limit time complexity exponentialy increases lookahead depth although relieff may overcome myopia useless assistantr change representation required cases constructive induction applied example krk2 problem assistantr achieves good result improved without constructive induction good idea constructive induction may use relieff instead combination lookahead naive bayesian classifier obvious advantage domains conditionally relatively independent attributes medical diagnostic problems domains naive bayesian classifier able reliably estimate conditional probabilities also able use tributes ie available information would interesting appropriately combine power relieff naive bayesian classifier current ilp systems 20 able use attributes appropriately demonstrated mesh3 domain attribute learn ers outperformed existing ilp systems enable ilp systems deal attributevalue rep resentation combination semi naive bayesian classifier could useful hand current ilp systems use greedy search techniques heuristics guide search myopic pompe kononenko 24 implemented adapted version relieff foil like ilp system called ilpr prelemi nary experiments show similar advantages system ilp systems assistantr assistanti 7 conclusion relieff efficient heuristic estimator attribute quality able deal data sets conditionally dependent independent tributes noisy incomplete multiclass data sets myopia current inductive learning systems partially overcome replacing existing heuristic functions reli eff acceptable increase computational complexity may certain domains payoff eventual discovery strong conditional dependencies attributes cannot detected using myopic impurity measure guide greedy search experimental results indicate majority real world problems myopia marginal effect one may wonder whether myopia really worth much attention however faced new data set unreasonable try myopic algorithm unless know advance data set strong conditional dependencies attributes serious application machine learning new data try discover much regularities data pos sible therefore nonmyopic approaches one described paper used indispensable tools analysing data acknowledgements use mestimate equation 1 proposed bojan cestnik thank matjaz zwitter prim brea data sets milan soklic lymp gail gong hepa padhraic smyth bool led saso dzeroski krk1 mesh bob hen ery diab heart sat data sets statlog database strathclyde university patrick murphy david aha data sets irvine database grateful colleagues saso dzeroski matevz kovacic matjaz kukar uros pompe tanja urbancic anonymous reviewers comments earlier drafts significantly improved paper work supported slovenian ministry science technology r wadsworth international group estimating probabilities crucial task machine learning estimating probabilities tree pruning assistant 86 general statistics application inductive logic programming finite element mesh de sign handling noise inductive logic pro gramming use contextual information feature ranking discretization experiments duction practical approach feature selection feature selection prob lem traditional methods new algorithm inductive bayesian learning medical diagnosis biases estimating multivalued attributes information based evaluation criterion classifiers performance id3 revisited distance based criterion attribute selection learning told learning examples experimental comparison two methods knowledge acquisition context developing expert system soybean disease diagnosis combinatorial optimization inductive concept learning uci repository machine learning databases learning decision rules noisy domains linear space induction first order logic relieff induction decision trees minimum description length principle categorical theories lookahead feature construction learning hard concepts learning complex realworld conceptsthrough feature construction constructive induction decision trees rule induction using information theory tr ctr xin jin rongyan li xian shen rongfang bie automatic web pages categorization relieff hidden naive bayes proceedings 2007 acm symposium applied computing march 1115 2007 seoul korea use contextual information feature ranking discretization ieee transactions knowledge data engineering v9 n5 p718730 september 1997 marko robnikikonja igor kononenko theoretical empirical analysis relieff rrelieff machine learning v53 n12 p2369 octobernovember david bell hui wang formalism relevance application feature subset selection machine learning v41 n2 p175195 november 2000 llus mrquez llus padr horacio rodrguez machine learning approach pos tagging machine learning v39 n1 p5991 april 2000 saher esmeir shaul markovitch anytime learning decision trees journal machine learning research 8 p891933 512007 huan liu hiroshi motoda lei yu selective sampling approach active feature selection artificial intelligence v159 n12 p4974 november 2004 foster provost venkateswarlu kolluri survey methods scaling inductive algorithms data mining knowledge discovery v3 n2 p131169 june 1999