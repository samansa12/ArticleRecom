using userlevel memory thread correlation prefetching paper introduces idea using userlevel memory thread ulmt correlation prefetching approach user thread runs generalpurpose processor main memory either memory controller chip dram chip thread performs correlation prefetching software sending prefetched data l2 cache main processor approach requires minimal hardware beyond memory processor correlation table software data structure resides main memory main processor needs modifications l2 cache accept incoming prefetches addition approach wide usability effectively prefetch even irregular applications finally flexible prefetching algorithm customized user application basis simulation results show new design correlation table prefetching algorithm scheme delivers good results specifically nine mostlyirregular applications show average speedup 132 furthermore scheme works well combination conventional processorside sequential prefetcher case average speedup increases 146 finally exploiting customization prefetching algorithm increase average speedup 153 b introduction data prefetching popular technique tolerate long memory access latencies past work data prefetching focused processorside prefetching 6 7 8 12 13 14 15 19 20 23 25 26 28 29 approach processor engine cache hierarchy issues prefetch requests interesting alternative memoryside prefetching engine prefetches data processor main memory system 1 4 9 11 22 28 memoryside prefetching attractive several reasons first eliminates overheads state bookkeeping prefetch requests introduce paths main processor caches second supported modifications controller l2 cache modification main pro cessor third prefetcher exploit proximity memory advantage example storing state memory fi nally memoryside prefetching additional attraction riding technology trend increased chip integration indeed popular platforms like pcs equipped graphics engines memory system 27 chipsets like nvidias nforce even integrate powerful processor north bridge chip 22 simpler work supported part national science foundation grants ccr9970488 eia0081307 eia0072102 che0121357 darpa grant f3060201c0078 michigan state university gifts ibm intel hewlettpackard engines provided prefetching existing graphics processors augmented prefetching capabilities moreover proposals integrate processing logic dram chips iram 16 unfortunately existing proposals memoryside prefetching engines narrow scope 1 9 11 22 28 indeed designs hardware controllers perform simple specific operations 1 9 22 designs specialized engines customdesigned prefetch linked data structures 11 28 instead would like engine usable wide variety workloads offers flexibility use programmer memoryside prefetching support variety prefetching algorithms one type particularly suited correlation prefetching 1 6 12 18 26 correlation prefetching uses past sequences reference miss addresses predict prefetch future misses since program knowledge needed correlation prefetching easily moved memory side past correlation prefetching supported hardware controllers typically require large hardware table keep correlations 1 6 12 18 cases one controllers placed l1 l2 caches processor l1 effective approach high hardware cost fur thermore often unable prefetch far ahead enough deliver good prefetch coverage paper present new scheme correlation prefetching performed userlevel memory thread ulmt running simple generalpurpose processor memory processor either memory controller chip dram chip prefetches lines l2 cache main processor scheme requires minimal hardware support beyond memory pro cessor correlation table software data structure resides main memory main processor needs modifications l2 cache controller accept incoming prefetches moreover scheme wide usability effectively prefetch even irregular applications finally flexible prefetching algorithm executed ulmt customized programmer application basis using new design correlation table correlation prefetching algorithm scheme delivers average speedup 132 nine mostlyirregular applications furthermore scheme works well combination conventional processorside sequential prefetcher case average speedup increases 146 fi nally exploiting customization prefetching algorithm increase average speedup 153 paper organized follows section 2 discusses memoryside correlation prefetching section 3 presents ulmt correla possible locations memory processor chip north bridge memory cpu proc main mem proc mem main memory system 2 lookup 2 reply 3 prefetch j k 1 fetch b proc main mem proc mem main memory system 1 execute 2 fetch 3 prefetch 3 reply c figure 1 memoryside prefetching locations memory processor placed actions push passive b push active c prefetching tion prefetching section 4 discusses evaluation setup section 5 evaluates design section 6 discusses related work section 7 concludes 2 memoryside correlation prefetching 21 memoryside prefetching memoryside prefetching occurs prefetching initiated engine resides either close main memory beyond memory bus inside 1 4 9 11 22 28 manufacturers built engines typically simple hardwired controllers probably recognize simple stridebased sequences prefetch data local buffers examples nvidias dasp engine north bridge chip 22 intels prefetch cache i860 chipset paper propose support memoryside prefetching userlevel thread running generalpurpose core core simple need support floating point illustration purposes figure 1a shows memory system pc core placed different places north bridge memory controller chip dram chips placing north bridge simplifies design dram modi fied moreover existing systems already include core north bridge graphics processing 22 could potentially reused prefetching placing core dram chip complicates design resulting highlyintegrated system lower memory access latency higher memory bandwidth paper examine performance potential designs memory processorside prefetching push pull ondemand prefetching 28 respectively push prefetching occurs prefetched data sent cache processor requested pull prefetching opposite clearly memory prefetcher act pull prefetcher simply buffering prefetched data locally supplying processor demand 1 22 general however memoryside prefetching interesting performs push prefetching caches processor hide larger fraction memory access latency memoryside prefetching also classified passive ac tive passive prefetching memory processor observes requests main processor reach main memory based examining internal state memory processor prefetches data main processor expects latter need future figure 1b active prefetching memory processor runs abridged version code running main processor execution code induces memory processor fetch data main processor need later data fetched requests also sent main processor figure 1c paper concentrate passive push memoryside prefetching l2 cache main processor memory processor aims eliminate l2 cache misses since ones sees typically l2 cache miss time important contributor processor stall due memory accesses usually hardest hide outoforder execution approach prefetching inexpensive support main processor core need modified l2 cache needs following supports first systems 11 15 28 l2 cache accept lines memory quested l2 uses free miss status handling registers mshrs events secondly l2 pending request prefetched line address arrives prefetch simply steals mshr updates cache reply finally prefetched line arriving l2 dropped following cases l2 cache already copy line writeback queue copy line l2 cache trying write back memory mshrs busy lines set prefetched line wants go transactionpending state 22 correlation prefetching correlation prefetching uses past sequences reference miss addresses predict prefetch future misses 1 6 12 18 26 two popular correlation schemes stridebased pairbased schemes stridebased schemes find stride patterns address sequences prefetch addresses accessed patterns continue future pairbased schemes identify correlation pairs groups addresses example miss sequence successor misses typical implementation pairbased schemes uses correlation table record addresses correlated later miss observed addresses correlated address prefetched pairbased schemes attractive general appli cability work miss patterns long miss address sequences repeat behavior common regular irregular applications including sparse matrices linked data structures furthermore pairbased schemes like correlation schemes need neither compiler support changes application binary pairbased correlation prefetching studied using hardwarebased implementations 1 6 12 18 26 typically placing custom prefetch engine hardware correlation table processor l1 cache l1 l2 caches typical correlation table used 6 12 26 organized follows row stores tag address missed addresses set immediate successor misses misses seen immediately follow first one different points application parameters table maximum number immediate successors per miss numsucc maximum number misses table store predictions numrows associativity table assoc according 12 best performance entries row replace lru policy figure 4a illustrates algorithm works call algorithm base figure shows two snapshots table different points miss stream ii within row successors listed mru order left right time hardware keeps pointer row last miss observed miss occurs table learns placing miss address one immediate successors last miss new row allocated new miss unless already exists table used prefetch iii reacts observed miss finding corresponding row prefetching numsucc successors starting mru one designs 1 18 work slightly differently discussed section 6 past work demonstrated applicability pairbased correlation prefetching many applications however also revealed shortcomings approach one critical problem effective approach needs large table proposed schemes typically need 12 mbyte onchip sram table 12 18 applications large footprints even need 76 mbyte offchip sram table 18 furthermore popular schemes prefetch several potential immediate successors miss 6 12 26 two limitations prefetch far ahead intuitively need observe one miss eliminate another miss immediate successor result tend low coverage coverage number useful prefetches original number misses 12 3 ulmt correlation prefetching propose use ulmt eliminate shortcomings pair based correlation prefetching enhancing advantages following discuss main concept section 31 architecture system section 32 modified correlation prefetching algorithms section 33 related operating system issues sec tion 34 31 main concept ulmt running generalpurpose core memory performs two conceptually distinct operations learning prefetching learning involves observing misses main processors l2 cache recording correlation table one miss time prefetching operation involves reacting one miss looking correlation table triggering prefetching several memory lines l2 cache main processor action taken writeback memory practice agreement past work 12 find combining learning prefetching works best correlation table continuously learns new patterns uninterrupted prefetching delivers higher performance consequently ulmt executes infinite loop shown figure 2 initially thread waits miss observed observes one looks table generates addresses lines prefetch prefetching step updates table address observed miss learning step resumes waiting prefetch addresses occupancy time miss address observed generated response time table updated learning step prefetching step wait figure 2 infinite loop executed ulmt prefetch algorithm executed ulmt characterized response occupancy times response time time ulmt observes miss address generates addresses prefetch best performance response time small possible always execute prefetching step learning one moreover shift much computation possible prefetching learning step retaining critical operations prefetching step occupancy time time ulmt busy processing single observed miss ulmt implementation prefetcher viable occupancy time smaller time two consecutive l2 misses times correlation table ulmt reads writes simply software data structure memory consequently scheme eliminates costly hardware table required current implementations correlation prefetching 12 18 moreover accesses software table inexpensive memory processor transparently caches table cache finally new scheme enables redesign correlation table prefetching algorithms sec tion 33 address lowcoverage shortdistance prefetching limitations current implementations 32 architecture system figures 3a b show architecture system integrates memory processor north bridge chip dram chip respectively first design requires modification dram interface largely compatible conventional memory systems second design needs changes dram chips interface needs special support work typical memory systems multiple dram chips however since goal examine performance potential two designs abstract away implementation complexity second design assuming singlechip main memory following outline systems work discussion consider memory accesses resulting misses ignore writebacks simplicity affect algorithms figure 3a key communication occurs queues 1 2 3 miss requests main processor deposited queues simultaneously ulmt uses entries queue 2 build table based generate addresses prefetch latter deposited queue 3 queues 1 3 compete access memory although queue 3 lower priority 1 address line prefetch deposited queue 3 hardware compares entries queue 2 match address address x detected x removed queues remove x queue 3 redundant higherpriority bridge chip units north memory controller memory61interface buscache memory processor main processor filter interface bus dramfilter memory controller main processor2north bridge chip cache memory processor units b figure 3 architecture system integrates memory processor north bridge chip dram chip b request x already queue 1 x removed queue 2 save computation ulmt note unclear whether lost opportunity prefetch xs successors processing x reason algorithms prefetch several levels successor section 33 result xs successors may already queue 3 processing x may help improve state correlation table however minimizing total occupancy ulmt crucial scheme similarly mainprocessor miss deposited queues 1 2 hardware compares address queue 3 match request put queue 1 matching entry queue 3 removed possible requests main processor arrive fast ulmt consume queue 2 overflows case memory processor simply drops requests figure 3a also shows filter module associated queue 3 module improves performance correlation prefetching may sometimes try prefetch address several times short time filter module drops prefetch requests directed address recently issued another prefetch requests module fixedsized fifo list records addresses recentlyissued requests request issued queue 3 hardware checks filter list finds address request dropped list left unmodified otherwise address added tail list support unnecessary prefetch requests eliminated completeness figure shows queues replies memory main processor go queue 4 addition ulmt needs access software correlation table main mem ory recall table transparently cached memory processor logical queues 5 6 provide necessary paths memory processor access main memory practice queues 5 6 merged others memory processor dram chip figure 3b system works slightly differently miss requests main processor deposited first queue 1 queue 2 ulmt memory processor accesses correlation table cache miss directly dram addresses prefetch passed filter module placed queue 3 figure 3a entries queues 2 3 checked common entries dropped replies prefetches mainprocessor requests returned memory controller reach memory controller addresses compared processor miss requests queue 1 memoryprefetched line matches miss request main processor former considered reply latter latter sent memory chip finally machines include form processorside prefetch ing envision architecture operate two modes verbose nonverbose verbose mode queue 2 figures 3a b receives mainprocessor misses mainprocessor prefetch quests nonverbose mode queue 2 receives mainprocessor misses mode assumes mainprocessor prefetch requests distinguishable requests example tag mips r10000 21 nonverbose mode useful reduce total occupancy ulmt case processorside prefetcher focus easytopredict sequential regular miss patterns ulmt focus hardtopredict irregular ones verbose mode also useful ulmt implement prefetch algorithm enhances effectiveness processorside prefetcher present example case section 52 33 correlation prefetching algorithms simply taking current pairbased correlation table algorithm implementing software good enough indeed indicated section 22 base algorithm two limitations prefetch far ahead intuitively needs observe one miss eliminate another miss immediate successor result tends low coverage increase coverage three things need occur first need eliminate two limitations storing table prefetch ing several levels successor misses per miss immediate succes sors successors immediate successors several lev els second prefetches highly accurate finally prefetcher take decisions early enough prefetched lines reach main processor needed conditions easier support ensure correlation algorithm implemented ulmt two reasons first one storage cheap therefore correlation table inexpensively expanded hold multiple levels successor misses per miss even means replicating formation second reason customizability provided software implementation prefetching algorithm rest section describe ulmt implementation correlation prefetching deliver high coverage describe three approaches using conventional table organization using table reorganized ulmt exploiting customizability c c miss prefetch b c c abcadc ii current miss c abcadc current miss miss sequence correlation table c c miss prefetch b prefetch c follow link c c abcadc ii current miss c abcadc current miss correlation table miss sequence b abcadc c c c last secondlast current miss c c last c c c secondlast abcadc current miss miss sequence correlation table miss prefetch dbc c figure 4 pairbased correlation algorithms base chain b replicated c 331 using conventional table organization first step attempt improve coverage without specifically exploiting lowcost storage customizability advantages ulmt simply take conventional table organization section 22 force ulmt prefetch multiple levels successors every miss resulting algorithm call chain chain takes parameters base plus numlevels number levels successors prefetched algorithm illustrated figure 4b chain updates table like base ii prefetches differently iii specifically prefetching row immediate suc cessors takes mru one among accesses correlation table address entry found prefetches numsucc successors takes mru successor row repeats process done numlevels1 times example suppose miss occurs iii ulmt first prefetches b takes mru entry looksup table prefetches ds successor c chain addresses two limitations base namely prefetching far ahead needing one miss eliminate second one however chain may deliver high coverage two reasons prefetches may highly accurate ulmt may high response time issue prefetches prefetches may inaccurate chain prefetch true mru successors level successors instead prefetches successors found along mru path ex ample consider sequence misses alternates abc bebf abcbebfabc miss encountered chain prefetches immediate successors b accesses entry b prefetch e f note c prefetched high response time chain miss comes make numlevels accesses different rows table access involves associative search table associative potentially one cache misses 332 using table reorganized ulmt attempt improve coverage exploiting low cost storage ulmt solutions specifically expand table allow replicated information row table stores tag miss address numlevels levels successors level contains numsucc addresses use lru replacement using table propose algorithm called replicated figure 4c replicated takes parameters chain shown figure 4c replicated keeps numlevels pointers table pointers point entries address last miss second last used efficient table access miss occurs pointers used access entries last misses insert new address mru successor correct level ii figure numsucc entries level mru ordered finally prefetching replicated simple miss seen entries corresponding row prefetched iii note replicated eliminates two problems chain first prefetches accurate contain true mru successors level result grouping together successors given level irrespective path taken sequence shown abcbebfabc miss replicated prefetches b c second response time replicated much smaller chain indeed replicated prefetches several levels successors single row access maybe even single cache miss replicated effectively shifts computation prefetching step learning one prefetching needs single table access learning miss needs multiple table updates good tradeoff prefetching step critical one furthermore multiple learning updates inexpensive use pointers eliminates need associative searches table rows updated likely still cache memory processor since updated recently 333 exploiting customizability ulmt also improve coverage exploiting second advantage ulmt solutions customizability programmer system choose run different algorithm ulmt ap plication chosen algorithm highly customized applications needs one approach customization use table organizations prefetching algorithms described tune parameters application basis example applications miss sequences highly predictable set number levels successors prefetch numlevels high value result characteristics base chain replicated levels successors prefetched 1 numlevels numlevels true mru ordering level yes number row accesses prefetching step requires search 1 numlevels 1 number row accesses learning step requires response time low high low space requirement constant number prefetches x x numlevels x table 1 comparing different pairbased correlation prefetching algorithms running ulmt prefetch levels successors high accuracy applications unpredictable sequences opposite also tune number rows table numrows applications large footprints set numrows high value hold information table small applications opposite save space second approach customization use different prefetching algorithm example add support sequential prefetching algorithms described resulting algorithms low response time sequential miss patterns another approach adaptively decide algorithm onthefly application executes fact approach also used execute different algorithms different parts one application intraapplication customizability may useful complex applications finally ulmt also used profiling purposes monitor misses application infer higherlevel information cache performance application access patterns page conflicts 334 comparing algorithms table 1 compares base chain replicated algorithms executing ulmt replicated highest potential high coverage supports farahead prefetching prefetching several levels successors prefetches high accuracy prefetch true mru successors level low response time part needs access single table row prefetching step accessing single row minimizes associative searches cache misses shortcoming replicated larger space requires correlation table however minor issue since table software structure allocated main memory note algorithms also implemented hardware however replicated suitable ulmt implementation providing larger space required hardware expensive 34 operating system issues operating system issues related ulmt operation outline protection ulmt separate address space instructions correlation table data structures ulmt shares neither instructions data application ulmt observe physical addresses application misses also issue prefetches addresses behalf main processor however neither read write addresses therefore protection guaranteed multiprogrammed environment poor approach applications share single table table likely suffer lot interference better approach associate different ulmt table application eliminates interference tables addition enables customization ulmt application conservatively assume 4mbyte table average per application 8 applications require 32 mbytes modest fraction todays typical main memory requirement excessive save space dynamically sizing tables case application use space table shrinks scheduling scheduler knows ulmt associated application consequently scheduler schedules preempts application ulmt group furthermore operating system provides interface application control ulmt page remapping sometimes page gets remapped since ulmts operate physical addresses events cause table entries become stale choose take action let table update automatically learning alternatively operating system inform corresponding ulmt remapping occurs passing old new physical page number ulmt indexes table line old page entry found ulmt relocates updates tag applicable successors row given current page sizes estimate table update take microseconds overhead may overlapped execution operating system page mapping handler main processor note entries table may still keep stale successor information information may cause useless prefetches table quickly update automatically 4 evaluation environment applications evaluate ulmt approach use nine mostly irregular memoryintensive applications irregular applications hardly amenable compilerbased prefetching consequently obvious target ulmt correlation prefetching exception cg regular application table 2 describes applications last four columns table explained later simulation environment evaluation done using executiondriven simulation environment supports dynamic superscalar processor model 17 model pc architecture simple memory processor integrated either north bridge chip dram chip following microarchitecture figure 3 table 3 shows parameters used component architecture cycles 16 ghz cycles architecture modeled cycle cycle model uniprogrammed environment single application single ulmt execute concurrently model contention system including contention application thread ulmt shared resources memory controller dram channels dram banks processorside prefetching main processor optionally includes hardware prefetcher prefetch multiple streams stride 1 1 l1 cache prefetcher monitors l1 cache misses identify prefetch numseq sequen correlation table appl suite problem input numrows size mbytes base chain repl cg nas conjugate gradient class 64 13 08 18 equake specfp2000 seismic wave propagation simulation test 128 25 15 35 ft nas 3d fourier transform class 256 50 30 70 gap specint2000 group theory solver rako subset test 128 25 15 35 mcf specint2000 combinatorial optimization test mst olden finding minimum spanning tree 1024 nodes 256 50 30 70 parser specint2000 word processing subset train 128 25 15 35 sparse sparsebench10 gmres compressed row storage tree univ hawaii3 barneshut nbody problem 2048 bodies 8 average 140 27 16 38 table 2 applications used main processor 6issue dynamic 16 ghz int fp ldst fus 4 4 2 pending ld st 8 16 branch penalty 12 cycles memory processor 2issue dynamic 800 mhz int fp ldst fus 2 0 1 pending ld st 4 4 branch penalty 6 cycles main processors memory hierarchy line 3cycle hit rt l2 data writeback 512 kb 4 way 64b line 19cycle hit rt rt memory latency 243 cycles row miss 208 cycles row hit memory bus splittransaction 8 b 400 mhz 32 gbsec peak memory processors memory hierarchy line 4cycle hit rt north bridge rt mem latency 100 cycles row miss cycles row hit latency prefetch request reach dram 25 cycles dram rt mem latency 56 cycles row miss cycles row hit internal dram data bus 32b wide 800 mhz 256 gbsec peak parameters applicable procs dual channel channel 2 b 800 mhz total 32 gbsec peak random access time trac ns time memory controller tsystem ns depth queues 1 filter module table 3 parameters simulated architecture latencies correspond contentionfree conditions rt stands roundtrip processor cycles 16 ghz cycles tial streams concurrently works follows third miss sequence observed prefetcher recognizes stream prefetches next numpref lines stream l1 cache furthermore stores stride next address expected stream special register processor later misses address register prefetcher prefetches next numpref lines stream updates register prefetcher contains numseq registers see scheme works somewhat like stream buffers 13 prefetched lines go l1 choose approach minimize hardware complexity shortcoming l1 cache may get polluted completeness resimulated system prefetches going separate buffers rather l1 found performance changes little part checking buffers l1 misses introduces delay algorithm parameters table 4 lists prefetching algorithms evaluate default parameters use sequential prefetching supported hardware main processor called conven4 conventional also implemented software ulmt evaluate two software implementations seq1 seq4 case prefetcher memory observes l2 misses rather l1 unless otherwise indicated processorside prefetcher ulmt algorithms operate nonverbose mode sec tion 32 base algorithm choose parameter values used joseph grunwald 12 compare work last four columns table 2 give conservative value size correlation table application table twoway setassociative sized number rows table numrows lowest power two trivial hashing function simply takes lower bits line address less 5 insertions replace existing entry generous allocation sophisticated hash function reduce numrows significantly without increasing conflicts much case knowing row base chain repl takes 20 12 28 bytes respectively 32bit machine compute total table size overall applications need space others average value tolerable 27 16 38 mbytes base chain repl respectively ulmt implementation wrote ulmts c handoptimized minimal response occupancy time one major performance bottleneck implementation frequent branches remove branches unrolling loops hardwiring algorithm parameters also perform optimizations increase spatial locality reduce instruction count none algorithms uses floatingpoint operations 5 evaluation 51 characterizing application behavior predictability miss sequences start characterizing well ulmt algorithms predict miss sequences applications run ulmt algorithm simply observing l2 cache miss addresses without performing prefetch ing record fraction l2 cache misses correctly predicted sequential prefetcher means upcoming miss address matches next address predicted one streams identified pairbased prefetcher upcoming address matches one successors predicted level figure 5 shows results prediction three levels suc cessors given miss level 1 chart shows predictability immediate successor level 2 shows predictability next successor level 3 successor one experiments pairbase schemes use large tables ensure practically prediction missed due conflicts table num rows 256 k assoc 4 numsucc 4 condi prefetching algorithm implementation name parameter values base base chain chain replicated software memory ulmt repl sequential 1stream seq1 sequential 4streams seq4 sequential 4streams hardware l1 main processor conven4 table 4 parameter values used different algorithms level correct prediction seq1 base level 210 20 50 6080 90cg equake ft gap mcf mst parser sparse tree average correct prediction chain repl level cg equake ft gap mcf mst parser sparse tree average correct prediction chain repl figure 5 fraction l2 cache misses correctly predicted different algorithms different levels successors tions level 1 chain repl equivalent base levels 2 3 base applicable figure also shows effect combining algorithms figure 5 shows ulmt algorithms effectively predict miss streams applications example level 1 seq4 base correctly predict average 49 82 misses spectively moreover best algorithms keep predicting correctly across several levels successors example repl correctly predicts average 77 73 misses levels 2 3 spectively therefore algorithms good potential figure also shows different applications different miss behavior instance applications mcf tree sequential patterns therefore pairbased algorithms predict misses applications cg instead sequential patterns dominate result sequential prefetching predict practically l2 misses applications mix patterns among pairbased algorithms repl almost always outperforms chain wide margin chain maintain true mru successors level however repl effective patterns better combined multistream sequential prefetching seq4repl time l2 misses another important issue time misses figure 6 classifies l2 misses according number cycles two consecutive misses arriving mem ory misses grouped bins corresponding 080 cycles cycles etc unit 16 ghz processor cycles significant bin 200280 contributes 60 miss distances average misses critical beyond numbers latencies hard hide outoforder execution indeed since roundtrip latency memory 208243 cycles dependent misses likely fall bin contribute processor stall figure suggests dependent misses cannot overlapped consequently want ulmt prefetch make sure ulmt fast enough learn misses occupancy less 200 cycles misses bins fewer less critical far apart put pressure ulmts timing 080 may give enough time ulmt respond fortunately misses likely overlapped computation 0 10 20 30 40 50 70 80 90 100 cg equake ft gap mcf mst parser sparse tree average figure 6 characterizing time l2 misses 52 comparing different algorithms figure 7 compares execution time applications different cases prefetching nopref processorside prefetching listed table 4 conven4 different ulmt schemes listed table 4 base chain repl combination conven4 repl conven4repl customized algorithms custom results case memory processor integrated dram application average bars normalized nopref bars show memoryinduced processor stall time caused requests processor l2 cache uptol2 requests beyond l2 cache be10nopref conven4 base chain repl conven4repl custom nopref conven4 base chain repl conven4repl nopref conven4 base chain repl conven4repl nopref conven4 base chain repl conven4repl nopref conven4 base chain repl conven4repl custom cg equake ft gap mcf normaized execution time beyondl2 base chain repl conven4repl custom nopref conven4 base chain repl conven4repl nopref conven4 base chain repl conven4repl nopref conven4 base chain repl conven4repl nopref conven4 base chain repl conven4repl mst parser sparse tree average normalized execution time beyondl2 busy100602 figure 7 execution time applications different prefetching algorithms yondl2 remaining time busy includes processor computation plus pipeline stalls system perfect l2 cache would busy uptol2 times average beyondl2 significant component execution time nopref accounts 44 time thus although ulmt schemes target l2 cache misses target main contributor execution time conven4 performs well cg sequential patterns dom inate however ineffective applications mcf tree purely irregular patterns average conven4 reduces execution time 17 pairbased schemes show mixed performance base shows limited speedups mostly prefetch far enough average reduces noprefs execution time 6 chain performs little better limited inaccuracy figure 5 high response time section 331 average reduces noprefs execution time 12 repl able reduce execution time significantly performs well almost applications outperforms base chain cases impact comes nice properties replicated algorithm discussed section 334 average application speedups repl nopref 132 finally conven4repl performs best average removes half beyondl2 stall time delivers average application speedup 146 nopref compare impact processorside prefetching conven4 memoryside prefetching repl see constructive effect conven4repl reason two schemes help specifically processorside prefetcher prefetches eliminates sequential misses memoryside prefetcher works non verbose mode section 32 therefore see prefetch requests therefore fully focus irregular miss patterns resulting reduced load ulmt effective algorithm customization first paper ulmt prefetch ing attempted simple customization ap plications table 5 shows changes cg run seq1repl verbose mode mst mcf run repl higher num levels cases conven4 results shown figure 7 custom bar three applications application customized ulmt algorithm cg seq1repl verbose mode mst mcf repl table 5 customizations performed conven4 also customization cg tries exploit positive interaction processor memoryside prefetching cg sequential miss patterns figure 5 multiple streams overwhelm conventional prefetcher indeed although processorside prefetches accurate 998 prefetched lines referenced timely enough 64 timely miss l2 cache customization turn verbose mode processorside prefetch requests seen ulmt furthermore ulmt extended singlestream sequential prefetch algorithm seq1 executing repl environment positive interaction two prefetchers increases specifically application references different streams interleaved manner processorside prefetcher unscrambles miss sequence chunks stream prefetch requests seq1 prefetcher ulmt easily identifies stream efficiently prefetches ahead result 81 processorside prefetches arrive timely manner customization speedup cg improves 219 conven4repl 259 case demonstrates even regular applications amenable sequential processorside prefetching benefit ulmt prefetching customization mst mcf tries exploit predictability beyond third level successor misses setting numlevels 4 repl shown figure 7 approach successful mst produces marginal gains mcf initial attempt customization shows promising sults applying customization three applications average execution speedup nine applications relative nopref becomes 153 cg equake ft gap mcf mst parser sparse tree average normalized execution time beyondl2 busy100602 figure 8 execution time different locations memory processor location memory processor figure 8 examines impact place memory processor figure 3 first two bars application taken figure 7 nopref con ven4repl last bar application corresponds conven4repl algorithm memory processor placed memory controller north bridge chip conven4replmc processor north bridge chip twice memory access latency 100 cycles vs 56 cycles eight times lower memory bandwidth 32 gbsec vs 256 gbsec additional 25cycle delay seen prefetch requests reach dram 1 however figure 8 shows impact execution time small results small decrease average speedups 146 141 impact small thanks ability repl accurately prefetch far ahead timeliness immediate successor prefetches affected prefetching levels successors still timely overall given results hardware cost two designs conclude putting memory processor north bridge chip costeffective design two prefetching effectiveness gain insight prefetching schemes figure 9 examines effectiveness lines prefetched l2 cache ulmt lines called prefetches figure shows data sparse tree average seven applications figure combines l2 misses prefetches breaks 5 categories prefetches eliminate l2 miss hits prefetches eliminate part latency l2 miss arrive bit late delayedhits l2 misses pay full latency nonprefmisses useless prefetches useless prefetches broken prefetches brought l2 referenced time replaced replaced prefetches dropped arrival l2 line already cache redundant since coverage fraction original l2 misses fully partially eliminated represented sum hits delayedhits shown figure 9 nonprefmisses figure 9 number l2 misses left prefetching relative original number l2 misses note nonprefmisses higher 10 algorithms 10 nonprefmisses number l2 misses eliminated relative original number l2 misses nonprefmisses broken two groups misses 10 line figure 9 10 hits delayed hits come original misses 10 line hits new l2 conflict misses caused prefetches looking average seven applications see base chain effective coverage small base hurt 1 cycle counts mainprocessor cycles05152535 base chain repl conven4repl conven4replmc nopref base chain repl conven4repl conven4replmc nopref base chain repl conven4repl conven4replmc sparse tree average 7 applications sparse tree hits delayedhits nonprefmisses replaced redundant figure 9 breakdown l2 misses lines prefetched ulmt prefetches original misses normalized 1 inability prefetch far ahead chain hampered high response time limited accuracy figure also shows repl high coverage 074 however comes cost useless prefetches replaced plus redundant equivalent 50 original misses additional misses due conflicts prefetches 20 original misses see therefore advanced pairbased schemes need additional bandwidth conven4repl seems low coverage despite high performance figure 7 reason prefetch requests issued processorside prefetcher effective eliminating l2 misses lumped nonprefmisses category figure reach memory since ulmt prefetcher nonverbose mode see requests conse quently ulmt prefetcher focuses irregular miss patterns ulmt prefetches eliminate irregular misses appear hitsdelayedhits finally figure 9 also shows sparse tree showed limited speedups figure 7 many conflicts cache results many remaining nonprefmisses furthermore prefetches accurate results large replaced redundant categories work load ulmt figure 10 shows average response time occupancy time section 31 ulmt algo rithms averaged applications times measured 16 ghz cycles bar broken computation time busy memory stall time mem numbers top bar show average ipc ulmt ipc calculated number base chain repl replmc base chain repl replmc response time occupancy time number processor cycles mem response time occupancy time figure 10 average response occupancy time different ulmt algorithms mainprocessor cycles instructions divided number memory processor cycles figure shows algorithms occupancy time less 200 cycles consequently ulmt fast enough process l2 misses figure 6 memory stall time roughly half ulmt execution time processor dram processor north bridge chip replmc chain repl lowest occupancy time note repls occupancy much higher chains despite higher number table updates performed repl reasons fewer associative searches better cache line reuse repl response time important prefetching effectiveness figure shows repl lowest response time around cycles response time replmc twice much fortunately replicated algorithm able prefetch far ahead accurately therefore effectiveness prefetching sensitive modest increase response time main memory bus utilization finally figure 11 shows utilization main memory bus various algorithms averaged applications increase bus utilization induced advanced algorithms divided two parts increase caused naturally reduced execution time additional increase caused prefetching traffic overall figure shows increase bus utilization tolerable utilization increases original 20 36 worst case conven4repl moreover increase comes faster execution 6 utilization directly attributable prefetches general fact memoryside prefetching adds oneway traffic main memory bus limits bandwidth needs 0 10 20 30 40 50 70 80 90 100 base chain repl utilization prefetching due reduced execution time due prefetching1006020 conven4 repl conven4 figure 11 main memory bus utilization 6 related work memoryside prefetching memoryside prefetchers simple hardware controllers example nvidia chipset includes dasp controller north bridge chip 22 seems mostly targeted stride recognition buffers data lo cally i860 chipset intel reported prefetch cache may indicate presence similar engine cooksey et al 9 propose contentbased prefetcher hardware controller monitors data coming memory item appears address engine prefetches alexander kedem 1 propose hardware controller monitors requests main memory observes repeatable patterns prefetches rows data dram sram buffer inside memory chip scheme different use generalpurpose processor running prefetching algorithm userlevel thread studies propose specialized programmable engines exam ple hughes 11 yang lebeck 28 propose adding specialized engine prefetch linked data structures hughes focuses multiprocessor processinginmemory system yang lebeck focus uniprocessor put engine every level cache hierarchy main processor downloads information engines linked structures prefetches perform scheme different general applicability another related system impulse intelligent memory controller capable remapping physical addresses improve performance irregular applications 4 impulse could prefetch data implements nextline prefetching furthermore buffers data memory controller rather sending processor correlation prefetching early work correlation prefetching found 2 24 recently several authors made contributions charney reeves study correlation prefetching suggest combining stride prefetcher general correlation prefetcher 6 joseph grunwald propose basic correlation table organization algorithm evaluate 12 alexander kedem use correlation prefetching slightly differently 1 indicate sherwood et al use help stream buffers prefetch irregular patterns 26 finally lai et al design slightly different correlation prefetcher 18 specifically prefetch triggered miss instead triggered deadline predictor indicating line cache used therefore new line prefetched scheme improves prefetching timeliness expense tighter integration prefetcher processor since prefetcher needs observe miss addresses also reference addresses program counters differ recent works important ways first propose hardwareonly engines often require expensive hardware tables use flexible userlevel thread generalpurpose core stores table software structure memory second except alexander kedem 1 place engines l1 l2 caches processor l1 place prefetcher memory focus l2 misses time intervals l2 misses large enough ulmt viable effective finally propose new table organization prefetching algorithm exploiting inexpensive memory space increases farahead prefetching prefetch coverage prefetching regular structures several schemes proposed prefetch sequential strided patterns include reference prediction table chen baer 7 stream buffers jouppi 13 palacharla kessler 23 sherwood et al 26 base processorside prefetcher schemes processorside prefetching many proposals processorside prefetching often irregular applications tiny nonexhaustive list includes choi et al 8 karlsson et al 14 lipasti et al 19 luk mowry 20 roth et al 25 zhang torrellas 29 schemes specifically target linked data structures tend rely program information available processor like addresses sizes data struc tures often need compiler support scheme needs neither program information compiler support related work chappell et al 5 use subordinate thread multithreaded processor improve branch prediction suggest using thread prefetching cache management fi nally work also related data forwarding multiprocessors processor pushes data cache hierarchy another processor 15 7 conclusions paper introduced memoryside correlation prefetching using userlevel memory thread ulmt running simple generalpurpose processor main memory scheme solves many problems conventional correlation prefetching provides several important additional features specifically scheme needs minimal hardware modifications beyond memory processor uses main memory store correlation table inexpensively exploit new table organization increase farahead prefetching coverage effectively prefetch applications largely miss pattern long repeats supports customization prefetching algorithm programmer individual applications results showed scheme delivers average speedup 132 nine mostlyirregular applications furthermore scheme works well combination conventional processorside sequential prefetcher case average speedup increases 146 finally exploiting customization prefetching algorithm increased average speedup 153 work extended designing effective techniques ulmt customization particular customizing linked data structure prefetching cache conflict detection elimination general application profiling customization cache conflict elimination improve sparse tree applications smallest speedups acknowledgments authors thank anonymous reviewers hidetaka magoshi jose martinez milos prvulovic marc snir james tuck r distributed predictive cache design high performance memory systems dynamic improvements locality virtual memory sys tems institute astronomy impulse building smarter memory controller simultaneous subordinate microthreading ssmt generalized correlation based hardware prefetching reducing memory latency via nonblocking prefetching cache iterative benchmark prefetching linked data structures systems merged dramlogic prefetching using markov predictors improving directmapped cache performance addition small fullyassociative cache prefetch buffers prefetching technique irregular accesses linked data structures comparing data forwarding prefetching communicationinduced misses sharedmemory mps scalable processors billiontransistor era iram directexecution framework fast accurate simulation superscalar processors software prefetching pointer call intensive environments nvidia nforce integrated graphics processor igp dynamic adaptive speculative preprocessor dasp evaluating stream buffers secondary cache replacement prefetching system cache second directory sequentially accessed blocks dependence based prefetching linked data structures sony computer entertainment inc push vs pull data movement linked data structures speeding irregular applications sharedmemory multiprocessors memory binding group prefetching tr reducing memory latency via nonblocking prefetching caches evaluating stream buffers secondary cache replacement speeding irregular applications sharedmemory multiprocessors compilerbased prefetching recursive data structures prefetching using markov predictors comparing data forwarding prefetching communicationinduced misses sharedmemory mps dependence based prefetching linked data structures simultaneous subordinate microthreading ssmt improving directmapped cache performance addition small fullyassociative cache prefetch buffers push vs pull predictordirected stream buffers deadblock prediction myampersandamp deadblock correlating prefetchers scalable processors billiontransistor era contentbased prefetching distributed prefetchbuffercache design high performance memory systems impulse directexecution framework fast accurate simulation superscalar processors memoryside prefetching linked data structures ctr kyle j nesbit james e smith data cache prefetching using global history buffer ieee micro v25 n1 p9097 january 2005 philip g emma allan hartstein thomas r puzak vijayalakshmi srinivasan exploring limits prefetching ibm journal research development v49 n1 p127144 january 2005 amir roth gurindar sohi quantitative framework automated preexecution thread selection proceedings 35th annual acmieee international symposium microarchitecture november 1822 2002 istanbul turkey adding logic close memory reduce latency indirect loads high miss ratios acm sigarch computer architecture news v33 n3 june 2005 brian rogers yan solihin milos prvulovic memory predecryption hiding latency overhead memory encryption acm sigarch computer architecture news v33 n1 march 2005 justin teller charles b silio jr bruce jacob performance characteristics maui intelligent memory system architecture proceedings 2005 workshop memory system performance june 1212 2005 chicago illinois dongkeun kim steve shihwei liao perry h wang juan del cuvillo xinmin tian xiang zou hong wang donald yeung milind girkar john p shen physical experimentation prefetching helper threads intels hyperthreaded processors proceedings international symposium code generation optimization feedbackdirected runtime optimization p27 march 2024 2004 palo alto california ilya ganusov martin burtscher efficient emulation hardware prefetchers via eventdriven helper threading proceedings 15th international conference parallel architectures compilation techniques september 1620 2006 seattle washington usa hur calvin lin memory prefetching using adaptive stream detection proceedings 39th annual ieeeacm international symposium microarchitecture p397408 december 0913 2006 lixin zhang mike parker john carter efficient address remapping distributed sharedmemory systems acm transactions architecture code optimization taco v3 n2 p209229 june 2006 wessam hassanein jos fortes rudolf eigenmann data forwarding inmemory precomputation threads proceedings 18th annual international conference supercomputing june 26july 01 2004 malo france ravi iyer cqos framework enabling qos shared caches cmp platforms proceedings 18th annual international conference supercomputing june 26july 01 2004 malo france stephen somogyi thomas f wenisch anastassia ailamaki babak falsafi andreas moshovos spatial memory streaming acm sigarch computer architecture news v34 n2 p252263 may 2006 zhen yang xudong shi feiqi su jihkwon peir overlapping dependent loads addressless preload proceedings 15th international conference parallel architectures compilation techniques september 1620 2006 seattle washington usa r shetty kharbutli solihin prvulovic heapmon helperthread approach programmable automatic lowoverhead memory bug detection ibm journal research development v50 n23 p261275 march 2006 zhen fang lixin zhang john b carter ali ibrahim michael parker active memory operations proceedings 21st annual international conference supercomputing june 1721 2007 seattle washington yan solihin jaejin lee josep torrellas correlation prefetching userlevel memory thread ieee transactions parallel distributed systems v14 n6 p563580 june jiwei lu abhinav das weichung hsu khoa nguyen santosh g abraham dynamic helper threaded prefetching sun ultrasparc cmp processor proceedings 38th annual ieeeacm international symposium microarchitecture p93104 november 1216 2005 barcelona spain adrin cristal oliverio j santana mateo valero jos f martnez toward kiloinstruction processors acm transactions architecture code optimization taco v1 n4 p389417 december 2004 marco galluzzi valentn puente adrin cristal ramn beivide josngel gregorio mateo valero first glance kiloinstruction based multiprocessors proceedings 1st conference computing frontiers april 1416 2004 ischia italy chialin yang alvin r lebeck hungwei tseng chienhao lee tolerating memory latency push prefetching pointerintensive applications acm transactions architecture code optimization taco v1 n4 p445475 december 2004 zhenlin wang doug burger kathryn mckinley steven k reinhardt charles c weems guided region prefetching cooperative hardwaresoftware approach acm sigarch computer architecture news v31 n2 may xianhe sun surendra byna yong chen serverbased data push architecture multiprocessor environments journal computer science technology v22 n5 p641652 september 2007 suleyman sair timothy sherwood brad calder decoupled predictordirected stream prefetching architecture ieee transactions computers v52 n3 p260276 march