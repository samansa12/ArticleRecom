protocolcentric approach onthefly race detection abstractwe present design evaluation new dataracedetection technique technique executes runtime rather postmortem handles unmodified sharedmemory applications run top cvm software distributed shared memory system assume explicit associations synchronization shared data require neither compiler support program source instead use binary code rewriter instrument instructions may access shared memory novel aspect system able use information underlying memory system implementation order reduce number comparisons made runtime present experimental evaluation techniques using system look data races five common sharedmemory programs quantify effect several optimizations basic technique data flow analysis instrumentation batching runtime code modification instrumentation inlining system correctly found races three five programs including two standard benchmark suite slowdown debugging technique averages less 25 applications b introduction despite potential savings time effort datarace detection techniques yet accepted tool builders parallel distributed systems part problem surely restricted domain mechanisms operate ie parallelizing compilers compiler support usually deemed necessary racedetection generally npcomplete 19 paper presents design evaluation onthefly racedetection technique explicitly parallel sharedmemory applications technique applicable shared memory programs written lazyreleaseconsistent lrc 11 see section 31 memory model work differs previous work 3 4 7 9 18 17 datarace detection performed onthefly without compiler support common dynamic systems address problem detecting data races occur given execution general problem detecting races allowed program semantics 19 25 earlier work 21 introduced approach demonstrating use less complex singlewriter protocol paper extends earlier work use advanced multiwriter protocol series optimizations basic technique find data races running applications modified version coherent virtual memory cvm 13 14 software distributed shared memory dsm system dsms support abstraction shared memory parallel applications running cpus connected generalpurpose interconnects networks workstations distributed memory machines like ibm sp2 key intuition work following lrc implementations already maintain enough ordering information make constanttime determination whether two accesses concurrent addition lrc information track individual shared accesses binary instrumentation run simple racedetection algorithm existing global synchronization points last task made much easier precisely synchronization ordering information maintained lrc system automatically generate global synchronization points longrunning applications none originally used technique check data races implementations five common parallel applications system correctly found races three waternsquared spatial splash2 27 benchmark suite data races constituted real bugs bugs reported splash authors fixed current version races could affect correctness unlikely occur platforms splash originally intended barnes hand modified locally order eliminate unnecessary synchronizations races introduced modifications affect correctness application since overhead still potentially exponential describe variety techniques greatly reduce number comparisons need made portions racedetection procedure largest theoretical complexity turn third fourthmost expensive component overall overhead specifically show statically eliminate 99 load store instructions potential race participants ii eliminate 80 potential comparisons runtime use lrc ordering information iii average slowdown use techniques currently less 28 applications could reduced even support inlining instrumentation code overhead still high system used time low enough use traditional debugging techniques insufficient even part standard debugging toolbox parallel programs problem definition paraphrase adves 1 terminology define data races detected system define happenedbefore1 partial order denoted hb1 shared accesses synchronization acquires releases follows 1 b ordinary shared memory accesses releases acquires processor occurs b program order b 2 release processor p 1 b corresponding acquire processor p 2 b lock acquire corresponds release intervening acquires releases lock barrier acquire corresponds release acquire departure release arrival instance barrier 3 hb1 c hb1 given definition 1 define data races follows shared accesses b constitute data race 1 b access word shared data least one write 2 neither hb1 true approximates notion actual data races defined netzer 20 common implemented systems without compiler support make claim detect data races allowed semantics program feasible races discussed netzer 20 program running completion system without data races guarantee subsequent executions free data races well however detect races occur given execution figure 1 shows two possible executions code processes access shared variable x synchronize synchronization variable l access pair w 1 execution left constitute data race semantics enforce ordering lock acquisitions execution might instead happened shown 1b case r 1 x ordered respect w 1 x two therefore constitute race note data races cause incorrect results generated correct results generated even execution right r 1 completes w 1 issued order system distinguish accesses figure 1 system must able detect understand semantics synchronization used programs practice requirement means programs must use systemprovided synchronization synchronization implemented top shared memory abstraction invisible system could result spurious race warnings however requirement stricter underlying dsm system programs must use systemvisible synchronization order run releaseconsistent system datarace detection system imposes additional consistency synchronization constraints 3 lazy release consistency data races b figure 1 ordering accesses lock l nondeterministic either b possible ordering events ordering given data race releaseacquire sequence pair conflicting accesses b race r 1 x w 1 x 31 lazy release consistency lazy release consistency 11 variant eager release consistency erc 8 relaxed memory consistency allows effects shared memory accesses delayed selected synchronization accesses occur simplifying matters somewhat shared memory accesses labeled either ordinary synchronization accesses latter category divided acquire release accesses acquires releases may thought conventional synchronization operations lock synchronization mechanisms mapped model well essentially erc requires ordinary shared memory accesses performed next release processor erc implementations delay effects shared memory accesses long meet constraint lrc protocols processors delay performing modifications remotely subsequent acquires processors modifications performed processor performed acquire central intuition lrc competing accesses shared locations correct programs separated synchronization deferring coherence operations synchronization acquired consistency information piggybacked existing synchronization messages lrc divides execution process intervals identified interval index figure 2 shows execution two processors two intervals second interval p 1 example denoted oe 2 1 time process executes release acquire new interval begins current interval index incremented relate intervals different processes happensbefore1 partial ordering similar defined shared accesses 1 intervals single processor totally ordered program order 2 interval oe oe j begins acquire corresponding release concluded interval oe wy 2 reply sync request figure 2 shared data x 1 x 2 assumed page 1 2 colocated another page pagebased comparison concurrent intervals would flag concurrent interval pairs oe 1 1 oe 2 containing possibly conflicting references common pages comparison associated bitmaps would reveal former false sharing latter true race rx 1 oe 2 1 wx 1 oe 2 3 transitive closure lrc protocols append consistency information synchronization messages information consists structures describing intervals seen releaser together enough information reconstruct hb1 ordering visible intervals example message granting lock p 2 figure 2 contains information intervals seen p 1 time release yet seen p 2 ie oe 1 1 system also records fact oe 1hb1 discuss locks barriers paper notion synchronization acquires releases easily mapped synchronization models well 32 datarace detection lrc system intuitively data race pair accesses intervening synchronization least one accesses write figure 2 read x 1 p 1 write x 1 p 2 constitute data race intervals oe 2 1 oe 2 concurrent ordered detecting data races generally requires comparing shared access every shared access lrc system system based hb1 partial ordering limit comparisons accesses pairs concurrent intervals example interval pair oe 1 2 figure 2 concurrent check order determine data race formed accesses intervals perform wordlevel comparisons first verified pages accessed two intervals overlap example assume 1 2 figure 2 reside page comparison pages accessed concurrent 1 oe 1 would reveal access overlapping pages ie page containing 1 2 would therefore need perform bitmap comparison order determine accesses constitute false sharing true sharing ie data race case answer would false sharing accesses distinct locations however p 2 first write z variable completely different page comparison pages accessed two intervals would reveal overlap bitmap comparison would performed even though intervals concurrent implementation 41 system changes implemented racedetection system top cvm 13 14 software dsm supports multiple protocols consistency models like commercially available systems treadmarks 12 cvm written entirely userlevel library runs unixlike systems unlike treadmarks cvm created specifically platform protocol experimentation system written c opaque interfaces strictly enforced different functional units system whenever possible base system provides set classes implement generic protocol lightweight threads network communication latter functionality consists efficient endtoend protocols built top udp new shared memory protocols created deriving classes base page protocol classes methods differ base classs methods need defined derived class underlying system calls protocol hooks page faults synchronization io events take place since many methods inlined resulting system able perform within percent severely optimized system treadmarks running similar protocol cvm also designed take advantage generalized synchronization interfaces well use multithreading latency toleration detection mechanism based cvms multiwriterlrc protocol protocol propagates modifications form diffs runlength encodings modifications single page 12 diffs created wordbyword comparisons current contents page copy page saved modifications made made three modifications basic cvm implementation added instrumentation collect read access information ii added lists pages read read notices message types already carry analogous information pages written iii potentially add extra message round barriers order retrieve wordlevel access information necessary 42 instrumentation use atom 26 coderewriter instrument shared accesses calls analysis routines atom allows executable binaries analyzed modified use atom identify instrument loads stores may access shared memory although atom currently available dec alpha systems port currently underway intels x86 architecture moreover tools provide similar support architectures becoming common examples eel 16 sparc mips shade 5 sparc etch 22 x86 base instrumentation consists procedure call analysis routine checks instruction accesses shared memory routine sets bits corresponding accesss page position page order indicate page word accessed analysis routine consists including 10 instructions saving registers restoring registers stack actual call analysis routine together instructions save registers outside call consume 7 8 instructions small number additional instructions needed batching runtime code modification optimizations information pages accessed together bitmaps placed known locations bitmap comparison barrier arrivals barrier releases interval comparison bitmap requests bitmaps figure 3 barrier algorithm following steps 1 read write notices sent barrier master 2 barrier master identifies concurrent intervals overlapping page access lists 3 bitmaps requested overlapping pages step 2 comparisons used identify data races dotted lines indicate events occur sharing detected step 2 cvm use execution application data structures including bitmaps statically allocated order reduce runtime cost shared memory bitmaps allocated fixed memory locations order decrease cost instrumentation code 43 algorithm overall procedure detecting data races illustrated figure 3 following 1 use atom instrument shared loads stores application binary problem lies determining references shared base case simply instrument reference use stack pointer global variables pointer 1 register loaded one values current basic block also skip library references know inspection applications make library calls modify shared memory absence guarantees contrary easily instrument noncvm libraries well instrumentation would affect slowdown applications spend time libraries initialization section 45 describes several extensions basic technique either eliminate memory accesses candidates instrumentation decrease cost resulting instrumentation 2 synchronization messages base cvm protocol carry consistency information form interval structures interval structure contains one write notices enumerate pages written interval cvm augmented interval structures also carry read notices lists pages read interval assumes shared data allocated dynamically interval structures also contain version vectors identify logical time associated interval permit checks concurrency 3 worker processes lrc system append interval structures together consistency information barrier arrival messages barrier therefore barrier master complete current information intervals entire system information sufficient barrier master locally determine set pairs concurrent intervals although algorithm must potentially compare version vector interval given processor version vector interval every processor exploiting synchronization program order allows many comparisons omitted 4 pair concurrent intervals read write notices checked overlap data race might exist page either written two concurrent intervals read one interval written interval pairs together list overlapping pages placed check list steps 5 6 performed check list nonempty ie datarace false sharing see figure 3 5 barrier release messages augmented carry requests bitmaps corresponding accesses covered check list read write notice corresponding bitmap describes precisely words page accessed interval hence pair concurrent intervals four bitmaps read write interval might needed order detect races bitmaps returned barrier master interval pair check list 6 barrier master compares bitmaps overlapping pages concurrent intervals single bitmap comparison constant time process dependent page size case readwrite writewrite overlap algorithm determined data race exists prints address offending race currently use simple interval comparison algorithm find pairs concurrent intervals primarily major system overhead elsewhere upper bound number intervals per processor pair comparison algorithm must compare oi 2 maximum number intervals single processor since last barrier however algorithm needs examine intervals created last barrier epoch barrier epoch interval time two successive barriers definition intervals separated intervals previous epochs synchronization therefore ordered respect since interval potentially needs compared every interval another process current epoch total comparison time per barrier bounded number processes maximum number intervals process current epoch practice however number comparisons usually quite small applications use barriers one interval per process per barrier epoch one interval per barrier created additional peertopeer synchronization exclusive locks however peertopeer synchronization also imposes ordering intervals synchronizing processes example lock release subsequent acquire order intervals prior release respect subsequent acquire since ordered pair intervals definition concurrent act creates intervals also removes many interval pairs consideration data races hence programs many intervals barriers usually also ordering constraints reduce number concurrent intervals note technique require frequent barriers accommodate longrunning barrierfree applications forcing global synchronization occur system buffers filled 44 example illustrate use technique example based figure 2 figure 2 shows portion execution two processes together synchronization memory accesses memory accesses statically identified nonshared shown data items x 1 x 2 located page 1 2 another assume barriers occur immediately accesses figure events figure correspond single barrier epoch barrier arrival messages p 1 p 2 therefore contain information four intervals oe 1 2 interval structures oe 1 2 contain single write notice oe 2 contains two read notices reads x 1 x 2 represented single read notice located page upon arrival processes second barrier six possible interval pairs eliminate oe 1 1 2 program order oe 1 2 synchronization order finally oe 1 2 eliminated intervals access pages common leaves oe 2 2 oe 2 2 possible causes races false sharing following use notation oe j rx refer read bitmap page x interval oe j similar notation writes barrier release messages include requests bitmaps oe 2 1 ry oe 1 2 wy order judge first pair oe 2 2 wx second pair comparison oe 2 1 ry oe 1 2 wy reveal false sharing intervals access different data items happen located page contrast comparison oe 2 show data race exists x 1 accessed intervals one accesses write 45 optimizations section describes three enhancements basic technique 451 dataflow analysis use limited form iterative interprocedural register dataflow analysis order identify additional nonshared memory accesses technique consists creating dataflow graph associating incoming outgoing sets registers basic block registers set define registers known pointers shared memory iteration analysis outgoing set defined incoming set minus registers loaded block incoming sets redefined intersection incoming set previous iteration outgoing sets preceding blocks procedure continues incoming register sets stabilize values left registers incoming register sets known pointers shared space memory accesses using registers need instrumented made two main assumptions first simplify interprocedural analysis exploiting fact function arguments usually passed registers tracking parameters passed manner entails tracking order stack accesses caller callee blocks conservatively assume parameters passed method might name pointers shared data second assume function calls register pointers calls complicate data flow analysis destination calls identified statically system could easily modified disable dataflow optimization calls detected 452 batching calls instrumentation routines batched combining accesses checks multiple instructions single procedure call implemented three different types batching accesses within single basic blocks ffl batching accesses memory location reference type either load store ffl batching accesses memory location different reference types ffl batching accesses consecutive memory location reference type largest performance improvement provided first method ie batching accesses memory location reference type instrumentation first access eliminated care data accessed care many times accessed duplicated loads stores memory location might occur within basic block register pressure aliasing example latter case pair loads one register sandwiched around store another register compilers static analysis generally way determining whether loads store access distinct locations memory hence second load left basic block batching methods less useful instrumentation eliminated however instrumentation remains less costly without batching second third methods avoid procedure calls consolidating instrumentation multiple accesses single routine resulting instrumentation also needs check whether shared valid even accesses consecutive memory locations assume shared nonshared regions located contiguously address space instrumentation generated second method additional advantage able use bitmap offset calculation accesses 453 runtime code modification use selfmodifying code remove instrumentation instructions turn reference private data memory reference instrumentation consists check distinguish private shared references code record access references shared memory runtime code modification overwrite instrumentation noop instructions instruction references private data advantage subsequent executions instrumented instruction delayed cost executing noop instructions rather cost executing instrumentation code includes additional memory references modifyingcode runtime requires text segment writable unprotect entire text segment beginning applications execution using atom routines obtain size applications instrumented text segment reply sync request figure 4 single diff describes modifications page x oe 1 2 oe 2 2 lazy diffing primary complication caused separation data instruction caches use data stores overwrite instrumentation code new instructions seen data system stalled writeback data cache problems remain even new instructions written memory stale copies might remain instruction cache solve problem issuing special pal imb alpha instruction makes caches coherent second complication naively overwriting entire instrumentation call inside call causes stack become corrupted get around problem merely saving indication affected instrumentation calls deleted rather performing deletion immediately instrumentation calls actually deleted code subsequent synchronization points technique applicable assume memory access instruction exclusively references either private shared data modified system detect instructions access shared nonshared data runtime information anecdotal provides guarantees behavior runs nonetheless technique useful applied caution used modified version cvm detect instructions access shared nonshared data two applications eliminated offending instructions manually cloning 6 routines contained 454 diffs one optimization exploit use diffs capture write behavior diffs summaries changes made single page interval created comparing current copy page twin copy saved modifications begun hence diff seemingly information write bitmaps use diffs could allow us dispense instrumentation write accesses however diffs created lazily meaning shared writes might assigned wrong portion processs execution example consider process figure 4 assume x 1 x 2 page lazy diff creation means diff describing p 2 first write immediately created end interval oe 1 problem subsequent write x 1 folded diff associated earlier interval merging violate consistency guarantees lrc systems require applications free data races however merging cause system incorrectly believe datarace exists oe 1 1 oe 1 another disadvantage approach use diffs would slightly weaken race detection technique diffs contain modifications shared data locations overwritten value appear diffs even apps input set sync memory intervals slowdown intervals bitmaps msg used used ohead barnes barrier 32768 1 242 4 47 11 spatial 512 mols 5 iters lock barrier 824 water 512 mols 5 iters lock barrier 344 84 318 5 19 31 table 1 application characteristics though use might constitute race performance evaluated performance prototype searching data races five common sharedmemory applications barnes barneshut algorithm splash2 27 benchmark suite fft fast fourier transform sor jacobi relaxation water molecular dynamics simulation splash2 suite spatial problem water different algorithm also splash2 optimized reduced synchronization applications run decstations four 275 mhz alpha processors connected 155 mbit atm performance numbers measured datarace free applications ie first detected identified removed dataraces water barnes spatial measured numbers shown table 1 summarizes application inputs runtime characteristics memory size size shared data segment intervals barrier average number intervals created barriers number interval comparisons potentially proportional square number intervals metric gives approximate idea worstcase cost running comparison algorithm roughly speaking new interval created synchronization acquire hence barrieronly applications single interval per barrier epoch slowdown runtime slowdown applications withoutany optimizations described section 45 compared uninstrumented version application running unaltered version cvm first iteration application timed interested steadystate behavior longrunning applications however slowdowns would even smaller first iteration counted five applications nonoptimized execution time slows average factor 38 number compares quite favorably even systems exploit extensive compiler analysis 17 7 last three columns discussed section 52 figure 5 breaks application slowdown five categories without optimizations described section 45 cvm mods overhead added modifications cvm primarily setting data structures necessary proper datarace detection additional bandwidth used read write notices bitmaps describes overhead extra barrier round required retrieve bitmaps together cost bitmap comparisons intervals refers time spent using interval comparison algorithm identify concurrent interval pairs barnes fft sor spatial water slowdown cvm mods bitmaps access check proc call orig figure 5 breakdown overhead unoptimized instrumentation techniques overlapping page accesses access time spent inside instrumentations procedure calls determining whether accesses shared memory setting proper bits proc call procedure call overhead instrumentation base version atom currently inline instrumentation procedure calls inserted existing code section 55 describes performance impact experimental version atom inline instrumentation orig refers original running time access check time dominates overhead two applications slow sor spatial neither application significant false sharing frequent synchronization therefore interval creations opportunities run interval comparison algorithm barnes highest proportion overhead spent interval comparison algorithm reason process determining whether given pair intervals access pages expensive barnes process accesses significant fraction entire shared address space interval current representation read write notices lists pages efficient large numbers pages overhead could reduced changing representation bitmaps intervals many notices following subsections describe overheads detail 51 instrumentation costs instrumented load store could potentially involved data race instrumentation consists procedure call analysis routine hence adds proc call access check overheads summing columns figure 5 see instrumentation accounts average 646 total racedetection overhead overhead reduced instrumenting fewer instructions goal difficult shared private data accessed using addressing modes sometimes even share base registers however eliminate stack accesses checking use stack pointer base register fact shared data system dynamically allocated allows us eliminate instructions access data base register points start staticallyallocated data segment finally instrument instructions shared libraries none applications pass segment pointers app load store instructions stack static library cvm inst barnes 558 320 118057 15759 933 fft 308 207 118057 15759 358 spatial 758 506 118057 15782 1043 water 613 503 118057 15759 940 table 2 categorization memory access instructions inst shows number instructions actually instrumented libraries case majority scientific programs data race detection important however easily instrument dirty library functions necessary table breaks load store instructions categories able statically distinguish base case ie without optimizations applied first five columns show number loads stores instrumented access stack staticallyallocated data library routines including cvm sixth column shows remainder instructions could eliminated therefore possible datarace participants use atom instrument access procedure call access check routine executed whenever instruction executed average able statically determine 99 loads stores applications nonshared data example fft binary contains 134993 load store instructions 118057 instructions libraries 308 instructions access data stack pointer hence reference stack data another 15759 cvm system finally 207 instructions access data global pointer register pointing base statically allocated global memory eliminate instructions well since cvm allocates shared memory dynamically entire binary remain 358 memory access instructions could possibly reference shared memory hence might part data race nonetheless section 53 show majority runtime calls analysis routines private shared data 52 cost comparison algorithm comparison algorithm three tasks first set concurrent interval pairs must found second list must reduced interval pairs access least one page common eg one interval read notice page x interval write notice page x pair concurrent intervals exhibits unsynchronized sharing however sharing may either false sharing ie loads stores page x reference different locations x data race true sharing loads stores reference least one common location page x data race column labeled intervals used table 1 shows percentage intervals involved least one concurrent interval pair number ranges zero sor unsynchronized sharing true false 86 spatial large amount true false sharing note number possible interval pairs quadratic respect number intervals even stage eliminates 14 intervals spatial may eliminating much higher percentage interval pairs column labeled bitmaps used shows average 27 bitmaps must retrieved constituent processors order identify data races distinguishing false true sharing page access lists concurrent intervals overlap cases false sharing actual data races percentage intervals bitmaps involved comparisons fairly small note however effect bitmap interval comparisons barnes although absolute amount overhead added comparisons large larger relative rest overhead application also seeming disparity utilization intervals bitmaps barnes 4 intervals used 47 bitmaps used implies majority shared pages accessed small number intervals probably one two phases timestep loop many phases fact exactly case version barnes work done force position computation phases separated barrier however processor accesses bodies phases sets bodies accessed different processors disjoint hence true sharing processors across barrier computations since bodies assigned processor iteration scattered throughout address space large amount false sharing occurs barrier serves double effect false sharing splitting intervals half causing bitmaps requested twice page instead note barrier removed without slight reorganization code synchronizes updates scalar global variables tested interpretation results implementing reorganization removal barrier effectively reduced interval bitmap overheads half reducing overall overhead approximately 30 final column table 1 shows amount additional data needed race detection technique compared uninstrumented system 53 effect optimizations table 3 shows effect optimizations number instructions actually instrumented opt refers base case optimizations df dataflow batching selfexplanatory code mod refers dynamic code modification includes three dfbatching included show synergy dataflow batching absence code modification code modification actually increases number instrumented sites fft water cloning last three columns table 3 show number times able apply batching method 23 represents combining two three instructions type consecutive addresses represents combining instructions type address finally mix combining instructions address different access types numbers imply batching applicable applications complex data structures access patterns apps instrumented instructions batching opt data flow batching code mod dfbatching 23 mix barnes 933 854 730 933 655 655 63 26 76 spatial 1043 844 904 1043 725 725 water 940 776 747 1156 604 733 39 38 58 table 3 static instrumentation statistics apps millions instrumented instructions flow batching code mod dfbatching barnes 4358 4150 4343 885 4135 870 fft 58 53 58 15 53 15 spatial 1083 395 882 285 221 200 water 1249 518 1050 216 326 75 table 4 dynamic optimization statistics table 4 shows effect optimizations number instrumented instructions executed runtime although data flow analysis batching together eliminate 298 229 local reference instrumentations barnes fft respectively accounts 51 88 instrumented references runtime hand 305 instrumentationsand 358 waters instrumentationsare eliminated yet eliminationof instrumentations accounts 800 739 runtime references respectively clearly effectiveness optimizations heavily applicationdependent figure 6 shows effect optimizations overall slowdown applications average slowdown three optimizations applied 28 improvement 26 fft lowest overhead 22 figure 6 also includes bars inlining optimization discussed section 55 54 cost cvm modifications figure 5 shows almost 158 overhead comes cvm mods modifications made cvm system order support racedetection algorithm overhead consists cost setting additional data structures datarace detection cost additional bandwidth consumed read notices last column table 1 shows bandwidth overhead adding read additional write notices synchronization messages individual read write notices size typically least five times many reads writes read notices consume proportionally larger amount space write notices additional write notices needed even notices longer created lazily even though diffs still barnes fft sor spatial water slowdown base data flow batch code mod inlining allinlining figure 6 optimizations bandwidth overhead water quite large finegrained synchronization means many intervals notices created contrast primary cost spatial false sharing quite prevalent leading large number bitmap requests inlining verify assumption procedure call access check overhead significantly reduced inlining used unreleased version atom called xatom inline read write access checks implementation decreases cost inlined code fragments using register liveness analysis identify dead registers dead registers used whenever possible avoid spilling contents registers needed instrumentation code table 5 shows effect inlining overall performance relative base case optimizations column labeled runtime shows effect percentage overall running time overhead shows quantity percentage instrumentation overhead static column shows percent inlined instructions eliminated register liveness analysis mostly load store instructions dynamic shows corresponding dynamic quantity elimination instructions useful majority memory access instructions hence relatively expensive improvements roughly correlate procedure call overhead shown figure 5 average 137 total overhead caused procedure calls however inlining also eliminate access check overhead liveness analysis reduce register spillage particularly important case sor overhead access checks important question answered effective optimizations combination inlining inlining certainly decreases potential techniques work decreasing cost number access checks however still effective combination inlining remaining overhead still significant runtime code modification particular would still useful inlining effect total number instructions instrumented however code modification mechanism would probably need change slightly order address fact inserted instrumentation longer bytes inserting unconditional branches end apps improvement register liveness runtime overhead static dynamic barnes 150 285 217 355 fft 138 235 168 158 spatial 13 19 135 73 water 140 257 178 244 table 5 inlining instrumentation code might effective overwriting inlined instrumentation large number noop instructions 6 discussion 61 reference identification system currently prints shared segment address together interval indexes detected race condition combination symbol tables information used identify exact variable synchronization context identifying specific instructions involved race difficult requires retaining program counter information shared accesses information available runtime scheme would require saving program counters shared access future barrier analysis phase determined access involved race storage requirements would generally prohibitive would also add runtime overhead second approach use conflicting address corresponding barrier epoch initial run program input second run second run program counter information gathered accesses conflicted address originate barrier epoch determined involve data race runtime overhead storage requirements thereby drastically reduced data race must occur second run exactly first happen application general races 20 ie synchronization order deterministic case water application found data races solution modify cvm save synchronization ordering information first run enforce ordering second run done work execution replay treadmarks similar dsm approach reconstruction lamport timestamps rolt 23 technique keeps track minimal ordering information saved initial run enforce exactly interleaving shared accesses synchronization second run second run complete address trace saved postmortem analysis although authors discuss race detection detail advantage approach initial run incurs minimal overhead ensuring tracing mechanism perturb normal interleaving shared accesses rolt approach complementary techniques described paper system could augmented include initial synchronizationtracing phase allowing us eliminate perturbation parallel computation second ie racereference identification phase currently use shared page variable involved datarace initial run target page save program counters identification run 62 global synchronization interval comparison algorithm run global synchronization operations ie barriers applications input sets study use barriers frequently enough otherwise synchronize infrequently enough number intervals compared barriers quite manageable nonetheless certainly exist applications global synchronization frequent enough keep number interval comparisons small number ideally system would able incrementally discard data races without global cooperation mechanisms would increase complexity underlying consistency protocol 10 global synchronization either used used often enough exploit cvm routines allow global state consolidated synchronizations currently mechanism used cvm garbage collection consistency information longrunning barrierfree programs 63 accuracy adve 2 discusses three potential problems accuracy race detection schemes concert weak memory systems systems support memory models lazy release consistency first whether return data races first data races 18 2 first races essentially caused affected prior race determining whether given race affected effectively consists deciding whether operations race precede via hb1 operations race question system currently reports data races however could easily capture approximation first races turning reporting races accesses occur via hb1 accesses data races second problem accuracy dynamic racedetection algorithms reliability information presence races race conditions could cause wild accesses random memory locations potentially corrupting interval ordering information access bitmaps problem exists dynamic racedetection algorithm expect occur infrequently final accuracy problem identified adve systems attempt minimize space overhead buffering limited trace information possibly resulting races remaining undetected system discards trace information checked races hence suffer limitation 64 limitations expect technique applicable large class applications applications test suite range sor bandwidthlimited water synchronizes modifies data fine granularity applications stress different portions racedetection technique raw cost access instrumentation sor complexity cost dealing large numbers intervals water furthermore applications exception sor modified order reduce false sharing multiwriter lrc tolerates false sharing much better protocols applications tuned lrc tend false sharing removed water spatial barnes large amounts false sharing nonetheless methodology clearly applicable situations chaotic algorithms example tolerate races means eliminating synchronization improving performance false positives caused tolerated races obscure unintended races rendering class applications illsuited techniques similarly protocolspecific optimization techniques may cause spurious races reported earlier version barnes barrier enforced antidependences barrier removed version barnes application still correct lrc delays propagation consistency information data however antidependences flagged system data races interfere normal operation technique techniques discuss paper necessarily limited lrc systems applications approach essentially use existing synchronizationordering information reduce number comparisons made runtime information could easily collected distributed systems memory models putting appropriate wrappers around synchronization calls appending small amount additional information synchronization messages multiprocessors could supported using wrappers appending small amount data synchronization state application rest techniques straightforward 65 performance enhancements performance underlying protocol could improved using write instrumentation create diffs rather using twin page comparison method investigate option complexity integrating mechanism runtime code modification example would nontrivial compiler techniques could used expose opportunities batching loop unrolling trace scheduling would particularly effective applications sor application largest overhead application testbed finally interval comparison algorithm could improved significantly overhead added comparison algorithm relatively small applications better worstcase bounds would desirable production system one promising approach use hierarchical comparison algorithms example two processes create large number intervals exclusively pairwise synchronization number intervals compared processes could reduced first aggregating intervals created isolation using aggregations compare intervals processes 7 related work great deal published work area data race detection however prior work dealt applications systems specialized domains bitmaps used track shared accesses 7 know language independent implementation onthefly datarace detection explicitlyparallel sharedmemory programs previously 21 described performance preliminary form racedetection scheme ran top cvms singlewriter lrc protocol 14 paper describes performance racedetection scheme top cvms multiwriter protocol protocol challenging target usually outperforms singlewriter protocol significantly making difficult hide racedetection overheads additionally work described paper includes several optimizations basic system ie batching dataflow analysis runtime code modification inlining work closely related work already alluded section 63 technique described implemented adve et al 2 authors describe postmortem technique creates trace logs containing synchronization events information allowing relative execution order derived computation events computation events correspond roughly cvms intervals computation events also read write attributes analogous read page lists bitmaps describe shared accesses interval trace files used offline perform essentially operations system differ minimallymodified system leverages lrc memory model order abstract synchronization ordering information onthefly therefore able perform analysis onthefly well away trace logs postmortem analysis much overhead work execution replay treadmarks could used implement racedetection schemes approach reconstruction lamport timestamps rolt 23 technique similar technique described section 61 identifying instructions involved races minimal ordering information saved initial run used enforce exactly interleaving shared accesses synchronization second run second run complete address trace saved postmortem analysis although authors discuss race detection detail advantage approach initial run incurs minimal overhead ensuring tracing mechanism perturb normal interleaving shared accesses rolt approach complementary techniques described paper primary thrust work using underlying consistency mechanism prune enough information onthefly postmortem analysis necessary techniques could used improve performance second phase rolt approach similarly system could augmented include initial synchronizationtracing phase allowing us reduce perturbations parallel computation recently work eraser 25 used verification lock discipline detect races multithreaded programs erasers main advantage detect races actually occur instrumented execution however guarantee racefree behavior dataraces found return false positives furthermore system support distributed execution finally overhead erasers approach order magnitude higher work detecting datarace detection nondistributed multithreaded programs also done recplay 24 recordreplay system multithreaded programs work similar rolt approach discussed applied multithreaded programs work uses happensbefore relation reconstruct replay execution initial run second run perform access checks conclusions paper presented design performance new methodology detecting data races explicitlyparallel sharedmemory programs technique abstracts synchronization ordering consistency information already maintained multiplewriter lazyreleaseconsistent dsm systems able use information eliminate access comparisons perform entire datarace detection onthefly used system analyze five sharedmemory programs finding data races three two data races standard benchmark programs bugs primary costs datarace detection system tracking shared data accesses able significantly reduce costs using three optimization techniques register dataflow analysis batching inlining nonetheless majority runtime calls library nonshared accesses therefore used runtime codemodification dynamically rewrite instrumentation order eliminate access checks instructions accessed nonshared data combining optimizations except inlining able reduce average slowdown applications approximately 28 12 one application expect combining inlining optimizations would reduce slowdown even implementation described specific lrc general approach system exploits synchronization ordering eliminate majority shared accesses without explicit comparison finegrained comparisons made onlywhere coarsegrained comparisons fail rule data races approach could used systems supporting programming models using wrappers around synchronization accesses track synchronization ordering believe utility techniques combination generality approach present help datarace detection become widely used r unified formalization four sharedmemory models detecting data races weak memory systems debugging fortran shared memory machine race frontier reproducing data races parallel program debugging fast instructionset simulator execution profiling methodology procedure cloning empirical comparison monitoring algorithms access anomaly detection memory consistency event ordering scalable sharedmemory multiprocessors parallel program debugging onthefly anomaly detection distributed shared memory using lazy release consistency lazy release consistency software distributed shared memory treadmarks distributed shared memory standard workstations operating systems coherent virtual machine relative importance concurrent writers weak consistency models make multiprocessor computer correctly executes multiprocess programs improving accuracy data race detection complexity event ordering sharedmemory parallel program executions race conditions online datarace detection via coherency guarantees instrumentation optimization win32intel executables using etch execution replay treadmarks work progress onthefly data race detector recplay dynamic data race detector multithreaded programs atom system buildingcustomized program analysis tools splash2 programs characterization methodological considerations tr ctr edith schonberg onthefly detection access anomalies acm sigplan notices v39 n4 april 2004 milos prvulovic josep torrellas reenact using threadlevel speculation mechanisms debug data races multithreaded codes acm sigarch computer architecture news v31 n2 may min xu rastislav bodk mark hill serializability violation detector sharedmemory server programs acm sigplan notices v40 n6 june 2005 chen ding xipeng shen kirk kelsey chris tice ruke huang chengliang zhang software behavior oriented parallelization acm sigplan notices v42 n6 june 2007 bohuslav krena zdenek letko rachel tzoref shmuel ur tom vojnar healing data races onthefly proceedings 2007 acm workshop parallel distributed systems testing debugging july 0909 2007 london united kingdom sudarshan srinivasan srikanth kandula christopher r andrews yuanyuan zhou flashback lightweight extension rollback deterministic replay software debugging proceedings usenix annual technical conference 2004 usenix annual technical conference p33 june 27july 02 2004 boston