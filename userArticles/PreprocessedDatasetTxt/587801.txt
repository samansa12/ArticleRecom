algorithms permuting large entries diagonal sparse matrix consider bipartite matching algorithms computing permutations sparse matrix diagonal permuted matrix entries large absolute value discuss various strategies consider implementation computer codes also consider scaling techniques increase relative values diagonal entries numerical experiments show effect reorderings scaling solution sparse equations direct method preconditioned iterative techniques b introduction say n theta n matrix large diagonal absolute value diagonal entry large relative absolute values offdiagonal entries row column permuting large nonzero entries onto diagonal sparse matrix useful several ways wish solve system nonsingular square matrix order n x b vectors length n preordering kind useful whether direct iterative methods used solution see olschowka neumaier 1996 duff koster 1997 work report continuation work reported duff koster 1997 presented algorithm maximizes smallest entry diagonal relies repeated applications depth first search algorithm mc21 duff 1981 harwell subroutine library hsl 1996 report concerned bipartite matching algorithms permuting rows columns matrix diagonal permuted matrix large algorithm central report computes matching corresponds permutation sparse matrix product sum diagonal entries maximized algorithm already mentioned used duff koster 1997 fully described report describe algorithm detail also consider modified version algorithm compute permutation matrix maximizes smallest diagonal entry compare performance algorithm duff koster 1997 also investigate influence scaling matrix scaling used computation matching make diagonal entries even larger relative offdiagonals particular look sparse variant bipartite matching scaling algorithm olschowka neumaier 1996 first maximizes product diagonal entries scales matrix entries one entries greater one rest report organized follows section 2 describe concepts bipartite matching need description algorithms section 3 review basic properties algorithm mc21 mc21 relatively simple algorithm computes matching corresponds permutation matrix puts many entries possible onto diagonal without considering numerical values algorithm maximizes product diagonal entries described section 4 section 5 consider modified version algorithm maximizes smallest diagonal entry permuted matrix section 6 consider scaling reordered matrix computational experience algorithms applied practical problems effect reorderings scaling direct iterative methods solution presented sections 7 72 effect preconditioning also discussed finally consider implications current work section 8 matching general n theta n sparse matrix matrix associate bipartite graph e consists two disjoint node sets v r v c edge set e u v 2 e implies sets v r v c cardinality n correspond rows columns respectively edge ij 6 0 define sets row c sets correspond positions entries row column j sparse matrix respectively use denote absolute value signify number entries set sequence matrix meaning always clear context subset e called matching assignment two edges incident node matching containing largest number edges possible called maximum cardinality matching simply maximum matching maximum matching perfect matching every node incident matching edge obviously every bipartite graph allows perfect matching however matrix nonsingular exists perfect matching ga perfect matching cardinality n defines n theta n permutation matrix pa ap matrices matching entries zerofree diagonal bipartite matching problems viewed special case network flow problems see example ford jr fulkerson 1962 efficient algorithms finding maximum matchings bipartite graphs make use augmenting paths let matching ga node v matched incident edge path p ga defined ordered set edges successive edges incident node path p called malternating path edges p alternately malternating path p called augmenting path connects unmatched row node unmatched column node bipartite graph figure 21 exists maugmenting path column node 8 row node 8 matching cardinality 7 represented thick edges black entries accompanying matrix correspond matching connected matrix entries maugmenting path clear context matching associated malternating maugmenting paths simply refer alternating augmenting paths let p subsets e define matching p maugmenting path phi p matching jm phi malternating cyclic path ie alternating path whose first last edge incident node phi p also matching jm phi figure 21 augmenting path r1368425791368in sequel matching often represented pointer array augmenting paths bipartite graph g found constructing alternating trees alternating tree subgraph g rooted row column node path malternating path alternating tree rooted column node j 0 grown following way start initial alternating tree fj 0 g consider column nodes j 2 c turn initially node j check row nodes 2 colj alternating path j 0 yet exist node already matched add row node column node matched extend row node edge path node root forms augmenting path key observation construction maximum perfect matching matching maximum augmenting path relative alternating trees implemented using pointer array c given edge either root node tree edges consecutive edges alternating path towards root augmenting paths alternating tree provided exist thus easily obtained p alternating trees unique general one construct several alternating trees starting root node equal node sets different edge sets different alternating trees general contain different augmenting paths matching algorithms describe next sections impose different criteria order paths alternating trees grown order obtain augmenting paths maximum matchings special properties matching asymptotically fastest currently known algorithm finding maximum matching hopcroft karp 1973 worstcase complexity number entries sparse matrix efficient implementation algorithm found duff wiberg 1988 algorithm mc21 implemented duff 1981 theoretically worstcase behaviour practice behaves like latter algorithm simpler concentrate following although note relatively straightforward use algorithm hopcroft karp 1973 similar way use mc21 later sections mc21 depthfirst search algorithm lookahead starts empty matching hence column nodes unmatched initially see figure 31 unmatched column node j 0 turn alternating tree grown augmenting path respect current matching found provided one exists set b used mark matched row nodes visited far initially first row nodes colj 0 searched lookahead unmatched node 0 one found singleton path maugmenting path unmatched node unmarked matched node marked nodes 0 edges added alternating tree setting search continues column node node j 1 row nodes colj 1 first checked unmatched node one exists say path forms augmenting path unmatched node remaining unmarked node 1 picked set j 1 search moves node j 2 continues similar depthfirst search fashion either augmenting path nodes j 0 k unmatched k 0 colj k contain unmarked node latter case mc21 backtracks resuming search previously visited column node j kgamma1 remaining unmarked node 0 backtracking mc21 resumes search column node j 0 colj 0 contain unmarked node maugmenting path starting node j 0 exist case mc21 continues construction new alternating tree starting next unmatched column node final maximum matching cardinality n gamma 1 hence perfect figure 31 outline mc21 repeat exists 2 colj unmatched else exists else iap 6 null iap 6 null augment along path node iap node j 0 end weighted matching section describe algorithm computes matching permuting sparse matrix product diagonal entries permuted matrix maximum absolute value algorithm determines matching corresponds permutation oe maximizes n maximization multiplicative problem translated minimization additive problem defining matrix log maximum absolute value column j matrix maximizing 41 equal minimizing log log oe log ja ioe log oe log ja ioe minimizing 42 equivalent finding minimum weight perfect matching edge weighted bipartite graph known literature bipartite weighted matching problem linear sum assignment problem linear programming combinatorial optimization numerous algorithms proposed computing minimum weight perfect matchings see example burkard derigs 1980 carpaneto toth 1980 carraresi sodini 1986 derigs metz 1986 jonker volgenant 1987 kuhn 1955 practical example assignment problem allotment tasks cost matrix c represents cost benefit assigning person task j realvalued n theta n e corresponding bipartite graph whose edges weight matching gc denoted cm defined sum edge weights ie ij2m perfect matching said minimum weight perfect matching smallest possible weight ie cm cm 0 possible maximum matchings 0 key concept finding minimum weight perfect matching socalled shortest augmenting path maugmenting path p starting unmatched column node j called shortest cm phi p possible maugmenting paths p 0 starting node j define length alternating path p matching called extreme allow alternating cyclic path negative length following two relations hold first perfect matching minimum weight extreme second matching extreme p shortest maugmenting path phi p extreme also proof goes roughly follows suppose phi p extreme exists alternating cyclic path q cm phi p extreme must exist subset forms maugmenting path shorter p hence p shortest maugmenting path contradicts supposition two relations form basis many algorithms solving bipartite weighted matching problem start possibly empty extreme matching successively augment along shortest augmenting paths maximum perfect literature problem finding minimum weight perfect matching often stated following linear programming problem find matrix minimizing x subject x solution linear program one x ij 2 f0 1g exists permutation matrix x 1g minimum weight perfect matching edmonds karp 1972 kuhn 1955 furthermore minimum weight exist dual variables u v j using reduced weight matrix reduced weight cm matching equals reduced length lp malternating path p equals ij2p nm phi p matching reduced weight phi p equals thus finding shortest augmenting path graph gc equivalent finding augmenting path graph g c minimum reduced length edge contains alternating paths p negative length leading subpath p 0 p shortest augmenting paths weighted bipartite graph e obtained means shortest alternating path tree shortest alternating path tree alternating tree whose paths shortest path g node define length shortest path node root node path exists shortest alternating path tree every edge nodes j outline algorithm constructing shortest alternating path tree rooted column node j 0 given figure 41 reduced weights c ij nonnegative graph g c contains alternating paths negative length use sparse variant dijkstras algorithm dijkstra 1959 set row nodes partitioned three sets b q w b set marked nodes whose shortest alternating paths distances node j 0 known q set nodes alternating path root known necessarily shortest possible w set nodes alternating path exist known yet note since w defined implicitly v r n b q actually used figure 41 algorithm starts shortest alternating tree extends tree augmenting path found guaranteed shortest augmenting path respect current matching initially length shortest augmenting path lsap tree set infinity length shortest alternating path lsp root node q set zero pass main loop another column node j chosen closest root j 0 initially row node 2 colj whose shortest alternating path root known yet 62 b considered p j 0 ji shortest alternating path root node 0 node j length lsp extended edge j node j node length longer tentative shortest augmenting path tree length lsap need modify tree p j 0 ji length smaller lsap unmatched new shorter augmenting path found lsap updated matched p j 0 ji also shorter current shortest alternating path length shorter alternating path node found tree updated updated node visited previously moved q next q empty node 2 q determined closest root since weights c ij bipartite graph nonnegative cannot alternating path node shorter current one node marked adding b search continues column node j continues column nodes searched new augmenting path found whose length smaller current shortest one line lsap lsp original dijkstra algorithm intended dense graphs 2 complexity sparse problems complexity reduced log n implementing set q kheap nodes sorted increasing distance root see example tarjan 1983 gallo pallottino 1988 running time algorithm dominated operations heap q delete operations insert operations modification operations necessary time distance updated insert modification operation runs olog k n time delete operation runs ok log k n time consequently algorithm finding shortest augmenting path sparse bipartite graph run time n total run time sparse bipartite weighted algorithm n choose 2 algorithm uses binary heaps obtain time bound n log 2 n choose obtain bound log n n implementation heap q similar implementation proposed derigs metz 1986 q pair q array contains row nodes distance root shortest lsp separating nodes q closest root may reduce number operations heap especially situations cost matrix c different numerical values many alternating paths length deleting node q smallest see figure 41 consists choosing arbitrary element q 1 q 1 empty first move nodes q 2 closest root q 1 augmentation reduced weights c ij updated ensure alternating paths new g nonnegative length done modifying figure 41 construction shortest augmenting path true dnew dnew lsap unmatched lsap dnew isap else choose lsap lsp exit whileloop lsap 6 1 augment along path node isap node j dual vectors u v shortest alternating path tree constructed shortest augmenting path found u v j updated updated dual variables u v satisfy 43 new reduced weights c ij nonnegative running time weighted matching algorithm decreased considerably means cheap heuristic determines large initial extreme matching use strategy proposed carpaneto toth 1980 calculate inspecting sets colj column node j turn determine large initial matching edges remaining unmatched column node j every node 2 colj considered matched column node j say j unmatched row node found c replaced repeated unmatched columns search shortest augmenting paths starts respect current matching finally note weighted matching algorithm also used maximizing sum diagonal entries matrix instead maximizing product diagonal entries minimize 42 redefine matrix c 0 otherwise maximizing sum diagonal entries equal minimizing 42 since oe oe 5 bottleneck matching describe modification weighted bipartite matching algorithm previous section permuting rows columns sparse matrix smallest ratio absolute value diagonal entry maximum absolute value column maximized modification computes permutation oe maximizes min 1in oe j maximum absolute value column j matrix similarly previous section transform minimization problem define matrix j maximizing 51 equal minimizing 1in oe oe 1in given matching bipartite graph e bottleneck value defined ij2m problem find perfect maximum bottleneck matching cm minimal ie cm cm 0 possible maximum matchings 0 matching called extreme allow alternating cyclic path p cm phip bottleneck algorithm starts extreme matching initial bottleneck value b set cm pass main loop alternating tree constructed augmenting path p found either cm phi p cm phi p small possible initializations main loop constructing augmenting path figure 41 figure 51 shows innerloop weighted matching algorithm figure 41 modified case bottleneck objective function main differences sum operation path lengths figure 41 replaced max operation soon augmenting path p found whose length lsap less equal current bottleneck value b main loop exited p used augment b set maxb lsap bottleneck algorithm modify edge weights c ij similarly implementation discussed section 4 set q implemented array q 1 contains nodes whose distance root less equal tentative bottleneck value b q 2 contains nodes whose distance root larger bottleneck value infinity q 2 implemented heap figure 51 modified inner loop figure 41 construction bottleneck augmenting path dnew dnew lsap unmatched lsap dnew isap lsap b exit whileloop else large initial extreme matching found following way define smallest entry row column j respectively lower bound b 0 bottleneck value extreme matching obtained edges j c ij b 0 scan nodes turn node 2 colj unmatched added remaining unmatched column node j every node 2 colj considered c ij b matched column node j say j unmatched row node 1 2 colj 1 found c replaced done unmatched columns search shortest augmenting paths starts respect current matching initialization procedures found literature example slightly complicated initialization strategy used finke smith 1978 context solving transportation problems every use number admissible edges incident row node column node j respectively idea behind using g h j admissible edge j added admissible edges incident nodes j longer candidates added therefore method tries pick admissible edges number admissible edges become unusable minimal first row node minimal g determined set row admissible entry provided one exists chosen h j minimal j added deleting edges edges k j k 2 colj method repeats another row node 0 minimal g 0 continues admissible edges deleted graph finally note instead maximizing 51 also could maximized smallest absolute value diagonal maximize min 1in define matrix c note problem rather sensitive scaling matrix suppose example matrix column containing one nonzero entry whose absolute value v smallest absolute value present applying bottleneck algorithm bottleneck value b equal small value smallest entry diagonal permuted matrix maximized algorithm influence values diagonal values scaling matrix prior applying bottleneck algorithm avoids duff koster 1997 different approach taken obtain bottleneck matching let ffl denote matrix obtained setting zero entries ij ja ij denote matching obtained removing matching entries j ja ij throughout algorithm fflmax fflmin maximum matching size jm j exist fflmax exist fflmin step ffl chosen interval fflmin fflmax maximum matching matrix ffl computed using variant mc21 matching size jm j fflmin set ffl otherwise fflmax set ffl hence size interval decreases step ffl converge bottleneck value termination algorithm 0 computed bottleneck matching ffl corresponding bottleneck value 6 scaling olschowka neumaier 1996 use dual solution produced weighted matching algorithm scale matrix let u v satisfy relation 43 define equality holds words 1 ad 2 matrix whose diagonal entries one absolute value whose offdiagonal entries less equal one olschowka neumaier 1996 call matrix imatrix use context dense gaussian elimination reduce amount pivoting needed numerical stability dominant diagonal matrix higher chance diagonal entries stable enough serve pivots elimination iterative methods transformation matrix imatrix also interest example gershgorins theorem know union discs contains eigenvalues n theta n matrix disc k center ii radius equal sum absolute offdiagonal values row since diagonal entries imatrix one n disks center 1 estimate eigenvalues sharper deviates less diagonal matrix smaller radii discs better know eigenvalues situated able reduce radii discs imatrix ie reduce offdiagonal values tend cluster eigenvalues around one ideal case discs imatrix radius smaller one case matrix strictly rowwise diagonally dominant guarantees many types iterative methods converge exact even simple ones like jacobi gaussseidel method however least one disc remains radius larger close one zero eigenvalues small eigenvalues possible straightforward expensive attempt decrease large offdiagonal entries matrix row column equalization olschowka neumaier 1996 let imatrix define matrix simplicity assume contains zero entries equalization consists repeatedly equalizing largest absolute value row largest absolute values column k n thus define 1 algorithm minimizes largest offdiagonal absolute value matrix 1 ad 2 diagonal entries change note scaling strategy guarantee offdiagonal entries imatrix smaller one absolute value example imatrix contains two offdiagonal entries kl lk k 6 l whose absolute values one 7 experimental results section discuss several cases reorderings algorithms previous section useful include solution sparse equations direct method iterative technique also consider use generating preconditioner iterative method set matrices used experiments unsymmetric matrices taken harwellboeing sparse matrix test collection duff grimes lewis 1992 sparse matrix collection university florida davis 1997 matrices initially row column scaled mean matrix scaled maximum entry row column one computer used experiments sun ultrasparc 256 mbytes main memory algorithms implemented fortran 77 use following acronyms mc21 matching algorithm harwell subroutine library computing matching corresponding permuted matrix zero freediagonal see section 3 bt bottleneck bipartite matching algorithm section 5 permuting matrix smallest ratio absolute value diagonal entry maximum absolute value column maximized bt bottleneck bipartite matching algorithm duff koster 1997 mpd weighted matching algorithm section 4 computes permutation product diagonal entries permuted matrix maximum absolute value mps equal mpd algorithm permutation matrix scaled imatrix see section 6 table 71 shows large sparse matrices order number entries time algorithms compute matching times mps listed almost identical mpd general mc21 needs least time compute matching except onetone twotone matrices matrices search heuristic used mc21 depthfirst search lookahead perform well probably caused ordering columns variables entries inside columns matrix random permutation matrix prior applying mc21 might lead results clear winner bottleneck algorithms bt bt although note bt requires entries inside columns sorted value sorting expensive relatively dense matrices mpd general expensive algorithm explained selective way algorithm constructs augmenting paths 71 experiments direct solution method direct methods putting large entries diagonal suggests pivoting diagonal might stable indeed stability still guaranteed solution scheme like multifrontal method duff reid 1983 symbolic phase chooses initial pivotal sequence subsequent factorization phase modifies sequence stability mean modification required less permutation applied multifrontal approach duff reid 1983 later developed amestoy duff 1989 analysis performed structure aa obtain ordering reduces fillin assumption diagonal entries numerically suitable pivoting numerical factorization guided assembly tree node tree steps gaussian elimination performed dense submatrix whose schur complement passed parent node tree assembled table 71 times seconds matching algorithms order matrix n number entries matrix goodwin 7320 324784 027 226 417 182 summed schur complements children original entries matrix however numerical considerations prevent us choosing pivot algorithm proceed schur complement passed parent larger usually work storage needed effect factorization logic first permuting matrix large entries diagonal computing ordering reduce fillin try reduce number pivots delayed way thereby reducing storage work factorization show effect table 72 see even using mc21 beneficial although algorithms show significant gains table 73 show effect number entries factors clearly mirrors results table 72 addition able select pivots chosen analysis phase multifrontal code ma41 better matrices whose structure symmetric nearly define structural symmetry matrix number entries ij ji also entry divided total number entries structural symmetry permutations shown table 74 matching orderings cases increase symmetry resulting reordered matrix particularly apparent sparse system many zeros diagonal case reduction number offdiagonal entries reordered matrix influence symmetry notice respect sophisticated matching algorithms may actually cause problems since could reorder symmetrically structured matrix zerofree diagonal whereas mc21 leave unchanged table 72 number delayed pivots factorization ma41 indicates ma41 needed 200 mbytes memory matrix matching algorithm used none mc21 bt mpd mps goodwin 536 1622 427 53 41 table 73 number entries 10 3 factors ma41 matrix matching algorithm used none mc21 bt mpd mps onetone2 14083 2876 2298 2170 2168 goodwin 1263 2673 2058 1282 1281 table 74 structural symmetry permutation matrix matching algorithm used none mc21 bt mpdmps gemat11 finally table 75 shows effect solution times ma41 sometimes observe dramatic reduction time solution preceded permutation table 75 solution time required ma41 matrix matching algorithm used none mc21 bt mpd mps goodwin 364 1463 798 356 356 implementations algorithms described paper used successfully li demmel 1998 stabilize sparse gaussian elimination distributedmemory environment without need dynamic pivoting method decomposes matrix n theta n block matrix a1 using notion unsymmetric supernodes demmel eisenstat gilbert li liu 1995 blocks mapped cyclically row column dimensions onto nodes processors twodimensional rectangular processor grid mapping step k numerical factorization column processors factorizes block column ak n k row processes participates triangular solves obtain block row u processors participate corresponding multiplerank update remaining numerical factorization phase method use dynamic partial pivoting block columns allows priori computation nonzero structure factors distributed data structures communication pattern good load balancing scheme makes factorization scalable distributedmemory machines factorizations computational communication tasks become apparent elimination process ensure solution numerically stable matrix permuted scaled factorization make diagonal entries large compared offdiagonal entries tiny pivots encountered factorization perturbed steps iterative refinement performed triangular solution phase solution accurate enough numerical experiments demonstrate method using implementation mps algorithm stable partial pivoting wide range problems 72 experiments iterative solution methods iterative methods simple techniques like jacobi gaussseidel converge quickly diagonal entry large relative offdiagonals row column techniques like block iterative methods benefit entries diagonal blocks large additionally preconditioning techniques example diagonal preconditioning incomplete lu preconditioning intuitively evident large diagonals beneficial 721 preconditioning incomplete factorizations incomplete factorization preconditioners pivots often taken diagonal fillin discarded falls outside prescribed sparsity pattern see saad 1996 overview incomplete factorizations used resulting factors economical store compute solve one reasons incomplete factorizations behave poorly pivots arbitrarily small benzi szyld van duin 1997 chow saad 1997 pivots may even zero case incomplete factorization fails small pivots allow numerical values entries incomplete factors become large leads unstable therefore inaccurate factorizations cases norm residual u large l u denote computed incomplete way improve stability incomplete factorization preorder matrix put large entries onto diagonal obviously successful factorization still cannot guaranteed nonzero diagonal entries may become small even zero factorization reordering may mean zero small pivots less likely occur table 76 shows results reorderings applied prior incomplete factorizations form ilu0 ilu1 ilut iterative methods gmres20 bicgstab qmr cases method converge permutation others greatly improves convergence however emphasize permuting large entries diagonal matrix always improve accuracy stability incomplete factorization inaccurate factorization also occur absence small pivots many especially large fillins dropped incomplete factors respect may beneficial apply symmetric permutation matching reordering reduce fillin another kind instability incomplete factorizations occur without small pivots severe illconditioning triangular factors situation jjrjj f need large jji gamma also common situation coefficient matrix far diagonally dominant table 76 number iterations required preconditioned iterative methods permutation matrix method matching algorithm qmr 72 21 12 12 mahindas west0497 also performed set experiments first permuted columns matrix using reordering computed one matching algorithms followed symmetric permutation generated reverse cuthillmckee ordering cuthill mckee 1969 applied motivation behind number entries dropped factors reduced applying reordering matrix reduces fillin experimental results noticed additional permutation sometimes positive well negative effect performance iterative solvers table 77 shows results three iterative methods table 76 preconditioned ilut west matrices harwellboeing collection table 77 number iterations required ilutpreconditioned iterative methods matching reordering without reverse cuthillmckee matrix method matching algorithm matching algorithm without rcm rcm 722 experiments block iterative solution method jacobi method particularly current powerful method focussed experiments block cimmino implementation arioli duff noailles ruiz 1992 equivalent using block jacobi algorithm normal equations implementation subproblems corresponding blocks rows matrix solved sparse direct method ma27 hsl 1996 show effect table 78 problem mahindas table 76 matching algorithm followed reverse cuthillmckee algorithm obtain block tridiagonal form matrix partitioned 2 4 8 16 blocks rows accelerations used block cg algorithms block sizes 1 4 8 block rows chosen equal nearly equal size table 78 number iterations block cimmino algorithm matrix mahindas acceleration matching algorithm block rows none mc21 bt mpd mps general noticed experiments block cimmino method often sensitive scaling mps less reorderings convergence properties block cimmino method independent row scaling however sparse direct solver ma27 hsl 1996 used solving augmented systems performs numerical pivoting factorizations augmented matrices row scaling might well change choice pivot order affect fillin factors accuracy solution column scaling affect convergence method since considered diagonal preconditioner details see ruiz 1992 8 conclusions future work considered sections 34 techniques permuting sparse matrix diagonal permuted matrix entries large absolute value discussed various criteria considered implementation computer codes also considered section 6 possible scaling strategies improve weight diagonal respect offdiagonal values section 7 indicated several cases permutation scaling useful include solution sparse equations direct method iterative technique also considered use generating preconditioner iterative method numerical experiments show multifrontal solver preconditioned iterative methods effect reorderings dramatic effect block cimmino iterative method seems less dramatic method discussed scaling tends important effect clear reordering matrices permuted matrix large diagonal significant effect solving sparse systems wide range techniques somewhat less clear universal strategy best cases one reason increasing size diagonal always sufficient improve performance method example incomplete preconditioners used numerical experiments section 7 size diagonal also amount size discarded fillin plays important role thus started experimenting combining strategies mentioned sections 34 particularly generating preconditioner block cimmino approach combining unsymmetric ordering symmetric orderings another interesting extension discussed reorderings block approach increase size diagonal blocks instead diagonal entries use example block jacobi preconditioner permuted matrix particular interest block cimmino method one could also build criteria weighting obtaining bipartite matching example incorporate markowitz cost sparsity would also preserved choice resulting diagonal pivot combination would make resulting ordering suitable wider class sparse direct solvers finally notice experiments ma41 one effect matching algorithm increase structural symmetry unsymmetric matrices exploring use ordering techniques directly attempt increase structural symmetry acknowledgments grateful michele benzi los alamos national laboratory miroslav tuma czech academy sciences assistance preconditioned iterative methods daniel ruiz enseeiht help block iterative methods r orderings incomplete factorization preconditioning nonsymmetric problems assignment matching problems solution methods fortranprograms experimental study ilu preconditioners indefinite matrices reducing bandwidth sparse symmetric matrices university florida sparse matrix collection supernodal approach sparse partial pivoting design use algorithms permuting large entries diagonal sparse matrices users guide harwellboeing sparse matrix collection release 1 making sparse gaussian elimination scalable static pivoting solution large sparse unsymmetric linear systems block iterative method multiprocessor environment iterative methods sparse linear systems data structures network algorithms tr ctr laura grigori xiaoye li new scheduling algorithm parallel sparse lu factorization static pivoting proceedings 2002 acmieee conference supercomputing p118 november 16 2002 baltimore maryland abdou guermouche jeanyves lexcellent gil utard impact reordering memory multifrontal solver parallel computing v29 n9 p11911218 september abdou guermouche jeanyves lexcellent constructing memoryminimizing schedules multifrontal methods acm transactions mathematical software toms v32 n1 p1732 march 2006 chi shen jun zhang kai wang distributed block independent set algorithms parallel multilevel ilu preconditioners journal parallel distributed computing v65 n3 p331346 march 2005 kai shen parallel sparse lu factorization secondclass message passing platforms proceedings 19th annual international conference supercomputing june 2022 2005 cambridge massachusetts patrick r amestoy iain duff jeanyves lexcellent xiaoye li analysis comparison two general sparse solvers distributed memory computers acm transactions mathematical software toms v27 n4 p388421 december 2001 kai shen parallel sparse lu factorization different message passing platforms journal parallel distributed computing v66 n11 p13871403 november 2006 xiren wang wenjian yu zeyi wang xianlong hong improved direct boundary element method substrate coupling resistance extraction proceedings 15th acm great lakes symposium vlsi april 1719 2005 chicago illinois usa anshul gupta recent advances direct methods solving unsymmetric sparse systems linear equations acm transactions mathematical software toms v28 n3 p301324 september 2002 timothy davis column preordering strategy unsymmetricpattern multifrontal method acm transactions mathematical software toms v30 n2 p165195 june 2004 michele benzi preconditioning techniques large linear systems survey journal computational physics v182 n2 p418477 november 2002