variable resolution discretization optimal control problem state abstraction central importance optimal control reinforcement learning markov decision processes paper studies case variable resolution state abstraction continuous time space deterministic dynamic control problems nearoptimal policies required begin defining class variable resolution policy value function representations based kuhn triangulations embedded kdtrie consider topdown approaches choosing cells split order generate improved policies core paper introduction evaluation wide variety possible splitting criteria begin local approaches based value function policy properties use features individual cells making split choices later introducing two new nonlocal measures influence variance derive splitting criteria allow one cell efficiently take account impact cells deciding whether split influence efficientlycalculable measure extent changes state effect value function states variance efficientlycalculable measure risky state markov chain low variance state one would surprised one execution longterm reward attained state differed substantially expected value given value functionthe paper proceeds graphically demonstrating various approaches splitting familiar nonlinear nonminimum phase two dimensional problem car hill evaluates performance variety splitting criteria many benchmark problems paying careful attention numberofcells versus closenesstooptimality tradeoff curves b introduction paper nonuniform discretization state spaces finding optimal controllers continuous time space markov processes uniform discretizations generally based finite element finite difference techniques kushner dupuis 1992 provide us important convergence results see analytical approach barles souganidis 1991 crandall ishii li ons 1992 crandall lions 1983 munos 1999 probabilistic results kushner dupuis 1992 dupuis james 1998 suffer impractical computational requirements size discretization step small especially state space high dimensional hand approximation methods bertsekas tsitsiklis 1996 baird 1995 sutton 1996 handle high emi munos andrew moore dimensionality general guarantee convergence optimal solution boyan moore 1995 baird 1995 munos 1999 local convergence results gordon 1995 baird 1998 paper try keep convergence properties discretized methods introducing approximation factor iterative designing variable resolution paper consider general towards specific approach initial coarse grid successively refined areas state space using splitting process desired approximation value function optimal control reached first implement two splitting criteria based value function see section 6 define criterion inconsistency value function policy see section 7 order define effect splitting state others states define section 8 notion influence estimate expected gain approximation value function splitting states defining section 9 variance markov chain combining two notions deduce given discretization states whose splitting mostly influence parts state space change optimal control leading increase resolution important areas illustrate different splitting criteria car hill problem section 11 show results control problems including well known 4 dimensions cartpole acrobot problems paper make assumption model dynamics reinforcement function moreover assume dynamics deterministic 2 description optimal control problem consider discounted deterministic control problems include wellknown reinforcement learning benchmarks car hill moore 1991 cartpole barto sutton anderson 1983 acrobot sutton 1996 let xt 2 x state system state space x compact subset ir evolution state depends control ut 2 u control space u finite set discrete actions differential equation called state dynamics dx dt initial state x control function ut equation leads unique trajectory xt let exit time state space convention xt always stays x 1 define gain j discounted cumulative reinforcement rx u current reinforcement rx boundary reinforcement fl discount factor 0 1 convenience reasons follows assume fl 1 however results apply undiscounted case assuming control ut trajectories loop ie variable resolution discretization optimal control 3 objective control problem find initial condition x control u optimizes functional j use method dynamic programming dp introduces value function vf maximum j function initial state x following dp principle prove fleming soner 1993 v satisfies firstorder nonlinear differential equation called hamiltonjacobibellman theorem 1 v differentiable x 2 x let dv x gradient v x following hjb equation holds x u2u dp computes vf order define optimal control feedback control policy u optimal control u time depends current state xt u xt indeed value function deduce following optimal feedback control policy 3 discretization process order discretize continuous control problem described previous sec tion use process based finite element methods kushner dupuis 1992 use class functions known barycentric interpolators munos moore 1998 built triangulation statespace functions piecewise linear inside simplex might discontinuous boundary two simplexes representation chosen fast computational properties description class functions statespace discretized variable resolution grid using structure tree root tree covers whole state space supposed hyper rectangle two branches divide state space two smaller rectangles means hyperplane perpendicular chosen splitting dimension way node except leaf ones splits direction 1d rectangle covers middle two nodes half area see figure 1 kind structure known kd trie knuth 1973 special kind kdtree friedman bentley finkel 1977 splits occur center every cell every leaf consider coxeterfreudenthalkuhn triangulation simply kuhn triangulation moore 1992 dimension 2 figure 1b rectangle composed 2 triangles dimension 3 see figure 2 composed 6 pyramids dimension simplexes interpolated functions consider defined values corners rectangles use kuhn triangulation linearly interpolate inside rectangles thus functions piecewise linear continuous inside rectangle may discontinuous boundary two rectangles emi munos andrew moore b corresponding tree example discretization figure 1 example discretization state space 12 rectangles 24 corners dots b corresponding tree structure area covered node indicated gray level implement kuhn triangulation every leaf remark going approximate value function v piecewise linear functions easy compute gradient dv almost point state space thus making possible use feedback rule 4 deduce corresponding optimal control x x figure 2 kuhn triangulation 3d rectangle point x simplex 31 computational issues although number simplexes inside rectangle factorial dimension computation time interpolating value point inside rectangle order ln corresponds sorting relative coordinates point inside rectangle assume want compute indexes vertices simplex containing point defined relative coordinates respect rectangle whose corners f dg indexes corners variable resolution discretization optimal control 5 uses binary decomposition dimension illustrated figure 2 computing indexes achieved sorting coordinates highest smallest exist indices permutation f0 gamma 1g 1 x j0 vertices simplex containing point 1 example coordinates illustrated point x figure 2 vertices 0 every simplex vertex well 2 added corresponding barycentric coordinates point inside simplex previous example barycentric coordinates approach using kuhn triangulations interpolate value function introduced reinforcement learning literature davies 1997 32 building discretized mdp given discretization build corresponding markov decision process mdp following way state space mdp set xi corners tree control space finite set u every corner 2 xi control u 2 u approximate part corresponding trajectory xt euler rungekuta method integrating state dynamics 1 initial state constant control u time u enters inside new rectangle point j u see figure 3 time also compute integral current reinforcement r u defines reinforcement function mdp compute vertices simplex containing j u corresponding barycentric coordinates 0 j u j u probabilities transition mdp state control u states defined barycentric coordinates see figure j u thus dp equation corresponding mdp j uv integrating 1 initial state control u trajectory exits state space time u u lead terminal state ie satisfying p reinforcement r u 33 resolution discretized mdp use classical methods solve discretized mdp ie value iteration policy iteration modified policy iteration puterman 1994 bertsekas emi munos andrew moore x x figure 3 according current variable res olution grid build discrete mdp every corner state mdp every control u integrate corresponding trajectory enters new rectangle j u interpolated value j u linear combination values vertices simplex furthermore linear combination positive coefficients sum one thus interpolation mathematically equivalent probabilistically jumping vertex probabilities transition mdp state control u states f g i02 barycentric coordinates j u 1987 barto bradtke singh 1995 prioritized sweeping moore atke son 1993 4 example car hill control problem description dynamics problem see moore atkeson 1995 problem dimension 2 experiments chose reinforcement functions follows current reinforcement rx u zero everywhere terminal reinforcement rx gamma1 car exits left side state space varies linearly 1 gamma1 depending velocity car exits right side state space best reinforcement 1 occurs car reaches right boundary null velocity figure 4 control u 2 possible values maximal positive negative thrust figure 6 represents interpolated value function mdp obtained regular discretization 257 257 states observe following distinctive features value function ffl discontinuity vf along frontier 1 see figure 6 results fact given initial point situated frontier optimal trajectory stays inside state space eventually leads positive reward value function point positive whereas initial point frontier control lead car exit left boundary initial velocity negative thus corresponding value function negative see optimal trajectories figure 5 observe change optimal control around frontier ffl discontinuity gradient vf along upper part frontier 2 results change optimal control example point frontier 2 reach directly top hill whereas point frontier go one loop gain enough velocity reach top see figure 5 moreover observe around lower part variable resolution discretization optimal control 7 goal thrust gravitation resistance reinforcement r1 null velocity r1 max velocity figure 4 car hill control problem frontier 3 frontier 2 upper part frontier 2 lower part goal position velocity figure 5 optimal policy indicated different gray levels several optimal trajectories drawn different initial starting points frontier 2 see figures 6 visible discontinuity vf despite fact change optimal control ffl discontinuity gradient vf along frontier 3 change optimal control frontier car accelerates order reach reward fast possible whereas decelerates reach top hill lowest velocity receive highest reward figure 6 value function caronthehill problem obtained regular grid 257 states frontier 1 illustrates discontinuity vf frontiers 2 3 dash lines stands change optimal control emi munos andrew moore deduce observations good approximation value function necessarily mean good approximation optimal control since ffl approximation value function sufficient predict change optimal control around lower part frontier 2 ffl good approximation value function necessary around frontier 1 since change optimal control 5 variable resolution approach idea start initial coarse discretization build corresponding mdp solve order coarse approximation value function locally refine discretization splitting cells according process 1 score cell direction according promising split according measure called splitcriterioni 2 pick top f f parameter highest scoring cells 3 split along direction given argmax splitcriterioni use dynamics reward model create new larger discretized mdp see splitting process figure 7 note cells split whose successive states involve split cell need state transition recomputed 4 go step 1 estimate approximation value function optimal control precise enough thus central purpose paper study several splitting criteria figure 7 several discretizations resulting successive splitting operations remark paper consider general towards specialized process sense discretization always refined could also consider generalization process example tree coding discretization could pruned order avoid non relevant partitioning small subsets follows present several local splitting criteria illustrate resulting discretizations previous car hill control problem variable resolution discretization optimal control 9 6 criteria based value function 61 first criterion average cornervalue difference every rectangle compute average absolute difference values corners edges directions us denote avei criterion direction example consider square described figure 2 splitcriterion figure 8 represents discretization obtained 15 iterations procedure starting 9 9 initial grid using cornervalue difference criterion splitting rate 50 rectangles iteration figure 8 discretization state space car hill problem using cornervalue difference criterion figure 9 discretization state space car hill problem using value nonlinearity criterion 62 second criterion value nonlinearity every rectangle compute variance absolute increase values corners edges directions criterion similar previous one except computes variance instead average figure 9 shows corresponding discretization using value nonlinearity criterion splitting rate 50 15 iterations comments results ffl observe cases splitting occurs around frontiers 1 3 upper part frontier 2 previously defined fact first criterion leads reduce variation values splits wherever value function constant figure 10ab shows 1dimension cut discontinuity emi munos andrew moore corresponding discretization approximation obtained using corner value difference split criterion ffl value nonlinearity criterion leads reduce change variation values thus splits wherever value function linear criterion also concentrate irregularities two important differences compared cornervalue difference criterion value nonlinearity criterion splits parsimoniously corner value difference see example difference splitting area frontier 3 discretization resulting split discontinuity corner value difference value nonlinearity criteria different see figure 10 value nonlinearity criterion splits approximated function kind sigmoid function whose slope depends density resolution least linear figure 10cd explains 2 parallel tails observed around frontiers mainly right part frontier 1 figure 9 ffl refinement process split around bottom part frontier 2 although change optimal control vf almost constant area moreover huge amount memory spent approximation discontinuity frontier 1 although optimal control constant area average split criterion coarse resolution b average split criterion dense resolution c variance split criterion coarse resolution variance split criterion dense resolution figure 10 discretization around discontinuity resulting cornervalue difference ab value nonlinearity cd split criterion coarse ac dense bd resolution thus wonder really useful split much around frontier 1 knowing result improved policy next section introduces new criterion takes account policy variable resolution discretization optimal control 11 7 criterion based policy figure 5 shows optimal policy several optimal trajectories different starting points would like define refinement process could refine around areas change optimal control around frontier 2 upper lower parts 3 around frontier 1 follows propose criterion based inconsistency control derived value function policy 71 policy disagreement criterion solve mdp compute value function dp equation 5 deduce following policy state 2 xi compare optimal control law 4 derived gradient policy disagreement criterion compares control derived local gradient v 4 control derived policy mdp 6 remark instead computing gradient dv simplexes rectangles compute approximated gradient dv 2 corners based finite difference quotient example figure 2 approximated gradient corner 0 thus every corner compute approximated gradient corresponding optimal control 4 compare optimal policy given 6 figure 11 shows discretization obtained splitting rectangles two measures optimal control diverge criterion interesting since splits places change optimal control thus refining resolution important parts state space approximation optimal control however expect use criterion value function well approximated thus process may converge suboptimal performance indeed observe figure 11 bottom part frontier 2 lightly situated higher optimal position illustrated figure 5 results underestimation value function area lack precision around discontinuity frontier 1 section 73 observe performance discretization resulting splitting criterion relatively weak however splitting criterion beneficially combined previous ones based approximation vf 72 combination several criteria combine policy disagreement criterion cornervalue difference value nonlinearity criterion order take advantages methods good approximation value function whole state space increase resolution around areas change optimal control combine emi munos andrew moore figure 11 discretization state space using policy disagreement criterion used initial grid 33 theta 33 splitting rate 20 figure 12 discretization state space car hill problem using combination value nonlinearity policy disagreement criterion previous criteria several ways example weighted sum respective criteria logical operation split andor combination criteria satisfied ordering criteria first split one criterion use one etc figure 12 shows discretization obtained alternatively iterations using value nonlinearity criterion obtain good approximation value function policy disagreement criterion increasing accuracy around area change optimal control 73 comparison performance order compare respective performance discretizations ran set 256 optimal trajectories using feedback control law 4 starting initial states regularly situated state space performance discretization sum cumulated reinforcement gain defined equation 2 obtained trajectories set start positions figure 13 shows respective performances several splitting criteria function number states 2 dimensional control problem variable resolution approach perform much better except policy disagreement criterion alone uniform grids however see later higher dimensional problems ressources allocated approximate discontinuities vf areas useful improving optimal control might prohibitely high better far considered local splitting criteria sense decide variable resolution discretization optimal control 13 figure 13 performance uniform versus variable resolution grids several splitting criterion cornervalue difference value nonlinearity splitting processes perform better uniform grids policy disagreement splitting good small number states improve thus leads suboptimal performance policy disagreement combined value nonlinearity gives best performances whether split rectangle according information value function policy relative rectangle however effect splitting local influence whole state space thus try see possible find better refinement process could split area useful improve performance sections follow presents two notions useful defining global splitting criterion influence measures extend local changes state effect global vf variance measures accurate current approximated vf 8 notion influence let us consider markov chain resulting choice control optimal policy u mdp convenience reasons let us denote 81 intuitive idea intuitive idea influence j state another state give measure extend vf state contributes vf state ie change vf resulting modification vf vf solution bellman equation depends structure markov chain reinforcement values thus cannot modify value state without violating bellman equation however notice value function state affected linearly reinforcement obtained state thus compute contribution state state estimating change vf resulting modification reinforcement r emi munos andrew moore 82 definition influence let us define discounted cumulative kgammachained probabilities p k j represent sum discounted transition probabilities sequences k states definition 1 let 2 xi define influence state state quantity sigma subset xi define influence state subset sigma quantity call influencers state respectively subset sigma set states nonzero influence resp sigma note definition influences nonnegative 83 properties influence first notice times 0 influence well defined bounded indeed definition discounted chainedprobabilities moreover relate definition intuitive idea previously stated following properties hold ffl influence j partial derivative v r ffl states proof bellman equation applying bellman equation v definition p 2 rewrite apply bellman equation v finally deduce thus contribution r v v variable resolution discretization optimal control 15 property 8 easily deduced definition influence chained probability property 7 since 84 computation influence equation 8 bellman equation since sum probabilities may greater 1 cannot deduce successive iterations n1 converge influence using classical contraction property maxnorm puterman 1994 however following property thus denoting ixij vector whose components j introducing 1norm jjixijjj deduce contraction property 1norm insures convergence iterated n j unique solution fixed point j 8 85 illustration car hill problem subsetomegagamma compute influencers exam ple figure 14 shows influencers 3 points figure 14 influencers 3 points crosses darker gray level important influence notice influence state follows direction optimal trajectory starting state see figure 5 kind diffusion process emi munos andrew moore let us define subset sigma states policy disagreement sense section 71 figure 15a shows sigma regular grid 129 theta 129 influencers sigma computed plotted figure 15b darkest zones figure 15b places whose splitting affect value function places illustrated figure 15a change optimal control states policy disagreement b influence states figure 15 set states policy disagreement influencers b would like define areas whose refinement could lead highest quantitative change value function closely related quality approximation value function given discretization indeed better approximation lower change value function may result splitting following section introduce variance markov chain order estimate quality approximation vf current discretization thus defining areas whose splitting may lead highest change vf 9 variance markov chain using notation previous section bellman equation states gammar discounted average next values weighted probabilities transition p j interested computing variance next values order get estimation range values averaged v first idea compute onestepahead variance e v theta however values v also average successive values v j would like variance also takes account secondstepahead average well following ones next sections define value function averager reinforcements present definition variance markov chain variable resolution discretization optimal control 17 91 value function averager reinforcements let us denote k sequence first one let k set possible sequences k let ps k product probabilities transition successive states sequence cumulated time th first states sequence definition 0 k following property sk 2sk ps k 1 prove value function satisfies following equation similar bellman equation kstepsahead k sk 2sk ps k let us denote 1 infinite sequence states starting s1 set possible sequences define ps 1 defined previously 0 still property value function satisfies 92 definition variance markov chain intuitively variance state measure dissimilar cumulative future reward obtained along possible trajectories starting precisely define variance quantities averaged equation definition 2 let 2 xi define variance oe 2 markov chain let us prove variance satisfies bellman equation selecting case summation theta p 1 rand 11 deduce theta theta successively applying 11 1 selecting state sequence theta theta p 1 theta emi munos andrew moore e satisfying 10 thus variance oe 2 sum immediate contribution e takes account variation values immediate successors discounted average variance oe 2 successors bellman equation solved value iteration remark give geometric interpretation term e related gradient value function iterated point barycentric coordinates j indeed definition discretized mdp section 32 v j piecewise linearity approximated functions v thus jfl 2 dv j expressed matrix qj defined elements thus e close 0 two specific cases either gradient iterated point j low ie values almost constant j close one vertex barycentric coordinate close 1 j close 0 thus qj low cases e low implies iteration lead degradation quality approximation value function variance increase 93 example variance car hill figure shows standard deviation oe caronthehill problem uniform grid 257 257 figure 16 standard deviation oe car hill standard deviation high around indeed discontinuity impossible ap proximateperfectly discretization tech niques whatever resolution observe fact figure 10 maximal error approximation equal half step discontinuity ever higher resolution lower integral error compare figure 10ac versus bd noticeable positive standard deviation around frontier 3 upper part value function average different values discounted terminal reinforcement refinement resolution areas standard deviation low chance producing important change value function thus appears areas splitting might affect approximation value function rectangles highest surface whose corners highest standard deviations variable resolution discretization optimal control 19 10 global splitting criterion going combine notions influence variance order define new nonlocal splitting criterion seen ffl states highest standard deviation oe states lowest quality approximation vf thus states could improve approximation accuracy split figure 17a ffl states highest influence set sigma states policy disagreement figure 15b states whose value function affects area change optimal control thus order improve precision approximation relevant areas state space split states highest standard deviation influence areas change optimal control according stdev inf criterion see figure figure shows discretization obtained using splitting criterion standard deviation b influence x standard deviation figure 17 standard deviation oe car hill equivalent figure 16 b stdev inf criterion product oe influence ijsigma figure 15b figure 18 discretization resulting stdev inf split criterion observe upper part frontier 1 well refined refinement occurred split according value function cornervalue difference value nonlinearity criterion splitting necessary good approximation value function around bottom part even upper part frontier 2 change optimal control fact stdev inf criterion split areas vf discontinuous unless refinement necessary get better approximation optimal control important since see simulations follow higher dimensions cost get accurate approximation vf high emi munos andrew moore remark performance criterion car hill problem similar combining value nonlinearity policy disagreement criterion didnt plot performances figure 13 clarity reasons represent major improvement however difference performances local criteria stdev inf criterion much significant case difficult problems acrobot cartpole illustrated follows 11 illustration control problems 111 cartpole problem dynamics 4dimensional physical system illustrated figure 19a described barto et al 1983 experiments chose following parameters follows state space defined position 2 gamma10 10 angle velocities restricted 2 control consists applying strength sigma10 newton goal defined area limits notably narrow goal try hit see projection state space goal 2d plan figure 19 notice task minimum time manoever small goal region arbitrary start state much harder merely balancing pole without falling barto et al 1983 current reinforcement r zero everywhere terminal reinforcement r gamma1 system exits state space jyj 10 jj system reaches goal position 43 02 goal b projection state space cartpole figure 19 description cartpole b projection discretization onto plane obtained stdev inf criterion trajectories several initial points figure 20 shows performance obtained several splitting criteria previously defined 4 dimensional control problem observe following points variable resolution discretization optimal control 21 ffl local criteria perform better uniform grids problem vf discontinuous several parts state space areas high jj late rebalance pole similar frontier 1 car hill problem valuebased criteria spend many resources approximating useless areas ffl stdev inf criterion performs well observe trajectories see figure 19b nearly optimal angle jj maximized order reach goal fast possible close limit value possible recover balance figure 20 performance cartpole figure 21 performance acrobot 112 acrobot acrobot 4 dimensional control problem consists twolink arm one single actuator elbow actuator exerts torque links see figure 22a dynamics similar gymnast high bar link 1 analogous gymnasts hands arms torso link 2 represents legs joint links gymnasts waist sutton 1996 goal controller balance acrobot unstable inverted vertical position minimum time boone 1997 goal defined narrow range angles around vertical position 22b system receives reinforcement r 1 anywhere else reinforcement zero two first dimensions state space structure torus 2 modulo angles implemented structure vertices 2 first dimensions angle 0 2 pointing entry value function interpolated kdtrie figure 21 shows performance obtained several splitting criteria previously defined respective performance different criteria similar cart pole problem local criteria better uniform grids stdev inf criterion performs much better figure 22b shows projection discretization obtained stdev inf criterion one trajectory onto 2dplane 1 2 22 r emi munos andrew moore goal goal acrobot b projection state space figure 22 description acrobot physical system b projection discretization onto plane 1 2 obtained stdev inf criterion one trajectory interpretation results noticed two previous 4d prob lems local splitting criteria fail improve performance uniform grids spend many resources local considerations either approximating value function optimal policy example cartpole problem value nonlinearity criterion concentrate approximating vf mostly parts state space already chance rebalance pole areas around vertical position low important areas refined time however continue simulations 90000 states local criteria start perform better uniform grids areas get eventually refined stdev inf criterion takes account global consideration splitting performs well problems described 12 conclusion future work paper proposed variable resolution discretization approach solve continuous time space control problems described several local splitting criteria based vf policy approximation observed approach works well 2d problems like car hill however complex problems local methods fail perform better uniform grids local valuebased splitting efficient modelbased relative qlearning based tree splitting criteria used example chapman kaelbling 1991 si mons van brussel de schutter verhaert 1982 mccallum 1995 combined new nonlocal measures able get truly effec tive nearoptimal performance control problems treebased statespace partitions moore 1991 moore atkeson 1995 produced different criteria empirical performance produced far parsimonious trees attempt made minimize cost merely find valid path variable resolution discretization optimal control 23 order design global criterion introduced notions influence estimates impact states others variance markov chain measure quality current approximation combining notions defined interesting splitting criterion gives good performance comparison uniform grids problems studied another extension measures could learn interactions environment order design efficient exploration policies reinforcement learning notion variance could used interval estima tion heuristic kaelbling 1993 permit optimisminthefaceofuncertainty exploration backpropagation exploration bonuses meuleau bourgine 1999 exploration continuous statespaces indeed observe learned variance state high good exploration strategy could inspect states high expected influence future seems important develop following points ffl generalization process order also specific towards general grouping areas example pruning tree overrefined ffl suppose want solve problem specific areaomega initial start states restrict refinement process areas used trajectories notion influence introduced paper used purpose computing stdev inf criterion respect set states policy disagreement influence area initial statesomegagamma instead sigma improve drastically performance observed starting specific area ffl would like deal stochastic case assume model noise change process building mdp kushner dupuis 1992 munos bourgine 1997 ffl release assumption model build approximation dynamics reinforcement deal exploration problem r residual algorithms gradient descent general reinforcement learning convergence approximation schemes fully nonlinear second order equations neuronlike adaptive elements learn difficult control problems learning act using realtime dynamic program ming dynamic programming learning delayed reinforcement complex domain users guide viscosity solutions second order partial differential equations viscosity solutions hamiltonjacobi equations multidimensional triangulation interpolation reinforcement learning rates convergence approximation schemes optimal control siam journal control optimization controlled markov processes viscosity solutions algorithm finding best matches logarithmic expected time stable function approximation dynamic programming learning embedded systems sorting searching machine learning exploration multistate environments local measures backpropagation uncertainty variable resolution dynamic programming efficiently learning action maps multivariate realvalued statespaces prioritized sweeping reinforcement learning less data less real time partigame algorithm variable resolution reinforcement learning multidimensional state space simplical mesh generation applications study reinforcement learning continuous case means viscosity solutions reinforcement learning continuous stochastic control problems neural information processing systems barycentric interpolators continuous space time reinforcement learning markov decision processes generalization reinforcement learning tr ctr david wingate kevin seppi p3vi partitioned prioritized parallel value iterator proceedings twentyfirst international conference machine learning p109 july 0408 2004 banff alberta canada chee wee phua robert fitch tracking value function dynamics improve reinforcement learning piecewise linear function approximation proceedings 24th international conference machine learning p751758 june 2024 2007 corvalis oregon philipp w keller shie mannor doina precup automatic basis function construction approximate dynamic programming reinforcement learning proceedings 23rd international conference machine learning p449456 june 2529 2006 pittsburgh pennsylvania sbastien jodogne justus h piater interactive learning mappings visual percepts actions proceedings 22nd international conference machine learning p393400 august 0711 2005 bonn germany feng richard dearden nicolas meuleau richard washington dynamic programming structured continuous markov decision problems proceedings 20th conference uncertainty artificial intelligence p154161 july 0711 2004 banff canada duncan potts claude sammut incremental learning linear model trees machine learning v61 n13 p548 november 2005 florian bauer lars grne willi semmler adaptive spline interpolation hamiltonjacobibellman equations applied numerical mathematics v56 n9 p11961210 september 2006 theodore j perkins andrew g barto lyapunov design safe reinforcement learning journal machine learning research 3 312003 lars grne willi semmler asset pricing dynamic programming computational economics v29 n34 p233265 may 2007