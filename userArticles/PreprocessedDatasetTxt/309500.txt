similaritybased models word cooccurrence probabilities many applications natural language processing nlp necessary determine likelihood given word combination example speech recognizer may need determine two word combinations eat peach eat beach likely statistical nlp methods determine likelihood word combination frequency training corpus however nature language many word combinations infrequent occur given corpus work propose method estimating probability previously unseen word combinations using available information similar wordswe describe probabilistic word association models based distributional word similarity apply two tasks language modeling pseudoword disambiguation language modeling task similaritybased model used improve probability estimates unseen bigrams backoff language model similaritybased method yields 20 perplexity improvement prediction unseen bigrams statistically significant reductions speechrecognition errorwe also compare four similaritybased estimation methods backoff maximumlikelihood estimation methods pseudoword sense disambiguation task controlled unigram bigram frequency avoid giving much weight easytodisambiguate highfrequency configurations similaritybased methods perform 40 better particular task b introduction data sparseness inherent problem statistical methods natural language processing methods use statistics relative frequencies configurations elements training corpus learn evaluate alternative analyses interpretations new samples text speech likely analysis taken one contains frequent configurations problem data sparseness also known zerofrequency problem witten bell 1991 arises analyses contain configurations never occurred training corpus possible estimate probabilities observed frequencies estimation scheme generalize training data used language processing applications sparse data problem occurs even large data sets example essen steinbiss 1992 report 7525 split millionword lob corpus 12 bigrams test partition occur training portion trigrams sparse data problem even severe instance researchers ibm brown dellapietra desouza examined training corpus consisting almost 366 million english words discovered one expect 147 word triples new english text absent training sample thus estimating probability unseen configurations crucial accurate language modeling since aggregate probability unseen events significant focus particular kind configuration word cooccurrence examples cooccurrences include relationships head words syntactic constructions verbobject adjectivenoun instance word sequences n grams commonly used models probability estimate previously unseen cooccurrence function probability estimates words cooc currence example word bigram models probability p w 2 w 1 conditioned word w 2 never occurred training following conditioning word w 1 typically calculated probability w 2 estimated w 2 frequency corpus jelinek mercer roukos 1992 katz 1987 method makes independence assumption cooccurrence w 1 w 2 frequent w 2 higher estimate p w 2 w 1 regardless w 1 classbased similaritybased models provide alternative independence assumption models relationship given words modeled analogy words sense similar given ones instance brown et al 1992 suggest classbased ngram model words similar cooccurrence distributions clustered word classes cooccurrence probability given pair words estimated according averaged cooccurrence probability two corresponding classes pereira tishby lee 1993 propose soft distributional clustering scheme certain grammatical cooccurrences membership word class probabilistic cooccurrence probabilities words modeled averaged cooccurrence probabilities word clusters dagan marcus markovitch 1993 1995 present similaritybased model avoids building clusters instead word modeled specific class set words similar using scheme predict unobserved cooccurrences likely others model however provide probability estimates cannot used component larger probabilistic model would required say speech recognition classbased similaritybased methods cooccurrence modeling may first sight seem special cases clustering weighted nearestneighbor approaches used widely machine learning pattern recognition aha kibler albert 1991 cover hart 1967 duda hart 1973 stanfill waltz 1986 devroye gyorfi lugosi 1996 atkeson moore schaal 1997 important dierences methods clustering nearestneighbor techniques often rely representing objects points multidimensional space coordinates determined values intrinsic object features however languagemodeling settings know word frequencies cooccurrences words certain configurations since purpose modeling estimate probabilities cooccurrences cooccurrence statistics basis similarity measure model predictions means measuring word similarity predictions words make words cooccur whereas typical instance nondistributional clustering learning methods word similarity defined intrinsic features independently predictions cooccurrence probabilities classifications associated particular words see instance work cardie 1993 ng lee 1996 ng 1997 zavrel daelemans 1997 11 main contributions main contributions general scheme using word similarity improve probability estimates backo models comparative analysis several similarity measures parameter settings two important language processing tasks language modeling disambiguation showing similaritybased estimates indeed useful initial study languagemodel evaluation used similaritybased model estimate unseen bigram probabilities wall street journal text compared standard backo model katz 1987 testing heldout sample similarity model achieved 20 perplexity reduction backo unseen bigrams constituted 106 test sample leading overall reduction testset perplexity 24 similaritybased model also tested speechrecognition task yielded statistically significant reduction 32 versus 64 mistakes cases disagreement backo model recognition error disambiguation evaluation compared several variants initial method cooccurrence smoothing method essen steinbiss 1992 estimation method katz decision task involving unseen pairs direct objects verbs found similaritybased models performed almost 40 better backo yielded 49 accuracy experimental setting furthermore scheme based jensenshannon divergence rao 1982 lin 1991 1 yielded statistically significant improvement error rate cooccurrence smoothing also investigated eect removing extremely lowfrequency events training set found contrast backo smoothing events often discarded training little discernible eect similaritybased smoothing methods suer noticeable performance degradation singletons events occur exactly omitted paper organized follows section 2 describes general similaritybased framework particular section 23 presents functions use measures similarity section 3 details initial language modeling experiments section 4 describes comparison experiments pseudoword disambiguation task section 5 discusses related work finally section 6 summarizes contributions outlines future directions 2 distributional similarity models wish model conditional probability distributions arising cooccurrence linguistic objects typically words certain configurations thus consider necessarily disjoint follows use subscript th element conditional probability rather empirical estimate drawn base language model true probability unknown pair second element given first element denotes probability estimate according base language model w 1 first word pair given second word w 2 p w denotes base estimate unigram probability word w similaritybased language model consists three parts scheme deciding word pairs require similaritybased estimate method combining information similar words course function measuring similarity words give details three parts following three sections concerned similarity words v 1 conditioning events probabilities p w 2 w 1 want estimate 21 discounting redistribution data sparseness makes maximum likelihood estimate mle word pair probabilities unreliable mle probability word pair w 1 w 2 conditional appearance word w 1 simply frequency w 1 w 2 training corpus cw 1 frequency w 1 however pml zero unseen word pair pair would predicted impossible generally mle unreliable events small nonzero counts well zero counts language modeling literature term smoothing used refer methods adjusting probability estimates smallcount events away mle try alleviate unreliability proposals address zerocount problem exclusively rely existing techniques smooth small counts previous proposals zerocount problem good 1953 jelinek et al 1992 katz 1987 church gale 1991 adjust mle total probability seen word pairs less one leaving probability mass redistributed among unseen pairs general adjustment involves either interpolation mle used linear combination estimator guaranteed nonzero unseen word pairs discounting reduced mle used seen word pairs probability mass left reduction used model unseen pairs backo method katz 1987 prime example discounting p represents goodturing discounted estimate katz 1987 seen word pairs p r denotes model probability redistribution among unseen word pairs w 1 normalization factor since extensive comparison study chen goodman 1996 indicated backo better interpolation estimating bigram probabilities consider interpolation methods however one could easily incorporate similaritybased estimates interpolation framework well original backo model katz used p w 2 model predicting word pairs model backed unigram model unseen bigrams however conceivable backing detailed model unigrams would advantageous therefore generalize katzs formulation writing p r w 2 w 1 instead p w 2 enabling us use similaritybased estimates unseen word pairs instead unigram frequency observe similarity estimates used unseen word pairs next investigate estimates p r w 2 w 1 derived averaging information words distributionally similar w 1 22 combining evidence similaritybased models make following assumption word w 1 similar word w 1 w 1 yield information probability unseen word pairs involving w 1 use weighted average evidence provided similar words neighbors weight given particular word w 1 depends similarity w 1 precisely let w w 1 w denote increasing function similarity denote set words similar w 1 general form similarity model consider wweighted linear combination predictions similar words normalization factor according formula w 2 likely occur w 1 tends occur words similar w 1 considerable latitude allowed defining set sw 1 evidenced previous work put form essen steinbiss 1992 karov edelman 1996 implicitly set sw 1 however may desirable restrict sw 1 fashion eciency reasons especially v 1 large instance language modeling application section 3 use closest k fewer words w 1 dissimilarity w 1 w 1 less threshold value k tuned experimentally one directly replace p r w 2 w 1 backo equation 2 p sim w 2 w 1 however variations possible interpolating unigram probability represents eect linear combination similarity estimate backo estimate exactly katzs backo scheme language modeling task section 3 set experimentally simplify comparison dierent similarity models sense disambiguation section 4 set 0 would possible make depend w 1 contribution similarity estimate could vary among words dependences often used interpolated models jelinek mercer 1980 jelinek et al 1992 saul pereira 1997 indeed advantageous however since introduce hidden vari ables require complex training algorithm pursue direction present work 23 measures similarity consider several word similarity measures derived automatically statistics training corpus opposed derived manuallyconstructed word classes yarowsky 1992 resnik 1992 1995 luk 1995 lin 1997 sections 231 232 discuss two related informationtheoretic functions kl divergence jensenshannon divergence section 233 describes l 1 norm geometric distance function section 234 examines confusion probability previously employed language modeling tasks course many possible functions opted restrict attention reasonably diverse set function corresponding weight function w w 1 w 1 given choice weight function extent arbitrary requirement increasing similarity w 1 w 1 extremely constraining clearly performance depends using good weight function would impossible try conceivable w w 1 w 1 therefore section 45 describe experiments evaluating similaritybased models without weight functions similarity functions describe depend base language model may may katz discounted model section 21 discuss complexity computing similarity function noted current implementation onetime cost construct wordtoword similarities parameter training takes place 231 kl divergence kullbackleibler kl divergence standard informationtheoretic measure dissimilarity two probability mass functions kullback 1959 cover thomas 1991 apply conditional distributions induced words v 1 words nonnegative zero p w 2 w 1 however kl divergence nonsymmetric obey triangle inequality 1 defined must case p w 2 w unfortunately generally hold mles based samples must use smoothed estimates redistribute probability mass zerofrequency events forces sum 4 w makes calculation expensive large vocabularies divergence dw 1 w 1 computed set role free parameter control relative influence neighbors closest nonnegligible w 1 extremely close w 1 whereas low distant neighbors also contribute estimate chose negative exponential function kl divergence weight function analogy form cluster membership function related distributional clustering work pereira et al 1993 also form probability w 1 distribution arose sample drawn distribution w 1 cover thomas 1991 lee 1997 however reasons heuristic rather theoretical since rigorous probabilistic justification similaritybased methods 232 jensenshannon divergence related measure jensenshannon divergence rao 1982 lin 1991 defined average kl divergence two distributions average distribution shorthand distribution2 p w 2 w 1 since kl divergence nonnegative jw 1 w letting pw 2 easy see log qw entropy discrete density q equation shows j gives information gain achieved distinguishing two distributions p p conditioning contexts w 1 w pooling two distributions ignoring distinction w 1 w 1 also easy see j computed eciently since depends conditioned words occur contexts indeed letting grouping terms 6 appropriately obtain ranging 0 log 2 smoothed estimates required probability ratios involved kl divergence case set w j w 1 w plays role 233 l 1 norm l 1 norm defined grouping terms express lw 1 w 1 form depending common follows triangle inequality 0 lw 1 w equality 2 words w 2 p w strictly positive since require weighting scheme decreasing l set free 2 higher relative influence accorded nearest neighbors interesting note following relations l 1 norm kl divergence jensenshannon divergence cover thomas 1991 give following lower bound b base logarithm function lin 1991 notes l upper bound j 234 confusion probability extending work sugawara nishimura toshioka okochi kaneko 1985 essen steinbiss 1992 used confusion probability estimate word cooccurrence probabilities 3 report 14 improvement testset perplexity defined small corpus confusion probability also used grishman sterling 1993 estimate likelihood selectional patterns confusion probability estimate probability word w 1 substituted word w 1 sense found contexts serves normalization factor contrast distance functions described pc curious property w 1 may necessarily closest word may exist word w 1 pc w 1 section 44 example confusion probability computed empirical estimates provided unigram estimates nonzero assume throughout fact use smoothed estimates provided katzs backo scheme problem atic estimates typically preserve consistency respect marginal estimates bayess rule may w2 using consistent estimates mle safely apply bayess rule rewrite pc follows table 1 summary similarity function properties name range base lm constraints tune j 0 log 2 none yes bayes consistency jensenshannon divergence l 1 norm sum requires computation common w 2 examination equation 8 reveals important dierence confusion probability functions j l described previous sec tions functions rate w 1 similar w 1 roughly p w 2 w 1 high however greater w 1 p w 1 w 2 large p w 2 w 1 p w 2 ratio large may think w 2 exceptional since w 2 infrequent expect p w 2 w 1 large 235 summary several features measures similarity listed summarized table 1 base lm constraints conditions must satisfied probability estimates base language model last column indicates whether weight w w 1 w associated similarity function depends parameter needs tuned experimentally 3 language modeling goal first set experiments described section provide proof concept showing similaritybased models achieve better language modeling performance backo therefore used one similarity measure success experiments convinced us similaritybased methods worth examining closely results second set experiments comparing several similarity functions pseudoword disambiguation task described next section language modeling experiments used similaritybased model kl divergence dissimilarity measure alternative unigram frequency backing bigram model used bigram language model defined entire vocabulary noted earlier estimates must smoothed avoid division zero computing employed standard katz bigram backo model purpose since v 20 000 application considered small fraction v computing using tunable thresholds k described section 22 purpose standard evaluation metric language models likelihood test data according model intuitively testset perplexity represents average number alternatives presented bigram model test word thus better model lower perplexity task lower perplexity indicate better prediction unseen bigrams evaluated model comparing testset perplexity eect speechrecognition accuracy baseline bigram backo model developed mit lincoln laboratories wall street journal wsj text dictation corpora provided arpas hlt program paul 1991 4 baseline backo model follows katz design except sake compactness frequency one bigrams ignored counts used model obtained 405 million words wsj text years 198789 perplexity evaluation tuned similarity model parameters minimizing perplexity additional sample 575 thousand words wsj text drawn arpa hlt development test set best parameter values found values improvement perplexity unseen bigrams heldout hlt evaluation test set 20 since unseen bigrams comprise 106 sample improvement unseen bigrams corresponds overall test set perplexity improvement 24 2374 2317 table 2 shows reductions training test perplexity sorted training reduction dierent choices number k closest neighbors used values best ones found k 5 equation 9 clear computational cost applying similarity model unseen bigram ok therefore lower values k computationally preferable table see reducing k incurs penalty less 1 perplexity improvement relatively low values k appear sucient achieve benefit similarity model table also shows best value increases k decreases lower k greater weight given conditioned words frequency suggests predictive power neighbors beyond closest modeled fairly well overall frequency conditioned word bigram similarity model also tested language model speech recog nition test data experiment pruned word lattices 403 wsj table 2 perplexity reduction unseen bigrams dierent model parameters training reduction test reduction 50 25 40 015 1838 2045 100 25 45 01 1823 2054 90 25 45 01 1823 2059 closedvocabulary test sentences arc scores lattices sums acoustic score negative log likelihood languagemodel score case negative log probability provided baseline bigram model given lattices constructed new lattices arc scores modified use similarity model instead baseline model compared best sentence hypothesis original lattice best hypothesis modified one counted word disagreements one hypotheses correct total 96 disagreements similarity model correct 64 cases backo model 32 advantage similarity model statistically significant 001 level overall reduction error rate small 214 209 number disagreements small compared overall number errors recognition setup employed experiments table 3 shows examples speech recognition disagreements two models hypotheses labeled b backo similarity boldface words errors similarity model seems better modeling regularities semantic parallelism lists avoiding past tense form hand similarity model makes several mistakes function word inserted place punctuation would found written text 4 wordsense disambiguation since experiments described previous section demonstrated promising results similaritybased estimation ran second set experiments designed help us compare analyze somewhat diverse set similarity measures given table 1 unfortunately kl divergence confusion probability dierent requirements base language model could run direct fourway comparison explained elected omit kl divergence consideration table 3 speech recognition disagreements models commitments leaders felt three point six billion dollars commitments leaders fell three point six billion dollars followed france us agreed italy followed france us greece italy whispers made whispers aide b necessity change exist necessity change exists b without additional reserves centrust would reported without additional reserves centrust would reported b darkness past church darkness passed church chose evaluate three remaining measures word sense disambiguation task method presented noun two verbs asked verb likely noun direct object thus measure absolute quality assignment probabilities would case perplexity evaluation rather relative quality could therefore ignore constant factors normalize similarity measures 41 task definition usual word sense disambiguation problem method tested presented ambiguous word context asked identify correct sense word context example test instance might sentence fragment robbed bank question whether bank refers river bank savings bank perhaps alternative meaning sense disambiguation clearly important problem language processing applications evaluation task presents numerous experimental dicul ties first notion sense clearly defined instance dictionaries may provide sense distinctions fine coarse data hand also one needs training data correct senses signed acquiring correct senses generally requires considerable human eort furthermore words many possible senses whereas others essentially monosemous means test cases uniformly hard circumvent diculties set pseudoword disambiguation experiment schutze 1992a gale church yarowsky 1992 format follows first list pseudowords constructed combination two dierent words v 2 word v 2 contributes exactly one pseudoword every w 2 test set replaced corresponding pseudoword example pseudoword created words make take data altered follows make plans make take plans take action make take action method tested must choose two words make pseudoword advantages using pseudowords twofold first alternative senses control experimenter test instance presents exactly two alternatives disambiguation method alternatives chosen frequency part speech secondly pre transformation data yields correct answer handtagging word senses necessary advantages make pseudoword experiments elegant simple means test ecacy dierent language models course may provide completely accurate picture models would perform real disambiguation tasks although one could create realistic settings making pseudowords two words varying frequencies alternative pseudosenses ease comparison consider interpolation unigram probabil ities thus model used experiments diers slightly used language modeling tests summarized follows 42 data used statistical partofspeech tagger church 1988 pattern matching concordancing tools due david yarowsky identify transitive main verbs nouns corresponding direct objects 44 million words 1988 associated press newswire selected nounverb pairs 1000 frequent nouns corpus pairs undoubtedly somewhat noisy given errors inherent partofspeech tagging pattern matching used 80 587 833 pairs derived building models reserving 20 testing purposes similarity measures require smoothed models calculated katz backo model p equation 2 p r w 2 w 1 maximumlikelihood model furthermore wished evaluate hypothesis compact language model built without aecting model quality deleting singletons word pairs occur training set claim made particular language modeling katz 1987 therefore built four base models summarized table 4 table 4 base language models singletons singletons katz bo1 boo1 since wished test eectiveness using similarity unseen word cooc currences removed test data verbobject pairs occurred training set resulted 17 152 unseen pairs occurred multiple times unseen pairs divided five equalsized parts 1 5 formed basis fivefold crossvalidation five runs one used performance test set four combined one set used tuning parameters necessary via simple grid search evaluated error tuning set regularly spaced points parameter space finally test pseudowords created pairs verbs similar frequencies control word frequency decision task method simply rank verbs frequency create pseudowords adjacent pairs thus verb participated exactly one pseudoword table 5 lists randomly chosen pseudowords frequencies corresponding verbs table 5 sample pseudoword verbs frequencies word meeet typo occurring corpus make 14782take 12871 fetch 35renegotiate 35 magnify 13exit 13 meeet 1stupefy 1 relabel 1entomb 1 use error rate performance metric defined asn incorrect choices ties2 n size test corpus tie occurs two words making pseudoword deemed equally likely 43 baseline experiments performances four base language models shown table 6 mle1 mleo1 error rates exactly 5 test sets consist unseen bigrams assigned probability 0 maximumlikelihood estimates thus ties method backo models bo1 boo1 also perform similarly table 6 base language model error rates since backo models consistently performed worse mle models chose use mle models subsequent experiments therefore ran comparisons measures could utilize unsmoothed data namely l 1 norm lw 1 w 1 jensenshannon divergence jw 1 w 1 confusion probability pc w 1 w 1 6 44 sample closest words section examine closest words randomly selected noun guy according three measures l j pc table 7 shows ten closest words order base language model mle1 overlap closest words l closest words j little overlap closest words measures closest words respect pc words man lot common three also observe word guy fourth list words highest confusion probability respect guy let us examine case nouns kid role closely according similarity functions l j kid second closest word guy role considered relatively distant pc case however role highest confusion probability respect guy whereas kid 80th highest confusion probability accounts dierences table gives ten verbs likely occur guy kid role indicates l j rate words similar tend cooccur verbs observe four ten likely verbs occur kid table 7 closest words word guy l j p c using mle1 base language model rank words role kid also shown among top ten guy kid 123 kid 015 people 0024 lot 135 thing 01645 fire 0013 thing 139 lot 0165 guy 00127 man 146 man 0175 man 0012 doctor 146 mother 0184 year 001 girl 148 doctor 0185 lot 00095 rest 1485 friend 0186 today 0009 son 1497 boy 0187 way 0008778 bit 1498 son 0188 part 0008772 role rank 173 role rank 43 kid rank 80 table 8 noun w 1 ten verbs w 2 highest p w 2 w 1 boldface verbs occur given noun guy base language model mle1 noun likely verbs guy see get play let give catch tell pick need kid get see take help want tell teach send give love role play take lead support assume star expand accept sing limit table 9 verbs highest p w 2 guyp w 2 ratios numbers parentheses ranks 1 electrocute 2 shortchange 3 bedevil 4 admire 5 bore 6 fool also likely occur guy whereas verb play commonly occurs role guy sort verbs decreasing p w 2 guyp w 2 dierent order emerges table 9 play likely verb cooccur role ranked higher get likely verb cooccur kid thus indicating role higher confusion probability respect guy kid finally examine eect deleting singletons base language model table shows ten closest words order base language model mleo1 relative order four closest words remains however next six words quite dierent mle1 data suggests table closest words word guy l j p c using mleo1 base language model guy kid 117 kid 015 people 0025 lot 140 thing 016 fire 0021 thing 141 lot 017 guy 0018 reason 1417 mother 0182 work 0016 break 142 answer 01832 man 0012 ball 1439 reason 01836 lot 00113 answer 144 doctor 0187 job 001099 tape 1449 boost 0189 thing 001092 rest 1453 ball 019 reporter 00106 eect singletons calculations similarity quite strong borne experimental evaluations described section 45 conjecture eect due fact many lowfrequency verbs data 65 verbs appeared 10 fewer nouns common verb occurred 710 nouns omitting singletons involving verbs may well drastically alter number verbs cooccur two given nouns w 1 w 1 since similarity functions consider set experiments depend words surprising eect deleting singletons rather dramatic contrast backo language model sensitive missing singletons goodturing discounting small counts inflation zero counts 45 performance similaritybased methods figure 1 shows results experiments five test sets using mle1 base language model parameter always set optimal value corresponding training set rand shown comparison purposes simply chooses weights w w 1 w set equal v 1 cases similaritybased methods consistently outperformed katzs backo method mle recall yielded error rates 5 large margin indicating information word pairs useful unseen pairs unigram frequency informative similaritybased methods also much better rand indicates enough simply combine information words arbitrarily word similarity taken account cases j edged methods average improvement using j instead pc 0082 dierence significant 1 level p 085 according paired ttest results mleo1 case depicted figure 2 see similaritybased methods achieving far lower error rates mle backo rand figure 1 error rates test set base language model mle1 methods going left right rand p c l j performances shown settings optimal corresponding training set ranged 40 45 l 20 26 j rand methods j always performed best however omitting singletons amplified disparity j pc average dierence 024 significant 01 level paired ttest important observation methods including rand suered performance hit singletons deleted base language model seems indicate seen bigrams treated dierently unseen bigrams even seen bigrams extremely rare thus conclude one cannot create compressed similaritybased language model omitting singletons without hurting performance least task analyze role parameter recall appears weight functions jensenshannon divergence l 1 norm controls relative influence similar words influence increases higher values figure 3 shows value aects disambiguation performance four curves shown corresponding choice similarity function base language model error bars depict average range error rates five disjoint test sets immediately clear get good performance results must set much higher jensenshannon divergence l 1 norm phenomenon results fact range possible values j much smaller error rates test sets base language model mleo1 rand figure 2 error rates test set base language model mleo1 ranged 6 11 l 21 22 j error rate beta effect beta test set error using different similarities jensen mle1 jensen mleo1 figure 3 average range testset error rates varied similarity function indicated point style base language model indicated line style l compression j values requires large scale dierences distances correctly also observe setting low causes substantially worse error rates however curves level rather moving upwards long suciently large value chosen setting suboptimally greatly impact performance furthermore shape curves base language models suggesting relation testset performance relatively insensitive variations training data fact higher values seem lead better error rates suggests role filter distant neighbors test hypothesis experimented using k similar neighbors figure 4 shows error rate depends k dierent fixed values two lowest curves depict performance jensenshannon divergence l 1 norm set optimal value respect average test set performance appears distant neighbors essentially eect error rate contribution sum 9 negligible contrast low value chosen upper two curves distant neighbors weighted heavily case including distant neighbors causes serious degradation performance02603034038042100 200 300 400 500 600 700 800 900 1000 error rate effect k test set error using different similarities mle1 jensen beta21 jensen beta2 confusion figure 4 average range testset error rates k varied base language model mle1 similarity function indicated point style dashed dotted lines indicate suboptimal choice interestingly behavior confusion probability dierent two cases adding neighbors actually improves error rate seems indicate confusion probability correctly ranking similar words order informativeness however alternative explanation pc disadvantage employed context tunable weighting scheme distinguish two possibilities ran experiment dispensed weights altogether instead took vote k similar neighbors alternative chosen likely one preferred majority similar neighbors note ignored degree alternatives preferred results shown figure 502803203604044100 200 300 400 500 600 700 800 900 1000 error rate effect k test set error ignoring weights probabilities jensen mle1 jensen mleo1 confusion mle1 confusion mleo1 figure 5 average range votingscheme testset error rates k varied similarity function indicated point style base language model indicated line style see k similar neighbors according j l always informative chosen according confusion probability largest performance gaps occurring low k course methods performed since case using set neighbors graph provides clear evidence confusion probability good measure informativeness words 5 related work large body work notions work similarity word clustering applications impossible compare methods directly since assumptions experimental settings applications methods vary widely therefore discussion mainly descriptive highlighting main similarities dierences methods 51 statistical similarity clustering disambiguation language modeling work instance growing body research using word similarity improve performance languageprocessing problems similaritybased algorithms either use similarity scores word words directly making predictions rely similarity scores word representatives precomputed similarity classes early attempt automatically classify words semantic classes carried linguistic string project grishman hirschman nhan 1986 semantic classes derived similar cooccurrence patterns words within syntactic relations cooccurrence statistics considered class level used alleviate data sparseness syntactic disambiguation schutze 1992b 1993 captures contextual word similarity first reducing dimensionality context representation using singular value decomposition using reduceddimensionality representation characterize possible contexts word information used word sense disambiguation occurrences ambiguous word clustered cluster mapped manually one senses word context vector new occurrence ambiguous word mapped nearest cluster determines sense occurrence schutze emphasizes method avoids clustering words predefined set classes claiming clustering likely introduce artificial boundaries cut words part semantic neighborhood karov edelman 1996 also addressed data sparseness problem word sense disambiguation using word similarity use circular definition word similarity measure context similarity measure circularity resolved iterative process system learns set typical usages senses ambiguous word given new occurrence ambiguous word system selects sense whose typical context similar current context applying procedure resembles sense selection process shutze scheme employing word similarity disambiguation influenced work dagan et al 1993 1995 method computes word similarity measure directly word cooccurrence data word modeled set similar words plausibility unseen cooccurrence judged cooccurrence statistics words set similarity measure weighted tanimoto measure version also used grefenstette 1992 1994 word association measured mutual information following earlier work word similarity hindle 1990 method dagan et al provide probabilistic models disambiguation decisions based comparing scores dierent alternatives produce explicit probability estimates therefore cannot integrated directly within larger probabilistic framework cooccurrence smoothing model essen steinbiss 1992 like model produces explicit estimates word cooccurrence probabilities based cooccurrence statistics similar words similaritybased estimates interpolated direct estimates ngram probabilities form smoothed ngram language model word similarity model computed confusion probability measure described evaluated earlier several language modeling methods produce similaritybased probability estimates classbased models methods use direct measure similarity word words instead cluster words classes using global optimization criterion brown et al 1992 present classbased ngram model records probabilities sequences word classes instead sequences individual words probability estimate bigram contains particular word aected bigram statistics words class words class considered similar cooccurrence behavior word classes formed bottomup hardclustering algorithm whose objective function average mutual information class cooc currence ushioda 1996 introduces several improvements mutualinformation clustering method applied partofspeech tagging records classes contained particular word bottomup merging process word represented mixture classes rather single class algorithms kneser ney 1993 ueberla 1994 similar brown et al 1992 although dierent optimization criterion used number clusters remains constant throughout membership assignment pro cess pereira et al 1993 use formalism statistical mechanics derive topdown softclustering algorithm probabilistic class membership word cooccurrence probability modeled weighted average class cooccurrence probabilities weights correspond membership probabilities words within classes 52 thesaurusbased similarity approaches described previous section induce word similarity relationships word clusters cooccurrence statistics corpus researchers developed methods quantify similarity relationships based information manually crafted wordnet thesaurus miller beckwith fellbaum gross miller 1990 resnik 1992 1995 proposes nodebased approach measuring similarity pair words thesaurus applies various disambiguation tasks similarity function informationtheoretic measure informativeness least general common ancestor two words thesaurus classification jiang conrath 1997 combine nodebased approach edgebased approach similarity nodes thesaurus influenced path connects similarity method tested data set word pair similarity ratings derived human judgments lin 1997 1998 derives general conceptsimilarity measure assumptions desired properties similarity measure function number bits required describe two concepts well commonality describes instantiation measure hierarchical thesaurus applies wordnet part word sense disambiguation algorithm 53 contextual similarity information retrieval query expansion information retrieval ir provides additional motivation automatic identification word similarity one line work ir literature considers two words similar occur often documents another line work considers type word similarity concerned similarity measured derived wordcooccurrence statistics grefenstette 1992 1994 argues cooccurrence within document yields similarity judgements sharp enough query expansion instead extracts coarse syntactic relationships texts represents word set wordcooccurrences within relation word similarity defined weighted version tanimoto measure compares cooccurrence statistics two words similarity method evaluated measuring impact retrieval performance ruge 1992 also extracted word cooccurrences within syntactic relationships evaluated several similarity measures data focusing versions cosine measure similarity rankings obtained measures compared produced human judges 6 conclusions similaritybased language models provide appealing approach dealing data sparseness work proposed general method using similaritybased models improve estimates existing language models evaluated range similaritybased models parameter settings important languageprocessing tasks pilot study compared language modeling performance similaritybased model standard backo model improvement achieved bigram backo model statistically signifi cant relatively modest overall eect small proportion unseen events second detailed study compared several similaritybased models parameter settings smaller manageable wordsense disambiguation task observed similaritybased methods perform much better unseen word pairs measure based jensenshannon divergence best overall experiments restricted bigram probability estimation reasons simplicity computational cost however relatively small proportion unseen bigrams test data makes eect similaritybased methods necessarily modest overall tasks believe benefits similaritybased methods would substantial tasks larger proportion unseen events instance language modeling longer contexts obstacle principle trigram case example would still determining probability pairs would consist word pairs instead single words however number possible similar events given element v 1 much larger bigram case direct tabulation events similar event would thus practical compact approximate representations would investigated would also worth investigating benefit similaritybased methods improve estimates lowfrequency seen events however would need replace backo model another one combines multiple estimates event example interpolated model contextdependent interpolation parameters another area investigation relationship similaritybased classbased approaches mentioned introduction rely common intuition namely events modeled extent similar events classbased methods computationally expensive training time nearest neighbor methods require searching best model structure number classes hard clustering class membership estimation hidden parameters class membership probabilities soft clustering hand classbased methods reduce dimensionality thus smaller ecient test time dimensionality reduction also claimed improve generalization test data evidence mixed furthermore classbased models theoretically satisfying probabilistic interpretations saul pereira 1997 whereas justification similaritybased models heuristic empirical present given variety classbased language modeling algorithms described section related work beyond scope paper compare performance two approaches ever comparison especially one would bring approaches common probabilistic interpretation would well worth pursuing acknowledgments thank hiyan alshawi joshua goodman rebecca hwa slava katz doug mcil roy stuart shieber yoram singer many helpful discussions doug paul help bigram backo model andrej ljolje michael riley providing word lattices speech recognition evaluation also thank reviewers paper constructive criticisms editors present issue claire cardie ray mooney help suggestions portions work appeared previously dagan pereira lee 1994 dagan lee pereira 1997 thank reviewers papers comments part work done first author member technical sta visitor att labs second author graduate student harvard university summer visitor att labs second author received partial support national science foundation grant iri9350192 national science foundation graduate fellowship att grpwalfp grant notes 1 best knowledge first use particular distribution dissimilarity function statistical language processing function implicit earlier work distributional clustering pereira et al 1993 used tishby pc distributional similarity work finch 1993 discusses use word clustering provide experimental evaluation actual data 2 experimented using 1 well yielded poorer performance results 3 actually present two alternative definitions use model 2b found yielded best experimental results 4 arpa wsj development corpora come two versions one verbalized punctuation without used latter experiments 5 values refer base 10 logarithms exponentials calculations 6 noted however bo1 data kldivergence performed slightly better l 1 norm r locally weighted learning empirical study smoothing techniques language modeling nearest neighbor pattern classification pattern classification scene analysis finding structure language work statistical methods word sense disambiguation population frequencies species estimation population parameters use syntactic context produce term association lists text retrieval explorations automatic thesaurus discovery discovery procedures sublanguage selectional patterns initial experiments noun classification predicateargument structures principles lexical language modeling speech recognition semantic similarity based corpus statistics lexical taxonomy academia sinica learning similaritybased word sense disambiguation sparse data estimation probabilities sparse data language model component speech recognizer improved clustering techniques classbased statistical language modelling information theory statistics using syntactic dependency local context resolve word sense ambiguity morgan kaufmann statistical sense disambiguation relatively small corpora using dictionary definitions wordnet online lexical database integrating multiple knowledge sources disambiguate word sense exemplarbased approach distributional clustering english words wordnet distributional analysis classbased approach lexical discovery disambiguating noun groupings respect wordnet senses experiments linguisticallybased term associations aggregate mixedorder markov models statistical language processing word space morgan kaufmann toward memorybased reasoning isolated word recognition using hidden markov models extended clustering algorithm statistical language models tech hierarchical clustering words applications nlp tasks zerofrequency problem estimating probabilities novel events adaptive text compression tr ctr yoichi tomiura shosaku tanaka toru hitaka estimating satisfactoriness selectional restriction corpus without thesaurus acm transactions asian language information processing talip v4 n4 p400416 december 2005 kilyoun kim keysun choi dimensionreduced estimation word cooccurrence probability proceedings 38th annual meeting association computational linguistics p571578 october 0306 2000 hong kong doina tatar word sense disambiguation machine learning approach short survey fundamenta informaticae v64 n14 p433442 january 2005 egidio terra charles l clarke fast computation lexical affinity models proceedings 20th international conference computational linguistics p1022es august 2327 2004 geneva switzerland yuseop kim jeongho chang byoungtak zhang comparative evaluation datadriven models translation selection machine translation proceedings 19th international conference computational linguistics p17 august 24september 01 2002 taipei taiwan mats rooth stefan riezler detlef prescher glenn carroll franz beil inducing semantically annotated lexicon via embased clustering proceedings 37th annual meeting association computational linguistics computational linguistics p104111 june 2026 1999 college park maryland anna korhonen yuval krymolowski robustness entropybased similarity measures evaluation subcategorization acquisition systems proceeding 6th conference natural language learning p17 august 31 2002 maayan geffet ido dagan feature vector quality distributional similarity proceedings 20th international conference computational linguistics p247es august 2327 2004 geneva switzerland egidio terra c l clarke frequency estimates statistical word similarity measures proceedings conference north american chapter association computational linguistics human language technology p165172 may 27june 01 2003 edmonton canada chinyew lin guihong cao jianfeng gao jianyun nie informationtheoretic approach automatic evaluation summaries proceedings main conference human language technology conference north american chapter association computational linguistics p463470 june 0409 2006 new york new york jianfeng gao hisami suzuki wei yuan empirical study language model adaptation acm transactions asian language information processing talip v5 n3 p209227 september 2006 yair evenzohar dan roth classification approach word prediction proceedings first conference north american chapter association computational linguistics p124131 april 29may lillian lee measures distributional similarity proceedings 37th annual meeting association computational linguistics computational linguistics p2532 june 2026 1999 college park maryland lillian lee fernando pereira distributional similarity models clustering vs nearest neighbors proceedings 37th annual meeting association computational linguistics computational linguistics p3340 june 2026 1999 college park maryland maria lapata scott mcdonald frank keller determinants adjectivenoun plausibility proceedings ninth conference european chapter association computational linguistics june 0812 1999 bergen norway sabine schulte im walde experiments automatic induction german semantic verb classes computational linguistics v32 n2 p159194 june 2006 yuseop kim byoungtak zhang yung taek kim collocation dictionary optimization using wordnetand knearest neighbor learning machine translation v16 n2 p89108 june 2001 viktor pekar acquisition verb entailment text proceedings main conference human language technology conference north american chapter association computational linguistics p4956 june 0409 2006 new york new york julie weeds david weir diana mccarthy characterising measures lexical distributional similarity proceedings 20th international conference computational linguistics p1015es august 2327 2004 geneva switzerland maria lapata frank keller scott mcdonald evaluating smoothing algorithms plausibility judgements proceedings 39th annual meeting association computational linguistics p354361 july 0611 2001 toulouse france zoltn szamonek istvn bir similarity based smoothing language modeling acta cybernetica v18 n2 p303314 january 2007 kristina toutanova christopher manning andrew ng learning random walk models inducing word dependency distributions proceedings twentyfirst international conference machine learning p103 july 0408 2004 banff alberta canada pablo gamallo alexandre agustini gabriel p lopes clustering syntactic positions similar semantic requirements computational linguistics v31 n1 p107146 march 2005 frank keller maria lapata olga ourioupina using web overcome data sparseness proceedings acl02 conference empirical methods natural language processing p230237 july 06 2002 alexander budanitsky graeme hirst evaluating wordnetbased measures lexical semantic relatedness computational linguistics v32 n1 p1347 march 2006 maria lapata disambiguation nominalizations computational linguistics v28 n3 p357388 september 2002 julie weeds david weir cooccurrence retrieval flexible framework lexical distributional similarity computational linguistics v31 n4 p439475 december 2005 claudio carpineto renato de mori giovanni romano brigitte bigi informationtheoretic approach automatic query expansion acm transactions information systems tois v19 n1 p127 jan 2001 claire grover alex lascarides mirella lapata comparison parsing technologies biomedical domain natural language engineering v11 n1 p2765 march 2005 frank keller mirella lapata using web obtain frequencies unseen bigrams computational linguistics v29 n3 p459484 september john chen srinivas bangalore k vijayshanker automated extraction treeadjoining grammars treebanks natural language engineering v12 n3 p251299 september 2006