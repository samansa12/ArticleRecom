learning distributed representations concepts using linear relational embedding abstractin paper introduce linear relational embedding means learning distributed representation concepts data consisting binary relations concepts key idea represent concepts vectors binary relations matrices operation applying relation concept matrixvector multiplication produces approximation related concept representation concepts relations learned maximizing appropriate discriminative goodness function using gradient ascent task involving family relationships learning fast leads good generalization b introduction given data consists concepts relations among concepts goal correctly predict unobserved instances relationships concepts representing concept vector euclidean space relationships concepts linear operations illustrate approach start simple task call number problem data consists integers operations among integers modular number problem numbers integers set set operations subscript indicates operations performed modulo data consists b figure 1 vectors handcoded solution number problem vectors solution found linear relational embedding 70 90 possible triplets used training testing system able correctly complete triplets operation 4 3 2 handcoded solution 144 108 72 36 0 36 72 108 144 lre solution 7200 3597 14401 10801 000 10802 14402 3598 7197 table 1 angles expressed degrees rotation matrices solutions number problem corresponding vectors g1 rotations lre solution dier slightly multiples 36 70 triplets randomly chosen 90 used training triplets num result applying operation op number num 1 example g main idea linear relational embedding lre represent concepts using ndimensional vectors relations n n matrices operation applying relation concept obtain another concept matrixvector multiplication within framework one could easily handcode solution number problem numbers represented vectors unit length disposed g1a relations represented rotation matrices r rotation angle multiple 210 rst row table 1 result applying example operation 3 number 4 obtained multiplying corresponding matrix vector amounts rotating vector located 144 degrees 108 degrees thus obtaining vector 252 degrees corresponds exactly vector representing number 7 paper show lre nds equivalent solution presented g 1b second row table 1 lre nd solution many triplets omitted training set learned way representing concepts relationships complete omitted triplets correctly moreover lre works well toy problems like one presented also symbolic domains task generalizing unobserved triplets nontrivial next section brie review related work learning distributed representations lre presented detail section 3 section 4 presents results obtained using lre number problem family tree task hinton 1986 well results obtained much larger version family tree problem uses data real family tree also compare results results obtained using principal components analysis section 5 examine solution obtained impoverished data set modied include information new concepts relations section 6 indicates ways lre could extended section 7 presents nal discussion method related work several methods already exist learning sensible distributed representations relational data multidimensional scaling kruskal 1964 young hamer 1987 nds representation concepts vectors multidimensional space way dissimilarity two concepts modeled euclidean distance vectors unfortunately dissimilarity relationship used multidimensional scaling cannot make use far specic information concepts contained triplet like john father mary latent semantic analysis lsa deerwester et al 1990 landauer dumais 1997 landauer et al 1998 assumes meaning word ected way cooccurs words lsa nds features performing singular value decomposition large matrix taking eigenvectors largest eigenvalues row matrix corresponds paragraph text entry column number times particular word occurs paragraph suitably transformed representation count word represented projection onto learned features words similar meanings similar projections lsa unable make use specic relational information triplet hinton 1986 showed multilayer neural network trained using backpropagation rumelhart et al 1986 could make explicit semantic features concepts relations present data unfortunately system problems generalizing many triplets missing training set shown simple task called family tree problem problem data consists persons relations among persons belonging two families one italian one english shown gure 2 colin charlotte alberto mariemma figure 2 two isomorphic family trees symbol means married information trees represented simple propositions form person1 relation person2 using relations father mother husband wife son daughter uncle aunt brother sister nephew niece 112 triplets two trees network architecture used hinton shown g3 two groups input units one role person1 one role relation one group output units represent person2 inputs outputs coded one unit active time standing particular person relation idea groups 6 units second fourth layer learn important features persons relations would make easy express regularities domain implicit examples given figure 4 shows activity level input colin aunt network learning notice 2 units high activation output layer marked black dots corresponding 2 correct answers colin 2 aunts jennifer margaret figure 5 shows diagrams weights connections 24 input units 6 units used networks internal distributed representation person1 learning clear unit number 1 primarily concerned distinction english italian unit 2 encodes generation person belongs unit 6 encodes branch family person belongs notice semantic features important expressing regularities domain never explicitly specied similarly relations encoded terms semantic features group 6 units layer 2 discovery semantic features gave network degree generalization tested four triplets shown training network usually able nd correct answers notice learning procedure relied nding direct correlations input output vectors would generalize badly family tree task structure must input local encoding person 1 output local encoding person 2 local encoding relation input 6 units learned distributed encoding person 1 6 units learned distributed encoding relation 6 units learned distributed encoding relation figure 3 architecture network used family tree task three hidden layers 6 units constructs internal representations input output layers forced use localist encodings figure 4 activity levels network learned bottom layer 24 input units left representing person1 12 units right representing relation white squares inside two groups show activity levels units one active unit rst group representing colin one second group representing aunt two groups input units totally connected group 6 units second layer two groups 6 must encode input terms distributed pattern activity second layer totally connected central layer 12 units layer connected penultimate layer 6 units activity penultimate layer must activate correct output units stands particular person2 case two correct answers marked black dots colin two aunts input output units laid spatially english people one row isomorphic italians immediately hinton 1986 figure 5 diagrams weights 24 input unit represent people 6 units second layer learn distributed representations people white rectangles stand excitatory weights black inhibitory weights area rectangle encodes magnitude weight weights 12 english people top row unit beneath weights weight isomorphic italian hinton 1986 discovered generalize correctly present pairwise correlations input output units biggest limitation system generalization limited system problems generalizing 4 triplets missing training set moreover guarantee semantic features learned person1 second layer would ones found fourth layer person2 network used hinton 1986 restricted completing triplets rst two terms exible way applying backpropagation learning procedure recurrent network receives sequence words one time continually predicts next word states hidden units must learn capture information word string relevant predicting next word elman 1990 presented version approach backpropagation time required get correct derivatives curtailed one time step simplify computation bridle 1990 showed forward dynamics particular type recurrent neural network could viewed way computing posterior probabilities hidden states hmm relationship recurrent neural networks hidden markov models extensively studied cleermans et al 1989 giles et al 1992 others hidden markov models interesting tractable compute posterior distribution hidden states given observed string generative model word strings assume word produced single hidden node seem appropriate goal learn realvalued distributed representations concepts denoted words linear dynamical systems seem promising assume observation generated realvalued vector hidden state space however linearity linear dynamical systems seems restrictive linearity shows dynamics output model x hidden state visible state r linear dynamics c linear output model noise dynamics noise output model linear relational embedding viewed way overcoming apparent restrictions linear dynamical systems protably applied task modeling discrete relational data first eliminate linear output model using discrete observation space assuming noisefree table relates vectors hidden state space discrete observations entries table change learning table xed hidden state precisely specied observed discrete symbol linearity dynamics made far less restrictive using switching linear dynamical system instead treating relational term triplet observation produced hidden state treat completely dierent kind observation provides information dynamics r rather hidden state x learned noisefree table exactly species linear dynamics associated relational term allow additional gaussian noise process dynamics ensures always probability density arriving point hidden space wherever start whatever dynamics particular ensures starting point hidden space specied rst term triple using dynamics specied relational term probability arriving point hidden space specied third term learning adjust tables probability arriving point specied third term much greater probability arriving points would specied possible third terms linear dynamical systems perspective useful understanding linear relational embedding relates recurrent neural networks proposal learning relational data lre suciently dierent standard linear dynamical system perspective also confusing next section therefore present lre technique right 3 linear relational embedding let us assume data consists c triplets concept1 relation concept2 containing n distinct concepts binary relations anticipated section 1 main idea linear relational embedding represent concept ndimensional vector relation nn matrix shall set vectors set matrices set triplets c b c 2 v r c 2 r operation relates pair c r c vector b c matrixvector multiplication r c c produces approximation b c goal learning nd suitable vectors matrices triplet c b c vector closest r c c obvious approach minimize squared distance r c c b c good causes vectors matrices collapse zero addition minimizing squared distance b c must also maximize squared distances concept vectors nearby achieved imagining r c c noisy version one concept vectors maximizing probability noisy version correct answer b c rather possibilities assume spherical gaussian noise variance 12 dimension probability concept would generate r c c proportional expjjr c c v jj 2 sensible discriminative cost function log e kr c c b c k 2 e kr c c k c number triplets rst two terms equal ones c diering third term understand need introduce factor let us consider set k triplets rst two terms r diering third term shall call b would like system assign equal probability correct answers therefore discrete probability distribution want approximate written discrete delta function x ranges vectors v system implements discrete distribution z expkr xk 2 5 expkr v normalization factor kullbackleibler divergence p x qx written x thus minimizing klpkq amounts minimizing z expkr uk 2 every u solution triplet exactly maximize eq3 results present next section obtained maximizing g using gradient ascent vector matrix components updated simultaneously iteration one eective method performing optimization scaled conjugate gradient mller 1993 learning fast usually requiring hundred updates learning virtually ceased probability correct answer approached 1 every data point also developed alternative optimization method less likely get trapped local optima task dicult objective function modied include temperature divides exponents eq 3 temperature annealed optimization method uses line search direction steepest ascent modied objective function small amount weight decay helps ensure exponents eq 3 cause numerical problems temperature becomes small general dierent initial congurations optimization algorithms caused system arrive dierent solutions solutions almost always equivalent terms generalization performance shall rst present results obtained applying lre number problem family tree problem learning representation matrices vectors checked triplet c whether vector smallest euclidean distance r c c indeed b c checked well system learned training set well generalized unseen triplets unless otherwise stated experiments optimized goodness function using scaled conjugate gradient two conditions simultaneously met order algorithm terminate absolute dierence values solution two successive steps less 10 4 absolute dierence objective function values two successive steps less 10 8 experiments presented repeated several times starting dierent initial conditions randomly splitting training test data general solutions found equivalent terms generalization performance algorithm usually converged within hundred iterations rarely got stuck poor local minima 41 results number problem let us consider modular number problem saw section 1 numbers operations exist 90 triplets num1 op num2 lre able learn correctly using 2dimensional vectors matrices n 2 figure 1 shows typical solution obtained training 70 triplets randomly chosen 90 scaled conjugate gradient algorithm converged within desired tolerance 125 iterations see vectors length make angle 210 matrices turn approximately orthogonal row column vectors length therefore approximately decomposed constant factor multiplies orthonormal matrix degrees rotation orthonormal matrix shown second row table 1 matrices multiplicative factor causes result rotation longer second vector triplet concept vectors lie vertices regular polygon centered origin lengthening increases squared distance incorrect answers increases squared distance correct answer thus improving discriminative goodness function eq 3 2dimensional matrix 4 degrees freedom matrices obtain learning 2 degrees freedom extent rotation multiplication factor interesting see simple problem lre often nds appropriate vectors matrices using rotation angles without use extra degree freedom oered matrices multiplicative factor kept every matrix vectors also kept length problem becomes complicated system typically make use extra degree freedom example try solve modular number problem numbers operations f1 two dimensions shall usually nd solution similar one g 6 obtained training system using 350 triplets randomly chosen 450 constituting data set optimization algorithm met convergence criteria 2050 iterations figure vectors obtained learning modular number problem numbers operations two dimensions dots result multiplication r c c triplet c solution obtained optimizing goodness function eq3 using scaled conjugate gradient 2050 iterations 350 triplets randomly chosen 450 used training let us consider nonmodular version number problem numbers operations 0g result operation outside corresponding triplet simply omitted data set two dimensions lre able nd correct solution 430 valid triplets problem training 330 randomly chosen triplets hundred iterations figure 7 shows typical vector conguration learning nonmodular number problem lre increases separation numbers using dierent lengths concept vectors numbers lie spiral gure also indicated cross result multiplying r result operation outside crosses clustered ideal continuation spiral answer located almost exactly point answers 48 system anticipates vectors representing numbers outside given interval ought placed information shall discuss next section consider nonmodular numbers problem numbers operations 3g tried solve 2 dimensions lre could nd solution satised triplets using gradient ascent optimize modied goodness function annealing temperature lre found solution gave correct answer addition subtraction operations matrices representing multiplications divisions mapped vectors tions two dimensions vector endpoints marked stars solid line connects ones representing consecutive numbers dots result multiplication r c c triplet c crosses result multiplication r result operation outside solution obtained optimizing goodness function eq3 using scaled conjugate gradient 1485 iterations 330 triplets randomly chosen 430 used training origin 3 dimensions however lre able nd perfect solution numbers 1 solution found optimizing eq3 using scaled conjugate gradient shown figure 8 optimization algorithm met convergence criteria 726 iterations 1 generalization results lre able generalize well 2 dimensions numbers operations able train system 70 90 triplets training set yet achieve perfect results testing 90 cases problem using numbers training 350 triplets randomly chosen 450 usually got errors testing occasionally errors solution shown g6 gave correct answers 446 cases solution nonmodular number problem numbers operations shown g7 trained 330 total 430 triplets yet able achieve perfect results testing worth pointing order generalizations system discover structure implicit data similar result obtained numbers 1 gure cluttered ations three dimensions dots result multiplication r c c triplet c solution obtained optimizing goodness function eq3 using scaled conjugate gradient 726 iterations 42 results family tree problem rst attempt use lre modied version family tree problem used family trees g2 6 sexless relations instead original 12 hinton 1986 relations spouse child parent sibling nipote zii last 2 italian words either nephew niece either uncle aunt 112 triplets using 2 dimensions lre able complete 112 triplets obtained perfect solution using 3 dimensions however 2dimensional solution quite interesting let us analyzing fig9ab show weight diagrams matrices vectors found training data g9c drawing concept vectors 2space vectors representing people family tree connected see nationality coded using sign second component vector negative english people positive italian people 2 rst component vector codes generation person belongs three valued feature english people rst generation negative values third generation large positive values second generation intermediate positive 2 sign weights typically agrees nationality researcher performed simulations christopher andrew arthur james charles colin penelope christine margaret victoria jennifer charlotte aurelio bortolo pierino pietro marcello alberto maria emma grazia giannina doralice mariemma spouse child parent sibling nipote zii english italian b alberto mariemma aurelio christopher charlotte andrew christine c figure 9 diagrams matrices b vectors obtained modied family tree problem c layout vectors 2d space vectors represented ones family tree connected dots result multiplication r c c triplet c solution shown obtained using gradient ascent optimize modied goodness function temperature annealed italian figure 10 layout vectors 3d space obtained family tree problem vectors represented ones family tree connected dots result multiplication r c c triplet c solution shown obtained using gradient ascent optimize modied goodness function temperature annealed values representations two families linearly separable two families exactly symmetric respect origin interesting fact people coded identical vectors english people christopher penelope andrew christine colin charlotte clever notice people children nephews nieces uncles aunts clearly side eect spouse sibling correct person also fact together fact people family close causes 14 errors 112 triplets tested used lre 3 dimensions family tree problem trained data lre could correctly complete 112 triplets resulting concept vectors shown gure 10 see italian english families symmetric respect origin linearly separable one answer correct aunts colin two concept vectors corresponding two correct answers always two vectors closest r c c table 2 reports distances vector result multiplying concept colin relation aunt person distance jennifer 16064 margaret 16549 charlotte 30865 penelope 32950 christopher 39597 giannina 41198 marcello 44083 alberto 51281 arthur 52167 colin 52673 james 54858 charles 55943 pietro 56432 andrew 63581 aurelio 63880 mariemma 65021 victoria 66853 christine 66973 maria 67626 grazia 71801 doralice 74230 table 2 distance concept vector result multiplying concept colin relation aunt solution family tree problem shown gure 10 generalization results modied version family tree problem 3 dimensions system generalized perfectly 8 new cases got 1 wrong tested 12 new cases original family tree problem 3 dimensions lre generalized perfectly 12 triplets held training particular even information aunts colin ie triplets colin aunt jennifer colin aunt margaret held training system still able answer correctly notice order system rst use implicit information triplets gure meaning relation aunt relative position colin margaret jennifer tree use information make correct inference generalization achieved lre much better neural networks hinton 1986 oreilly 1996 typically made one two errors even 4 cases held training 43 results principal components analysis number family tree problem used principal components analysis pca complete triplets modular nonmodular number problem family tree problem order see compares lre number problems used numbers operations f1 1 2 2 3 3 4 4 0g concept relation used oneoutofn codication thus triplet number problems point dimensions triplet family tree problem point dimensional space chosen certain number principal components tried complete triplets triplet c given rst 2 terms c r c choose completion concept b point c closest pca plane every possible choice b 3 figure 11 shows number triplets correctly completed vs number principal components used modular nonmodular family tree problem respectively 0 10 20 triplets omitted training set notice pca excellent performance nonmodular numbers problem modular version general performance method much worse lre 44 results family tree problem real data used lre solve much bigger family tree task tree branch real family tree one authors containing 49 people using 12 relations seen earlier generates data set 644 triplets lre able learn tree 6 dimensions using scaled conjugate gradient used small number dimensions sometimes possible recognize semantic features learned representation figure 12 shows diagram components vectors column represents person rst 22 vectors represent males others females family tree numbers denote generation tree belong see sign magnitude third component vector codes generation person belongs rst third fth generation negative sign decreasing magnitudes second forth positive sign decreasing magnitude generalization performance good figure 13 plot number errors made system tested whole data set trained subset using 10 dimensions triplets extracted randomly training set system run 5000 iterations 3 also tried reconstruct third term triplet setting components zero projecting resulting triplet principal components space back original space results obtained good number principal components number correct 3010305070number principal components number correct 602060100number principal components number correct figure 11 number triplets correctly completed vs number principal components used solid lines obtained triplets used training dashed lines obtained omitting triplets training set dashdotted lines obtained omitting 20 triplets training set modular number problem numbers operations f1 b number problem numbers operations f1 1 2 2 3 3 4 4 0g c family tree problem figure 12 diagram components vectors obtained learning family tree problem real data 2000 iterations using scaled conjugate gradient 644 triplets used training testing system correctly completed 635 triplets column represents person rst 22 vectors represent males others females family tree numbers denote generation tree belong 1005152535number triplets omitted training number errors figure 13 plot errors made system tested whole set 644 triplets vs number triplets omitted training omitted triplets chosen randomly convergence criteria met results shown median number errors 3 dierent runs since system occasionally failed converge see performance degrades slowly increasing number triplets omitted training data 5 generalization new concepts relations pointed earlier system anticipates vectors numbers outside learned range ought placed information section investigated solution obtain learning set data modied include information new concepts relations let us start training system using lre nonmodular number problem numbers operations f1 1 2 2 3 3 4 4 0g omitting information certain number training set ie omitting triplets contain number either rst third term figure 14a shows vectors found 242 iterations scaled conjugate gradient learning two dimensions eliminated information number 10 training data notice point clearly missing spiral place number 10 go add information missing number training data continue training see iterations vector representing number placed exactly supposed go interesting single triplet containing information new number enough system position correctly happens allow vectors matrices continue learning added extra data point keep vectors matrices xed allow new vector learn figure 14bc shows solution obtained starting solution shown g14a training using triplet 10 1 11 learned position new number one triplet system able generalize answers correctly triplets complete data set also tried learn new relationship clearly dicult since new matrix learned degrees freedom general saw number problem several triplets necessary order learn new matrix would able correctly complete triplets datawhen trained system using lre nonmodular number problem 2 dimensions numbers operations f1 1 2 3 3 4 4 0g usually possible learn matrix 2 operation using four triplets vectors matrices allowed learn six triplets usually necessary new matrix allowed learn everything else kept xed finally tried experiment family tree real data situation b c figure 14 vectors obtained learning number problem numbers operations 0g vector endpoints marked stars solid line connects ones representing consecutive numbers dots result multiplication r c c triplet c information number 10 omitted optimization algorithm met convergence criteria 242 iterations using scaled conjugate gradient triplet 10 1 11 added data set system trained starting conguration b matrices vectors allowed learn algorithm met convergence criteria 239 iterations c vector representing number 10 allowed learn everything else kept xed algorithm met convergence criteria 2 iterations straightforward numbers since triplets contain amount information concept relation triplet pietro wife giannina makes possible locate pietro exactly family tree triplet pietro nephew giulio leaves lot uncertainty pietro could sibling one giulios parents someone married one sibling giulios parents similarly giannina son alberto information relation son giannina aunt virginia relation aunt reason performance system learning new person vector new relation matrix depended triplets added training father one author mentioned 14 triplets data set information omitted lre able complete remaining triplets correctly trained 1501 iterations sucient add single triplet stating person married order locate correctly family tree hand made 5 errors trained adding triplet specied one nephews tried learn new matrix high dimensionality required problem means high number triplets necessary learning 6 developments minor modication tried yet allow system make use negative data form christopher father colin could handled minimizing g instead maximizing using christopher correct answer one limitation version lre presented always picks answer question even correct answer one concepts presented training limitation overcome using threshold distance system answers dont know vector generates threshold distance known concepts preliminary experiments nonmodular number problems successful instead ignoring triplets operation produces answer outside set known numbers include triplets make correct answer dont know example largest known number 10 lre must learn make answer 9 3 threshold distance known numbers succeeds locates answer 9 3 almost exactly point answers 4 sense constructed new concept see gure 15 another limitation separate matrix needed relation requires lot parameters number parameters matrix square dimensionality concept vector space many dierent relations may advantageous model matrices linear combinations smaller set learned basis matrices similarity work tenenbaum figure 15 vectors obtained learning number problem numbers operations 0g vector endpoints marked stars solid line connects ones representing consecutive numbers dots result multiplication r c c triplet c answer among known concepts crosses result multiplication r k k triplet k correct answer dont know cases system answered correctly questions data set triplets used training freeman 1996 paper assumed concepts relations presented arbitrary symbols inherent constraint mapping concepts vectors represent lre also applied concepts already rich informative representation consider task mapping presegmented intensity images pose parameters object contain mapping nonlinear average two intensity images image object average positions orientations scales objects two images suppose discrete sequence images stationary object taken moving camera know camera motion successive image pair appropriately parameterized space pose parameters camera motion represented transformation matrix rt 1 converts one pose vector next central assumption lre therefore exactly satised representation wish learn possible learn mapping intensity images pose vectors sensory representations camera motions transformation matrices backpropagating derivatives obtained eq 3 nonlinear function approximator multilayer neural network preliminary simulations sam roweis personal communication show feasible learn mapping preprocessed intensity images pose vectors mapping camera motions appropriate transformation matrices already given linear relational embedding new method discovering distributed representations concepts relations data consisting binary relations concepts task tried able learn sensible representations data allowed generalize well family tree task real data great majority generalization errors specic form system appears believe brother means son parents fails model extra restriction people cannot brother failure nicely illustrates problems arise explicit mechanism variable binding key technical trick required get lre work use discriminative goodness function eq 3 simply minimize squared distance r c c b c concept vectors rapidly shrink 0 may possible apply technical trick ways implementing relational structures neural networks pollacks raam pollack 1990 sperdutis lraams sper duti 1994 minimize squared distance input output autoencoder also learn representations used input avoid collapsing vectors 0 insisting symbols terminals labels represented xed patterns cannot modied learning possible dispense restriction squared error objective function replaced discriminative function forces output vector autoencoder closer input alternative input vectors acknowledgments authors would like thank peter dayan sam roweis zoubin ghahramani carl van vreeswijk hagai attias marco buiatti many useful discussions r probabilistic interpretation feedforward classi finite state automata simple recurrent neural networks indexing latent semantic analysis finding structure time learning extracting learning distributed representations concepts multidimensional scaling optimizing goodness solution platos problem latent semantic analysis theory acquisition learning humanlike knowledge singular value decom position progress report leabra model neural interactions learning neocortex learning internal representation error propaga tion labeling raam separating style content mit press multidimensional scaling history lawrence erlbaum associates tr ctr petridis vassilis g kaburlasos finknn fuzzy interval number knearest neighbor classifier prediction sugar production populations samples journal machine learning research 4 p1737 1212003 peter dayan images frames connectionist hierarchies neural computation v18 n10 p22932319 october 2006