efficient algorithms blockcyclic array redistribution processor sets abstractruntime array redistribution necessary enhance performance parallel programs distributed memory supercomputers paper present efficient algorithm array redistribution cyclicx p processors cyclickx q processors algorithm reduces overall time communication considering data transfer communication schedule index computation costs proposed algorithm based generalized circulant matrix formalism algorithm generates schedule minimizes number communication steps eliminates node contention communication step network bandwidth fully utilized ensuring equalsized messages transferred communication step furthermore time compute schedule index sets significantly smaller takes omaxpq time less 1 percent data transfer time comparison schedule computation time using stateoftheart scheme based bipartite matching scheme 10 50 percent data transfer time similar problem sizes therefore proposed algorithm suitable runtime array redistribution evaluate performance scheme implemented algorithm using c mpi ibm sp2 results show algorithm performs better previous algorithms respect total redistribution time includes time data transfer schedule index computation b introduction many high performance computing hpc applications including scientific computing signal processing consist several stages 2 10 22 examples applications include multidimensional fast fourier transform alternative direction implicit adi method solving twodimensional diffusion equation linear algebra solvers executing applications distributed memory supercomputer data distribution needed stage reduce performance degradation due remote memory accesses program execution proceeds one stage another data access patterns number processors required exploiting parallelism application may change changes usually cause data distribution stage unsuitable subsequent stage data redistribution relocates data distributed memory reduce remote access overheads since parameters redistribution generally unknown compile time runtime data redistribution necessary however cost redistribution offset performance benefits achieved redistribution therefore runtime redistribution must implemented efficiently ensure overall performance improvement array data typically distributed blockcyclic pattern onto given set processors blockcyclic distribution block size x denoted cyclicx block contains x consecutive array elements blocks assigned processors roundrobin fashion distribution patterns cyclic block distribution special cases blockcyclic distribution gen eral blockcyclic array redistribution problem reorganize array one blockcyclic distribution another ie cyclicx cyclicy important case problem redistribution cyclicx cyclickx arises many scientific signal processing applications type data redistribution occur within processor set different processor sets data redistribution given initial layout final layout consists four major steps index computation communication schedule message packing unpacking finally data transfer four steps contribute total array redistribution cost briefly explain four costs associated data redistribution array elements belong computes local memory location local index array element local index used pack array elements message similarly destination processor determines source processor indices received messages computes local indices find location received message stored total time compute indices denoted index computation cost schedule computation cost communication schedule specifies collection senderreceiver pairs communication step since communication step processor send one message processor receive one message careful scheduling required avoid contention minimizing number communication steps time compute communication schedule significant reducing cost important criteria performing runtime redistribution message packingunpacking cost sender message consists words different memory locations need gathered buffer sending node typically requires memory read memory write operation gather data form compact message buffer time perform data gathering sender message packing cost similarly receiving side message unpacked data words need stored appropriate memory locations data transfer cost data transfer cost communication step consists startup cost transmission cost startup cost incurred software overheads communication operation total startup cost reduced minimizing number message transfer steps transmission cost incurred transferring bits network depends network bandwidth table 1 summarizes key features well known data distribution algorithms literature 4 13 14 15 21 known algorithms ignore one costs schemes focus efficient index set computation completely ignore scheduling communication events based index block schemes focus finding destination processor generating messages destination communication scheduling considered lead node contention performing communication inturn leads higher data transfer costs nodes incur additional delays schemes eliminate node contention explicitly scheduling communication events 3 17 23 although schemes 3 17 23 efficient scheduling algorithm designed data redistribution processor set redistribution different processor sets caterpillar algorithm proposed 11 uses simple round robin schedule avoid node contention key features schedule index computation communication pitfalls 14 communication scheduling index computation using line segment node contention occurs minimize transmission cost communication scheduling efficient index computation source destination sets node contention occurs minimize transmission cost caterpillar simple scheduling algorithm index computation scanning array segments node contention minimize transmission cost number communication steps bipartite matching scheme large schedule computation overhead schedule computation time node contention stepwise strategy minimizes number communication steps greedy strategy minimizes transmission cost scheme fast schedule index computations schedule computation time node contention minimizes number communication steps data transfer cost table 1 comparison various schemes array redistribution however algorithm fully utilize network bandwidth ie size data sent nodes communication step varies node node leads increased data transfer cost schemes 5 reduce data transfer cost however schedule computation cost significant bipartite graph matching used 5 takes op state oftheart workstation time range 100s msecs p q interest problems interest schedule computation cost larger data transfer cost algorithm 5 optimizes data transfer cost number communication steps non alltoall communication case one three cases occur performing redistribution considered algorithm 5 optimize data transfer cost alltoall communication case different message sizes optimize data transfer cost necessary transferred messages equal size communication step paper propose novel efficient algorithm data redistribution cyclicx p processors cyclickx q processors algorithm uses optimal number communication steps fully utilizes network bandwidth step communication schedule determined using generalized circulant matrix framework schedule computation cost q implementations show schedule computation time range 100s secs p q range 50100 processor computes index set communication schedule using set equations derived generalized circulant matrix formulation experimental results show schedule computation time negligible compared data transfer cost array sizes interest message packingunpacking cost scheme generates optimal communication schedule thus scheme minimizes total time data redistribution makes scheme attractive runtime well compiletime data redistribution techniques used implementing scalable redistribution libraries implementing directive hpf 1 developing parallel algorithms supercomputer applications particular techniques lead efficient distributed corner turn operation communication kernel needed parallelizing signal processing applications 26 27 redistribution scheme implemented using mpi c easily ported various hpc platforms performed several experiments illustrate improved performance compared stateoftheart experiments performed determine data transfer schedule index computation costs one experiments used 64 processors ibm sp2 partitioned 28 source processors 36 destination processors expansion factor set 14 array size varied 226 mbytes 564 mbytes compared caterpillar algorithm data transfer times lower ratio data transfer time algorithm caterpillar algorithm 492 537 schedule computation time proposed algorithm much less bipartite matching scheme 5 p q 64 schedule computation time bipartite matching scheme 100s msecs algorithm 100s secs example schedule computation time using bipartite matching scheme 1332 msecs time using algorithm 1786 secs rest paper organized follows section 2 explains tablebased framework also discusses generalized circulant matrix formalism deriving conflict free communication schedules section 3 explains redistribution algorithm index computation section 4 reports experimental results ibm sp2 concluding remarks made section 5 array n48 c cyclic4 q6 processors b cyclic2 p3 processors figure 1 blockcyclic redistribution array point view array elements b p processors c cyclicx p processors cyclickx q processors example 2 approach redistribution section present approach blockcyclic redistribution problem subsection 21 discuss two views redistribution illustrate concept superblock following subsection explain tablebased framework redistribution using destination processor table column row reorganizations subsection 23 discuss generalized circulant matrix formalism allows us compute communication schedule efficiently 21 array processor points view blockcyclic distribution cyclicx array defined follows given array n elements p processors block size x array elements partitioned contiguous blocks x elements th block b consists array elements whose indices vary ix x blocks distributed onto processors roundrobin fashion block b assigned processor j p j paper study problem redistributing cyclicx p processors cyclickx q processors denoted x array point view elements array shown along single horizontal axis processor indices marked block redistribution x q periodicity found p processors cyclic4 q processors c initial distribution table f final distribution table f20 b 22 b 9 b 22 b 9 initial layout b final layout figure 2 blockcyclic redistribution cyclicx p processors cyclickx q processors processor point view example 2 block movement pattern example figure 1 b 0 b 3 b 6 b 9 initially assigned p 0 moved q respectively b 12 p 0 moved q 0 find communication pattern b 0 b 11 repeated blocks collection blocks called superblock period block movement pattern kq size superblock figure 1 superblock size lcm3 2 delta next superblock blocks b 12 b 23 moved fashion processor point view blockcyclic distribution represented 2 dimensional table column corresponds processor row index local block index entry table global block index therefore element j table represents th local block j th processor figure 2 shows example 2 3 2 6 global block index destination processor index figure 3 example destination processor table processor point view blocks distributed table roundrobin fashion table corresponding source processors denoted initial layout representing blocks initially assigned source processors similarly final layout represents blocks assigned destination processors problem redistribute blocks initial layout final layout layouts shown figure 2a respectively initial layout partitioned collections rows size l similarly final layout partitioned disjoint collections rows collection rows note collection corresponds superblock blocks located relative position within superblock moved way redistribution blocks transferred single communication step mpi derived data type handle blocks single block without loss generality consider first superblock following illustrate algorithm refer tables representing indices blocks within first superblock initial final layout initial distribution table final distribution table f shown figure 2c f respectively cyclic redistribution problem essentially involves reorganizing blocks within superblock initial distribution table final distribution table f 22 tablebased framework redistribution given redistribution parameters p k q blocks location f determined redistribution block moves initial location final location f thus processor ownership local memory location block changed redistribution redistribution conceptually considered table conversion process f decomposed independent column row reorganizations column row reorganization reorganization column reorganization 9 7 11 figure 4 table conversion process redistribution column reorganization blocks rearranged within column table therefore local operation within processors memory row reorganization blocks within row rearranged operation therefore leads change ownership blocks requires interprocessor communication destination processor block initial distribution table determined redistribution parameters global block index send communication events table constructed replacing block index initial distribution table destination processor index shown figure 3 denoted destination processor table dpt th entry destination processor index th local block source processor j 0 considers one superblock l theta p matrix row corresponds communication step algorithm communication step processor sends data atmost one destination processor q p atmost p processors destination processor set receive data destination processors remain idle communication step therefore communication step p communicating pairs hand q destination processors receive data time maximum number communicating pairs communication step minp q without loss generality following discussion assume q p figure 4 shows tablebased framework redistribution convert initial distribution table final distribution table f dpt used use communication schedule efficient leads node contention since several processors try send data destination processor communication step example figure 4 step 0 source processors 0 1 try communicate destination processor 0 however every row consists p distinct destination processor indices among node contention avoided communication step motivation column reorganizations eliminate node contention dpt reorganized column reorganizations reorganized table called send communication schedule table section 3 discuss reorganizations performed l theta p matrix well entry destination processor index row corresponds contentionfree communication step maintain correspondence set column reorganizations applied results distribution table 0 corresponding communication step blocks row 0 transferred destination processors specified corresponding entries referring figure 4 first communication step source processors 0 1 2 transfer blocks 0 4 2 destination processors 0 2 1 respectively specified step called row reorganization distribution table 0 f corresponding received blocks destination processors reorganized final distribution table f another set column reorganizations example need operation received blocks stored memory locations destination processors key idea choose required row reorganizationscommunication events performed efficiently supports easytocompute contentionfree communication scheduling far discussed redistribution problem cyclicx p processors cyclickx q processors dual relationship exists problem cyclicx p processors cyclickx q processors problem cyclickx p processors cyclicx processors redistribution cyclickx p processors cyclicx q processors redistribution reverse direction redistribution x q send receive communication schedule table receive send communication schedule table q therefore scheme x extended redistribution problem cyclickx p processors cyclicx q processors 23 communication scheduling using generalized circulant matrix framework communication schedule performs local rearrangement data within processor well interprocessor communication local rearrangement data call column reorganization results send communication schedule table show p k q send communication schedule indeed generalized circulant matrix avoids node contention matrix circulant matrix satisfies following properties 1 n row row 0 circularly right shifted k times 0 k 2 shifted l times note definition extended block circulant matrices changing row row block matrix generalized circulant matrix matrix partitioned blocks size theta n resulting block matrix forms circulant matrix block either circulant matrix generalized circulant matrix figure 5 illustrates generalized circulant matrix two observations generalized circulant matrix blocks along block diagonal identical ii elements row 0 distinct row elements distinct show approach destination processor table transformed generalized circulant matrix distinct elements row 3 efficient redistribution algorithms discussing communication schedule algorithm redistribution classify communication patterns 3 classes redistribution problem x alternative formulation cyclicx cyclicy problem according following lemma let g denote kq generalized circulant matrix circulant matrix submatrix figure 5 generalized circulant matrix lemma 1 communication pattern induced x p k q requires non alltoall communication communication fixed message size k ffg ff integer greater 0 iii alltoall communication different message sizes g k k 6 ffg among three cases case alltoall processor communication message size optimally scheduled using trivial roundrobin schedule however non trivial achieve message size pairs nodes communication step alltoall case different message sizes therefore focus two cases redistribution requiring scheduling non alltoall communication alltoall communication different message sizes 31 non alltoall communication given redistribution parameters p q k get l theta p initial distribution table dpt let g dpt every k th 1 row similar pattern different destination processor indices shuffle rows rows similar pattern adjacent resulting shuffled dpt 1 shuffled dpt 1 divided q 1 slices row direction divided p 1 slices column direction dpt considered k 1 theta p 1 block matrix made q 1 theta g 1 submatrices block matrix converted generalized circulant matrix reorganizing blocks column block matrix reorganizing individual columns within submatrix appropriate amounts results generalized circulant matrix communication schedule matrix b i1 c figure steps column reorganization procedure k identical values row 0 dpt distributed k distinct rows hence row 0 distinct values since generalized circulant matrix elements row distinct achieve conflictfree schedule rigorous proof fact dpt transformed generalized circulant matrix using column reorganizations found 25 schedule communication step p source processors transfer equalsize message p distinct destination processors ensures network bandwidth fully utilized number communication steps also minimized therefore data transfer cost minimized reorganization element moved within column incur interprocessor communication figure 6 shows example dpt x 6 4 converted generalized circulant matrix form column reorganizations example 3 figure 6a shows initial distribution table figure 6d shows corresponding dpt rows shuffled shown figure 6b e partition shuffled tables submatrices size 3 theta 2 diagonalization submatrices diagonalization elements submatrix shown figure 6c f figure 6f generalized circulant matrix gives communication schedule dpt converted send communication schedule table set reorganizations applied initial data distribution table converted 0 shown figure 6 expensive reorganize large amount data within local memory instead reorganization done maintaining pointers elements array source processor table points data blocks packed communication step denoted send data location table entry local block index corresponding entry 0 entry si j points destination processor corresponding entry j scheme computes schedule data index set time algebraic manipulations procedure gives following two equations directly compute individual entries equations denotes quotient integer division remainder integer division similarly n solutions nk proof correctness mathematical formulations found 25 formulae computing communication schedule index set redistribution extremely efficient compared methods presented 5 use bipartite matching algorithm furthermore using formulae processor computes entries needs send communication schedule table hence schedule index set computation performed distributed way total cost computing schedule index set q amortized cost compute step communication schedule index set computation o1 scheme minimizes number communication steps avoids node contention communication step equalsized messages transferred therefore scheme minimizes total data transfer cost 32 alltoall communication different message sizes alltoall communication case arises g g 1 g stated lemma 1 g q first superblock dpt constructed dpt therefore column entries q destination processors column several blocks transferred 31354131020355241357destination processor table send communication schedule table transformed dpt message message figure 7 example illustrating alltoall case different message sizes x 4 3 6 destination column reorganizations stated section 31 applied dpt results generalized circulant matrix k 1 theta p 1 circulant block matrix block q 1 theta g 1 submatrix also circulant matrix block matrix first g 2 blocks column distinct blocks every g th row entries different circularshifted patterns blocks folded onto blocks first row therefore first g 2 rows block matrix used determining send communication schedule table q theta p generalized circulant matrix since blocks every g th 2 row folded onto blocks first row alltoall communication case different message sizes blocks first k 1 mod g 2 rows size k 1 e whole blocks remaining rows size b k 1 c figure 7 shows example send communication schedule table x 4 3 6 generated alltoall case different message sizes example processor entries 6 destination processors corresponding dpt l theta p matrix l applying column reorganizations results generalized circulant matrix considered k 1 theta p 1 block matrix k block 1 first g used table 3 rd row folded onto st row hence message size 1 st row 2 2 nd row 1 k 1 multiple g 2 message size every row therefore network bandwidth fully utilized sending equal sized messages communication step 33 data transfer cost distributed memory model communication cost two parameters startup time transfer time startup time incurred communication event independent message size generally startup time consists transfer request acknowledgment latencies context switch latency latencies initializing message header unit transmission time cost transferring message unit length network total transmission time message proportional size thus total communication time sending message size units one processor another modeled model reorganization data elements among processors processor units data another processor also takes time model assumes node contention ensured communication schedules redistribution using distributed memory model performance algorithm analyzed follows assume array n elements initially distributed cyclicx p processors redistributed cyclickx q processors using algorithms communication costs performing x l case non alltoall communication ii case alltoall communication proof analysis found 25 4 experimental results experiments conducted ibm sp2 algorithms written c mpi table 2 shows comparison proposed algorithm caterpillar algorithm11 bipartite matching scheme5 respect data transfer cost schedule index computation costs alltoall communication case equalsized messages data transfer cost communication step three algorithms also schedule computation performed simple way hence considered table 2 table 2 size array assigned source processor non alltoall communication case p algorithm well bipartite matching scheme perform less number communication steps compared caterpillar algorithm alltoall communication case different message sizes messages transmitted communication step size bipartite matching scheme well algorithm therefore network bandwidth fully utilized total transmission cost caterpillar algorithm transmission cost communication step dominated largest message transferred step let denote size largest message sent communication step note total startup cost algorithms qt since number communication steps hand total transmission cost table 2 comparison data transfer cost schedule index computation costs caterpillar algorithm bipartite matching scheme algorithm note l q non alltoall communication case maximum transfered data size communication step non alltoall communication alltoall communication different message sizes data transfer cost schedule index computation cost data transfer cost schedule index computation cost caterpillar algorithm bipartite matching scheme 5 algorithm p q q bipartite matching scheme algorithm less caterpillar algorithm caterpillar algorithm well algorithm perform schedule index computation oq time however schedule index computation cost bipartite matching scheme op evaluate total redistribution cost data transfer cost consider 3 different scenarios corresponding relative size p q scenario 1 p q scenario scenario experiments choose 3 array consisted single precision integers size element 4 bytes array size chosen multiple size superblock avoid padding using dummy data rest section organized follows subsection 41 reports experimental results overall redistribution time algorithm caterpillar algorithm subsection 42 shows experimental results data transfer time algorithm caterpillar algorithm subsection 43 compares algorithm bipartite matching scheme respect schedule computation time j0 jn1 j ts redistribution routine compute schedule index set i0 in2 source processor source processor pack message send message destination processor else destination processor receive message source processor unpack message ts compute tavg nodetime node compute figure 8 steps measuring redistribution time 41 total redistribution time subsection report experimental results total redistribution time algorithm caterpillar algorithm total redistribution time consists schedule computation time index computation time packingunpacking time data transfer time experiments source destination processor sets disjoint communication step sender packs message sending receiver unpacks message receiving pack operations source processors unpack operations destination processors overlapped ie sending message communication step senders start pack message communication step receivers start unpack message received step methodology measuring total redistribution time shown figure 8 time measured using mpiwtime call n1 number runs run execution redistribution number communication steps processor measures nodetimej j th run generally source destination processors perform interprocessor communication last step complete redistribution earlier processors receive message unpack barrier synchronization mpibarrier performed end redistribution measuring nodetime average nodetime p processors total array size5001500250035004500total redistribution time msec ibm sp2 algorithm caterpillar algorithm total array size5001500250035004500total reditribution time msec ibm sp2 algorithm caterpillar algorithm total array size5001500250035004500total redistribution time msec ibm sp2 algorithm caterpillar algorithm total array size10003000500070009000total redistribution time msec ibm sp2 algorithm caterpillar algorithm maximum time b average time c median time minimum time figure 9 maximum average median minimum total redistribution time computed saved tavg measured value stored array shown figure 8 redistribution performed n1 times maximum minimum median average total redistribution time computed n1 runs experiments n1 set 20 figure 9 total redistribution time algorithm caterpillar algorithm compared ibm sp2 experiments 64 nodes used 28 source processors 36 destination processors total number array elements single precision varied 564480 226 mbytes 14112000 564 mbytes k set 14 figure 9a shows maximum time tmax figure 8 observed large variance measured values figure 9b shows results average time tavg figure 8 figure 9c shows results using median time tmed figure 8 still variance measured values however smaller variance found average maximum time figure 9d shows minimum time redistribution tmin figure 8 plot accurate observation redistribution time since minimum time smallest component due os interference effects related environment remaining plots section show tmin redistribution 2 28 14 36 non alltoall communication case non alltoall communication case messages communication step size total number communication steps algorithm 36 caterpillar algorithm therefore redistribution time algorithm theoretically 50 caterpillar algorithm experimental results shown figure 9d redistribution time algorithm 518 551 caterpillar algorithm figure shows several experimental results non alltoall communication case figure 10a b c show results used number communication steps using algorithm 26 39 52 respectively number communication steps using caterpillar algorithm 78 therefore redistribution time algorithm expected reduced 67 50 33 compared caterpillar algorithm experimental results confirm similar reduction time achieved experimental results shown figure 10 figure 11 compares overall redistribution time alltoall communication case different message sizes figure 11a reports experimental results 4 28 6 36 array size varied 677376 271 mbytes 16934400 677 mbytes case total redistribution time msec ibm sp2 algorithm caterpillar algorithm total array size5001500250035004500 total redistribution time msec ibm sp2 algorithm caterpillar algorithm total array size500150025003500 total redistribution time msec ibm sp2 algorithm caterpillar algorithm total array size500150025003500 total redistribution time msec ibm sp2 algorithm caterpillar algorithm total array size50015002500total redistribution time msec ibm sp2 algorithm caterpillar algorithm total array size50015002500 total redistribution time msec ibm sp2 algorithm caterpillar algorithm c figure 10 total redistribution time non alltoall communication cases total redistribution time msec ibm sp2 algorithm caterpillar algorithm total array size50015002500total redistribution time msec ibm sp2 algorithm caterpillar algorithm total array size50015002500 total redistribution time msec ibm sp2 algorithm caterpillar algorithm total array size50015002500 total redistribution time msec ibm sp2 algorithm caterpillar algorithm c figure 11 total redistribution time alltoall communication cases different message sizes j0 jn1 j redistribution routine compute schedule index set i0 in2 source processor source processor pack message ts send message destination processor ts else destination processor ts receive message source processor ts unpack message compute tavg nodetr node compute figure 12 steps measuring data transfer time algorithms number steps 36 within superblock half messages two blocks half one block algorithm equalsized messages transferred communication step therefore half steps two block messages sent half one block messages sent caterpillar algorithm attempt schedule communication operations send equalsized messages therefore redistribution time step determined time transfer largest message theoretically total redistribution time algorithm reduced 25 compared caterpillar algorithm experiments achieved 179 reduction redistribution time array size small algorithms approximately performance since startup cost dominates overall data transfer cost array size increased reduction time perform distribution using algorithm improves scenarios obtained similar results see figure 11b c 42 data transfer time subsection report experimental results data transfer time algorithm caterpillar algorithm experiments performed manner discussed subsection 41 data sets used experiments used previous subsection data transfer time communication step first measured total data transfer time computed summing measured time communication steps methodology measuring time shown figure 12 figure 13 data transfer time algorithm caterpillar algorithm reported experiments performed ibm sp2 figure 13a reports maximum data transfer time tmax figure 12 large variation measured values observed figure 13b c show average time tavg figure 12 median time tmed figure 12 data transfer time respectively values computed using maximum timetmax figure 13d shows minimum data transfer timetmin plot accurate observation data transfer time since minimum time smallest component due os interference effects related environment therefore accurate comparison relative performance redistribution algorithms remainder section show plots corresponding tmin redistribution 2 28 14 36 non alltoall communication case messages communication step size total number communication steps using algorithm total number steps 36 using caterpillar algorithm therefore data transfer time algorithm theoretically 50 caterpillar algorithm experimental results see figure 13d redistribution time algorithm 492 537 caterpillar algorithm figure 14 shows several experimental results non alltoall communication case similar reductions time achieved experiments figure 15 reports experimental results alltoall communication case different message sizes data transfer time alltoall communication case sensitive network contention since every source processor communicates every destination processor algorithms number steps 36 within superblock half messages two blocks half one block algorithm equalsized messages transferred communication step therefore half steps two block messages sent one block messages sent half caterpillar algorithm attempt send equalsized messages communication step therefore data transfer time step determined time transfer largest message theoretically data transfer time algorithm reduced 25 compared caterpillar 25003500data transfer time msec ibm sp2 algorithm caterpillar algorithm total array size500data transfer time msec ibm sp2 algorithm caterpillar algorithm total array size500data transfer time msec ibm sp2 algorithm caterpillar algorithm total array size500 data transfer time msec ibm sp2 algorithm caterpillar algorithm maximum time b average time c median time minimum time figure 13 maximum average median minimum data transfer times data transfer time msec ibm sp2 algorithm caterpillar algorithm total array size5001500 data transfer time msec ibm sp2 algorithm caterpillar algorithm total array size5001500 data transfer time msec ibm sp2 algorithm caterpillar algorithm total array size5001500 data transfer time msec ibm sp2 algorithm caterpillar algorithm total array size500data transfer time msec ibm sp2 algorithm caterpillar algorithm total array size500 data transfer time msec ibm sp2 algorithm caterpillar algorithm c figure 14 data transfer time non alltoall communication cases data transfer time msec ibm sp2 algorithm caterpillar algorithm total array size500data transfer time msec ibm sp2 algorithm caterpillar algorithm total array size500data transfer time msec ibm sp2 algorithm caterpillar algorithm total array size500data transfer time msec ibm sp2 algorithm caterpillar algorithm c figure 15 data transfer time alltoall communication cases different message sizes table 3 comparison schedule computation time secs procedure 23 used bipartite matching p q k scheme bipartite matching scheme 5 48 276587 207762816 algorithm experiments large message sizes achieved 155 reduction small messages algorithms approximately performance since startup time dominates data transfer time experimental results reported figure 15b c 43 schedule computation time time computing schedule caterpillar algorithm well algorithm negligible compared total redistribution time even though schedule caterpillar algorithm simpler caterpillar algorithm needs time index computation identify blocks packed communication step time approximately schedule computation time schedule computation time bipartite matching scheme 5 much higher caterpillar algorithm algorithm range hundreds msecs quite significant schedule computation cost bipartite matching scheme increases rapidly number processors increases hand algorithm computes communication schedule efficiently processor computes entries send communication schedule table thus schedule computed distributed way schedule computation time range 100s secs comparison scheme bipartite matching scheme respect schedule computation time shown table 3 time scheme includes index computation time bipartite matching scheme time shown schedule computation time conclusions paper showed efficient algorithm performing redistribution cyclicx p processors cyclickx q processors proposed algorithm represented using generalized circulant matrix formalism algorithm minimizes number communication steps avoids destination node contention communication step network bandwidth fully utilized ensuring messages size transferred communication step therefore total data transfer cost minimized schedule index computation costs also important performing runtime redistri bution algorithm schedule index sets computed omaxp q time computation extremely fast compared bipartite matching scheme 5 takes op schedule index computation times small enough negligible compared data transfer time making algorithms suitable runtime data redistribution acknowledgment would like thank staff mhpcc assistance evaluating algorithms ibm sp2 also would like thank manash kirtania assistance preparing manuscript r redistribution blockcyclic data distributions using mpi processor mapping techniques toward efficient data redistribution scheduling blockcyclic array redistribution parallel matrix transpose algorithms distributed memory concurrent computers parallel implementation synthetic aperture radar high performance computing platforms fast runtime block cyclic data redistribution multipro cessors message passing interface forum runtime array redistribution hpf programs efficient algorithms array redistribution automatic generation efficient array redistribution routines distributed memory multicomputers compilation techniques blockcyclic distributions multiphase array redis tribution modeling evaluation multiphase array redis tribution modeling evaluation approach communication efficient data redistribution communication issues heterogeneous embedded systems basiccycle calculation technique efficient dynamic data redistribution scalable portable implementations spacetime adaptive processing efficient algorithms blockcyclic redistribution array shortest augmenting path algorithm dense sparse linear assignment problems tr ctr stavros souravlas manos roumeliotis pipeline technique dynamic data transfer multiprocessor grid international journal parallel programming v32 n5 p361388 october 2004 wang minyi guo daming wei divideandconquer algorithm irregular redistribution parallelizing compilers journal supercomputing v29 n2 p157170 august 2004 jihwoei huang chihping chu efficient communication scheduling method processor mapping technique applied data redistribution journal supercomputing v37 n3 p297318 september 2006 ian n dunn gerard g l meyer qr factorization shared memory message passing parallel computing v28 n11 p15071530 november 2002 chinghsien hsu shihchang chen chaoyang lan scheduling contentionfree irregular redistributions parallelizing compilers journal supercomputing v40 n3 p229247 june 2007 chinghsien hsu sparse matrix blockcyclic realignment distributed memory machines journal supercomputing v33 n3 p175196 september 2005 emmanuel jeannot frdric wagner scheduling messages data redistribution experimental study international journal high performance computing applications v20 n4 p443454 november 2006 minyi guo yi pan improving communication scheduling array redistribution journal parallel distributed computing v65 n5 p553563 may 2005 chinghsien hsu kunming yu compressed diagonals remapping technique dynamic data redistribution banded sparse matrix journal supercomputing v29 n2 p125143 august 2004 minyi guo ikuo nakata framework efficient data redistribution distributed memory multicomputers journal supercomputing v20 n3 p243265 november 2001