optimizing direct threaded code selective inlining achieving good performance bytecoded language interpreters difficult without sacrificing simplicity portability due complexity dynamic translation justintime compilation bytecodes native code mechanism employed universally highperformance interpreterswe demonstrate simple techniques make possible create highlyportable dynamic translators attain much 70 performance optimized c certain numerical computations translators based techniques offer respectable performance without sacrificing either simplicity portability much slower pure bytecode interpreters b introduction bytecoded languages smalltalk gol83 caml ler97 java arn96 lin97 offer significant engineering advantages conventional languages higher levels abstraction dynamic execution environments incremental debugging code modification compact representation executable code cases platform independence success java due largely promise platform independence compactness code compactness bytecodes important advantages network computing code must downloaded ondemand execution arbitrary platform operating system keeping bandwidth requirements minimum disadvantage bytecode interpreters typically offer lower performance compiled code consume significantly resources modern virtual machines perform degree dynamic translation improve program performance deu84 techniques significantly increase complexity virtual machine must tailored permission make digital hard copies part work personal classroom use granted without fee provided copies made distributed profit commercial advantage copies bear notice full citation first page copy otherwise republish post servers redistribute lists requires prior specific permission andor fee sigplan 98 montreal canada c hardware architecture much way conventional compilers backend increases development costs requiring specific knowledge target architecture time writing specific code reduces reliability introducing code debug support languages caml example also traditional compilers produce highperformance native code defeats advantages come platform independence compactness propose novel dynamic retranslation technique applied certain class virtual machines technique delivers high performance 70 optimized c easy retrofit existing virtual machines requires almost effort port new architecture paper continues follows next section gives brief survey bytecode interpretation mechanisms providing context remainder paper novel dynamic retranslation technique explained section 3 section 4 presents results applying technique two interpreters small risclike interpreter inspired work production virtual machine objective caml last two sections contrast technique related work present concluding remarks background interpreter performance depend heavily representation chosen executable code mechanism used dispatch opcodes section describes common techniques 21 pure bytecode interpreters inner loop pure bytecode interpreter simple fetch next bytecode dispatch implementation using switch statement figure 1 shows typical pure bytecode interpreter loop array bytecodes calculate use running example interpreter infinite loop containing switch statement dispatch successive bytecodes case body switch implements one bytecode passes control next bytecode breaking switch pass control back start infinite loop assuming compiler optimizes jump chains breaks implicit jump end body back beginning overheads associated approach follows compiled code unsigned char bytecodeadd bytecode implementations unsigned char unsigned char switch bytecode case bytecodepush3 break case bytecodepush4 break case bytecodeadd stackpointer stackpointer1 break figure 1 pure bytecode interpreter ffl increment instructionpointer ffl fetch next bytecode memory ffl redundant range check argument switch ffl fetch address destination case label table ffl jump address end bytecode ffl jump back start body fetch next bytecode eleven machine instructions must executed powerpc perform push3 bytecode nine instructions dedicated dispatch mechanism including two memory references two jumps among expensive instructions modern architectures pure bytecoded interpreters easy write stand highly portable rather slow case bytecodes perform simple operations push3 example majority execution time wasted performing dispatch 22 threaded code interpreters threaded code bel73 popularized forth programming language moo70 various kinds threaded code efficient generally direct threading ert93 bytecodes simply integers dispatch involves fetching next opcode bytecode looking address associated implementation either explicit table implicitly using switch transferring control address direct threaded code improves performance eliminating table lookup executable code represented sequence opcode implementation addresses dispatch involves fetching next opcode implemen tation address jumping directly address additional optimization eliminates centralized dis patch instead returning central dispatch loop compiled code void opcodeadd opcode implementations dispatch next instruction define next goto instructionpointer void start execution dispatch first opcode opcode implementations opcodepush3 opcodepush4 opcodeadd stackpointer stackpointer1 figure 2 direct threaded code direct threaded opcodes implementation ends code required dispatch next opcode direct threaded version 3 example shown figure 2 1 execution begins fetching address first op codes implementation compiled code jumping address opcode performs work dispatches next opcode implied compiled code hence name control flow threads way opcodes order implied compiled code without ever returning central dispatch loop overheads associated threaded code much lower associated pure bytecode inter preter opcode executed additional overhead dispatching next opcode ffl increment instructionpointer ffl fetch next opcode address memory ffl jump address five machine instructions required implement push3 powerpc three associated opcode dispatch one memory reference one jump saved six instructions pure bytecode approach importantly saved one memory reference one jump instruction ex pensive 23 dynamic translation threaded code benefits direct threaded code easily obtained bytecoded language translating bytecodes 1 threaded code examples written using firstclass labels provided gnu c expression void assigns address type void statement attached given label addr control transferred location using goto dereferences address goto addr note gccs firstclass labels required implement techniques effects achieved couple macros containing lines asm translation table void opcodes dynamic translator unsigned char bytecodepointer firstbytecode void morebytecodestotranslate figure 3 dynamic translation bytecodes threaded code direct threaded code execution illustrated figure 3 translation loop reads bytecode looks address implementation table writes address direct threaded code complication bytecode sets extension bytes provide additional information cannot encoded within bytecode branch offsets indices literal tables environments extension bytes normally placed inline translated threaded code translator immediately threaded opcode corresponding bytecode translation threaded code permits kinds op timization example smalltalk provides four bytecodes pushing implicit integer constant 1 onto stack translator loop could easily translate single pushinteger opcode followed constant pushed inline operand treatment applied kinds literal quantity relative branch offsets another possibility partial decoding translator loop examines loaded bytecode translation time translates one several threaded opcodes translator loop must aware kind operand copying relative offset example might require modification scaling translation loop possible make approximate evaluation approach realistic system squeak ing97 portable pure bytecode implementation smalltalk80 performs numerical computations approximately 37 speed optimized c brouhaha mir87 portable smalltalk virtual machine similar squeak vm except dynamically translates bytecodes direct threaded code execution mir91 brouhaha performs numerical computations 15 speed optimized c implementations carefully handtuned performance essential difference use dynamic translation direct threaded code brouhaha 24 optimizing common bytecode sequences bytecodes typically represent threaded opcodes represent many since encoded pointers translating bytecodes threaded code therefore gives us opportunity make arbitrary transformations executable code one transformation detect common sequences bytecodes translate single threaded macro opcode macro opcode performs work entire sequence original bytecodes example bytecodes push literal push variable add store variable translated single addliteraltovariable opcode threaded code optimizations effective avoid overhead multiple dispatches implied original bytecodes elided within macro opcode single macro opcode translated sequence n original bytecodes avoids dispatches execution time technique particularly important cases bytecodes simple 3 implementation bytecode short single registerregister machine instruction cost threading often significantly larger cost useful execution three instructions must executed dispatch next opcode overhead threading instructions executed 12 instructions dispatching threaded opcodes overhead drops 43 operation optimized single macro opcode four useful instructions 3 instructions threading 2 dispatching opcode implementations noncontiguous addresses also undermines code locality causing unnecessary processor pipeline stalls inefficient utilization instruction cache tlbs combining common sequences bytecodes single macro opcode considerably reduces effects compiler also chance make interbytecode optimizations within implementation single macro opcode impossible make implementations individual bytecodes determining appropriate set common bytecode sequences difficult virtual machine instrumented record execution traces simple offline analysis reveal likely candidates corresponding pattern matching macro opcode implementations incorporated manually vm example analysis applied earlier version objective caml bytecode set resulting new set bytecodes includes several macrostyle operations 25 problems static optimization significant problem static approach number possible permutations even shortest common sequences consecutive bytecodes pro hibitive example smalltalk provides 4 bytecodes push popular integer constants minus one two bytecodes load store 32 temporary 256 receiver variables manually optimizing possible permutations incrementing decrementing variable small constant would require translator implement 2304 explicit special cases clearly unreasonable problem made acute since different applications running virtual machine favor different sequences bytecodes statically chosing single optimal set common sequences therefore impossible technique focuses making choice runtime allows set common sequences nearly optimal particular application run instruction counting accurate way estimate savings since instructions avoid expensive execute dynamicopcodepush3push4add stackpointer stackpointer stackpointer1 goto figure 4 equivalent macro opcode push3 push4 add int nfibsint n return n 2 figure 5 benchmark function c dynamically rewriting opcode sequences generate implementations common bytecode sequences dynamically implementations available new macro opcodes single macro opcode replaces several threaded opcodes generated original common bytecode sequence dynamically generated macro opcodes executed precisely manner interpreters predefined opcodes original execution mechanism direct threading requires modification transformation performed either bytecodetothreaded code translation separate pass already threaded code figure 4 shows equivalent c dynamically generated threaded opcode sequence three bytecodes needed evaluate 3 4 example translator concatenates compiled c implementations several intrinsic threaded opcodes one corresponding bytecode sequence optimized since involves relocating code safe perform concatenation threaded opcodes whose implementation position independent general three cases consider concatenating opcode implementations ffl threaded opcode cannot inlined implementation contains call c function destination address relative processors pc destination addresses would invalidated copied form new macro opcodes implementation ffl threaded opcode changes flow control threaded code must appear end translated sequence different paths sequence might consume different numbers inline arguments ffl threaded opcode branch destination appear beginning macro opcode since incorporating middle macro opcode would delete branch destination final threaded code simplified following rule consider basic blocks inlining basic block begins jump destination ends either jump nfibs push r1 r1 saved call move jge r0 r1 cont pop r1 restore r1 return cont move r0 r1 else arg r1 call nfibs call nfibs add add pop r1 restore r1 return start move 32 r0 call nfibs32 call nfibs print figure threaded code nfibs benchmark inlining destination change control flow inlining pur poses opcodes contain c function call considered singleopcode basic blocks restriction relaxed target architecture andor compiler used build vm uses absolute addresses function call destinations technique designed works best finegrained opcodes implementations short typically machine instructions therefore cost opcode dispatch dominates next section presents example context 31 simple example illustrate technique applying simple risclike virtual machine executing nfibs func tion shown figure 5 3 example interpreter implements registerbased execution model handful registers performing arithmetic stack used saving return addresses contents clobbered registers subroutine calls direct threaded code two kinds inline operand instruction pointerrelative offsets branch destinations absolute addresses function call destinations interpreter translates bytecodes threaded code two passes makes first pass bytecodes expanding threaded opcodes inlining exactly explained section 23 figure 6 shows symbolic listing nfibs function implemented example interpreters opcode set initial translation threaded code bytecode operands placed inline threaded code translation example offset jge opcode call destinations placed directly opcode stream immediately associated opcode represented pseudooperand fig 3 doublyrecursive function interesting property result number function calls required calculate result nfibs cont nfibs nfibs figure 7 threaded code nfibs benchmark inlining implementations new macro opcodes shown right ure appear separate line code prefixed initial translation threaded code second pass performs inlining threaded code basic blocks identified used dynamically generate new threaded macro opcodes corresponding original sequences threaded opcodes replaced single macro opcodes rewriting threaded code performed insitu since optimizing opcode sequence always result shorter sequence optimized code possibility overwriting opcode yet considered inlining figure 7 shows code nfibs function inlining taken place function reduced five threaded macro opcodes shown 1 5 replacing basic block original code implementation new macro opcode concatenation implementations opcodes replaces new implementations written separate area memory called macro cache five implementations required nfibs shown within curly braces figure one ends copy implementation pseudoopcode thr threading operation dispatch next opcode inline arguments copied verbatim except cont jump offset adjusted appropriately transla tor inline arguments used macro opcode implementations points marked figure help identification basic blocks divide threaded opcodes four classes follows inline opcodes implementation inlined macro opcode without restriction arithmetic opcodes belong class protect implementation contains c function call therefore cannot inlined print opcode belongs class final opcode changes flow control therefore defines end basic block eg call relative opcode changes flow control therefore defines end basic block eg conditional branch jge difference final relative way opcodes inline operand treated first case operand absolute copied directly final translated code second case operand relative current threaded program counter must adjusted appropriately final translated code figure 8 shows translator code initializes threaded opcode table along representative implementations several threaded opcodes four classes threaded opcode represented define define pop sp define get longip read inline operand define next goto ip dispatch next opcode define protect 0x00 never expanded define inline 10 expanded define final 11 expanded ends basic block define relative 12 expanded ends basic block offset follows define opname nargs flags case initialip break startname opcode body define initialize rather execute see macro op int switch op opjger0r1 1 relative register long r0 r1 ip offset opcall 1 final register long dest get dest default fprintfstderr panic op undefinedn op abort figure 8 opcode table initialization translators inlining loop shown figure 9 complex might first appear code pointer translated threaded code rewritten insitu indices code pointing next opcode copied inlined location copied respectively times loop considers opcode inlining inlining loop entered current opcode opcode following inlined case opcode copied along inline arguments directly nextmacro pointer next unused location macro cache inlining loop first writes address represents threaded opcode macro implementation generated copies compiled implementations opcodes macro cache inlined threaded opcodes copied although inline arguments encountered copied directly inlining loop continues copies implementation opcode explicitly ends basic block relative next opcode either noninlinable int int nextin long infothisopflags inline infonextopflags protect inline create new macro opcode nextmacro void new macro opcode infothisopflags protect icopyinfothisopaddr ep infothisopsize infothisopflags relative locn offset int infothisopflags final infothisopflags relative destinationin break end basic block copy threading operation icopyinfothraddr ep infothrsize cant inline copy opcode inline arguments infothisopflags relative copy literal arguments int figure 9 dynamic translator loop protected branch destination implicitly ending current basic block translator appends implementation pseudoopcode thr thre ading operation finally nextmacro location updated ready next inlining operation translator loop uses array flags destination identify branch destinations within threaded code array easily constructed translators first pass bytecodes expanded noninlined threaded code loop also creates two arrays relocations patchlist used recalculate relative branch offsets 4 inlining loop concatenates opcode implementations using icopy function shown figure 10 function similar bcopy except also synchronizes pro cessors instruction data caches ensure new macro opcodes implementation executable contains line platformdependent code interpreter 4 branch destination identification relative offset recalculation shown seen full source code example interpreter see appendix static inline void icopyvoid source void dest sizet size bcopysource dest size size asm dcbst 00 sync icbi 00 isync rp elif definedsparc asm flush 0 stbar rp noop elif defined endif dest 4 size 4 figure 10 icopy function containing single line platformdependent code 32 saving space translating multiple copies opcode sequences would waste space therefore keep cache dynamically generated macro opcodes keyed hash value computed incoming unoptimized opcodes translation case cache hit reuse existing macro opcode translated code immediately reclaim macro cache space occupied newly translated version case cache miss newly generated macro opcode used translated code hash table updated include new opcode ensures never one macro opcode corresponding given sequence unoptimized opcodes 4 experimental results particularly interested performance benefits dynamic inlining applied interpreters finegrain instruction sets nevertheless also curious see technique would perform applied interpreter coarsegrained bytecode set took measurements contexts using risclike interpreter widelyused less suited interpreter objective caml language 41 finegrained opcodes risclike interpreter opcode set similar presented section 31 configured compile time use bytecodes direct threaded code direct threaded code dynamicallygenerated macro opcodes performance two benchmarks measured using terpreter functioncall intensive fibonacci benchmark presented earlier nfibs memory intensive function call free prime number generator sieve table 1 shows number seconds required execute benchmarks several architectures 133mhz pentium sparcstation 20 200mhz powerpc 603ev figures shown simple bytecode interpreter interpreter performing translation direct threaded code direct threaded code dynamic inlining common opcode sequences benchmark written c compiled optimization options o2 interpreter final column shows performance inlined threaded code compared optimized c nfibs machine bytecode threaded inlined c inlinedc pentium 632 371 223 111 498 sieve machine bytecode threaded inlined c inlinedc pentium 251 176 132 46 348 table 1 nfibs sieve benchmark results three architectures tested final column shows speed inlined threaded code relative optimized c pentium pentium bytecode direct threaded inlined figure 11 benchmark performance relative optimized c nfibs spends much time performing arithmetic registers memory stack operations performed function call return interpreter allocates first vm registers physical machine registers whenever possible opcodes perform arithmetic therefore typically compiled single machine instruction sparc powerpc two architectures show marked improvement performance common sequences inlined single macro opcodes due significantly reduced ratio opcode dispatch real work effect less pronounced pentium machine registers vm registers must kept memory arithmetic opcode compiles several pentium instructions therefore ratio dispatch overhead real work lower risc architectures observe marked improvement approximately factor two successive versions interpreter nfibs sieve shows less pronounced improvement spends majority time performing memory opera tions contribution opcode dispatch overall execution time therefore smaller nfibs also interesting observe performance version interpreter relative optimized c figure 11 shows nfibs gains approximately 14 speed optimized c moving bytecoded representation threaded code gain moving threaded inlined threaded code dependent architecture approximately 20 pentium 38 sparc gains sieve smaller less dependent architecture approximately 9 step three architectures 42 objective caml also applied technique objective caml bytecode interpreter order obtain realistic measurements performance overheads less favorable environment objective caml chosen design implementation interpreters core clean simple understanding making required modifications present significant challenge furthermore fullyfledged system includes bytecode com piler benchmark suite large applications made easier collect meaningful statistics interpreter also equipped mechanism bulktranslate bytecodes threaded code startup platforms support 5 needed extend initial translation phase perform analysis opcode sequences generate macro opcode implementa tions rewrite threaded code insitu use dynamicallygenerated macro opcodes implementing technique caml virtual machine took one day two small details required careful attention first presence switch opcode performs multiway branch followed threaded code inline table mapping values onto branch offsets added special case translator loop handle opcode second existence handful opcodes consume two inline arguments literal relative offset introduced new opcode class relative2 differs relative copying additional inline literal argument offset translator loop translation algorithm identical respects one presented section 3 ran standard objective caml benchmark suite 6 modified vm see table 2 vm instrumented gather statistics relating execution speed 5 uses gccs firstclass labels portablyftpftpinriafrinriaprojectscristalxavierleroy benchmarksobjcamltargz boyer fib genlex qsort qsort sieve soli soli takc taku speed inlinednoninlined pentium sparc powerpc figure 12 objectivecaml benchmark results three architectures tested vertical axis shows performance relative original noninlining interpreter asterisks indicate versions benchmarks compiled array bounds checking disabled boyer term processing function calls fib integer arithmetic function calls 1 arg genlex lexing parsing symbolic processing kb term processing function calls functionals qsort integer arrays loops sieve integer arithmetic list processing functionals soli puzzle solving arrays loops takc integer arithmetic function calls 3 args curried taku integer arithmetic function calls 3 args tuplified table 2 objective caml benchmarks memory usage characteristics dynamically generated macro opcodes figure 12 shows performance benchmarks inlining relative original performance without inlining important note objective caml bytecode set already optimized statically described section 24 ler98 improvements therefore due mainly elimination dispatch overhead common sequences particular application virtual machines whose bytecode sets stat ically optimized way would benefit technique see figure majority benchmarks benefit significant performance advantage inlining cases inlined version runs 50 faster original two benchmarks running twice fast original noninlined version sparc clear improvements related processor architecture probably due differences cost threading operation sparc ex ample avoiding pipeline stalls associated threading seems make significant difference figure 13 shows final size macro cache benchmark sparc plotted factor size original unoptimized code final macro cache135 cache size original code size original code size kbytes figure 13 macro cache size diamonds optimized threaded code size crosses plotted factor original code size sizes vary slightly architecture since depend size bytecode implementations however shape case average ratios original bytecode size macro cache size show cost three four times size original code sparc ratio almost identical powerpc slightly smaller pentium observe ratio decreases gradually original code size increases expected since larger bodies code tend reuse macro opcodes rather generating new ones tested translating bytecoded version objective caml compiler 421532 bytes original code generated 941008 bytes macro opcode implementation sparc approximately 22 times size original code shown rightmost point graph inlined threaded code always smaller original code generated figure 13 also shows final optimized code size benchmark observe ratio independent size benchmark also expected since reduction size dependent average number opcodes common sequence density corresponding macro opcodes final code depend mainly characteristics language opcode set systems longlived object memory generate new executable code runtime realistic implementation systems would recycle macro cache space possibly use profiling optimize popular areas program example 68040lc emulator found macintosh systems performs dynamic translation 68040 powerpc code normally requires 250kb cache commonly used translated code sequences stored tho95 similar fixed cache size effective brouhaha smalltalk system mir97 translation speed also important factor measure ran object caml bytecode compiler much larger program benchmarks modified interpreter 105383 opcodes objective caml compiler translated 022 seconds sparc rate 480000 opcodes per second inlining interpreter executes compiler rate 24 million opcodes per sec ond translation therefore approximately five times slower execution 7 5 related work brouhaha objective caml demonstrated benefits creating specialized macro opcodes perform work sequence common opcodes objective caml led new bytecode set brouhaha standard smalltalk80 bytecodes translated threaded code execution detection limited number predetermined common bytecode sequences performed translation specialized opcode substituted executable code contribution extension technique dynamically analyze generate implementations new macro opcodes runtime several systems use concatenation precompiled sequences code runtime aus96 noe98 completely different context precompiled code sequences generic templates parameterized runtime particular constant values templatebased approach also used commercial smalltalk virtual machines perform dynamic compilation native code mir97 however technique complex requires significant effort implement templates new architecture interesting system portable dynamic code generation vcode eng96 architectureneutral runtime sembler generates code approaches performance c architectures main disadvantage retrofitting existing virtual machine requires significant amount effort certainly single day required implement technique production virtual machine simple nfibs benchmark runs 40 faster using vcode compared risclike inlined threaded code virtual machine superoperators pro95 technique specializing bytecoded c interpreter according program execute possible specialized 7 since translation performed opcode breakeven point passed program executes six times number opcodes contains interpreter generated time compiled bytecoded representation program compiletime analysis program chooses likely candidates super operators implemented new interpreter bytecodes superoperators similar macro opcodes one advantage corresponding synthesized bytecodes benefit interopcode optimizations simple concatenation implementations fails exploit however superoperators require bytecodes corresponding precisely nodes used build parse trees might always best choice bytecode set would also tricky use superoperators incremental system smalltalk new executable code generated runtime nevertheless investigation merging techniques superoperators dynamicallygenerated macro opcodes might worthwhile 6 conclusions work inspired need create interpreter finegrain risclike opcode set general tied particular highlevel language amenable traditional compiler optimizations cost opcode dispatch significant context compared abstract interpreters whose bytecodes carefully matched language semantics expected benefits technique related average semantic content bytecode would expect languages tcl perl relatively highlevel opcodes benefit less macroization interpreters risclike opcode set benefit since cost dispatch significant compared cost executing body bytecode objective caml bytecode set positioned two extremes containing simple complex opcodes 8 vcode better performance technique instruction set matches closely underlying architecture exert fine control code generated performing degree reordering better instruction scheduling believe similar results achieved risclike inlining threaded code interpreter portable manner performance macro opcodes limited inability compiler perform interopcode optimizations possible static analysis performed new macro opcodes implemented manually terpreter believe limitations less important using finegrain opcode set corresponding closely traditional risc architecture opcodes implemented single machine instruction new opportunities interopcode optimization available translators code generator technique portable simple implement orthogonal implementation virtual machines op codes reducing overhead opcode dispatch helps bring performance finegrained bytecodes level abstract languagedependent opcode sets 8 significant overheads associated technique used check stack overflow pending signals objective caml discussion beyond scope paper speed seconds space bytes pentium sparc powerpc sparc benchmark original inlined original inlined original inlined original inlined cache boyer 20 181 111 23 150 154 14 119 113 13800 8324 42012 fib 20 144 140 40 247 163 16 112 139 5288 3320 20160 genlex 10 093 110 11 084 127 07 059 118 45696 26856 156892 kb 103 815 126 169 771 219 63 536 118 20968 13048 75868 qsort 58 395 146 95 539 175 41 298 137 6676 3932 26416 qsort 48 304 158 80 426 188 33 227 147 6532 3884 25280 sieve 30 279 107 25 222 110 19 186 100 5200 3312 20124 soli 31 218 144 51 298 170 21 150 142 6644 3952 25516 soli 24 138 172 40 200 202 16 093 168 6544 3908 24548 takc 28 191 144 50 326 152 21 147 142 4784 3012 18652 taku 49 320 152 70 414 170 32 233 139 4812 3036 18296 table 3 raw results objectivecaml benchmarks acknowledgements authors would like thank xavier leroy john mal oney eliot miranda dave ungar mario wolczko anonymous referees helpful comments draft paper r java programming lan guage communications acm efficient implementation smalltalk80 system engler vcode retargetable portable forth engine back future story squeak objective caml system release 105 java virtual machine specification fast direct optimizing ansi c interpreter superoperators building better virtual cpu tr smalltalk80 language implementation brouhaha portable smalltalk interpreter fast effective dynamic compilation back future java programming language 2nd ed java virtual machine specification efficient implementation smalltalk80 system ctr alex iliasov templatesbased portable justintime compiler acm sigplan notices v38 n8 august fabrice bellard qemu fast portable dynamic translator proceedings usenix annual technical conference 2005 usenix annual technical conference p4141 april 1015 2005 anaheim ca jinzhan peng gansha wu gueiyuan lueh code sharing among states stackcaching interpreter proceedings 2004 workshop interpreters virtual machines emulators june 0707 2004 washington dc ben stephenson wade holst multicodes optimizing virtual machines using bytecode sequences companion 18th annual acm sigplan conference objectoriented programming systems languages applications october 2630 2003 anaheim ca usa brian davis john waldron survey optimisations java virtual machine proceedings 2nd international conference principles practice programming java june 1618 2003 kilkenny city ireland anton ertl david gregg combining stack caching dynamic superinstructions proceedings 2004 workshop interpreters virtual machines emulators june 0707 2004 washington dc andrew beatty kevin casey david gregg andrew nisbet optimized java interpreter connected devices embedded systems proceedings acm symposium applied computing march 0912 2003 melbourne florida bertil folliot ian piumarta fabio riccardi dynamically configurable multilanguage execution platform proceedings 8th acm sigops european workshop support composing distributed applications p175181 september 1998 sintra portugal marc berndl laurie hendren dynamic profiling trace cache generation proceedings international symposium code generation optimization feedbackdirected runtime optimization march 2326 2003 san francisco california brian davis andrew beatty kevin casey david gregg john waldron case virtual register machines proceedings workshop interpreters virtual machines emulators p4149 june 1212 2003 san diego california anton ertl david gregg retargeting jit compilers using ccompiler generated executable code proceedings 13th international conference parallel architectures compilation techniques p4150 september 29october 03 2004 henrik nssn mats carlsson konstantinos sagonas instruction merging specialization sicstus prolog virtual machine proceedings 3rd acm sigplan international conference principles practice declarative programming p4960 september 0507 2001 florence italy mourad debbabi abdelouahed gherbi lamia ketari chamseddine talhi hamdi yahyaoui sami zhioua synergy efficient interpretation fast selective dynamic compilation acceleration embedded java virtual machines proceedings 3rd international symposium principles practice programming java june 1618 2004 las vegas nevada mathew zaleski marc berndl angela demke brown mixed mode execution context threading proceedings 2005 conference centre advanced studies collaborative research p305319 october 1720 2005 toranto ontario canada anton ertl david gregg optimizing indirect branch prediction accuracy virtual machine interpreters acm sigplan notices v38 n5 may yunhe shi david gregg andrew beatty anton ertl virtual machine showdown stack versus registers proceedings 1st acmusenix international conference virtual execution environments june 1112 2005 chicago il usa marc berndl benjamin vitale mathew zaleski angela demke brown context threading flexible efficient dispatch technique virtual machine interpreters proceedings international symposium code generation optimization p1526 march 2023 2005 benjamin vitale tarek abdelrahman catenation specialization tcl virtual machine performance proceedings 2004 workshop interpreters virtual machines emulators june 0707 2004 washington dc k venugopal geetha manjunath venkatesh krishnan sec portable interpreter optimizing technique embedded java virtual machine proceedings 2nd java virtual machine research technology symposium p127138 august 0102 2002 anton ertl david gregg andreas krall bernd paysan vmgen generator efficient virtual machine interpreters softwarepractice experience v32 n3 p265294 march 2002 jeffery von ronne ning wang michael franz interpreting programs static single assignment form proceedings 2004 workshop interpreters virtual machines emulators june 0707 2004 washington dc mathew zaleski angela demke brown kevin stoodley yeti gradually extensible trace interpreter proceedings 3rd international conference virtual execution environments june 1315 2007 san diego california usa mourad debbabi abdelouahed gherbi azzam mourad hamdi yahyaoui selective dynamic compiler embedded java virtual machines targeting arm processors science computer programming v59 n12 p3863 january 2006 arun kejariwal xinmin tian milind girkar wei li sergey kozhukhov utpal banerjee alexander nicolau alexander v veidenbaum constantine polychronopoulos tight analysis performance potential thread speculation using spec cpu 2006 proceedings 12th acm sigplan symposium principles practice parallel programming march 1417 2007 san jose california usa david gregg andrew beatty kevin casey brain davis andy nisbet case virtual register machines science computer programming v57 n3 p319338 september 2005 etienne gagnon laurie j hendren sablevm research framework efficient execution java bytecode proceedings javatm virtual machine research technology symposium javatm virtual machine research technology symposium p33 april 2324 2001 monterey california gregory sullivan derek l bruening iris baron timothy garnett saman amarasinghe dynamic native optimization interpreters proceedings workshop interpreters virtual machines emulators p5057 june 1212 2003 san diego california ana azevedo arun kejariwal alex veidenbaum alexandru nicolau high performance annotationaware jvm java cards proceedings 5th acm international conference embedded software september 1822 2005 jersey city nj usa scott thibault charles consel julia l lawall renaud marlet gilles muller static dynamic program compilation interpreter specialization higherorder symbolic computation v13 n3 p161178 sept 2000 mahmut taylan kandemir improving wholeprogram locality using intraprocedural interprocedural transformations journal parallel distributed computing v65 n5 p564582 may 2005 john aycock brief history justintime acm computing surveys csur v35 n2 p97113 june