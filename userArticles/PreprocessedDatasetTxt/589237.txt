bfgs update skipping varying memory give conditions limitedmemory quasinewton methods exact line searches terminate n steps minimizing ndimensional quadratic functions show although broyden family methods terminate n steps fullmemory versions bfgs limitedmemory additionally show fullmemory broyden family methods exact line searches terminate n steps p matrix updates skipped introduce new limitedmemory bfgs variants test nonquadratic minimization problems b introduction quasinewton family algorithms remains standard workhorse minimization many methods share properties finite termination strictly convex quadratic functions linear superlinear rate convergence general convex functions need store evaluate second derivative matrix general approximation second derivative matrix built accumulating results earlier steps descriptions many quasinewton algorithms found books luenberger 16 dennis schnabel 7 golub van loan 11 although infinite number quasinewton methods one method surpasses others popularity bfgs algorithm broyden fletcher goldfarb shanno see eg dennis schnabel 7 method exhibits robust behavior relatives many attempts made explain robustness complete understanding yet obtained 23 one result work paper small step toward understanding since investigate question much information dropped bfgs quasinewton methods without destroying property quadratic termination answer question context exact line search methods find minimizer onedimensional subspace every iteration practice inexact line searches satisfy side conditions proposed wolfe see x43 substituted exact line searches focus modifications wellknown quasinewton algorithms resulting limiting memory either discarding results early steps x2 skipping updates second derivative approximation x3 give conditions quasinewton methods terminate n steps minimizing quadratic functions n variables although broyden family methods see x2 terminate n steps fullmemory versions show bfgs nstep termination limitedmemory also show methods broyden family terminate n steps even p updates skipped termination lost skip updates limit memory applied mathematics program university maryland college park md 20742 gibsonmathumdedu work supported part national physical science con sortium national security agency university maryland z department computer science institute advanced computer studies university maryland college park md 20742 olearycsumdedu work supported national science foundation grant nsf 9503126 x department pure applied mathematics washington state university pullman wa 99164 nazarethamathwashingtonedu gibson p oleary l nazareth x4 report results experiments new limitedmemory bfgs variants problems taken cute 3 test set showing savings time achieved notation matrices vectors denoted boldface uppercase lowercase letters respectively scalars denoted greek roman letters superscript denotes transpose subscripts denote iteration number products always taken left right notation spanfx 1 denotes subspace spanned vectors x whenever refer ndimensional strictly convex quadratic function assume form positive definite n theta n matrix b nvector 2 limitedmemory variations quasinewton algorithms section characterize fullmemory limitedmemory methods terminate n iterations ndimensional strictly convex quadratic minimization problems using exact line searches fullmemory versions methods discuss known terminate n iterations limitedmemory bfgs lbfgs shown nocedal 22 terminate n steps preconditioned conjugate gradient method cast limitedmemory quasinewton method also known terminate n iterations see eg luenberger 16 little else known termination limitedmemory methods let fx denote strictly convex quadratic function minimized let gx denote gradient f define kth iterate let denote change current iterate denote change gradient let x 0 starting point let h 0 initial inverse hessian approximation 1 compute 2 choose ff k 0 fx k 3 set 4 set x 5 compute 7 choose h k1 fig 21 general quasinewton method present general result characterizes quasinewton methods see figure 21 terminate n iterations restrict methods update form mk lbfgs variations 3 1 h 0 n theta n symmetric positive definite matrix remains constant k fl k nonzero scalar thought iterative rescaling 2 p k n theta n matrix product projection matrices form n theta n matrix product projection matrices form u nvector 3 k nonnegative integer w ik nvector z ik vector spanfs g refer form general form general form fits many known quasinewton methods including broyden family limitedmemory bfgs method assume quasinewton methods satisfy secant condition h k1 positive definite symmetric symmetric positive definite updates desirable since guarantees quasinewton method produces descent directions note update positive definite may produce k case choose ff k negative ff rather positive ff example method steepest descent 16 fits general form 21 k define note neither w z vectors specified since example 1st update conjugate gradient method preconditioner fits general form 21 example lbfgs update see nocedal 22 limitedmemory constant written kgammam k1k lbfgs fits general form 21 iteration k choose 4 gibson p oleary l nazareth observe p k q k z ik obey constraints imposed construction bfgs related lbfgs following way use every pair formation update ie unlimited memory would creating updates bfgs practice however one would never would take memory storing bfgs matrix example define limitedmemory dfp ldfp definition consistent definition limitedmemory bfgs given nocedal 22 let 1 mg order define ldfp update need create sequence auxiliary matrices u dfp h ss matrix k1 result applying dfp update k times matrix h 0 k recent pairs thus 1st ldfp matrix given simplify description note k1 rewritten kgammam ki kgammam ki kgammam ki kgammam ki kgammam ki kgammam kj kgammam kj kgammam kj kgammam kl kgammam kl thus h k1 written mk ik kgammam ki kgammam ki kgammam ki mk kgammam kj h j gamma1 kgammam kj h j gamma1 lbfgs variations 5 equation 27 looks much like general form given 21 ldfp fits general form following choices kgammam ki kgammam ki z except choice p k trivial verify choices satisfy general form prove p k satisfies requirements need show proposition 21 limitedmemory dfp following two conditions hold value k proof prove via induction suppose recall spanfs 0 g trivially equal spanfh 0 g 0 g furthermore conclude base case holds assume use induction show 210 1st case using induction assumptions induction k get 6 gibson p oleary l nazareth assume induction assumption next values maps vector v using induction assumptions k get continue induction hence induction complete proves 210 k 1st case consider mk kgammam ki kgammam ki using structure v jk 210 see hence 211 also holds k 1st case example broyden class broyden family class quasinewton methods whose matrices linear combinations dfp bfgs matrices see eg luenberger 16 chap 9 parameter oe usually restricted values guaranteed produce positive definite update although recent work sr1 broyden class method khalfan byrd schnabel 14 may change practice restriction oe necessary development theory broyden class update expressed variations 7 sketch explanation fullmemory version fits general form given 21 limitedmemory case similar rewrite broyden class update follows hence hi left reader show h k k spanfs thus broyden class updates fit form 21 21 termination limitedmemory methods section show methods fitting general form 21 produce conjugate search directions orem 22 terminate n iterations corollary 23 p k maps spanfy spanfy n furthermore condition p k satisfied k used formation corollary 24 theorem 22 suppose apply quasinewton method figure 21 update form 21 minimize ndimensional strictly convex quadratic function k termination ie g k1 6 0 proof assume 215 holds prove 212214 induction since line searches exact g 1 orthogonal 0 using fact p 0 8 gibson p oleary l nazareth 215 fact z i0 2 spanfs 0 g implies g see 1 conjugate 0 since z i0 w 0 lastly spanfs 0 g base case established assume claims 212214 hold also hold vector g k1 orthogonal k since line search exact using induction hypotheses g k orthogonal fs conjugate g see hence 212 holds prove 213 note sufficient prove g use following facts k1 since v projections used form q k k1 orthogonal span since z k spanfs orthogonal span iii since assuming 215 holds true exists expressed p iv orthogonal h 0 g k1 orthogonal spanfs 214 thus variations 9 0 thus 213 holds k lastly using ii maps vector v spanfv construction exist hence show equality sets show h 0 g k1 linearly independent g already know basis fh 0 linearly independent since spans space linearly independent set fs number elements suppose h 0 g k1 linearly indepen dent exist oe k zero recall g k1 orthogonal fs g induction assumption implies g k1 also orthogonal fh 0 g thus j 0 k positive definite g j nonzero conclude oe j must zero since true every j zero k contradiction thus set fh 0 linearly independent hence 214 holds k assume 212214 hold k g k1 6 0 215 hold ie exist j k g k1 6 0 j 0 k 2 gibson p oleary l nazareth lead contradiction construction p k exist assumption 216 k must nonzero 213 follows g using facts ii iv 214 217 get mk z ik w ik mk thus since neither fl k k zero must contradiction since h 0 positive definite g k1 assumed nonzero method produces conjugate search directions say something termination corollary 23 suppose method type described theorem 22 satisfying 215 suppose h j scheme reproduces iterates conjugate gradient method preconditioner h 0 terminates n iterations proof let k nonzero h method type described theorem 22 satisfying 215 conditions 212 214 hold claim k 1st subspace search directions spanfs equivalent 1st krylov subspace g 214 know spanfs g show via induction spanfh 0 g base case trivial assume lbfgs variations 11 214 induction hypothesis implies hence search directions span krylov subspace since search directions conjugate 213 span krylov subspace iterates produced conjugate gradients preconditioner h 0 since produce iterates conjugate gradient method conjugate gradient method wellknown terminate within n iterations conclude scheme terminates n iterations note require h j g j nonzero whenever g j nonzero requirement necessary since methods produce positive definite updates possible construct update maps g j zero happen would breakdown method next corollary defines role latest information k k plays formation kth hupdate corollary 24 suppose method type described theorem 22 satisfying 215 suppose kth iteration p k composed p projections form 22 least one projections must single projection p 1 v must form proof consider case p 1 k1g assume scalars oe ae 215 exist 2 12 gibson p oleary l nazareth 213 set fs conjugate thus linearly independent since working quadratic since symmetric positive definite set fy also linearly independent coefficient k lefthand side 218 must match righthand side thus hence k must make nontrivial contribution p k next show ae assume j 0 j j j j nonzero positive definite ae j nonzero coefficient u nonzero k must make nontrivial contribution p k j implying g contradiction hence ae show ae k 6 0 consider p k k suppose ae k1 contradicts p k k 2 spanfy must nonzero discuss p 1 case label ucomponents p projections scalars fl 1 fl p know lbfgs variations 13 thus since u conclude least one u must nontrivial contribution k 22 examples methods reproduce cg iterates specific examples methods fit general form satisfy condition 215 theorem 22 thus terminate n iterations example conjugate gradient method preconditioner h 0 see 24 satisfies condition 215 theorem 22 since example limitedmemory bfgs see 26 satisfies condition 215 theorem 22 since ae 0 example dfp full memory see 28 satisfies condition 215 theorem 22 consider p k full memory case fullmemory dfp h 1 using fact one easily verify p k therefore fullmemory dfp satisfies condition 215 theorem 22 reasoning apply limitedmemory case shall show x23 next corollary gives ideas methods related lbfgs terminate n iterations strictly convex quadratics corollary 25 lbfgs 25 method terminate n iterations ndimensional strictly convex quadratic function even combination following modifications made update 1 vary limitedmemory constant keeping k 1 2 form projections used v k recent along set pairs fs 3 form projections used v k recent along linear combinations pairs fs 0 4 iteratively rescale h 0 proof variant show method fits general form 21 satisfies condition 215 theorem 22 hence terminates corollary 23 1 let 0 value may change iteration iteration define 14 gibson p oleary l nazareth choose choices fit general form furthermore variation satisfies condition 215 theorem 22 2 special case next variant 3 iteration k let k denote ith choice linear combination span set let choose choices satisfy general form 21 furthermore ae k hence variation satisfies condition 215 theorem 22 4 let fl k 21 scaling constant choose vectors matrices lbfgs 26 combinations variants left reader remark part 3 previous corollary shows accumulated step method gill murray 10 terminates quadratics remark part 4 previous corollary shows scaling affect termination lbfgs fact method fits general form easy see scaling affect termination quadratics 23 examples methods reproduce cg iterates discuss several methods fit general form given 21 satisfy conditions theorem 22 lbfgs variations 15 example steepest descent see 23 satisfy condition 215 theorem 22 thus produce conjugate search directions fact well known see eg luenberger 16 example limitedmemory dfp see 28 satisfy condition p k 215 k method produce conjugate directions example suppose convex quadratic using limitedmemory constant exact arithmetic seen iteration terminate within first 20 iterations limitedmemory dfp maple notebook file used compute example available world wide web 9 remark using example easily see limitedmemory broyden class method except limitedmemory bfgs terminates within first n iterations 3 updateskipping variations broyden class quasinewton algo rithms previous section discussed limitedmemory methods behave like conjugate gradients ndimensional strictly convex quadratic functions sec tion concerned methods skip updates order reduce memory demands establish conditions finite termination preserved delayed broyden class 31 termination updates skipped shown powell 26 skip every update take direct prediction steps ie steps length one broyden class method procedure terminate 2n1 iterations ndimensional strictly convex quadratic function alternate proof result given nazareth 21 prove related result suppose exact line searches using broyden class quasinewton method strictly convex quadratic function decide skip p updates h ie choose h occasions algorithm terminates n iterations contrast powells result matter updates skipped multiple updates skipped row theorem 31 suppose broyden class method using exact line searches applied ndimensional strictly convex quadratic function p updates skipped let update iteration j skippedg furthermore method terminates n iterations exact minimizer proof use induction k show 31 gibson p oleary l nazareth 32 follows easily since j 2 jk 0 least value k jk nonempty ie jk 0 g g k01 orthogonal k0 since line searches exact h k01 since members broyden family satisfy secant condition hence base case true assume 31 33 hold values show also hold case suppose k 62 j k h k j j 2 j k j case ii suppose k 2 j k h k1 satisfies secant condition kg g k1 orthogonal k since line searches exact orthogonal older j argument 34 secant condition guarantees h k1 k j j either case induction result follows suppose skip p updates set jn cardinality n without loss generality assume set fs g i2jngamma1p zero elements 32 vectors linearly independent 31 gnp must zero implies xnp exact minimizer f lbfgs variations 17 32 loss termination update skipping limitedmemory unfortunately updates use limitedmemory repeated updateskipping produce n conjugate search directions ndimensional strictly convex qua dratics termination property lost show simple example limitedmemory skipping every update note according corollary 24 would still guaranteed termination used recent information update example suppose convex quadratic apply limitedmemory bfgs limitedmemory constant skip everyother update h using exact arithmetic maple observe process terminate even 100 iterations 9 4 experimental results results x2 x3 lead number ideas new methods unconstrained optimization section motivate de velop test ideas describe collection test problems x42 test environment described x43 section 441 outlines implementation lbfgs method base comparisons xx442447 describe varia tions pseudocode lbfgs variations given appendix b complete numerical results many graphs numerical results original fortran code available 9 41 motivation far given results convex quadratic func tions termination quadratics beautiful theory necessarily yield insight methods practice present new results relating convergence algorithms general functions however many shown converge using convergence analysis presented x7 15 15 liu nocedal show limitedmemory bfgs method implemented line search satisfies strong wolfe conditions see x43 definition rlinearly convergent convex function satisfies modest conditions 42 test problems test problems used constrained unconstrained testing environment cute bongartz conn gould toint package documented 3 obtained via world wide web 2 via ftp 1 package contains large collection test problems well interfaces necessary using problems test problems stored sif files chose collection 22 unconstrained problems problems ranged size 10000 variables took lbfgs limitedmemory constant least 60 iterations solve table 41 enumerates problems giving sif file name dimension n description problem cute package also provides starting point 43 test environment used fortran77 code sgi indigo 2 run algorithms fortran blas routines netlib used compilers default optimization level figure 21 outlines general quasinewton implementation followed line search use routines cvsrch cstep written jorge j gibson p oleary l nazareth sif name n description reference extended rosenbrock function nonseparable version 30 problem 10 problem 17 problem 20 3 tointgor 50 toints operations research problem 29 4 tointpsp 50 toints psp operations research problem 29 5 chnrosnb 50 chained rosenbrock function 29 6 errinros 50 nonlinear problem similar chnrosnb 28 7 fletchbv 100 fletchers boundary value problem 8 problem 1 8 fletchcr 100 fletchers chained rosenbrock function 8 problem 2 9 penalty2 100 second penalty problem 17 problem 24 problem 5 11 bdqrtic 1000 quartic banded hessian band diagonal variant broyden tridiagonal system band away diagonal 29 first penalty problem 17 problem 23 14 power 1000 power problem oren 25 msqrtals 1024 dense matrix square root problem nocedal liu case 0 seen nonlinear equation problem 4 problem 204 msqrtbls 1025 dense matrix square root problem nocedal liu case 1 seen nonlinear equation problem 4 problem 201 17 cragglvy 5000 extended cragg levy problem 30 problem test problem 5 problem 57 19 powellsg 10000 extended powell singular function 17 problem 13 another function nontrivial groups repetitious elements 12 tridiagonal matrix square root problem 4 problem 151 22 tridia 10000 shannos tridia quadratic tridiagonal problem 30 problem 8 table test problem collection problems chosen cute package david thuente 1983 version minpack line search routine finds ff meets strong wolfe conditions see eg nocedal 23 used 09 except first iteration always attempt step length 10 first use alternate value 10 satisfy wolfe conditions first iteration initially try step length equal kg remaining line search parameters detailed appendix generate matrix h k either limitedmemory update one variations described x44 storing matrix implicitly order save memory computation time terminate iterations following conditions met iteration lbfgs variations 19 k 1 inequality satisfied 2 line search fails 3 number iterations exceeds 3000 say iterates converged first condition satisfied otherwise method failed 44 lbfgs variations tried number variations standard lbfgs algorithm lbfgs variations described subsection summarized table 42 441 lbfgs algorithm 0 limitedmemory bfgs update given 25 described fully byrd nocedal schnabel 22 implementation following description come essentially 22 let h 0 symmetric positive definite assume k pairs satisfy let positive integer assume h 0 iteratively rescaled constant fl k commonly done practice matrix h k obtained k applications limitedmemory bfgs update expressed gammau u k k k theta k matrices given ae describe compute k case k 0 let x k current iterate let given matrices vectors 1 update n theta kgamma1 matrices kgamma1 kgamma1 get n theta k matrices using kgamma1 2 compute k vectors 3 compute k vectors using fact already know components k g kgamma1 kgamma1 g kgamma1 likewise need compute subtractions 20 gibson p oleary l nazareth reference brief description x441 lbfgs options allow vary iteratively basing choice kgk allowing decrease allow vary iteratively basing choice kgk allowing decrease allow vary iteratively basing choice kgxk allowing decrease allow vary iteratively basing choice kgxk allowing decrease 5 x443 dispose old information step length greater one 6 x444 variation 1 backup current iteration odd 7 x444 variation 2 backup current iteration even 8 x444 variation 3 backup step length 10 used last iteration 9 x444 variation 4 backup kg k k kg backup step length 10 used last iteration backup last iteration backup last iteration neither two vectors merged result merge 2nd 3rd recent steps taken length 10 13 x445 variation 2 merge merge last iteration least two old vectors merge 14 x446 variation 1 skip update odd iterations update even iterations alg 5 alg 8 dispose old information backup next iteration step length greater one alg 13 alg 1 merge merge last iteration least two old vectors merge allow vary iteratively basing choice kgk allowing decrease 19 alg 13 alg 3 merge merge last iteration least two old vectors merge allow vary iteratively basing choice kgxk allowing decrease alg 13 alg 2 merge merge last iteration least two old vectors merge allow vary iteratively basing choice kgk allowing decrease alg 13 alg 2 merge merge last iteration least two old vectors merge allow vary iteratively basing choice kgxk allowing decrease table description numerical algorithms 4 compute k rather recomputing u k update matrix previous iteration deleting leftmost column topmost row appending new column right new row bottom let ae 1s lower right submatrix u let upper lbfgs variations 21 note already computed 5 assemble already computed components 6 update k using kgamma1 7 compute note 8 compute two intermediate values 9 compute storage costs low order reconstruct h k need store diagonal matrix mvectors requires 2mn assuming n much less storage n 2 storage required typical implementation bfgs step operation count 9 table operations count computation h k g k steps operations shown computation hg takes omn operations assuming n see table 43 much less 2 normally needed compute hg whole matrix h stored using lbfgs basis comparison information performance lbfgs see liu nocedal 15 nash nocedal 19 442 varying iteratively algorithms 14 typical implementations lbfgs fixed throughout iterations updates accumulated updates always used considered possibility varying iteratively preserving finite termination convex quadratics using argument similar presented 15 also prove algorithm linear rate convergence convex function satisfies modest conditions tried four different variations theme based following linear formula scales relation size kgk let k number iterates saved kth iteration think maximum allowable value k let convergence test given kg k kkx k k ffl formula k iteration k ae log oe 22 gibson p oleary l nazareth alg table number failures algorithms 22 test problems algorithm said failed particular problem line search fails maximum allowable number iterations 3000 case exceeded two choices ffi k choice whether allow k decrease well increase four variations 1 2 3 4 used four values 51015 50 algorithm results summarized tables 44 48 extensive results obtained 9 table 44 shows algorithms roughly number failures lbfgs table 45 compares algorithm lbfgs terms function evaluations algorithm value number times algorithm used fewer function evaluations lbfgs listed relative total number admissible problems problems admissible least one two methods solved observe three cases algorithm used fewer function evaluations lbfgs half test problems table 46 compares algorithm lbfgs terms time entries similar table 45 observe algorithms 14 well terms time well better lbfgs nearly every case problem algorithm computed ratio number function evaluations algorithm number function evaluations l bfgs table 47 lists means ratios mean 10 implies algorithm better lbfgs average average better algorithms 6 16 cases first four algorithms observe however means close one lbfgs variations 23 alg 5 5 1922 2022 2022 2122 7 822 1222 1022 1022 8 1222 1422 1222 1522 9 622 1322 1222 1622 13 322 422 422 422 14 221 222 222 221 19 222 322 422 422 table function evaluations comparison first number entry number times algorithm well better normal lbfgs terms function evaluations second number total number problems solved least one two methods algorithm andor lbfgs experience savings terms time first four algorithms algorithms tend save fewer vectors lbfgs since k typically less less work done computing h k g k algorithms table 48 gives mean ratios time solve value algorithm note ratios far one case variations particularly well problem 7 see 9 information 443 disposing old information algorithm 5 may decide storing much old information stop using example may choose throw away everything except recent information whenever take big step since old information may relevant new neighborhood use following test last step length bigger 1 dispose old information algorithm performed nearly lbfgs substantial deviation one two problems value seemed evenly divided terms better worse table 44 see algorithm successfully converged every problem table 45 shows almost always well better lbfgs terms function evaluations however table 47 shows differences minor terms time observe algorithm generally faster lbfgs table 46 considering mean ratios time table 48 differences minor method also particularly well problem 7 9 444 backing update h algorithms 611 discussed x22 always use recent update preserve quadratic termination regardless older values use gibson p oleary l nazareth alg 5 1522 1322 1422 1522 7 1122 1122 1022 722 9 922 1022 722 822 13 522 1022 1322 1722 14 221 222 222 321 19 1122 1122 1722 1922 table time comparison first number entry number times algorithm well better normal lbfgs terms time second number total number problems solved least one two methods algorithm andor lbfgs using idea created algorithms certain conditions discard next recent values h although still use recent vectors vectors saved previous iterations call backing backup next recent values algorithms used following four tests trigger backing 1 current iteration odd 2 current iteration even 3 step length 10 used last iteration 4 kg k k kg two additional algorithms varied situations 3 4 allowing backup backup performed previous iteration backing strategy seemed robust terms failures 4 6 variations algorithm failures see table 44 information interesting observe backing odd iterations algorithm backing even iterations algorithm 7 caused different results backing odd iterations seemed almost effect number function evaluations table 47 little effect time table 48 however backing even iterations causes much different behavior lbfgs worse lbfgs problems better algorithms two variations idea backing previous step length one wipes data previous iteration used one update show improvement lbfgs terms function evaluations fact two algorithms best function evaluation ratio case table 47 unfortunately algorithms compete lbfgs terms time table 48 little difference lbfgs variations 25 alg 5 6 1000 1000 1000 1000 9 1035 1371 1005 0947 14 7521 7917 8288 8502 19 1212 1959 1242 1387 table mean function evaluations ratios algorithm compared lbfgs problems either method failed used mean algorithms probably rarely many steps length one row algorithms 9 11 also two variations idea backup iteration norm g k bigger norm g k1 larger difference results 9 11 8 10 terms function evaluation ratios table 47 algorithm 11 better indicating may wise backup twice row poorly terms time compared lbfgs table 48 445 merging information update algorithms 12 13 yet another idea merge data takes less storage computation time merging mean forming linear combination various vectors vectors would merged correspondingly corollary 25 shows long recent used without merge old vectors may replaced linear combination old vectors lbfgs used idea following way certain criteria met replaced second third newest vectors collection sum similarly vectors used various tests determine would merge 1 neither two vectors merged result merge second third recent steps taken length 10 2 merge last iteration least two old vectors merge first variation algorithm 12 performs almost identically lbfgs especially terms time table 45 occasionally worse terms time table 46 observations also reflected results table 47 table 48 likely vectors merged second variation algorithm 13 makes gains terms time especially 26 gibson p oleary l nazareth alg 6 1007 0983 0977 0995 9 1032 1220 1043 1173 14 4585 3703 3228 2417 table mean time ratios algorithm compared lbfgs problems either method failed used mean larger values table 46 table 48 unfortunately reflects saving amount linear algebra required number function evaluations generally larger algorithm lbfgs table 45 table 47 446 skipping updates h algorithms 1416 every update h skipped step length one always chosen bfgs terminate 2n iterations strictly convex quadratic function holds true exact line search see x3 unfortunately neither property holds limitedmemory case however try algorithms motivated idea idea every often use current update h instead use old h three variations theme 1 skip update odd iterations 2 skip update even iterations 3 skip update kg k1k kg k k algorithms backups results skipping odd even iterations quite different skipping odd updates algorithm 14 extremely well every value two problems 1 12 otherwise badly skipping even updates algorithm 15 performed somewhat better extremely well problem 7 problems 1 12 also better lbfgs terms time occasions algorithm 14 table 46 neither well terms function evaluations mean ratios function evaluations table 47 time table 48 usually far greater one skipping update norm g increased algorithm 16 well better terms function evaluations one problem value table 45 rarely better terms time table 46 ratios bad function evaluations table 47 time table 48 lbfgs variations 27 447 combined methods algorithms 1721 experimentation combinations methods described previous sections algorithm 17 combined algorithms 5 8 dispose old information backup next iterations step length greater one essentially assuming stepped region modeled quasinewton matrix take long step thus rid quasinewton matrix information algorithm well terms function evaluations mean ratios less one three values table 47 well terms time algorithms 1921 combined merging varying algorithms well terms time larger table 48 terms function evaluations table 47 5 conclusions spectrum quasinewton methods ranging require storage n theta n approximate hessian eg broyden fam ily require storage vectors eg conjugate gradients limitedmemory quasinewton methods fall extremes terms performance storage methods fall middle ground example conjugate gradient methods proposed shanno 27 nazareth 20 truncatednewton method 24 6 partitioned quasinewton method 13 characterized limitedmemory quasinewton methods fitting general form 21 property producing conjugate search directions convex quadratics shown limitedmemory bfgs broyden family member limitedmemory analog property also considered updateskipping something may seem attractive parallel environment show update skipping quadratic problems acceptable fullmemory broyden class members delays termination lose property finite termination limit memory skip updates also introduced simpletoimplement modifications standard limitedmemorybfgs algorithm seem behave well practical problems appendix line search parameters table a1 give line search parameters used code note first iteration initial steplength rather 10 variable value description stp 10 step length try first gamma4 value 1 wolfe conditions gtol 09 value 2 wolfe conditions relative width interval uncertainty maximum number function evaluations table a1 line search parameters appendix b pseudocode b1 lbfgs algorithm 0 pseudocode computation gammah k g k iteration k lbfgs given figure b2 update h also handled implicitly computation 28 gibson p oleary l nazareth sze else needed step 3 overwrite stg ytg fig b1 matlab pseudocode computation hg lbfgs sze number vectors available update iteration oldsze number vectors available previous iteration lbfgs sze chosen minimum oldsze limitedmemory constant b2 varying iteratively algorithms 14 suppose k denotes number pairs used kth update simply chose sze minimum oldsze computing k b3 disposing old information algorithm 5 disposal criterion met iteration k set oldsze zero sze one computing k b4 backing update h algorithms 611 backup iterations k set oldsze one less previous value sze set sze minimum oldsze usual b5 merging information update algorithms 12 13 merging complicated variation handle determine newest sze compute k execute pseudocode given figure b1 set oldsze one less previous value sze set sze minimum oldsze usual assuming iteration k variations 29 newest values yet added execute choosing new value sze computing fig b2 matlab pseudocode merge variation fixes values components used computation k b6 skipping updates h algorithms 1416 skip update iteration k set sze oldsze compute stg ytg step 0 skip step 8 continue r ftpthales test functions unconstrained minimization performance multifrontal scheme partially separable optimization numerical methods unconstrained optimization nonlinear equations optimal positive definite update sparse hessian matrices httpwww matrix computations private communication authors partitioned variable metric updates large structured optimization problems theoretical experimental study symmetric rank one update limited memory bfgs method large scale optimization linear nonlinear programming numerical study limited memory bfgs method truncatednewton method large scale optimization relationship bfgs conjugate gradient algorithms implications new algorithms updating quasinewton matrices limited storage discrete newton algorithm minimizing function many variables quadratic termination properties minimization algorithms conjugate gradient methods inexact line searches error specifying problem chnrosnb tr ctr sun linping updating selfscaling symmetric rank one algorithm limited memory largescale unconstrained optimization computational optimization applications v27 n1 p2329 january 2004 adi ditkowski gadi fibich nir gavish efficient solution using a1 journal scientific computing v32 n1 p2944 july 2007 albaali extraupdates criterion limited memory bfgs algorithm large scale nonlinear optimization journal complexity v18 n2 p557572 june 2002