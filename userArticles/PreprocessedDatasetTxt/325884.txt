using model trees classification model trees type decision tree linear regression functions leaves form basis recent successful technique predicting continuous numeric values applied classification problems employing standard method transforming classification problem problem function approximation surprisingly using simple transformation model tree inducerm5 based quinlans m5 generates accurate classifiers stateoftheart decision tree learner c50 particularly attributes numeric b introduction many applications machine learning practice involve predicting class takes continuous numeric value technique model tree induction proved successful addressing problems quinlan 1992 wang witten 1997 structurally model tree takes form decision tree linear regression functions instead terminal class values leaves numerically valued attributes play natural role regression functions discrete attributes also handledthough less natural way converse classical decisiontree situation classification discrete attributes play natural role prompted symmetry situation wondered whether model trees could used classification discovered turned classifiers surprisingly accurate order apply continuousprediction technique model trees discrete classification problems consider conditional class probability function seek modeltree approximation classification class whose model tree generates greatest approximated probability value chosen predicted class results presented paper show model tree inducer used generate classifiers significantly accurate decision trees produced c50 1 next section explains method use reviews features responsible good performance experimental results thirtythree standard datasets reported section 3 section 4 briefly reviews related work section 5 summarizes results 2 applying model trees classification model trees binary decision trees linear regression functions leaf nodes thus represent piecewise linear approximation unknown function model tree generated two stages first builds ordinary decision tree using splitting criterion maximization intrasubset variation target value second prunes tree back replacing subtrees linear regression functions wherever seems appropriate whenever model used prediction smoothing process invoked compensate sharp discontinuities inevitably occur adjacent linear models leaves pruned tree although original formulation model trees linear models internal nodes used smoothing process incorporated leaf models manner described section first describe salient aspects model tree algorithm describe procedure new paper model trees used classification justification procedure given next subsection following give example inferred class probabilities artificial situation true probabilities known 21 modeltree algorithm construction use model trees clearly described quinlans 1992 account m5 scheme implementation called m5 0 described wang witten 1997 along implementation details freely available version 2 m5 0 used paper differs described wang witten 1997 improved handling missing values describe appendix 3 changes tuning parameters necessary elaborate briefly two key aspects model trees surface discussion experimental results section 3 first central idea model trees linear regression step performed leaves pruned tree variables involved regression attributes participated decisions nodes subtree pruned away step omitted target taken average target value training examples reach leaf tree called regression tree instead second aspect smoothing procedure original formulation occurred whenever model used prediction idea first use leaf model compute predicted value filter value along path back root smoothing node combining value predicted linear model node quinlans 1992 calculation using model trees classification 3 prediction passed next higher node p prediction passed node q value predicted model node n number training instances reach node k constant quinlans default value used experiments implementation achieves exactly effect using slightly different representation final stage model formation create new linear model leaf combines linear models along path back root leaf models automatically create smoothed predictions without need adjustment predictions made example suppose model leaf involved two attributes x linear coefficients b model parent node involved two attributes z combine two models single one using formula z 3 continuing way root gives us single smoothed linear model install leaf use prediction thereafter smoothing substantially enhances performance model trees turns applies equally application classification 22 procedure figure 1 shows diagrammatic form model tree builder used classifi cation data taken wellknown iris dataset upper part depicts training process lower part testing process training starts deriving several new data sets original dataset one possible value class case three derived datasets setosa virginica versicolor varieties iris derived dataset contains number instances original class value set 1 0 depending whether instance appropriate class next step model tree inducer employed generate model tree new datasets specific instance output one model trees constitutes approximation probability instance belongs associated class since output values model trees approximations necessarily sum one testing process instance unknown class processed model trees result approximation probability belongs class class whose model tree gives highest value chosen predicted class derived datasets original dataset predicted class attributes target 44 30 47 32 67 31 58 27 01 attributes target 57 30 attributes class 44 30 47 32 67 31 58 27 versicolor virginica instance model trees attributes target attributes target 44 30 47 32 67 31 58 27 00 44 30 47 32 67 31 58 27 01 setosa b virginica c versicolor c versicolor 57 30 093 f 57 30 005 57 30 007 figure 1 m5 0 used classification using model trees classification 5 x x 00408y02061 c x 00408y02061 figure 2 example use model trees classification class probabilities data generation b training dataset c inferred class probabilities 23 justification learning procedure m5 0 effectively divides instance space regions using decision tree strives minimize expected mean squared error model trees output target values zero one training instances within particular region training instances lie particular region viewed samples underlying probability distribution assigns class values zero one instances within region standard procedure statistics estimate probability distribution minimizing mean square error samples taken devroye gyoerfi lugosi 1996 breiman friedman olshen stone 1984 24 example consider twoclass problem true class probabilities linear functions two attributes x pclassjx depicted figure 2a summing 1 point dataset 600 instances generated randomly according probabilities uniformly distributed x values chosen probability x value used determine whether instance assigned first second class data generated depicted 6 frank et al figure 2b classes represented filled hollow circles apparent density filled circles greatest lower left corner decreases towards upper right corner converse true hollow circles data figure 2b submitted m5 0 generates two model trees case structure trees generated trivialthey consist single node root figure 2c shows linear functions fclassjx represented trees discussion intimates excellent approximations original class probabilities data generated class boundary point intersection two planes figure 2c example illustrates classifiers based model trees able represent oblique class boundaries one reason model trees produced m5 0 outperform univariate decision trees produced c50 another m5 0 smooths regression functions adjacent leaves model tree 3 experimental results experiments designed explore application model trees classification comparing results decision tree induction linear regression determining components essential good performance specifically address following questions 1 classifiers based model trees compare stateoftheart decision trees classifiers based simple linear regression 2 important linear regression process leaves b smoothing process answer first question compare accuracy classifiers based smoothed model trees generated m5 0 pruned decision trees generated c50 see m5 0 often performs better however performance improvement might conceivably due aspects procedure m5 0 converts nominal attribute n attribute values binary attributes using procedure employed cart breiman et al 1984 generates one model tree class test ran c50 using exactly encodings transforming nominal attribute binary ones using procedure employed m5 0 generating one dataset class building decision tree dataset using class probabilities provided c50 arbitrate classes refer resulting algorithm c50 0 also report results linear regression lr using inputoutput encoding investigate second question first compare accuracy classifiers based model trees generated m5 0 ones based smoothed regression trees srt noted regression trees model trees constant functions leaf nodes thus cannot represent oblique class bound aries apply smoothing operation m5 0 routinely applies model trees compare accuracy classifiers based smoothed model trees m5 0 based unsmoothed model trees umt using model trees classification 7 table 1 datasets used experiments dataset instances missing numeric binary nominal classes values attributes attributes attributes balancescale 625 breastw glass g2 163 glass 214 heartstatlog 270 hepatitis ionosphere iris 150 letter 20000 pimaindians 768 segment 2310 sonar 208 vehicle 846 vote 435 56 0 waveformnoise 5000 anneal audiology 226 20 0 61 8 24 australian autos 205 11 15 4 6 6 breastcancer 286 03 horsecolic hypothyroid german 1000 labor 57 39 8 3 5 2 lymphography 148 primarytumor sick 3772 55 7 soybean 683 98 0 vowel smoothed regression tree special case smoothed model tree unsmoothed tree special case smoothed tree minor modifications code m5 0 needed generate srt umt models 31 experiments thirtythree standard datasets uci collection merz murphy 1996 used experiments summarized table 1 first sixteen involve numeric binary attributes last seventeen involve nonbinary nominal attributes well 4 since linear regression functions designed numericallyvalued domains binary attributes special case numeric attributes expect classifiers based smoothed model trees particularly appropriate first group table 2 summarizes accuracy methods investigated results give percentage correct classifications averaged ten tenfold nonstratified crossvalidation runs standard deviations ten also shown folds used scheme results c50 starred show significant improvement corresponding result m5 0 vice versa throughout speak results significantly different difference statistically significant 1 level according paired twosided ttest pair data points consisting estimates obtained one tenfold crossvalidation run two learning schemes compared table 3 shows different methods compare entry indicates number datasets method associated column significantly accurate method associated row 32 discussion results answer first question observe table 2 m5 0 outperforms c50 fifteen datasets whereas c50 outperforms m5 0 five numbers also appear boldface table 3 sixteen datasets numeric binary attributes m5 0 significantly accurate nine significantly less accurate none remaining datasets significantly accurate six significantly less accurate five results show classifiers based smoothed model trees generated m5 0 significantly accurate pruned decision trees generated c50 majority datasets particularly numeric attributes table 3 shows c50 0 significantly less accurate c50 twelve datasets first column last row significantly accurate five first row last column significantly less accurate m5 0 seventeen datasets significantly accurate three results show superior performance m5 0 due change inputoutput encoding complete discussion first question comparing simple linear regression lr m5 0 c50 table 3 shows lr performs significantly worse m5 0 seventeen datasets significantly worse c50 eighteen lr outperforms m5 0 eleven datasets c50 fourteen results linear regression surprisingly good however datasets application linear regression leads disastrous results one cannot recommend general technique answer second two questions begin comparing accuracy classifiers based m5 0 ones based smoothed regression trees srt assess importance linear regression process leaves former incorporates latter table 3 shows m5 0 produces significantly accurate classifiers twentythree datasets significantly less accurate ones two compared c50s pruned decision trees classifiers based smoothed regression trees significantly less accurate fifteen datasets using model trees classification 9 table 2 experimental results percentage correct classifications standard deviation balancescale 776sigma10 864sigma07 867sigma03 753sigma11 788sigma09 789sigma07 breastw 945sigma03 953sigma03 958sigma01 943sigma05 942sigma04 945sigma03 glass g2 787sigma21 818sigma22 704sigma04 755sigma17 793sigma23 788sigma22 glass 675sigma26 705sigma28 600sigma13 676sigma16 678sigma27 700sigma20 heartstatlog 787sigma14 822sigma10 837sigma04 799sigma18 784sigma15 786sigma14 hepatitis 793sigma12 819sigma22 856sigma15 796sigma15 788sigma30 797sigma11 ionosphere 889sigma16 897sigma12 866sigma05 882sigma07 873sigma10 889sigma16 iris 945sigma07 947sigma07 827sigma09 940sigma10 939sigma08 947sigma07 letter 880sigma02 903sigma01 555sigma01 863sigma02 867sigma01 875sigma01 pimaindians 745sigma12 762sigma08 772sigma05 757sigma10 720sigma07 745sigma12 segment 968sigma02 970sigma02 845sigma01 962sigma02 959sigma03 957sigma02 sonar 747sigma28 785sigma34 756sigma18 780sigma24 758sigma27 747sigma28 vehicle 729sigma12 765sigma13 757sigma05 709sigma12 693sigma12 720sigma10 vote 963sigma06 962sigma03 956sigma00 956sigma00 959sigma05 964sigma05 waveformnoise 754sigma05 820sigma02 859sigma02 803sigma03 723sigma04 752sigma05 zoo 918sigma11 921sigma13 942sigma18 893sigma15 905sigma13 891sigma14 anneal 987sigma03 988sigma02 931sigma02 973sigma01 985sigma02 990sigma02 audiology 765sigma14 767sigma10 686sigma16 679sigma12 768sigma18 739sigma09 australian 853sigma05 858sigma09 511sigma36 857sigma07 828sigma09 838sigma11 autos 800sigma25 744sigma19 590sigma15 700sigma22 717sigma18 756sigma17 breastcancer 733sigma16 696sigma23 700sigma15 729sigma10 675sigma24 688sigma17 heartc 768sigma14 809sigma14 850sigma04 797sigma16 763sigma13 788sigma16 hearth 798sigma09 790sigma08 819sigma10 792sigma11 769sigma15 775sigma13 horsecolic 853sigma06 846sigma07 827sigma07 845sigma09 834sigma15 845sigma06 hypothyroid 995sigma00 966sigma01 909sigma31 956sigma01 962sigma02 994sigma01 german 712sigma10 729sigma07 754sigma06 741sigma09 699sigma08 716sigma14 krvskp 995sigma01 994sigma01 940sigma01 985sigma01 993sigma01 994sigma01 labor 781sigma48 797sigma46 874sigma61 714sigma36 779sigma36 768sigma45 lymphography 754sigma28 798sigma14 836sigma13 761sigma16 775sigma29 759sigma22 primarytumor 418sigma13 451sigma16 472sigma09 451sigma13 414sigma12 403sigma21 sick 988sigma01 983sigma01 923sigma25 982sigma00 986sigma01 989sigma01 soybean 913sigma05 925sigma05 873sigma06 884sigma05 913sigma05 923sigma05 vowel 798sigma13 817sigma11 431sigma10 739sigma15 783sigma08 781sigma10 significantly accurate five results show linear regression functions leaf nodes essential classifiers based smoothed model trees outperform ordinary decision trees finally complete second question compare accuracy classifiers based m5 0 classifiers based unsmoothed model trees umt table 3 shows m5 0 produces significantly accurate classifiers twentyfive datasets significantly less accurate classifiers one comparison c50s pruned decision trees also leads conclusion smoothing process necessary ensure high accuracy modeltree based classifiers table 3 results paired ttests p001 number indicates often method column significantly outperforms method row lr 4 related work neural networks obvious alternative model trees classification tasks applying neural networks classification standard procedure approximate conditional class probability functions output node neural network approximates probability function one class contrast neural networks probability functions classes approximated single network model trees necessary build separate tree class model trees offer advantage neural networks user make guesses structure size obtain accurate results built fully automatically much efficiently neural networks moreover offer opportunities structural analysis approximated class probability functions whereas neural networks completely opaque idea treating multiclass problem several twoway classification prob lems one possible value class applied standard decision trees dietterich bakiri 1995 used c45 quinlan 1993 predecessor c50 generate twoway classification tree class however found accuracy obtained significantly inferior direct application c45 original multiclass problemalthough able obtain better results using errorcorrecting output code instead simple oneperclass code smyth gray fayyad 1995 retrofitted decision tree classifier kernel density estimators leaves order obtain better estimates class probability functions although improve accuracy class probability estimates three artificial datasets classification accuracies significantly better moreover resulting structure opaque includes kernel function every training instance torgo 1997 also investigated fitting trees kernel estimators leaves time regression trees rather classification trees could applied classification problems manner model trees advantage able represent nonlinear class boundaries rather linear oblique class boundaries model trees however suffer incomprehensibility models employ kernel estimators important difference smyth et al 1995 torgo 1997 m5 model tree algorithm m5 smooths models using model trees classification 11 adjacent leaves model tree substantially improves performance model trees classification problems saw also closely related method linear regression methods finding linear discriminants comparing experimental results obtained ordinary linear regression find although many datasets linear regression performs well several cases gives disastrous results linear models simply appropriate 5 conclusions work shown classification problems transformed problems function approximation standard way successfully solved constructing model trees produce approximation conditional class probability function individual class classifiers derived outperform stateoftheart decision tree learner problems numeric binary attributes often problems multivalued nominal attributes although resulting classifiers less comprehensible decision trees opaque produced statistical kernel density approximators expected time taken build model tree loglinear number instances cubic number attributes thus model trees class built efficiently dataset modest number attributes acknowledgments waikato machine learning group supported new zealand foundation research science technology provided stimulating environment research thank anonymous referees helpful constructive comments zwitter soklic donated lymphography primary tumor dataset appendix treatment missing values explain instances missing values treated version used results paper testing whenever decision tree calls test attribute whose value unknown instance propagated paths results combined linearly standard way quinlan 1993 problem deal missing values training tackle problem breiman et al 1984 describe surrogate split method whenever split value v attribute considered particular instance missing value different attribute used surrogate split instead appropriately chosen value v test v replaced v attribute value v selected maximize probability latter test effect former work described paper made two alterations pro cedure first simplification breimans original procedure follows let set training instances node whose values splitting attribute known let l subset split v assigns left branch r corresponding subset right branch define l r way surrogate split v number instances correctly assigned left subnode surrogate split corresponding number right subnode probability v predicts v correctly estimated chosen surrogate split v maximizes estimate whereas breiman chooses attribute value v maximize estimate simplification always choose surrogate attribute class continue select optimal value v described stratagem reported wang witten 1997 second difference blur sharp distinctions made breimans pro cedure according original procedure training instance whose value attribute missing assigned left right subnode according whether produces sharp stepfunction discontinuity inappropriate cases v poor predictor v modification employed version m5 0 used present paper soften decision making stochastic according probability curve illustrated figure a1 steepness transition determined likelihood test v assigning instance incorrect subnode assessed considering training instances value attribute known first estimate probability p r v assigns instance missing value rightmost subnode probability assigned left node probability instance incorrectly assigned left subnode v estimated p il likewise probability correctly assigned right subnode p cr l mean class value instances l r corresponding value r estimate p r model form x class value b chosen make curve pass points l p il cr shown figure a1 desired effect approximating sharp step function v good predictor p il 0 p cr 1 decision unimportant l r however prediction unreliablethat p il significantly greater 0 p cr significantly less 1the decision softened particularly importantthat l r differ appreciably training instance stochastically assigned right subnode testing surrogate splitting cannot used class using model trees classification 13 v0 class r cr figure a1 soft step function model fitted training data value course unavailable instead instance propagated left right subnodes resulting outcomes combined linearly using weighting scheme described quinlan 1993 left outcome weighted proportion training instances assigned left subnode right outcome proportion assigned right subnode notes 1 c50 successor c45 quinlan 1993 although commercial product test version available httpwwwrulequestcom 2 see httpwwwcswaikatoacnzml 3 realistic evaluation standard datasets imperative missing values accom modated removed instances missing values half datasets lower part table would instances usable 4 following holte 1993 g2 variant glass dataset classes 1 3 combined classes 4 7 deleted horsecolic dataset attributes 3 25 26 27 28 deleted attribute 24 used class also deleted identifier attributes datasets r classification regression trees probabilistic theory pattern recognition solving multiclass learning problems via errorcorrecting output codes simple classification rules perform well commonly used datasets uci repository machine learning databases httpwww learning continuous classes retrofitting decision tree classifiers using kernel density estimation ca morgan kaufmann kernel regression trees induction model trees predicting continuous classes tr c45 programs machine learning simple classification rules perform well commonly used datasets ctr niels landwehr mark hall eibe frank logistic model trees machine learning v59 n12 p161205 may 2005 donato malerba floriana esposito michelangelo ceci annalisa appice topdown induction model trees regression splitting nodes ieee transactions pattern analysis machine intelligence v26 n5 p612625 may 2004 rudy setiono feedforward neural network construction using cross validation neural computation v13 n12 p28652877 december 2001 v zorkadis karras panayotou efficient information theoretic strategies classifier combination feature extraction performance evaluation improving false positives false negatives spam email filtering neural networks v18 n56 p799807 june 2005 ronny kohavi j ross quinlan data mining tasks methods classification decisiontree discovery handbook data mining knowledge discovery oxford university press inc new york ny 2002 duncan potts incremental learning linear model trees proceedings twentyfirst international conference machine learning p84 july 0408 2004 banff alberta canada duncan potts claude sammut incremental learning linear model trees machine learning v61 n13 p548 november 2005 saso deroski bernard enko combining classifiers stacking better selecting best one machine learning v54 n3 p255273 march 2004 eibe frank leonard trigg geoffrey holmes ian h witten technical note naive bayes regression machine learning v41 n1 p525 oct 2000 joo gama functional trees machine learning v55 n3 p219250 june 2004 p solomatine b siek modular learning models forecasting natural phenomena neural networks v19 n2 p215224 march 2006 musavi h ressom srirangam p natarajan r w virnstein l j morris w tweedale neural networkbased light attenuation model monitoring seagrass population indian river lagoon journal intelligent information systems v29 n1 p6377 august 2007