accurate solution weighted least squares iterative methods consider weighted leastsquares wls problem illconditioned weight matrix wls problems arise many applications including linear programming electrical networks boundary value problems structures roundoff errors standard iterative methods solving wls problem illconditioned weights may give correct answer indeed difference true computed solution forward error may large propose iterative algorithm called minresl solving wls problems minresl method application minres krylovspace method due paige saunders siam j numer anal 12 1975 pp 617629 certain layered linear system using simplified model effects roundoff error prove minresl ultimately yields answers small forward error present computational experiments applications b introduction consider weighted leastsquares wls problem formula remainder article k delta k indicates 2norm make following assumptions diagonal positive definite matrix rank n assumptions imply 1 nonsingular linear system unique solution normal equations 1 form weighted leastsquares problems arise several application domains including linear programming electrical power networks elliptic boundary value problems structural analysis observed strang 21 article focuses case matrix severely illconditioned happens certain classes electrical power networks case nodearc adjacency matrix matrix load conductivities b vector voltage sources x vector voltages nodes illconditioning occurs resistors scale instance modeling leakage current insulators illconditioning also occurs linear programming interiorpoint method used compute newton step interiorpoint method need solve weighted leastsquares equation form 2 since slack variables become zero solution matrix always becomes illconditioned iterations approach boundary feasible region section 9 cover application detail illconditioning also occurs finite element methods certain classes boundary value problems example heat equilibrium equation r delta thermal conductivity field c varies widely scale important property problem 1 2 norm bound solution obtained independently stewart 20 todd 22 several authors see 6 complete bibliography state result paper stewart theorem 1 let denote set positive definite theta real diagonal matrices let theta n real matrix rank n exist constants 2 note matrix appearing 3 solution operator normal equations 2 words 2 rewritten since bounds 3 4 exist hope exist algorithms 2 possess property namely forward error bound depend call algorithms stable stability defined vavasis 23 means forward error computed solution x satisfies ffl machine precision fa function depending note underlying rationale kind bound conditioning problems 1 stem illconditioned rather illconditioned stability property possessed standard direct methods qr factorization cholesky factorization symmetric indefinite factoriza tion rangespace nullspace methods standard iterative methods conjugate gradient applied 2 two algorithms literature proved property nsh algorithm vavasis 23 complete orthogonal decomposition cod algorithm hough vavasis 12 direct see bjorck 1 information algorithms leastsquares problems would like stable iterative methods problem iterative methods much efficient direct methods large sparse problems common setting applications article presents iterative algorithm wls problems called minresl minresl consists applying minres algorithm paige saunders 14 certain layered linear system prove minres satisfies 5 proof forward error bound minresl based simplified model roundoff error affects krylov space methods analysis confirmed computational experiments section 8 simplified model described section 5 analysis roundoff minresl starting first principles presented effect roundoff minres iteration still fully understood minresl imposes additional assumption wls problem instance layered assumption made without loss generality ie every weighted leastsquares problem rewritten layered form minresl algorithm inefficient problems many layers article organized follows section 2 state layering assumption also layered leastsquares lls problem section 3 consider previous work section 4 describe minresl method twolayered wls problems section 5 analyze convergence twolayered case using simplifying assumptions roundoff error section 6 section 7 extend algorithm analysis case p layers section 8 present computational experiments support claims section 9 consider application minresl interiorpoint methods linear programming 2 layering assumption recall already assumed weight matrix appearing 1 diagonal positive definite illconditioned rest article impose additional layering assumption assume suitable permutation rows b corresponding symmetric permutation structure k wellconditioned scaled smallest diagonal entry 1 denote maximum diagonal entry among layering assumption much larger 1 note assumption made without loss generality could assume since could place diagonal entry layer unfortunately complexity algorithm grows quadratically p furthermore upper bound forward error degrades p increases see 39 thus tacit assumption number layers p large write partitioned form pc c correspond partitioning partition similarly assumption say 1 layered wls problem context electrical networks assumption means several distinct classes wires circuit resistance wires class l order 1ffi l instance one class wires might transmission lines whereas class might consist broken wires open lines resistance much higher context heat equilibrium equation layering assumption means object consideration composed small number different materials within material conductivity ffi l constant different materials different conductivities linear programming taking means slack variables current interiorpoint iterate small others large limiting case layered wls occurs gaps ffi l tend infinity ffi 1 infinitely larger ffi 2 weight gaps tend infinity solution 1 tends solution following problem refer layered least squares lls construct sequence nested affine subspaces l 0 oe l 1 oe delta delta delta oe l p r n spaces defined recursively l fminimizers kd 12 l finally x solution lls problem unique element l p layered leastsquares problem first introduced vavasis ye 25 technique accelerating convergence interiorpoint methods also established result mentioned paragraph solution wls problem limit ffi l1 ffi l 0 l converges solution lls problem combining result theorem 1 yields following corollary also proved vavasis ye corollary 1 let x solution lls problem posed matrix righthand side vector b kxk akbk kaxk akbk choice diagonal positive definite weight matrices 3 previous work standard iterative method leastsquares problems including wls problems conjugate gradient see golub van loan 7 saad 18 applied normal equations 2 algorithm commonly referred cgnr denote several variants cgnr literature see eg bjorck elfving strakos 2 note variants one form triple product da applying cg 2 instead one forms matrixvector products involving matrices trick result substantial savings running time since da could much denser alone trick applicable minresl method used computational experiments difficulty cgnr inaccurate solution returned da illconditioned illconditioned understand difficulty consider twolayered wls problem obtained subtituting 6 case observe sequence db constructed cgnr close words information 2 2 b 2 lost forming krylov sequence different framework interpreting difficulty described section 5 another iterative method leastsquares problems lsqr due paige saunders 15 method shares difficulty cgnr works krylov space standard technique handling illconditioning conjugate gradient reorthogonalization see example paige 16 parlett scott 17 reorthogonalization however cannot solve difficulty illconditioning 2 even act forming first krylov vector db causes loss information another technique addressing illconditioned linear systems iterative methods called regularization typical regularization technique modifies illconditioned system additional terms see hanke 10 regularization appear good approach solving 1 1 already welldefined solution particular theorem 1 implies solutions highly sensitive perturbation data vector b regularization technique would compute completely different solution previous work 3 proposed iterative method 2 based correcting standard cgnr search directions since dropped approach found case seemingly could handled detected algorithm 4 minresl two layers section next consider twolayered case 6 consider twolayered case separately playered case twolayered case contains main ideas general case easier write analyze algorithm reduces minres applied 2 hence novel furthermore case expected occur commonly practice mention also twolayered wls lls problems considered x22 lawson hanson 13 noted preceding section twolayered wls problem written form 7 diagonal entries 1 2 order 1 us introduce new variable v note equation always solution v righthand side range 1 multiplying 8 ffi 2 adding 7 yields putting 8 9 together get x algorithm call minresl minres layered application minres iteration due paige saunders 14 10 note 10 symmetric linear system general linear system rank deficient x v solution solution thus 10 rank deficient whenever rank 1 less n means must address existence uniqueness solution existence follows original wls problem 7 guaranteed solution uniqueness x established follows add times first row 10 ffi 1 times second row recover original wls problem 7 since 7 unique solution 10 must uniquely determine x since x uniquely determined 1 v question arises whether minres exact arithmetic find solution 10 minres find solution lies krylov space rank deficiency necessarily full dimensional question answered affirmatively theorem 24 brown walker 4 analysis concerns gmres result applies minres exact arithmetic furthermore result states assuming initial guess 0 computed solution x v minimum norm possible solutions since x uniquely determined result implies minimum norm recall section 3 problem applying conjugate gradient directly 7 linear system may illconditioned hence conjugate gradient may return inaccurate answer thus may seem paradoxical remedy problem caused illconditioning iterative method based truly rankdeficient system one explanation paradox concerns limiting behavior 1 case 7 tends linear system system general unique solution 1 assumed rank n cgnr compute solution may nothing thus cgnr solution expected forward accuracy demand hand see 10 tends x system easily seen lagrange multiplier conditions twolayered lls problem recall section 2 twolayered lls problem minimize kd 12 subject correct limiting behavior wls solution tends lls solution explanation minresls convergence behavior follows convergence analysis two layers section consider convergence minresl presence roundoff error case 2 mentioned introduction make simplifying assumption concerning effect roundoff error krylov space methods assumption concerns either cg minres applied symmetric linear system use algorithms preconditioner initial guess x use minres c lies rangespace ie system consistent use cg positive definite restrictions mind assumption effect roundoff sufficient number iterations either method compute iterate x satisfying c modest constant ffl machine epsilon x true solution multiple solutions exist take x minimumnorm solution far know bound rigorously proved related bound proved greenbaum 9 case conjugate gradient particular greenbaums result implies 11 would hold cg guaranteed recursively updated residual drops well machine precision always happens test cases minres less known bound like 11 known hold gmres implemented householder transformations 5 gmres equivalent minres augmented full reorthogonalization process content assert 11 minres evidence coming computational experiments bound sheds light minresl attain much better accuracy cgnr cgnr error bound 11 implies ka da xk gets small x computed solution latter quantity xk recall seeking bound forward error xk case factor greatly skew norm close zero bound kx gamma xk independent ffi 1 ffi 2 5 expected satisfied cgnr confirmed computational experiments contrast analysis minresl starting 11 yield accuracy bound 5 need following preliminary lemma theta n matrix rank n r theta n submatrix suppose linear system consistent c given vector given diagonal positive definite matrix solution x delta kck 12 furthermore exists solution x satisfying proof first note following preliminary result let hk two symmetric n theta n matrices h positive semidefinite k positive definite let b nvector range space h h converges solution proved reducing diagonal case using simultaneous diagonalization hk let extension theta diagonal matrix obtained filling zeros since limit solution x noted preceding paragraph let theta diagonal matrix 1s diagonal positions corresponding zeros elsewhere ck ck 15 ffl0 delta kck last line obtained transpose 4 proves 12 note holds x satisfying c since latter equation uniquely determines ax similarly demonstrate 13 start 15 ck ffl0 ck second part proof observe first part thus ffl0 axk combining 12 proves 14 resume analysis minresl define x v solution computed minresl 11 applied yields bounds formula h 2 shorthand coefficient matrix 10 extract another equation 16 17 particular multiply 16 multiply 17 ffi 1 add eliminate terms involving v let x exact solution wls problem last two terms equation replaced terms involving x using 7 interchanging left righthand sides yields goal derive accuracy bound like 5 18 19 start bounding quantity righthand side 18 note bounded largest entries 1 2 bounded bound kxk akbk using theorem 1 next turn bounding kvk 18 recall mentioned preceding section v uniquely determined minres find minimumnorm v satisfying 10 recall v determined constraint one way pick v make minimize ka 2 vk subject constraint case v layered leastsquares solution righthand side data b yields bound choice v factor improved using analysis gonzaga lara 8 combining x v contributions means bounded righthand side 18 let us rewrite 18 new bound next write new equations r observe r 1 lies range tand find h 1 satisfying similarly 17 exists h 2 satisfying applying 13 r 1 r 2 separately c lemma taken first r 1 r 2 conclude 21 22 substituting 21 22 19 yields notice analogy 7 preceding equation exactly weighted leastsquares computation unknown righthand side data thus theorem 1 build chain inequalities righthand side preceding inequality bounded 23 24 righthand side 23 24 bounded 20 combining yields obtain preceding inequality used facts assumption kdiagd gamma1 assumption since smallest entry taken 1 thus error bound form 5 desired particular dependence error bound ffi 2 ffi 1 note bound depends recall defined maximum entry assumed small indeed noted section 2 always assume willing divide problem many layers 6 minresl p layers section present minresl algorithm playered wls problem algorithm application minres symmetric linear system h p square matrix size 1 12n theta 1 pp gamma 12n c p vector order w vector unknowns matrix h p partitioned 1 blocks size n theta n vectors c p w similarly partitioned wls solution vector first subvector w detail vector w composed x concatenated ppgamma12 nvectors denote v ij lies lies recall playered wls problem may written let x solution equation see equation lies span therefore exists solution v equation equation first blockrow h words first block row h p contains one copy matrices first block c p general p 1th blockrow h equation completes description blockrows establish properties blockrows postpone description blockrows lemma suppose w solution linear equation 28 denotes concatenation x v ij x solution wls problem 26 proof multiply 28 ffi sum p equations obtained manner observe v ij terms cancel end exactly 26 also need converse true lemma 3 suppose x solution 26 exist vectors v ij 28 satisfied proof proof induction decreasing assume already determined v ij 28 satisfied must determine v kj particular value k base case induction select v satisfy 28 case lies range 26 induction case k p rewrite 28 case multiply recall goal choose v kj make equation valid multiply 28 add 29 rearranging summations cancelling common terms lefthand side end dividing ffi k separating v kj terms second summation yields 26 know lies range rightmost summation 31 also lies range therefore exist v kj choices make 29 valid algebraic steps used derive 31 29 reversed proves lemma note preceding proof actually demonstrates strengthened version lemma strengthened version states given x satisfying 26 k vectors v ij k satisfy 28 extend given data solution 28 strengthened version needed explain remaining pp gamma 12 blockrows h p rows exist solely purpose making h p symmetric first order variables equations correctly variables listed order x v first equations listed order 28 1 means first p rows h p format p p theta p gamma 1p gamma 22 matrix furthermore easily checked p symmetric first blockrow first blockcolumn consist listed order 1st entry main diagonal gammaffi p ffi blocks zeros define h p define c p pp gamma 12 blocks zeros example following linear system h 3 x must consider whether solutions particular must demonstrate new group equations first p rows w 0 denotes first p blocks w studying structure p see indexed correspondence columns p correspond variables v ij range row indexed j exactly two nonzero block entries yield equation task therefore show simultaneously satisfy 28 approach select v pj order v particular assuming v already selected define v pj solution following lemma shows linear system consistent lemma 4 v pj chosen reverse order satisfy 33 step linear system consistent 32 satisfied proof proof reverse induction j base case case 33 solution noted lies span case vacuously true specified range consider case range start version 33 satisfied v pi holds induction hypothesis move terms first summation righthand side second line obtained first applying 32 inductively j 32 taken k third line obtained merging two summations right notice preceding equation means v pi satisfies linear system v pj 33 except righthand side scaled proves 33 consistent j case since constructed solution although linear system necessarily unique solution linear system form uniquely determines ax thus also proved result actually strengthening 32 j equation need specific case reader may noticed preceding proof apparently complicated could establish result simply solving v ppgamma1 33 setting v 2 simpler approach yield bounds kv pj k needed next section proof shows method selecting v consistent satisfies 32 also see 27 satisfied follows immediately taking 33 complete proof solution h p need verify 28 case recall proof lemma 3 remaining v ij determined sequentially using construction proof thus arguments section established following theorem theorem 2 exists least one solution w h p solution first n entries vector x solves 26 7 convergence analysis p layers convergence analysis p layers follows basic outline convergence analysis two layers particular use 11 starting point error analysis observe 11 norm true solution righthand side thus apply bound must get norm bound v ij start bounds v pj apply lemma 1 33 case lemma take noted lies range 33 consistent righthand side 33 case form c note kd p b 1kbk thus 12 derive third line second used facts kd gamma1 use line reasoning get bound v ppgamma2 based 33 case 2 case righthand side 33 thus kck bounded ffi pgamma2 continue argument inductively time bound grows factor 2 take account fact v pi appears righthand side equation determining v pigamma1 end conclude next must bound v ij 1 vectors determined 28 find solution 28 first solving z already known consistent furthermore preceding equation set v using 12 conclude claim p proved induction decreasing using recurrence 36 righthand side 36 bounded 35 remaining terms bounded induction hypothesis omit details righthand side 11 need bound kv ij k note uniquely determined v ij recall case lemma 1 used bound ka k v ij k force unique determination choosing v ij proof lemma 1 yielding 14 note minres necessarily select v ij minimization property theorem 24 brown walker 4 described section 4 select v ij whose norm larger preceding bound apply 11 factor righthand side namely easily seen bounded p w solution computed minresl let substituting 37 righthand side 11 yields let r first p blockentries r note r j must lie span order equation h solution seen 28 p 1st blockrow us find h solves r 13 know ka let x first n entries computed wls solution multiply p gamma i1st block row h add p rows obtain third line obtained second interchanging order summation thus see third line solves wls problem ith entry data vector range conclude data vector bounded norm k theorem 1 implies substituting 38 yields delta 4 bound form 5 desired computational experiments section present computational experiments minresl cgnr compare accuracy efficiency first tests involve small nodearc adjacency matrix remaining tests matrices arising linear programming boundary value problems tests conducted matlab 42 running intel pentium microsoft windows nt 40 matlab software package programming language numerical computation written mathworks inc computations ieee double precision machine epsilon approximately 22 matlab sparse matrix operations used tests implementation cgnr based cgls1 32 bjorck elfving strakos 2 authors conclude cgls1 good way organize cgnr two matrixvector products per cgls1 eration one matrix 12 one 12 implemen tation cgnr iteration terminates scaled computed residual ks k kka dbk drops 10 gamma13 implementation minres based 14 except givens rotations used instead 2 theta 2 householder matrices inconsequential sign differences minresl iteration terminates scaled computed residual first matrix used following tests reduced nodearc adjacency matrix graph depicted figure 1 nodearc adjacency matrix contains one column node graph one row edge row contains exactly two nonzero entries 1 gamma1 columns corresponding endpoints edge choice endpoint assigned 1 assigned gamma1 induces orientation edge often orientation irrelevant application reduced nodearc incidence rnai matrix obtained nodearc incidence matrix deleting one column rnai matrices arise analysis electrical network batteries resistors see 23 also arise network flow problems case figure 1 column corresponding figure 1 based graph used first group tests column corresponding top node deleted edges marked heavy lines weighted 1 edges marked light lines weighted varies test test top node deleted thus 9 matrix well known rnai matrix connected graph always full rank rnai matrices known small values 23 tests weight matrix two layers took vary experiment experiment rows correspondence 2 drawn thinner lines figure 1 finally righthand side b chosen first prime numbers results displayed table 1 cases plotted figure 2 scaled error tabulated plotted cases defined k x gamma xkkbk choose particular scaling error goal investigate stability bound 5 true solution x computed using cod method 12 note accuracy cgnr decays ffi 2 gets smaller whereas minresls accuracy stays constant minresl requires many flops cgnr system matrix larger running time cgnr first four rows table illconditioning increases last two rows running time cgnr drops matrix da masquerades lowrank matrix small values ffi 2 causing early termination lanczos process besides returning inaccurate solution cgnr additional difficulty residual quantity normally measured practical use algorithm reflect forward error simple way table 1 behavior twolayered minresl algorithm compared cgnr decreasing values ffi 2 error reported scaled error defined text note cg accuracy degrades minres accuracy stays minresl minresl minresl cgnr cgnr cgnr iterations error flops iterations error determine whether cgnr computing good answers contrast error residual minresl closely correlated correlation predicted theory next computational test involved larger matrix taken netlib linear programming test set namely matrix problem afiro 51 theta 27 used matrix 1s first 27 diagonal positions remaining 24 positions ie righthand side vector b chosen contain first primes minresl required 137 iterations 250 kflops yielded solution x scaled error 30 respect true solution computed cod method matrix known cgnr problem required 69 iterations 61 kflops returned answer scaled error 22 convergence plots depicted figure 3 excessive number iterations required minres apparently caused loss orthogonality lanczos process verify hypothesis ran gmres layered matrix gmres 19 symmetric matrix equivalent minres full reorthogonalization exact arithmetic two algorithms identical call algorithm gmresl termination tests used result depicted figure 4 case gmresl ran 50 iterations fewer 1 returned accurate answer one forward error however number flops higher 350 k scaled error scaled residual scaled error scaled residual figure 2 convergence behavior cgnr minresl rnai test case plots plots follow xaxis iteration number algorithms computed ie recursively updated residual plotted rather true residual experiments reported indicate usually indistinguishable theta yaxis indicates cutoff cgnr scaled residual must drop order 11 true ffi yaxis analog minresl figure 3 convergence behavior cgnr minresl afiro curves labeled figure 2 gramschmidt process gmres main loop next computational test involves larger matrix arising finiteelement analysis application solution boundary value problem r delta polygonal domain depicted figure 5 dirichlet boundary conditions conductivity field c 1 outer part domain 10 12 darker triangles discussed 24 type problem gives rise weighted leastsquares problem encodes information geometry encodes illconditioned conductivity field values matrix known although bounds known variants parameters particular matrix 652 theta 136 righthand side vector b chosen according dirichlet boundary conditions described 24 minresl method problem gave scaled error 13 iterations 65 mflops compute true solution used nshi method 24 case surprisingly cgnr gave almost accurate answer termination test never activated cut cgnr 10n iterations residual cgnr quite oscillatory depicted figure 6 finiteelement literature cgnr would referred conjugate gradient assembled stiffness matrix da cause odd behavior cgnr follows note region high conductivity incident boundary domain figure 4 convergence behavior gmresl delta delta figure 5 domain finite element mesh used finite element exper iment conductivity dark triangles 10 12 light triangles figure convergence cgnr minresl finite element test problem curves labeled figure 2 thus starts righthand side already almost zero furthermore righthand side nearly orthogonal span dominates stiffness matrix da thus cgnr trouble making progress surprisingly accurate answer cgnr example useful practice apparent way detect convergence underway final test threelayered problem based matrix adlittle netlib test set 138 theta 56 matrix matrix first 28 diagonal entries 1 next 28 diagonal entries 10 gamma8 last 82 entries righthand side vector first 138 prime numbers convergence depicted figure 7 expected scaled error minresl decreased scaled error cgnr 03 note excessive number iterations required minresl apparently due loss orthogonality number iterations 118 gmresl achieve scaled error 94 fact test gmresl efficient minresl terms flop count cases see minresl algorithm performs essentially expected except two cases loss orthogonality causes many iterations expected every case minresls running time higher cgnrs cgnr produce bad solutions measured forward error figure 7 convergence cgnr minresl adlittle curves labeled figure 2 note excessive number iterations minresl caused loss orthogonality 9 issue interiorpoint methods section describe issue arises using minresl algorithm interiorpoint method linear programming full consideration matter postponed future work well known system equations newton step interiorpoint method expressed weighted leastsquares problem precise consider linear programming problem subject whose dual subject ay standard form except transposed consistent leastsquares notation primaldual method starting feasible interior point problem computes update deltay satisfying algorithmdependent parameter usually 0 1 duality gap e vector 1s see wright 26 since 40 form wls problem obtain deltay using minresl algorithm one way compute deltas via deltas gammaadeltay method stable deltas small entries positions small en small entries must computed accurately respect corresponding entry contrast error components deltas arising product adeltay order ffl delta ksk ffl machine epsilon direct method accurately computing components deltas proposed hough 11 obtains bound form deltas consider methods extending minresl accurate computation deltas future work noted hough deltax easily computed deltas similar accuracy bound assuming deltas satisfies 41 conclusions presented iterative algorithm minresl solving weighted least squares theory computational experiments indicate method accurate cgnr weight matrix highly ill conditioned work raises number questions 1 iterative method require layering assumption 2 layering indeed required get parsimonious layered linear system p 3 particular 3n theta 3n system equations desired properties 3layered case instead 4n theta 4n system presented 3 best way handle loss orthogonality minres observed section 8 4 work extended stable computation deltax deltas interiorpoint method question raised section 9 5 preconditioning computational tests ran minres cg n iterations aim compute solution vector accurately possible prac tice one hopes convergence much fewer n iterations techniques preconditioning wls problems note analysis minresls accuracy section 5 section 7 presupposes preconditioner used acknowledgments helpful discussions work anne greenbaum mike overton nyu roland freund david gay margaret wright bell labs patty hough sandia rich lehoucq steve wright argonne homer walker utah state zdenek strakos czech academy sciences thank patty hough gail pieper carefully reading earlier draft paper addition received netlib linear programming test cases matlab format patty hough r numerical methods least squares problems stability conjugate gradient lanczos methods linear least squares problems iterative methods weighted least squares gmres nearly singular systems numerical stability gmres linear leastsquares problems diagonally dominant weight matrices matrix computations note properties condition numbers estimating attainable accuracy recursively computed residual methods conjugate gradient type methods illposed problems stable computation search directions neardegenerate linear programming problems complete orthogonal decomposition weighted least squares solving least squares problems solution sparse indefinite systems linear equations lsqr algorithm sparse linear equations sparse least squares practical use symmetric lanczos process orthogonalization lanczos algorithm selective reorthog onalization iterative methods sparse linear systems gmres generalized minimum residual algorithm solving nonsymmetric linear systems scaled projections pseudoinverses framework equilibrium equations dantzigwolfelike variant karmarkars interiorpoint linear programming algorithm stable numerical algorithms equilibrium systems stable finite elements problems wild coefficients primaldual interior point method whose running time depends constraint matrix tr