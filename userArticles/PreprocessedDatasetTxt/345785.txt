applicationcontrolled paging shared cache propose provably efficient applicationcontrolled global strategy organizing cache size k shared among p application processes application access information future page requests using local information along randomization context global caching algorithm able break conventional hk sim ln k lower bound competitive ratio caching problem p application processes always make good cache replacement decisions online applicationcontrolled caching algorithm attains competitive ratio 2hp12 sim 2 ln p typically p much smaller k perhaps several orders magnitude competitive ratio improves upon 2p2 competitive ratio achieved deterministic applicationcontrolled strategy cao felten li show online applicationcontrolled algorithm competitive ratio better minhp1hk even application process perfect knowledge individual page request sequence results respect worstcase interleaving individual page request sequences p application processeswe introduce notion fairness realistic situation application processes always make good cache replacement decisions show algorithm ensures application process needs evict one cached pages service page fault caused mistake application algorithm fair remains efficient global paging performance bounded terms number mistakes application processes make b introduction caching useful technique obtaining high performance days latency disk access relatively high todays computers typically several application processes running concurrently means time sharing multiple processors processes special knowledge future access patterns cao et al cfl94a cfl94b exploit special knowledge develop effective file caching strategies application providing specific information future needs equivalent application caching strategy managing pages cache consider multiapplication caching problem formally defined section 3 p concurrently executing application processes share common cache size k section 4 propose online applicationcontrolled caching scheme decisions need taken two levels page needs evicted cache global strategy chooses victim process process decides pages evicted cache application process may use available information future page requests deciding pages evict however assume global information interleaving individual page request sequences bounds respect worstcase interleaving individual request sequences competitive ratios smaller h k lower bound classical caching fkl possible multiapplication caching application may employ future information individual page request sequence 1 applicationcontrolled algorithm proposed cao felten li cfl94a achieves competitive ratio prove appendix show sections 57 new online applicationcontrolled caching algorithm improves competitive ratio optimal factor 2 realistic scenario k use algorithm fkl 91 case p k resulting bound optimal factor 2 p results significant since p often much smaller k perhaps several orders magnitude scenario application processes occasionally make bad page replacement decisions mistakes show section 8 online algorithm incurs page faults globally function number mistakes algorithm also fair sense mistakes made one processor page replacement decisions worsen page fault rate processors classical caching competitive analysis wellknown classical caching paging problem deals twolevel memory hierarchy consisting fast cache size k slow memory arbitrary size hn represents nth harmonic number sequence requests pages satisfied order occurrence order satisfy page request page must fast memory requested page fast memory page fault occurs page must evicted fast memory slow memory order make room new page put fast memory caching paging problem decide page must evicted cache cost minimized number page faults incurred course servicing page requests belady bel66 gives simple optimum offline algorithm caching problem page chosen eviction one cache whose next request furthest future order quantify performance online algorithm sleator tarjan st85 introduce notion competitiveness context caching defined follows caching algorithm let fa oe number page faults generated processing page request sequence oe randomized algorithm let fa oe expected number page faults generated processing oe expectation respect random choices made algorithm online algorithm called ccompetitive every page request sequence oe fa oe c delta fopt oe fixed constant constant c called competitive ratio measure online algorithms performance needs relatively good worstcase page request sequences order algorithm considered good sleator tarjan st85 show lower bound k competitive ratio deterministic caching algorithm fiat et al fkl prove lower bound h k randomized algorithms allowed also give simple elegant randomized algorithm problem achieves competitive ratio 2h k sleator mcgeoch ms91 give rather involved randomized algorithm attains theoretically optimal competitive ratio h k 3 multiapplication caching problem paper take theoretical issue best use application pro cesses knowledge individual future page requests optimize caching performance analysis purposes use online framework similar mentioned caching algorithms fkl use absolutely information future page requests intuitively knowledge future page requests exploited decide page evict cache time page fault practice application often advance knowledge individual future page requests cao felten li cfl94a cfl94b introduced strategies try combine advance knowledge processors order make intelligent page replacement decisions multiapplication caching problem consider cache capable storing k pages shared p different application processes denote page cache memory belongs online algorithm multiapplication caching 3 exactly one process individual request sequences processes may interleaved arbitrary worstcase manner worstcase measure often criticized used evaluating caching algorithms individual application request sequences birs91 kpr92 feel worstcase measure appropriate considering global paging strategy cache shared concurrent application processes knowledge individual page request sequences locality reference within applications individual request sequence accounted model application processs knowledge future requests worstcase nature model assumes nothing order durations time application processes active model worstcase measure competitive performance amounts considering worstcase interleaving individual sequences approach cao et al cfl94a kernel deterministically choose process owning least recently used page time page fault ask process evict page choice may different least recently used page appendix show assumption processes always make good page replacement decisions cao et als algorithm competitive 2 algorithm present next section analyze thereafter improves competitive ratio 2h p online algorithm multiapplication caching algorithm online applicationcontrolled caching strategy operating system kernel manage shared cache efficient fair manner show subsequent sections competitive ratio algorithm 2h optimal within factor 2 among online algorithms use algorithm fkl page fault first choose victim process ask evict suitable page algorithm detect mistakes made application processes enables us reprimand application processes pay mistakes scheme mark pages well processes systematic way processing requests constitute phase 1 global sequence page requests partitioned consecutive sequence phases phase sequence page requests beginning phase pages processes unmarked page gets marked phase requested process marked pages cache marked new phase begins page requested cache pages cache marked page accessed phase called clean respect phase online algorithms cache beginning phase request clean page called clean page request phase always begins clean page request online algorithm multiapplication caching 4 marking scheme similar one fkl 91 classical caching problem however unlike algorithm fkl 91 algorithm develop nonmarking algorithm sense algorithm may evict marked pages addition notion phase definintion 1 different notion phase fkl 91 looked upon special case general notion put differences perspective section 41 algorithm works follows page p belonging process p r requested 1 p cache p marked mark b process p r unmarked pages cache mark p r 2 p cache process p r unmarked page p clean page respect ongoing phase ie p r made mistake earlier phase evicting p ask process p r make page replacement decision evict one pages cache order bring page p cache mark page p also mark process p r unmarked pages cache b else process p r marked page p clean page pages cache marked remove marks pages processes start new phase beginning current request p ii let denote set unmarked processes pages cache randomly choose process p e process chosen uniform probability 1jsj iii ask process p e make page replacement decision evict one pages cache order bring page p cache mark page p also mark process p e unmarked page cache note steps 2ai 2biii algorithm seeks paging decisions application processes unmarked consider unmarked process p asked evict page phase consider p pages cache time let u denote farthest unmarked page process p u unmarked page process p whose next request occurs furthest future among p unmarked cached pages note process p may marked pages cache whose next requests occur request u 2 good set unmarked process p current point phase set consisting farthest unmarked page u cache every marked page p cache whose next request occurs next request page u page replacement decision made unmarked process p either step 2ai step 2biii evicts page good set regarded good decision respect ongoing phase page good set p good page eviction purposes time decision decision made unmarked process p good decision regarded mistake process p process p makes mistake evicting certain page cache detect mistake made p page requested p phase p still unmarked sections 6 7 specifically assume application processes always able make good decisions page replacement section 8 consider fairness properties algorithm realistic scenario processes make mistakes 41 relation previous work classical caching marking scheme approach inspired similar approach classical caching problem fkl 91 however phases defined algorithm significantly different nature fkl 91 phase ends k distinct marked pages cache k distinct pages may requested phase phases depend random choices made algorithm probabilistic nature hand phase defined fkl exactly k distinct pages accessed given input request sequence phases determined independently caching algorithm used definition fkl 91 suited facilitate analysis online caching algorithms never evict marked pages called marking algorithms case marking algorithms since marked pages never evicted soon k distinct pages requested k distinct marked pages cache means phases determined definition special case marking algorithms exactly phases determined definition fkl 91 note algorithm general marking algorithm since may evict marked pages marking algorithms always evict unmarked pages algorithm always calls unmarked processes evict pages actual pages evicted may marked 5 lower bounds opt competitive ratio section prove competitive ratio online caching algorithm better minfh g let us denote opt optimal offline algorithm caching works follows page fault occurs opt evicts page whose next request furthest future request sequence among pages cache fkl 91 compare number page faults generated online algorithm phase number page faults generated opt phase express number page fronts function number clean page requests phase state prove lower bound amortized number page faults generated opt single phase proof simple generalization analogous proof fkl 91 deals deterministic phases marking algorithms lemma 1 consider phase oe online algorithm clean pages requested opt incurs amortized cost least 2 requests made phase 2 number clean pages opt cache beginning phase number pages requested oe opt cache algorithms cache beginning oe let i1 represent quantity next phase oe i1 let dm i1 clean pages opt cache beginning oe i1 marked oe u marked oe note clean pages requested oe opt cache opt generates least oe hand processing requests oe opt cannot use u cache locations since beginning oe i1 u pages opt cache marked oe u pages would opt cache oe even began k marked pages algorithms cache end oe dm pages marked oe algorithms cache number distinct pages requested oe least dm k hence opt serves least dm corresponding oe without using u cache locations means opt generates least k oe therefore number faults opt generates oe least let us consider first j phases jth phase sequence opt least faults first phase opt generates k faults k thus sum opt faults phases least use fact 2 thus definition amortized number faults opt generates phase oe least 2 amortized lemma 1 mean j 1 number page faults made opt serving first j phases least number clean page requests ith phase next construct lower bound competitive ratio randomized online algorithm even application processes perfect knowledge individual request sequences proof straightforward adaptation proof h k lower bound classical caching fkl 91 however situation hand adversary restrictions request sequence use prove lower bound thereby resulting lowering lower bound theorem 1 competitive ratio randomized algorithm multiapplication caching problem least minfh p even application processes perfect knowledge individual request sequences lower bound classical caching problem fkl directly applicable considering case process accesses one page gives lower bound h k competitive ratio case p k construct multiapplication caching problem based nemesis sequence used fkl 91 classical caching fkl 91 lower bound h k 0 proved special case cache size k 0 total k pages denote c 1 c 2 c k 0 1 one pages fit cache time corresponding multiapplication caching problem consists one process corresponding page classical caching lower bound instance k 0 sized cache process total number pages among processes k cache size one pages among processes fit memory simultaneously instance multiapplication caching problem construct request sequence process p consists repetitions double roundrobin sequence 1 length 2r refer double roundrobin sequence 1 touch process p adversary generates requests corresponding touch process p say touches process p given arbitrary adversarial sequence classical caching problem described construct adversarial sequence multiapplication caching problem replacing request page c former problem touch process latter problem transform algorithm instance multiapplication caching one classical caching problem following correspondence multiapplication algorithm evicts page process p j servicing touch process p classical caching algorithm evicts page c j order service request page c lemma 2 show optimum online algorithm instance multiapplication caching never evicts page belonging process p servicing fault request page process p thus transformation valid page c always resident cache page request c serviced reduction immediately implies competitive ratio instance multiapplication caching must least h k 0 lemma 2 instance multiapplication caching online algorithm converted online algorithm 0 least good amortized sense property pages process p cache immediately touch p processed intuitively double roundrobin sequences force optimal online algorithm service touch process evicting page belonging another pro cess construct online algorithm 0 online manner suppose 0 fault touch process p algorithm evicts page p j j 6 0 algorithm evicts page p first roundrobin servicing touch p page fault second roundrobin evicts page another process second roundrobin 0 evicts page first roundrobin incurs fault second roundrobin first page fault wasted page could evicted instead first roundrobin instead evicts another page p second roundrobin 0 evicts arbitrary page another process first roundrobin 0 incurs page fault second roundrobin thus evicts page p incurs least one page fault 0 faults touch p 0 doesnt paging decision 0 make fault touch p 0 fault 0 evicts page cache page fault 0 charged extra page fault incurred earlier 0 evicted one p pages thus number page faults 0 incurs number page faults incurs construction pages process p algorithm 0 cache immediately touch process p double roundrobin sequences reduction replaced single roundrobin sequences redoing explicit lower bound argument fkl 6 holes section introduce notion holes plays key role analysis online caching algorithm section 62 mention crucial properties holes algorithm assumption applications always make good page replacement decisions properties also useful bounding page faults occur phase applications make mistakes page replacement decisions definition 3 eviction cached page time page fault clean page request said create hole evicted page intuitively hole lack space page pages place cache contains hole page page p 1 evicted servicing clean page request page p 1 said associated hole page p 1 subsequently requested another page p 2 evicted service request hole said move p 2 p 2 said associated hole end phase say hole h moves process p mean hole h moves page p belonging process p 61 general observations holes requests clean pages phase page faults create holes number holes created particular phase equals number clean pages requested phase apart clean page requests requests holes also cause page faults occur request hole mean request page associated hole proceed request sequence phase page associated particular hole varies time consider hole h created page p 1 evicted serve request clean page p c request made page p 1 page p 2 evicted h moves p 2 similarly page p 2 requested h moves p 3 let temporal sequence pages associated hole h particular phase page p 1 evicted clean page p c requested page evicted p igamma1 requested request p falls next phase number faults incurred particular phase considered due requests h gamma 1 62 useful properties holes section make following observations holes assumption application processes make good decisions lemma 3 let u farthest unmarked page cache process p point phase process p marked process time request page u served follows definition farthest unmarked page nature marking scheme employed algorithm lemma 4 suppose request page p associated hole h suppose process p owns page p process p already marked time present request page p associated hole h process p evicted page p asked make page replacement decision order serve either clean request page fault previous page associated h either case page p good page time process p made particular paging decision since process p unmarked time decision made p either farthest unmarked page process p marked page process p whose next request request p farthest unmarked page lemma 3 process p marked process time request page p lemma 5 suppose page p associated hole h let p denote process owning page p suppose page p requested time phase hole h move process p subsequently current phase proof hole h belongs process p lemma 4 request made h marked remain marked end phase since unmarked processes chosen evict pages request h thereafter cannot result eviction page belonging p hole never move process let r unmarked processes time request hole h unmarked process denote farthest unmarked page process p j time request hole h without loss generality let us relabel processes temporal order first subsequent appearance pages u j global page request sequence lemma 6 situation described 2 suppose page request hole h hole moves good page p unmarked process p serve current request h h never move processes current phase proof first subsequent request good page p p evicts definition must must first subsequent request farthest unmarked page u process p marked next time hole h requested lemma 4 hand first subsequent requests respective farthest unmarked pages u 1 u appear page u thus lemma 3 already marked next time hole h page gets requested remain marked remainder phase hence fact unmarked processes get chosen hole h never move 7 competitive analysis online algorithm main result theorem 2 states online algorithm multiapplication caching problem roughly 2 ln pcompetitive assuming application processes always make good decisions eg process knows future page requests lower bound theorem 1 follows algorithm optimal terms competitive ratio factor 2 competitive analysis online algorithm 11 theorem 2 competitive ratio online algorithm section 4 multiapplication caching problem assuming good evictions always made 2 competitive ratio within factor 2 best possible competitive ratio problem rest section devoted proving theorem 2 count number faults generated algorithm phase make use properties holes previous section requests made clean pages phase holes move phase count number faults generated algorithm phase n number times hole h requested phase assuming good decisions always made prove phase hole h expected value n bounded h p gamma1 consider first request hole h phase let r h number unmarked processes point time let cr h random variable associated number page faults due requests hole h phase lemma 7 expected number ecr h page faults due requests hole h hr h prove induction r h ec suppose using terminology notation lemma 6 let farthest unmarked pages r h unmarked processes time request h appear temporal order global request sequence renumber r h unmarked processes convenience page u farthest unmarked page unmarked process p hole h requested algorithm randomly chooses one r h unmarked processes say process p asks process p evict suitable page assumption hole h moves good page p process p lemmas 5 6 algorithm chooses unmarked process p good page p evicted r h gamma processes remain unmarked next time h requested since r h unmarked processes chosen probability 1r h ecr h r h ecr h gammai r h applicationcontrolled caching fairness 12 last equality follows easily induction algebraic manipulations let us complete proof theorem 2 lemma 4 maximum possible number unmarked processes time hole h first requested implies average number times hole requested phase bounded h p gamma1 3 total number page faults phase 1 already shown lemma 1 opt algorithm incurs amortized cost least 2 requests made phase therefore competitive ratio algorithm bounded 1 h p 2 applying lower bound theorem 1 completes proof applicationcontrolled caching fairness section analyze algorithms performance realistic scenario application processes make mistakes defined definition 2 bound number page faults incurs phase terms page faults caused mistakes made application processes phase main idea application process p commits mistake evicting certain page p phase requests page p process p still unmarked algorithm makes process pay mistake step 2ai hand page ps eviction process p mistake process marked page p later requested phase say time process p mistake worth detecting following reason since evicting page p mistake must mean time 1 ps eviction existed set u one unmarked pages process p cache whose subsequent requests appear next request page p process p marked time next request p implying pages u evicted p times 2 3 ju j1 mistake evicting p instead time 1 2 ju j1 process p makes specific good paging decisions evicting farthest unmarked pages set fpg u pages cache time notion fairness choose ignore mistakes consider worth detecting definition 4 ongoing phase page fault corresponding request page p unmarked process p called unfair fault request page p clean page request faults phase unfair called faults unfair faults precisely page faults caused mistakes considered worth detecting state following two lemmas follow trivially definitions mistakes good decisions unfair faults fair faults applicationcontrolled caching fairness 13 lemma 8 phase page requests get processed step 2ai algorithm precisely unfair faults phase unfair faults correspond mistakes get caught step 2ai algorithm lemma 9 fair faults precisely requests get processed step 2biii consider behavior holes current mistakeprone scenario number holes phase equals number clean pages requested phase lemma 11 consider hole h associated page p process p request h unfair fault process p still unmarked hole h moves page belonging process p request hole h fair fault process p already marked hole h never move process p subsequently phase request hole h unfair fault definition process p unmarked lemma 8 h moves page p 0 process p request h fair fault definition fact request h clean page request process p marked since algorithm never chooses marked process eviction follows h never visit process p subsequently phase phase hole h created process say p 1 clean page request moves around zero times within process p 1 account mistakes request hole h fair fault upon moves process p 2 never come back process p 1 phase behaves similarly process p 2 end phase let h denote total number faults attributed requests hole h phase f h faults fair faults u h faults unfair faults lemma 11 proof techniques proofs lemma 7 theorem 2 prove following key lemma lemma 12 expected number ef h page requests hole h phase result fair faults h p gamma1 lemma 10 algorithm incurs page faults phase clean page requests expected value quantity h p lemma 12 expression number unfair faults number mistakes considered worth detecting algorithm efficient number unfair faults additive term phase oe clean requests denote oe 9 conclusions 14 theorem 3 number faults phase oe clean page requests oe unfair faults bounded 1 time oe unfair faults application process makes mistake causes fault must evict page cache application process ever asked evict page service unfair fault caused application process 9 conclusions cache management strategies prime importance high performance comput ing consider case p independent processes running computer system sharing common cache size k applications often advance knowledge page request sequences paper address issue exploiting advance knowledge devise intelligent strategies manage shared cache theoretical setting presented simple elegant applicationcontrolled caching algorithm multiapplication caching problem achieves competitive ratio 2h 2 result significant improvement competitive ratios 2p multiapplication caching thetah k classical caching since cache size k often orders magnitude greater p proven online algorithm problem competitive ratio smaller minfh even application processes perfect knowledge individual request sequences conjecture upper bound h p gamma1 proven second order terms perhaps using techniques ms91 although resulting algorithm likely practical using notion mistakes able consider realistic setting application processes make bad paging decisions show algorithm fair efficient algorithm situation application needs pay application processs mistake bound global caching performance algorithm terms number mistakes notions good page replacement decisions mistakes fairness context new one related area possible future work consider alternative models model worstcase interleaving another interesting area would consider caching situation applications good knowledge future page requests applications knowledge future requests could also consider pages shared among application processes r study replacement algorithms virtual storage com puters competitive pag cao implementation performance applicationcontolled file caching competitive algorithms paging problems markov paging strongly competitive randomized paging algorithm amortized efficiency list update paging rules tr ctr guy e blelloch phillip b gibbons effectively sharing cache among threads proceedings sixteenth annual acm symposium parallelism algorithms architectures june 2730 2004 barcelona spain