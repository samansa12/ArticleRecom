learning functions represented multiplicity automata study learnability multiplicity automata angluins exact learning model investigate applications starting point known theorem automata theory relating number states minimal multiplicity automaton function rank hankel matrix theorem hand present new simple algorithm learning multiplicity automata improved time query complexity prove learnability various concept classes include among others class disjoint dnf generally class polynomials finite fields class boundeddegree polynomials infinite fields class xor terms certain classes boxes high dimensions addition obtain best query complexity several classes known learnable methods decision trees polynomials gf2 multiplicity automata shown useful prove learnability subclasses dnf formulae various classes study limitations method prove method cannot used resolve learnability open problems learnability general dnf formulas even kterm dnf results proven exhibiting functions classes require multiplicity automata superpolynomial number states b introduction exact learning model introduced angluin 5 since attracted lot attention particular following classes shown learnable model deterministic automata 4 various types dnf formulae 1 2 3 6 16 17 18 20 29 33 multilinear polynomials gf2 44 learnability model also implies learnability pac model membership queries 46 5 one classes shown learnable model class multiplicity automata 12 1 40 multiplicity automata essentially nondeterministic automata weights field k edges automaton computes function follows every path automaton assign weight product weights edges path function computed automaton essentially sum weights paths consistent input string sum value k 2 multiplicity automata generalization deterministic automata algorithms learn class 12 13 40 generalizations angluins algorithm deterministic automata 4 use algebraic approach learning multiplicity automata similar 40 approach based fundamental theorem theory multiplicity automata theorem relates size smallest automaton function f rank k socalled hankel matrix f 22 26 see also 25 15 background multiplicity 1 full description work appears 13 automata known literature various names paper refer multiplicity automata functions computed automata usually referred recognizable series automata using theorem ideas algorithm 42 learning deterministic automata develop new algorithm learning multiplicity automata efficient algorithms 12 13 40 particular give refined analysis complexity algorithm learning functions f finite domain different algorithm similar complexity found 21 3 work show learnability multiplicity automata implies learnability many important classes functions 4 first shown learnability multiplicity automata implies learnability class satisfys dnf formulae assignment satisfies terms class includes special case class disjoint dnf includes class decision trees results improve previous results 18 2 16 generally consider boxes discrete domain points ie boxes considered many works eg 37 38 23 7 27 31 39 prove learnability union olog n boxes time polyn learnability union disjoint boxes generality boxes point contained time polyn 5 special case results implies learnability corresponding classes dnf formulae show learnability class xor terms open problem 44 class polynomials finite fields open problem 44 19 class boundeddegree polynomials infinite fields well classes functions finite infinite fields also prove learnability certain class decision trees whose learnability open problem 18 multiplicity automata proved useful solve many open problems regarding learnability dnf formulae classes polynomials decision trees study limitations method prove method cannot used resolve learnability open problems learnability general dnf formulae even kterm dnf results tight sense olog nterm dnf formulae satisfyo1 dnf learnable using multiplicity automata impossibility results proven exhibiting functions classes require multiplicity automata superpolynomial number states proving results use relation multiplicity automata hankel matrices 3 fact 21 show algorithm generalized k necessarily field rather certain type ring 33 shown learnability deterministic automata used learn certain much restricted classes functions 5 9 using additional machinery dependency improved organization section 2 present background multiplicity automata well definition learning model section 3 present learning algorithm multiplicity automata section 4 present applications algorithm learning various classes functions finally section 5 study limitations method background 21 multiplicity automata section present definitions basic result concerning multiplicity au tomata let k field sigma alphabet f sigma k function associate f infinite matrix f rows indexed string x 2 sigma columns indexed string 2 sigma x entry f contains value fxffiy ffi denotes concatenation automata literature function f often referred formal series f hankel matrix use f x denote xth row f x entry f may therefore denoted f x f xy notation adapted matrices used sequel next define automaton representation field k functions automaton size r consists jsigmaj matrices f oe oe 2 sigmag r theta r matrix elements k rtuple automaton defines function first associate every string sigma r theta r matrix k defining ffl 4 id id denotes identity matrix 6 string let w simple useful property denotes first row matrix w words automaton r states transition state q state q j letter oe weight oe ij weight path whose last state q product weights along path multiplied fl function computed string w sum weights paths corresponding w following fundamental theorem theory formal series relates size minimal automaton f rank f 22 26 theorem 21 let f sigma k let f corresponding hankel matrix size r smallest automaton fa j f satisfies field k although theorem basic provide proof sheds light way algorithm section 3 works 6 matrix 1s main diagonal 0s elsewhere direction given automaton f size r prove rankf r define two matrices r whose rows indexed sigma columns indexed c whose columns indexed sigma rows indexed r x entry r contains value x 1i entry c contains value delta fl show follows following sequence simple equalities r c denotes yth column c obviously rank r c bounded r linear algebra rankf minfrankr rankcg therefore rankf needed direction ii given function f corresponding matrix f rank r show construct automaton size r computes function let f x r independent rows f ie basis corresponding strings x define first define next every oe define ith row matrix oe unique coefficients row f x ffioe expressed linear combination f x 1 r prove induction jwj length string w w follows fa choose x induction base ffl case needed induction step using equation 1 r induction hypothesis equals r needed 22 learning model learning model use exact learning model 5 let f target function learning algorithm may propose step hypothesis function h making equivalence query eq oracle h logically equivalent f answer query yes learning algorithm succeeds halts otherwise answer equivalence query algorithm receives counterexample assignment z fz 6 hz learning algorithm may also query oracle value function f particular assignment z making membership query mq z response query value fz 7 say learner learns class functions c every function f 2 c learner outputs hypothesis h logically equivalent f time polynomial size shortest representation f 3 algorithm section describe exact learning algorithm multiplicity automata size parameter case multiplicity automata number states minimal automaton f algorithm efficient number length longest counterexample provided k target function algebraic operations algorithm done field k 8 algorithm learns function f using hankel matrix representation f difficulty f infinite large even restricting inputs length n however theorem 21 direction ii implies sufficient maintain independent rows f fact r theta r submatrix f full rank suffices therefore learning algorithm viewed search appropriate r rows r columns algorithm works iterations beginning th iteration algorithm holds set rows x ae sigma set columns ae sigma fy f z denote restriction row f z coordinates ie f z note given z vector b f z computed using queries hold b linearly independent vectors using vectors algorithm constructs hypothesis h manner similar proof direction ii theorem 21 asks equivalence query counterexample h leads adding new element x way preserves properties 7 f boolean standard membership query 8 assume every arithmetic operation field takes one time unit immediately implies number iterations bounded r assume without loss generality fffl 6 0 9 algorithm works follows 1 x fx 2 define hypothesis h following direction ii theorem 21 every oe define matrix b oe letting ith row coefficients vector b expressed linear combination vectors b coefficients exist b independent tuples b define theta matrix b w follows let b id string finally h defined 3 ask equivalence query eqh answer yes halt output h otherwise answer z counterexample find using mqs f string wffioe prefix z b b exists 2 fwffioe 6 go 2 following two claims used proof correctness show every iteration algorithm prefix required step 3 found result number independent rows grows 1 31 let z counterexample h found step 3 ie fz 6 hz exists prefix wffioe satisfying b proof assume towards contradiction prefix satisfies b prove induction length every prefix w z condition satisfied 9 check value fffl ask membership query learn f 0 identical f except ffl gets value different 0 note matrix f 0 identical f entries except one rank f 0 differs rank f 1 change makes algorithm asking eq modify hypothesis h value ffl alternatively find string z fz 6 0 asking eq0 start algorithm gives 2 theta 2 matrix full rank proof theorem 21 follows however need fact analyzing algorithm algorithm know r advance induction base trivial since b ffl induction step consider prefix wffioe induction hypothesis b implies assumption prefix satisfies b b satisfied respect prefix wffioe b definition b definition matrix multiplication together b completes proof induction induction claim get b particular b f z however lefthand side equality fz righthand side hz thus get contradiction since z counterexample 32 whenever step 2 starts vectors b defined current x linearly independent proof proof induction first time step 2 starts ffflg assumption fffl 6 0 single vector b f ffl zero vector hence claim holds induction assume claim holds step 2 starts show also holds step 3 ends note step 3 new vector b fw added vectors new coordinate corresponding oe ffiy induction hypothesis step 2 starts f x linearly independent tuples particular implies step 2 starts b fw unique representation linear combination b since w satisfies linear combination given b remain linearly independent respect new however time fw becomes linearly independent b respect new otherwise linear combination must given b however wffioe satisfies b get b fw oe fwffioe 6 oe ffiy eliminates linear combination note oe ffiy added b f defined coordinates refer conclude step 3 ends b f x 1 w linearly independent summarize analysis algorithm following theorem let denote size longest counterexample z obtained execution algorithm denote complexity multiplying two r theta r matrices theorem 33 let k field f sigma k function k f learnable algorithm time ojsigmaj using r equivalence queries ojsigmaj queries proof claim 31 guarantees algorithm always proceeds since algorithm halts eqh returns yes correctness follows complexity claim 32 implies number iterations therefore number equivalence queries r fact theorem 21 implies number iterations exactly r number mqs asked step 2 whole algorithm jsigmaj since every x 2 x 2 need ask value fxy values fxoey oe 2 sigma analyze number mqs asked step 3 first need specify way appropriate prefix found naive way go prefixes z finding one satisfying b efficient search based upon following generalization claim 31 suppose v prefix z condition holds b f x exists wffioe prefix z extends v satisfies b proof identical proof claim 31 except base induction use v instead ffl using generalized claim desired prefix wffioe found using binary search log jzj log steps follows middle prefix v check whether holds make v left border search hold equation 2 condition b holds v v becomes right border search step binary search 2 2r membership queries asked note values b known step 2 together number mqs asked execution algorithm olog jsigmajr 2 running time compute matrices b oe observe matrix whose rows b f x ffioe product b oe matrix whose rows b therefore finding b oe done one matrix inversion whose cost also omr one matrix multiplication hence complexity step 2 ojsigmaj delta mr step 3 difficult part compute value b w prefixes z simple way computing matrix multiplications z better way computation step 3 observing need compute actually first row matrix b wm first row matrix simply written w thus compute row first compute 1 multiply result b therefore computation done vectormatrix multiplications requires om delta r 2 time together running time ojsigmaj complexity algorithm compared complexity algorithm 12 13 uses r equivalence queries ojsigmajmr 2 membership queries runs time algorithm 40 uses r1 equivalence queries ojsigmaj mr 2 membership queries runs time ojsigmaj mr 4 31 case functions many cases interest domain target function f sigma rather sigma n value n view f function sigma whose value 0 strings whose length different n show case complexity analysis algorithm improved reason case matrix f simpler structure row column indexed string whose length n alternatively rows columns corresponding longer strings contain 0 entries moreover string x length 0 n nonzero entries row f x correspond ys length denote f submatrix f whose rows strings sigma columns strings sigma ngammad see fig 1 observe structure f learn function f use algorithm ask membership queries strings length exactly n strings return 0 without actually asking query equivalence queries view hypothesis h restricted sigma n length counterexamples case always n looking closely algorithm follows since b f submatrix f b f x always independent vectors rankf every number x x whose length bounded rankf denote r r max d0 r number equivalence queries remains r number membership queries however becomes smaller due fact many entries f known 0 step 2 whole execution ask every x 2 x length every 2 length one mq fxy every 2 length every oe 2 sigma ask mq fxoey together step 2 algorithm asks every x r queries total delta r max jsigmaj membership queries step 3 r iterations log n search steps ask 2r max membership queries entries row contain 0s together orr max log n membership queries step 3 whole algorithm log n f f figure 1 hankel matrix f running time note matrices b oe also special structure entries j 0 corresponding vectors x jx multiplication matrices done therefore invocation step 2 requires time ojsigmajn delta mr max similarly b w 1 entries 0 corresponding strings column b units furthermore need multiply r max columns nonzero coordinates b therefore step 3 takes nr 2 counterexample z together running time onrr 2 corollary 34 let k field f sigma n k d0 rankf rank taken k f learnable algorithm using equivalence queries ojsigmajlog nr delta r queries 4 positive results section show learnability various classes functions algorithm done proving every function f class question corresponding hankel matrix f low rank theorem 33 implies learnability class algorithm first observe possible associate multiplicity automaton every nondeterministic automaton every string w multiplicity automaton counts number accepting paths nondeterministic automaton w see define j entry matrix oe 1 given automaton move letter oe state state j otherwise entry 0 addition define fl 1 accepting state 0 otherwise thus automaton deterministic unambiguous 11 associated multiplicity automaton defines characteristic function language 33 class deterministic automata contains class olog nterm dnf fact class boolean functions olog nterms hence classes learned algorithm note general nondeterministic automata learned implies learnability dnf 41 classes polynomials first results use learnability multiplicity automata learn various classes multivariate polynomials start following claim theorem 41 let p ij z arbitrary functions single variable 1 k defined finally let f sigma n k defined f hankel matrix corresponding f f submatrices defined section 31 every 0 n proof recall definition f every string z 2 sigma n viewed partitioned two substrings every row f indexed hence written function f x every x term constant ff ix 2 k means every function f x linear combination functions value implies rankf needed corollary 42 class functions expressed functions gfp summands summand product form p i1 arbitrary functions learnable time polyn p 11 nondeterministic automata unambiguous every w 2 sigma one accepting path corollary implies special case learnability polynomials gfp extends result 44 multilinear polynomials arbitrary polynomials algorithm see corollary 34 polynomials n variables terms uses ont equivalence queries ot 2 n log n membership queries special case class class multilinear polynomials gf2 known learnable 44 algorithm uses ont equivalence queries ot 3 n membership queries worse values corollary 42 discusses learnability certain class functions includes class polynomials finite fields complexity algorithm depends size field following theorem extends result infinite fields assuming functions p ij boundeddegree polynomials also improves complexity learning polynomials finite fields degree polynomials significantly smaller size field theorem 43 class functions field k expressed summands summand form p i1 polynomials degree k learnable time polyn k furthermore jkj nk class learnable membership queries time polyn small probability error proof show although field k may large run algorithm using alphabet k elements field g need show queries asked answered membership queries asked algorithm present queries taken domain sigma n equivalence queries following instead representing hypothesis jsigmaj matrices b oe k1 represent single matrix hx entries degree k polynomial k every oe 2 sigma oe find use interpolation entries also terminology hypothesis easy see target function hypothesis degreek polynomials n variables therefore given counterexample w 2 k n modify sigma n follows ith step fix z hypothesis target function become degree k polynomials variable z hence exists oe 2 sigma two polynomials disagree set w end new counterexample w 2 sigma n desired assume k contains least nk subset k schwartz lemma 45 two different polynomials z variable agree knjlj ngamma1 assignments l n therefore picking random polyn random elements l n obtain high probability counterexample hypothesis counterexample exists proceed ie modify counterexample domain sigma n etc algorithm learns multivariate polynomials using membership queries called interpolation algorithm eg 10 28 47 24 43 30 background references see 48 10 shown interpolate polynomials infinite fields using 2t membership queries 47 shown interpolate polynomials finite fields elements number elements field less k every efficient algorithm must use equivalence queries 24 43 theorem 43 polynomials interpolate general form standard interpolation require number elements field least kn 1 42 classes boxes section consider unions ndimensional boxes n denotes set formally box n defined two corners view box boolean function gives 1 every point n inside box 0 point outside box start general claim theorem 44 let p ij z arbitrary functions single variable 1 defined assume point satisfies functions g finally let f sigma n f0 1g defined f hankel matrix corresponding f every field k every proof function f expressed jsjt jsjs last equality assumption point satisfies functions note every function form i2s g product n functions one function single variable therefore applying theorem 41 complete proof corollary 45 class unions disjoint boxes learned time polyn number boxes target function class unions olog n boxes learned time polyn proof let b box denote two corners b functions single 1g 1 j z defined belongs box b therefore corollary 34 theorem 44 imply corollary 43 classes dnf formulae section present several results classes dnf formulae related classes first consider following special case corollary 42 solves open problem 44 corollary 46 class functions expressed exclusiveor necessarily monotone monomials learnable time polyn corollary 46 refer subclass dnf already implies learnability disjoint ie satisfy1 dnf also since dnf special case union boxes 2 get learnability disjoint dnf corollary 45 next discuss positive results satisfys dnf larger values following two important corollaries follow theorem 44 note theorem 44 holds field convenience efficiency use corollary 47 class satisfys dnf formulae corollary 48 class satisfys tterm dnf formulae learnable following choices 1 log log n 3 log log n 44 classes decision trees mentioned algorithm efficiently learns class disjoint dnf formulae particular includes class decisiontrees using algorithm decision trees size n variables learnable using otn equivalence queries ot 2 n log n membership queries better best known algorithm decision trees 18 uses ot 2 equivalence queries ot 2 follows consider general classes decision trees corollary 49 consider class decision trees compute functions f gfp follows node v contains query form x 2 v v gfp computation proceeds left child v x computation proceeds right child leaf tree marked value output assignments reach leaf class learnable time polyn jlj p l set leaves proof tree written function whose value 1 assignment reaches leaf 0 otherwise note decision tree assignment reaches single leaf consider specific leaf assignments reach expressed n sets n assignment reaches leaf x j 2 j j define p j 1 corollary 42 result follows result implies special case learnability decision trees greater queries nodes open problem 18 note every decision tree greaterthan queries computes boolean function expressed union disjoint boxes hence case also derived corollary 45 next theorem used learn classes decision trees theorem 410 let defined f hankel matrix corresponding f g hankel matrix corresponding g rankf proof two matrices b dimension hadamard product fi b defined c well known rankc ranka delta rankb note f hence theorem follows theorem interesting applications corollary 411 let c class functions expressed following way arbitrary functions single variable 1 defined sigma n finally defined learnable time polyn jsigmaj corollary 412 consider class decision trees depth query node v boolean function f v r defined section 31 t1 class learnable time polyn jsigmaj proof leaf write function g product functions follows node v along path use edge labeled 1 take f v product use edge labeled 0 take product note value r max corresponding 1 gamma f v 1 theorem 410 g hankel matrix corresponding g rankg t1 follows rankf 2 jlj 2 rankb corollary follows class contain example decision trees depth olog n contain node term xor subset variables defined 34 5 negative results purpose section study limitation learnability via automaton representation show algorithm well algorithm whose complexity polynomial size automaton algorithms 12 13 40 efficiently learn several important classes functions precisely show classes contain functions f small automaton theorem 21 enough prove rank corresponding hankel matrix f large every field k define function f exists z 1 function f nk expressed dnf formula note formula readonce monotone k terms first observe rank hankel matrix corresponding f nk equals rank f hankel matrix corresponding f 2kk also clear rankf rankf k prove rankf k 1 consider complement matrix k obtained f k switching 0s 1s prove induction k rankd k note implies rankd 1 follows rankf k j all1 matrix 12 using functions f nk prove main theorem section fact function f 0 z ngammak1 similar properties f nk shown rankomegagamman k delta n hence slightly improving results theorem 51 following classes learnable multiplicity automata field k 1 dnf 2 monotone dnf 3 2dnf 4 readonce dnf 5 kterm dnf 6 satisfys dnf 7 readj satisfys dnf n classes known learnable methods monotone dnf 5 readonce dnf 6 1 41 2dnf 46 natural generalizations classes known learnable automata log nterm dnf 17 18 20 33 satisfys dnf methods readj satisfys log log n 16 learnability others still open problem proof observe f nn2 belongs classes dnf monotone dnf 2dnf readonce dnf argument every automaton size 2 n2 shows every n function f nk exactly kterms every automaton size 2 n superpolynomial proves 5 consider function f ns log n every automaton size 2 log superpolynomial show function f ns log n small satisfy dnf representation partition indices sets log n indices set formula 2 log n variables 1 iff exists 2 z 1 moreover formula satisfy1 ie disjoint dnf n 2 terms standard dnf representation disjunction formulas gives satisfys dnf sn 2 terms proves 6 finally function f nk requires automaton superpolynomial size hand partitioning variables sets log j variables observe standard dnf representation variable appears 2 log function readj satisfys dnf proves 7 follows wish strengthen previous negative results motivation context automata fixed order characters string however general particular functions sigma n natural order indeed important functions disjoint dnf learnable automata using order variables hand functions certain orders much better others example function f nk requires automaton size exponential k standard order considered instead read variables order small even deterministic automaton size additional example every readonce formula good order order leaves tree representing formula goal show even oracle could give us good necessarily best order variables could somehow learn order still classes cannot learned automata shown exhibiting function small automaton every order variables show define function n follows denote input variables g nk w k function g nk outputs 1 iff exists w intuitively g nk similar f nk instead comparing first k variables next k variables first shift first k variables 13 first show express g nk dnf formula fixed define function 1 iff holds observe g n 0 kt isomorphic f n 0 k representable dnf formula k terms size 2 write g therefore g nk written monotone readk dnf k 2 terms size 3 show every order variables rank matrix corresponding g nk large sufficient prove value rank matrix corresponding g n 0 kt large since submatrix matrix corresponding g nk see fix w sufficient prove rank g 2kkt large main technical issue choose value look order induces z ignoring w look first k indices order assume without loss generality least half hence last k indices least half 13 rank method used prove every automaton f nk large similar rank method communication complexity technique use next also similar methods used variable partition communication complexity background see eg 36 35 denote set indices appear among first k indices order denote b set indices appears among last k indices order b subsets assumption jaj jbj k2 define ag show size b omegagamma k write let 0 b size jsj k4 denote g matrix corresponding g 2kkt 0 particular let g 0 submatrix g rows strings x length k according order whose bits fixed 0s columns strings length k whose bits fixed 0s matrix matrix obtained proof f kk2 whose rank therefore 2 k2 gamma 1 corollary 52 following classes learnable automata field k even best order known 1 dnf 2 monotone dnf 3 3dnf 4 kterm dnf 5 satisfys dnf r exact learning readtwice dnf formulas exact learning readk disjoint dnf notsodisjoint dnf learning kterm dnf formulas using queries counterexamples learning regular sets queries counterexamples machine learning learning readonce formulas queries applications multiplicity automata learning learning boxes high dimension deterministic algorithm sparse multivariate polynomial interpolation learning satkdnf formulas membership queries learning behaviors automata multiplicity equivalence queries learning behaviors automata multiplicity equivalence queries learning behaviors automata shortest coun terexamples rational series languages learning readksatisfy j dnf fast learning kterm dnf formulas queries exact learning via monotone theory note learning multivariate polynomials uniform distribu tion simple learning algorithms using divide conquer learning matrix functions rings realization stochastic finite automaton zerotesting interpolation ksparse multivariate polynomials finite fields matrices de hankel learning unions boxes membership equivalence queries fast parallel algorithms sparse multivariate polynomial interpolation finite fields learning 2 dnf formulas k decision trees interpolation sparse multivariate polynomials large finite fields applications efficient membershipquery algorithm learning dnf respect uniform distribution introduction computational learning theory simple algorithm learning olog nterm dnf learning decision trees using fourier spectrum communication complexity vlsi theory complexity learning counterexamples algorithms lower bounds online learning geometrical concepts efficient learning virtual threshold gates polynomial time learning algorithm recognizable series inference finite automata using homing sequences interpolation approximation sparse multivariate polynomials gf 2 learning sparse multivariate polynomials field queries counterexamples fast probabilistic algorithms verification polynomial identities theory learnable interpolating polynomials values efficient polynomial computation tr theory learnable learning regular sets queries counterexamples rational series languages deterministic algorithm sparse multivariate polynomial interpolation interpolating polynomials values introduction algorithms fast parallel algorithms sparse multivariate polynomial interpolation finite fields interpolation approximation sparse multivariate polynomials gf2 learning 2u dnf formulas italickuitalic decision trees vlsi theory zerotesting interpolation inlineequation f kf inlineequationsparse multivariate polynomials finite fields exact learning readtwice dnf formulas extended abstract online learning rectangles random dfas approximately learned sparse uniform examples exact learning readitalickitalic disjoint dnf notsodisjoint dnf learning readonce formulas queries c45 programs machine learning learning decision trees using fourier spectrum cryptographic hardness distributionspecific learning online learning rectangles noisy environments cryptographic limitations learning boolean formulae finite automata inference finite automata using homing sequences learning readitalickitalicsatisfyitalicjitalic dnf learning unions boxes membership equivalence queries algorithms lower bounds online learning geometrical concepts introduction computational learning theory readtwice dnf formulas properly learnable fast learning italickitalicterm dnf formulas queries exact learning boolean functions via monotone theory note learning multivariate polynomials uniform distribution extended abstract learning sparse multivariate polynomials field queries counterexamples learning satitalickitalicdnf formulas membership queries learning behaviors automata multiplicity equivalence queries simple learning algorithms using divide conquer simple algorithm learning log italicnitalicterm dnf communication complexity efficient membershipquery algorithm learning dnf respect uniform distribution art computer programming volume 2 3rd ed efficient learning virtual threshold gates interpolation sparse multivariate polynomials large finite fields applications fast probabilistic algorithms verification polynomial identities automata languages machines induction decision trees queries concept learning probabilistic algorithms sparse polynomials learning behaviors automata shortest counterexamples simple learning algorithms decision trees multivariate polynomials applications multiplicity automata learning ctr amir shpilka interpolation depth3 arithmetic circuits two multiplication gates proceedings thirtyninth annual acm symposium theory computing june 1113 2007 san diego california usa nader h bshouty lynn burroughs proper learning axisparallel concepts journal machine learning research 4 p157176 1212003 lane hemaspaandra sigact news complexity theory column 32 acm sigact news v32 n2 june 2001 ricard gavald pascal tesson denis thrien learning expressions programs monoids information computation v204 n2 p177209 february 2006