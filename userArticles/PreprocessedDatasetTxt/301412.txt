parsimonious least norm approximation theoretically justifiable fast finite successive linear approximation algorithm proposed obtaining parsimonious solutionto corrupted linear system axbp corruptionp due noise error measurement proposedlinearprogrammingbased algorithm finds solutionx parametrically minimizing number nonzeroelements x error verbaraxbpverbar1numerical tests signalprocessingbased exampleindicate proposed method comparable method parametrically minimizesthe1 norm solution x error verbaraxbpverbar1 methods superior byorders magnitude solutions obtained least squares well combinatorially choosing optimal solution specific number nonzero elements b introduction wide range important applications reduced problem estimating vector x minimizing norm residual vector ax gamma b arising possibly inconsistent system linear equations theta n real matrix b theta 1 vector b subject error methods solving problems include least squares 15 total least squares 11 14 structured total least norm 24 13 paper consider closely related problem minimizing 1norm residual vector subject error additional condition specified number columns used clearly combinatorial problem closely related nphard problem considered amaldi kann 2 consisting solving consistent system linear inequalities equalities rational entries solution x minimal number nonzeros shall solve problem novel method based minimizing concave function polyhedral set successfully used machine learning problems misclassification minimization 17 feature selection 6 data mining 5 19 mathematical programming technical report 9703 march 1997 revised september november 1997 material based research supported national science foundation grants ccr9322479 ccr9509085 air force office scientific research grant afosr144gc92 computer sciences department university wisconsin 1210 west dayton street madison wi 53706 paulbcswiscedu z computer sciences department university wisconsin 1210 west dayton street madison wi 53706 olvicswiscedu author gratefully acknowledges gracious hospitality mathematics department university california san diego sabbatical leave januarymay 1997 x computer science engineering university california san diego la jolla ca 92093 jbrosenucsdedu idea behind using columns possible span b motivated parsimony principle machine learning known occams razor 26 4 says essence simplest best principle highly effective generalization purposes 16 25 30 example one wishes use solution x 1 new data represented rows b would case either b corrupted noise use 1norm enable us use finite algorithm based polyhedral concave minimization approach indicated successfully used difficult machine learning problems particular eventually cast problem minimizing concave function polyhedral set begin following unconstrained minimization problem min e column vector ones prime denotes transpose j delta j denotes absolute value function applied componentwise vector delta step function applied componentwise also step function takes value 0 argument nonpositive value 1 argument positive vector b corrupted noise vector p application note immediately problem 2 classical least 1norm approximation problem problem 2 trivially solved interest interested solutions problem 2 2 0 1 make e 0 jxj k desired k n acceptably small fact problem 2 viewed multiobjective optimization problem 8 two objectives parsimony number nonzero components x smallness error kax gamma bk 1 letting range interval 0 1 cardinality nonzero elements solution x varies maximum n 0 error kax nondecreasing monotonically depending problem one xs desirable many machine learning applications small values 005 often gave parsimonious results improved tenfold crossvalidation 6 shall call problem 2 possibly noisecorrupted b parsimonious least norm approximation problem plna approach solving 2 convert concave minimization problem polyhedral set problem 12 first show problem always solution theorem 21 replace discontinuous step function objective function exponential smooth function problem 14 done 18 6 relate two problems novel theorem theorem 21 shows continuous problem yields exact solution discontinuous problem repeating optimal vertex identified increasing finite values smoothing parameter ff prescribe linearprogrammingbased successive linearization algorithm sla 31 solution smooth problem establish finite termination theorem 32 comparative purposes shall also employ vapniks support vector machine approach 29 3 minimizing size solution vector x well error kax decreasing vc dimension 29 p 76 capacity measure improving generalization shall parametrically minimizing 1norm x well 1norm error ax gamma b min shall call problem possibly noisecorrupted b least least norm approximation problem solve solving equivalent linear programming formulation min word notation background material vectors column vectors unless transposed row vector prime superscript 0 vector x ndimensional real space r n jxj denote vector absolute values components x x scalar product two vectors x ndimensional real space denoted x 0 linear program min c 0 x notation arg vertex min c 0 x denote set vertex solutions linear program x 2 r n norm kxk 2 denote 2norm denote 1norm theta n matrix denote ith row ij denote element row column j identity matrix real space arbitrary dimension denoted column vector ones arbitrary dimension denoted e base natural logarithm denoted gammay denote vector r component gammay function f r n r concave r n supergradient fx f x vector r n satisfying 2 r n set dfx supergradients f point x nonempty convex compact reduces ordinary gradient rfx f differentiable x 22 23 vector denote cardinality nonzero elements x 2 concave minimization problem section shall consider minimization problem min f concave function r k bounded nonnegative real number h nonnegative vector r k polyhedral set r k containing straight lines go infinity directions note objective function 6 concave general nonconcavity h 0 jsj 23 corollary 3233 problem 6 solution 23 corollary 3234 vertex solution since contains straight lines go infinity directions however despite lack concavity shall show precisely existence vertex solution novel approach approximates step function nonnegative real line exponential smooth approximation also serve means generating finitely terminating algorithm stationary point 6 another important feature exact solution 6 obtained solution smooth approximation finite value smoothing parameter state smooth approximation 6 follows min ff positive number obvious relation hence smooth problem 7 minimum provides underestimate minimum problem 6 fact used establish exact solution latter former following principal theorem paper also provides method solution well theorem 21 existence exact vertex solution finite value smoothing parameter r bounded polyhedral set contains straight lines going infinity directions let f concave r k let h 0 let fixed positive number sufficiently large positive finite value ff 0 ff smooth problem 7 vertex solution also solves original nonsmooth problem 6 proof note first smooth problem 7 equivalent following concave minimization problem min since objective function problem concave z r 2k bounded follows 23 corollaries 3233 3234 vertex sff zff solution ff 0 since finite number vertices one vertex say z repeatedly solve problem hence ff ff 0 last inequality follows 8 letting gamma 1 follows z solves 6 since z vertex follows vertex theorem immediately suggests algorithmic approach solving problem 2 follows first rewrite 2 following equivalent problem min making identifications x problem 12 hence problem 2 becomes special case problem 6 shall solve smooth version 7 specifically smooth version 2 following concave minimization problem min solving problem sufficiently large positive ff follows theorem 21 solved original problem 2 turn attention solving 14 finitely terminating successive linearization algorithm 3 concave minimization algorithm finite method shall propose successive linear approximation sla method minimizing concave function polyhedral set finitely terminating stepless frank wolfe algorithm 9 18 finite termination sla established differentiable concave function 20 nondifferentiable concave function using supergradient state sla problem 14 differentiable concave objective function algorithm 31 successive linearization algorithm sla start random x 0 2 r n stop 18 theorem 42 following finite termination result sla algorithm theorem 32 sla finite termination sla 31 generates finite sequence fx strictly decreasing objective function values problem 14 terminating satisfying minimum principle necessary optimality condition 4 application numerical testing wish determine whether xcomponent suppression xnorm reduction observed linear system corruption true system leads improved approximation true system one relate machine learning framework treating first system training set second system testing set 12 linear systems used based upon ideas related signal processing 10 28 specifically example 1 equation 8 consider following true signal assume true signal gt cannot sampled precisely following observed signal sampled sampled times assume know true signal gt 18 attempt model problem compute coefficients x adequately recover gt given noisy data gt 19 notice substituting following coefficient vector x 20 thus true linear system testing set given solved exactly x 21 observed linear system training set number refer solution problem 14 b 14 replaced b computed successive linearization algorithm sla 31 plna solution similarly shall refer solution problem 4 b replaced b llna solution note experiments value ff negative exponential 14 50 scalars considered zero interval gamma1e gamma 8 1e gamma 8 components initial starting point x 0 sla 31 sampled normal distribution mean 0 standard deviation 1 components initial point sampled fixed runs focus attention four approaches compare solutions obtained plna llna methods solutions obtained least squares combinatorial search 41 comparison plna llna least squares compute solutions observed system defined 23 plna llna least squares solutions evaluated observed system training set residual kax true system testing set residual kax gamma bk 1 graphically comparing recovered signal gt 20 true signal gt 18 plna solution x given computed solving sla 31 concave minimization problem 14 b replaced b min llna solution x ax bp given computed solving linear program 4 b replaced b min least squares solution minimizer kax solution normal equations although 26 theta 10 matrix defined 23 rank 10 matrix 0 numerically singular smallest eigenvalue less 10 gamma14 thus resort singular value decomposition approach solving 27 determine approximate solution xls 27 following method utilizes singular value decomposition 27 ordinary matlab 21 commands perturbed system give x error compared given method described x efined 21 perturbation vector p components sampled normal distribution mean 0 standard algorithm 41 least squares via singular value decomposition let 2 r mthetan small positive tolerance 1 determine economy singular value decomposition 21 svda0 u 2 r mthetan r nthetan 2 determine index r oe 3 set mthetar first r columns u nthetar first r columns v rthetar diagoe 4 compute solution min runs fixed 00001 specific matrix defined 23 led algorithm discarded last 4 columns u v plna problem 25 llna problem 26 solved values 2 10g figures display results averaged 5 noise vectors elements sampled normal distribution mean 0 standard deviation 1 average kpk figure 1 plot averages kax various values measuring well plna llna solutions solve corrupted observed linear system also plotted average kaxls measuring well least squares solution algorithm 41 solves observed system p proved plna llna errors nondecreasing functions worse corresponding least squares error however true system results reversed see next paragraph figure 2 plot averages kax gamma bk 1 plna llna various values measuring well plna solution 25 solves true linear system also plotted suppression parameter l observed system axbpleast squares plna llna figure 1 average versus x plna solution 25 curve marked plna llna solution 26 curve marked llna compared average least squares solution 27 algorithm 41 results averaged 5 noise vectors p plna llna solutions computed values suppression parameter l true system 26 107 28 28 26 53 26 22 44 24 14 18 40 22 13 18 40 20 12 18 39 16 11 16 39 10 13 10 04 1004least squares plna llna figure 2 average kax gamma bk 1 versus x plna solution 25 curve marked plna llna solution 26 curve marked llna compared average least squares solution 27 solved algorithm 41 results averaged 5 noise vectors p plna llna solutions computed values abovebelow curves labelled plna llna various values indicate average number nonzero elements x followed second number number denotes kxk 1 average kaxls gamma bk 1 measuring well least squares solution algorithm 41 solves figure 3 compare averages 1norm distances true solution x 21 plna llna solutions x averages 1norm distances x least squares solution xls recall true solution x ax b note 001 plna llna distances smaller least squares distance 1 x 0 even though kx gamma x k 1 small solution poor signal recovery point view since zero vector gives worst discrepancy true signal recovered signal 26 discrete points see figure 2 figure 4a plot true signal observed signal signal recovered solving one noise vector p plna 25 figure 4b displays true signal observed signal signal recovered problem least squares solved algorithm 41 probably significant result signal recovered plna llna considerably closer true signal obtained least squares solution 42 comparison plna llna combinatorial search section reformulate plna problem solution x fixed number nonzero elements k 2 gammay nonzero elements also formulate llna similarly follows similarly k 2 ng combinatorial search solution x c obtained solving nonzero elements notice x c determined enumerating subsets size k set n elements subsets rather expensive procedure computationally requiring two orders magnitude time plna llna figure 5 displays results averaged 5 noise vectors p 2 r elements sampled normal distribution mean 0 standard deviation 201777 plotted averages kax measuring well plna llna combinatorial solutions solve observed system also plotted averages kax gamma bk 1 kax c gamma bk 1 k measuring well solutions solve true system figure 6 displays average 1norm distance x 21 solutions obtained plna llna combinatorial search averages 5 noise vectors p figure 7a convenience duplicates figure 4a displays true signal observed signal signal recovered solving plna 25 value signal suppression parameter l distance true solution xx least squares plna llna figure 3 average kx gamma x k 1 versus x plna solution 25 curve marked plna llna solution 26 curve marked llna compared average least squares solution 27 solved algorithm 41 true solution x 21 ax b plna llna solutions computed values 012observed actual plna llna dashed curves recovered signal gt coefficient vector x determined 25 03 kax gamma 26 observed actual least squares b dashed curve recovered signal gt coefficient vector xls determined least squares solution 27 solved algorithm 41 note figure 4 signal recovery solid curve true signal gt circles observed signal sampled discrete times dashed curves recovered signals number nonzeros k solution x average observed true figure 5 comparison plna 30 llna 31 combinatorial search 32 average 2 plna 4 llna average kax c gamma bk 1 combinatorial solution x c number nonzeros k solution x average distance true xx figure comparison plna 30 llna 31 combinatorial search 32 average 2 plna 4 llna true solution x ax observed actual plna llna dashed curves recovered signal gt coefficient vector x determined 25 03 kax gamma llna combinatorial observed actual b dashed curve recovered signal gt coefficient vector xc determined combinatorial search figure 7 signal recovery solid curve true signal gt circles observed signal sampled discrete times dashed curves recovered signals recovered llna 26 08 figure 7b displays true signal observed signal signal recovered combinatorial search solution x c 32 2 43 observations make following observations respect comparison plna llna solutions least squares solutions 1 values 005 tested average observed system training set residual kaxls gamma strictly less average kax llna least squares algorithm 41 solving 27 produced better solutions observed system figure 1 however 2 values 2 001 090 tested plna average true system testing set residual strictly less average kaxls gamma bk 1 indicating plna produced better solutions true system comparison least squares values tested average true system residual solutions determined llna also strictly less corresponding least squares true system residuals see figure 2 plna average 22 nonzero terms achieved error reduction 3885 corresponding error obtained least squares solution llna produced average 1norm true system residual 5298 less least squares residual 3 values 01 tested average determined plna llna 2 orders magnitude less average kxls gamma x k hence plna llna solutions closer recovering true signal gt 18 see figure 3 4 figure 4 shows significant comparison plna llna least squares much accurate recovery true signal plna llna least squares note following respect comparison plna llna solutions solutions obtained combinatorial search 1 7 average plna kax gamma bk 1 strictly less average kax c gamma average plna kax gamma bk 1 less equal 1634 times average kax c gamma bk 1 7 average llna kax gamma bk 1 strictly less corresponding average true system residual combinatorial solutions 4 average llna kax gamma bk 1 less equal 1114 times corresponding average kax c gamma bk see figure 5 2 k 3 average kx gamma x k 1 plna llna strictly less average orders magnitude equal average kx c gamma x k 1 see figure 6 3 minimum true system 1norm residual 53867 occurs solution obtained combinatorial search true system residual plna true system residual llna 60022 note computing plna llna solutions 2 first value found bisection search solution 2 nonzero elements chosen fact accounts discrepancy true system residuals figure 5 figure 2 4 figure 7 shows recovery true signal plna llna good even better recovered signal lengthy combinatorial search time needed approach compute solution determined performing single run sun sparcstation 20 96 megabytes memory running matlab 51 using commands tic toc 21 linear programs solved cplex 7 interfaced matlab solving plna problem seconds solving llna problem seconds determining least squares solution algorithm 41 seconds determining solution combinatorial search seconds solutions computed plna llna superior least comparable obtained combinatorial search 32 yet needing two orders magnitude less time compute 5 conclusion theoretically justifiable fast finite algorithm proposed solving linear systems corrupted noise errors measurement parsimonious approach plna attempts set zero many components solution vector possible minimizing residual error corrupted system whereas least norm approach llna minimizes norm solution well residual numerical evidence indicates two approaches lead solutions many zero components solutions may closer orders magnitude solution underlying uncorrupted system solutions corrupted system obtained either least squares even timeconsuming combinatorial search solution minimal number nonzero components interesting note parametricly minimizing norm solution leads also suppression components conversely parametrically suppressing components solution also leads solution reduced norm importantly plna llna recover much accurate signal obtained least squares much faster obtained lengthy combinatorial search acknowledgement indebted referees constructive comments led considerable improvements particular indebted referee suggested inclusion llna approach comparative tests r constrained total least squares technique application harmonic superposition approximability minimizing nonzero variables unsatisfied relations linear systems support vector machine approach decision trees occams razor clustering via concave minimization feature selection via mathematical programming cplex optimization inc theory vector optimization algorithm quadratic programming least square estimation applications digital signal processing analysis total least squares problem fundamentals artificial neural networks formulation solution structured total least norm problems parameter estimation total least squares problem solving least squares problems optimal brain damage misclassification minimization machine learning via polyhedral concave minimization mathematical programming data mining solution general linear complementarity problems via nondifferentiable concave minimization mathworks introduction optimization convex analysis total least norm formulation solution structured problems overfitting avoidance bias readings machine learning introduction linear algebra discrete random signals statistical signal processing nature statistical learning theory mathematics generalization tr ctr jinbo bi kristin bennett mark embrechts curt breneman minghu song dimensionality reduction via sparse support vector machines journal machine learning research 3 312003 glenn fung olvi l mangasarian data selection support vector machine classifiers proceedings sixth acm sigkdd international conference knowledge discovery data mining p6470 august 2023 2000 boston massachusetts united states glenn fung disputed federalist papers svm feature selection via concave minimization proceedings conference diversity computing p4246 october 1518 2003 atlanta georgia usa glenn fung olvi l mangasarian alexander j smola minimal kernel classifiers journal machine learning research 3 p303321 312003 gunnar rtsch ayhan demiriz kristin p bennett sparse regression ensembles infinite finite hypothesis spaces machine learning v48 n13 p189218 2002 p bradley l mangasarian r musicant optimization methods massive data sets handbook massive data sets kluwer academic publishers norwell 2002 gunnar rtsch sebastian mika bernhard schlkopf klausrobert mller constructing boosting algorithms svms application oneclass classification ieee transactions pattern analysis machine intelligence v24 n9 p11841199 september 2002