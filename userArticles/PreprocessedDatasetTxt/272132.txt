static parallelization loops recursions demonstrate approaches static parallelization loops recursions example polynomial product phrased loop nest polynomial product parallelized automatically applying spacetime mapping technique based linear algebra linear programming one choose parallel program optimal respect objective function like number execution steps processors channels etc howeverat bestlinear execution time complexity atained phrasing polynomial product divideandconquer recursion one obtain parallel program sublinear execution time case target program derived automatic search given program skeleton deduced sequence equational program transformations discuss use skeletons compare assess models loops divideandconquer resursions parallelized comment performance properties resulting parallel implementations b introduction give overview several approaches static parallelization loops recursions 1 pursue university passau emphasis paper divideand conquer recursions static parallelization following benefits 1 efficiency target code one avoids overhead caused discovering parallelism run time minimizes overhead caused administering parallelism run time 2 precise performance analysis question parallelism settled compile time one predict performance program accurately 3 optimizing compilation one compile specific parallel architectures one limitation static parallelization methods identify large amounts parallelism usually must exploit regular structure source program mainly 1 equate loops tail recursions done systolic design 28 36 structure concerns dependences program steps dependences impose execution order still source program adapted satisfy requirements parallelization method programmer need think parallelism may simply state hisher priorities resource consumption let method make choices illustrate static parallelization methods recursive programs example polynomial product proceed four steps sect 2 provide specification polynomial product specification executed dynamic parallelism drawback explicit control use resources sect 3 refine specification double loop nest additional dependences parallelize loop nest spacetime mapping method based polytope model 29 method searches automatically large number possible parallel implementations optimizing respect objective function like number execution steps processors communication channels etc sect 4 refine specification divideandconquer dc algorithm fewer dependences loop nest central section paper dc parallelization methods well understood nested loops thus one derives parallel implementations hand albeit formally equational reasoning however parallelization process problemindependent starting point program schema called skeleton 9 discuss two dc skeletons instantiated polynomial product parallelizations subsect 41 first skeleton callbalanced fixeddegree dc parallelize adapted spacetime mapping method based method nested loops 25 target parallel loop nest also represented spmd program subsect 42 second skeleton bit less general parallelized based algebraic properties constituents 20 used generate coarsergrained parallelism form spmd program paper mainly comparing evaluating references cited individual sections contain full details respective technical development comparison concerned models methods used parallelization asymptotic performance respective parallel implementations 2 polynomial product illustrating example product two polynomials b degree n specified quantifier notation dijkstra 14 let us name product polynomial c note specification prescribe particular order computation cumulative sums define coefficients product polynomial make specification executable without think simple switch syntax programming language haskell 40 yields c b haskell reduce sums total order given sequential semantics partial order given parallel semantics programmer pays benefit choose order lack control use resources computation main resources time length execution space number processors others number communication channels memory requirements etc 3 nested loop program 31 source program target program apply loop parallelization one must first impose total order reductions specification means adding dependences dependence graph specification choice order may influence potential parallelism one careful choose count subscript b automatic methods help exploring search space choice 3 another change make convert updates c singleassignment form gives rise doubly indexed variable c also automatic techniques kind conversion 15 leads following program elements contain final values coefficients product polynomial n dependence graph loop nest affine bounds index expressions forms polytope loop represents one dimension vertices graphs form partitioned temporal spatial components done linear algebra integer linear programming example choices fairly obvious consider fig 1 polynomial product upper left shows polytope integer lattice dot represents one loop step upper right dependence graph dependences c shown since c updated lines lower left represent time slices points line executed parallel choice minimal respect execution time note dependence arrows must covered lines lines lower right represent processors line contains sequence loop steps executed fixed processor lines may parallel temporal r r r r r r r r r r r r r r r r r r r r r r r r r source polytope data dependences temporal partitioning spatial partitioning figure 1 polytope partitionings 2deltan synchronous program seqfor 0 n parfor p n asynchronous program parfor p 0 2 n seqfor max0 minn p figure 2 target polytope target programs lines chosen cover dependences ie minimized number communication channels processor zero partitionings time space combined spacetime mapping affine transformation coordinate system axis exclusively devoted time ie represents sequential loop space ie represents parallel loop like adjusting polarizing filter source polytope make time space explicit fig 2 depicts target polytope two target programs derived choice order namely order loops nest outer loop time inner loop space synchronous program global clock typical simd style enforce clock tick need global synchronization step time loop order loops vice versa asynchronous program private clock processor typical spmd style need communication statements enforce data dependences chosen spatial partitioning communications requiredat expense twice number processors necessary programs code loop body source program indices change inverse spacetime mapping must applied 32 complexity considerations interesting performance criteriaat least ones want consider hereare execution time number processors overall cost cost defined product execution time number processors one interested cost maintenance ie property parallelization increase cost complexity program polytope model best time complexity one achieve linearity problem size 2 least one loop must enumerate time ie sequential pure version model one usually get away one sequential loop 5 remaining loops enumerate space ie parallel requires polynomial amount processors since loops bounds affine expressions cost affected parallelization one simply trades time space product time space stays 33 evaluation let us sum essential properties polytope model purpose comparison 1 dependence graph embedded higherdimensional space dimensionality fixed equals number loops loop nest 2 extent dependence graph dimension usually variable depends problem size however important property polytope model complexity optimizing search independent problem size 3 vertex dependence graph represents roughly amount work precisely put constant bound amount work performed one vertex 2 exception trivial case dependences iterations executed one common step 4 large choice problemdependent affine dependences thus given loop nest individual dependences automatic optimizing search performed maximize parallelism 5 one save processors trading one spatial dimension time ie emulating set processors single processor 6 end result nest sequential parallel loops affine bounds dependences 7 one loop sequential rest parallel always achieved 5 one obtains execution time linear problem size save processors one trade space time price increased execution time complexity particular one make number processors independent problem size partitioning resulting processor array fixedsize tiles 13 39 4 divideandconquer algorithm rather enforcing total order cumulative summation specification 2 coefficients product polynomial accumulate summands dc algorithm exploiting associativity polynomial addition phi left side following write explicitly showing degrees variable polynomials let mn div 2 assume rest paper simplicity n power 2 suffix l stands lower part h higher part polynomial b input polynomials dependence graph algorithm depicted fig 3 c e f resulting polynomials four subproblems starting point parallelization give us sublinear execution time fundamental difference parallelization dc opposed nested loops choice dependences dependence graph always call graph since problemdependent dependences need automatic parallelization based instead take skeleton approach 9 provide program schema socalled algorithmic skeleton dc filled program pieceswe call customizing functionsin order obtain dc ap plication task offer dc skeleton one several highquality parallel implementations call architectural skeletons may involve search search space problemindependent thus need redone every application user application end challenge remains cast problem form algorithmic skeleton alternatively user might develop architectural skeleton even better performance exploiting problemspecific properties hisher application combine divide recursion ah bh ah bl al bh al ch cl dh dl eh el fh fl ch cl fh fl bl dh dl el eh al bh bl figure 3 call graph dc polynomial product research parallelization dc earlier stage nested loops many different algorithmic skeletonsand even architectural skeletonscan envi sioned common yardstick evaluate found yet discuss two algorithmic skeletons respective approaches parallelization 41 spacetime mapping dc 411 source program target program call graph fig 3 matches algorithmic skeleton callbalanced fixeddegree dc developed skeleton phrased higherorder function haskell 25 divcon divcon k basic divide solve indata length indata j 1 map basic indata else let solve transpose x l map fst l map snd l map combine else error list length right let us comment functions see paper map op xs applies unary operator op list xs values returns list results map one main source parallelism higherorder programs xsys returns concatenation list xs list ys zip xs ys zips lists xs ys list corresponding pairs elements details body divcon irrelevant points want make paperand indeed irrelevant user skeleton matters higherorder specification specifies generalized version schema depicted fig 3 whose parameters k basic divide combine caller fills division degree appropriate functions computing basic case dividing combining recursive case example shows using names fig 3 express polynomial product terms divcon skeleton polynomials multiplied represented lists coefficients order x preadapt x postadapt fst z map snd z divide ah bh al combine ch cl dh dl eh el fh fl skeleton takes input delivers output lists size n operands result polynomial product formatted accordingly function preadapt zips input polynomials single list function postadapt unpacks zipped higher lower parts result given unlimited resources clear without search temporal spatial partitioning horizontal layer call graph one time slice seems suggest twodimensional geometrical model one temporal axis pointing one spatial axis pointing sideways however pays convert call graph higherdimensional structure reason vertices graph represent grossly unequal amounts work words amount work one vertex cannot capped constant binary division data node fixed layer divide phase represents double amount work node layer reverse applies combine phase behaviour holds algorithms fit skeleton obtain graph work node represents bounded constant split node works aggregate data set nodes works atomic data fragmentation nodes spread across additional dimensions yielding higherdimensional graph fig 4 time points depth given size call graph time slice twodimensional onedimensional increasing problem size spatial dimensions added parallel loop program scans graph similar manner would scan polytope derived 25 r number recursive calls log n elements reading input data divide dim 0 solve basic cases divide dim 1 combine dim 0 result depth dimension combine dim 1 figure 4 higherdimensional call graph dc polynomial product ab pairs input coefficients elements c pairs output coefficients higher lower result polynomial seqfor r parfor parfor seqfor parfor program consists sequence three loop nests divide sequential combine phase loops enumerate levels graph therefore sequential whereas loops q parallel enumerate spatial dimensions spatial dimensions indexed digits q radix k representation allows us describe iterations across arbitrary number dimensions single loop makes text program independent problem size q k denotes vector digits q radix k q k selects dth digit accesses values points whose index differs dimension use notation q k number one obtains q replacing dth digit representation differs target programs polytope model dimension corresponds separate loop data indexed time component space component q divide combine functions given actual coordinate parameter order select appropriate functionality particular subproblem divide resp data partition combine loop programs also derived formally equational reasoning 26 program dataparallel therefore implemented directly simd chines additionally program transformed easily spmd program parallel machines distributed memory using message passing twodimensional arrays become onedimensional spacecomponent projected selecting particular processor alltoall communications restricted groups k processors processor q executes following seqfor r alltoall list values received alltoall seqfor alltoall list values received alltoall one interested transforming computation domain fig 4 two ways 1 loop parallelization approach potential finegrained paral lelism loop parallelization spatial dimensions moved time save processors urgent since demand processors grows faster increasing problem size 2 spatial part computation domain remains higher dimensionality dimensionality reduced depicted fig 5 done eg target processor topology mesh works extent dimension fixed x x z figure 5 dimensionality reduction computation domain 412 complexity considerations implementation efficient assign basic problem separate physical processor instead spatial dimensions mapped time result slightly modified spmd program brief operations single elements replaced operations segments data let n size polynomials p number processors basic phase segments work equally distributed among processors ie processor responsible n2 log p log elements time computing basic phase segments therefore 2 p computation divide combine phase takes olog n steps step segment size n p divided combined parallel entire computation time phases therefore olog n deltan dimensionality target mesh equals number dimensions mapped space total time execution time sublinear number processors asymptotically greater problem size maintains cost 2 number processors asymptotically greater n best execution time achieved cost maintenance olog 2 n dimensionality target topology taken account calculations revealed following 1 sublinear execution time achieved dimensionality least 3 2 computation sublinear costmaintaining cubic threedimensional mesh number processors asymptotically n n 12 algorithm polynomial product sequential time complexity log 3 socalled karatsuba algorithm 1 sect 26 division degree 3 expressed skeleton whose parallel implementation 25 algorithm cost maintaining reasonable problem sizes experiments shown sequential version karatsuba algorithm beats sequential version conventional algorithm polynomials size least 16 since subproblems slightly computationintensive parallel version karatsuba algorithm skeleton bit slower parallel version conventional algorithm one saves processors precisely 413 evaluation properties spacetime mapping model compared polytope model previous section 1 dimensionality call graph variable equals number layers divide phase depends problem size 2 extent dependence graph spatial dimension fixed degree problem division 3 vertex call graph represents amount work 4 choice dependences search parallelism necessary 5 variety parallelism given option trade spatial dimensions time 6 end result nest sequential parallel loops 7 upper bound temporal loop logarithmic problem size upper bound spatial loop exponential upper bound temporal loop ie polynomial problem size looking computation domain fig 4 extent spatial dimension constant number spatial dimensions grows problem size sublinear execution time root problem size possible mesh topologies conditions maintaining costoptimality case restrictive 42 homomorphisms simple dc skeleton homomorphism capture dc situations defined often lists 7 37 although also defined data structures eg trees 17 arrays 33 421 source program target program unary function h list homomorphism 7 iff value concatenation two lists computed combining values h two parts operation fi significance homomorphisms parallelization given promotion property version follows red dist 6 equality also proved equational reasoning literature one used bird meertens formalism bmf 7 equational theory functional programs red map basic functions lists red reduces list values binary operator case inherits associativity list concatenation returns result value map seen previous subsection red map high potential parallelism red performed time logarithmic length list map performed constant time given many processors list elements third function appearing promotion property dist distribute partitions argument list list sublists right inverse reduction concatenation red equality 6 reveals every homomorphism h computed three stages 1 input list distributed 2 function h computed sublists independently 3 results combined operator fi efficiency parallel implementation depends largely form operation fi eg specialization homomorphism skeleton called dh distributable homomorphism family practical efficient implementations exists 19 21 similarity 3 5 obvious h polynomial product fi operation fi polynomial addition phi however two mismatches 1 operations fi phi defined polynomials possibly different degree thus list representation polynomial needs refined pair int list int power polynomial list list coefficients simplicity ignore subtlety remainder paper 2 serious departure given homomorphism skeleton polynomial product fi equivalent h requires two arguments one match skeleton might give h list coefficient pairs destroys homomorphism property format provided h product two polynomials cannot constructed products polynomials halves well cannot fit binary polynomial product unary homomorphism skeleton second argument gives us additional dimension parallelism unary homomorphism cannot offer two arguments divided obtain four partial results combined rather two prescribed skeleton exploit quadratic parallelism use different binary homomorphism skeleton polynomial product fits perfectly resulting promoted twodimensional skeleton everything twiceonce dimension dividing dist computing parallel map combining red write map 2 f map map f zip 2 dist theta map copy ffi dist 8 complication distribute due fact list portions must spread across two dimensions note also provided definition twodimensional reduction red 2 fi computed two orders rowmajor columnmajor actually two choices leads equal amount parallelism however problemspecific optimization combine stage even bet ter fig 6 depicts optimized solution virtual square processors make assumptions physical topology exploiting commutativity customizing operator phi reduce first along diagonalsthe corresponding polynomials equal powerand reduce partial results located northern eastern bor ders latter step improved pairwise exchange neighbouring processors allow result polynomial product distributed blockwise along border processors threestage format promoted homomorphism skeleton suggests spmd program also three stages binary homomorphism optimized polynomial product every processor q executes following mpilike program distribute compute combine figure three stages twodimensional promoted homomorphism skeleton broadcast row broadcast b column reduce phi diagonalq exchangeneighbours program show loops explicitlybut course outer spatial loops implicit spmd model inner sequential loop hidden call function polyprod sequential implementation fi mpilike communications spmd program could point programmer stops refinement machine vendor takes alternatively user himherself program broadcasts reductions exchanges neighbours define suitable physical processor topology eg mesh trees 18 422 complexity considerations consider multiplying two polynomials degree n virtual square p 2 processors time complexity pipelined broadcasting reduction 18 deltam n p mn p1 value p chosen 1 n p n log n ologn yields optimal logarithmic time complexity processors cost 2 thus maintained interesting cases parallelization worsened sequential cost olog n n 2 processors solution yields optimal time clearly costmaintaining log n costmaintaining solution olog 2 n n log n 2 processors n n processors equal systolic solution sect 3 cost maintaining practice processor number arbitrary fixed value problem size n relatively large term np 2 dominates expression time complexity guarantees socalled scaled linear speedup 35 term improved onp log 3 onpdelta lognp applying karatsuba fftbased algorithm respectively processors compute stage whether karatsuba algorithm phrased homomorphism open question 423 evaluation let us review parallelization process homomorphism approach actually different skeleton approach previous section one uses different skeleton led parallelization algebra rather geometry 1 homomorphism skeleton restrictive haskell skeleton previous section also restrictive earliest dc skeleton 34 corresponds postmorphism see 21 classification dc skeletons strength homomorphism direct correspondence straightforward threestage spmd program polynomial product yields parallel implementation timeoptimal costmaintaining time optimality cannot achieved mesh topology constant dimensional ity homomorphism approach obtain topologyindependent program mpilike communication primitives best implementations primitives topologies hypercube mesh trees timeoptimal 27 2 many skeletons homomorphism skeleton come many different varieties unary binary ternary operations lists data structures present state understanding versions developed separately 3 even skeletons available user adaptation problem remains true previous approaches well eg loop parallelization dependence satisfy restrictions model replaced set dependences 29 previous subsection format input output polynomial product adaptation functions make match haskell skeleton homomorphic form problem may exist immediately clear example scan 20 algorithms turned dc homomorphic form aid auxiliary functions 10 38 4 application promotion property gives us parametrized granularity parallelism controlled size chunks distribution function dist splits list depending available number processors input data distributed coarsely finely single list element per processor requirement number processors case twodimensional homomorphism square restrictive skeleton previous subsection power division degree k required moreover homomorphic solution restricted polynomials whose length power 2 5 note promotion property applied onceas opposed previous subsection parallelize level call graph decreases amount necessary communication number parallelized levels depends choice granularity remaining levels captured call sequential implementation polyprod h2 enables additional optimization processors call optimal sequential algorithm given problem rather algorithm chosen parallelization case polyprod efficient karatsuba fftbased algorithm polynomial product summary let us summarize present state art static parallelization loops recur sions illustrated polynomial product static parallelization works best programs exhibit regular structure structural requirements captured restrictions form program many applications satisfy restrictions immediately thus static parallelization definitely dusty decks however many algorithms put required form parallelized partic ular certain computationintensive application domains like image signal text speech processing numerical graph algorithms candidates static parallelization dense data structures hold promise regular dependences sparse data structures might also amenable processing loop nests less regular forms parallel dc 51 loop parallelization loop parallelization much better understood parallelization divideandconquer polytope model extended significantly recently 1 dependences spacetime mappings may piecewise affine number affine pieces must constant ie independent problem size 16 2 loop nests may imperfect ie computations must innermost loop 16 3 upper loop bounds may arbitary indeed unknown compile time 23 consequence 3 loops handled 12 22 30 entails serious departure polytope model spacetime mapping loops becoming viable component parallelizing compilation 31 loop parallelizers based polytope model include bouclettes 8 loopo 24 opera 32 feautriers paf pips 2 however recent sophisticated techniques spacetime mapping yet filtered commercial compilers particular automatic methods partitioning projecting ie trading space time need carried code generation stage large academic parallelizing compilation projects involve also loop parallelization techniques phrased even based polytope model links provided web pages cited good unhurried introduction loop parallelization emphasis polytope model book series banerjee 4 5 6 52 divideandconquer parallelization parallelization dc yet unified model different choices parallelization evaluated common yardstick compared empirical approach taken presently uses skeletons algorithm patterns high potential parallelism linked semantically equivalent architectural patterns provide efficient implementations algorithms available parallel machines approach makes fewer demands compiler technology however expects support systems programming community provides architectural skeletons existing parallel machines application programmer simply look schema given skeleton library adapt hisher application schema last couple years development study skeletons received increasing amount attention research community forming 11 skeleton approach become viable paradigm parallel programming 1 parallel programming community manages agree set algorithmic skeletons capture large number applications relatively easy fill 2 parallel machine vendors community application sector supporting succeeds providing efficient implementations skeletons products one attempt classify best parallel implementations skeleton represents popular programming paradigm tabulating special cases done paradigm linear recursion 41 interesting special cases copy red scan compositions cases optimized 53 conclusions ultimately one wait see whether static dynamic approach parallelism win upper hand since parallelism stands performance lack overhead precision performance analysis two things favor static opposed dynamic parallelismfor problems lend static parallelization acknowledgements work received financial support dfg projects recur recur2 daad arc procope exchange programs thanks jf collard reading comments parsytec gcel 1024 paderborn center parallel computing used performance measurements r design analysis computer algorithms pips framework building interprocedural compilers efficient exploration nonuniform spacetime transformations optimal systolic array synthesis loop transformations restructuring compilers foundations loop parallelization dependence analysis lectures constructive functional programming reference manual bouclettes parallelizer algorithmic skeletons structured management parallel computation parallel programming list homomorphisms theory practice higherorder parallel programming automatic parallelization whileloops using speculative execution regular partitioning synthesizing fixedsize systolic arrays predicate calculus program semantics array expansion automatic parallelization polytope model upwards downwards accumulations trees transformations methodology parallel program development case study systematic efficient parallelization scan list homomorphisms systematic extraction implementation divideandconquer paral lelism formal derivation divideandconquer programs case study multidimensional ffts mechanical parallelization loop nests containing loops classifying loops spacetime mapping loop parallelizer loopo spacetime mapping class divideand conquer recursions parallelization divideandconquer translation nested loops introduction parallel computing design analysis algorithms view systolic design loop parallelization polytope model parallelization loop nests containing loops loop parallelization opera toolbox loop parallelization constructive theory multidimensional arrays algebraic model divideandconquer algorithms parallelism parallel computing systolic algorithms architectures foundations parallel programming applications strategy designing divideandconquer algorithms control generation design processor arrays parallel implementations combinations broadcast tr applications strategy designing divideandconquer algorithms algorithms array expansion scans primitive parallel operations predicate calculus program semantics control generation design processor arrays regular partitioning synthesizing fixedsize systolic arrays algorithmic skeletons introduction parallel computing automatic parallelization italicwhileitalicloops using speculative execution foundations parallel programming transformations methodology parallel program development haskell dependence analysis loop parallelization loop transformations restructuring compilers design analysis computer algorithms systematic extraction implementation divideandconquer parallelism transformation divide myampersandamp conquer nested parallel loops classifying loops spacetime mapping systematic efficient parallelization scan list homomorphisms loop parallelization polytope model automatic parallelization polytope model upwards downwards accumulations trees opera parallelization loop nests containing loops parallel implementations combinations broadcast reduction scan