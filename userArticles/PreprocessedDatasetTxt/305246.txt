parallel hierarchical solvers preconditioners boundary element methods method moments important tool solving boundary integral equations arising variety applications transforms physical problem dense linear system due large number variables associated computational requirements systems solved iteratively using methods gmres cg variants core operation iterative solvers application system matrix vector requires thetan2 operations memory using accurate dense methods computational complexity reduced log n memory requirement thetan using hierarchical approximation techniques algorithmic speedup approximation combined parallelism yield fast dense solvers paper present efficient parallel formulations dense iterative solvers based hierarchical approximations solving potential integral equations first kind study impact various parameters accuracy performance parallel solver demonstrate parallel formulation incurs minimal parallel processing overhead scales large number processors present two preconditioning techniques accelerating convergence iterative solver techniques based innerouter scheme blockdiagonal scheme based truncated greens function present detailed experimental results 256 processors cray t3d code achieves raw computational speeds 5 gflops compared accurate solver corresponds speed approximately 776 gflops b introduction method moments 12 popular method solving integral equations extensive applications computational electromagnetics wave propagation heat transfer 22 21 3 11 transforms physical problem defined integral equation dense linear system integral equation termed volume boundary integral equation depending whether variables defined volume surface modeled object paper address solution boundary integral equations complex 3d objects modeling arbitrarily complex 3d objects may require large number boundary el ements objects boundary element method results dense linear systems hundreds thousands unknowns memory computational requirements solving systems formidable iterative solution techniques generalized minimal residual gmres 18 method choice memory computational requirements solvers grow thetan 2 per iteration solving systems 10k variables manner challenge current supercomputers memory requirements methods reduced forming coefficient matrix ex plicitly addition hierarchical algorithms fast multipole method fmm related particle dynamics methods allow us reduce computational complexity iteration approximate hierarchical techniques received lot attention context particle simulations given system n particles particle influences every particle system total n 2 interactions must computed however physical systems influence particle another diminishes distance systems possible aggregate single expression impact several particles another distant particle using approach total number interactions system reduced significantly forms basis hierarchical methods methods provide systematic ways aggregating entities computing interactions controlling overall error modeling algorithms based hierarchical techniques include barneshut 2 fast multipole 10 appels 1 algorithms approximating long range interactions manner reduces sequential complexity typical simulations involving n particles 2 log n clearly reduced computational complexity hierarchical methods represents significant reduction time solving system however modeling hundreds thousands boundary elements still take inordinately large amount time conventional serial computers parallel processing offers tool effectively speeding computation enables us solve problems large number elements increase accuracy simulation incorporating higher precision approximate hierarchical matvec parallel formulations hierarchical methods involve partitioning domain among various processors combined objectives optimizing communication balancing load particle densities uniform across domain objectives easily met 4 25 13 19 9 irregular distributions objectives hard achieve highly unstructured nature computation commu nication singh et al 20 warren salmon 24 23 presented schemes irregular distributions try meet objectives 6 8 5 presented alternate schemes irregular distributions improve performance earlier schemes 7 5 used parallel hierarchical techniques computing dense matrixvector products studied impact various parameters accuracy performance important aspect using iterative solvers solving large systems use effective preconditioning techniques accelerating convergence use hierarchical methods computing matrixvector products parallel processing significant implications choice preconditioners since system matrix never explicitly constructed preconditioners must derived hierarchical domain representation furthermore preconditioning strategies must highly parallelizable since early work rokhlin16 relatively little work done dense hierarchical solvers even serial context 14 17 22 3 paper investigate accuracy convergence gmres solver built around parallel hierarchical matrixvector product investigate impact various parameters accuracy performance propose two preconditioning strategies accelerating convergence solver preconditioners based innerouter scheme truncated greens function demonstrate excellent parallel efficiency performance solver 256 processor cray t3d rest paper organized follows section 2 presents brief overview hierarchical methods use solving integral equations section 3 describes parallel formulations hierarchical methods section 4 describes preconditioning techniques section 5 presents experimental results cray t3d section 6 draws conclusions outlines ongoing research hierarchical methods solving integral equations boundary element methods bem solve integral equations using potential theory methods discretize boundary domain panels using associated greens function potential panel represented sum contributions every panel applying dirichlet boundary conditions yields large scale linear system equations n basis boundary discretization n theta n linear system arising approach dense iterative solution system requires application system matrix vector iteration process facilitated fact coupling coefficient two boundary elements greens function integral equation diminishing function distance r elements instance laplace equation greens function 1r three dimensions logr two dimensions functions decreasing functions distance r allows us aggregate impact several boundary elements single expression apply constant time similar principle single iteration nbody algorithm5 integrals boundary elements performed using gaussian quadrature nearby elements higher number gauss points used desired accuracy computing coupling coefficients distant basis functions fewer gauss points may used simplest scenario far field evaluated using single gauss point assuming triangular surface elements process involves computing mean basis functions triangle scaling area triangle computing matrixvector product manner involves following steps 1 construct hierarchical representation domain particle simulation method particles injected empty domain every time number particles subdomain exceeds preset constant partitioned eight octs manner oct tree structure computed boundary element method element centers correspond particle coordinates octtree therefore constructed based element centers node tree stores extremities along x z dimensions subdomain corresponding node 2 number particles tree corresponding boundary element method equal product number boundary elements number gauss points far field case single gauss point far field multipole expansions computed center triangle particle coordinate mean basis functions scaled triangle area charge addition single gauss point code also supports three gauss points far field 3 computing matrixvector product need compute potential n basis functions done using variant barneshut method hierarchical tree traversed boundary elements boundary element falls within near field observation element integration performed using direct gaussian quadrature code provides support integrations using 3 13 gauss points near field invoked based distance source observation elements contribution basis functions observation element accrued farfield contributions computed using multipole expansions ff criterion barneshut method slightly modified size subdomain defined extremities boundary elements corresponding node tree unlike original barneshut method uses size oct computing ff criterion 3 parallel gmres using hierarchical matrixvector products implement parallel formulation restart gmres 18 algorithm critical components algorithm product system matrix vector x n dot products vectors distributed across processors first np elements vector going processor p 0 next np processor p 1 matrixvector product computed using parallel hierarchical treecode parallel treecode comprises two major steps tree construction hierarchical representation domain tree traversal starting distribution panels processors processor constructs local tree set nodes highest level tree describing exclusive subdomains assigned processors referred branch nodes processors communicate branch nodes tree form globally consistent image tree processor proceeds compute potential panels assigned traversing tree encountering node locally available two possible scenarios panel coordinates communicated remote processor evaluates interaction node communicated requesting processor refer former function shipping latter data shipping parallel formulations based function shipping paradigm discuss advantages function shipping 5 7 loadbalancing technique efficient implementation costzones scheme messagepassing computers node tree contains variable stores number boundary elements interacted computing previous matvec computing first matvec variable summed along tree value load node stores number interactions nodes rooted subtree load balanced inorder traversal tree assigning equal load processor figure 1 illustrates parallel formulation barneshut method since discretization assumed static load needs balanced parallel formulation assigns boundary elements associated basis func tions processors two implications multiple processors may contributing element matrixvector product mapping basis functions processors may match partitioning assumed gmres algorithm problems solved hashing vector elements processor designated gmres partitioning destination processor job accruing vector elements adding necessary communication performed using single alltoall personalized communication variable message sizes15 alltoall broadcast insert branch nodes recompute top part traverse local tree needed insert remote processor buffer send buffer corresponding broadcast branch nodes processors full messages process force computation tree construction aggregate loads local tree periodically check pending branch nodes broadcast loads nodes local tree aggregate toplevel loads root node processor total load w within processors domain locate nodes correspond load wp 2wp left determine destination point communicate points using alltoall personalized communication insert loads branch assume initial particle distribution construct local trees branch nodes balance load move particles b balancing load communicating particles particle schematic parallel algorithm figure 1 schematic parallel treecode formulation load balancing technique preconditioning techniques iterative solver section present preconditioning techniques iterative solver since coefficient matrix never explicitly computed preconditioners must constructed hierarchical representation domain limited explicit representation coefficient matrix forms basis two preconditioners 41 innerouter schemes hierarchical representation domain provides us convenient approximation coefficient matrix increasing accuracy matrixvector product increases number direct interactions thus runtime conversely reducing accuracy reduces runtime therefore possible visualize two level scheme outer solve desired accuracy preconditioned inner solve based lower resolution matrixvector product accuracy inner solve controlled ff criterion matrixvector product multipole degree since top nodes tree available processors matrixvector products require relatively little communication degree diagonal dominance determines method controlling accuracy coefficient matrix highly diagonally dominant case many applications high value ff desirable ensures minimum communication overheads however matrix diagonally dominant desirable use lower values ff correspondingly lower values multipole degrees fact possible improve accuracy inner solve increasing multipole degree reducing value ff inner solve solution converges used flexible preconditioning gmres solver however paper present preconditioning results constant resolution inner solve 42 truncated greens function primary drawback two level scheme inner iteration still poorly condi tioned diagonal dominance many problems allows us approximate system truncating greens function leaf node hierarchical tree coefficient matrix explicitly constructed assuming truncated greens function done using criteria similar ff criterion barneshut method follows let constant fi define truncated spread greens function boundary element traverse barneshut tree applying multipole acceptance criteria constant fi nodes tree using determine near field boundary element corresponding constant fi construct coefficient matrix 0 corresponding near field preconditioner computed direct inversion matrix 0 approximate solve basis functions computed dotproduct specific rows corresponding basis functions near field elements number elements near field controlled preset constant k closest k elements near field used computing inverse number elements near field less k corresponding matrix assumed smaller easy see preconditioning strategy variant block diagonal preconditioner simplification scheme derived follows assume leaf node barneshut tree hold elements coefficient matrix corresponding elements explicitly computed inverse matrix used precondition solve performance preconditioner however expected worse general scheme described hand computing preconditioner require communication since data corresponding node locally available paper reports performance general preconditioning technique based truncated greens function simplification 5 experimental results objectives experimental study follows ffl study error parallel performance iterative solvers based hierarchical matrixvector products ffl study impact ff criterion multipole degree accuracy performance solver ffl study impact number gauss points far field performance ffl study preconditioning effect iteration count solution time preconditioners impact parallel performance section report performance gmres solver preconditioning techniques cray t3d 256 processors variety test cases highly irregular geometries used evaluate performance solver preconditioner tested sphere 24k unknowns bent plate 105k unknowns experimental results organized three categories performance raw parallel efficiency solver accuracy stability solver preconditioning techniques problem runtime eff mflops runtime eff mflops pscan 374 093 1352 100 087 5056 28060 053 089 1293 016 075 4357 table 1 runtimes seconds efficiency computation rates t3d different problems 51 performance matrixvector product computation intensive part gmres method application coefficient matrix vector remaining dot products computations take negligible amount time therefore raw computation speed matvec good approximation overall speed solver two important aspects performance raw computation speed terms flop count parallel efficiency addition since hierarchical methods result significant savings computation larger problems useful determine computational speed dense solver using hierarchical metvec required solve problem time present parallel runtime raw computation speed efficiency four different problem instances impossible run instances single processor memory requirements therefore use force evaluation rates serial parallel versions compute efficiency compute mflop ratings code count number floating point operations inside force computation routine applying mac internal nodes using number macs force computations determine total number floating point operations executed code divided total time obtain mflop rating code table 1 presents runtimes efficiencies computation rates four problems value ff parameter cases 07 degree multipole expansion 9 efficiencies computed determining sequential time mac force computation sequential times larger problem instances projected using values efficiencies computed code achieves peak performance 5 gflops although may appear high must noted code little structure data access resulting poor cache performance furthermore divide squareroot instructions take significantly larger number processor cycles hand performance achieved hierarchical code corresponds 770 gflops dense matrixvector product clearly loss accuracy acceptable application use hierarchical methods results two orders magnitude improvement performance combined speedup 200 256 processors parallel treecode provides powerful tool solving large dense systems loss parallel efficiency results communication overheads residual load imbalances also exist minor variations raw computation rates across different problem instances identical runtimes different percentages mac computations near field interactions farfield interactions computed instances farfield interactions computed using particleseries interac tions involves evaluating complex polynomial length 2 degree multipole series computation good locality properties yields good flop counts conventional risc processors alpha contrast nearfield interactions mac computations exhibit good data locality involve divide square root instructions results varying raw computation speeds across problem instances detailed studies impact various parameters accuracy matrixvector product presented authors 5 7 52 parallel performance unpreconditioned gmres solver one important metrics performance code time solution investigate solution time different number processors different accuracy parameters objectives follows show speedup fast matvecs translates scalable solution times large number processors study impact ff criterion multipole degree solution times case assume desired solution reached residual norm reduced factor 10 gamma5 choice reduction residual norm lower accuracy matvecs may become unstable beyond point first set experiments study impact ff criterion solution time degree multipole expansion fixed 7 parallel runtimes reduce residual norm factor 10 gamma5 noted times presented table 2 capped 3600 seconds therefore one missing entry table number useful inferences drawn table ffl cases relative speedup 8 64 processors around 6 corresponds relative efficiency 74 demonstrates parallel solver highly scalable ffl given number processors multipole degree increasing accuracy matvec reducing ff results higher solution times lower efficiencies former increasing number interactions computed near field resulting higher computational load loss efficiency processors 8 64 8 64 table 2 time reduce relative residual norm 10 gamma5 degree multipole expansion fixed 7 times seconds processors 8 64 8 64 degree 5 2692 471 20103 3296 6 3823 652 27296 4412 table 3 time reduce relative residual norm 10 gamma5 value ff fixed 0667 times seconds increase communication overhead increasing number interactions need performed lower tree since locally available element coordinates need communicated processors farther away consistent observations computing matvec study impact increasing multipole degree solution time parallel performance value ff fixed 0667 multipole degree varied 5 7 table 3 records solution time reducing residual norm factor 10 gamma5 8 64 processors expected increasing multipole degree results increasing solution times modulo parallel processing overheads serial computation increases square multipole degree since communication overhead high trend visible parallel runtimes also increasing multipole degree also results better parallel efficiencies raw computational speeds communication overhead remains constant computation increases furthermore longer polynomial evaluations conducive cache performance table leads us believe desirable accuracy point identified better use higher degree multipoles opposed tighter ff criterion achieve accuracy approx accur figure 2 relative residual norm accurate approximate iterative schemes 53 accuracy gmres solver use approximate hierarchical matvecs several implications iterative solver important course error solution often possible compute accurate solution due excessive memory computational requirements therefore difficult compute error solution however norm axgammab good measure close current solution desired solution unfortunately possible compute since never explicitly assembled compute corresponds approximate matvec value matches ax closely say measure confidence approximate solution mathes real solution examine norm vector iterations study stability unpreconditioned gmres iterations 531 convergence accuracy iterative solver section demonstrate possible get nearaccurate convergence significant savings computation time using hierarchical methods fix value ff multipole degree compare reduction error norm iteration table 4 presents log relative residual norm gmres various degrees approximation executed 64 processor t3d following inferences drawn experimental data ffl iterative methods based hierarchical matvecs stable beyond residual norm reduction 10 gamma5 also illustrated figure 2 plots reduction residual norm iterations accurate worst case inaccurate matvec seen even worst case accuracy residual norms near agreement relative residual norm 10 gamma5 many problems accuracies adequate ffl increasing accuracy matvec results closer agreement accurate hierarchical solvers also accompanied increase solution time therefore desirable operate desired accuracy range ffl parallel runtime indicates hierarchical methods capable yielding significant savings time expense slight loss accuracy iter accurate time 12446 15619 9216 11202 table 4 convergence log 10 relative error norm runtime seconds gmres solver 64 processor cray t3d problem consists 24192 unknowns 532 impact number gauss points far field computing farfield interactions code allows flexibility using either 3point gaussian quadratures single point gaussian quadratures investigate impact computing farfield potentials using overall runtime error table 5 presents convergence solver two cases case value ff fixed 0667 multipole degree 7 near point interactions computed identical manner either case depending distances boundary elements code allows 3 13 point gaussian quadratures nearfield following inferences drawn experimental results ffl using larger number gauss points yields higher accuracy also requires computation consistent understanding fixed ff criterion computational complexity increases number gauss points far field ffl single gauss point integrations farfield extremely fast adequate approximate solutions iter gauss time 11202 689 table 5 convergence log 10 relative error norm runtime seconds gmres solver 64 processor cray t3d problem consists 24192 unknowns value ff 0667 multipole degree 7 5 unpreconditioned block diag innerouter 5 block diag innerouter unpreconditioned figure 3 relative residual norm accurate approximate iterative schemes 54 performance preconditioned gmres section examine effectiveness blockdiagonal innerouter preconditioning schemes fix value ff 05 multipole degree 7 effectiveness preconditioner judged number iterations computation time reduce residual norm fixed factor although certain preconditioners may yield excellent iteration counts may difficult compute vice versa third perhaps equally important aspect parallel processing overhead incurred preconditioners table 6 presents reduction error norm iterations unpreconditioned innerouter blockdiagonal preconditioning schemes figure 3 illustrates convergence two problems graphically easy see innerouter scheme converges small number outer iterations however runtime fact block diagonal scheme number inner iterations innerouter scheme relatively high drawback innerouter scheme since attempt improve conditioning inner solve currently investigating techniques solving hand since block diagonal matrix factored communication overhead high block diagonal preconditioner provides effective lightweight preconditioning technique reflected slightly higher iteration count lower solution times iter unprecon innerouter block diag unprecon innerouter block diag time 15619 12540 10361 70978 58477 51106 table relative error norm runtime seconds preconditioned gmres solver 64 processor cray t3d 6 concluding remarks paper presented dense iterative solver based approximate hierarchical matrixvector product using solver demonstrate possible solve large problems hundreds thousands unknowns extremely fast problems cannot even generated let alone solved using traditional methods memory computational requirements show possible achieve scalable high performance solver terms raw computation speeds parallel efficiency 256 processors cray t3d combined improvements use hierarchical techniques parallelism represents speedup four orders magnitude solution time reasonable sized problems also examine effect various accuracy parameters solution time parallel efficiency overall error presented two preconditioning techniques innerouter scheme blockdiagonal scheme evaluated performance preconditioners terms iteration counts solution time although innerouter scheme requires fewer iterations iteration inner solve may expensive hand due diagonal dominance many systems blockdiagonal scheme provides us effective lightweight preconditioner treecode developed highly modular nature provides general framework solving variety dense linear systems even serial context relatively little work done since initial work rokhlin16 prominent pieces work area include 14 17 22 3 best knowledge treecode presented paper among first parallel multilevel solverpreconditioner toolkit currently extending hierarchical solver scattering problems electromagnetics 17 16 22 21 3 freespace greens function field integral equation depends wave number incident radiation high wave numbers boundary discretizations must fine corresponds large number unknowns applications hierarchical methods particularly suitable desired level accuracy high r efficient program manybody simulation hierarchical log n force calculation algorithm guidelines using fast multipole method calculate rcs large objects accelerated molecular dynamics fast multipole algorithm efficient parallel formulations hierarchical methods applications scalable parallel formulations barneshut method nbody simulations parallel matrixvector product using hierarchical methods parallel version fast multipole method fast algorithm particle simulations field computation method moments matrix methods field problems mapping adaptive fast multipole algorithm mimd systems multipole accelerated preconditioned iterative methods threedimensional potential integral equations first kind rapid solution integral equations classical potential theory rapid solutions integral equations scattering theory two dimensions gmres generalized minimal residual algorithm solving nonsymmetrical linear systems implementing fast multipole method three dimen sions load balancing data locality hierarchical nbody methods fast multipole method solution using parametric geometry multilevel fast multipole algorithm solving combined field integral equation electromagnetic scattering astrophysical nbody simulations using hierarchical tree data structures parallel hashed oct tree nbody algorithm parallel multipole method connection machine tr ctr vivek sarin ananth grama ahmed sameh analyzing error bounds multipolebased treecodes proceedings 1998 acmieee conference supercomputing cdrom p112 november 0713 1998 san jose ca sreekanth r sambavaram vivek sarin ahmed sameh ananth grama multipolebased preconditioners large sparse linear systems parallel computing v29 n9 p12611273 september ananth grama vivek sarin impact farfield interactions performance multipolebased preconditioners sparse linear systems proceedings 18th annual international conference supercomputing june 26july 01 2004 malo france hariharan srinivas aluru balasubramaniam shanker scalable parallel fast multipole method analysis scattering perfect electrically conducting surfaces proceedings 2002 acmieee conference supercomputing p117 november 16 2002 baltimore maryland qian xi wang variable order revised binary treecode journal computational physics v200 n1 p192210 10 october 2004