parallel two level block ilu preconditioning techniques solving large sparse linear systems discuss issues related domain decomposition multilevel preconditioning techniques often employed solving large sparse linear systems parallel computations implement parallel preconditioner solving general sparse linear systems based two level block ilu factorization strategy give new data structures strategies construct local coefficient matrix local schur complement matrix processor preconditioner constructed fast robust solving certain large sparse matrices numerical experiments show domain based two level block ilu preconditioners robust efficient published ilu preconditioners based schur complement techniques parallel sparse matrix solutions b introduction high performance computing techniques including parallel distributing computa tions undergone gradual maturation process past two decades moving experimental laboratory studies many engineering scientific appli cations although shared memory parallel computers relatively easy program commonly used architecture parallel computing practices distributed technical report 30500 department computer science university kentucky lexington ky 2000 research supported part us national science foundation grants ccr 9902022 ccr9988165 part university kentucky center computational sciences university kentucky college engineering email cshencsukyedu z email jzhangcsukyedu url httpwwwcsukyedujzhang memory computers using mpi pvm message passing 17 20 even shared memory parallel computers use mpi code portability made distributed programming style prevalent result developing efficient numerical linear algebra algorithms specifically aiming high performance computers becomes challenging issue 9 10 many numerical simulation modeling problems cpu consuming part computations solve large sparse linear systems accepted solving large sparse linear systems iterative methods becoming method choice due favorable memory computational costs comparing direct solution methods based gaussian elimination one drawback many iterative methods lack robustness ie iterative method may yield acceptable solution given problem common strategy enhance robustness iterative methods exploit preconditioning techniques however robust preconditioners derived certain type incomplete lu factorizations coefficient matrix efficient implementations parallel computers nontrivial challenge recent trend parallel preconditioning techniques general sparse linear systems use ideas domain decomposition concepts processor assigned certain number rows linear system solved discussions related point view comparisons different domain decomposition strategies see 3 19 34 references therein simple parallel preconditioner derived using simple parallel iterative methods commonly used parallel preconditioners engineering computations point block jacobi preconditioners 4 36 preconditioners easy implement efficient sense number preconditioned iterations required solve realistic problems still large 35 sophisticated approach parallel preconditioning use domain decomposition schur complement strategies constructing parallel preconditioners 34 preconditioners constructed approach may scalable ie number preconditioned iterations increase rapidly number processors increases techniques class include various distributed schur complement methods solving general sparse linear systems developed 2 5 28 27 sparse matrices arising finite difference discretized partial differential equations pdes level set technique usually employed extract inherent parallelism discretization schemes ilu0 factorization performed forward backward triangular solves associated preconditioning parallelized within level set approach seems suitable implementations shared memory machines small number processors 11 many realistic problems unstructured meshes parallelism extracted level set strategy inadequent furthermore ilu0 preconditioner may accurate enough subsequent preconditioned iterations may converge slowly may converge thus higher accuracy preconditioners advocated authors increased robustness 8 21 37 45 24 30 however higher accuracy preconditioners usually means fillin entries kept preconditioners couplings among nodes increased well 24 increased couplings reduce inherent parallelism new ordering techniques must employed extract parallelism higher accuracy preconditioners addition standard domain decomposition concepts preconditioning techniques designed specifically target parallel computers include sparse approximate inverse multilevel treatments 1 7 14 39 40 although claimed inherently parallel precon ditioners efficient sparse approximate inverse techniques run respectfully distributed parallel computers scarce 9 recently class high accuracy preconditioners combine inherent parallelism domain decomposition robustness ilu factorization scalability potential multigrid method developed 30 31 multilevel block ilu preconditioners bilum bilutm tested show promising convergence rate scalability solving certain problems construction preconditioners based block independent set ordering recursive block ilu factorization schur complements although class preconditioners contain obvious parallelism within level parallel implementations yet reported study mainly address issue implementing multilevel block ilu preconditioners distributed environment using distributed sparse matrix template 26 bilutm preconditioner saad zhang 31 modified implemented two level block ilu preconditioner distributed memory parallel architectures pbilu2 used saads psparslib library 1 mpi basic communication routines pbilu2 preconditioner compared one favorable schur complement based preconditioners 27 numerical experiments article organized follows section 2 background block independent set ordering bilutm preconditioner given section 3 outline distributed representations general sparse linear systems section 4 discuss construction preconditioner pbilu2 based two level block ilu factorization numerical experiments comparison two schur complement based preconditioners solving various distributed linear systems presented section 5 demonstrate merits two level block ilu preconditioner concluding remarks comments future work given section 6 independent set bilutm distributed sparse matrix solvers rely classical domain decomposition concept partition adjacency graph coefficient matrix graph partitioning 1 psparslib library available online httpwwwcsumneduresearcharpap sparslibpspabshtml algorithms software packages available 16 18 22 techniques extract parallelism incomplete lu factorizations bilum bilutm usually relay fact many rows sparse matrix eliminated simultaneously given stage gaussian elimination set consisting rows called independent set 13 large scale matrix computations degree parallelism extracted traditional point independent set ordering inadequent concept block independent set proposed 30 thus block independent set set groups blocks unknowns coupling unknowns two different groups blocks 30 various heuristic strategies finding point independent sets may extended find block independent set different properties 30 simple usually efficient strategy socalled greedy algorithm groups nearest nodes together considering general sparse linear system form unstructured realvalued matrix order n greedy algorithm graph partitioners used find block independent set adjacency graph matrix initially candidate nodes block include nodes corresponding row matrix given block size k greedy algorithm starts first node groups nearest k neighboring nodes drops nodes linked grouped k nodes vertex cover set vertex cover set set nodes least one link least one node least one block block independent set process repeated times candidate nodes gone either one independent blocks vertex cover set number remaining candidate nodes less k put vertex cover set meaning vertex cover set generalized cover case detailed algorithm descriptions see 30 remark necessary independent blocks number nodes 33 chosen cardinality sake load balance parallel computations sake easy programming parallel implementations graph partitioner similar greedy algorithm described first invoked partition adjacency graph based resulting partitioning matrix corresponding right hand side unknown vectors b x distributed individual processors suppose block independent set uniform block size k found matrix symmetrically permuted two two block matrix form p permutation matrix diagonal matrix dimension ks number uniform blocks size k blocks usually dense k small sparse k large implementation bilum exact inverse technique used compute b gamma1 inverting small independently noted 33 direct inversion strategy usually produces dense inverse matrices even original blocks highly sparse large size several sparsification strategies proposed maintain sparsity b gamma1 33 addition sparse approximate inverse based multilevel block ilu preconditioners proposed 43 article employ ilu factorization strategy compute sparse incomplete lu factorization b approach similar one used bilutm 31 construction bilutm preconditioner based restricted ilu factorization 2 dual dropping strategy ilut 31 multilevel block ilu preconditioner bilutm retains robustness flexibility ilut also powerful ilut solving difficult problems offers inherent parallelism exploited parallel distributed architectures 3 distributed sparse linear system slu preconditioner distributed sparse linear system collection sets equations assigned different processors parallel solution sparse linear system begins partitioning adjacency graph coefficient matrix based resulting partitioning data distributed processors pairs equationsunknowns assigned processor type distributed matrix data structure based subdomain decomposition concepts proposed 29 26 also see 28 based concepts matrix assigned processor unknowns processor divided three types 1 interior unknowns coupled local equations 2 local interface unknowns coupled nonlocal external local equations 3 external interface unknowns belong subdomains coupled local equations submatrix assigned certain processor say processor split two parts local matrix acts local variables interface matrix x acts external variables accordingly local equations given processor written local matrix reordered way interface points listed last interior points local system written block format n indices subdomains neighbors reference subdomain exactly set processors reference processor needs communicate receive information part product x iext reflects contribution local equation neighboring subdomain j sum contributions result multiplying x external interface unknowns ie preconditioners built upon distributed data structure original matrix form approximation global schur complement explicitly domain decomposition based preconditioners exploited 27 simplest one additive schwarz procedure form block jacobi bj iteration blocks refer submatrices associated subdomains ie even though constructed easily block jacobi preconditioning robust inefficient comparing schur complement type preconditioners one best among schur complement based preconditioners slu distributed approximate schur lu preconditioner 27 preconditioning global matrix defined terms block lu factorization involves solve global schur complement system preconditioning step incomplete lu factorization used slu approximate local schur complements numerical results reported 27 show schur ilu preconditioner demonstrates superior scalability performance block jacobi preconditioner efficient latter terms parallel run time 4 class two level block preconditioning techniques pbilu2 two level block ilu preconditioner based bilutm techniques described 31 noted bilutm offers good parallelism robustness due large size block independent set graph partitioner bilutm greedy algorithm finding block independent set 30 31 41 distributed matrix based block independent set implementation block size block independent set must given search algorithm starts choice block size k based problem size density coefficient matrix choice k may also depend upon number available processors assume block independent set uniform block size k found coefficient matrix permuted block form small independent blocks divided several groups according number available processors sake load balance processor group holds approximate number independent blocks numbers independent blocks different groups may differ 1 time global vector unknowns x split two subvectors right hand side vector b also conformally split subvectors f g reordering leads block systemb bm fm um number processors used computation block diagonal contains several independent blocks note submatrix f row numbers block submatrix b submatrices e c also divided parts according load balance criterion order approximately amount loads processor e c also row numbers submatrices assigned processor u local part unknown vector f g local part right hand side vectors partitioned assigned certain processor time matrix distributed processordata assignment done processor holds several rows equations local system equations processor written u part unknown subvector acts submatrices another part unknown subvector acts f c b acts completely local vector u take u local unknowns completely interior vectors preconditioners based type block independent set ordering domain decomposition different straightforward domain decomposition based rowwise strip partitioning 3 used 27 obvious difference partitionings 3 5 5 action f completely local local 3 however since nature submatrices different two decomposition strategies easy say one better stage 42 derivation schur complement techniques key idea domain decomposition techniques develop preconditioners global system 1 exploiting methods approximately solve schur complement system parallel parallel construction pbilu2 preconditioner based block independent set domain decomposition computing approximation global schur complement described deriving global schur complement another parts coefficient matrix need partitioned sent certain processor rewrite reordered coefficient matrix system 2 bm fm thus two ways partition submatrix e one partition e rows columns submatrices also assigned processor number columns block diagonal submatrix number rows submatrix c remark 41 clear potential confusion two representations submatrix e row partitioning e 4 used representing local matrix form 5 different local matrix 3 kept throughout computational process column partitioning e 6 convenience computing schur complement matrix parallel column partitioning e kept construction schur complement matrix cases submatrix e small highly sparse b large consider block lu factorization 2 form 0 global schur complement suppose invert b means rewrite equation 8 fmc processor compute one component sum independently partitions rows submatrix part rows must conformal submatrix c scattered processors global communication needed scattering finally local part schur complement matrix constructed independently processor simplest implementation approach constructing distributed schur complement matrix incomplete lu factorization use parallel block restricted ikj version gaussian elimination similar sequential algorithm used bilutm 31 method decrease communications among processors offers flexibility controlling amount fillin ilu factorization 43 parallel restricted gaussian elimination bilutm high accuracy preconditioner based incomplete lu factorization utilizes dual dropping strategy ilut control computational storage memory costs 24 implementation based restricted ikj version gaussian elimi nation discussed detail 31 remaining part subsection outline parallel implementation restricted ikj version gaussian elimination using distributed data structure discussed previous subsection ith processor local submatrix formed based submatrices assigned processor ilu factorization local matrix performed 2 local matrix processor looks like pbilu2 local matrix means stored local processor necessarily mean acts interior unknowns note submatrix c size submatrix c equation 7 submatrix elements corresponding nonzero entries submatrix may zero others zero elements recalling permuted matrix 2 left hand side 7 let submatrices c submatrix c obtained perform restricted gaussian elimination local matrix 10 slightly different elimination procedure first perform ilu factorization gaussian elimination upper part local matrix ie submatrix b f continue gaussian elimination lower part elimination performed respect nonzero accepted fillin entries submatrix entries modified accordingly performing operations lower part upper part matrix accessed modified see figure 1 f processed accessed processed accessed accessed modified accessed modified accessed modified figure 1 illustration restricted ikj version gaussian elimination submatrices respectively equation 10 done three kinds submatrices formed used later iterations 1 upper part matrix upper part gaussian elimination ub l gamma1 2 upper part matrix also performed ilu factorization block diagonal submatrix b b lb thus extract submatrices lb ub upper part factored local matrix later use 3 restricted factorization lower part obtain new reduced submatrix represented c form piece global schur complement matrix fact submatrix note b gamma1 f factor matrix l gamma1 f already available factorization upper part computed processor first solving auxiliary matrix q ub followed matrixmatrix multiplication q however part computation done implicitly restricted ikj gaussian elimination process sense computations constructing piece schur complement matrix processor done restricted ilu factorization lower part local matrix words c formed without explicit linear system solve matrixmatrix multiplication detailed computational procedure see 31 considering equation 9 schur complement computation rewritten form computation done parallel thanks block diagonal structure b gaussian elimination exact factorization global schur complement matrix formed summing submatrices together submatrix parts use partitionings original submatrix c corresponding parts scattered relevant processors receiving summing parts submatrices scattered different processors local schur complement matrix formed local means rows global schur complement held given processor note remark 42 restricted ikj gaussian elimination yields block ilu factorization local matrix 10 form however submatrices l gamma1 longer needed later computations discarded strategy saves considerable storage space different current implementation slu psparslib library 25 29 27 44 induced global preconditioner possible develop preconditioners global system 1 exploiting methods approximately solve reduced system 8 techniques based reordering global system two two block form 2 consider block lu factorization equation 7 block factored matrix preconditioned approximate lu factorization 0 approximation global schur complement matrix formed 12 therefore global preconditioning operation induced schur complement solve equivalent solving lu f forward solve l backward substitution u computational procedure would consist following three steps g used auxiliary 1 compute schur complement right hand side 2 approximately solve reduced system 3 back substitution u variables ie solve steps computed parallel processor communications boundary information exchange among processors matrix partitioning approach different one used 27 needs communications among processors computing global schur complement right hand side g processor easy see mc b mc gammab local schur complement right hand side computed way rewrite approximate reduced schur complement system submatrix x ij boundary matrix acts external variables numerous ways solve reduced system one option considered 27 starts replacing approximate system form local approximation local schur complement matrix formulation viewed block jacobi preconditioned version schur complement system 13 system solved iterative accelerator gmres requires solve step current implementation ilut factorization performed purpose block jacobi preconditioning third step schur complement preconditioning performed without problem since b block diagonal solution computed parallel iteration step processor actually solve lb factors lb available need exchange boundary information among processors since components required f processor 5 numerical experiments numerical experiments compared performance previously described pbilu2 preconditioner distributed schur complement lu slu preconditioner 27 solving sparse matrices discretized two dimensional convection diffusion problems application problems computational fluid dynamics computations carried 32 processor 200 mhz subcomplex 64 processor hp exemplar 2200 xclass supercomputer university kentucky 8 super nodes interconnected high speed low latency network super node 8 processors attached supercomputer total 16 gb shared memory theoretical operation speed 51 gflops used mpi library interprocessor communications major parts code mainly written fortran 77 programming language c routines handling dynamic allocations memory many communication subroutines slu preconditioner code taken psparslib library 25 tables containing numerical results n denotes dimension matrix nnz represents number nonzeros sparse matrix np number processors used iter number preconditioned fgmres iterations outer erations ftime cpu time seconds preconditioned solution process fgmres ptime total cpu time seconds solving given sparse matrix starting initial distribution matrix data processor master processor processor 0 ptime include graph partitioning time initial permutation time associated partitioning done sequentially processor 0 thus ptime includes matrix distribution local reordering preconditioner construction iteration process time ftime sratio stands sparsity ratio ratio number nonzeros preconditioner number nonzeros original matrix k block size used pbilu2 p number nonzeros allowed l u factors ilu factorizations drop tolerance p meaning used saads ilut 24 preconditioners use flexible variant restarted gmres fgmres 23 solve original linear system since accelerator permits change preconditioning operation step current case since used iterative process approximately solving schur complement matrix outer fgmres iteration size krylov subspace set 50 linear systems formed assuming exact solution vector unit initial guess random vectors components 0 1 convergence achieved 2norm residual approximate solution reduced 6 orders magnitude used innerouter iteration process maximum number outer preconditioned fgmres iterations 500 inner iteration solve schur complement system used gmres5 without restart block jacobi type preconditioner inner iteration stopped 2norm residual inner iteration reduced 100 number inner iterations greater 5 51 5point 9point matrices first compared parallel performance different preconditioners solving 5point 9point matrices 5point 9point matrices generated discretizing following convection diffusion equation two dimensional unit square socalled reynolds number convection coefficients chosen px expgammaxy right hand side function used since generated artificial right hand sides sparse linear systems stated 5point matrices generated using standard central difference discretization scheme 9point matrices generated using fourth order compact difference scheme 15 two types matrices used test bilum ilu type preconditioners 30 31 41 comparison results parallel iterative solvers report cpu timing results iteration numbers however general difficult make fair comparison two different preconditioning algorithms without listing resource costs achieve given results since accuracy preconditioners usually influenced fillin entries kept memory storage cost preconditioner important indicator efficiency preconditioner preconditioners use memory space general faster use less memory space good preconditioner use much memory space still achieve fast convergence end report paper number preconditioned iterations parallel cpu time preconditioned solution process parallel cpu time entire computational process sparsity ratio first chose 0 5point matrix block size chosen 200 dropping parameters chosen slu used one level overlapping among subdomains suggested 27 test results listed table 1 found pbilu2 preconditioner faster slu preconditioner 27 solving problem pbilu2 takes smaller number iterations converge slu convergence rates pbilu2 slu strongly affected number processors employed indicates good scalability respect parallel system two preconditioners moreover pbilu2 took much less parallel cpu time slu needed half memory space consumed slu solve matrix see remark 42 explanation difference storage space pbilu2 slu 3 also tested matrix smaller value case report two test cases slu one level overlapping subdomains nonoverlapping subdomains test results listed table 2 experiments found 3 sparsity ratios pbilu2 slu measured storage spaces used storing preconditioner may case storage spaces slu could released however sparsity ratios slu reported article based slu code distributed psparslib library version 30 downloaded httpwwwcsumneduresearcharpap sparslibpspabshtml november 1999 table 1 5point matrix one level overlapping slu preconditioner np iter ftime ptime sratio pbilu2 slu 34 3422 5437 1202 pbilu2 table 2 5point matrix one level overlapping nonoverlapping results brackets slu preconditioner np iter ftime ptime sratio slu 44 50 3147 5159 915 925 overlapping nonoverlapping subdomains make much difference terms parallel run time parallel cpu timings overlapping cases reported table 2 observation agreement made 27 however overlapping version slu converged faster nonoverlapping version ironically nonoverlapping version slightly larger sparsity ratio storage space preconditioner primarily determined dual dropping parameters p overlapping makes local submatrix look larger thus reduces sparsity ratio relative number nonzeros coefficient matrix remark pbilu2 seen converge faster take less parallel run time slu overlapped nonoverlapped solve 5point matrix using given parameters since costs performance overlapping nonoverlapping slu close report results overlapping version slu remaining numerical tests comparing results tables 1 2 see higher accuracy pbilu2 preconditioner using larger p performed better lower accuracy pbilu2 terms iteration counts parallel run time higher accuracy one course takes memory space store slu preconditioner overlapping tested 27 compared preconditioners bj block jacobi si pure schur complement iteration sapinv distributed approximate block lu factorization sparse approximate etc numerical experiments 27 showed slu retains superior performance bj si preconditioners comparable schur complement preconditioning local schur complements inverted sapinv ever parallel pbilu2 preconditioner shown efficient slu explain communication cost pbilu2 experimental data corresponding numerical results table 2 example total parallel computation time ptime 1803 seconds pbilu2 communication time constructing schur complement matrix 145 seconds communication constructing pbilu2 case costs 804 total parallel computation time 4 total parallel computation time ptime 11425 seconds communication time 072 second communication time constructing pbilu2 063 total parallel computation time cost communication constructing pbilu2 preconditioner high also used larger n varied generate larger 5point 9point matrices comparison results given tables 3 4 results comparable results listed tables 1 2 however parallel run time ptime slu tables 3 4 increased dramatically tripled number processors increased 24 32 results 5point matrix given table 5 see pbilu2 performed much better slu furthermore scalability slu degenerating test problem number iterations 13 4 processors used increased 22 processors used pbilu2 preconditioner number iterations almost constant 12 number processors increased 4 32 large ptime results slu especially distribution large amount data given parallel computer using partitioning strategy slu may present problems another set tests run solving 9point matrix parallel iteration times ftime respect different number processors pbilu2 slu plotted figure 2 pbilu2 solved 9point matrix faster slu figure 3 numbers preconditioned fgmres iterations pbilu2 slu compared respect number table 3 9point matrix gamma4 one level overlapping slu preconditioner np iter ftime ptime sratio slu 19 1676 4666 450 pbilu2 slu 19 2431 15124 451 table 4 5point matrix gamma4 one level overlapping slu preconditioner np iter ftime ptime sratio table 5 5point matrix one level overlapping slu preconditioner np iter stime ptime sratio k slu 19 4180 10168 711 slu 22 3522 163028 710 time number processors dash line slu iteration solid line pbilu2 iteration figure 2 comparison parallel iteration time ftime pbilu2 slu preconditioners solving 9point matrix parameters used processors employed solve 9point matrix figure 3 indicates convergence rate pbilu2 improved number processor increased convergence rate slu deteriorated number processors increased summarize comparison results subsection 5point 9point matrices finite difference discretized convection diffusion problems tests seen pbilu2 needs less half storage space required slu parameters chosen comparably storage space consumed slu pbilu2 still outperformed slu faster convergence rate less parallel run time meanwhile see number processors increases parallel cpu time decreases number iterations affected significantly pbilu2 52 fidap matrices set test matrices extracted test problems provided fidap package 12 4 many matrices small zero diagonals difficult solve standard ilu preconditioners 42 tested 31 fidap matrices preconditioners found pbilu2 solve twice many fidap matrices slu tests pbilu2 solved 20 fidap matrices slu solved 9 tests show parallel two level block ilu preconditioner 4 matrices available online matrixmarket national institute standards technology httpmathnistgovmatrixmarket number number processors dash line slu iteration solid line pbilu2 iteration figure 3 comparison numbers preconditioned fgmres iterations pbilu2 slu preconditioners solving 9point matrix parameters used robust slu preconditioner approach also shown merits terms smaller construction parallel solution costs smaller memory cost smaller number iterations compared slu preconditioner sake brevity listed results three representative large test matrices tables 6 7 figure 4 note table 6 means preconditioned iterative method converge number iterations greater 500 varied parameters fillin p drop tolerance table 6 preconditioners adjusted size block independent set pbilu2 approach pbilu2 clearly shown robust slu solve fidap matrix fidap035 matrix larger fidapm29 test also adjusted fillin drop tolerance parameters p 50 slu pbilu2 test results pbilu2 convergence reported table 7 note small values required ilu factorizations seems difficult slu converge test problem parameter pairs listed table 7 parameter pairs tested slu results listed table 7 figure 4 shows parallel iteration time ftime respect number processors pbilu2 solve fidap019 matrix see parallel iteration time decreased number processors increased demonstrates good speedup solving unstructured general sparse matrix even fidap matrices pbilu2 slu converge pbilu2 usually shows superior performance slu terms number iterations table preconditioner np p iter ftime ptime sratio table 7 fidap035 matrix preconditioner np p k iter ftime ptime sratio 28 169 413 396 iteration time number processors solid line pbilu2 iteration figure 4 parallel iteration time ftime pbilu2 fidap019 matrix parameters used pbilu2 900 sparsity ratio approximately 345 table 8 flat10a matrix preconditioner np k iter ftime ptime sratio table 9 flat30a matrix preconditioner np k iter sratio sparsity ratio 53 flat matrices flat matrices fully coupled mixed finite element discretization three dimensional navierstokes equations 4 44 5 flat10a means matrix first newton step nonlinear iterations 10 elements x coordinate directions 1 element z coordinate direction one element z coordinate direction limitation computer memory used generate matrices explanation holds flat30a matrix uses 30 elements x coordinate directions matrices generated keep variable structural couplings navierstokes equations may nonzero entries actually numerical zero value note two matrices actually symmetric since first newton step velocity vector set zero however symmetry information utilized computation see tables 8 9 pbilu2 able solve two cfd matrices small values two matrices difficult slu converge small sparsity ratios reflect previous remark two flat matrices many numerical zero entries ignored thresholding based ilu factorization counted towards sparsity ratio calculations matrices fully coupled mixed finite element discretizations navierstokes equations notoriously difficult solve preconditioned iterative methods 6 44 standard ilu type preconditioners tend fail produce unstable factorizations unless 5 flat matrices available second author variables orders properly 44 suitable orderings difficult implement sequential environments 6 44 seems however nontrivial task perform analogous orderings parallel environment 6 concluding remarks future work implemented parallel two level block ilu preconditioner based schur complement preconditioning discussed details distribution small independent blocks form subdomain processor gave computational procedure constructing distributed schur complement matrix parallel compared parallel preconditioner pbilu2 scalable parallel two level schur lu preconditioner published recently numerical experiments show pbilu2 demonstrates good scalability solving large sparse linear systems parallel computers also found pbilu2 faster computationally efficient slu test cases pbilu2 also efficient terms memory consumption since uses less memory space slu achieve better convergence rate fidap flat matrices tested sections 52 53 small zero main diagonal entries poor convergence performance pbilu2 slu mainly due instability associated ilu factorizations matrices diagonal thresholding strategies 32 38 employed pbilu2 exclude rows small diagonals submatrix b ilu factorization stable parallel implementation diagonally thresholded pbilu2 investigated future study plan extend parallel two level block ilu preconditioner truly parallel multilevel block ilu preconditioners future research also plan test parallel preconditioners emerging high performance computing platforms pc clusters r mpi implementation spai preconditioner t3e parallel nonoverlapping domain decomposition algorithm compressible fluid flow problems triangulated mains comparison domain decomposition ilu preconditioned iterative methods nonsymmetric elliptic problems parallel finite element solution threedimensional rayleigh benardmarangoni flows parpre parallel preconditioners package reference manual version 20 preconditioned conjugate gradient methods incompressible navierstokes equations priori sparsity patterns parallel sparse approximate inverse precondi tioners towards cost effective ilu preconditioner high level fill numerical linear algebra highperformance computers developments trends parallel solution linear systems parallelization ilu0 preconditioner cfd problems sharedmemory computers fidap examples manual computer solution large sparse positive definite systems parallel preconditioning approximate inverse connection machines single cell high order scheme convectiondiffusion equation variable coefficients chaco users guide scalable parallel computing parallel multilevel kway partitioning scheme irregular graphs comparison domain decomposition techniques elliptic partial differential equations parallel implementation introduction parallel computing direct methods sparse matrices partitioning sparse matrices eigenvectors graphs flexible innerouter preconditioned gmres algorithm ilut dual threshold incomplete lu preconditioner parallel sparse matrix library p sparslib iterative solvers module iterative methods sparse linear systems distributed schur complement techniques general sparse linear systems domain decomposition multilevel type techniques general sparse linear systems design iterative solution module parallel sparse matrix library p sparslib bilum block versions multielimination multilevel ilu preconditioner general sparse linear systems bilutm domainbased multilevel block ilut preconditioner general sparse matrices diagonal threshold techniques robust multilevel ilu preconditioners general sparse linear systems enhanced multilevel block ilu preconditioning strategies general sparse linear systems domain decomposition parallel multilevel methods elliptic partial differential equations high performance preconditioning parallel computation incompressible flows materials processing numerical experiments diagonal preconditioning application sparse matrix solvers effective preconditioners multilevel dual reordering strategy robust incomplete lu factorization indefinite matrices parallelizable preconditioner based factored sparse approximate inverse technique sparse approximate inverse parallel preconditioning sparse matrices preconditioned iterative methods finite difference schemes convectiondiffusion preconditioned krylov subspace methods solving nonsymmetric matrices cfd applications sparse approximate inverse multilevel block ilu preconditioning techniques general sparse matrices performance study incomplete lu preconditioners solving linear systems fully coupled mixed finite element discretization 3d navierstokes equations use iterative refinement solution sparse linear systems tr comparison domain decomposition techniques elliptic partial differential equations parallel implementation high performance preconditioning application sparse matrix solvers effective preconditioners partitioning sparse matrices eigenvectors graphs introduction parallel computing flexible innerouter preconditioned gmres algorithm towards costeffective ilu preconditioner highlevel fill domain decomposition parallel computation incompressible flows materials processing parallel multilevel series ikiway partitioning scheme irregular graphs developments trends parallel solution linear systems preconditioned iterative methods finite difference schemes convectiondiffusion distributed schur complement techniques general sparse linear systems priori sparsity patterns parallel sparse approximate inverse preconditioners sparse approximate inverse multilevel block ilu preconditioning techniques general sparse matrices enhanced multilevel block ilu preconditioning strategies general sparse linear systems scalable parallel computing numerical linear algebra high performance computers computer solution large sparse positive definite multilevel dual reordering strategy robust incomplete lu factorization indefinite matrices iterative methods sparse linear systems ctr chi shen jun zhang fully parallel block independent set algorithm distributed sparse matrices parallel computing v29 n1112 p16851699 novemberdecember jun zhang tong xiao multilevel block incomplete cholesky preconditioner solving normal equations linear least squares problems korean journal computational applied mathematics v11 n12 p5980 january chi shen jun zhang kai wang distributed block independent set algorithms parallel multilevel ilu preconditioners journal parallel distributed computing v65 n3 p331346 march 2005