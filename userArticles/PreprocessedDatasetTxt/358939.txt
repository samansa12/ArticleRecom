data prefetch mechanisms expanding gap microprocessor dram performance necessitated use increasingly aggressive techniques designed reduce hide latency main memory access although large cache hierarchies proven effective reducing latency frequently used data still uncommon many programs spend half run times stalled memory requests data prefetching proposed technique hiding access latency data referencing patterns defeat caching strategies rather waiting cache miss initiate memory fetch data prefetching anticipates misses issues fetch memory system advance actual memory reference effective prefetching must implemented way prefetches timely useful introduce little overhead secondary effects cache pollution increased memory bandwidth requirements must also taken consideration despite obstacles prefetching potential significantly improve overall program execution time overlapping computation memory accesses prefetching strategies diverse single strategy yet proposed provides optimal performance following survey examines several alternative approaches discusses design tradeoffs involved implementing data prefetch strategy b introduction metric microprocessor performance increased dramatic rate past decade trend sustained continued architectural innovations advances microprocessor fabrication technology contrast main memory dynamic ram dram performance increased much leisurely rate shown figure 1 expanding gap microprocessor dram performance necessitated use increasingly aggressive techniques designed reduce hide large latency memory accesses 16 chief among latency reducing techniques use cache memory hierarchies 34 static ram sram memories used caches managed keep pace processor memory request rates continue expensive main store technology although use large cache hierarchies proven effective reducing average memory access penalty programs show high degree locality addressing patterns still uncommon scientific dataintensive programs spend half run times stalled memory requests 25 large dense matrix operations form basis many applications typically exhibit little locality therefore defeat caching strategies poor cache utilization applications partially result demand memory fetch policy caches policy fetches data cache main memory processor requested word found absent cache situation illustrated figure 2a computation including memory references satisfied within cache hierarchy represented upper time line main memory access time represented lower time line figure data blocks associated memory references r1 r2 r3 found cache hierarchy must therefore fetched main memory assuming referenced data word needed immediately processor stalled waits corresponding cache block fetched data returns main memory cached forwarded processor computation may proceed year performance101000 system figure 1 system dram performance since 1988 system performance measured specfp92 dram performance row access times values normalized 1988 equivalents source internet spectable ftpftpcstorontoedupubjddspectable note fetch policy always result cache miss first access cache block since previously accessed data stored cache cache misses known cold start compulsory misses also referenced data part large array operation likely data replaced use make room new array elements streamed cache data block needed later processor must bring main memory incurring full main memory access latency called capacity miss many cache misses avoided augment demand fetch policy cache addition data prefetch operation rather waiting cache miss perform memory fetch data prefetching anticipates misses issues fetch memory system advance actual memory reference prefetch proceeds parallel processor computation allowing memory system time transfer desired data main memory cache ideally prefetch complete time processor access needed data cache without stalling processor increasingly common mechanism initiating data prefetch explicit fetch instruction issued processor minimum fetch specifies address data word brought cache space fetch instruction executed address simply passed memory system without forcing processor wait response cache responds fetch manner similar ordinary load instruction exception aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa3010a b c time aaaa aaaa aaaa aaaa aaaa aaaa computation memory acesss cache hit cache miss prefetch figure 2 execution diagram assuming prefetching b perfect prefetching c degraded prefetching referenced word forwarded processor cached figure 2b shows prefetching used improve execution time demand fetch case given figure 2a latency main memory accesses hidden overlapping computation memory accesses resulting reduction overall run time figure represents ideal case prefetched data arrives requested processor less optimistic situation depicted figure 2c figure prefetches references r1 r2 issued late avoid processor stalls although data r2 fetched early enough realize benefit note data r3 arrives early enough hide memory latency must held processor cache period time used processor time prefetched data exposed cache replacement policy may evicted cache use occurs prefetch said useless performance benefit derived fetching block early prematurely prefetched block may also displace data cache currently use processor resulting known cache pollution note effect distinguished normal cache replacement misses prefetch causes miss cache would occurred prefetching use defined cache pollution however prefetched block displaces cache block referenced prefetched block used ordinary replacement miss since resulting cache miss would occurred without prefetching subtle side effect prefetching occurs memory system note figure 2a three memory requests occur within first 31 time units program startup whereas figure 2b requests compressed period 19 time units removing processor stall cycles prefetching effectively increases frequency memory requests issued processor memory systems must designed match higher bandwidth avoid becoming saturated nullifying benefits prefetching particularly true multiprocessors bus utilization typically higher single processor systems also interesting note software prefetching achieve reduction run time despite adding instructions execution stream figure 3 memory effects figure 2 ignored computational components run time shown seen three prefetch instructions actually increase amount work done processor several hardwarebased prefetching techniques also proposed require use explicit fetch instructions techniques employ special hardware monitors processor attempt infer prefetching opportunities although hardware prefetching incurs instruction overhead often generates unnecessary prefetches software prefetching unnecessary prefetches common hardware schemes speculate future memory accesses without benefit compiletime information speculation incorrect cache blocks actually needed brought cache although unnecessary prefetches affect correct program behavior result cache pollution aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa prefetch overhead prefetching figure 3 software prefetching overhead consume memory bandwidth effective data prefetching must implemented way prefetches timely useful introduce little overhead secondary effects memory system must also taken consideration designing system employs prefetch strategy despite obstacles data prefetching potential significantly improve overall program execution time overlapping computation memory accesses prefetching strategies diverse single strategy yet proposed provides optimal performance following sections alternative approaches prefetching examined comparing relative strengths weaknesses 2 background prefetching form existed since midsixties early studies 1 cache design recognized benefits fetching multiple words main memory cache effect block memory transfers prefetch words surrounding current reference hope taking advantage spatial locality memory references hardware prefetching separate cache blocks later implemented ibm 370168 amdahl 470v 33 software techniques recent smith first alluded idea survey cache memories 34 time doubted usefulness later porterfield 29 proposed idea cache load instruction several risc implementations following shortly thereafter prefetching restricted fetching data main memory processor cache rather generally applicable technique moving memory objects memory hierarchy actually needed processor prefetching mechanisms instructions file systems commonly used prevent processor stalls example 3828 sake brevity techniques apply data objects residing memory considered nonblocking load instructions share many similarities data prefetching like prefetches instructions issued advance datas actual use take advantage parallelism processor memory subsystem rather loading data cache however specified word placed directly processor register nonblocking loads example binding prefetch named value prefetched variable bound named location processor register case time prefetch issued although nonblocking loads discussed forms binding prefetches examined data prefetching received considerable attention literature potential means boosting performance multiprocessor systems interest stems desire reduce particularly high memory latencies often found systems memory delays tend high multiprocessors due added contention shared resources shared bus memory modules symmetric multiprocessor memory delays even pronounced distributedmemory multiprocessors memory requests may need satisfied across interconnection network masking significant memory latencies prefetching effective means speeding multiprocessor applications due emphasis prefetching multiprocessor systems many prefetching mechanisms discussed studied either largely exclusively context several mechanisms may also effective single processor systems multiprocessor prefetching treated separate topic prefetch mechanism inherent systems 3 software data prefetching contemporary microprocessors support form fetch instruction used implement prefetching 33137 implementation fetch simple load processor register hardwired zero slightly sophisticated implementations provide hints memory system prefetched block used information may useful multiprocessors data prefetched different sharing states example although particular implementations vary fetch instructions share common characteristics fetches nonblocking memory operations therefore require lockupfree cache 21 allows prefetches bypass outstanding memory operations cache prefetches typically implemented way fetch instructions cannot cause exceptions exceptions suppressed prefetches insure remain optional optimization feature affect program correctness initiate large potentially unnecessary overhead page faults memory exceptions hardware required implement software prefetching modest compared prefetching strategies complexity approach lies judicious placement fetch instructions within target application task choosing program place fetch instruction relative matching load store instruction known prefetch scheduling practice possible precisely predict schedule prefetch data arrives cache moment requested processor case figure 2b execution time prefetch matching memory reference may vary memory latencies uncertainties predictable compile time therefore require careful consideration scheduling prefetch instructions program fetch instructions may added programmer compiler optimization pass unlike many optimizations occur frequently program tedious implement hand prefetch scheduling often done effectively programmer studies indicated adding prefetch directives program substantially improve performance 24 however programming effort kept minimum program contains many prefetching opportunities compiler support may required whether handcoded automated compiler prefetching often used within loops responsible large array calculations loops provide excellent prefetching opportunities common scientific codes exhibit poor cache utilization often predictable array referencing patterns establishing patterns compiletime fetch instructions placed inside loop bodies data future loop iteration prefetched current iteration example loopbased prefetching may used consider code segment shown figure 4a loop calculates inner product two vectors b manner similar innermost loop matrix multiplication calculation assume fourword cache block code segment cause cache miss every fourth iteration attempt avoid cache misses adding prefetch directives shown figure 4b note figure source code representation assembly code would generated compiler simple approach prefetching suffers several problems first need prefetch every iteration loop since fetch actually brings four words one cache block cache although extra prefetch operations illegal unnecessary degrade performance assuming b cache block aligned prefetching done every fourth iteration one solution problem surround fetch directives condition tests modulo true overhead explicit prefetch predicate however would likely offset benefits prefetching therefore avoided better solution unroll loop factor r r equal number words prefetched per cache block shown figure 4c unrolling loop involves replicating loop body r times increasing loop stride r note fetch b c figure 4 inner product calculation using prefetching b simple prefetching c prefetching loop unrolling software pipelining directives replicated index value used calculate prefetch address changed i1 ir code segment given figure 4c removes cache misses unnecessary prefetches improvements possible note cache misses occur first iteration loop since prefetches never issued initial iteration unnecessary prefetches occur last iteration unrolled loop fetch commands attempt access data past loop index boundary problems remedied using software pipelining techniques shown figure 4d figure extracted select code segments loop body placed either side original loop fetch statements prepended main loop prefetch data first iteration main loop including ip segment code referred loop prolog epilog added end main loop execute final inner product computations without initiating unnecessary prefetch instructions code given figure 4 said cover loop references reference preceded matching prefetch however one final refinement may necessary make prefetches effective examples figure 4 written implicit assumption prefetching one iteration ahead datas actual use sufficient hide latency main memory accesses may case although early studies 4 based assumption klaiber levy 20 recognized sufficiently general solution loops contain small computational bodies may necessary initiate prefetches iterations data referenced known prefetch distance expressed units loop iterations mowry et al 25 later simplified computation l l average memory latency measured processor cycles estimated cycle time shortest possible execution path one loop iteration including prefetch overhead choosing shortest execution path one loop iteration using ceiling operator calculation designed err conservative side thus increase likelihood prefetched data cached requested processor returning main loop figure 4d let us assume average miss latency 100 processor cycles loop iteration time 45 cycles 3 figure 5 shows final version inner product loop altered handle prefetch distance three note prolog expanded include loop prefetches several cache blocks initial three iterations main loop also main loop shortened stop prefetching three iterations end computation changes necessary epilog carries remaining loop iterations prefetching loop transformations outlined fairly mechanical refinements applied recursively nested loops sophisticated compiler algorithms based approach developed automatically add fetch instructions optimization pass compiler 25 varying degrees success bernstein et al 3 measured runtimes twelve scientific benchmarks without use prefetching powerpc 601 based system prefetching typically improved runtimes less 12 although one benchmark ran 22 faster three others actually ran slightly slower due prefetch instruction overhead santhanam et al 31 found six ten specfp95 benchmark programs ran 26 98 faster pa8000based system prefetching enabled three four remaining specfp95 programs showed less 7 improvement runtime one program slowed 12 compiler must able reliably predict memory access patterns prefetching normally restricted loops containing array accesses whose indices linear functions loop indices loops relatively common scientific codes far less general applications attempts establishing similar software prefetching strategies applications hampered irregular referencing patterns 92223 given complex control structures typical general applications often limited window reliably predict particular datum accessed moreover cache block accessed less chance several successive cache blocks also requested data structures graphs linked lists used finally comparatively high temporal locality many general applications often result high cache utilization thereby diminishing benefit prefetching even restricted wellconformed looping structures use explicit fetch instructions exacts performance penalty must considered using software prefetching fetch instructions add processor overhead require extra execution cycles also fetch source addresses must calculated stored processor ideally prefetch address retained need recalculated matching load store instruction allocating retaining register space prefetch addresses however compiler less register space allocate active variables addition fetch instructions therefore said increase register pressure turn may result additional spill code manage variables spilled main memory due insufficient register space problem exacerbated prefetch distance greater one since implies either maintaining address registers hold multiple prefetch addresses storing addresses memory required number address registers available comparing transformed loop figure 5 original loop seen software prefetching also results significant code expansion turn may degrade instruction cache performance finally software prefetching done statically unable detect prefetched block prematurely evicted needs refetched prolog prefetching main loop prefetching computation computation figure 5 final inner product loop transformation 4 hardware data prefetching several hardware prefetching schemes proposed add prefetching capabilities system without need programmer compiler intervention changes existing executables necessary instruction overhead completely eliminated hardware prefetching also take advantage runtime information potentially make prefetching effective 41 sequential prefetching prefetching schemes designed fetch data main memory processor cache units cache blocks noted however multiple word cache blocks form data prefetching grouping consecutive memory words single units caches exploit principle spatial locality implicitly prefetch data likely referenced near future degree large cache blocks effective prefetching data limited ensuing cache pollution effects cache block size increases amount potentially useful data displaced cache make room new block sharedmemory multiprocessors private caches large cache blocks may also cause false sharing occurs two processors wish access different words within cache block least one accesses store although accesses logically applied separate words cache hardware unable make distinction since operates whole cache blocks accesses therefore treated operations applied single object cache coherence traffic generated ensure changes made block store operation seen processors caching block case false sharing traffic unnecessary since processor executing store references word written increasing cache block size increases likelihood two processors sharing data block hence false sharing likely arise sequential prefetching take advantage spatial locality without introducing problems associated large cache blocks simplest sequential prefetching schemes variations upon one block lookahead obl approach initiates prefetch block b1 block b accessed differs simply doubling block size prefetched blocks treated separately regard cache replacement coherence policies example large block may contain one word frequently referenced several words use assuming lru replacement policy entire block retained even though portion blocks data actually use large block replaced two smaller blocks one could evicted make room active data similarly use smaller cache blocks reduces probability false sharing occur obl implementations differ depending type access block b initiates prefetch b1 smith 34 summarizes several approaches prefetchonmiss tagged prefetch algorithms discussed prefetchonmiss algorithm simply initiates prefetch block b1 whenever access block b results cache miss b1 already cached memory access initiated tagged prefetch algorithm associates tag bit every memory block bit used detect block demandfetched prefetched block referenced first time either cases next sequential block fetched smith found tagged prefetching reduced cache miss ratios unified instruction data cache 50 90 set tracedriven simulations prefetchonmiss less half effective tagged prefetching reducing miss ratios reason prefetchon miss less effective illustrated figure 6 behavior algorithm accessing three contiguous blocks shown seen strictly sequential access pattern result cache miss every cache block prefetchonmiss algorithm used access pattern results one cache miss employing tagged prefetch algorithm hp pa7200 5 serves example contemporary microprocessor uses obl prefetch hardware pa7200 implements tagged prefetch scheme using either directed undirected mode undirected mode next sequential line prefetched directed mode prefetch direction forward backward distance determined prepostincrement amount encoded load store instructions contents address register autoincremented cache block associated new address prefetched compared base case prefetching pa7200 achieved runtime improvements range 0 80 10 specfp95 benchmark programs 35 although performance found applicationdependent two programs ran 20 faster prefetching enabled note one shortcoming obl schemes prefetch may initiated far enough advance actual use avoid processor memory stall sequential access stream resulting tight loop example may allow sufficient lead time use block b request block b1 solve problem possible increase number blocks prefetched demand fetch one k k known degree prefetching prefetching k 1 subsequent blocks aids memory system staying ahead demandfetched prefetched demandfetched prefetched demandfetched prefetched demandfetched prefetched demandfetched prefetched demandfetched prefetched prefetched0 demandfetched prefetched prefetched prefetched demandfetched prefetched demandfetched prefetched prefetched demandfetched prefetched prefetched prefetched1 prefetched prefetched prefetched c b figure 6 three forms sequential prefetching prefetch miss b tagged prefetch c sequential prefetching 2 rapid processor requests sequential data blocks prefetched block b accessed first time cache interrogated check blocks b1 bk present cache missing blocks fetched memory note scheme identical tagged obl prefetching although increasing degree prefetching reduces miss rates sections code show high degree spatial locality additional traffic cache pollution generated sequential prefetching program phases show little spatial locality przybylski 30 found overhead tends make sequential prefetching unfeasible values k larger one dahlgren stenstrm 11 proposed adaptive sequential prefetching policy allows value k vary program execution way k matched degree spatial locality exhibited program particular point time prefetch efficiency metric periodically calculated cache indication current spatial locality characteristics program prefetch efficiency defined ratio useful prefetches total prefetches useful prefetch occurs whenever prefetched block results cache hit value k initialized one incremented whenever prefetch efficiency exceeds predetermined upper threshold decremented whenever efficiency drops lower threshold shown figure 7 note k reduced zero prefetching effectively disabled point prefetch hardware begins monitor often cache miss block b occurs block b1 cached restarts prefetching respective ratio two numbers exceeds lower threshold prefetch efficiency simulations shared memory multiprocessor found adaptive prefetching could achieve appreciable reductions cache miss ratios tagged prefetching however simulated runtime comparisons showed slight differences two schemes lower miss ratio adaptive sequential prefetching found partially nullified associated overhead increased memory traffic contention jouppi 19 proposed approach k prefetched blocks brought fifo stream buffer brought cache buffer entry referenced brought cache remaining blocks moved queue new block prefetched tail position note since prefetched data placed directly cache scheme avoids cache pollution however miss occurs cache desired block also found head stream buffer buffer flushed therefore prefetched blocks must accessed order brought buffer stream buffers provide performance benefit k k time upper threshold lower threshold prefetch efficiency figure 7 sequential adaptive prefetching palacharla kessler 27 studied stream buffers replacement secondary cache primary cache miss occurs one several stream buffers allocated service new reference stream stream buffers allocated lru order newly allocated buffer immediately fetches next k blocks following missed block buffer palacharla kessler found eight stream buffers performance simulation study parameters stream buffer hit rates percentage primary cache misses satisfied stream buffers typically fell 50 90 however memory bandwidth requirements found increase sharply result large number unnecessary prefetches generated stream buffers help mitigate effect small history buffer used record recent primary cache misses history buffer indicates misses occurred block b block b 1 stream allocated blocks b prefetched buffer using selective stream allocation policy bandwidth requirements reduced expense slightly reduced stream buffer hit rates stream buffers described palacharla kessler found provide economical alternative large secondary caches eventually incorporated cray t3e multiprocessor 26 general sequential prefetching techniques require changes existing executables implemented relatively simple hardware compared software prefetching sequential hardware prefetching performs poorly nonsequential memory access patterns encountered however scalar references array accesses large strides result unnecessary prefetches types access patterns exhibit spatial locality upon sequential prefetching based enable prefetching strided irregular data access patterns several elaborate hardware prefetching techniques proposed 42 prefetching arbitrary strides several techniques proposed employ special logic monitor processors address referencing pattern detect constant stride array references originating looping structures 21332 accomplished comparing successive addresses used load store instructions chen baers scheme 7 perhaps aggressive proposed thus far illustrate design assume memory instruction references addresses 1 2 3 three successive loop iterations prefetching initiated assumed stride series array accesses first prefetch address 3 predicted value observed address 3 prefetching continues way equality longer holds true note approach requires previous address used memory instruction stored along last detected stride recording reference histories every memory instruction program clearly impossible instead separate cache called reference prediction table rpt holds information recently used memory instructions organization rpt given figure 8 table entries contain address memory instruction previous address accessed instruction stride value entries established stride state field records entrys current state state diagram rpt entries given figure 9 rpt indexed cpus program counter pc memory instruction executed first time entry made rpt state set initial signifying prefetching yet initiated instruction executed rpt entry evicted stride value calculated subtracting previous address stored rpt current effective address illustrate functionality rpt consider matrix multiply code associated rpt entries given figure 10 example load instructions arrays b c considered assumed arrays begin addresses 10000 20000 30000 respectively simplicity one word cache blocks also assumed first iteration innermost loop state rpt given figure 10b instruction addresses represented pseudocode mnemonics since rpt yet contain entries instructions stride fields initialized zero entry placed initial state three references result cache miss second iteration strides computed shown figure 10c entries array references b c placed transient state newly computed strides match previous stride state indicates instructions referencing pattern may transition tentative prefetch issued block address effective address already cached rpt entry reference array placed steady state previous current strides match since entrys stride zero prefetching issued instruction although reference array hits cache due demand fetch previous iteration references arrays b c result cache miss third iteration entries array references b c move steady state tentative strides computed previous iteration confirmed prefetches issued second iteration result cache hits b c references provided prefetch distance instruction tag previous address stride state pc effective address prefetch address figure 8 organization reference prediction table one sufficient discussion seen rpt improves upon sequential policies correctly handling strided array references however described rpt still limits prefetch distance one loop iteration remedy shortcoming distance field may added rpt specifies prefetch distance explicitly prefetch addresses would calculated effective address addition distance field requires method establishing value given rpt entry calculate appropriate value chen baer decouple maintenance rpt use prefetch engine rpt entries maintained direction pc described prefetches initiated separately pseudo program counter called lookahead program counter lapc allowed precede pc difference pc lapc prefetch distance several implementation issues arise addition lookahead program counter interested reader referred 2 complete description 8 chen baer compared rpt prefetching mowrys software prefetching scheme 25 found neither method showed consistently better performance simulated shared memory multiprocessor instead found performance depended individual program characteristics four benchmark programs upon study based software prefetching found effective certain irregular access patterns indirect reference used calculate prefetch address rpt may able establish access pattern instruction uses indirect address instruction may generate effective addresses separated constant stride also rpt less efficient beginning end loop prefetches issued rpt access pattern established means prefetches issued array data least first two iterations chen baer also noted may take several iterations rpt achieve prefetch distance completely masks memory latency lapc used finally rpt always prefetch past array bounds incorrect prediction necessary stop subsequent prefetching however loop steady state rpt able dynamically adjust prefetch distance achieve better overlap memory latency software scheme array access patterns also software prefetching incurred instruction overhead resulting prefetch address calculation fetch instruction execution spill code initial steady transient prediction correct stride prediction incorrect stride prediction incorrect prediction stride update initial start state prefetching transient stride transition tentative prefetch steady constant stride prefetch stride 0 prediction prefetching figure 9 state transition graph reference prediction table entries dahlgren stenstrm 10 compared tagged rpt prefetching context distributed shared memory multiprocessor examining simulated runtime behavior six benchmark programs concluded rpt prefetching showed limited performance benefits tagged prefetching tends perform well better common memory access patterns dahlgren showed array strides less block size therefore captured tagged prefetch policy addition found scalar references showed limited amount spatial locality could captured tagged prefetch policy rpt mechanism memory bandwidth limited however conjectured conservative rpt prefetching mechanism may preferable since tends produce fewer useless prefetches software prefetching majority hardware prefetching mechanisms focus regular array referencing patterns notable exceptions however harrison mehrotra 17 proposed extensions rpt mechanism allow prefetching data objects connected via pointers approach adds fields rpt enable detection indirect reference strides arising structures linked lists sparse matrices joseph grunwald 18 studied use markov predictor drive data prefetcher dynamically recording sequences cache miss references hardware table prefetcher attempts predict previous pattern misses begun repeat float a100100 b100100 c100100 tag previous address stride state ld bik 20000 0 initial ld ckj 30000 0 initial ld aij 10000 0 initial b tag previous address stride state ld bik 20004 4 transient ld ckj 30400 400 transient ld aij 10000 0 steady c tag previous address stride state ld bik 20008 4 steady ld ckj 30800 400 steady ld aij 10000 0 steady figure 10 rpt execution matrix multiply current cache miss address found table prefetches likely subsequent misses issued prefetch request queue prevent cache pollution wasted memory bandwidth prefetch requests may displaced queue requests belong reference sequences higher probability occurring 5 integrating hardware software prefetching software prefetching relies exclusively compiletime analysis schedule fetch instructions within user program contrast hardware techniques discussed thus far infer prefetching opportunities runtime without compiler processor support noting approaches advantages researchers proposed mechanisms combine elements software hardware prefetching gornish veidenbaum 15 describe variation tagged hardware prefetching degree prefetching k particular reference stream calculated compile time passed prefetch hardware implement scheme prefetching degree pd field associated every cache entry special fetch instruction provided prefetches specified block cache sets tag bit value pd field cache entry holding prefetched block first k blocks sequential reference stream prefetched using instruction tagged block b demand fetched value pd field k b added block address calculate prefetch address pd field newly prefetched block set k b tag bit set insures appropriate value k propagated reference stream prefetching nonsequential reference patterns handled ordinary fetch instructions zheng torrellas 39 suggest integrated technique enables prefetching irregular data structures accomplished tagging memory locations way reference one element data object initiates prefetch either elements within referenced object objects pointed referenced object array elements data structures connected via pointers therefore prefetched approach relies compiler initialize tags memory actual prefetching handled hardware within memory system use programmable prefetch engine proposed chen 6 extension reference prediction table described section 42 chens prefetch engine differs rpt tag address stride information supplied program rather dynamically established hardware entries inserted engine program entering looping structures benefit prefetching programmed prefetch engine functions much like rpt prefetches initiated processors program counter matches one tag fields prefetch engine vanderwiel lilja 36 propose prefetch engine external processor engine general processor executes program prefetch data cpu shared secondlevel cache producerconsumer relationship established two processors engine prefetches new data blocks cache previously prefetched data accessed compute processor processor also partially directs actions prefetch engine writing control information memorymapped registers within prefetch engines support logic integrated techniques designed take advantage compiletime program information without introducing much instruction overhead pure software prefetching much speculation performed pure hardware prefetching also eliminated resulting fewer unnecessary prefetches although commercial systems yet support model prefetching simulation studies used evaluate techniques indicate performance enhanced pure software hardware prefetch mechanisms 6 prefetching multiprocessors addition prefetch mechanisms several multiprocessorspecific prefetching techniques proposed prefetching systems differs uniprocessors least three reasons first multiprocessor applications typically written using different programming paradigms uniprocessors paradigms provide additional array referencing information enable accurate prefetch mechanisms second multiprocessor systems frequently contain additional memory hierarchies provide different sources destinations prefetching finally performance implications data prefetching take added significance multiprocessors systems tend higher memory latencies sensitive memory interconnects fu patel 12 examined data prefetching might improve performance vectorized multiprocessor applications study assumes vector operations explicitly specified programmer supported instruction set vectorized programs describe computations terms series vector matrix operations compiler analysis stride detection hardware required establish memory access patterns instead stride information encoded vector references made available processor caches associated prefetch hardware two prefetching policies studied first variation upon prefetchonmiss policy k consecutive blocks following cache miss fetched processor cache implementation prefetchonmiss differs presented earlier prefetches issued scalars vector references stride less equal cache block size second prefetch policy referred vector prefetching similar first policy exception prefetches vector references large strides also issued vector reference block b misses cache blocks b b fetched fu patel found prefetch policies improve performance prefetch case alliant fx8 simulator speedups pronounced smaller cache blocks assumed since small block sizes limit amount spatial locality nonprefetching cache capture prefetching caches offset disadvantage simply prefetching blocks contrast studies fu patel found sequential prefetching policies effective values k 32 apparent conflict earlier studies found sequential prefetching degrade performance k 1 much discrepancy may explained noting vector instructions exploited prefetching scheme used fu patel case prefetchonmiss prefetching suppressed large stride specified instruction avoids useless prefetches degraded performance original policy although vector prefetching issue prefetches large stride referencing patterns precise mechanism sequential schemes since able take advantage stride information provided program comparing two schemes found applications large strides benefited vector prefetching expected programs scalar unitstride references dominate prefetchonmiss policy tended perform slightly better programs lower miss ratios resulting vector prefetching policy offset corresponding increase bus traffic gornish et al 14 examined prefetching distributed memory multiprocessor global local memory connected multistage interconnection network data prefetched global local memory large asynchronous block transfers achieve higher network bandwidth would possible wordatatime transfers since large amounts data prefetched data placed local memory rather processor cache avoid excessive cache pollution form softwarecontrolled caching assumed responsible translating global array addresses local addresses data placed local memory software prefetching singleprocessor systems loop transformations performed compiler insert prefetch operations user code however rather inserting fetch instructions individual words within loop body entire blocks memory prefetched loop entered figure 11 shows block prefetching may used vectormatrix product calculation figure 11b iterations original loop figure 11a partitioned among nproc processors multiprocessor system processor iterates 1 nproc th c also note array c prefetched row time although possible pull prefetch c entire array fetched local memory entering outermost loop assumed c large prefetch entire array would occupy local memory available block fetches given figure 11b add processor overhead original computation manner similar software prefetching scheme described earlier although blockoriented prefetch operations require size stride information significantly less overhead incurred wordoriented scheme since fewer prefetch operations needed assuming equal problem sizes ignoring prefetches loop given figure 11 generate n1 block prefetches compared 12 c hprefetches would result applying wordoriented prefetching scheme although single bulk data transfer efficient dividing transfer several smaller messages former approach tend increase network congestion several messages transferred combined increased request rate prefetching induces network contention lead significantly higher average memory latencies b figure 11 block prefetching vectormatrix product calculation set six numerical benchmark programs gornish noted prefetching increased average memory latency factor 53 127 prefetch case implication prefetching local memory rather cache array figure 11 cannot prefetched general scheme requires data must readonly prefetch use coherence mechanism provided allows writes one processor seen processors data transfers also restricted control dependencies within loop bodies array reference predicated conditional statement prefetching initiated array done two reasons first conditional may test true subset array references initiating prefetch entire array would result unnecessary transfer potentially large amount data second conditional may guard referencing nonexistent data initiating prefetch data could result unpredictable behavior honoring data control dependencies limits amount data prefetched average 42 loop memory references six benchmark programs used gornish could prefetched due constraints together increased average memory latencies suppression prefetches limited speedup due prefetching less 11 five six benchmark programs mowry gupta 24 studied effectiveness software prefetching dash dsm multiprocessor architecture study two alternative designs considered first places prefetched data remote access cache rac lies interconnection network processor cache hierarchy node system second design alternative simply prefetched data remote memory directly primary processor cache cases unit transfer cache block use separate prefetch cache rac motivated desire reduce contention primary data cache separating prefetched data demandfetched data prefetch cache avoids polluting processor cache provides overall cache space approach also avoids processor stalls result waiting prefetched data placed cache however case remote access cache remote memory operations benefit prefetching since rac placed system bus access times approximately equal main memory simulation runs three scientific benchmarks found prefetching directly primary cache offered benefit average speedup 194 compared average 170 rac used despite significantly increasing cache contention reducing overall cache space prefetching primary cache resulted higher cache hit rates proved dominant performance factor software prefetching single processor systems benefit prefetching applicationspecific speedups two arraybased programs achieved speedups nonprefetch case 253 199 third less regular program showed speedup 130 7 conclusions prefetching schemes diverse help categorize particular approach useful answer three basic questions concerning prefetching mechanism 1 prefetches initiated 2 prefetched data placed 3 prefetched prefetches initiated either explicit fetch operation within program logic monitors processors referencing pattern infer prefetching combination approaches however initiated prefetches must issued timely manner prefetch issued early chance prefetched data displace useful data higher levels memory hierarchy displaced use prefetch issued late may arrive actual memory reference thereby introduce processor stall cycles prefetching mechanisms also differ precision software prefetching issues fetches data likely used hardware schemes tend data speculative manner decision place prefetched data memory hierarchy fundamental design decision clearly data must moved higher level memory hierarchy provide performance benefit majority schemes place prefetched data type cache memory schemes place prefetched data dedicated buffers protect data premature cache evictions prevent cache pollution prefetched data placed named locations processor registers memory prefetch said binding additional constraints must imposed use data finally multiprocessor systems introduce additional levels memory hierarchy must taken consideration data prefetched units single words cache blocks contiguous blocks memory program data objects often amount data fetched determined organization underlying cache memory system cache blocks may appropriate size uniprocessors smps larger memory blocks may used amortize cost initiating data transfer across interconnection network large distributed memory multiprocessor three questions independent example prefetch destination small processor cache data must prefetched way minimizes possibility polluting cache means precise prefetches need scheduled shortly actual use prefetch unit must kept small prefetch destination large timing size constraints relaxed prefetch mechanism specified natural wish compare schemes unfortunately comparative evaluation various proposed prefetching techniques hindered widely varying architectural assumptions testing procedures however general observations made majority prefetching schemes studies concentrate numerical arraybased applications programs tend generate memory access patterns although comparatively predictable yield high cache utilization therefore benefit prefetching general applications result automatic techniques effective general programs remain largely unstudied effective prefetch mechanism must perform well common types memory referencing patterns scalar unitstride array references typically dominate applications prefetching mechanisms capture type access pattern sequential prefetching techniques concentrate exclusively access patterns although comparatively infrequent large stride array referencing patterns result poor cache utilization rpt mechanisms sacrifice scalar performance order cover strided referencing patterns software prefetching handles types referencing patterns introduces instruction overhead integrated schemes attempt reduce instruction overhead still offering better prefetch coverage pure hardware techniques finally memory systems must designed match added demands prefetching imposes despite reduction overall execution time prefetch mechanisms tend increase average memory latency result effectively increasing memory reference request rate processor thereby introducing congestion within memory system particularly problem multiprocessor systems buses interconnect networks shared several processors despite application system constraints data prefetching techniques produced significant performance improvements commercial systems efforts improve extend known techniques diverse architectures applications active promising area research need new prefetching techniques likely continue motivated increasing memory access penalties arising widening gap microprocessor memory performance use complex memory hierarchies 8 r performance evaluation computing systems memory hierarchies effective onchip preloading scheme reduce data access penalty compiler techniques data prefetching powerpc software prefetching design hp pa 7200 cpu effective programmable prefetch engine onchip caches effective hardwarebased data prefetching high performance processors performance study software hardware data prefetching schemes data access microarchitectures superscalar processors compilerassisted data prefetching effectiveness hardwarebased stride sequential prefetching sharedmemory multiprocessors fixed adaptive sequential prefetching sharedmemory multiprocessors data prefetching multiprocessor vector cache memories stride directed prefetching scalar processors compilerdirected data prefetching multiprocessors memory hierarchies integrated hardwaresoftware scheme sharedmemory multiprocessors comparative evaluation latency reducing tolerating techniques data prefetch mechanism accelerating general computation prefetching using markov predictors improving directmapped cache performance addition small fullyassociative cache prefetch buffers architecture softwarecontrolled data prefetching lockupfree instruction fetchprefetch cache organization spaid software prefetching pointer callintensive environments compilerbased prefetching recursive data structures tolerating latency softwarecontrolled prefetching sharedmemory multiprocessors design evaluation compiler algorithm prefetching cray t3e architecture overview evaluating stream buffers secondary cache replacement exposing io concurrency informed prefetching software methods improvement cache performance supercomputer applications performance impact block sizes fetch strategies data prefetching hp pa8000 prefetch unit vector operations scalar computers sequential program prefetching memory hierarchies cache memories caches enough data prefetching techniques hiding memory latency data prefetch engine mips r10000 superscalar microprocessor intelligent icache prefetch mechanism speeding irregular applications sharedmemory multiprocessors memory binding group prefetching tr software prefetching tolerating latency softwarecontrolled prefetching sharedmemory multiprocessors architecture softwarecontrolled data prefetching data prefetching multiprocessor vector cache memories data access microarchitectures superscalar processors compilerassisted data prefetching effective onchip preloading scheme reduce data access penalty prefetch unit vector operations scalar computers abstract design evaluation compiler algorithm prefetching directed prefetching scalar processors cache coherence largescale sharedmemory multiprocessors evaluating stream buffers secondary cache replacement performance study software hardware data prefetching schemes speeding irregular applications sharedmemory multiprocessors compiler techniques data prefetching powerpc effective programmable prefetch engine onchip caches compilerbased prefetching recursive data structures compilerdirected data prefetching multiprocessors memory hierarchies prefetching using markov predictors data prefetching hp pa8000 dependence based prefetching linked data structures performance impact block sizes fetch strategies improving directmapped cache performance addition small fullyassociative cache prefetch buffers cache memories exposing io concurrency informed prefetching caches arent enough mips r10000 superscalar microprocessor limited bandwidth affect processor design effective hardwarebased data prefetching highperformance processors branchdirected stridebased data cache prefetching lockupfree instruction fetchprefetch cache organization study branch prediction strategies effectiveness hardwarebased stride sequential prefetching sharedmemory multiprocessors distributed prefetchbuffercache design high performance memory systems software methods improvement cache performance supercomputer applications ctr nathalie drach jeanluc bchennec olivier temam increasing hardware data prefetching performance using secondlevel cache journal systems architecture euromicro journal v48 n45 p137149 december 2002 alexander gendler avi mendelson yitzhak birk pabbased multiprefetcher mechanism international journal parallel programming v34 n2 p171188 april 2006 addressing mode driven low power data caches embedded processors proceedings 3rd workshop memory performance issues conjunction 31st international symposium computer architecture p129135 june 2020 2004 munich germany binny gill dharmendra modha sarc sequential prefetching adaptive replacement cache proceedings usenix annual technical conference 2005 usenix annual technical conference p3333 april 1015 2005 anaheim ca ismail kadayif mahmut kandemir guilin chen studying interactions prefetching cache line turnoff proceedings 2005 conference asia south pacific design automation january 1821 2005 shanghai china ismail kadayif mahmut kandemir feihui li prefetchingaware cache line turnoff saving leakage energy proceedings 2006 conference asia south pacific design automation january 2427 2006 yokohama japan aviral shrivastava eugene earlie nikil dutt alex nicolau aggregating processor free time energy reduction proceedings 3rd ieeeacmifip international conference hardwaresoftware codesign system synthesis september 1921 2005 jersey city nj usa vladmihai panait amit sasturkar wengfai wong static identification delinquent loads proceedings international symposium code generation optimization feedbackdirected runtime optimization p303 march 2024 2004 palo alto california resit sendag ying chen david j lilja impact incorrectly speculated memory operations multithreaded architecture ieee transactions parallel distributed systems v16 n3 p271285 march 2005 franoise fabret h arno jacobsen franois llirbat joo pereira kenneth ross dennis shasha filtering algorithms implementation fast publishsubscribe systems acm sigmod record v30 n2 p115126 june 2001 jike cui mansur h samadzadeh new hybrid approach exploit localities lrfu adaptive prefetching acm sigmetrics performance evaluation review v31 n3 p3743 december jean christophe beyler philippe clauss performance driven data cache prefetching dynamic software optimization system proceedings 21st annual international conference supercomputing june 1721 2007 seattle washington wang kuanching li kuojen wang ssuhsuan lu design implementation effective prefetch strategy dsm systems journal supercomputing v37 n1 p91112 july 2006 kenneth ross conjunctive selection conditions main memory proceedings twentyfirst acm sigmodsigactsigart symposium principles database systems june 0305 2002 madison wisconsin weidong shi hsienhsin lee mrinmoy ghosh chenghuai lu alexandra boldyreva high efficiency counter mode security architecture via prediction precomputation acm sigarch computer architecture news v33 n2 p1424 may 2005 brendon cahoon kathryn mckinley simple effective array prefetching java proceedings 2002 joint acmiscope conference java grande p8695 november 0305 2002 seattle washington usa kenneth ross selection conditions main memory acm transactions database systems tods v29 n1 p132161 march 2004 luis ramos jos luis briz pablo e ibez victor vials data prefetching cache hierarchy high bandwidth capacity proceedings 2006 workshop memory performance dealing applications systems architectures p3744 september 1620 2006 seattle washington sangyeun cho lei jin managing distributed shared l2 caches oslevel page allocation proceedings 39th annual ieeeacm international symposium microarchitecture p455468 december 0913 2006 zhen yang xudong shi feiqi su jihkwon peir overlapping dependent loads addressless preload proceedings 15th international conference parallel architectures compilation techniques september 1620 2006 seattle washington usa gokul b kandiraju anand sivasubramaniam going distance tlb prefetching applicationdriven study acm sigarch computer architecture news v30 n2 may 2002 david siegwart martin hirzel improving locality parallel hierarchical copying gc proceedings 2006 international symposium memory management june 1011 2006 ottawa ontario canada alireza adltabatabai richard l hudson mauricio j serrano sreenivas subramoney prefetch injection based hardware monitoring object metadata acm sigplan notices v39 n6 may 2004 jun yan wei zhang hybrid multicore architecture boosting singlethreaded performance acm sigarch computer architecture news v35 n1 p141148 march 2007 weidong shi hsienhsin lee accelerating memory decryption authentication frequent value prediction proceedings 4th international conference computing frontiers may 0709 2007 ischia italy trishul chilimbi martin hirzel dynamic hot data stream prefetching generalpurpose programs acm sigplan notices v37 n5 may 2002 jacob leverich hideho arakida alex solomatnikov amin firoozshahian mark horowitz christos kozyrakis comparing memory systems chip multiprocessors acm sigarch computer architecture news v35 n2 may 2007 mahjur h jahangir h gholamipour performance trace locality reference performance evaluation v60 n14 p5172 may 2005 kristof beyls erik h dhollander generating cache hints improved program efficiency journal systems architecture euromicro journal v51 n4 p223250 april 2005