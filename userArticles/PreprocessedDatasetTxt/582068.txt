using scalea performance analysis distributed parallel programs paper give overview scalea new performance analysis tool openmp mpi hpf mixed paralleldistributed programs scalea instruments executes measures programs computes variety performance overheads based novel overhead classification source code hwprofiling combined single system significantly extends scope possible overheads measured examined ranging hwcounters number cache misses floating point operations complex performance metrics control loss parallelism moreover scalea uses new representation code regions called dynamic code region call graph enables detailed overhead analysis arbitrary code regions instrumentation description file used relate performance information code regions input program reduce instrumentation overhead several experiments realistic codes cover mpi openmp hpf mixed openmpmpi codes demonstrate usefulness scalea b introduction hybrid architectures eg smp clusters become mainstay distributed parallel processing market computing community busily developing languages software tools machines besides openmp 27 mpi 13 hpf 15 mixed programming paradigms openmpmpi increasingly evaluated paper introduce new performance analysis system scalea distributed parallel programs covers mentioned programming paradigms scalea based novel classica tion performance overheads shared distributed memory parallel programs includes data move ment synchronization control parallelism additional computation loss parallelism unidentied heads scalea among rst performance analysis tools combines source code hw proling single system signicantly extending scope possible overheads measured examined include use hw counters cache analysis complex performance metrics control loss parallelism specic instrumentation performance analysis conducted determine category overhead individual code regions instrumentation done fully automatically usercontrolled directives postexecution performance analysis done based performance traceles novel representation code regions named dynamic code region call graph drg drg ects dynamic relationship code regions subregions enables detailed overhead analysis every code gion drg restricted function calls also covers loops io communication statements etc moreover allows arbitrary code regions alyzed code regions vary single statement entire program unit contrast existing approaches frequently use call graph considers function calls prototype scalea implemented present several experiments realistic programs including molecular dynamics application openmp version nancial modeling hpf openmpmpi versions material science code mpi version demonstrate usefulness scalea rest paper structured follows section describes overview scalea 24 section 3 present novel classication performance overheads based scalea instruments code analyses performance dynamic code region call graph described next section experiments shown section 5 related work outlined section 6 conclusions future work discussed section 7 scalea postexecution performance tool struments measures analyses performance behavior distributed memory shared memory mixed parallel programs figure 1 shows architecture scalea consists two main components scalea instrumentation system sis post execution performance analysis tool set sis integrated vfc 3 compiler translates fortran programs mpi openmp hpf mixed programs tran90mpi mixed openmpmpi programs input programs scalea processed compiler frontend generates abstract syntax tree ast sis enables user select directives commandline options code regions interest based preselected code regions sis automatically inserts probes code collect relevant performance information set proletrace les execution program target architecture sis also generates instrumentation description le see section 22 enables gathered performance data related back input program reduce instrumentation overhead sis 25 targets performance measurement system based tau performance framework tau integrated toolkit performance instrumentation mea surement analysis parallel multithreaded pro grams tau measurement library provides portable proling tracing capabilities supports access hardware counters sis automatically instruments parallel programs vfc using tau instrumentation library builds abstract syntax tree vfc tau measurement system create dynamic code region call graph see section 4 main functionality sis given follows automatic instrumentation predened code regions loops procedures io statements hpf independent loops openmp parallel loops openmp sections mpi sendreceive etc various performance overheads using commandline options manual instrumentation sis directives inserted program directives also allow dene user dened code regions instrumentation control instrumentation overhead size performance data gathered execution program postexecution analysis sisprofile sisoverhead pprof racy vampir dynamic code region call graph raw performance data instrumentation description file preprocessing profiletrace files compilation linking instrumented code automatic instrumentation sis manual instrumentation input program fortran mpi program openmp program hybrid program executable program users instrumentation control execution sis runtime system sisprofiling tausis papi load imbalance timing result hardware counter result visualization racy vampir target machine subselect select command visualize performance data intermediate database training sets database performance database data repository data object physical resource data processing data flow external input control control flow diagram legend figure 1 architecture scalea manual instrumentation turn ono proling given code region preprocessing phase scalea lters extracts relevant performance information prolestrace les yields ltered performance data dynamic code region call graph drg drg ects dynamic relationship code regions subregions used precise overhead analysis every individual subregion contrast existing approaches based conventional call graph considers function calls code regions postexecution performance analysis also employs training set method determine specic information eg time penalty every cache miss head overhead probes time access lock etc every target machine interest following describe scalea instrumentation system instrumentation description le details sis scaleas postexecution performance analysis found 24 25 21 scalea instrumentation system based userprovided commandline options di rectives sis inserts instrumentation code program collect performance data inter est sis supports programmer control prol ingtracing generate performance data selective instrumentation specic code region types loops procedures io statements hpf independent loops openmp parallel loops openmp sections openmp critical mpi barrier state ments etc sis also enables instrumentation arbitrary code regions finally instrumentation turned specic instrumentation directive order measure arbitrary code regions sis provides following instrumentation code region directive sis cr begin sis cr end must respectively inserted programmer region starts nishes note several entry exit nodes code gion appropriate directives must inserted idf entry description id code region identier type code region types le source le identier unit program unit identier encloses region line start line number region starts column start column number starts line end line number ends column end column number ends performance data performance data collected computed region aux auxiliary information table 1 contents instrumentation description le idf programmer every entry exit node given code region alternatively compiler analysis used automatically determine entry exit nodes furthermore sis provides specic directives order control tracingproling directives measure enable measure disable allow programmer turn tracingproling program code region instance following example instruments portion openmp pricing code version see section 52 sake demonstration call function random path measured using facilities control prolingtracing mentioned end note sis directives inserted programmer based scalea automatically instruments code 22 instrumentation description file crucial aspect performance analysis relate performance information back original input program instrumented program sis generates instrumentation description le idf correlates proling trace overhead information corresponding code regions idf maintains every instrumented code region variety information see table 1 code region type describes type code gion instance entire program outermost loop read statement openmp section openmp parallel loop mpi barrier etc program unit corresponds subroutine function encloses code region idf entry performance data actually link separate repository stores information note information stored idf actually made runtime data structure compute performance overheads properties execution program idf also helps keep instrumentation code minimal every probe insert single identier allows relate associated probe timer counter corresponding code region 3 classication temporal according amdahls law 1 theoretically best sequential algorithm takes time nish program p time required execute parallel version p processors temporal overhead parallel program dened ects dierence achieved optimal parallelization divided u overhead identied u overhead fraction could analyzed detail theory never negative implies speedup p never exceed p 16 ever practice occurs temporal overhead become negative due super linear speedup applica tions eect commonly caused increased available cache size figure 2 give classication temporal overheads based performance analysis scalea conducted temporal overheads data movement local memory access remote memory access level 2 level 1 level 3 level 2 level n level sendreceive putget synchronisation barriers locks conditional variable control parallelism scheduling inspector executor forkjoin additional computation algorithm change compiler change frontend normalization loss parallelism unparallelised code replicated code partial parallelised code unidentified figure 2 temporal overheads classication data movement corresponds data transfer within single address space process local memory access processes remote memory access synchronization eg barriers locks used coordinate processes threads accessing data maintaining consistent computations data etc control parallelism eg forkjoin operations loop scheduling used control manage parallelism program caused runtime library user compiler operations additional computation ects change original sequential program including algorithmic compiler changes increase parallelism eg eliminating data dependences data locality eg changing data access patterns loss parallelism due imperfect parallelization program classied follows unparallelized code executed one processor replicated code executed proces sors partially parallelized code executed one processors unidentied overhead corresponds overhead covered categories note mentioned classication stimulated 6 diers several respects 6 synchronization part information movement load imbalance separate overhead local remote memory merged single overhead class loss parallelism split two classes unidentied overhead considered load imbalance opinion overhead represents performance property caused one overheads 4 dynamic code region call graph every program consists set code regions range single statement entire program unit code region respectively entered exited multiple entry exit control ow points see figure 3 cases however code regions singleentry singleexit code regions order measure execution behavior code region instrumentation system detect entry exit nodes code region insert probes nodes basically task done support compiler guided manual insertion directives figure 3 shows example code region entry exit nodes select arbitrary code region user respectively marks two statements entry exit statements time entry exit nodes code region eg using sis directives 25 compiler analysis sis automatically tries determine entry exit nodes code region node represents statement program figure 3 shows example code region multiple entry exit nodes instrumentation tries detect nodes automatically inserts probes entry exit nodes respectively code regions overlapping scalea currently support instrumentation overlapped code gions current implementation scalea supports mainly instrumentation singleentry multipleexit code regions enhance sis support also multipleentry multipleexit code regions 41 dynamic code region call graph scalea set predened code regions classied common eg program procedure loop function call statement programming paradigm specic code regions mpi calls hpf independent loops openmp parallel regions loops sections etc moreover sis provides directives dene arbitrary code regions see section 21 input program based code regions dene new data structure called dynamic code region call graph drg dynamic code region call graph drg program q dened directed ow graph set nodes r set edges e node r 2 r represents code region executed least runtime q edge r indicates code region r 2 called inside r 1 execution q r 2 dynamic subregion r 1 rst code region executed execution q dened drg used key data structure conduct detailed performance overhead analysis scalea notice timing overhead code region r explicitly instrumented subregions r 1 r n given r timing overhead explicitly instrumented code region r 1 n start r correspond overhead beginning eg fork threads redistribute data etc end join threads barrier synchronization process reduction operation etc r remain corresponds code regions explicitly instrumented entry point statement begins selected code region statement ends selected code region exit point statement control flow control flow exit point entry point figure 3 code region several entry exit points however easily compute remain region r instrumented well figure 4 shows excerpt openmp code together associated drg call graph techniques widely used performance analysis tools vampir 20 gprof 11 10 cxperf 14 support call graph shows much time spent function children 7 call graph used improve search strategy automated performance diagnosis however nodes call graph tools represent function calls 10 14 contrast drg denes node arbitrary code region eg function function call loop statement etc integerx print input n read n call 0 call i1n a1 end call call sisfstart5 call sisfstop5 call end program r 4 r 5 r 4 r 5 figure 4 openmp code excerpt drg 42 generating building dynamic code region call graph calling code region r 2 inside code region r 1 execution program establishes parentchildren relationship r 1 r 2 instrumentation library capture relationships maintain execution program code region r 2 called inside r 1 data entry representing relationship r 1 r 2 generated stored appropriate prolestrace les code region r encountered isnt child code region eg code region executed rst abstract code region assigned parent every code region unique identier included probe inserted sis stored instrumentation description le drg data structure maintains information code regions instrumented executed every thread process build maintain subdrg executing preprocessing phase cf figure 1 drg application built based individual subdrgs threads subdrg thread computed processing prolestrace les contain performance data thread algorithm generating drgs described detail 24 figure 5 execution times md application 5 experiments implemented prototype scalea controlled commandline options user directives code regions including arbitrary code regions selected specic sis directives inserted input program temporal performance overheads according classication shown figure 2 selected commandline options visualization capabilities currently restricted textual put plan build graphical user interface end 2001 graphical output except tables following experiments generated manually information use sis postexecution analysis found 25 24 overhead 2cpus 3cpus 4cpus loss parallelism 0025 0059 0066 control parallelism 1013 0676 0517 synchronization 1572 127 0942 total execution time 146754 98438 74079 table 2 overheads sec md application identied unidentied total overhead respectively section present several experiments demonstrate usefulness scalea experiments conducted gescher 23 smp cluster 6 smp nodes connected fasteth ernet comprises 4 intel pentium iii xeon 700 mhz cpus 1mb fullspeed l2 cache 2gbyte ecc ram intel pro100fast ethernet ul tra160 36gb hard disk run linux 2218smp patched perfctr hardware counters performance use mpich 12 pgf90 compiler version 33 portland group inc 51 molecular dynamics md applica tion md program implements simple molecular dynamics simulation continuous real space program obtained 27 implemented openmp program written bill magro kuck associates inc kai performance md application measured single smp node gescher figure 5 table 2 show execution time behavior measured overheads respectively results demonstrate good speedup behavior nearly linear see table 2 total overhead small large portions temporal overhead identied time sequential code regions unparal lelized doesnt change always executed figure l2 cache missescache accesses ratio omp regions md application one processor loss parallelism unparallelized code region r program q dened r processors used execute q r sequential execution time r increasing p easily shown loss parallelism increases well also conrmed measurements shown table 2 control parallelism mostly caused loop scheduling actually decreases increasing number processors possible explanation eect larger number processors master thread processes less loop scheduling phases smaller number processors load balancing improves increasing number processorsthreads one smp node time decreases synchronization time examine cache miss ratio dened number l2 cache misses divided number l2 cache accesses two important omp code regions namely omp compute omp update shown figure 6 ratio nearly using single processor implies good cache behavior sequential execution code data seem l2 cache case however parallel version cache miss ratio increases substantially threads process data global arrays kept private l2 caches cache coherency protocol causes many cache lines figure 7 execution times hpf openmpmpi version backward pricing application exchanged private caches induces cache misses unclear however master thread considerably higher cache miss ratio threads overall cache behavior little impact speedup code 52 backward pricing application backward pricing code 8 implements backward induction algorithm compute price interest rate dependent nancial product variable coupon bond two parallel code versions created first hpf version exploits data parallelism compiled mpi program second mixed version combines hpf openmp latter version vfc generates openmpmpi program hpf directives used distribute data onto set smp nodes within node openmp program executed communication among smp nodes realized mpi calls execution times versions shown figure 7 term legend denotes entire program whereas loop refers main computational loops hpf independent loop openmp parallel loop version 1 2 spectively hpf version performs worse openmpmpi version shows almost linear speedup 2 nodes overall 8 processors tables 3 5 display overheads hpf mixed openmpmpi version respectively cases largest overhead caused control parallelism overhead rises signicantly hpf version increasing number nodes eect less severe openmpmpi version order nd cause high control parallelism overhead use scalea determine individual components overhead see tables 4 6 two routines update halo mpi init mainly responsible high control parallelism overhead version update halo updates overlap areas distributed arrays causes communication one process requires data owned another process dierent node mpi init initializes mpi runtime system also involves communication version implies much higher overhead two routines compared openmpmpi reason employs separate process every cpu smp node whereas openmpmpi version uses one process per node 53 lapw0 lapw0 4 material science program calculates eective potential kohnsham eigenvalue problem lapw0 implemented fortran mpi code run across several smp nodes pgf90 compiler takes care exchanging data processors within across smp nodes used scalea localize important code regions lapw0 subdivided sequentialized code regions fft rean0 fft rean3 fft rean4 parallelized code regions interstitial potential loop 50 energy output execution time behavior speedups based sequential execution time code region code regions shown figures 8 9 respectively lapw0 examined problem size 36 atoms distributed onto processors set smp nodes clearly using 8 16 24 processors cant reach optimal load balance whereas processors display much better load imbalance eect conrmed scalea see figure computationally intensive routines lapw0 interstitial potential loop 50 scales poorly due load imbalances large overheads due loss parallelism data move ment synchronization see table 7 lapw0 uses many blas scalapack library calls currently instrumented scalea reason large fraction unidentied overhead see table 7 main sources control parallelism overhead caused mpi init see figure 10 scalea also discovered main subroutines cause loss parallelism overhead fft rean0 fft rean3 fftp rean4 sequentialized 6 related work paraver 26 performance analysis tool openmpmpi tools dynamically instruments binary codes determines various performance param eters tool cover range performance overheads supported scalea moreover tools use dynamic interception mechanisms commonly problems relate performance data back input program ovaltine 2 measures analyses variety performance overheads fortran77 openmp programs paradyn 18 automatic performance analysis tool uses dynamic instrumentation searches performance bottlenecks based specication language function call graph employed improve performance tuning 7 recent work openmp performance interface 19 based directive rewriting similarities sis instrumentation approach scalea scala 9 predecessor system scalea implementation interface eg performance measurement library tau allows proling tracing per formed conceivably interface could used generate performance data rest scalea system could analyze tau 22 17 performance framework integrated toolkit performance instrumentation mea surement analysis parallel multithreaded pro grams scalea uses tau instrumentation library one tracing libraries papi 5 species standard api accessing hardware performance counters available modern mi croprocessors scalea uses papi library measuring hardware counters gprof 11 10 compilerbased proling framework mostly analyses execution behavior counts functions function calls vampir 20 performance analysis tool processes trace les generated vampirtrace 21 supports various performance displays including timelines statics visualized together call graphs source code 7 conclusions future work paper described scalea performance analysis system distributed parallel pro grams scalea currently supports performance analysis openmp mpi hpf mixed parallel programs eg openmpmpi scalea based novel classication performance overheads shared distributed memory parallel programs scalea among rst performance analysis tools combines source code hw proling single system signicantly extends scope possible overheads measured examined specic instrumentation performance analysis conducted determine category overhead individual code regions instrumentation done fully automatically usercontrolled directives postexecution performance analysis done based performance traceles novel representation code regions named dynamic code region call graph drg drg ects dynamic relationship code regions subregions enables detailed overhead analysis every code region drg restricted function calls also covers loops io communication statements etc allows analyze arbitrary code regions vary single statement entire program unit processors sequential 1n 1p 1n 4p 2n 4p 3n 4p 4n4p 5n4p 6n4p data movement control parallelism 0 0244258 659928 172419 289781 414966 564554 707302 tu 3139742 1726465 1835957 2047059 23549 299739 25173 3384 833775 1910787 310459 438749 5948315 732829 total execution time 316417 319801 87442 5866 57414 63651 75304 86467 table 3 overheads hpf version backward pricing application u identied unidentied total overhead respectively 1n 4p means 1 smp node 4 processors processors 1n 1p 1n 4p 2n 4p 3n 4p 4n4p 5n4p 6n4p inspector work distribution 0000258 0000285 update halo 0149 3114 9110 16170 24060 33868 43830 mpi init 0005 3462 8113 12784 17420 22568 26860 table 4 control parallelism overheads hpf version backward pricing application contrast existing approaches frequently use call graph considers function calls based prototype implementation scalea presented several experiments realistic codes implemented mpi hpf mixed openmpmpi experiments demonstrated usefulness scalea nd performance problems causes currently integrating scalea database store derived performance data moreover plan enhance scalea performance specication language order support automatic performance bottleneck analysis sisprofiling measurement library drg overhead proling extension taus prol ing capabilities hope integrate features future releases tau performance system features oered portably instrumentation tools access api r validity single processor approach achieving large scale computing capabili ties automatic overheads pro vfc vienna fortran compiler scalable crossplatform infrastructure application performance tuning using hardware counters hierarchical classi callgraphbased search strategy automated performance diagnosis pricing constant maturity floaters embeeded options using monte carlo simulation gnu gprof call graph execution pro mpi standard message pass ing cxperf users guide high performance fortran forum introduction parallel comput ingdesign analysis parallel algorithms performance technology complex parallel distributed sys tems towards performance tool interface openmp approach based directive rewriting vampir visualization analysis mpi resources vampirtrace 20 installation users guide portable pro gescher system scalea version 10 users guide tr introduction parallel computing highperformance portable implementation mpi message passing interface standard portable profiling tracing parallel scientific applications using c executiondriven performance analysis distributed parallel systems performance technology complex parallel distributed systems scalable crossplatform infrastructure application performance tuning using hardware counters paradyn parallel performance measurement tool callgraphbased search strategy automated performance diagnosis distinguished paper hierarchical classification overheads parallel programs mpi standard message passing gprof ctr thomas fahringer clvis seragiotto jnior modeling detecting performance problems distributed parallel programs javapsl proceedings 2001 acmieee conference supercomputing cdrom p3535 november 1016 2001 denver colorado ming wu xianhe sun grid harvest service performance system grid computing journal parallel distributed computing v66 n10 p13221337 october 2006