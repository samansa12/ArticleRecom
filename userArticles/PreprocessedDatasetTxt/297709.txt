automatic compilerinserted prefetching pointerbased applications abstractas disparity processor memory speeds continues grow memory latency becoming increasingly important performance bottleneck softwarecontrolled prefetching attractive technique tolerating latency success limited thus far arraybased numeric codes paper expand scope automatic compilerinserted prefetching also include recursive data structures commonly found pointerbased applicationswe propose three compilerbased prefetching schemes automate widely applicable scheme greedy prefetching optimizing research compiler experimental results demonstrate compilerinserted prefetching offer significant performance gains uniprocessors largescale sharedmemory multiprocessors b introduction oftware controlled data prefetching 1 2 offers potential bridging everincreasing speed gap memory subsystem todays highperformance processors recognition potential number recent processors added support prefetch instructions 3 4 5 prefetching enjoyed considerable success arraybased numeric codes 6 potential pointerbased applications remained largely unexplored paper investigates compilerinserted prefetching pointerbased applicationsin par ticular containing recursive data structures recursive data structures rdss include familiar objects linked lists trees graphs etc individual nodes dynamically allocated heap nodes linked together pointers form overall structure purposes recursive data structures broadly interpreted include pointerlinked data structures eg mutuallyrecursive data structures even graph heterogeneous objects memory performance perspective pointerbased data structures expected important concern following reasons application suffer large memory penalty due data replacement misses typically must large data set relative cache size aside multidimensional arrays recursive data structures one common convenient methods building large data structures eg btrees database applications octrees graphics applications etc traverse ck luk department computer science university toronto toronto ontario m5s 3g4 canada email lukeecgtorontoedu c mowry computer science department carnegie mellon university pittsburgh pa 15213 email tcmcscmuedu large rds may potentially visit enough intervening nodes displace given node cache revisited hence temporal locality may poor finally contrast arrayswhere consecutive elements contiguous addressesthere little inherent spatial locality consecutivelyaccessed nodes rds since dynamically allocated arbitrary addresses cope latency accessing pointerbased data structures propose three compilerbased schemes prefetching rdss described section ii implemented widelyapplicable schemesgreedy prefetchingin modern research compiler suif 7 discussed section iii evaluate schemes performed detailed simulations impact uniprocessor multiprocessor systems sections iv v respectively finally present related work conclusions sections vi vii ii softwarecontrolled prefetching rdss key challenge successfully prefetching rdss scheduling prefetches sufficiently far advance fully hide latency introducing minimal runtime overhead contrast arraybased codes prefetching distance easily controlled using software pipelining 2 fundamental difficulty rdss must first dereference pointers compute prefetch addresses getting several nodes ahead rds traversal typically involves following pointer chain however act touching intermediate nodes along pointer chain means cannot tolerate latency fetching one node ahead overcome pointerchasing problem 8 propose three schemes generating prefetch addresses without following entire pointer chain first two schemes greedy prefetching historypointer prefetchinguse pointer within current node prefetching address difference greedy prefetching uses existing point ers whereas historypointer prefetching creates new point ers third schemedatalinearization prefetching generates prefetch addresses without pointer dereferences greedy prefetching kary rds node contains k pointers nodes greedy prefetching exploits fact one k neighbors immediately followed next node traversal often good chance neighbors visited sometime future therefore prefetching k pointers node first visited hope enough preordertreenode f prefetchtleft prefetchtright preordertleft preordertright 4 5partial latency cache miss cache hit cache miss 9 15122 code greedy prefetching b cache miss behavior fig 1 illustration greedy prefetching prefetches successful hide least fraction miss latency illustrate greedy prefetching works consider preorder traversal binary tree ie figure 1a shows code greedy prefetching added assuming computation process takes half long cache miss latency l would want prefetch two nodes ahead fully hide latency figure 1b shows caching behavior node obviously suffer full cache miss root node node 1 since opportunity fetch ahead time however would suffer half miss penalty l visit node 2 miss penalty eventually visit node 3 since time visit subtree rooted node 2 greater l example latency fully hidden roughly half nodes reduced 50 half minus root node greedy prefetching offers following advantages low runtime overhead since additional storage computation needed construct prefetch point ers ii applicable wide variety rdss regardless accessed whether structure modified frequently iii relatively straightforward implement compilerin fact implemented suif compiler describe later section iii main disadvantage greedy prefetching offer precise control prefetching distance motivation next algorithm b historypointer prefetching rather relying existing pointers approximate prefetch addresses potentially synthesize accurate pointers based observed rds traversal pat terns prefetch nodes ahead historypointer prefetching scheme 8 add new pointer called historypointer node n record observed address n id node visited nodes n recent traversal rds subsequent traversals rds prefetch nodes pointed history pointers scheme effective traversal pattern change rapidly time construct historypointers maintain fifo queue length contains pointers last nodes visited visit new node n oldest node queue n igammad ie node visited nodes ear lier hence update historypointer n igammad point n first complete traversal rds historypointers set contrast greedy prefetching historypointer prefetching offers improvement first traversal rds potentially hide latency subsequent traversals historypointer prefetching offers potential advantage improved latency tolerance comes expense execution overhead construct historypointers ii space overhead storing new pointers minimize execution overhead potentially update historypointers less frequently depending rapidly rds structure changes one extreme rds never changes set historypointers problem space overhead potentially worsens caching behavior desire eliminate space overhead altogether motivation next prefetching scheme c datalinearization prefetching idea behind datalinearization prefetching 8 map heapallocated nodes likely accessed close together time contiguous memory locations mapping one easily generate prefetch addresses launch early enough another advantage scheme improves spatial locality major challenge however generate data layout theory one could dynamically remap data even rds initially constructed may result large runtime overheads may also violate program semantics instead easiest time map nodes creation time appropriate either creation order already matches traversal der safely reordered since dynamic remapping expensive impossible scheme obviously works best structure rds changes slowly rds change radically program still behave correctly prefetching improve performance iii implementation greedy prefetching three schemes propose greedy prefetching perhaps widely applicable since rely traversal history information requires additional storage computation construct prefetch ad dresses reasons implemented version greedy prefetching within suif compiler 7 simulate two algorithms hand implementation consists analysis phase recognize rds accesses scheduling phase insert prefetches analysis recognizing rds accesses recognize rds accesses compiler uses type declaration information recognize data objects rdss control structure information recognize objects traversed rds type record type r containing least one pointer points either directly indirectly record type note r restricted type since rdss may struct f int data struct left struct right struct f int struct b kids8 struct c f int j double f rds type b rds type c rds type fig 2 examples types recognized rds types l f list f list n ftree f ktree tn f b c fig 3 examples control structures recognized rds traversals comprised heterogeneous nodes example type declarations figure 2a figure 2b would recognized rds types whereas figure 2c would discovering data structures appropriate types compiler looks control structures used traverse rdss particular compiler looks loops recursive procedure calls new loop iteration procedure invocation pointer p rds assigned value resulting dereference pwe refer recurrent pointer update heuristic corresponds rds codes typically written detect recurrent pointer updates compiler propagates pointer values using simplified less pre cise version earlier pointer analysis algorithms 9 10 figure 3 shows example program fragments compiler treats rds accesses figure 3a l updated lnextnext inside whileloop figure 3b n assigned result function call gn inside forloop since implementation perform interprocedural analysis assumes gn results value nnext figure 3c two dereferences function argument passed parameters two recursive calls figure 3d similar figure 3c except record rather pointer passed function argument ideally next step would analyze data locality across rds nodes eliminate unnecessary prefetches although automated step compiler evaluated potential benefits earlier study 8 b scheduling prefetches rds accesses recognized compiler inserts greedy prefetches follows point rds object traversedie recurrent pointer update occursthe compiler inserts prefetches pointers within object point rdstype objects earliest points addresses available within surrounding loop procedure body availability prefetch addresses computed prop l f l f prefetchlnext loop tree q testtdata else q null tree q prefetchtleft prefetchtright testtdata else q null b procedure fig 4 examples greedy prefetch scheduling benchmark characteristics node recursive data input memory benchmark structures used data set allocated octree bisort binary tree 250000 1535 kb integers em3d singlylinked lists 2000 hnodes 1671 kb 100 enodes 75 local health fourway tree level 5 925 kb doublylinked lists mst array singly 512 nodes 10 kb linked lists perimeter quadtree 4kx4k image 6445 kb power multiway tree 10000 418 kb singlylinked lists customers treeadd binary tree 1024k nodes 12288 kb binary tree 100000 cities 5120 kb doublylinked lists voronoi binary tree 20000 points 10915 kb agating earliest generation points pointer values along values two examples greedy prefetch scheduling shown figure 4 details implementation found luks thesis 11 iv prefetching rdss uniprocessors section quantify impact prefetching schemes uniprocessor performance later section v turn attention multiprocessor systems experimental framework performed detailed cyclebycycle simulations entire olden benchmark suite 12 dynamically scheduled superscalar processor similar mips r10000 5 olden benchmark suite contains ten pointerbased applications written c briefly summarized table rightmost column table shows amount memory dynamically allocated rds nodes simulation model varies slightly actual mips r10000 eg model two memory units ii uniprocessor simulation parameters pipeline parameters issue width 4 functional units 2 int 2 fp 2 memory 1 branch reorder buffer size integer multiply 12 cycles integer divide 76 cycles integer 1 cycle fp divide 15 cycles fp square root 20 cycles fp 2 cycles branch prediction scheme 2bit counters memory parameters primary instr data caches 16kb 2way setassociative unified secondary cache 512kb 2way setassociative line size 32b primarytosecondary miss 12 cycles primarytomemory miss 75 cycles data cache miss handlers 8 data cache banks 2 data cache fill time 4 cycles requires exclusive access main memory bandwidth 1 access per 20 cycles assume functional units fullypipelined model rich details processor including pipeline register renaming reorder buffer branch pre diction instruction fetching branching penalties memory hierarchy including contention etc table ii shows parameters model use pixie 13 instrument optimized mips object files produced com piler pipe resulting trace simulator avoid misses initialization dynamically allocated objects used modified version irix mallopt routine 14 whereby prefetch allocated objects initialized determining prefetch addresses straightforward since objects size typically allocated contiguous memory optimization alone led twofold speedups relative using malloc majority applications particularly frequently allocate small objects b performance greedy prefetching figure 5 shows results uniprocessor experi ments overall performance improvement offered greedy prefetching shown figure 5a two bars correspond cases without prefetching n greedy prefetching g bars represent execution time normalized case without prefetching broken four categories explaining happened potential graduation slots number graduation slots issue width4 case multiplied number cycles bottom section busy number slots instructions actually graduate top two sections nongraduating slots immediately caused oldest instruction suffering either load store miss inst stall section slots instructions graduate note load stall store stall sections firstorder approximation performance loss due cache misses since delays also exacerbate subsequent data dependence stalls see figure 5a half applications enjoy speedup ranging 4 45 half within 2 original performance applications largest memory stall penaltiesie health perimeter treeaddmuch stall time eliminated cases bisort mst prefetching overhead offset reduction memory stalls thus resulting slight performance degradation problem eight applications understand performance results greater depth figure breaks original primary cache misses three categories prefetched subsequently hit primary cache pf hit ii prefetched remain primary misses pf miss iii prefetched nopf miss sum pf hit pf miss cases also known coverage factor ideally 100 em3d power voronoi coverage factor quite low un der 20 misses caused array scalar referenceshence prefetching rdss yields little improvement cases coverage factor 60 four cases achieve nearly perfect coverage pf miss category large indicates prefetches scheduled effectivelyeither issued late hide latency else early prefetched data displaced cache could referenced category prominent mst compiler unable prefetch early enough traversal short linked lists within hash table since greedy prefetching offer little control prefetching distance surprising scheduling imperfectin fact encouraging pf miss fractions low help evaluate costs prefetching figure 5c shows fraction dynamic prefetches unnecessary data found primary cache application show four different bars indicating total dynamic unnecessary prefetches caused static prefetch instructions hit rates given threshold hence bar labeled 100 corresponds unnecessary prefetches whereas bar labeled 99 shows total unnecessary prefetches exclude prefetch instructions hit rates 99 etc breakdown indicates potential reducing overhead eliminating static prefetch instructions clearly little value example eliminating prefetches hit rates 99 would eliminate half unnecessary prefetches perimeter thus decreasing overhead significantly con trast reducing overhead flat distribution eg bh difficult since prefetches sometimes hit also miss least 10 time therefore eliminating may sacrifice latencyhiding benefit found eliminating prefetches hit rates 95 improves performance 17 applications 8 finally measured impact greedy prefetching memory bandwidth consumption observe av 0normalized execution time load stall 1000 966 1000 1012 1000 998 10001000 1016 10001000 999 100010001000 994 bh bisort em3d health mst perimeter power treeadd tsp voronoi store stall inst stall busy execution time original load dcache nopfmiss bisort health perimeter treeadd voronoi bh em3d mst power tsp pfmiss pfhit hit 10099 95 90 10099 95 90 10099 95 90 10099 95 90 10099 95 90 10099 95 90 10099 95 90 10099 95 90 10099 95 90 10099 95 90 bh bisort em3d health mst perimeter power treeadd tsp voronoi b coverage factor c unnecessary prefetches fig 5 performance impact compilerinserted greedy prefetching uniprocessor erage greedy prefetching increases traffic primary secondary caches 127 traffic secondary cache main memory 78 experiments almost impact perfor mance hence greedy prefetching appear suffering memory bandwidth problems summary seen automatic compilerinserted prefetching result significant speedups uniprocessor applications containing rdss investigate whether two sophisticated prefetching schemes offer even larger performance gains c performance historypointer prefetching data linearization prefetching applied historypointer prefetching data linearization prefetching hand several applica tions historypointer prefetching applicable health list structures accessed key procedure remain unchanged across ten thousand times called result historypointer prefetching achieves 40 speedup greedy prefetching better miss coverage fewer unnecessary prefetches although historypointer prefetching fewer unnecessary prefetches greedy prefetching significantly higher instruction overhead due extra work required maintain historypointers datalinearization prefetching applicable perimeter treeadd creation order identical major subsequent traversal order cases result data linearization require changing data layout cases hence spatial locality unaffected reducing number unnecessary prefetches hence prefetching overhead maintaining good coverage factors datalinearization prefetching results speedups 9 18 greedy prefetching perimeter treeadd respectively overall see schemes potentially offer significant improvements greedy prefetching applicable v prefetching rdss multiprocessors observed benefits automatic prefetching rdss uniprocessors investigate whether compiler also accelerate pointerbased applications running multiprocessors earlier studies mowry demonstrated compiler successfully prefetch parallel matrixbased codes 2 15 compiler used studies attempt prefetch pointerbased access patterns however handinserted prefetch ing mowry able achieve significant speedup barnes 15 pointerintensive sharedmemory parallel application splash suite 16 barnes performs hierarchical nbody simulation evolution galaxies main computation consists depthfirst traversal octree structure compute gravitational force exerted given body bodies tree repeated body system bodies statically assigned processors duration time step cache misses occur whenever processor visits part octree already cache either due replacements communication insert prefetches hand mowry used strategy similar greedy prefetching upon first arriving node prefetched immediate children descending depthfirst first child iii memory latencies multiprocessor simulations destination access read write primary cache 1 cycle 1 cycle secondary cache 15 cycles 4 cycles remote node 101 cycles 89 cycles dirty remote remote home 132 cycles 120 cycles normalized execution time memory stalls synchronization instructions86 85 original dcache nopfmiss pfmiss pfhit hit dcache execution b coverage c unnecessary time factor prefetches fig 6 impact compilerinserted greeding prefetching barnes multiprocessor compilerinserted greedy prefetching handinserted prefetching evaluate performance compilerbased implementation greedy prefetching multiprocessor compared handinserted prefetching barnes sake comparison adopted simulation environment used mowrys earlier study 15 briefly summarize simulated cachecoherent sharedmemory multiprocessor resembles dash multiprocessor 17 simulated machine consists 16 processors two levels directmapped caches using 16 byte lines table iii shows latency servicing access different levels memory hierarchy absence contention simulations model contention however make simulations fea sible scaled problem size cache sizes accordingly ran 8192 bodies 3 times steps 8k64k cache hierarchy done explained detail original study 2 figure 6 shows impact compilerinserted greedy prefetching g handinserted prefetching h barnes execution times figure 6a broken follows bottom section amount time spent executing instructions including prefetching instruction overhead middle top sections synchronization memory stall times respectively see figure 6a compiler achieves nearly identical performance handinserted prefetching compiler prefetches 90 original cache misses 15 misses unnecessary see figures 6b 6c respectively prefetched misses latency fully hidden half cases pf hit partially hidden cases pf miss eliminating roughly half original memory stall time compiler able achieve 16 speedup compilers greedy strategy inserting prefetches quite similar done hand following exception effort minimize unnecessary prefetches compilers default strategy prefetch first 64 bytes within given rds node case barnes nodes longer 64 bytes discovered handinserted prefetching achieves better performance prefetch entire nodes case improved miss coverage prefetching entire nodes worth additional unnecessary prefetches thereby resulting 1 speedup compilerinserted prefetching overall however quite pleased compiler able well nearly matching best performance could achieve hand vi related work although prefetching studied extensively arraybased numeric codes 6 18 relatively little work done nonnumeric applications chen et al 19 used global instruction scheduling techniques move address generation back early possible hide small cache miss latency 10 cycles found mixed results contrast algorithms focus rds accesses issue prefetches much earlier across procedure loop iteration boundaries overcoming pointerchasing problem zhang torrellas 20 proposed hardwareassisted scheme prefetching irregular applications sharedmemory multiprocessors scheme programs annotated bind together groups data eg fields record two records linked pointer prefetched hardware con trol compared compilerbased approach scheme two shortcomings annotations inserted manually ii hardware extensions likely applicable uniprocessors joseph grunwald 21 proposed hardwarebased markov prefetching scheme prefetches multiple predicted addresses upon primary cache miss markov prefetching potentially handle chaotic miss patterns requires considerably hardware support less flexibility selecting prefetch controlling prefetch distance compilerbased schemes knowledge compilerbased pointer prefetching scheme literature spaid scheme proposed lipasti et al 22 based observation procedures likely dereference pointers passed arguments spaid inserts prefetches objects pointed pointer arguments call sites therefore scheme effective interval start procedure call dereference pointer comparable cache miss latency earlier study 8 found greedy prefetching offers substantially better performance spaid hiding latency paying less overhead vii conclusions automatic compilerinserted prefetching shown considerable success hiding memory latency arraybased codes compiler technology successfully prefetching pointerbased data structures thus far lacking paper propose three prefetching schemes overcome pointerchasing problem automate widely applicable scheme greedy prefetching compiler evaluate performance modern superscalar uniprocessor sim ilar mips r10000 largescale sharedmemory multiprocessor uniprocessor experiments show automatic compilerinserted prefetching accelerate pointerbased applications much 45 addition sophisticated algorithms currently simulate hand offer even larger performance gains multiprocessor experiments demonstrate compiler potentially provide equivalent performance handinserted prefetching even parallel ap plications encouraging results suggest latency problem pointerbased codes may addressed largely prefetch instructions already exist many recent microprocessors acknowledgments work supported grant ibm canadas centre advanced studies chikeung luk partially supported canadian commonwealthfellowship todd c mowry partially supported faculty development award ibm r software prefetching tolerating latency softwarecontrolled data prefetching com piler techniques data prefetching powerpc data prefetching hp pa8000 mips r10000 superscalar microprocessor design evaluation compiler algorithm prefetching suif infrastructure research parallelizing optimizing compilers compilerbased prefetching recursive data structures contextsensitive interprocedural pointsto analysis presence function pointers interprocedural modification side effect analysis pointer aliasing optimizing cache performance nonnumeric applications support ing dynamic data structures distributed memory machines tracing pixie fast fits tolerating latency multiprocessors compilerinserted prefetching parallel applications shared memory stanford dash multiproces sor effective onchip preloading scheme reduce data access penalty data access microarchitectures superscalar processors compilerassisted data prefetching speeding irregular applications sharedmemory multiprocessors memory binding group prefetching prefetching using markov predic tors spaid software prefetching pointer callintensive environments tr ctr subramanian ramaswamy jaswanth sreeram sudhakar yalamanchili krishna v palem data trace cache application specific cache architecture acm sigarch computer architecture news v34 n1 march 2006 shimin chen phillip b gibbons todd c mowry improving index performance prefetching acm sigmod record v30 n2 p235246 june 2001 tatsushi inagaki tamiya onodera hideaki komatsu toshio nakatani stride prefetching dynamically inspecting objects acm sigplan notices v38 n5 may evangelia athanasaki nikos anastopoulos kornilios kourtis nectarios koziris exploring performance limits simultaneous multithreading memory intensive applications journal supercomputing v44 n1 p6497 april 2008 chikeung luk tolerating memory latency softwarecontrolled preexecution simultaneous multithreading processors acm sigarch computer architecture news v29 n2 p4051 may 2001