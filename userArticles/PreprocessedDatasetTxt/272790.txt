approximate inverse techniques blockpartitioned matrices paper proposes preconditioning options system matrix blockpartitioned form form may arise naturally example incompressible navierstokes equations may imposed domain decomposition reordering approximate inverse techniques used generate sparse approximate solutions whenever needed forming preconditioner storage requirements preconditioners may much less incomplete lu factorization ilu preconditioners tough largescale computational fluid dynamics cfd problems numerical experiments show preconditioners help solve difficult linear systems whose coefficient matrices highly indefinite b introduction consider block partitioning matrix form 1 blocking naturally occurs due ordering equations variables matrices form arise many applications incompressible navierstokes equations scalar momentum equations continuity condition form separate blocks equations 2d case system form b pc b f u f pc 2 u v represent velocity components p represents pressure b submatrix convectiondiffusion operator f submatrices pressure gradient operators e submatrices velocity divergence operators traditional techniques uzawa algorithm used problems often linear systems must solved much smaller zeros small values diagonal fullycoupled system socalled segregated approaches however suffer slow convergence rates compared aggregated fullycoupled solution techniques another source partitioned matrices form 1 class domain decomposition methods methods interior nodes subdomain ordered consecutively subdomain subdomain followed interface nodes ordered end ordering unknowns gives rise matrices following structure 0 typically linear systems associated b matrix produced reordering easy solve result restricting original pde problem set independent similar pde problems much smaller meshes one motivations approach parallelism approach ultimately requires solution methods schur complement danger however general matrices b may singular reordering much work done exploiting form blocking conjunction preconditioning one earlier papers subject concus golub meurant introduce idea block preconditioning designed blocktridiagonal matrices whose diagonal blocks tridiagonal inverses tridiagonal matrices encountered approximations approximated tridiagonal matrices exploiting exact formula inverse tridiagonal matrix later extended general case diagonal blocks arbitrary 4 17 many cases incomplete block factorizations developed matrices arising discretization pdes 2 3 7 17 19 utilize approximate inverses diagonal blocks need inverted recently elman silvester 13 proposed techniques specific case stokes navierstokes problems number variations block preconditioners also developed 1 9 techniques offblock diagonal terms either neglected attempt made approximate effect paper explores preconditioning options matrix expressed blockpartitioned form either naturally domain decomposition type ordering iterative method acts fullycoupled system preconditioning similarity segregated methods approach requires preconditioning approximate solves submatrices submatrices correspond combination operators reaction diffusion convection particularly advantageous use blockpartitioned form know enough submatrices apply specialized preconditioners example operatorsplitting semidiscretization well lowerorder discretizations blockpartitioned techniques also require sparse approximate solution sparse linear systems solutions need sparse form rows columns preconditioner used computations dense solutions cause construction application preconditioner expensive problem ideally suited sparse approximate inverse techniques approximate solution sparse system found using iterative method implemented sparse matrixsparse vector sparse vectorsparse vector operations intermediate final solutions forced sparse numerically dropping elements x small magnitudes righthand side b initial guess x sparse economical method computing sparse approximate solution used technique construct preconditioners based approximating inverse directly 6 paper organized follows section 2 describe sparse approximate inverse algorithm techniques finding sparse approximate solutions schur complement section 3 describes blockpartitioned factorizations may used preconditioners effective approximate block lu factorization approximate block gaussseidel preconditioner section 4 reports results several numerical experiments including performance new preconditioners problems arising incompressible navierstokes equations sparse approximate inverses use common developing preconditioners based block techniques face need compute approximation inverse sparse matrix approximation columns f b f sparse particularly case block preconditioners blocktridiagonal matrices 7 19 algorithms practical must provide approximations sparse number techniques recently developed construct sparse approximate inverse matrix used preconditioner 5 6 8 10 15 17 18 many techniques approximate row column independently focusing columnoriented case individual minimizations e j jth column identity matrix preconditioner distinctly easier existing preconditioners construct apply massively parallel computer rely matrix factorizations preconditioners often complementary ilu preconditioners 6 22 previous approaches select sparsity pattern x minimize 4 least squares sense approach minimize 4 method reduces residual norm step minimal residual fgmres 20 beginning sparse initial guess sparsity preserved dropping elements search direction current solution step based magnitude criteria related residual norm reduction final number nonzeros column guaranteed parameter lfil case fgmres krylov basis also kept sparse dropping small elements keep iterations economical computations performed sparse matrixsparse vector sparse vectorsparse vector operations application point approximate inverse technique column may generalized find sparse approximate solution sparse linear problem minimizing possibly existing preconditioner 21 approximate inverse algorithm describe modification technique reported 6 guarantees reduction residual norm minimal residual step starting sparse initial guess fillin increased one iteration end iteration possible use second stage exchanges entries solution new entries causes reduction residual norm without second stage entries solution cannot annihilated introduced problems paper however second stage necessary first stage search direction derived dropping entries residual direction r sparsity pattern solution x controlled chosen sparsity pattern x plus one new entry largest entry absolute value minimization performed choosing steplength ad ad thus residual norm new solution guaranteed previous residual norm solution residual updated end stage indefinite normal equations residual direction r may used search direction simply determine location new fillin interesting note largest entry r gives greatest residual norm reduction onedimensional minimization explains transpose initial guess approximate inverse combined selfpreconditioning preconditioning r current approximate inverse effective problems 6 many possibilities second stage choose drop one entry x introduce one new entry causes decrease residual norm candidate dropping smallest absolute nonzero entry x candidate added largest absolute entry previous search direction beginning stage 1 already included previous direction used candidate may determined stage 1 additional search required steplength fi chosen minimizing new residual norm e ith coordinate vector x entry x dropped position fi entry added position l largest generalized notation b righthandside vector previously denoted j let j denote jth column minimization gives involves one sparse saxpy since b gamma ax already available r one sparse dotproduct since may scale columns unit 2norm guaranteed 6 l since l chosen among entries including preconditioned version algorithm minimizing kb gamma axk 2 explicit preconditioner may summarized follows assumed scaled columns unit 2norm number inner iterations usually chosen lfil somewhat larger algorithm 21 approximate inverse algorithm 1 starting initial guess x r b gamma ax 2 3 mr 4 choose pattern x one entry largest remaining entry absolute value 5 q ad 6 ff rq 7 r r gamma ffq 8 x 9 index smallest nonzero absx 10 l index largest nonzero abst gamma 11 fi r 12 r r 13 krk krk 14 set x 0 x l fi 15 r r 16 end 17 end 22 sparse solutions schur complement sparse approximate solutions schur complement often required preconditioning blockpartitioned matrices briefly describe three approaches section 1 approximating 2 approximating gamma1 3 exploiting partial approximate inverse 221 approximating approximate sparse matrix use computed approximate inverse technique possibly preconditioned whatever using solve b since sparse computed way also sparse moreover since usually relatively dense solving economical approach typically zero initial guess used remark usually expensive form solving b approximately dropping small elements since rather costly search elements drop also note generate columnbycolumn necessary compute factorization columnbycolumn basis well linear systems solved fashion including iterative process without preconditioning 222 approximating gamma1 another method compute approximation gamma1 using idea induced pre conditioning 22 block gammas compute sparse approximation using approximate inverse technique applied last blockcolumn throwing away upper block practice upper part column may discarded computing next column experiments since approximate inverse algorithm applied indefinite matrix problems normal equations search direction r used algorithm scaled identity initial guess inverse 223 partial approximate inverse drawback approach top submatrix last blockcolumn discarded resulting approximation gamma1 may actually contain nonzeros related technique compute partial approximate inverse last blockrow technique give approximation gamma1 defines simple preconditioning method writing inverse form get approximate solution f necessary solve accurately b normal equations search direction used approximate inverse algorithm numerical experiments results relatively inexpensive method given section 4 blockpartitioned factorizations consider sparse linear system put block form condition require partitioning b nonsingular use extensively following block lu factorization schur complement wellknown solve 12 solving reduced system compute backsubstitute first blockrow system 11 obtain x ie compute x block structure exploited several different ways define preconditioners thus block preconditioners defined section combine one preconditioners seen section 22 choice block factorization next describe options 31 solving preconditioned reduced system method often used solve reduced system 14 possibly help certain preconditioner schur complement matrix although involve block factorizations discussed indirectly related wellknown algorithms example uzawa method typically formulated full system viewed richardson fixed point iteration applied reduced system matrix need computed explicitly instead one perform matrixvector product matrix via following sequence operations 1 compute 2 solve 3 compute wish use krylov subspace technique gmres preconditioned reduced system need solve systems step 2 exactly ie direct solver iterative solver requiring high accuracy matrix coefficient matrix system solved must constant throughout gmres iteration experimented approach found serious limitation convergence reached number steps typically comparable obtained methods based full matrix however step costs much unless direct solution technique used case initial lu factorization may expensive alternatively highly accurate ilu factorization employed b reduce cost many systems must solved successive outer steps 32 approximate block diagonal preconditioner one simplest block preconditioners matrix partitioned 1 blockdiagonal matrix mc preconditioning matrix c case incompressible navierstokes equations define example interesting particular case c nonsingular mc c corresponds blockjacobi iteration case eigenvalues square roots eigenvalues matrix c convergence fast eigenvalues small 33 approximate block lu factorization block factorization 12 suggests using preconditioners based block lu factor ization precondition preconditioner schur complement matrix sparse approximation schur complement could compute preconditioning matrix example form approximate lu factorization must point preconditioner induce preconditioner discussed section 31 notable disadvantage approach based solving reduced system 14 iterative process action vector must computed accurately krylov acceleration part approach based larger system 11 necessary fact iterative process used solving b provided use flexible variant gmres fgmres 20 systems involving b may solved many ways depending difficulty know b b known wellconditioned triangular solves incomplete lu factors may sufficient difficult b matrices incomplete factors may used preconditioner inner iterative process b incomplete factors unstable see section 42 approximate inverse b may used either directly preconditioner b operator approximation may used factors may used either directly preconditioner kind flexibility typical available using iterative methods blockpartitioned matrices important observation solve exactly b error block ilu factorization lies entirely 22 block since one raise question whether approach better one based solving reduced system 14 preconditioned known fact two approaches mathematically equivalent start proper initial guesses specifically initial guess make xpart residual vector equal 0 original system 11 ie initial guess result due eisenstat reported 16 immediately follows 16 shows preconditioned matrix particular form thus initial residual xcomponent equal zero iterates vectors components gmres iteration system reduce gmres iteration matrix gamma1 involving variable many possible options choosing matrix among consider following ones ffl preconditioning ffl precondition c matrix nonsingular alternatively precondition ilu factorization c construct sparse approximation use preconditioner general need approximate action vector example methods described sections 221 222 following algorithm applies one preconditioning step get algorithm 31 approximate block lu preconditioning 1 x 2 3 x experimented number options solving systems step 2 algorithm example may approximated computed approximate inverse technique approximation used possible also use place b gamma1 f step 3 34 approximate block gaussseidel ignoring u factor approximate block lu factorization led form block gaussseidel preconditioning defined remarks ways solve systems b ways define preconditioning matrix apply algorithm preconditioner algorithm 31 without step 3 analyze preconditioner start observing showing difference preconditioned matrix 17 additional 12 position iterates associated block form associated schur complement approach gamma1 longer simply related however connections 17 19 first spectra two matrices identical mean however two matrices require number iterations converge general consider gmres iteration solve preconditioned system take initial guess form x 0 arbitrary denote preconditioned initial residual gmres find vector u form belonging krylov subspace minimize km arbitrary u affine space preconditioned residual form 19 becomes result ks note ks represents preconditioned residual norm reduced system obtained approximation large system ks implies residual bigger system less ffl residual obtained using full gmres associated preconditioned reduced system also less ffl observe passing second term righthandside 21 always reduced zero postprocessing step consists forcing first part residual zero changing ffi equivalently current pair x obtained x recomputed satisfying first block equation ie postprocessing step requires one additional b solve assume know something residual vector associated steps gmres applied preconditioned reduced system say something residual norm associated preconditioned unreduced system begin establishing simple lemma lemma 31 let following equality holds proof first easy prove multiply members equality gamma z obtain state main result concerning comparison two approaches theorem 31 assume reduced system 14 solved gmres using preconditioner starting arbitrary initial guess 0 let preconditioned residual obtained mth step preconditioned residual vector r m1 obtained 1st step gmres solving block system 11 preconditioned matrix 18 initial guess u x 0 arbitrary satisfies inequality particular proof preconditioned matrix unreduced system form 22 residual vector mth gmres approximation associated reduced system form ae mth residual polynomial minimizes kpgs 0 k 2 among polynomials p degree satisfying constraint consider polynomial degree 1 defined clear residual um1 m1st approximate solution obtained gmres algorithm solving preconditioned unreduced system minimizes pzr 0 polynomials p degree 1 consistent ie therefore using equality established lemma observe first matrix righthandside last equality nothing gamma z hence residual vector r m1 completes proof 2 also interesting relate convergence algorithm blockdiagonal approach particular case case corresponds block gaussseidel iteration exploit young frankels theory 2cyclic matrices compare convergence rates block jacobi approach indeed case 19 therefore eigenvalues matrix squares matrix associated blockjacobi preconditioner section 32 4 numerical experiments section organized follows section 41 describe test problems list methods use section 42 illustrate comparison purposes difficulty incomplete lu factorizations solving problems fullycoupled manner section 43 make comments regard domain decomposition types reorderings section 44 show results new preconditioners simple pde problem finally sections 45 46 present results new preconditioners realistic problems arising incompressible navierstokes equations linear systems constructed solution vector ones zero initial guess rightpreconditioned fgmres 20 restarted every 20 iterations used solve systems tables show number iterations required reduce residual norm 10 gamma7 iterations stopped 300 matrixvector multiplications reached indicated dagger codes written fortran 77 using many routines sparskit 23 run single precision cray c90 supercomputer 41 test problems methods first set test problems finite difference laplace equation dirichlet boundary conditions three different sized grids used matrices reordered using domain decomposition reordering 4 subdomains following tables n order matrix nnz number nonzero entries nb order b submatrix nc order c submatrix second set test matrices extracted example incompressible navierstokes problems fidap 14 package problems zero c submatrix tested case transient problems matrices jacobians newton iterations converged matrices reordered continuity equations grid n nnz nb nc 48 48 2209 10857 2116 93 64 3969 19593 3844 125 table 1 laplacian test problems ordered last scaling many matrices poor since matrix contains different types equations thus scale row unit 2norm scale column way problems originally nonsymmetric except 4 12 14 32 matrix n nnz nb nc hamel flow ex12 3973 79078 2839 1134 stokes flow surface disturbance attenuation ex23 1409 42761 1008 401 fountain flow coating ex26 2163 74465 1706 457 driven thermal convection ex28 2603 77031 1853 750 two merging liquids species deposition radiation heat transfer ex36 3079 53099 2575 504 chemical vapor deposition table 2 fidap example matrices third set test problems finiteelement discretization square lid driven cavity problem rectangular elements used biquadratic basis functions velocities linear discontinuous basis functions pressure show results problems reynolds number 0 500 1000 matrices arise mesh 20 20 elements leading matrices size nnz 138187 nonzero entries matrices 3363 velocity unknowns 1199 pressure un knowns matrices scaled way fidap matricesthe problems otherwise difficult solve use following names denote methods tested ilutnfil ilutpnfil incomplete lu factorization threshold nfil nonzeros per row l u factors preconditioner described section 42 parlfil partial approximate inverse preconditioner described section 223 using lfil nonzeros per row 2 abj approximate blockjacobi preconditioner described section 32 preconditioner applies c 6 0 ablulfil approximate block lu factorization preconditioner described section 33 approximation 6 lfil nonzeros per column used ablu ylfil using whenever b needs applied step 3 algorithm 31 ablu slfil approximate block lu factorization preconditioner using 7 approximate gamma1 lfil nonzeros per column approximating last block column inverse abgslfil approximate block gaussseidel preconditioner described section 34 approximation 6 lfil nonzeros per column used storage requirements preconditioner given table 3 ilut preconditioner described next subsection requires considerably storage approximate blockpartitioned factorizations since storage depends n rather nc approximation gamma1 discards upper block storage less lfil thetan c storage required difficult estimate since least product two sparse matrices generally less 2 theta lfil theta nc table 11 section 45 gives exact number nonzeros fidap problems 42 ilu fullycoupled system wish compare new preconditioners general experi ence one effective generalpurpose preconditioners solving fullycoupled system particular show results ilut dualthreshold incomplete lu factorization preconditioner based droptolerance maximum number new fillin elements allowed per row l u factor latter threshold allows storage preconditioner known beforehand droptolerance ilu rather levelfill ilu often effective indefinite problems numerical values play much important role variant performs column pivoting called ilutp even suitable highly indefinite problems matrices matrix locations abj none none less 2 theta lfil theta nc ablu ylfil lfil theta nc less 2 theta lfil theta nc table 3 storage requirements preconditioner use small modification found often perform better rarely worse matrices wide ranging number elements per row column arises various reasons including fact matrix contains discretization different equations instead counting number new fillins keep nonzeros row l u fixed nfil regardless number original nonzeros row also found better performance keeping nfil constant rather increase decrease factorization progresses highly indefinite large nonsymmetric parts ilu factorization often produces unstable l u factors ie klu gamma1 k extremely large caused long recurrences forward backward triangular solves 11 illustrate point computed number factorizations rough lower bound e vector ones fidap example matrix ex07 modeling natural convection order 1633 46626 nonzeros see table 4 norm bound increases dramatically nfil decreased incomplete factorization gmres could solve linear systems factorizations preconditioner matrix chose striking example solved without preconditioning log table 4 estimate klu gamma1 k1 ilut factors ex07 illustrate difficulty solving fidap problems ilutp progressively allowed fillin problem could solved incrementing nfil multiples 10 drop tolerance results shown table 5 types prob lems typical large amounts fillin must used factorizations successful iterative solution attempted lu condition lower bound greater 10 zero pivot must used ilut ilutp attempt complete factorization using small value proportional norm row matrices taken original banded ordering degrees freedom node element numbered together discussed next subsection type ordering low bandwidth often essential ilutype preconditioningmany problems including cannot solved otherwise matrix nfil ex06 50 table 5 nfil required solve fidap problems ilutp note ilutp occasionally worse ilut alleviated somewhat using low value mbloc parameter ilutp determines far search pivot summary indefinite problems arising incompressible navierstokes equations may tough ilutype preconditioners 43 domain decomposition reordering considerations graph partitioners subdivide domain number pieces used give domain decomposition reordering described section 1 technique impose blockpartitioned structure matrix adapts parallel processing since b blockdiagonal matrix technique also useful b highly indefinite produces unstable lu factorization limiting size factorization instability cannot grow beyond point factorization useful general nonsymmetric matrices partitioner may applied symmetrized graph table 6 show results ilut40 driven cavity problem different matrix reorderings used original unblocked ordering degrees freedom elements ordered together blocked ordering continuity equations ordered last domain decomposition reordering found using simple automatic recursive dissection procedure four subdomains latter ordering found nodes internal subdomains 882 interface nodes unblocked blocked dd ordered 1000 78 51 table effect ordering ilut cavity problems poorer quality incomplete factorization driven cavity problems blockpartitioned form due poor ordering rather instability l u factors fact zero pivots encountered problem reynolds number 0 unblocked format produces 745187 nonzeros strictly lowertriangular part incomplete factorization dropped less ntheta nonzeros blockpartitioned format produces 2195688 nonzeros almost three times factorization domain decomposition reordered matrices encounters many zero pivots reaches 22 block latter orderings necessarily cause ilut fillin zeros diagonal nevertheless substitution small pivot described seems effective domain decomposition reordering also reduces amount fillin shape matrix downward pointing arrow combined tendency limit growth instability results show reordering advantageous even serial computers table 7 compare difficulty solving b subsystems blocked domain decomposition reorderings driven cavity problems computed computed using approximate inverse technique lfil 30 used ilut30 solved linear systems tolerance 10 gamma5 solves submatrices blockpartitioned preconditioners usually need much less accurate experiments follow used unpreconditioned iterations tolerance 10 gamma1 100 matrixvector multiplications solve b methods would necessary depending difficulty problems table gives idea difficult solve b shows advantage using domain decomposition reorderings hard problems blocked dd ordered 1000 7 table 7 solving b different orderings 44 test results laplacian problem tables 8 9 present results laplacian problem three different grid sizes using preconditioning approximate block diagonal partial approximate inverse approximate block lu approximate block gaussseidel preconditioners note table 9 lfil zero approximate block lu gaussseidel preconditioners respectively indicate preconditioners grid nopre abj par 48 48 367 50 29 21 19 17 64 532 57 36 33 25 20 table 8 test results laplacian problem grid ablu abgs 48 48 17 64 19 20 table 9 test results laplacian problem 45 test results fidap problems blockpartitioned factorization preconditioners unpreconditioned gmres restarted every 20 iterations used approximately solve inner systems involving b reducing initial residual norm factor 01 using 100 matrixvector multiplications solves matrix b usually difficult problems positive definite zero initial guess solves used results number preconditioners various options shown table 10 best preconditioner appears ablu using b gamma1 f better solving system b inaccurately number nonzeros small illustrated table 11 two values lfil matrix table 10 test results fidap problems 46 test results driven cavity problems driven cavity problems much challenging b block longer positive definite fact acquires larger larger negative eigenvalues reynolds number increases problems unpreconditioned gmres iterations b done tolerance 10 gamma3 maximum 100 matrixvector multiplications ablu appears best preconditioner results shown table lfil ex26 13395 21468 ex36 13621 21063 table 11 number nonzeros ablu ablu abgs 1000 164 118 table 12 test results driven cavity problems conclusions presented preconditioners defined combining two ingredients 1 sparse approximate inverse technique obtaining preconditioner schur complement part inverse 2 block factorization full sys tem schur complement appears block factorization approximated preconditioner approximate inverse techniques 6 used different ways approximate either directly part gamma1 seen comparing tables 5 10 solve problems block approach standard ilu factorization addition typically achieved far smaller memory requirement ilut direct solver better robustness methods due fact solves performed small matrices effect implicitly using power divideandconquer strategy characteristic domain decomposition methods smaller matrices obtained block partitioning preconditioned standard ilut approach larger matrices use blockilu glue two preconditioning schur complement acknowledgements authors wish acknowledge support minnesota supercomputer institute provided computer facilities excellent environment conduct research r incomplete block matrix factorization preconditioning methods iterative solution methods approximate factorization methods block matrices suitable vector parallel processors iterative solution large sparse linear systems arising certain multidimensional approximation problems approximate inverse preconditioners general sparse matri ces block preconditioning conjugate gradient method approximate inverse preconditioning sparse linear systems parallelizable block diagonal preconditioners compressible navierstokes equations new approach parallel preconditioning sparse approximate inverses stability analysis incomplete lu factorizations multigrid krylov subspace methods discrete stokes equa tions fast nonsymmetric iterations preconditioning navierstokes equations fidap examples manual parallel preconditioning approximate inverses connection machine comparison domain decomposition techniques elliptic partial differential equations parallel implementation family twolevel preconditionings incomplete block factorization type factorized sparse approximate inverse precon ditionings incomplete block factorizations preconditioners sparse spd matrices flexible innerouter preconditioned gmres algorithm ilut dual threshold incomplete lu factorization preconditioned krylov subspace methods cfd applications sparskit basic tool kit sparse matrix computations tr ctr n guessous souhar multilevel block ilu preconditioner sparse nonsymmetric mmatrices journal computational applied mathematics v162 n1 p231246 1 january 2004 kai wang jun zhang multigrid treatment robustness enhancement factored sparse approximate inverse preconditioning applied numerical mathematics v43 n4 p483500 december 2002 prasanth b nair arindam choudhury andy j keane greedy learning algorithms sparse regression classification mercer kernels journal machine learning research 3 312003 edmond chow michael heroux objectoriented framework block preconditioning acm transactions mathematical software toms v24 n2 p159183 june 1998 howard c elman victoria e howle john n shadid ray tuminaro parallel block multilevel preconditioner 3d incompressible navierstokes equations journal computational physics v187 n2 p504523 20 may michele benzi preconditioning techniques large linear systems survey journal computational physics v182 n2 p418477 november 2002