accurate worst case timing analysis risc processors accurate safe estimation tasks worst case execution time wcet crucial reasoning timing properties realtime systems risc processors execution time program construct eg statement affected various factors cache hitsmisses pipeline hazards factors impose serious problems analyzing wcets tasks analyze timing effects riscs pipelined execution cache memory propose extensions original timing schema timing information associated program construct simple timebound approach associated program construct worst case timing abstraction wcta contains detailed timing information every execution path might worst case execution path program construct extension leads revised timing schema similar original timing schema except concatenation pruning operations wctas newly defined replace add max operations timebounds original timingschema revised timing schema accurately accounts timing effects pipelined execution cache memory within also across program constructs paper also reports preliminary results wcet analysis risc processor results show tight wcet bounds within maximum 30 overestimation obtained using revised timing schema approach b introduction realtime computing systems tasks timing requirements ie deadlines must met correct operation thus utmost importance guarantee tasks finish deadlines various scheduling techniques static dynamic proposed ensure guarantee scheduling algorithms generally require wcet worst case execution time task system known priori therefore surprising considerable research focused estimation wcets tasks nonpipelined processor without cache memory relatively easy obtain tight bound wcet sequence instructions one simply sum individual execution times usually given table wcet program calculated traversing programs syntax tree bottomup applying formulas calculating wcets various language constructs however risc processors simple analysis may appropriate pipelined execution cache memory risc processors instructions execution time varies widely depending many factors pipeline stalls due hazards cache hitsmisses one still obtain safe wcet bound assuming worst case execution scenario eg instruction suffers every kind hazard every memory access results cache miss however pessimistic approach would yield extremely loose wcet bound resulting severe underutilization machine resources goal predict tight safe wcet bounds tasks risc processors achieving goal would permit risc processors widely used realtime systems approach based extension timing schema 1 timing schema set formulas computing execution time bounds language constructs original timing schema timing information associated program construct simple timebound choice timing information facilitates simple accurate timing analysis processors fixed execution times however risc processors timing information sufficient accurately account timing variations resulting pipelined execution cache memory paper proposes extensions original timing schema rectify problem associate program construct call worst case timing abstraction program construct contains timing information every execution path might worst case execution path program construct timing information includes information factors may affect timing succeeding program construct also includes information needed refine execution time program construct timing information preceding program construct becomes available later stage wcet analysis extension leads revised timing schema accurately accounts timing variation results history sensitive nature pipelined execution cache memory assume task sequential form cache partitioning 2 3 used prevent tasks affecting others timing behavior without assumptions would possible eliminate unpredictability due task interaction example consider realtime system preemptive scheduling policy used cache partitioned system burst cache misses usually occurs previously preempted task resumes execution increase task execution time resulting burst cache misses cannot bounded analyzing task isolation paper organized follows section ii survey related work section iii focuses problems associated accurately estimating wcets tasks pipelined processors present method solving problems section iv describe accurate timing analysis technique instruction cache memory explain technique combined pipeline timing analysis technique given section iii section v identifies differences wcet analysis instruction caches data caches explains address issues resulting differences section vi report preliminary results wcet analyses risc processor finally conclusion given section vii ii related work timing prediction method realtime systems able give safe accurate wcet bounds tasks measurementbased analytical techniques used obtain bounds measurementbased techniques many cases inadequate produce timing estimation realtime systems since predictions usually guaranteed enormous cost needed due limitations analytical approaches becoming popular 4 5 6 7 8 9 10 11 12 13 14 15 16 many analytical studies however consider simple machine model thus largely ignoring timing effects pipelined execution cache memory 8 12 13 15 timing analysis pipelined execution timing effects pipelined execution recently studied harmon baker whalley 6 harcourt mauney cook 5 narasimhan nilsen 11 choi lee kang 4 studies execution time sequence instructions estimated modeling pipelined processor set resources representing instruction process acquires consumes subset resources time order mechanize process calculating execution time use various techniques pattern matching 6 sccs synchronous calculus communicating systems 5 retargetable pipeline simulation 11 acsr algebra communicating shared resources 4 although approaches advantage formal machine independent applications currently limited calculating execution time sequence instructions given sequence basic blocks 1 therefore rely ad hoc methods calculate wcets programs pipeline timing analysis technique zhang burns nicholson 16 mechanically calculate wcets programs pipelined processor analysis technique based mathematical model pipelined intel 80c188 processor model takes account overlap instruction execution opcode prefetching 80c188 approach wcet basic block program individually calculated based mathematical model wcet program calculated using wcets constituent basic blocks timing formulas calculating wcets various language constructs although approach represents significant progress previous schemes consider timing effects pipelined execution still suffers two inefficiencies first pipelining effects across basic blocks accurately accounted general due data dependencies resource conflicts within execution pipeline basic blocks execution time differ depending surrounding basic blocks however since approach requires wcet basic block independently calculated make worst case assumption preceding basic block eg last instruction every basic block precede basic block analyzed data memory access prevents opcode prefetching first instruction basic block analyzed assumption reasonable target processor since pipeline two stages however completely ignoring pipelining effects across basic blocks may yield loose wcet estimation deeply pipelined processors second although mathematical model effective intel 80c188 processor model general enough applicable pipelined processors due many machine specific assumptions made model difficult generalize 1 basic block sequence consecutive instructions flow control enters beginning leaves end without halt possibility branching except end 17 b timing analysis cache memory cache memories widely used bridge speed gap processor main memory however designers hard realtime systems wary using caches systems since performance caches considered unpredictable concern stems following two sources intertask interference intratask interference intertask interference caused task preemption task preempted cache blocks 2 displaced newly scheduled task tasks scheduled thereafter preempted task resumes execution makes references previously displaced blocks experiences burst cache misses type cache miss cannot avoided realtime systems preemptive scheduling tasks result wide variation task execution time execution time variation eliminated partitioning cache dedicating one partitions realtime task 2 3 cache partitioning approach eliminates intertask interference caused task preemption intratask interference caches occurs one memory block task compete cache block interference results two types cache miss capacity misses conflict misses 19 capacity misses due finite cache size conflict misses hand caused limited set associativity types cache miss cannot avoided cache limited size andor set associativity among analytical wcet prediction schemes aware four schemes take account timing variation resulting intratask cache interference three instruction caches 10 9 7 one data caches 14 static cache simulation approach statically predicts hits misses instruction references due arnold mueller whalley harmon 10 approach instructions classified following four categories based data flow analysis ffl alwayshit instruction always cache ffl alwaysmiss instruction never cache ffl firsthit first reference instruction hits cache however subsequent references miss cache block minimum unit information either present present cachemain memory hierarchy 18 cond elsek fig 1 sample c program fragment ffl firstmiss first reference instruction misses cache however subsequent references hit cache approach simple number limitations one limitation analysis conservative example consider program fragment given fig 1 assume instruction memory blocks corresponding ie mapped cache block instruction memory block mapped cache block assume execution time much longer j assumptions worst case execution scenario program fragment repeatedly execute within loop worst case scenario first access b miss cache subsequent accesses within loop hit cache however classified alwaysmiss references b treated cache misses approach leads loose estimation loops wcet another limitation approach approach address issues regarding pipelined execution use data caches commonly found risc processors 9 niehaus et al discuss potential benefits identifying instruction references corresponding alwayshit firstmiss static cache simulation approach however stated 10 analysis rather abstract general method analyzing worst case timing behavior programs given 7 liu lee propose techniques derive wcet bounds cached program based transition diagram cached states wcet analysis uses exhaustive search technique state transition diagram exponential time complexity reduce time complexity approach propose number approximate analysis methods makes different tradeoff analysis complexity tightness resultant wcet bounds although paper mentions methods equally applicable data cache main focus instruction cache since issues pertinent data cache handling write references references unknown addresses cf section v considered also clear one incorporate analysis pipelined execution framework rawat performs static analysis data caches 14 approach similar graph coloring approach register allocation 20 analysis proceeds follows first live ranges variables memory blocks computed 3 second interference graph constructed cache block edge interference graph connects two memory blocks mapped cache block live ranges overlap third live ranges memory blocks split overlap live range memory block overlap memory block memory block never gets replaced cache execution within live range therefore number cache misses due memory block calculated frequency counts live ranges ie many times program control flows live ranges finally total number data cache misses estimated summing frequencies live ranges memory blocks used program although analysis method step forward analysis methods every data reference treated cache miss still suffers following three limitations first analysis allow function calls global variables severely limits applicability second analysis leads overestimation data cache misses resulting assumption every possible execution path worst case execution path limitation similar first limitation static cache simulation approach third limitation approach address issues locating worst case execution path calculating wcet limiting applicability 3 live range variable memory block set basic blocks whose execution variable memory potentially resides cache 14 rd alu md mult 25 24 nop lw 24 1622 nop lw 25 1623 fig 2 sample mips assembly code corresponding reservation table iii pipelining effects pipelined processors various execution steps instructions simultaneously overlapped due overlapped execution instructions execution time differ depending surrounding instructions however timing variation could accurately accounted original timing schema since timing information associated program construct simple timebound section extend timing schema rectify problem extended timing schema timing information program construct set reservation tables rather timebound reservation table originally proposed describe analyze activities within pipeline 21 reservation table vertical dimension represents stages pipeline horizontal dimension represents time fig 2 shows sample basic block mips assembly language 22 corresponding reservation table figure x reservation table specifies use corresponding stage indicated time slot proposed approach analyze timing interactions among instructions within basic block building reservation table reservation table conflicts use pipeline stages also data dependencies among instructions considered program construct statement may one execution path moreover pipelined processors always possible determine one execution paths worst case execution path analyzing program construct alone example suppose statement two execution paths corresponding two reservation tables shown fig 3 worst case execution path depends instructions preceding program constructs example one instructions near end preceding program construct uses md stage execution path corresponding r 1 become worst case execution path hand instruction using div stage instead execution path corresponding r 2 become worst case execution path therefore keep alu rd md fig 3 two reservation tables equal struct pipeline timing information f time max reservation table headffi head reservation table tailffi tail md rd head tail fig 4 reservation table data structure reservation tables timing information preceding program constructs known fig 4 shows data structure reservation table used approach textual graphical form data structure max worst case execution time reservation table determined number columns reservation table implementation columns reservation table maintained instead maintain first ie columns last ie ffi tail columns larger ffi head ffi tail tighter resulting wcet estimation since execution overlap program constructs modeled see later corresponds case full reservation table maintained explained earlier associate program construct set reservation tables reservation table contains timing information execution path might worst case execution path program construct call set wcta worst case timing abstraction program construct wcta corresponds timebound original timing schema element wcta denoted framework timing schema extended timing interactions across alu rd md alu rd md alu rd md fig 5 example application phi operation program constructs accurately accounted extended timing schema timing formula sequential statement wctas 1 2 respectively operation two wctas defined reservation tables phi operation concatenates two reservation tables resulting another reservation table concatenation operation models pipelined execution sequence instructions followed another sequence instructions semantics operation target processor deduced data book fig 5 shows application phi operation figure one note columns maintained head tail overlap adjacent program constructs modeled therefore tighter wcet estimation obtained timing formula effectively enumerates possible candidates worst case execution path 1 however instantiation timing formula check made see whether resulting wcta pruned element wcta removed wcta guarantee elements wcet worst case scenario shorter best case scenario wcet element wcta pruning condition formally specified follows reservation table w wcta w pruned without affecting prediction worst case timing behavior w condition wt max ws execution time assume worst case scenario w ie part ws head tail overlapped surrounding program constructs hand w tail execution time w 0 assume best case scenario w 0 ie head completely overlapped tail preceding program construct tail completely overlapped head succeeding program construct timing formula statement exp 1 else 2 given wctas exp 1 2 respectively set union operation previous timing formula pruning performed instantiation timing formula function calls processed like sequential statements approach functions processed reverse topological order call graph 4 since wcta function calculated functions call processed 4 call graph contains information functions call 23 example f calls g arc connects f vertex g call graph finally timing formula loop statement exp 1 given n loop bound provided external means eg user input timing formula effectively enumerates possible candidates worst case execution scenario loop statement approach exact computationally intractable large n following provide approximate methods loop timing analysis approximate loop timing analysis problem finding worst case execution scenario loop statement loop bound n formulated problem find longest weighted path necessarily simple containing exactly n arcs weighted directed graph thus approximate loop timing analysis method explained using graph theoretic formulation weighted directed graph set execution paths loop body might worst case execution path ie associated arc weight w ij execution time path execution immediately preceded path p define ij weight longest path necessarily simple p p j g containing exactly arcs definition max loops worst case execution scenario starts path p ends path p j given p max dn gamma1ij p max max path p wcta worst case execution scenario inherits p head since starts p likewise inherits p j tail loops worst case execution scenario starts path p ends path p j denoted wctawp n ij given p since actual worst case execution scenario loop depends program constructs surrounding loop statement know paths actual worst case execution scenario starts ends analyze loop statement therefore one consider possibilities corresponding wcta loop statement given exp remaining problem determine dn gamma1ij determine value solving following equations pk 2p computation ij using dynamic programming takes thetajp time large n time complexity still unacceptable following describe faster technique gives tight upper bound ij technique based calculation maximum cycle mean g maximum cycle mean weighted directed graph g ranges directed cycles g mc mean weight c maximum cycle mean calculated ojp j theta jaj time independent n using algorithm due karp 24 let maximum cycle mean g ij safely approximated prove following proposition proposition 1 ij maximum weight path necessarily simple p p j containing exactly arcs complete weighted directed graph maximum cycle mean g ij 0 proof assume sake contradiction ij greater theta gamma w ji construct cycle containing adding arc p j p path ij calculated arc exist since g complete graph resulting cycle mean weight greater since ij w ji implies existence cycle g whose mean weight greater contradicts hypothesis maximum cycle mean g thus ij theta moreover shown 0 ij gamma ij indicates looseness approx imation bounded 3 theta gamma wmin wmin minimum weight arc 25 expect bound tight since wmin remember p consists paths w exp cannot pruned interference assumed tasks execute without preemption however bb contents cache cache cache fig 6 sample instruction block references program construct real systems tasks may preempted various reasons preemptive scheduling external inter rupts resource contention task preemptions interference breaks tasks execution flow problem regarding interference adjusting prediction made assumption interference prediction applicable environment interference fortunately additional perpreemption delay introduced pipelined execution bounded maximum number cycles instruction remains pipeline mips r3000 36 cycles case div instruction information available adjusting predictions reflect interference done using techniques explained 26 iv instruction caching effects processor instruction cache execution time program construct differ depending execution path taken prior program construct result history sensitive nature instruction cache example consider program construct accesses instruction blocks 5 b 2 b 3 b 2 b 4 sequence given cf fig 6 assume instruction cache two blocks directmapped directmapped cache instruction block placed exactly one cache block whose index given instruction block number modulo number blocks cache example second reference b 2 always hit cache first reference b 2 bring b 2 cache cache block replaced mean time 5 regard sequence consecutive references instruction block single reference instruction block without loss accuracy analysis struct pipeline cache timing information f time max reservation table headffi head reservation table tailffi tail block address first referencen block block address last referencen block fig 7 structure element hand reference b 4 always miss cache even b 4 previously cache prior program construct first reference b 2 replace b 4 copy cache note b 2 b 4 mapped cache block assumed cache configuration unlike two references whose hits misses determined local analysis hit miss first reference b 2 cannot determined locally dependent cache contents immediately executing program construct similarly hit miss reference b 3 depend previous cache contents hits misses two references affect worst case execution time program construct moreover cache contents executing program construct turn affect execution time succeeding program construct similar way timing variations cannot accurately represented simple timebound original timing schema situation similar case pipelined execution discussed previous section therefore adopt strategy simply extend timing information elements wcta leaving timing formulas intact element wcta two sets instruction block addresses addition max head tail used timing analysis pipelined execution fig 7 gives data structure element wcta new setting n block denotes number blocks cache given data structure first set instruction block addresses ie first reference maintains instruction block addresses references whose hits misses depend cache contents prior program construct words set maintains cache block instruction block address first reference cache block second set ie last reference maintains addresses instruction blocks remain cache execution program construct words set maintains cache block head tail bb firstreference lastreferencex x alu rd md rd alu md fig 8 contents element corresponding example fig 6 instruction block address last reference cache block cache contents determine hits misses instruction block references first reference succeeding program construct calculating max accurately account hits misses locally determined second reference b 2 reference b 4 previous example however instruction block references whose hits misses known ie first reference conservatively assumed miss cache initial estimate max initial estimate later refined information hits misses references becomes available later stage analysis fig 8 shows timing information maintained program construct given previous example extension timing formula timing formula structurally identical one given previous section sequential statement differences structure elements wctas semantics phi operation revised semantics phi operation procedurally defined fig 9 function concatenate given figure concatenates two input elements puts result w 3 thus implementing phi operation lines 912 function concatenate first reference corresponding cache block accessed w 1 cache block accessed w 1 first reference cache block w 1 phi w 2 w 2 therefore struct pipeline cache timing information concatenatestruct pipeline cache timing information w 1 3 struct pipeline cache timing information w 2 struct pipeline cache timing information w 3 8 9 w 1 first referencei null first referencei else first referencei last referencei null last referencei else last last referencei last referencei w 2 first referencei head 22 w 3 26 g fig 9 semantics phi operation would inherit w 2 first reference likewise lines 1316 w 3 inherits w 2 last reference corresponding cache block accessed w 2 w 1 last reference otherwise comparing first reference w 1 last reference lines 1718 determine many memory references w 2 first reference hit cache cache hits used refine w 3 remember memory references w 2 first reference previously assumed miss cache initial estimate w 2 max lines 2021 w 3 inherits w 1 head taking account pipelined execution across w 1 w 2 cache hits determined lines 1718 calculation phi pipeline operation phi operation defined previous section timing analysis pipelined execution miss penalty time needed service cache miss element wcta safely eliminated ie pruned wcta guarantee elements wcet always shorter element regardless surrounding program constructs condition pruning procedurally specified fig 10 function prune given figure checks whether either one struct pipeline cache timing information prunestruct pipeline cache timing information w 1 3 struct pipeline cache timing information w 2 7 8 w 1 first referencei w 2 first referencei 9 n diff last referencei w 2 last referencei else else 19 return null fig 10 semantics pruning operation two execution paths corresponding two input elements pruned returns pruned element pruning successful null neither pruned function prune lines 612 determine many entries w 1 first reference last reference different corresponding entries w 2 first reference last reference difference bounds cache memory related execution time variation checks whether w 2 pruned w 1 pruning w 2 w 1 made w 2 wcet assuming worst case scenario w 2 shorter w 1 wcet assuming best case scenario likewise line 16 checks whether w 1 pruned w 2 timing formula exp 1 else 2 given previous section problem calculating w loop statement exp 1 formulated graph theoretic problem wctawp n ij given first reference p j last reference calculating wctawp n computed follows loop timing analysis discussed previous section assumes loop iteration benefits immediately preceding loop iteration calculation w ij consider execution time reduction p j due execution overlap p assumption holds case pipelined execution since execution time iterations head affected tail immediately preceding iteration case cache memory however assumption hold general example instruction memory reference may hit cache block loaded cache iteration immediately preceding one nevertheless since assumption conservative resulting worst case timing analysis safe sense result underestimate wcet loop statement degradation accuracy resulting conservative assumption reduced analyzing sequence k k 1 iterations time rather one iteration 25 case vertex represents execution sequence k iterations w ij execution time sequence j execution immediately preceded execution sequence analysis corresponds analysis loop unrolled k times trades increased analysis complexity accurate set associative caches considered simplest cache organization called directmapped cache instruction block placed exactly one cache block general cache organization called nway set associative cache instruction block placed one n blocks mapped set 6 set associative caches need policy decides block replace among blocks set make room block fetched cache miss lru least recently used policy typically used purpose replacement policy given assuming random straightforward implement phi prune operations needed analysis method 6 set associative cache index mapped set given instruction block number modulo number sets cache timing analysis data caches analogous instruction caches however former differs latter several important ways first unlike instruction references actual addresses data references known compiletime complicates timing analysis data caches since calculation first reference last reference important aspect cache timing analysis assumes actual address every memory reference known compiletime complication however avoided completely simple hardware support form one bit loadstore instruction available bit called allocate bit decides whether memory block fetched miss loaded cache data reference whose address cannot determined compiletime allocate bit set zero preventing memory block fetched miss loaded cache references bit set one allowing fetched block loaded cache hardware support worst case timing analysis data caches performed much like instruction caches ie treating references whose addresses known compiletime misses completely ignoring calculation first reference last reference even hardware support available worst case timing analysis data caches still possible taking two cache miss penalties data reference whose address cannot determined compiletime ignoring reference analysis 27 one cache miss penalty due fact reference may miss cache due fact reference may replace cache block contributes cache hit analysis second difference stems accesses local variables general data area local variables function called activation record function pushed popped runtime stack associated function called returned implementations specially designated register called sp stack pointer marks top stack local variable addressed offset relative sp offsets local variables determined compile time however sp value function differs depending function called however number distinct sp values function may bounded therefore function computed sp value function may sp values calculated activation record sizes functions call graph final difference due write accesses unlike instruction references readonly data references may read write memory data caches either writethrough writeback policy used handle write accesses 18 writethrough policy effect write reflected block cache block main memory hand writeback policy effect reflected block cache dirty bit set indicate block modified block whose dirty bit set replaced cache blocks contents written back main memory timing analysis data caches writethrough policy relatively simple one simply add delay write access account accompanying write access main memory however timing analysis data caches writeback policy slightly complicated writeback cache sequence write accesses cached memory block without replacement inbetween call write run requires one writeback main memory attribute writeback overhead ie delay last write write run call tail write run setting one determine whether given write access tail accurately estimate delay due writebacks cases local analysis determine whether write access tail case hitmiss analysis memory reference however local analysis sufficient determine whether write access tail every case hence possible conservatively assume write access tail add writeback delay max however later analysis program syntax tree reveals write access tail subtract incorrectly attributed writeback delay max global analysis performed providing bits block first reference last reference augmenting phi pruning operations 27 vi experimental results tested whether extended timing schema approach could produce useful wcet bounds building timing tool based approach comparing wcet bounds predicted timing tool measured times timing tool consists compiler timing analyzer cf fig 11 compiler modified version ansi c compiler called lcc 28 modified compiler accepts c source program generates assembly code along program syntax information call graph timing analyzer uses assembly code program syntax wcet wcep modified information userprovided graph call information program code assembly program analyzer timing fig 11 overview timing tool information along userprovided information eg loop bound compute wcet program chose idt7rs383 board timing tools target machine target machines cpu 20 mhz r3000 processor typical risc processor r3000 processor fivestage integer pipeline interface offchip instruction data caches also interface offchip floatingpoint unit fpu idt7rs383 board contains instruction data caches 16 kbytes caches directmapped block sizes 4 bytes data cache uses writethrough policy oneentry deep write buffer cache miss service times instruction data caches 4 cycles fpu used board mips r3010 although board timer chip provides userprogrammable timers resolutions low measurement purposes facilitate measurement program execution times machine cycles built daughter board consists simple decoding circuits counter chips provides one userprogrammerable timer timer starts stops writing specific memory locations resolution one machine cycle 50 ns three simple benchmark programs chosen clock sort mm clock benchmark program used implement periodic timer program periodically checks 20 linkedlisted timers expires calls corresponding handler function sort benchmark sorts array 20 integer numbers mm program multiplies two 5 theta 5 floatingpoint matrices table 1 compares wcet bounds predicted timing tool measured execution times three benchmark programs three cases tool gives fairly tight wcet bounds within maximum 30 overestimation closer inspection results revealed clock sort mm predicted measured 2768 11471 6346 unit machine cycles table 1 predicted measured execution times benchmark programs 90 overestimation due data references whose addresses known compiletime remember account two cache miss penalties data reference program execution time heavily dependent program execution path logic programs severely limits set possible execution paths however intentionally chose benchmark programs suffer overestimation due infeasible paths rationale behind selection predicting tighter wcet bounds eliminating infeasible paths using dynamic path analysis issue orthogonal approach analysis introduced existing timing tool without modifying extended timing schema framework fact method analyzing dynamic program behavior eliminate infeasible paths program within original timing schema framework given 29 feel timing tool equally benefit proposed method view experimental work reported initial step toward validating extended timing schema approach clearly much experimental work especially programs used real systems need follow demonstrate approach practical realistic systems vii conclusion paper described technique aims accurately estimating wcets tasks risc processors proposed technique two kinds timing information associated program construct first type information factors may affect timing succeeding program construct second type information factors needed refine execution time program construct first type timing information preceding program construct becomes available later stage wcet analysis extended existing timing schema using two kinds timing information accurately account timing variations resulting history sensitive nature pipelined execution cache memory also described optimization minimizes overhead proposed technique pruning timing information associated execution path cannot part worst case execution path also built timing analyzer based proposed technique compared wcet bounds sample programs predicted timing analyzer measured execution times timing analyzer gave fairly tight predictions within maximum 30 overestimation benchmark programs used sources overestimation identified proposed technique following advantages first proposed technique makes possible accurate analysis combined timing effects pipelined execution cache memory previously possible second timing analysis using proposed technique accurate technique aware third proposed technique applicable risc processors inorder issue singlelevel cache memory finally proposed technique extensible general rule may used model machine features history sensitive timing behavior example used underlying general rule model timing variation due write buffers 27 one direction future research investigate whether proposed technique applies advanced processors outoforder issue 30 andor multilevel cache hierarchies 18 another research direction development theory methods design retargetable timing analyzer initial investigation issue made 31 results indicated machinedependent components timing analyzer routines implement concatenation pruning operations extended timing schema automatically generated architecture description target processor details approach repeated interested readers referred 31 r reasoning time higherlevel language software smart strategic memory allocation realtime cache design softwarebased cache partitioning realtime applications timing analysis superscalar processor programs using acsr highlevel timing specification instructionlevel parallel processors retargetable technique predicting execution time deterministic upperbounds worstcase execution times cached programs evaluating tight execution time bounds programs annotations predictable realtime caching spring system bounding worstcase instruction cache performance portable execution time analysis risc processors experiments program timing tool based sourcelevel timing schema calculating maximumexecution time realtime programs static analysis cache performance realtime programming pipelined processors worstcase execution times computer architecture quantitative approach aspects cache memory instruction buffer performance register allocation prioritybased coloring architecture pipelined computers englewood cliffs crafting compiler c characterization minimum cycle mean digraph instruction cache pipelining analysis technique realtime systems predicting deterministic execution times realtime programs data cache analysis techniques realtime systems code generation interface ansi c predicting program execution times analyzing static dynamic program paths lookahead processors retargetable timing analyzer risc processors tr ctr hassan aljifri alexander pons moiez tapia estimation wcet superscalar realtime system realtime system security nova science publishers inc commack ny jan staschulat rolf ernst scalable precision cache analysis preemptive scheduling acm sigplan notices v40 n7 july 2005 minsoo ryu jungkeun park kimoon kim yangmin seo seongsoo hong performance reengineering embedded realtime systems acm sigplan notices v34 n7 p8086 july 1999 dongkun shin jihong kim seongsoo lee lowenergy intratask voltage scheduling using static timing analysis proceedings 38th conference design automation p438443 june 2001 las vegas nevada united states xianfeng li abhik roychoudhury tulika mitra modeling outoforder processors wcet analysis realtime systems v34 n3 p195227 november 2006 jurgen schnerr oliver bringmann wolfgang rosenstiel cycle accurate binary translation simulation acceleration rapid prototyping socs proceedings conference design automation test europe p792797 march 0711 2005 dongkun shin jihong kim seongsoo lee intratask voltage scheduling lowenergy hard realtime applications ieee design test v18 n2 p2030 march 2001 jrn schneider christian ferdinand pipeline behavior prediction superscalar processors abstract interpretation acm sigplan notices v34 n7 p3544 july 1999 sheayun lee sang lyul min chong sang kim changgun lee minsuk lee cacheconscious limited preemptive scheduling realtime systems v17 n23 p257282 nov 1999 henrik theiling generating decision trees decoding binaries acm sigplan notices v36 n8 p112120 aug 2001 joosun hahn rhan ha sang lyul min jane ws liu analysis worst case dma response time fixedpriority bus arbitration protocol realtime systems v23 n3 p209238 november 2002 tobias schuele klaus schneider abstraction assembler programs symbolic worst case execution time analysis proceedings 41st annual conference design automation june 0711 2004 san diego ca usa thomas lundqvist per stenstrm integrated path timing analysis method based oncyclelevel symbolic execution realtime systems v17 n23 p183207 nov 1999 daniel kstner stephan thesing cache aware preruntime scheduling realtime systems v17 n23 p235256 nov 1999 colin fidge peter kearney mark utting formal method building concurrent realtime software ieee software v14 n2 p99106 march 1997 henrik theiling christian ferdinand reinhard wilhelm fast precise wcet prediction separated cache andpath analyses realtime systems v18 n23 p157179 may 2000 joan krone william f ogden murali sitaraman performance analysis based upon complete profiles proceedings 2006 conference specification verification componentbased systems november 1011 2006 portland oregon jungkeun park minsoo ryu seongsoo hong lucia lo bello rapid performance reengineering distributed embedded systems via latency analysis klevel diagonal search journal parallel distributed computing v66 n1 p1931 january 2006 andreas ermedahl friedhelm stappert jakob engblom clustered worstcase executiontime calculation ieee transactions computers v54 n9 p11041122 september 2005 xianfeng li tulika mitra abhik roychoudhury modeling control speculation timing analysis realtime systems v29 n1 p2758 january 2005 friedhelm stappert andreas ermedahl jakob engblom efficient longest executable path search programs complex flows pipeline effects proceedings 2001 international conference compilers architecture synthesis embedded systems november 1617 2001 atlanta georgia usa andreas ermedahl friedhelm stappert jakob engblom clustered calculation worstcase execution times proceedings international conference compilers architecture synthesis embedded systems october 30november 01 2003 san jose california usa sheayun lee jaejin lee chang yun park sang lyul min selective code transformation dual instruction set processors acm transactions embedded computing systems tecs v6 n2 p10es may 2007 gustavo gmez yanhong liu automatic timebound analysis higherorder language acm sigplan notices v37 n3 p7586 march 2002 ian j hayes procedures parameters realtime program refinement calculus science computer programming v64 n3 p286311 february 2007 karl lermer colin j fidge ian j hayes theory executiontime derivation realtime programs theoretical computer science v346 n1 p327 23 november 2005 liu g gmez automatic accurate costbound analysis highlevel languages ieee transactions computers v50 n12 p12951309 december 2001 c j fidge realtime schedulability tests preemptive multitasking realtime systems v14 n1 p6193 jan 1998 christian ferdinand reinhard wilhelm efficient precise cache behavior prediction realtimesystems realtime systems v17 n23 p131181 nov 1999 vasanth venkatachalam michael franz power reduction techniques microprocessor systems acm computing surveys csur v37 n3 p195237 september 2005