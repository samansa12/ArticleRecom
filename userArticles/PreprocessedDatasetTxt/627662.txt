unified integration explicit knowledge learning example recurrent networks abstractwe propose novel unified approach integrating explicit knowledge learning example recurrent networks explicit knowledge represented automaton rules directly injected connections network accomplished using technique based linear programming instead learning random initial weights learning conceived refinement process mainly responsible uncertain information management present preliminary results problems automatic speech recognition b introduction resurgence interest connectionist models led several researchers investigate application building intelligent systems unlike symbolic models proposed artificial intelligence learning plays central role connectionist models many successful applications mainly concerned perceptual tasks see eg 2 6 10 20 research partially supported murst 40 cnr grant 9001530ct01 1 authors dipartimento di sistemi e informatica universita di firenze via di santa italy tel 39 554796265 telex 580681 unfing fax 39 554796363 discovering explicit rules seem either natural easy connectionist models appear better suited low level tasks symbolic ones like humans rely learning perceptual tasks hand learning example paradigm cannot stressed much emulating intelligent behavior many cases intelligent behavior follows explicit rules matter fact machine conceived situations ignore knowledge order prove potential power learning example paradigm problems learning sequences researchers recently shown explicit rules also discovered learning tabula rasa configurations particular cleeremans 3 elman 5 williams 22 demonstrated full connected recurrent network capable learning small automata investigations interesting since show connectionist models learn rules relying presentation examples however closer investigation shows cannot stress much learning example paradigm problem complex tasks may give raise local minima ordinary gradient descent learning algorithms likely fail cases least feedforward nets analysis problems carried allows us understand success backpropagation 18 several problems pattern recognition 11 however using theory easily proven simple examples exist learning algorithm fails discover optimal solution integration explicit knowledge learning example appears natural way evolving intelligent systems based connectionist models hypothesis model effective integration uniform consequence explicit learned rules represented way weight connections neural network paper address problem learning sequences assume explicit knowledge problem available terms automaton rules basic assumption implementing automata relying state equations efforts focussed finding method injecting automaton rules connections recurrent network section ii demonstrate automaton states coded neuron activities section iii automaton rule realized terms constraints weights particular automaton rules translated set inequalities according linear programming framework basis remarks section iv propose unified approach integrating explicit knowledge learning example paradigm recurrent neural networks propose architecture composed two cooperating subnets first one designed order inject available explicit knowledge whereas second one mainly responsible uncertain information effectiveness proposed model currently evaluation problems automatic speech recognition section v report preliminary results problem isolated word recognition chosen test based italian speech database quite difficult since words composed nasals vowels purpose experiments mainly providing material discuss behavior proposed model practice unlike many suggested solutions proposed literature based learning example 4 13 17 proposed model likely scale well increasing lexicon ii information latching let n u set neurons external network inputs respectively neuron receives inputs n u recurrent network model consider based following equations 1 network status contains outputs neurons also denote w vector weights towards neuron feeding network sequence inputs status represents codification information extracted sequence let us investigate possibility latching information given state show next section useful order investigate automaton realization definition 1 say given dynamic hidden neuron latches information 0 represented activation following inequalities hold 2 concept information latching introduced 8 discussing properties local feedback multilayered networks definition suggests interpretation figure 1 graphical interpretation information latching neuron output boolean status x b relevant henceforth referring state state transition network assume tacitly x b involved actual output x theorem 1 given generic neuron following facts hold 1 latching occurs provided w ii 1f 2 w ii 2 latching condition also holds ji tj 3 w ii 2 state transition occurs finite number steps low high transition high low transition proof let us consider generic neuron 2 n follows equation hypothesis w ii 2 equation 3 three equilibrium points see line 1 fig 1 one corresponds points asymptotically stable prove fact point satisfies let us define lyapunov function 14 pp 166221 v hypothesis w ii 2 follows ga result first factor positive consequently therefore first factor negative deltav 0 hence stability 2 0 1 similar proof provided stability non null solution 3 equilibrium point fact starting point neighborhood zero state trajectory goes one two stable points according initial sign proof also valid neuron receives constant input 0 suchthatji effect translating input line fig 1 ji line becomes tangent curve fa situation corresponds degeneration two equilibrium points straightforward analysis allows us check relationship given second statement theorem 2 let us consider effect adding timevariant forcing term bounded module constant 0 previously done let us limit analysis positive solution previous discussion follows system stable equilibrium point ff easily prove activation system satisfies inequality assuming null initial state eq 9 obviously valid us suppose valid previous considerations stability ff see eq 7 activation ff therefore cannot change sign information latching occurs remember thatf 0 steps neuron input 9 0593 2008 3637 4493 6255 8063 table 1 relationship transient duration l neuron input different values w ii example w using leads state transition three steps order prove third theorems statement let us consider case neuron latched high state input gammai applied input line one intersection curve fa see fig 1 line2 therefore ts evolution follows attractive trajectory towards unique equilibrium point see dotted lines fig 1 corresponds low state boolean value similar proof given low high transition 2 theorem indicates conditions information latching occurs makes clear local weights increase latching related saturated configurations moreover theorem 1s second statement defines conditions current state latched indicates limit condition guarantees information latching consequently state transitions increase w ii increases thus indicating robustness latching information value w ii also affects transient duration l state transition occurs greater w ii longer transient behavior summarized table 1 case low high transition table created assuming transient duration evaluated assuming column values belonging interval extremes given two subsequent row values determine number steps specified first row value iii k algorithm learning linear programming pointed introduction intelligent behavior often follows rules explicit extent order limit complexity learning phase intelligent system exploit rules show section v input information sometimes represented continuous signal however cases derive symbolic representation information means input quanti zation string obtained way may contain subsequent repetitions symbols eg nnuuummaaa problems automatic speech recognition repetitions related phoneme duration since consider uncertain information number repetitions help detecting low level errors assume sequence futg belongs certain class c accepted particular automaton c representing class c order understand automaton operates think machine cascade two blocks first one task modeling duration provides sort filtering input sequence produces instance symbol provided repeated least given number steps eg number steps 2 processing nnmnuumummaaa would produce numa second block simply finite state automaton fsa represents basic knowledge assume problem notice unlike mentioned fsa cascade two blocks may regarded nondeterministic automaton 19 section demonstrate automaton rules realized network 1 terms weight boundaries turns useful integrating rules learning example 7 first step choose proper coding automaton states means boolean states x b neurons recurrent network 1 assume pair present nextstate fsa hamming distance corresponding codifications one thereafter neuron straightforward derive set r neuron switching rules automaton rules rule r 2 r denote 3 x b ir x b ir respectively present next boolean state neuron neuron switching rules implemented using results contained theorem 1 let vector weights rules r hold input neuron fulfilling rule r ir 3 sequel index may omitted sake simplicity coding assumption constant state transition result directly apply theorem 1 following linear constraints weights must hold ir ir oe boolean state switching required otherwise oe gamma1 example let us consider rule r requires low high switching boolean state neuron case x b equation 12 becomes ir according theorem 1s result equations 12 framework linear programming feasible solution point w weight space lies convex region bounded set hyperplanes h ir w solution 12 satisfies requirements arisen fsa important remember network state transition 1 may need one step number steps depends strictly relationship fact shown clearly table 1 considerations make clear evaluation region weight space automaton rules valid important parametric weight representation space quite difficult achieve although restrictive spherical subset space easily determined changing equations 12 basic idea relies computation distance ir weight solution w hyperplanes distance written ir ir c r put together equations 12 13 setup following optimization problem still solved framework linear programming ffl solving neuron following set equations ir ir obtain optimal spherical regions weights space center coordinates described procedure referred k algorithm recurrent network 1 weights belonging spheres actually nondeterministic automaton 19 weights specified automaton becomes deterministic section b input symbol neuron figure 2 chain automaton example b neural implementation iv determination weights proposed using supervised learning example let us consider simple automaton ordered states chain structure see fig 2a basically state transition nextstate permitted generic state coded follows neural realization based recurrent network composed dynamic neurons w ii 2 worth mentioning particular codification adopted network fig 2b used instead full connected net equations 12 case follows 8 gammaw exclusive binary coding chosen inputs neuron receives one bit input coding determine maximum sphere included weight space solving equations 14 terms w found sphere radius ae center latching condition imposed choosing w iv integration rules learning output network inputs rules learned rules priori figure 3 kl network shown previous section neural realization nondeterministic automaton leads network whose weights belong specific regions weight space choice particular point space associated modeling symbol duration moreover must remember explicit knowledge defined automaton based input quantization information conveyed continuous signal quantization represents approximated view original problem model duration using supervised learning scheme based presentation examples learning scheme also prove useful dealing continuous nature input information many problems however prioriknowledge injected network connections may limit possibility learning new rules specified explicit model reason propose kl prioriknowledge learning architecture shown fig 3 based two cooperating subnets nk nl devoted explicit learned rule representation respectively third subnet takes input subset nk nl neurons provides external output simplest case consists single output neuron see example section v weights first subnet nk quickly initialized thanks method shown section iii permits begin learning configuration already represents problem explicit knowledge learning uncertain information mainly accomplished second subnet kl architecture fullconnected recurrent network randomly initialized task discovering hidden rules weights optimization carried means modified version pearlmutters learning algorithm 16 adapted discrete time formal definition procedure may found example 23 algorithm discover solution optimizes cost function i2no flag oe means supervision request takes place neuron time length input sequence ordinary gradient descent accomplished order optimize weights relevant difference nk weights constrained spherical region described section iii guarantees automaton rules destroyed proposed model learning example essentially conceived refinement process relieved problem discovering complex deterministic rules cases use learning example paradigm likely fail presence local minima failures understood framework complexity theory least feedforward nets proven loading problem npcomplete 12 particular class automata consequently recurrent nets interest application going propose following nets referred chainlike nets next section discuss chainlike nets bearing mind application speech recognition v applications automatic speech recognition order validate theoretical hypotheses better understand proposed model carried several preliminary experiments automatic speech recognition one primary goals applications area demonstrating capability proposed model deal isolated word recognition iwr large lexicons far many attempts build neuralbased classifiers iwr assumed small lexicons see eg 4 13 17 neural classifiers succeeded problems acoustic feature extraction exhibited significant results applications large lexicons basically due intrinsic limitations methods rely learning examples although solutions proposed building modular architectures 21 13 scaling large lexicons appears serious problem order overcome difficulties propose model word given dictionary kl net one must detect word built reject words dictionary recognition phase word recognized presented nets simple decision criteria choosing highest output value used performing word prediction experiment isolated word recognition henceforth propose experiment discriminating 10 italian words composed vowels nasals order accomplish task selected hierarchical network architecture first net n p devoted perform phoneme hypotheses u u figure 4 automaton devoted detect italian word numa table shows codification automaton states nets nw fed n p outputs used modeling words indicated section iv detailed description phonetic network n p found 9 net nw devoted detect word dictionary highest network output criterion used perform word prediction fig 5b shows particular net chosen modeling italian word numa subnet priori rule representation chainlike structure conceived representing automaton fig 4 state subsequent occurrences phonetic symbol produce state transitions automaton capable dealing phoneme skips behaves like string parser whose final accepted states reached right phoneme string applied basically automata kind solve directly problem insertions deletions phonetic symbols important iwr practice want kl nets consider state transitions 2 3 speech frames order avoid several noisy predictions phoneme net n p see fig 5a 6a feature particularly useful decrease crosstalk words moreover forgotten nets nw deal analog values representing evidence given phoneme fullconnected network composed two neurons adopted net nl investigated effect learning particularly nl neurons expected theoretical considerations rules included nk automaton net automatically figure 5 phoneme outputs word numa b network models word numa fed word numa input level activation neurons proportional gray level black high value figure phoneme outputs word inumano b network models word numa fed word inumano learned example fact clearly understood inspecting behavior network associated word numa words numa inumano presented input respectively worth mentioning discrimination words possible consider first subnet since inumanocontains numa substring fact comes also inspecting networks state first subnet however learning example makes possible develop internal rep resentation neurons subnet nl permits discrimination two words quick glance fig 5b fig 6b suggests word discrimination gained thanks different information coding created nl neurons explicit example shows new rules discovered included net nk initialized prioriknowledge obviously case discrimination words could attained also directly using complex automata injected nk since difference quite explicit practice want learning process develop rules appear explicit affected heavily uncertainty preliminary speaker independent small test based 284 words performed maximum output decision criterion adopted found recognition rate high 923 7 task simple since words considered composed vowels nasals notice although dictionary small 10 words model proposed likely scale much better others suggested literature 4 13 17 mainly due subnet nk accept acoustic strings corresponding words models worth mentioning learning example approach used modeling word guarantee provided ensure given word net reacts words vi conclusions paper propose novel method integrating prioriknowledge learning example recurrent networks show behavior nondeterministic automata injected networks connections behavior difficult learn using learning example approach presence local minima proposed model optimization procedures must discover solution beginning tabula rasa must rather produce refinement find additional regularities captured explicit rules preliminary applications problems automatic speech recognition promising importantly unlike many proposals iwr neural nets proposed model scales well increasing lexicon dimension finally worth mentioning although mainly conceived speech recognition understanding tasks model turn useful applications well r approximation boolean functions sigmoidal networks part xor twovariable functions speech pattern discrimination multilayered perceptrons finite state automata simple recurrent networks use neural networks speaker independent isolated word recognition finding structure time learning hidden structure speech unified approach integrating explicit knowledge learning example recurrent networks local feedback multilayered networks recurrent networks continuous speech recognition bps learning algorithm capturing dynamical nature speech problem local minima backpropagation neural network design complexity learning design hierarchical perceptron structures application task isolated word recognition stability motion logical calculus ideas immanent nervous activity learning state space trajectories recurrent neural net works multilayer perceptron tool speech pattern processing research learning internal representation error propagation formal languages relation automata phoneme recognition using timedelay neural networks modularity neural networks speech recognition learning algorithm continually running fully recurrent networks efficient gradientbased algorithm online training recurrent networks trajectories tr ctr ben choi applying learning examples digital design automation applied intelligence v16 n3 p205221 mayjune 2002 christian w omlin c lee giles rule revision recurrent neural networks ieee transactions knowledge data engineering v8 n1 p183188 february 1996 barbara hammer peter tio recurrent neural networks small weights implement definite memory machines neural computation v15 n8 p18971929 august pasquale foggia roberto genna mario vento symbolic vs connectionist learning experimental comparison structured domain ieee transactions knowledge data engineering v13 n2 p176195 march 2001 steve lawrence c lee giles sandiway fong natural language grammatical inference recurrent neural networks ieee transactions knowledge data engineering v12 n1 p126140 january 2000 stefan c kremer spatiotemporal connectionist networks taxonomy review neural computation v13 n2 p249306 february 2001 michael berthold david j hand references intelligent data analysis springerverlag new york inc new york ny