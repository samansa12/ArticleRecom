asynchronous problems simd parallel computers abstractone essential problems parallel computing simd machines handle asynchronous problems difficult unsolved problem mismatch asynchronous problems simd architectures propose solution let simd machines handle general asynchronous problems approach implement runtime support system run mimdlike software simd hardware runtime support system named p kernel threadbased two major advantages threadbased model first application problems irregular andor unpredictable features automatic scheduling move threads overloaded processors underloaded processors second importantly granularity threads controlled reduce system overhead p kernel also able handle bookkeeping message management well make lowlevel tasks transparent users substantial performance obtained maspar mp1 b introduction 11 simd machines handle asynchronous problems current parallel supercomputers developed two major architectures simd single instruction multiple data architecture mimd multiple instruction multiple architecture simd architecture consists central control unit many processing units one instruction executed time every processor executes instruction advantages simd machine include simple architecture makes machine potentially inexpensive synchronous control structure makes programming easy 4 34 communication overhead low 26 designers simd architectures motivated fact important though limited class problems fit simd architecture extremely well 35 mimd architecture based duplication control units individual processor different processors execute different instructions time 15 flexible different problem structures applied general applications however complex control structure mimd architecture makes machine expensive system overhead large application problems classified three categories synchronous loosely synchronous asynchronous table shows application problems three categories 13 ffl synchronous problems uniform problem structure time step every processor executes operation different data resulting naturally balanced load ffl loosely synchronous problems structured iteratively two phases computation phase synchronization phase synchronization phase processors exchange information synchronize computation load also redistributed phase computation phase different processors operate independently ffl asynchronous problems synchronous structure processors may communicate time computation structure irregular load imbalanced table classification problem structures synchronous loosely synchronous asynchronous matrix algebra molecular dynamics nqueen problem finite difference irregular finite elements region growing qcd unstructured mesh eventdriven simulation synchronous problems naturally implemented simd machine loosely synchronous problems mimd machine implementation loosely synchronous problems simd machines easy computation load must balanced load balance activity essentially irregular example simple 2 algorithm nbody simulation synchronous easy implement simd machine 12 bernuthut algorithm log n nbody simulation loosely synchronous difficult implement simd machine 3 solving asynchronous problems difficult first direct implementation mimd machines nontrivial user must handle synchronization load balance issues time could extremely difficult application problems general runtime support system linda 2 5 reactive kernel 27 33 chare kernel 30 necessary solving asynchronous problems implementation asynchronous problems simd machines even difficult needs runtime support system support system asynchronous particular support system must arrange code way processors execute instruction time taking nqueen problem example since known prior execution time many processes generated large computation runtime system necessary establish balanced computation efficient execution summarize discussion table ii various application problems require different programming methodologies two essential table ii implementation problems mimd simd machines synchronous loosely synchronous asynchronous mimd easy natural need runtime support simd natural difficult difficult need runtime support programming methodologies arraybased threadbased problem domain synchronous applications naturally mapped onto array resulting arraybased programming methodology problems lend efficient programming arraybased methodology mismatch model problem structure solution asynchronous problems cannot easily organized aggregate operations data domain uniformly structured naturally demands threadbased programming methodology threads individually executed information exchange happen time loosely synchronous problems either arraybased threadbased programming methodology applied 12 let simd machines handle asynchronous problems make simd machine serve general purpose machine must able solve asynchronous problems addition solving synchronous loosely synchronous problems major difficulties executing asynchronous applications simd machines ffl gap synchronous machines asynchronous applications ffl gap array processors threadbased programming one solution called applicationoriented approach lets user fill gap application problems architectures approach user must study problem look specific method solve 7 36 37 39 alternative applicationoriented approach systemoriented approach provides system support run mimdlike software simd hardware systemoriented approach superior applicationoriented approach three reasons ffl system usually knowledgeable architecture details well dynamic states ffl efficient develop sophisticated solution system instead writing similar code repeatedly user programs ffl general approach enhances portability readability application programs systemoriented approach carried two levels instructionlevel thread level share underlying idea one treat program data simd machine could interpret data like machinelanguage instruction interpreted machines instruction cycle mimdlike program could efficiently execute simd machine 17 instructionlevel approach implements idea directly instructions interpreted parallel across processors control signals emanating central control unit 41 major constraint approach central control unit cycle almost entire instruction set instruction execution processor may execute different instructions furthermore approach must insert proper synchronization ensure correct execution sequence programs communication synchronization could suspend large number processors finally approach unable balance load processors unlikely produce good performance general applications propose threadbased model runtime system support loosely synchronous asynchronous problems simd machines threadlevel implementation offers great flexibility compared instructionlevel approach least two advantages ffl execution order threads exchanged avoid processor suspension load balanced application problems irregular dynamic features ffl system overhead overwhelming since granularity controlled threadbased approach runtime support system named process kernel p kernel p kernel threadbased assign computation threadlevel p kernel able handle bookkeep ing scheduling message management well make lowlevel tasks transparent users 13 related research existing work solving loosely synchronous asynchronous problems simd machines mostly application oriented 7 36 37 39 region growing algorithm asynchronous irregular problem difficult run simd machines 39 merge phase major part algorithm performs two three orders magnitude worse counterpart mimd machines result due communication cost lack load balancing mechanism authors concluded behavior region growing algorithm like asynchronous problems difficult characterize work needed developing better model two implementations mandelbrot set algorithm 36 molecular dynamics algorithm 7 37 contain parallelizable outer loop inner loop number iterations varies different iterations outer loop structure occurs several different problems iteration advance method employed rearrange iterations simd execution called simlad simd model local indirect addressing 36 loop flattening 37 recent studies conducted simulation logic circuits simd machine 6 20 major difference works approach handle general asynchronous loosely synchronous problems instead studying individual problems instructionlevel approach studied number researchers 8 9 22 25 41 40 mentioned major restriction approach entire instruction set must cycled execute one instruction step every processor common method reduce average number instructions emanated execution cycle perform global ors determine whether instruction needed processor 9 41 might necessary insert barrier synchronizations points limit degree divergence certain applications barrier end statement well forall statement good idea 8 work includes adaptive algorithm changes order instructions emanated maximize expected number active processors 25 besides issue load balancing processor suspension also unsolved problems applications implemented systems noncommunicating 41 barrier end program 9 collins discussed communication issue proposed scheme delay execution communication instructions 8 similar technique used 9 expensive operations including communication however clear whether method work many applications dependences loads imbalanced inevitably leading high communication overhead processor suspension summary instructionlevel approach large interpreter overhead technique literally transforms pure mimd code pure simd code eliminate overhead proposed 10 perhaps works closest approach graphinators 17 early work combinators 21 16 work interpretation prolog flat ghc 19 23 24 graphinators implementation mimd simulator simd machines achieved simd processor repeatedly cycle entire set possible instructions work distinguished work terms granularity control graphinator model suffered fine granularity authors mentioned communication requires roughly millisecond unacceptably long finegrained application 17 p kernel user control grainsize process grainsize well beyond one millisecond thus p kernel implementation obtain acceptable performance besides model general instead dedicating merely functional language graphinator model model similar largegrain data flow lgdf model 18 model computation combines sequential programming dataflowlike program activation lgdf model implemented shared memory machines implemented mimd distributed memory machines well 42 30 11 14 show model implemented simd distributed memory machine 2 p kernel approach computation model language computation model p kernel originated chare kernel 30 messagedriven nonpreemptive threadbased model parallel computation viewed collection processes turn consists set threads called atomic computations processes communicate via messages atomic computation result processing message execution create new processes generate new messages 1 message trigger atomic computation whereas atomic computation cannot wait messages atomic computations process share one common data area thus process p k consists set atomic computations k one common data area process scheduled processor atomic computations executed processor presumed sequence indicate atomic computation carried first instead depends order arrival messages figure 1 shows general organization processes atomic computations common data areas general number processes much larger number processors processes moved around balance load p kernel runtime support system simd machine built manipulate schedule processes well messages program written p kernel language consists mainly common data area process atomic computation figure 1 process atomic computation common data area collection process definitions subroutine definitions process definition includes process name preceded keyword process followed process body shown process procname f common data area declarations entry label1 message msg1 code1 entry label2 message msg2 code2 g boldface letters denote keywords language process body enclosed braces consists declarations private variables constitute common data area process followed group atomic computation definitions atomic computation definition starts keyword entry label followed declaration corresponding message arbitrary user code one process definitions must main process first entry point main process place user program starts overall structure p kernel language differs traditional programming languages mainly explicit declarations basic units allocation processes b basic units indivisible computation atomic computations c communication media mes sages fundamental structures available computation carried parallel assistance primitive functions provided p kernel oscreateproc os etc user write program p kernel language deal creation processes send messages details computation model language refer 29 following illustrate write program p kernel language using nqueen problem example algorithm used attempts place queens board one row time particular position valid queen placed board positions row column diagonals marked invalid queen placement program sketched figure 2 atomic computation queeninit main process creates n processes type subqueen empty board one candidate queen column first row two types atomic computations process subqueen parallelqueen responsequeen common data area consists solutioncount responsecount atomic computation parallelqueen receives message represents current placement queens position next queen placed following invalidation processing creates new subqueen seqqueen processes placing one queen every valid position next row atomic computation responsequeen processes subqueen main counts total number successful queen configurations triggered number times response expected child process atomic computation sequentialqueen invoked rest rows manipulated sequentially granularity controlled example two process definitions besides process main atomic computations share common data area single process parallelqueen responsequeen atomic computation sequentialqueen share common data area atomic computations therefore separate process preserve good data encapsulation save memory space general atomic computations logically coherent share common data area process 3 design implementation main loop p kernel system shown figure 3 starts system phase includes placing processes transferring data messages selecting atomic computations execute followed user program phase execute selected atomic computation iteration continue computations completed p kernel software consists three major components computation selection communication memory management 31 computation selection fundamental difference mimd simd systems degree synchronization required mimd system different processors execute different threads code simd system p kernel system implemented mimd machine atomic computation executed next individual decision processor whereas due lockstep synchronization simd machine issue becomes global decision lets assume k atomic computation types corresponding atomic computation definitions represented lifetime execution total number atomic computations executed far k atomic computations dynamically distributed among processors iteration computation selection function f applied select atomic computation type k k following user program phase iteration processor active least one atomic computation selected type k let function numi p record number atomic computation type processor p iteration let function acti count number process main entry queeninit message msg1 f int k read n input oscreateprocsubqueenparallelqueenmsg21kempty board entry responsequeen message msg3m f print solutions solutioncount process subqueen entry parallelqueen message msg2ijboard f int k invalidate row column j diagonals ij position i1k marked valid f ni larger grainsize else entry responsequeen message msg3m f process seqqueen f entry sequentialqueen message msg2ijboard f int k count call sequential routine recursively generating valid configurations figure 2 nqueen program system phase process placement message transmission computation selection user program atomic computations start figure 3 flow chart p kernel system active processors iteration atomic computation type selected n number processors present three computation selection algorithms first one f cyc simple algorithm algorithm cyclic algorithm basically repeatedly cycles atomic computation types however acti equal zero type skipped always necessary carry k reductions compute acti since long first nonzero acti found value function f cyc determined algorithm similar method used instructionlevel approach processors repeatedly cycle entire set possible instructions global reduction essentially similar globalor method used reduce number instructions emanated execution iteration however instructionlevel approach processor executes exactly one instruction per cycle threadlevel approach processor may execute many threads per cycle also execution order program fixed instructionlevel approach order exchanged threadbased approach complete computation shortest time number iterations mini mized maximizing processor utilization iteration one possible heuristics 900 computation selection function ft selecting k 2 intuitively better leading immediate good processor utilization auction algorithm f auc proposed based observation algorithm ii auction algorithm atomic computation calculate acti iteration atomic computation maximum value acti chosen execute next f auc cyclic algorithm nonadaptive sense selection made almost independent distribution atomic computations way could case processors executing one atomic computation type many processors waiting execution atomic computation types auction algorithm runtime adaptive maximize utilization cases adaptive algorithm sophisticated general however experimental results show cases cyclic algorithm performs better auction algorithm observed auction algorithm applied near end execution parallelism becomes smaller smaller program takes long time finish low parallelism phenomenon degrades performance seriously characterized tailing effect propose improved adaptive algorithm overcome tailing effect retain advantage auction algorithm intend maximize processor utilization long large pool atomic computations available hand available parallelism falls certain degree try exploit large parallelism assigning priorities different atomic computation types atomic computation whose execution increases parallelism gets higher priority vice versa priority either assigned programmer automatically generated dependency analysis algorithm iii priority auction algorithm simplicity assume atomic computations 0 presorted according priorities 0 highest priority ak gamma1 lowest use gauge available parallelism c constant n number processors f pri max 0ik acti larger indicating degree available parallelism high auction algorithm applied otherwise among atomic computation types one highest priority executed next constant c set 05 half processors active auction algorithm used maximize processor utilization otherwise priority considered favor parallelism increase tailing effect prevention table iii execution time 12queen problem cyclic auction priority auction 104 seconds 111 seconds 101 seconds algorithm constantly provide better performance provided cyclic algorithm table iii shows performance different computation selection algorithms 12queen problem 1kprocessor mp1 32 communication two kinds messages transferred one data message specifically addressed existing process kind process message represents newly generated process process messages transferred depends scheduling strategy used two kinds messages handled separately transfer data messages assume processor initially holds 0 p data messages sent end computation phase simd characteristics one message transferred time thus message transfer step must repeated least real situation even complicated time message transfer collision may occur two messages different processors destination processor therefore need prevent message loss due collision let destp destination message processor p srcq source processor q going receive message collision two processors p 1 p 2 sending messages processor q destp 1 q processor q receive one say successfully deliver message destination general perform parallel assignment operation processors collision happens sendwithoverwrite semantics applied collision prevention scheme applied processors p must wait next time compete first transfer data messages may still unsent messages processor p one message send 0 p 1 every processor able send message first transfer due collisions hence long k 0 need continue k k1 dm message transfer process illustrated figure 4 many optimizations could applied reduce number time steps send messages one let destp 1 destp destp destp figure 4 message transfer collision notice later transfers likely processors actively sending messages resulting low utilization simd system avoid case require messages transferred instead attempt send majority data messages residual messages buffered sent next iteration atomic execution model processors fail send messages stalled instead processor continue execution long messages queue use theta k measure percentage data messages left k times data transfers thus constant used control number transfers limiting theta k algorithm data message transfer summarized figure 5 experiment value determined around 02 figure 6 shows example 12queen problem mp1 minimal execution time reached theta k processor p k p 0 assign destp srcp send data messages else assign theta k processor k p 0 buffer residual data messages figure 5 data message transfer procedure process placement handling process messages almost data messages except need assign destination processor id process message assignment called process placement random placement algorithm implemented p kernel random placement algorithm simple moderate performance algorithm n number processors destp assigned follow procedure transfer data messages however two kinds messages different destp data message fixed whereas process message varied taking advantage able reschedule process message destination processor could accept newly generated process resource execution time figure execution time different values 12queen constraint collision rescheduling simply task assigns another random number destination processor id repeat rescheduling process messages assigned destination processor ids however better choice offer one two chances rescheduling instead repeating satisfaction thus unsuccessfully scheduled process messages buffered similar residual data messages wait next communication phase load balancing strategy called runtime incremental parallel scheduling implemented mimd version p kernel 31 scheduling strategy system scheduling activity alternates underlying computation work implementation simd machine expected deliver better performance random placement 33 memory management simd machines massively parallel processors system one thousands processors could easily run memory resulting system failure certainly undesired situation ideally system failure avoided unless memory space processors exhausted memory management provides features improve system robustness available memory space specific processor becomes tight restrict new resource consumption release memory space moving unprocessed processes processors p kernel system implemented simd machines use two marks m1 m2 identify state memory space usage function jp used measure current usage memory space processor p allocated memory space processor p total memory space processor p processor p said normal state 0 jp m1 nearlyfull state full state m2 jp 1 emergency state running memory ie nearlyfull state need limit new resource consumption since available memory space getting tight accomplished preventing newly generated process messages scheduled thus process message scheduled processor p m1 jp rescheduling performed find another destination processor full state addition action taken nearlyfull state restricted scheme applied data messages addressed processor p m2 jp deferred buffered original processor wait change destination processors state although data messages eventually sent destination processor delay sending help processor relax memory tightness note deferred data messages buffered separately counted calculating theta k emergency state processor p runs memory several actions taken declare failure one clear residual data messages process messages another redistribute unprocessed process messages previously placed processor memory space released time rescue processor emergency state let system continue 4 performance p kernel currently written mpl running 16kprocessor maspar mp1 32k bytes memory per processor mpl cbased data parallel programming language tested p kernel system using two sample programs nqueen problem gromos molecular dynamics program 38 37 gromos loosely synchronous problem test data bovine superoxide dismutase molecule sod 6968 atoms 28 cutoff radius predefined 8 12 16 total execution time p kernel program consists two parts time execute system program sys time execute user program usr system efficiency defined follows total execution time system efficiency example shown figure 7 04 3 system efficiency depends ratio system overhead grainsize atomic computations table iv shows system efficiency 1kprocessor mp1 high efficiency results high ratio granularity system overhead p kernel executes loosely synchronous fashion divided iterations iteration execution consists system program phase user program phase execution time system program varies 8 20 milliseconds mp1 average grainsizes user phase test programs 55 330 milliseconds system phase every processor participates global actions hand user program phase processors involved execution selected atomic idle system phase user computation0104030304phase 1 phase 2 phase 3 figure 7 illustration efficiencies computation iteration ratio number participating processors n active total number processors n defined utilization ut active example utilization iteration 1 figure 7 75 since three four processors active utilization efficiency defined follows active table iv system efficiencies sys nqueen gromos 12queen 13queen 14queen 8 12 16 934 922 903 980 986 985 execution time tth iteration utilization efficiency depends computation selection strategy load balancing scheme table v shows utilization efficiencies different problem sizes priority auction algorithm random placement load balancing table v utilization efficiencies sys nqueen gromos 12queen 13queen 14queen 8 12 16 770 892 962 802 873 896 irregular problems grainsizes atomic computations may vary substantially grainsize variation heavily depends irregular problem program partitioned iteration execution time user phase depends largest atomic computation among active processors processors execute atomic computation smaller grainsizes fully occupy time period use index called fullness measure grainsize variation atomic computation iteration pn active active user computation time kth participated processor iteration example fullness iteration 1 figure 7 04 fullness efficiency defined active active pn active active table vi shows fullness efficiencies different problem sizes nqueen extremely irregular problem substantial grainsize variation low fullness efficiency gromos program loosely synchronous grainsize variation small table vi fullness efficiencies sys nqueen gromos 12queen 13queen 14queen 8 12 16 192 180 176 946 961 982 define overall efficiency follows full system efficiency increased reducing system overhead increasing grainsize realistic expect low system overhead system overhead dominated communication overhead major technique increasing system efficiency grainsize control average grainsize atomic computations much larger system overhead utilization efficiency depends computation selection load balancing algorithms algorithms discussed paper deliver satisfactory performance long number processes much larger number processors difficult task reduce grainsize variation determines fullness efficiency grainsize variation depends characteristics application problem however still possible reduce grainsize variation first method select proper algorithm second method select good program partition given application may different algorithms solve different partitioning patterns may small grainsize variation may carefully selected algorithm partitioning pattern result higher fullness efficiency overall efficiencies nqueen problem gromos program 1k processor mp1 shown table vii compared nqueen problem gromos much higher efficiency mainly small grainsize variation table vii efficiencies nqueen gromos 12queen 13queen 14queen 8 12 16 138 152 153 744 827 867 tables viii ix show execution times speedups nqueen problem gromos program respectively high speedup obtained gromos program nqueen difficult problem solve fairly good performance achieved table viii nqueen execution times speedups mp1 number 12queen 13queen 14queen processors time sec speedup time sec speedup time sec speedup table ix gromos execution times speedups mp1 number 8 12 16 processors time sec speedup time sec speedup time sec speedup 1k 133 761 343 867 686 887 8k 253 4001 690 4209 107 5688 5 concluding remarks motivation research twofold prove whether simd machine handle asynchronous application problems serving generalpurpose machine study feasibility providing truly portable parallel programming environment simd mimd machines first version p kernel written cm fortran running 4kprocessor tmc cm2 1991 performance reported 32 fortran best language system programs used multiple dimension array indirect addressing implement queues hence indirect addressing extremely slow accessing different addresses cm2 costed much 9 1993 p kernel rewritten mpl ported maspar mp1 mpl version reduces system overhead improves performance substantially experimental results shown p kernel able balance load fairly well simd machines nonuniform applications system overhead reduced minimum granularity control acknowledgments grateful reinhard hanxleden providing gromos program terry clark providing sod data also thank alan karp guy steele hank dietz jerry roth comments would like thank anonymous reviewers constructive comments research partially supported nsf grants ccr9109114 ccr8809615 performance data gathered mp1 npac syracuse university r model concurrent computation distributed systems linda friends hierarchical onlogn force calculation algorithm vector models dataparallel computing linda context data parallel simulation using timewarp connection machine evaluating parallel languages molecular dynamics computations multiple instruction multiple data emulation connection machine massively parallel mimd implemented simd hardware scheduling parallel program tasks onto arbitrary target machines solving problems concurrent processors architecture problems portable parallel software systems comparison clustering heuristics scheduling dags multiprocessors microprocessorbased hypercube supercomputer data parallel algorithms graphinators duality simd mimd babb ii david c dap prolog setoriented approach prolog logic simulation massively parallel architectures simulating applicative architectures connection machine exploration asynchronous dataparallelism flat ghc implementation supercomputers massively parallel implementation flat ghc connection machine mimd execution simd computers array processor supercomputers reactive kernel molecular dynamics simulation superoxide interacting superoxide dismutase chare kernel implementation multicomputers chare kernel runtime support system parallel computations runtime incremental parallel scheduling rips distributed memory computers solving dynamic irregular problems simd architectures runtime support portable multicomputer communication library atop reactive kernel thinking machines corp thinking machines corp indirect addressing load balancing faster solution mandelbrot set simd architectures relaxing simd control flow constraints using loop transformations gromos groningen molecular simulation software solving nonuniform problems simd computers case study region growing exploiting simd computers general purpose computation concurrent execution noncommunicating programs simd processors programming aid messagepassing systems tr ctr nael b abughazaleh philip wilsey xianzhi fan debra hensgen synthesizing variable instruction issue interpreters implementing functional parallelism simd computers ieee transactions parallel distributed systems v8 n4 p412423 april 1997 andrea di blas arun jagota richard hughey optimizing neural networks simd parallel computers parallel computing v31 n1 p97115 january 2005 mattan erez jung ho ahn jayanth gummaraju mendel rosenblum william j dally executing irregular scientific applications stream architectures proceedings 21st annual international conference supercomputing june 1721 2007 seattle washington