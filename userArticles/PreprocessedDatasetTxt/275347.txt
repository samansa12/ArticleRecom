generalized queries probabilistic contextfree grammars abstractprobabilistic contextfree grammars pcfgs provide simple way represent particular class distributions sentences contextfree language efficient parsing algorithms answering particular queries pcfg ie calculating probability given sentence finding likely parse developed applied variety patternrecognition problems extend class queries answered several ways 1 allowing missing tokens sentence sentence fragment 2 supporting queries intermediate structure presence particular nonterminals 3 flexible conditioning variety types evidence method works constructing bayesian network represent distribution parse trees induced given pcfg network structure mirrors chart standard parser generated using similar dynamicprogramming approach present algorithm constructing bayesian networks pcfgs show queries patterns queries network correspond interesting queries pcfgs network formalism also supports extensions encode various context sensitivities within probabilistic dependency structure b introduction patternrecognition problems start observations generated structured stochastic process probabilistic contextfree grammars pcfgs 1 2 provided useful method modeling uncertainty wide range structures including natural languages 2 programming languages 3 images 4 speech signals 5 rna sequences 6 domains like plan recognition nonprobabilistic grammars provided useful models 7 may also benefit explicit stochastic model created pcfg model process apply existing pcfg parsing algorithms answer variety queries instance standard techniques efficiently compute probability particular observation sequence find probable parse tree sequence section ii provides brief description pcfgs associated algorithms however techniques limited types evidence exploit types queries answer particular existing pcfg techniques generally require specification complete observation sequence many contexts may partial sequence available also possible may evidence beyond simple observations example natural language processing may able exploit contextual information sentence determining beliefs certain unobservable variables parse tree addition may interested computing probabilities alternate types events eg future observations abstract features parse extant techniques directly support restricted query classes addressed existing algorithms limit applicability pcfg model domains may require answers complex queries flexible expressive representation distribution structures generated grammar would support broader forms evidence queries supported specialized algorithms currently exist adopt bayesian networks purpose define algorithm generate network representing distribution possible parse trees specified string length generated flies 04 like 04 flies 045 verb pp 02 noun ants 05 fig 1 probabilistic contextfree grammar charniak 2 given pcfg section iii describes algorithm well algorithms extending class queries include conditional probability symbol appearing anywhere within region parse tree conditioned evidence symbols appearing parse tree restrictive independence assumptions pcfg model also limit applica bility especially domains like plan recognition natural language complex dependency structures flexible framework bayesiannetwork representation supports extensions contextsensitive probabilities probabilistic parse tables briscoe carroll 8 section iv explores several possible ways relax independence assumptions pcfg model within approach modified versions pcfg algorithms support class queries supported contextfree case ii probabilistic contextfree grammars probabilistic contextfree grammar tuple hsigma disjoint sets sigma n specify terminal nonterminal symbols respectively 2 n start symbol p set productions take form probability e expanded string sum probabilities p expansions given nonterminal e must one examples paper use sample grammar charniak 2 shown fig 1 definition pcfg model prohibits rules empty string however rewrite pcfg eliminate rules still represent original distribution 2 long note probability prs clarity algorithm descriptions paper assume prs negligible amount additional bookkeeping correct nonzero probability probability applying particular production intermediate string conditionally independent productions generated string productions applied symbols string given presence e therefore probability given derivation simply product probabilities individual productions involved define parse tree representation derivation nonprobabilistic contextfree grammars 9 probability string language sum taken possible derivations vpverb np pp 00014 vpverb np 000216 npnoun pp 0036 npnoun verbflies 04 preplike 10 nounants 05 nounswat nounflies 045 verblike 04 fig 2 chart swat flies like ants standard pcfg algorithms since number possible derivations grows exponentially strings length direct enumeration would computationally viable instead standard dynamic programming approach used probabilistic nonprobabilistic cfgs 10 exploits common production sequences shared across derivations central structure table chart storing previous results subsequence input sentence entry chart corresponds subsequence x observation string symbol e entry contains probability corresponding subsequence derived symbol prx je index refers position subsequence within entire terminal string indicating start sequence index j refers length subsequence bottom row table holds results subsequences length one top entry holds overall result probability observed string compute probabilities bottomup since know prx e observed symbol x define probabilities recursively sum productions product p delta altering procedure take maximum rather sum yields probable parse tree observed string algorithms require time ol 3 string length l ignoring dependency size grammar compute probability sentence swat flies like ants would use algorithm generate table shown fig 2 eliminating unused intermediate entries also separate entries production though necessary interested final sentence probability top entry two listings production np vp different subsequence lengths righthand side symbols sum probabilities productions lefthand side entry yields total sentence probability 0001011 algorithm capable computing inside probability probability particular string appearing inside subtree rooted particular symbol work topdown analogous manner compute outside probability 2 probability subtree rooted particular symbol appearing amid particular string given probabilities compute probability particular nonterminal appearing parse tree root subtree covering subsequence example sentence swat flies like ants compute probability like ants prepositional phrase using combination inside outside probabilities swat 111 verb 112 flies 211 prep 312 noun 412 ants 411 like 311 noun 212 fig 3 parse tree swat flies like ants j labeled lefttoright inside lri algorithm 10 specifies use inside probabilities obtain probability given initial subsequence probability sentence length beginning words swat flies furthermore use initial subsequence probabilities compute conditional probability next terminal symbol given prefix string b indexing parse trees yet conceivable queries covered existing algorithms answerable via straightforward manipulations inside outside probabilities example given observations arbitrary partial strings unclear exploit standard chart directly similarly unaware methods handle observation nonterminals eg last two words form prepositional phrase seek therefore mechanism would admit observational evidence form part query pcfg without requiring us enumerate consistent parse trees first require scheme specify events appearance symbols designated points parse tree use indices j delimit leaf nodes subtree standard chart parsing algorithms example pp node parse tree fig 3 root subtree whose leaf nodes like ants 2 however cannot always uniquely specify node two indices alone branch parse tree passing np n flies three nodes differentiate introduce k index defined recursively node child j indices index one k index child thus flies node noun node parent np 3 labeled node parse tree fig 3 j indices think k index node level abstraction higher values indicating abstract symbols instance flies symbol specialization noun concept turn specialization np concept possible specialization corresponds abstraction production form one symbol righthand side parse tree involving production nodes e e 0 identical j values k value e one e 0 denote set abstraction productions pa p productions decomposition productions set two symbols righthand side node e expanded decomposition production sum j values children equal j value since length original subsequence derived e must equal total lengths subsequences children addition since child must derive string nonzero length child j index e must therefore abstraction productions connect nodes whose indices match j components decomposition productions connect nodes whose indices differ iii bayesian networks pcfgs bayesian network 11 12 13 directed acyclic graph nodes represent random variables associated node specification distribution variable conditioned predecessors graph network defines joint probability distributionthe probability assignment random variables given product probabilities node conditioned values predecessors according assignment edges included graph indicate conditional independence specifically node conditionally independent nondescendants given immediate predecessors algorithms inference bayesian networks exploit independence simplify calculation arbitrary conditional probability expressions involving random variables expressing pcfg terms suitable random variables structured bayesian network could principle support broader class inferences standard pcfg algorithms demonstrate expressing distribution parse trees given probabilistic grammar incorporate partial observations sentence well forms evidence determine resulting probabilities various features parse trees pcfg random variables base bayesiannetwork encoding pcfgs scheme indexing parse trees presented section iib random variable n ijk denotes symbol parse tree position indicated j indices looking back example parse tree fig 3 symbol e labeled j indicates n combinations appearing tree correspond n variables taking null value nil assignments variables n ijk sufficient describe parse tree however construct bayesian network using variables dependency structure would quite complicated example example pcfg fact n 213 value np would influence whether n 321 takes value pp even given parent parse tree vp thus would need additional link n 213 n 321 fact possible sibling nodes whose parents multiple expansions simplify dependency structure introduce random variables p ijk represent productions expand corresponding symbols n ijk instance add node p 141 would take value vpverb np pp example n 213 n 321 conditionally independent given p 141 link siblings necessary case however even know production p ijk corresponding children parse tree may conditionally independent instance chart fig 2 entry 14 two separate probability values production np vp corresponding different subsequence lengths symbols righthand side given production used multiple possibilities connected n variables n four sibling nodes conditionally dependent since knowing one determines values three therefore dictate variable p ijk take different values breakdown righthand symbols subsequence lengths domain p ijk variable therefore consists productions augmented j k indices symbols righthand side previous example domain p 141 would require two possible values np1 3vp3 1 numbers brackets correspond j k values respectively associated symbol know p 141 former n n probability one deterministic relationship renders child n variables conditionally independent given p ijk describe exact nature relationship section iiic2 identified random variables domains complete definition bayesian network specifying conditional probability tables representing interdependencies tables n variables represent deterministic relationship parent p variables however also need conditional probability p variable given value corresponding n variable e pcfg specifies relative probabilities different productions nonterminal must compute probability fie j analogous inside probability 2 symbol e righthand side root node subtree abstraction level k terminal subsequence length j b calculating fi b1 algorithm calculate values fi modified version dynamic programming algorithm sketched section iia standard chartbased pcfg algorithms define function recursively use dynamic programming compute values since terminal symbols always appear leaves parse tree terminal symbol x 2 sigma fix nonterminal symbol since nonterminals never leaf nodes sum productions expanding e probability production expanding e producing subtree constrained parameters j k abstraction productions possible abstraction production need probabilities e expanded e 0 e 0 derives string length j abstraction level immediately e former given probability associated production latter simply according independence assumptions pcfg model expansion e 0 independent derivation joint probability simply product compute probabilities every abstraction production expanding e since different expansions mutually exclusive events value fie j merely sum separate probabilities assume abstraction cycles grammar sequence productions since cycle existed recursive calculation would never halt assumption necessary termination standard parsing algorithm assumption restrict classes grammars algorithms applicable restrictive domains interpret productions specializations since cycles would render abstraction hierarchy impossible productions possible decomposition production need probability e thus expanded e derives subsequence appropriate length former given p latter computed values fi function must consider every possible subsequence length j e addition could appear level abstraction k must consider possible values given subsequence length obtain joint probability combination t1 values computing since derivation independent others sum joint probabilities possible t1 yields probability expansion specified productions righthand side product resulting probability p yields probability particular expansion since two events independent sum relevant decomposition productions find value fie j 1 algorithm fig 4 takes advantage division abstraction decomposition productions compute values fie j strings bounded length array kmax keeps track depth abstraction hierarchy subsequence length b2 example calculations illustrate computation fi values consider result using charniaks grammar fig 1 input initialize entries probability one terminal symbol fig 5 fill entries look abstraction productions symbols noun verb prep expanded one terminal symbols nonzero fi values 1 enter three nonterminals values equal sum relevant abstraction productions product probability given production value righthand symbol 1 instance compute value noun adding product probability nounswat value swat nounflies flies nounants ants yields value one since noun always derive string length one single level abstraction terminal string given grammar abstraction phase continues find 4 abstractions go computebetagrammarlength symbol x 2terminalsgrammar symbol e 2nonterminalsgrammar decomposition phase production sequence fj g t1 sequence fk g t1 1 k kmaxj result p 1 result abstraction phase production fie return fi kmax fig 4 algorithm computing fi values begin decomposition phase illustrate decomposition phase consider value fis 3 1 one possible decomposition production snp vp however must consider two separate cases noun phrase covers two symbols verb phrase one noun phrase covers one verb phrase two subsequence length two np vp nonzero probability bottom level abstraction length one third compute probability first subsequence length combination multiply probability production finp 2 1 fivp 1 3 probability second combination similar product sum two values provides value enter abstractions decompositions proceed along similar lines additional summation required multiple productions multiple levels abstraction possible final table shown fig 5 lists nonzero values np 00672 np 0176 np 008 vp 03 vp 01008 vp 0104 vp 012 2 prep 10 pp 0176 pp 008 pp 04 verb 10 noun 10 like 10 swat 10 flies 10 ants 10 fig 5 final table sample grammar b3 complexity analysis complexity computing fi values given pcfg useful define maximum length possible chains abstraction productions ie maximum k value maximum production length number symbols righthand side single run abstraction phase requires time ojpa j subsequence length od runs specific value j decomposition phase requires time ojpd jj decomposition production must consider possible combinations subsequence lengths levels abstractions symbol righthand side therefore whole algorithm would take time ondjp j c network generation phase use fi function calculated described compute domains random variables n ijk p ijk required conditional probabilities c1 specification random variables procedure createnetwork described fig 6 begins top abstraction hierarchy strings length n starting position 1 root symbol variable n 1nkmaxn either start symbol indicating parse tree begins nil indicating parse tree begins must allow parse tree start j k fis possibly derive strings length bounded n within language createnetwork proceeds downward n ijk random variables specifies domain corresponding production variables p ijk production variable takes values set possible expansions possible nonterminal symbols domain n ijk k 1 abstraction productions possible procedure abstractionphase described fig 7 inserts possible expansions draws links p ijk random variable n ijkgamma1 takes value righthand side symbol procedure decompositionphase described fig 8 performs analogous task decomposition productions except must also consider possible length breakdowns abstraction levels symbols righthand side fis length insertstaten 1lengthkmaxlength startprobfikmaxlengthkmaxlengthgamma1 00 insertstaten 1lengthkmaxlength nil k kmaxj downto 1 symbol e 2domainn ijk else else fig 6 procedure generating network production insertstaten fig 7 procedure finding possible abstraction productions production sequence fj g t1 sequence fk g t1 1 k kmaxj insertstatep ijk 1 fig 8 procedure finding possible decomposition productions fis insertstatep ijk nil sj insertstaten ijkgamma1 else insertstatep ijk nil insertstaten ij gamma1kmaxj gamma1 startprobfikmaxjk 00 insertstatep ijk nil nil addparentn ijkgamma1 p ijk else addparentn ij gamma1kmaxj gamma1 p ijk insertstaten ij gamma1kmaxj gamma1 nil fig 9 procedure handling start parse tree next level j0 return 00 else k0 return else return fis fig 10 procedure computing probability start tree occurring particular string length abstraction level createnetwork calls procedure starttree described fig 9 handle possible expansions nil either nil indicating tree starts immediately nil nil indicating tree starts starttree uses procedure startprob described fig 10 determine probability parse tree starting anywhere current point expansion insert possible value domain production node add parent nodes corresponding symbol righthand side also insert symbol righthand side domain corresponding symbol variable algorithm descriptions assume existence procedures insertstate addparent procedure insertstatenodelabel inserts new state name label domain variable node procedure addparentchildparent draws link node parent node child c2 specification conditional probability tables createnetwork specified domains random variables specify conditional probability tables introduce lexicographic order oe set fj kj1 j purposes simplicity specify exact value probability specify weight compute exact probabilities normalization divide weight sum prior probability table top node parents defined follows jkoenkmaxn given state ae domain p ijk node ae represents production corresponding assignment j k values symbols righthand side form p define conditional probability state e p symbol e domain n ijk prp productions starting delaying tree probabilities j probability tables n ijk nodes much simpler since productions specified symbols completely determined therefore entries either one zero example consider nodes n parent node p among others rule ae representing symbols e domain n conditional probability zero fill entry configurations parent nodes represented ellipsis condition part probability though know conflicting configurations ie two productions trying specify symbol n impossible configuration parent nodes specify certain symbol indicates node takes value nil probability one c3 network generation example illustration consider execution algorithm using fi values fig 5 start root variable n 142 start symbol fi value greater zero well points domain must include nil obtain prn simply divide fis 4 2 sum fi values yielding 0055728 domain p 142 partially specified abstraction phase symbol domain n 142 one relevant production vp possible expansion since fivp 4 1 0 therefore insert production domain p 142 conditional probability one given n since possible expansions also draw link p 142 n 141 whose domain includes vp conditional probability one given p complete specification p 142 must consider possible start tree since domain n 142 includes nil conditional probability p 024356 ratio fis 4 1 sum fis j 1 link p 142 n 141 already made abstraction phase must also insert nil domain n 141 conditional probability one given appropriate value p 142 proceed n 141 bottom level abstraction must perform decomposition phase production np vp three possible combinations subsequence lengths add total length four np derives string length one vp string length three possible levels abstraction three one respectively since others zero values therefore insert production snp13 vp31 domain numbers brackets correspond subsequence length level abstraction respectively conditional probability value given n product probability production finp 1 3 fivp 3 1 normalized probabilities possible expansions draw links p 141 n 113 n 231 whose domains insert np vp respectively values obtained noting subsequence np begins point original string vp begins point shifted length subsequence np occurs probability one given value p 141 appropriate production similar actions taken possible subsequence length combinations operations random variables performed similar fashion leading network structure shown fig 11 c4 complexity network generation resulting network 2 nodes domain n i11 variable ojsigmaj states represent possible terminal symbols n ijk variables ojn possible states n variables former 2 latter k 1 p ijk variables 2 domain ojpa states p ij1 variables states possible decomposition production possible combination subsequence lengths possible level abstraction symbols righthand side therefore p ij1 variables domain ojpd jj states defined maximum value k maximum production length unfortunately even though particular p variable corresponding n variable parent given n variable could potentially p variables fig 11 network example grammar maximum length 4 parents size conditional probability table node exponential number parents although given n determined one p ie interactions possible specify table linear number parameters define maximum number entries conditional probability table network abstraction phase algorithm requires time ojpa jt decomposition phase requires time ojpd jn handling start parse tree potential space holders requires time ot total time complexity algorithm 2 jp jn ojp jn m1 dwarfs time complexity dynamic programming algorithm fi function however network created particular grammar length bound pcfg queries use bayesian network compute joint probability express terms n p random variables included network standard bayesian network algorithms 11 12 14 return joint probabilities form conditional probabilities form prx x either n p obviously interested whether symbol e appeared particular location parse tree need examine marginal probability distribution corresponding variable thus single network query yield probability prn e results network query implicitly conditional event length terminal string exceed n obtain joint probability multiplying result probability string language length exceeding n j probability expand start symbol terminal string length j sum obtain appropriate unconditional probability query network queries reported section must multiplied d1 probability conjunctive events bayesian network also supports computation joint probabilities analogous computed standard pcfg algorithms instance probability particular terminal string swat flies like ants corresponds probability ants probability initial subsequence like swat flies computed lri algorithm 10 corresponds probability prn like since bayesian network represents distribution strings bounded length find initial subsequence probabilities completions length bounded however although case bayesian network approach requires modification answer query standard pcfg algorithm needs modification handle complex types evidence chart parsing lri algorithms require complete sequences input gaps uncertainty particular symbols would require direct modification dynamic programming algorithms compute desired probabilities bayesian network hand supports computation probability evidence regardless structure stance sentence swat flies ants know third word single network query provide conditional probability possible completions well probability specified evidence prn approach handle multiple gaps well partial information example know exact identity third word sentence swat flies ants know either swat like use bayesian network fully exploit partial information augmenting query specify domain values n 311 swat like zero probability although types queries rare natural language domains like speech recognition often require ability reason presented noisy observations answer queries nonterminal symbols well instance sentence swat flies like ants query network obtain conditional probability like ants prepositional phrase prn like ants answer queries specify evidence nonterminals within parse tree instance know like ants prepositional phrase input network query specify n well specifying terminal symbols alternate network algorithms compute probable state random variables given evidence instead conditional probability 11 15 14 example consider case possible fourword sentences beginning phrase swat flies probability maximization network algorithms determine probable state terminal symbol variables n 311 n 411 like flies given n 111 swat n 211 flies n 511 nil d2 probability disjunctive events also compute probability disjunctive events multiple network queries express event union mutually exclusive events form x query network compute probability sum results obtain probability union instance want compute probability sentence swat flies like ants contains prepositions would query network probabilities domain like plan recognition query could correspond probability agent performed complex action within specified time span example individual events already mutually exclusive sum results produce overall probability general ensure mutual exclusivity individual events computing conditional probability conjunction original query event negation events summed previously example overall probability would prn prepjeprn prep n 312 6 prepje e corresponds event sentence swat flies like ants bayesian network provides unified framework supports computation probabilities described compute probability event set mutually exclusive events fx t1 j t1 k t1 t1 x either n p also compute probabilities events specify relative likelihoods instead strict subset restrictions addition given event determine probable configuration uninstantiated random variables instead designing new algorithm query express query terms networks random variables use bayesian network algorithm compute desired result d3 complexity network queries unfortunately time required standard network algorithms answering queries potentially exponential maximum string length n though exact complexity depend connectedness network particular network algorithm chosen algorithm current implementation uses great deal preprocessing compiling networks hope reducing complexity answering queries algorithm exploit regularities networks eg conditional probability tables n ijk consist zeroes ones provide reasonable response time answering queries unfortunately compilation prohibitive often produce networks exponential size exist bayesian network algorithms 16 17 offer greater flexibility compilation possibly allowing us limit size resulting networks still providing acceptable query response times determining optimal tradeoff require future research determining class domains bayesian network approach preferable existing pcfg algorithms clear standard dynamic programming algorithms efficient pcfg queries address domains requiring general queries types described flexibility bayesian network approach may justify greater complexity iv context sensitivity many domains independence assumptions pcfg model overly restrictive definition probability applying particular pcfg production expand given nonterminal independent symbols come expansions occur even papers simplified example illustrates weaknesses assumption consider intermediate string swat ants like noun implausible probability expand noun flies instead ants independent choice swat verb choice ants object course may able correct model expanding set nonterminals encode contextual information adding productions expansion thus preserving structure pcfg model however obviously lead unsatisfactory increase complexity design use model instead could use alternate model relaxes pcfg independence assumptions model would need complex production andor probability structure allow complete specification distribution well modified inference algorithms manipulating distribution direct extensions network structure bayesian network representation probability distribution provides basis exploring context sensitivities networks generated algorithms paper implicitly encode pcfg assumptions assignment single nonterminal node parent production node single link indicates expansion conditionally independent nondescendant nodes know value nonterminal could extend contextsensitivity expansions within network formalism altering links associated production nodes introduce context sensitivity even without adding links since production node conditional probability table define production probabilities function j values instance number words group strongly influences likelihood group forming noun phrase could model belief varying probability np appearing different string lengths encoded j index cases modify standard pcfg representation probability information associated production function j k instead constant dynamic programming algorithm fig 4 easily modified handle production probabilities depend j k however dependency index well would require adding parameter fi introducing additional loop possible values would replace reference production probability either dynamic programming network generation algorithm appropriate function j k alternatively may introduce additional dependencies nodes net work pcfg extension conditions production probabilities parent lefthand side symbol already proved useful modeling natural language 18 case production set associated probabilities one nonterminal symbol possible parent symbol lefthand side new probability structure requires modifications dynamic programming network generation algorithms must first extend probability information fi function include parent nonterminal additional parameter straightforward alter dynamic programming algorithm fig 4 correctly compute probabilities bottomup fashion modifications network generation algorithm complicated whenever add p ijk parent symbol node n k also add n ijk parent p k example dotted arrow subnetwork fig 12 represents additional dependency p 112 n 113 must add link n 112 possible child nonterminal indicated link p 113 conditional probability tables p node must specify probabilities given current nonterminal parent nonterminal symbols compute combining modified fi values conditional production probabilities returning example beginning section may want condition production probabilities terminal string expanded far first approximation context sensitivity imagine model production associated set probabilities one terminal symbol language represents conditional probability particular expansion given corresponding terminal symbol occurs immediately previous subsequence derived nonterminal symbol lefthand side fi function requires additional parameter need modified version dynamic programming algorithm compute values however network generation algorithm needs introduce one additional link n i11 p i1jk node dashed arrows fig 12 subnetwork incorporating parent symbol dependency fig 13 subnetwork capturing dependency previous terminal symbol subnetwork fig 13 reflect additional dependencies introduced context sensitivity using network example fig 11 p 1jk nodes special case preceding terminal steps original algorithm sufficient extend conditioning cover preceding terminal sequences rather individual symbols production could associated set probabilities one possible terminal sequence length bounded parameter h fi function requires additional parameter specifying preceding sequence network generation algorithms must add links p ijk nodes n igammah11 h n 111 n igamma111 h conditional probability tables specify probability particular expansion given symbol lefthand side preceding terminal sequence many cases may wish account external influences explicit context representation natural language problems influences current world state planning required many plan recognition problems 19 instance processing multiple sentences may want draw links symbol nodes one sentence production nodes another reflect thematic connections long network include random variables represent external context represent dependency adding links corresponding nodes appropriate production nodes altering conditional probability tables reflect effect context general bayesian networks currently generated contain set random variables sufficient expressing arbitrary parse tree events introduce context sensitivity adding appropriate links production nodes events wish condition expansion probabilities correct network use query algorithms section iiid produce corresponding conditional probability b extensions grammar model context sensitivities expressed incremental changes network dependency structure represent minor relaxation conditional independence assumptions pcfg model global models context sensitivity likely require radically different grammatical form probabilistic interpretation framework historybased grammar hbg 20 provides rich model context sensitivity conditioning production probabilities potentially entire parse tree available current expansion point since bayesian networks represent positions parse tree theoretically possible represent conditional probabilities introducing appropriate links however since hbg model uses decision tree methods identify equivalence classes partial trees thus produce simple event structures condition unclear exactly replicate behavior systematic generation algorithm restrict types context sensitivity likely find network generation algorithm nonstochastic case contextsensitive grammars 9 provide structured model general unrestricted grammar allowing productions form ff 1 ffs arbitrary sequences terminal andor nonterminal symbols restriction eliminates productions righthand side shorter lefthand side production indicates expanded b appears surrounding context ff 1 immediately precedent ff 2 immediately subsequent therefore perhaps extension probabilistic contextsensitive grammar pcsg similar pcfgs could provide even richer model types conditional probabilities briefly explored intuitive extension involves associating likelihood weighting contextsensitive production computing probability particular derivation based weights weights cannot correspond probabilities know priori expansions may applicable given point parse due different possible contexts therefore set fixed production values may produce weights sum one particular context instead use weights determine probabilities know productions applicable probability particular derivation sequence uniquely determined though could sensitive order apply productions could define probability distribution strings contextsensitive language probability particular string sum probabilities possible derivation sequences string definition appears theoretically sound though unclear whether realworld domains exist model would useful create model able generate bayesian network proper conditional dependency structure represent distribution would draw links production node potential context nodes conditional probability tables would reflect production weights particular context possibility open question whether could create systematic generation algorithm similar defined pcfgs although proposed pcsg model cannot account dependence position parent symbol described earlier section could make similar extensions account types dependencies result would similar contextsensitive probabilities pearl 21 however pearl conditions probabilities partofspeech trigram well sibling parent nonterminal symbols allow model specify conjunctions contexts may able represent types probabilities well general contexts beyond siblings trigrams clearly difficult select model powerful enough encompass significant set useful dependencies restricted enough allow easy specification productions probabilities particular language chosen grammatical formalism capable representing context sensitivities wish model must define network generation algorithm correctly specify conditional probabilities production node however network use query algorithms section iiid thus unified framework performing inference regardless form language model used generate networks probabilistic parse tables 8 stochastic programs 22 provide alternate frameworks introducing context sensitivity former approach uses finitestate machine chart parser underlying structure introduces context sensitivity transition probabilities stochastic programs represent general stochastic processes including pcfgs ability maintain arbitrary state information could support general context sensitivity well unclear whether approaches advantages generality efficiency others v conclusion algorithms presented automatically generate bayesian network representing distribution parses strings bounded length parameter language pcfg first stage uses dynamic programming approach similar standard parsing algorithms second stage generates network using results first stage specify probabilities network generated particular pcfg length bound created use network answer variety queries possible strings parse trees using standard bayesian network inference algorithms compute conditional probability probable configuration collection basic random variables given event expressed terms variables algorithms implemented tested several grammars results verified existing dynamic programming algorithms applicable enumeration algorithms given nonstandard queries answering standard queries time requirements network inference comparable dynamic programming techniques network inference methods achieved similar response times types queries providing vast improvement much slower brute force algorithms however current implementation memory requirements network compilation limit complexity grammars queries unclear whether results hold larger grammars string lengths preliminary investigation also demonstrated usefulness network formalism exploring various forms contextsensitive extensions pcfg model relatively minor modifications pcfg algorithms generate networks capable representing general dependency structures required certain context sen sitivities without sacrificing class queries answer future research need provide general model context sensitivity sufficient structure support corresponding network generation algorithm although answering queries bayesian networks exponential worst case method incurs cost service greatly increased generality hope enhanced scope make pcfgs useful model plan recognition domains require flexibility query forms probabilistic structure addition algorithms may extend usefulness pcfgs natural language processing pattern recognition domains already successful acknowledgments grateful anonymous reviewers careful reading helpful suggestions work supported part grant f496209410027 air force office scientific research r introduction probabilistic languages review open questions recognition equations using twodimensional stochastic contextfree grammar stochastic grammars pattern recognition stochastic contextfree grammars modeling rna getting serious parsing plans grammatical analysis plan recognition generalized probabilistic lr parsing natural language corpora unificationbased grammars introduction automata theory basic methods probabilistic context free grammars probabilistic reasoning intelligent systems networks plausible inference probabilistic reasoning expert systems theory algorithms introduction bayesian networks bucket elimination unifying framework probabilistic inference costbased abduction map explanation topological parameters timespace tradeoff query dags practical paradigm implementing beliefnetwork infer ence contextsensitive statistics improved grammatical language models accounting context plan recognition application traffic monitoring towards historybased grammars using richer models probabilistic parsing pearl probabilistic chart parser effective bayesian inference stochastic programs tr ctr jorge r ramos vernon rego featurebased generators time series data proceedings 37th conference winter simulation december 0407 2005 orlando florida