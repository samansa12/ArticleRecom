instruction fetch mechanisms vliw architectures compressed encodings vliw architectures use wide instruction words conjunction high bandwidth instruction cache achieve multiple instruction issue report uses tinker experimental testbed examine instruction fetch instruction cache mechanisms vliws compressed instruction encoding vliws defined classification scheme ifetch hardware encoding introduced several interesting cache ifetch organizations described evaluated tracedriven simulations new ifetch mechanism using silo cache found best performance b introduction vliw architectures use wide instruction words achieve multiple instruction issue architectures require high bandwidth instruction fetch ifetch mechanisms transport instruction words cache execution pipeline complexity hardware support required ifetch related type instruction encoding used general vliw instructions horizontally encoded wide words issue operation every clock cycle functional units fus machine sequence instruction words compose program schedule c fl1996 ieee paper appear proceedings 29th annual symposium microarchitecture dec24 1996 paris france personal use material permitted however permission reprintrepublish material advertising promotional purposes creating new collective works resale redistribution servers lists reuse copyrighted component work works must obtained ieee program instruction words encoded several ways choice encoding greatly influence hardware required ifetch vliw uncompressed encoding one explicitly stores nop operations instruction word vliw instruction stores nop operation slot particular fu fu scheduled execute operation point schedule use uncompressed encoding yields fixed length instruction word simplify ifetch hardware expense potentially poor memory utilization another class encodings compressed encodings store nops vliw instructions encoded using compressed encoding variably sized size instruction dependent number fus receive operation point schedule type encoding higher memory utilization allows greater effective memory bandwidth uncompressed encoding compressed encoding also aids objectcode compatibility vliws dynamic rescheduling algorithm proposed tinker vliw testbed 9 drawback encoding requires complicated ifetch handle variable length instructions paper focuses requirements ifetch imposed compressed encoding several mechanisms ifetch presented effect mechanism instruction cache icache design described paper organized follows section 2 introduces basic instruction fetch mechanism details implementation compressed encoding previous work area also dis cussed section 3 presents scheme classifying icaches compressed encoding introduces four different ganizations uncompressed cache banked cache rigid silo cache flexible silo cache sections 41 43 describe icache designs detail including respective performance section 6 discusses performance designs also discusses directions future research 2 basic instruction fetch model 21 instruction fetch compressed encoding memoryl2 cache missrepair logic block fetch tag compare valid select nextpc computation expander execution pipeline expander expander must used either cache hit cache miss path figure 1 basic instruction fetch model necessary stages ifetch compressed encoding shown simplified instruction fetch model compressed encodings shown figure 1 solid lower borders diagram indicate pipeline latches delineate stages fetch pipeline main blocks model missrepair logic pipelined instruction cache expander discussed missrepair logic handles cache miss repair requests could implemented pipelined interface next level memory hierarchy inside instruction cache cache block holds one vliw instructions dependent instruction fetch mechanism explained depth section 3 cache pipelined consists block fetch stage selects fetches cache block presented address tag compare valid select stage performs tag comparisons parallel selecting ops block belong requested vliw instruction compressed instruction encoding using tinker experimental testbed used study 8 compressed encoding combines individual operations ops issued parallel unit parallel issue called multiop tinker op risclike instruction 64 bits length tinker op execute operation encoding pred h theader bit tail bit figure 2 integer add op tinker encoding one four types functional units integer computation predicate handling memory loads stores fp floating point addmuldivconvert branch example tinker integer add op shown figure 2 tinker encoding uses header tail bits within op delineate beginning end multiop ie first op multiop header bit set last op multiop tail bit set branch architecture compilerdirected similar playdoh specification hewlett packard laboratories 16 n issue machine tinkern maximum multiop size n 64 bits maximumsized multiop contains op functional unit machine load latency increases one less ialu nop nop nop nop nop nop nop nop g nop nop nop nop nop nop h nop f dependent c bytes total nop nop nop nop nop nop nop nop nop nop nop nop nop nop nop nop nop nop nop nop f nop nop nop nop nop nop nop g nop nop nop nop nop h nop 336 bytes total ialu fpadd fpmul ld st cmpp br ialu ialu fpadd fpmul ld st cmpp br figure 3 object code compatibility using uncompressed encoding size code changes rescheduled different machine organization useful property compressed encodings rescheduling size invariance rsi rsi means size program vary across different generations vliw architecture figure 3 shows using uncompressed nonrsi encoding size executable image change change code size cause problems branch target invalidation constrained speculation 9 solution code size change problem use rsi encoding like tinker result using tinker encoding reschedule code figure 3 shown figure 4 manipulation header tail bits pause fields requirement modification schedule execute correctly different generations architecture 1 header bit pause optype tail bit figure 4 object code compatibility using tinker compressed encoding size binary remains pause field first multiop incremented one multiop originally consisting ops e f made two multiops althougha compressed encoding several advantages also requires complex ifetch mechanism one step fetch nextpc generation pc generated subsequent icache access nextpc generation uncompressed encoding consists adding constant size fixed instruction pc using new quantity nextpc address icache next cycle architectures use variable length instructions first determine length current instruction fetched determine quantity add pc get nextpc another step ifetch determine individual ops multiop routed fus uncompressed encoding fu fixed position op multiop case compressed encoding atinkerop contains fut field indicating futype destined op destined particular futype reside anywhere multiop depending types ops multiop intrinsic compressed encoding requires ops must partially decoded routed appropriate functional unit example instruction issue logic expects integer ops op position one multiop one op multiop composed floating general relative op ordering may also change without ill effects although demonstrated example point op op must routed floating point unit integer unit expander stage performs routing routing ops one multiop parallel expander functionality full crossbar route type op fu requirement relaxed somewhat compiler enforces partial ordering ops based futype expander placed either cache hit path cache miss path shown figure 1 miss path expander used cache miss occurs operates follows 1 multiop fetched memory placed expander 2 entire multiop received expander routes ops specific positions cache selected miss address ie cache holds ops specific positions corresponding futypes miss path expansion adds extra stages miss penalty number extra stages equal number stages needed expansion contrast hit path expander used every cache access multiop fetched cache processed expander optofunctionalunit routing expander cache miss path therefore affect miss penalty however number cycles needed expansion fetch path therefore adds branch misprediction penalty reason hit path expansion performed one cycle requiring complex potentially costly implementation meet single cycle constraint 22 related work classes encodings also used vliw architectures 4s architecture proposed sun microsystems used frame encoding grouped multiops instruction frames instruction frame size ifetch width machine frame encoding supports variable size instructions enforces restriction ops multiop must reside frame ease requirements ifetch mechanism 3 requires nops thereby violating rsi cydrome cydra 5 vliw machine used split encoding instruction cache blocks composed either one multiopor multiple one op multiops called uniops 20 5 cache blocks composed one multiop uncompressed form composed uniops padded nops needed cache block alignment also nonrsi another commercial vliw architecture multiflow trace family machines used compressed encoding 17 nops stored instruction words memory instructions expanded fetched memory instruction cache trace machines used cache miss ex pansion much related work instruction fetch mechanisms concentrated superscalar cisc architectures especially arena x86 architectures patt et al studied use fill buffer decoded instruction cache break cisc instructions microoperations efficiently scheduled using dynamic scheduling hardware 18 smotherman franklin adapted fill unit decoded instruction cache use decoding x86 instructions 23 design associates nextpc field cache block intel pentium pro processor employs multistage ifetch fetches 16 bytes per cycle icache uses three stages align instructions 19 nextpc pc16 absence branch instruction amd k5 stores decode information related instruction length l1 instruction cache later used nextpc computation ifetch stage 7 like pentium pro k5 uses multiple stages fetch align x86 instruction stream x86 processor design nexgen uses different approach nextpc generation dedicated logic performs instruction alignment fetch time compute nextpc 11 arena risc architectures crisp processor used decoded instruction cache 10 crisp instructions converted inmemory format 1680 bits 192 bit expanded form icache expanded instruction occupied cache block associated nextpc field effect encoding instructions compressed manner studied wolfe chanin 24 compressed code risc processor designed conserve memory bandwidth storing instructions compressed format memory decompressing instruction cache cache miss time r1 sparc processor hal performs limited decoding memory l1 icache aid decoding ifetch 21 3 classification vliw icaches vliw cache organizations classified based two factors degree partitioning dop nops policy cache dop describes number independent memory units partitions used implement cache placement ops memory units independent memory unit tag data array searched parallel independent memories traditional cache cache one partition ops stored location within entire cache one tag set tags searched 2 alternative cache uses multiple partitions op maps particular partition based futype described section 21 tinker encoding uses four futypes permits parallel tag compares orthogonal partitioning shown section 43 integer floating point f memory branch b cache organization assigns futype separate partition represented labeled fully partitioned using notation unified partition cache traditional cache represented extremes unified partition fully partitioned designs flexible designs permit multiple futypes reside partition example design allows integer floating point ops share partition assigns futypes partitions represented f b implementation fully partitioned cache tinkern machine n partitions every partition size flexible partitioned cache combines partitions sharing futypes size overall fully partitioned cache also flexible design allows sharing futypes reside arbitrary locations within combined partition second factor classification whether cache explicitly contains nops nop cache nops cache multiops held nop cache uncompressed form whereas nopsfree cache compressed form nops policy cache closely tied placement expander icache pipeline explained section 4 ialu fpadd fpmul pred ld st br aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa pipeline latency figure 5 tinker8 machine model table 1 benchmarks used evaluation benchmark programs integer floating point 129compress 052alvinn 130li 056ear 134perl 089su2cor 147vortex 090hydro2d permutation dop nops policies yields wide variety cache configurations paper four representative organizations explored uncompressed cache ops stored one partition nop policy banked cache uses nopsfree policy silo cache fully partitioned policy lastly flexible silo cache uses combined partition two futypes separate partitions remaining two futypes nop policy four designs assumed cycle time trace driven simulations used evaluate performance schemes impact compiler employing superblock based scheduling used compile benchmark programs using tinker compressed encoding tinker8 machine organization 13 functional unit configuration pipeline latencies shown figure 5 functional unit pipelined depth indicated pipelined cachememory interface three cycle latency one op bandwidth assumed three cycle latency chosen similar l2 latency contemporary microprocessors 19 6 perfect l1 data cache l2 cache assumed prevent data cache effects coloring performance measurements integer floating point programs spec92 spec95 suiteswere used benchmarks evaluations listed table two million ops sampled across entire program simulation run 4 instruction fetch mechanisms 41 ifetch using uncompressed cache figure 6 shows organization uncompressed cache cache block size machine width miss path expander used cache miss repair mechanism two cycle expander assumed branch misprediction penalty one cycle modified direct mapped addressing scheme used uncompressed cache example illustrates traditional directmapped addressing bit selection cannot used assume tinker4 machine uses 32 bit addresses consider two multiops figure 7 shown would appear memory ops first multiop marked second multiop marked b multiop 2 ops mul tiop b single op assume contain branch op cache addressed normal directmapped cache physical addresses multiops b 0x80000000 0x80000080 respectively would tag index hence map onto cache block depicted figure 8a sequential access pattern conflict miss generated second multiop access ie b multiops b create contention cache block tags 3 results presented floating point programs lower normal due lack software pipelining programs missrepair expander multiop fetch nextpc logic multiop length matrix memory l2 cache execution pipeline connections shown figure 6 uncompressed cache two cycle miss path expander used place ops positions within cache block length field associated every cache block identical cannot determined one resident cache x80000000 memory memory cache multiop b figure 7 traditional address mapping uncompressed cache one way differentiate sequential ops use offset bits part index figure 8b depicts cache interprets address using scheme offset reduced addressing maps individual multiop memory block frame separate cache block associativity required possibility sequential multiops mapping cache block eliminated uncompressed cache holds multiop uncompressed form hence valid select logic hit path expander shown stage basic model figure unnecessary nextpc computation still needs performed fetch hardware must add length current multiop pc generate nextpc computation length performed runtime block containing multiop fetched cache matrix places nextpc computation block fetch adding sequential computation end fetch cycle possibly increasing cycle time avoid tag index 000000 b offset reduced traditional tag index 000000 figure 8 interpreting cache address traditional scheme b reduced offset scheme situation multiop length field associated every cache block length field accessed parallel tag data addition length pc done beginning tag compare stage020611418integer floating point benchmarks opc perfect icache uncompressed cache uncompressed cache figure 9 performance uncompressed cache metric used useful ops completed per cycle opc individual bar represents harmonic mean combination cache size benchmark programs graph figure 9 presents results simulations performance uncompressed cache compared perfect icache cache never misses metric used simulations report useful ops completed per cycle opc opc used captures effect icache performance overall per formance results grouped based cache size type benchmarks uncompressed cache performs poorly compared perfect icache although increasing cache size 16 kb 32 kb yields better performance resulting opcs still much smaller perfect cache integer benchmarks increasing cache size yields 22 better performance still five times slower perfect icache floating point benchmarks larger cache performed 19 better still four times slower perfect icache reason extremely low space utilization uncompressed design maximum opc 2 perfect cache indicates two eight words cache block typically used possible solution design place one multiop cache block explored next section 42 ifetch using banked cache figure shows organization instruction fetch mechanism using banked cache cache organized two data tag arrays intel pentium processor 2 cache block size machine width n multiop span two cache blocks reason every clock cycle two cache blocks ac cessed block requested multiop could reside current block next sequential block successor block contrast uncompressed cache expander cache hit path adds extra stage processor pipeline extra stage increases branch misprediction penalty two cycles scan op hdr tail bits pick mop swap blocks maybe tag compare nextpc generation expander b b c c c decoder bank 0 tags offsets bank bits tags offsets bank bits nextpc 512 bits wide figure 10 banked cache fetch multiopc shown blocks banks fetched swapped needed passed singlecycle expander nextpc computation performed parallel cache access addressing banked cache similar traditional cache high order bits address used tag middle bits used index multiop variable length always begin nword boundary memory low order bits used offset index start multiop cache block pc presented address cache cache address decoder selects consecutive blocks cache banks fetch mechanism shown figure 10 example fetch operation first three ops multiop c occupy last three ops cache block bank one last two ops c occupy first two ops next cache block bank zero pc address first op c beginning fetch cycle three sequential steps required fetch multiop c also shown figure 10 detailed 1 current block containing first three ops successor block containing fourth fifth ops requested cache 2 correct alignment multiop fetch hardware must know last op multiop c lies successor cache block searches tail bit last op c information permits cache fetch stage perform correct alignment swapping two multiop fragments note banking factor two facilitates exchange 3 header bits ops blocks scanned determine ops belonging multiop c starting location first op requested mul tiop valid select lines enabled pass requested ops expander stage fetching current multiop nextpc must also computed absence op changes control flow program accomplished using extra bits store offset information multiops cache details hardware pictured figure 11 offset field bank bit maintained every op cache block offset field indicates offset within cache block next multiop bank bit indicates next multiop resides bank current multiop next bank values fields set ops received cachememory interface cache miss time complete algorithm nextpc computation available technical report 4 offset fields bank bits also used twoway set associative design tandem way prediction techniques 15 use dedicated storage withinthe cache aid nextpc computation similar use successor indices initially proposed johnson 14 implemented amd k5 7 although scheme unbanked cache tinkern banked cache log 2 n offset bits per word op needed plus valid bit bank bit directmapped configuration total size cache tag data arrays increased approximately 8 traditional cache figure 12 presents results simulations banked cache compared perfect icache banked design performs least order magnitude better uncompressed design greater utilization assume cache block shown low order index bits 01 offset fields multiop b b c c c bank bits bank bits offset field figure 11 nextpc computation banked cache cache access multiop c shown offset field c placed appropriate positions bank bit added remaining high order bits form new index tag data storage area banked design offsets greater branch misprediction penalty performance still lower perfect cache much 25 penalty may due overfetching entire cache block portion block requested hypothesis tested next section 421 subblocking banked cache variant banked cache use subblocking 22 subblocking partitions cache block smaller units subblocks subblocking usually increases miss ratio cache reduces amount memory traffic subblock block independently filled placed miss repair subblocking ops retrieved starting address beginning multiop address beginning cache block done without subblocking called block fetching advantage reduction time miss repair expense small increase miss ratio instructions preceding requested instruction filled cache block miss occurs length missing multiop known unless miss occurs cache block boundary enough memory fetches generated fill end successor block required multiop span two blocks multiop resides entirely within current block successor block filled valid bit associated subblock set subblock filled subblock valid bits preceding ops current block unset offset fields bank bits used nextpc computation prefetching used subblocked banked cache block fetching performs implicit prefetching requested multiop loaded cache fragments integer floating point benchmarks opc perfect icache banked cache banked subblocked cache banked cache banked subblocked cache figure 12 performance banked cache metric used useful ops completed per cycle opc individual bar represents harmonic mean combination cache size benchmarks programs multiops reside blocks also loaded subblocked cache employ form prefetching termed load forwarding 12 load forwarding instruction cache consists fetching subblock requested lies subsequent subblocks end cache block banked icache one op subblock size simulated results presented figure 12 performance subblocked design benchmark dependent subblocking effective integer programs outperforming nonsubblocked design 9 45 16 kb 32 kb sizes respectively floating point benchmarks performance slightly lower nonsubblocked design examining behavior programs revealed reasons differences performance integer programs many branches branch latter half cache block branch causes cache miss latter half block next block filled contrast block fetch design blocks fetched fill time subblocked design potentially almost 50 less floating point programs total number stall cycles due icache slightly higher much code involved backward branches branched beginning cache blocks although latter part block filled earlier demand request beginning words contrast block fetch design entire block filled miss occurs subblocked designs therefore generated memory requests situations block fetch designs 43 ifetch using silo cache silo cache organized series partitions silos silo holds ops particular fu set fus shown figure 13 dop range unified partition onefutypeperpartition entry silo hold one op associated tag length field valid bit nextpc compu tation silos organized directmapped setassociative manner setassociativity used silos independently searched within silo parallel tag compares performed among sets 431 rigid silo cache rigid silo cache pictured figure 13 cache block size machine width design uses miss path expander two cycle expansion assumed branch misprediction penalty one cycle length field tags data silo intcmpp silo silo silo br tag compare nextpc computation expander execution units memory figure 13 instruction fetch rigid silo cache silo searched independently cycle silo holds ops destined one futype two cycle expander cache lower levels memory routes ops appropriate silo uncompressed cache silo cache sequential multiops size less machine width map cache block cause conflict misses address interpreted traditional manner shown figure 8a reason silo cache uses offset reduced scheme interpret address see section 41 index bits used address silos map location silo tag comparisons silos done parallel hit signaled tag match valid length field similar banked cache shown multiop b figure 13 tags length bits ops multiop identical cache hit ops multiop directed functional units associated specific silos silos organized directmapped setassociative manner lru replacement used setassociative silos miss occurs either none tags silos match tag match occurs length invalid fetch missing multiop initiated next level memory hierarchy multiop fetched cachememory interface expanded op routed appropriate silo values length fields computed parallel expansion silo receives op corresponding tag length fields updated situation occur ops multiop replaced cache fill ops replaced need flagged indicate entire parent multiop present accomplished searching ops tags ops replaced unsetting length valid bits ops next cache access partially displaced multiop results miss due unset length valid bits cache fill particular silo op routed expander op position corresponding fill address silo left empty respect silo cache uses nop policy however several multiops reside address across silos hybrid nop nopsfree policies understand note silo addressed individually perform placementsreplacements degree autonomy shown figure 13 multiop b consists integercomparetopredicate op memory op place ops silos floating point branch operations fact silos currently empty indicated valid bits multiopx containing single floating point operation maps cache location placed floating point silo update tag length fields proper location silo multiops b x coexist address across silos silo cache tag every op stored cache contrast uncompressed cache banked cache use tag cache block tag storage requirements silo cache obviously higher however data array silo cache need hold bits op header tail bits fut field see figure 2 longer needed tinker8 machine 30 storage traditional cache performance results rigid silo cache shown figure 14 even low degrees associativity direct mapped twoway silo cache outperforms banked cache 618 integer programs higher degrees associativity reduces number conflict020611418integer floating point benchmarks opc perfect icache silo silo way silo silo silo floating point benchmarks opc perfect icache silo silo way silo silo silo way figure 14 performance rigid silo cache metric used useful ops completed per cycle opc individual bar represents harmonic mean combination cache size benchmarks programs misses yields improvements 432 flexible silo cache performance silo cache dependent fu type distribution workload program contain ops mapping silo utilize silo effect program sees smaller icache silos used starve example behavior could occur integerintensive application starves floating point silos 1futypeto1silo requirement relaxed complimentary types ops typically execute together allowed share silo starvation problem might eased exam ple floating point integer operations placed silo f b partitioning silo would well utilized programs floating point integer intensive well programs mix types ops small hitpath expander required route ops silo appropriate functional units addition misspath expander required op placement hitpath expander increases branch misprediction penalty two cycles silo allows multiple futypes per silo termed flexible silo cache silo holds multiple futypes flexible silo flexible silo supplies ops n fus tag array size n silos fus rigid silo cache data array slightly larger fut field stored flexible silo use miss path expander rigid design flexible silo holds ops compressed fashion overall design rigid int int mem int br mem br mem flexible silo configuration opc figure 15 evaluation shared silos integer programs metric used useful ops completed per cycle opc exception first bar chart individual bar represents harmonic mean combination benchmarks programs using one flexible silo fu types indicated private silos remaining futypes caches simulated 16 kb associativities 1 2 4 8 16 first bar represents harmonic mean benchmarks using cache size associativity parameters rigid silo design nominally nop cache offset bits ops map flexible silo used determine cache locations flexible silo holds futypes size tag data arrays silos futypes rigid silo cache particular interest design stores two futypes flexible silo assigns remaining two futypes 1futypeperpartition silos simulations performed integer benchmark programs 16 kb cache variety flexible partitions determine futypes best suited share flexible silo results presented figure 15 exception floating pointbranch combination shared silo designs performed better rigid silo design performance gains ranged 19 tegermemory combination 31 integerfloating point combination based preliminary results flexible silo design clearly outperforms rigid silo design well uncompressed banked cache designs 5 analysis terms complexity uncompressed cache viewed simplest ifetch mechanism op place ment op access nextpc schemes similar techniques used previous designs drawback simplicity workload presented study nop policy causes low space utilization reflected poor performance banked cache uses different approach place multiple multiops cache block implicit prefetching nextpc generation performed differently uncompressed cache hardware requirements mechanisms adder extra storage lengthoffset information equivalent overall though banked cache requires complex logic also must able interchange blocks different banks subsequently scan ops later expand requested multiop extra work must performed one clock cycle requires sequential logic uncompressed design cycle time banked cache potentially longer uncompressed cache however performance substantially better indicates extra complexity may warranted rigid silo cache nominally nop cache allows limited sharing cache locations multiple multiops rigid silo cache requires considerably storage either previous designs opwide storage location associated tag length field extra comparators also required perform multiple parallel tag compares silo nextpc computation logic identical used uncompressed cache directmapped silo cache performs roughly equivalent banked cache associativity increased rigid silo cache outperforms banked cache however set associativity requires extra levels logic select correct block within set flexible silo design allows multiple futypes reside silo conceptually design stores ops flexible silo compressed fashion space utilization flexible silo high cache placement access straightforward complex designs extra expander required cache hit path smaller used miss path hit path expander used banked cache separate stage set associativity used cycle time might stretched due extra levels logic required preliminary evaluation flexible silo cache indicates might outperform three designs work needed determine potentially greater performance countered increase cycle time 6 conclusion study investigated issues involved ifetch support vliw architectures use compressed encodings effect ifetch mechanisms icache architecture discussed taxonomy classifying vliw icaches based degree partitioning nops policy introduced four cache designs presented evalu ated uncompressed cache banked cache rigid silo cache flexible silo cache performance cycle times issues design discussed silobased designs proved best performers flexible silo design showing promise preliminary evalu ation based results plan tinker testbed complete evaluation flexible silo design investigate implementation details tradeoffs rigid silo flexible silo cache designs acknowledgments discussions members tinker group proved useful development ideas pa per work supported intel corporation national science foundation grants mip9696010 mip9625007 authors would like express gratitude university illinois impact group use impact compiler system comments anonymous referees also appreciated r architecture pentium micro processor architecture high instruction level parallelism nextpc computation banked instruction cache cydra 5 minisupercomputer architecture implementation 300 mhz 64b quadissue cmos risc processor developing amdk5 architecture tinker machine language manual dynamic rescheduling technique object code compatibility vliw archi tectures branch folding crisp microprocessor reducing branch delay zero 93 mhz experimental evaluation onchip microprocessor cache memories differencebit cache hpl playdoh architecture specification version 10 hardware support large atomic units dynamically scheduled machines tuning pentium pro microarchitecture cydra 5 departmental supercomputer 64b 4issue outoforder execution risc processor cache memories improving cisc instruction decoding performance using fill unit executing compressed programs embedded risc architecture tr branch folding crisp microprocessor reducing branch delay zero cydra 5 departmental supercomputer hardware support large atomic units dynamically scheduled machines superscalar processor design executing compressed programs embedded risc architecture multiflow trace scheduling compiler cydra 5 minisupercomputer superblock dynamic rescheduling improving cisc instruction decoding performance using fill unit differencebit cache cache memories architecture pentium microprocessor tuning pentium pro microarchitecture developing amdk5 architecture architecture high instruction level parallelism experimental evaluation onchip microprocessor cache memories ctr zhao wu wayne wolf tracedriven studies vliw video signal processors proceedings tenth annual acm symposium parallel algorithms architectures p289297 june 28july 02 1998 puerto vallarta mexico jeremy lau stefan schoenmackers timothy sherwood brad calder reducing code size echo instructions proceedings international conference compilers architecture synthesis embedded systems october 30november 01 2003 san jose california usa partha biswas nikil dutt reducing code size heterogeneousconnectivitybased vliw dsps synthesis instruction set extensions proceedings international conference compilers architecture synthesis embedded systems october 30november 01 2003 san jose california usa partha biswas nikil dutt code size reduction heterogeneousconnectivitybased dsps using instruction set extensions ieee transactions computers v54 n10 p12161226 october 2005 praveen raghavan andy lambrechts murali jayapala francky catthoor diederik verkest distributed loop controller architecture multithreading unithreaded vliw processors proceedings conference design automation test europe proceedings march 0610 2006 munich germany shail aditya scott mahlke b ramakrishna rau code size minimization retargetable assembly custom epic vliw instruction formats acm transactions design automation electronic systems todaes v5 n4 p752773 oct 2000 larin thomas conte compilerdriven cached code compression schemes embedded ilp processors proceedings 32nd annual acmieee international symposium microarchitecture p8292 november 1618 1999 haifa israel n p carter hussain modeling wire delay area power performance simulation infrastructure ibm journal research development v50 n23 p311319 march 2006 benjamin j welch shobhit kanaujia adarsh seetharam deepaksrivats thirumalai alexander g dean supporting demanding hardrealtime systems sti ieee transactions computers v54 n10 p11881202 october 2005 john w sias hillery c hunter wenmei w hwu enhancing loop buffering media telecommunications applications using lowoverhead predication proceedings 34th annual acmieee international symposium microarchitecture december 0105 2001 austin texas sanjeev banerjia sumedh w sathaye kishore n menezes thomas conte mps misspath scheduling multipleissue processors ieee transactions computers v47 n12 p13821397 december 1998 oliver wahlen tilman glkler achim nohl andreas hoffmann rainer leupers heinrich meyr application specific compilerarchitecture codesign case study acm sigplan notices v37 n7 july 2002 murali jayapala francisco barat tom vander aa francky catthoor henk corporaal geert deconinck clustered loop buffer organization low energy vliw embedded processors ieee transactions computers v54 n6 p672683 june 2005