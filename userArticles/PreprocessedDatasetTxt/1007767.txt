subquadratic approximation algorithms clustering problems high dimensional spaces one central problems information retrieval data mining computational biology statistical analysis computer vision geographic analysis pattern recognition distributed protocols question classification data according clustering rule often data noisy even approximate classification extreme importance difficulty classification stems fact usually data many incomparable attributes often results question clustering problems high dimensional spaces since require measuring distance every pair data points standard algorithms computing exact clustering solutions use quadratic nearly quadratic running timesemi ie odn2d time n number data points dimension space approaches 0 grows paper show three fairly natural clustering rules computing approximate solution done much efficiently specifically agglomerative clustering used example alta vista search engine clustering defined sparse partitions clustering based minimum spanning trees derive randomized 1 approximation algorithms running times d2 n2 0 depends approximation parameter epsi independent dimension b introduction clustering data essential ingredient many information retrieval systems eg building maintaining taxonomies plays central role statis tics pattern recognition biology web search engines distributed networks fields recently concept clustering taken added significance researchers begun view data mining question finding hidden clusters large collections data survey clustering methods see 18 7 references therein informally clustering algorithms attempt form groups similar objects clusters based attributes ob jects question best define clustering problem seems particularly difficult large unstructured databases whose members viewed points high dimensional vector space successful formulations clustering seem graphtheoretic formulations providing results best agreement human performance 13 formulation main ingredient graphtheoretic clustering stated follows given n data points metric space following 1 compute spanning graph complete graph minimumspanning tree original data set 2 delete parallel edges graph ac cording criterion distance 3 output clustering connected components partitioning nodes depends topology resulting graph concrete example consider framework hierarchical clustering data points joined sets objects called clusters property two sets either disjoint nested agglomerative approach hierarchical cluster ing clusters joined form larger clusters based distance clusters clusters intercluster distance certain threshold joined form bigger clusters distance clusters defined number ways simple common choice compute distance centroids clusters examples applications use clustering rules include methods determining consensus biological sequencing data mutual fragments heuristic computing traveling salesman tours see eppstein 7 choice distance function socalled single linkage distance distance clusters minimum distance point one cluster point cluster course measuring distance centroids well choices distance function formulated singlelinkage distance cluster representatives ie sets points replace clusters formulation core task computing agglomerative clustering following start complete weighted graph data set points cluster representatives delete edges whose weight exceeds given absolute bound output connected components resulting graph addition aforementioned application hierarchical clus tering graphtheoretic clustering used alta vista tm search engine order prune identical documents worldwide web database 4 shall refer somewhat imprecisely agglomerative clustering second type clustering many applications distributed computing eg routing tables load file allocation defined sparse partitions awerbuch peleg 1 roughly speaking setting says given n points distance r define graph two points adjacent weighted distance r less nodes must partitioned collection possibly overlapping clusters constraints clusters unweighted graph theoretic diameter cluster small typical value ologn ii clusters belong small number typical number ologn color classes clusters within class disjoint iii every point x neighbors contained entirely least one cluster notice type clustering hierarchical clusters different classes may overlap third type clustering requires computing mini mumspanning tree mst call mstclustering clusters connected components forest results eliminating edges mst whose length exceeds given parameter exact mstclustering equivalent exact agglomerative clustering hard see however approximate versions may differ substantially approximation mst respect total length edges respect length edge separately remark mst used subroutine variety clustering methods study clustering problems points dimensional euclidean space results extend norms leave full version paper natural setting applications information retrieval data mining main issue address following naive approach one problems high dimension compute distances among pairs points execute clustering rules resulting graph course naive approach quadratic n worse time complexity quadratic behavior avoided context similar problems minimum spanning tree closest pair studied computational geometry see 9 often solutions better quadratic low dimensions example yao 19 shows mst computed subquadratic time exponent rapidly converging 2 dimension grows however recent work another geometric problem nearest neighbor search nns shows performance degradation dimension increases avoided approximation distances allowed 14 10 15 therefore focus approximate versions agglomerative mst clustering problems applications choice distances clustering rule done heuristic basis approximate clustering quite sufficient problem sparse partition clustering already contains certain degree approximation implicit use big oh notation methods used derive improved bounds exact problem defined understanding big oh hides factor uniformity refer sparse partition algorithm approximation algorithm problems approximation factor ffl derive clustering algorithms time complexity od depends ffl notation also hides factors log n subquadratic algorithms use recent results approximate nns particular use modified versions algorithms data structures kushilevitz et al 15 one difficulties obtaining results nns algorithms randomized probabilistic guarantees strong enough allow simple highprobability successful termination clustering algorithms without requiring quadratic worse running time note indyk motwani 10 suggest nns algorithms used derive subquadratic 2 approximations problems mentioned subsequent acceptance paper informed us full version paper 11 explicitly states improved approximation guarantee 1 ffl well mentioning applications approximate nearest neighbor algorithms additional problems including approximate mst computation willing settle approximate solutions appeal dimension reduction techniques johnson lindenstrauss 12 see also 8 16 10 simply stated n points euclidean space dimension probabilistically mapped olog nffl 2 dimensional space distance increased high probability distance shrinks factor mapping time complexity odn log nffl 2 follows approximate clustering algorithms consider assume olog n however sake completeness dependence interesting even small state results terms n well approximation factor ffl naive approach clustering euclidean space first compute gamma ndelta distances apply clustering algorithm resulting weighted complete graph agglomerative clustering standard implementations connected components algorithm naive approach would require 2 time broder glassman manasse zweig 4 give heuristic used alta vista tm seems give good running time practice requires quadratic time worst case sparse partitions awerbuch peleg 1 give polynomial time algorithm number edges naive approach weighted complete graph gamma ndelta edges present following bounds stated setting linial saks 17 yield randomized notation hides polylogn factors best result date setting complete graph deterministic awerbuch berger cowen peleg 2 bottleneck mstclustering computing mst show apply agglomerative clustering algorithm derive subquadratic approximate euclidean minimum spanning tree mst algorithm sollin see 3 shows reduce mst problem problem similar exact nns yao 19 gives another reduction uses give subquadratic algorithm exact mst low dimension yao mentions obtaining fast approximate mst algorithm open problem yaos reduction seems specific exact nns whereas sollins may adapted approximate nns though clear overcome probabilistic guarantees problem without resorting methods reduction yao yields mst algorithm time complexity 2gamma2 gammad1 log n 1gamma2 gammad1 somewhat better would result using yaos algorithm sollins reduction chazelle 6 improves bound still bound approaches quadratic behavior dimension grows notice dimension reduction techniques cannot guarantee low distortion unless dimension reduces toomegagamma23 n chazelles algorithm still quadratic also briefly indicate applications methods algorithms traditional computational geometry problems example subquadratic algorithm euclidean closest pair problem problem many applications full history see cohen lewis 5 eppstein7 references therein kleinberg 14 uses approximate nns results get approximate closest pair algorithm eliminating dependency dimension n log n show solve approximate closest pair approximate furthest pair problems subquadratic time preliminaries denote e dimensional euclidean space ie ir metric induced l 2 norm fflapproximate deltaproximity table finite set data structure supporting following operations ffl constructp creates new instance data structure data structure consists jp j o1 entries o1 may depend ffl ffl neighborsx x 2 e returns subset obtained one entries containing points p within distance delta x perhaps additional points p within distance 1 ffldelta x notice may several possible correct answers neighbors efficient construction fflapproximate deltaproximity table lies heart results get construction adapting approximate nearest neighbor search algorithm 15 results provide following guarantee lemma 1 every ffl 0 every fi 0 exists c 0 randomized implementation fflapproximate deltaproximity tables following properties 1 construct takes jp operations 2 neighbors takes qjp operations 3 x 2 e probability entry hence neighborsx returns incorrect list jp j gammafi also need apply frequently unionfind algo rithm purposes sufficient assume simply say using balanced trees union find operation universe n elements performed within steps also need call approximate nearest fur thest neighbor algorithms 1 15 algorithms work queries distances particular fflannfflafn table finite set p ae e data structure supporting following operations creates new instance data structure data structure consists may depend ffl every entry contains either single element p symbol indicating entry empty ffl closestx x 2 e returns point p contained one entries whose distance x 1 times minimum distance point p x ffl furthestx x 2 e returns point p contained one entries whose distance x least 1 gamma ffl times maximum distance point p x 1 algorithms 15 easily adapted finding furthest neighbors efficient construction fflannfflafn table easily constructed approximate nearest neighbor search algorithm 15 results provide following guarantee lemma 2 every ffl 0 every fi 0 exists c 0 randomized implementation fflannfflafn table following properties 1 constructannconstructafn takes jp 2 closest takes 3 furthest takes 4 x 2 e probability closestx furthestx returns incorrect answer jp j gammafi agglomerative clustering section discuss following clustering prob lem given set p ae e n points delta 0 partition p connected components following graph graph gpdelta node set p edge connecting every pair nodes distance less approximate version problem given additional parameter ffl graph replaced graph gpdeltaffl graph gpdeltaffl node set gpdelta edge set contains edges gpdelta addition may contain edges connecting pairs nodes distance greater delta greater 1 ffldelta notice choice gpdeltaffl necessarily unique graph gives correct solution approximate problem remark addition separating graph connected components algorithm easily modified output witness spanning tree component algorithm maintain unionfind structure element set p multigraph gp e represented adjacency list node addi tion maintain several proximity tables subsets p set partition p arbitrarily n 1gammaffi sets repeat k times initialize unionfind structure element set mark entry 0 node x l marked 1 distx l marked 0 8i distx 8i unionxy mark l 1 figure 1 agglomerative clustering algorithm constant guaranteed lemma 1 set algorithm shown figure 1 end algorithm desired partition clusters partition sparse graph g constructed algorithm connected components notation let x 2 ir let 0 denote bx closed ball around x radius ie set fy 2 ir j g correctness correctness algorithm immediate corollary following claim 3 1 high probability ie connected component nodes bu delta 2 u v 2 p connected component g sequence nodes v see 1 consider used algorithm let u point p event u retrieves proximity table entry containing bu delta p none call entry good pre failure probability query thus n nodes n 1gammaffi sets p high probability every event pens happens u retrieves good entry marked 0 connect u nodes entry otherwise entry marked 1 another node v connected nodes entry u connects one nodes entry 2 use induction number union operations performed consider unionxy operation joins two components case follow induction hypothesis induction hypothesis sequence another sequence v every ffldelta specification algorithm analysis analyze complexity one iteration outer repeat loop multiply k obtain total running time simplicity suppress influence c ffl big oh notation time complexity repeat loop upper bounded sum cost building n 1gammaffi proximity ta bles n minimum search cost nodes ie assuming retrieve entries marked 1 b cost retrieving bad entries g cost induced retrievals good 0marked entries u cost performing unionfind analyze term separately 4 let cfi constant lemma 1 let n worst case cost unionfind algorithm ffl iteration construct n 1gammaffi proximity tables n ffi points construction table takes n ffi steps ffl n iteration process n points point requires search n 1gammaffi proximity tables search proximity table takes qn time retrieved entry empty marked 1 additional cost od compute distance first point list ffl b iteration access n 1gammaffi proximity tables searching n points table table point probability point retrieves bad entry gamma therefore expected number bad entries handled proximity table n 1gammaffi bad entry may compute distance points table costs ffl g iteration handle n 1gammaffi proximity tables table n ffi entries entry accessed good entry marked 0 happens compute distances points listed n ffi distance costs od ffl u bound follows estimate total number union operations performed iteration handle n 1gammaffi proximity table proximity table accessed n points point retrieves entry marked 1 causes one union operation n operations per table good entries marked 0 cause n ffi union operations entry thus number union operations per table n ffi n theorem 5 total running time agglomerative clustering algorithm od 2 n 2gammaffl 2 2c log n partitions section discuss computing sparse partition clustering finite set points xng definition sparse partitions euclidean analogue definitions given 1 2 17 undirected graphs papers distance refers path length sparse partition c p parameter r 0 collection subsets called clusters p satisfying following conditions 1 euclidean diameter cluster log n 2 every contained completely least one clusters bx r euclidean ball radius r centered x 3 clusters grouped ologn classes clusters class disjoint follows x 2 p ologn clusters containing x furthermore distance two clusters class greater r clusters j class x euclidean distance x greater r original definition sparse partitions general allows tradeoffs values conditions 1 3 definition uses common values applications results extend general tradeoffs algorithm 17 run ologn phases phase grow new class clusters contains constant fraction nodes phase status node either free used initially phase nodes free grow cluster picking free node adding free neighbors breadth first search manner stopping distance chosen random using truncated geometric distribution 17 ie choose distance repeat olog n times initialize proximity tables see mark x 2 p free choose random 2 ng geometric truncated distribution free node x initialize new cluster mark node x used initialize f fxg f 0 steps free z 2 mark z used figure 2 sparse partitions algorithm probability 2 gammai ranging 1 2 log probability 2 gamma2 log n1 n main difference 17 implementation breadth first search exploits underlying structure graph giving speedup running time particular need able high probability determine points approximate ball r centered radius r contains r may also include far free points slightly larger ball fflr formal description algorithm appears figure 2 computing neighbors compute set using proximity tables initialized beginning phase first explain initialization let ffi k previous section partition disjoint sets size n ffi set construct k independent fflapproximate rproximity tables also mark entries tables completes initialization construct set r follows fetch neighborsy kn 1gammaffi tables let resulting lists ignore lists marked 1 every list l marked 0 compute disty z z 2 l point l 1 fflr leave l marked 0 otherwise add elements l r mark l 1 correctness prove algorithm indeed produces desired sparse partition clustering high probability main issue show r computed correctly rest argument follows general line argument 17 added difficulty approximate distances proximity points longer symmetric property least 19 20 following holds every algorithm computes set contains points r still free none points 1 proof latter claim obvious algorithm checks distances points adds r see former claim argue proof claim 4 probability good entries retrieved contain points r n gamma2 therefore probability happens n gamma1 finally notice entry marked 1 status points listed entry current phase used 7 consider particular phase let 2 p first point bx r placed f phase placed f execution iteration loop phase bx r contained cluster generated phase probability least 1 5 set points bx r f start iteration j points placed f 0 iteration 1 assume moment x 62 f j clearly moreover start jth iteration points bx r n f j particular x free jth iteration executed know given j j 2 log conditional probability 4 assume event assume event claim 6 holds happens 2 choice constant arbitrary probability least 1 5 2 bx r r x placed f 0 jth iteration claim 6 let f j1 set points bx r placed f 0 jth iteration end jth iteration points f j added cluster points f 0 placed f particular x placed f points f j1 assumed j2 therefore iteration j1 executed notice start iteration points bx r n free denote points f j2 x 2 f iteration bx r placed f 0 include points f j2 end iteration 1 points f particular points f j1 placed cluster points f 0 particular f j2 placed f assuming iteration executed well end iteration points f j2 placed cluster points bx r claim follows x 2 f j similar argument shows contain points bx r even larger probability 8 consider particular phase 8x probability first 2 bx r clustered reached last two iterations main loop phase 4 phase probability claims imply theorem 9 high probability algorithm produces sparse partitioning clustering parameter r following holds phase probability completely contained cluster generated phase bit 1 5 clearly say 1 6 thus olog n phases high probability property holds points x bound diameters clusters follows choice bound number clusters containing point follows fact olog n iterations iteration generates collection disjoint clusters analysis let ffi k described implemen tation time complexity phase algorithm similar agglomerative clustering upper bounded sum cost building kn 1gammaffi log n rproximity tables n minimum search cost cost retrieving pointers b cost retrieving entries marked 0 contain points 1 fflr g cost induced retrievals good 0marked entries ie distance within 1 fflr u cost performing set unions analyze cost separately cfi constant lemma 1 analogous lemma 4 theorem 11 total running time sparse partitions algorithm od 2 n 2gammaffl 2 2c log 2 n 5 computing approximate mst related problems section discuss several geometric problems approximated subquadratic time using methods first consider problems computing 1 appoximation closest furthest pair points application also considered kleinberg 14 following result lemma 12 constant ffl exists ffi agglomerative clustering time od 2 n 2gammaffi compute whp pair points within distance closest furthest pair points proof sketch idea partition set p n 1gammaffi subsets size n ffi subset build fflann fflafn data structure search structures point repeating search several times taking best answer reduce error probability sufficiently consider problem computing approximate minimum spanning tree mst lemma 13 constant ffl exists ffi compute whp 1 fflapproximation minimum spanning tree time od proof sketch idea iterate several computations agglomerative clustering time duing parameter delta factor 1 ffl notice agglomerative clustering algorithm outputs sparse graph g witness connectivity cluster use witness ie black box agglomerative clustering algorithm outputs partition clusters alone insufficient whenever cluster one iteration split next add growing forest edges connecting current fragments cluster edges taken graph g computed previous iteration tice graph current iteration may differ graph previous iteration add edges g connect two points currently also cluster close cycle g sparse enough check edges begin delta one hand guarantees single cluster oher hand big close enough lower bound mst example take 1 times result approximate furthest pair search stop reaches value small enough allow us take remaining connections please mst contains value ffln times initial value delta sufficient recall initial value close lower bound mst thus number agglomerative clustering computations fixed ffl olog n mentioned previously use approximate mst threshold value basis approximate mst clustering deleting mst edges distance larger viewing resulting components clusters note mst clustering might different approximate agglomerative clustering using parameter acknowledgment thank andrei broder motivating discussions regarding altavista tm search engine r sparse partitions syntactic clustering web approximating matrix multilication pattern recognition tasks search history fast hierachical clustering applications dynamic closest pair johnson lindenstrauss lemma sphericity graphs handbook decrete computational geometry approximate nearest neighbors towards removing curse dimen sionality private communica tion extensions lipschitz mappings hilbert space relative neighborhood graphs relatives two algorithms nearestneighbor search high dimensions efficient search approximate nearest neighbor high dimensional spaces geometry graphs algorithmic ap plications decomposing graphs regions small diamater pattern recogni tion constructing minimum spannning trees kdimensional spaces related prob lems tr search history johnsonlindenstrauss lemma sphericity graphs decomposing graphs regions small diameter two algorithms nearestneighbor search high dimensions approximate nearest neighbors efficient search approximate nearest neighbor high dimensional spaces syntactic clustering web handbook discrete computational geometry pattern recognition approximating matrix multiplication pattern recognition tasks fast hiearchical clustering applications dynamic closet pairs pattern classification 2nd edition