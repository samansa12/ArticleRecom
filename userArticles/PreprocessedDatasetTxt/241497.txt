static assignment stochastic tasks using majorization abstractwe consider problem statically assigning many tasks smaller system homogeneous processors tasks structure modeled branching process tasks assumed identical behavior tasks may synchronize frequently show theory majorization used obtain partial order among possible task assignments show vector numbers tasks assigned processor one mapping majorized another mapping former mapping better latter respect large number objective functions particular show metrics finishing time spacetime product reliability captured also apply majorization problem partitioning pool processors distribution among parallelizable tasks limitations approach include static nature assignment also discussed b introduction consider problem statically assigning tasks processors tasks unknown random processing times certain type stochastic structure synchronize periodically stochastic structure examine embodies notion one task spawning set others examine static assignments assumption offspring task executed processor task static assignment type consider likely used tasks state large thereby making dynamic assignment costly terms communication semistatic assignments frequently also make sense13 14 one periodically adjusts load globally executing static assignment algorithm performance degradation due imbalance severe enough justify suffering task migration overheads consider different variations synchronization situation tasks generation offspring synchronize globally tasks generation offspring situation tasks execute independently synchronize establish termination paper examines theoretical issues associated comparing different static mappings set complex stochastic tasks particular show theory majorization used derive strong results concerning comparison different mappings strength contribution lies providing formal underpinning analysis mapping complex stochastic tasks form common parallel processing optimization rich class objective functions theory apply schurconvex objective functions drawn 10 main result symmetric convex objective functions develop overall main contribution demonstrating majorization applied parallel processings mapping problem also comment limitations application previous work load balancing task assignment 3 4 7 8 9 12 21 parallel systems may loosely divided three categories first category deterministic structure involves task structures execution times known prior assignment case 15 includes study problem complexity various constraints heuristic algorithms task scheduling second class load balancing formulations task execution times random characterized queueingtheoretic considerations 4 19 21 much work pertains steadystate expectations task delays statedependent 4 21 stateindependent 19 assignment policies work closest third category 7 8 9 13 also takes task execution times random focuses minimizing expected processing times fixed set tasks discussed 9 assumption random execution times given set tasks justified applications montecarlo simulations approach problem differs previous work 7 8 9 13 several ways paper concern explicit optimization task assignment rather comparison different assignments wide range possible objective functions past approaches typically address question given k processors tasks random execution requirements find assignment tasks processors minimizes expected maximum workload makespan paper well addressing optimality address another related question given two assignments say one better class objective functions make assertion results simple general form describe mapping probabilistically homogeneous tasks processors vector whose ith component number tasks assigned ith processor let 0 describe two different mappings bounded 0 using notion majorization 10 written oe 0 see definition 21 objective functions f class c may say assignment described better assignment described 0 class c often quite general includes many commonly used objective functions eg expected maximum workload note interest inequalities stochastic orderings useful merely searching optimal assignments orderings may derived variety cases expensive search optimal assignment inequalities also useful constraints assignment eg heterogeneous memory capacity among processors prohibit one adopting otherwise obvious optimal policy note stochastic orderings independent interest 16 also cases consider optimal strategy apparent derived ordering interest obtaining stochastic orderings also stems observation often results available small numbers random variables wide variety distributions consider fact 8 9 results asymptotic least one variable n k fact 9 results asymptotically correct number tasks n number processors k approaches based use central limit theorem 8 large deviation theory 9 among limit results available hold variety distributions contrast approach concerned finite possibly small n k make use theory stochastic majorization 10 thus results strong terms optimality obtained fundamental limit theorems accuracy results depend number tasks processors new results concerning stochastic majorization often developed applied problems queueing systems eg 17 22 flexible manufacturing systems 18 typical result majorizations vectors processor service rates induce inequalities overall network throughput performance measures utilization queue lengths also considered key issue work showing performance measures interest exploitable mathematical structure eg monotonicity convexity concavity viewed function design variables paper similar spirit different detail first application majorization assess effects load imbalance synchronization parallel processing system knowledge unique model queueing model flexible manufacturing model applications notion global synchronization however like queueing related work focus show performance metrics interest parallel processing mathematical structure theory majorization applied discuss specific differences work past efforts structural model single task branching process completing process spawns random number subprocesses type behavior appears diverse applications branchandbound searching algorithms 2 branching structure obvious dynamic regridding algorithms numerical computations 1 sections coarse grid serve processes give rise subprocesses associated finer grids furthermore results permit analysis much complex objective functions typically studied stochastic task models model differs significantly 8 9 12 tasks 9 taken individual independent identically distributed iid samples drawn common distribution synchronization behavior periodic global synchronization 8 complex task comprised fixed number tasks random iid execution times however analyses 9 8 concerned overheads eg synchronization communication costs model include present paper extension earlier work obtained assumption workload assigned processor causes processor behave markov chain 13 like earlier work new results show quality static assignment persists across numerous stochastic transformations workload model study present paper distinct improvement 13 stochastic behavior processor explicitly dependent volume workload contains related research directed computing expected completion time single complex task possibly random acyclic structure 6 20 another related publication 11 studies problem scheduling subtasks single task subtasks form tree lastly analytic study loadbalancing statistically homogeneous workload hypercube presented 7 mean variance difference load processor average load derived work based results study stochastic majorization fundamental theory majorization originates economic study income distributiona sort load balancing believe majorization finds natural application area mapping parallel workload one contributions demonstrate uses powerful theory parallel processing respect work similar 3 3 focus new stochastic ordering based class symmetric lsubadditive functions applications routing designing processor speeds load balancing emphasis 3 scheduling structurally simple tasks queue noted theory use develop sometimes severe limitations underlying requirement probabilistic uniformity basic workload description symmetry cost function many practical load balancing problems cannot directly addressed majorization however experience complex problems sometimes approached piecemeal fashion applying different majorization results regions problem underlying requirements satisfied many results develop appear intuitively obvious admit intuitive nature caution intuitively obvious solutions problems probability theory sometimes fact wrong results must rest carefully developed basis one contributions paper provide basis preliminaries 21 workload system model model workload produced single task branching process 16 pp 116117 follows task begins single work unit wu computation wu executed upon completion random number wus created placed tasks work list one models synchronization consider task synchronizes globally tasks continuing process newly generated wus initial wu thus thought containing seeds number additional wus possibly zero similarly contain seeds additional wus one first generation wus may executed children 2 nd generation wus spawned placed tasks work list global synchronization may occur general consider global synchronization wus common generation number children wu spawns assumed random chosen probability distribution known branching distribution process repeated tasks work list empty task workload comprised computation related wus ultimately descended initial task wu tasks execution may also punctuated idle periods waits tasks complete processing current wu generation assume order wu execution way affects spawning children wu work list destined spawn j children regardless length time spends list easily understood one views wu generation reflecting intrinsic structural property problem eg branching search tree independence every wu belongs generation independent execution order initial wu generation 0 children spawned generation 1 wu generation 2 assume given wu may executed constant cost one k homogeneous processors every wu executed processor parent therefore map computation associated task map tasks initial wu consider evolution initial task wu let n q denote number wus q th generation size q th generation given number wus generated j th wu q gamma 1 th generation assume fz jq 1g collection independent identically distributed iid random variables rvs following notation employed ffl k number processors number initial task wus integer assignment vector whose th component gives number initial task wus assigned th processor thus size generation q descended single initial wu branching distribution understood subset sa sum sizes generations jfold convolution probability mass function f x random variable also use x j denote sum j independent instances x random vector generation q wus resulting assignment vector q fold convolution random variable n q denote th component w q w q notation extended arbitrary subsets majorization permits us compare different mappings variety objective functions even though mapping vectors integer components functions consider ir k domain results focus comparing values e oew q deriving conditions inequalities involving initial task assignments following given two assignments expectations exist applicable functions oe include symmetric convex function maximum operator powers maximum sum operator product operator particular interest thus single comparison assignment vectors 0 vectors yield wealth information comparative behaviors complex stochastic tasks two mappings results applicable least two different types processor synchronization study generational synchronization gs processors engage barrier synchronization wu generation processor executes wus given generation say q synchronizes barrier released barrier processors executed generation q wus reached barrier process repeats subsequent generations type synchronization appropriate computation generation q one task may depend results computed generation another task also study termination synchronization ts processor engages barrier synchronization work lists initial tasks empty appropriate tasks independent synchronization serves aggregate final results respective computations figure 1 illustrates gs ts synchronization three complex tasks mapped statically two processors fact results also apply mixtures gs ts styles given could impose global synchronization execution n th th wus effect imposing barriers different wu generations allowing different processors execute wus different generations barriers simplicity exposition focused pure gs ts synchronization natural question arises given stochastic structure tasks given tasks arbitrary distributions closely workload model approximate true distributions branching distributions answer course exact general distributions easy match first two moments common approximation technique thus model provide reasonable within first two moments approximation workload distribution beyond saying unable formally characterize accuracy approximation example whether class branching distributions dense space distributions surprisingly optimal way assigning n tasks k processors usually assign nk face obvious one may well ask study partial orderings primarily majorization proves optimality respect large number objective functions thereby lending theoretical support intuition secondly majorization works even presence constraints disallow uniform assignment complicate ones intuition concerning optimality example constraints may exist forbid one processors assigned ffnk tasks majorization identifies optimal assignment heterogeneous constraints also apply concepts issue partitioning pool processors among set complex parallelizable tasks well take k number parallelizable tasks task generation 0 generation 1 generation 2 time processor 1 processor 2 generation generation generation barrier barrier barrier generational synchronization processor 1 processor 2 time termination synchronization barrier c e f task c f e b c figure 1 generational termination synchronization three complex tasks abc mapped statically b assigned processor 1 c assigned processor 2 use describe number processors assigned constraints feasible easily envisaged assignment may need consider natural partition sizes arise communication topology system usage time assignment optimal solution constraintfree version problem may apparent theory provides means comparing feasible solutions 22 stochastic ordering majorization introduce majorization partial ordering oe using notation largely taken 10 definition 21 majorization vector x majorized vector written x oe iff 1 2 notation x taken th largest element x definition 22 schurconvex function function oe said schurconvex x oe ir n implies oex oey ir examples schurconvex functions include ir following intuition may provided majorization consider two vectors describe assignments nonrandom workload three processors inspection know x loadbalanced question much load balanced may answered examining distance optimal assignment 4 4 4 distances might say x loadbalanced dx dy majorization makes notion precise true x oe implies dx dy fact x oe implies oex oey many functions oe giving rise class schurconvex functions informally schurconvex function function almost convex symmetric theory establishes precise relationship majorization various classes functions nontrivial reader referred 10 mentioned earlier power theory lies richness class schurconvex functions also useful generalization majorization random vectors let c 0 class increasing functions increasing component ir n onto ir wellknown stochastic ordering random variables 16 defined follows random vectors x distribution functions f g respectively st iff integrals well defined majorization deterministic quantities extended random variables like manner using appropriate class functions symmetric defined oe symmetric oex permutation integers define respectively schurconvex partial ordering denoted oe scx convex symmetric partial ordering denoted oe cas notation used 10 thus note c 2 ae c 1 thus oe scx stronger ordering oe cas also worth noting contribution chang field 3 demonstrates different class functions oe scx results also hold easier check schurconvexity less restrictive ordinary multidimensional convexity employ functions paper stochastic orderings based likelihood ratio play especially important role paper definition 23 likelihood ratio consider nonnegative integer valued rvs x probability mass functions f g x defined smaller likelihood ratio written monotonicity likelihood ratio also important property definition 24 ilr nonnegative integer valued rv x said increasing likelihood ratio ilr distribution function f said ilr iff next define another class probability mass functions increasing likelihood ratio convolution definition 25 ilrc let x random variable probability mass function f defined x f said increasing likelihood convolution ilrc iff f lr f j ilr distributions known closed convolution even number times convolution applied random provided distribution number also ilr 10 lemma 21 let f ilr probability mass function ffl f ilrc ffl fixed integer k 0 f k ilr ffl let n ilr positive integervalued random variable f n ilr using facts straightforward prove following lemma 22 let f ilr probability mass function ffl f branching distribution task generations q n q ilr ffl subset i2a n finite mean sa ilrc proof proof first claim simple induction q uses closure ilr property random ilr mixtures proof second rewrites n q least element c almost surely whenever j result follows definition 24 fact n q ilr see assumption ilr branching distribution often yields oe scx orderings ilr condition true discrete uniform poisson geometric binomial distributions showing results apply branching assumes wellknown distributions next show stochastic orderings may used develop stochastic majorizations different static mappings 3 branching stochastic majorization section establish conditions either oe cas oe scx orderings established workload vectors different mappings notion workload seen quite general throughout section important remember results relate intrinsic properties branching behavior depend assumptions execution behavior eg synchronization central result oe scx ordering based following theorem application theorem 3j2 10 correspondence form original pointed appendix theorem 31 let x ilrc random variable probability mass function f let vector nonnegative integers distribution f j suppose set rvs independent let oe ir k ir schur convex function schurconvex function using theorem 31 one obtains central basic oe scx ordering result theorem 32 consider set n tasks common ilr branching distribution f let 0 two mapping vectors oe 0 ffl generations q w q oe scx w q 0 ffl subset sa finite mean w oe scx w 0 proof lemma 22 shows distributions n q sa ilrc result follows definitions w q w theorem 31 observe statement theorem 31 applies generally notion random cost associated initial wu states initial wu incurs random ilrc cost cost processor sum rewards incurred independent wus stochastic majorization costs follows deterministic majorization initial wus oe scx result seems require assumption ilr ilrc branching distributions however constraining attention symmetric convex functions able obtain oe cas orderings completely general branching distributions unlike oe scx result orderings details numerous developed appendix oe cas counterpart theorem 32 theorem 33 consider set n tasks common nonnegative branching distribution f let 0 two mapping vectors oe 0 ffl generations q w q oe cas w q 0 ffl subset sa finite mean w oe cas w 0 31 heterogenous constraints kvector majorized vector whose components nonnegative sum n applied assignment problem shows obvious way balance workload indeed best even complex stochastic tasks optimality less clear however obvious assignment prohibited constraints processor let c upper bound number wus processor may given constraints might arise instance processors different memory capacities obvious mapping prohibited c nk majorization provides way identify best assignment complex stochastic tasks even face constraints consider feasible vector k suppose exist j j construct new vector x transferring one unit j ie x j shown 10 5d x oe observation gives rule iteratively improve feasible solution improvement possible say vector x resulting processed balanced without loss generality assume c 1 c apparent x balanced whenever characterization balanced vectors index j x furthermore x balanced index j follows x oe oe x shows essential uniqueness balanced vectors balanced vectors thus optimal heterogeneous constraints simple algorithm construct balanced assignment assume processors ordered increasing constraint value initially set x loop repeatedly indices 1 k pass loop increment x provided essentially assigns one unit th processor repeat loop n units assigned main results section shows stochastic branching preserves stochastic majorization additive cost systems seen useful cost systems derived generation sizes section follow illustrates results fruitfully applied various objective functions 4 objective functions establish number interesting objective functions either schurconvex convex symmetric functions notion workload objective functions include finishing time different synchronization schemes spacetime product overall reliability diversity application demonstrates utility theory 41 finishing time one use majorization show whenever oe 0 computations expected finishing time better 0 established using different models execution example one easily envisions computation tasks must synchronize globally barrier every generation ie gs synchronization typical tasks associated numerical computations model processors cost processing x wus generation function ex course e includes execution costs may also incorporate perwu communication io costs assume ex increasing convex x instance fix reasonable model ff perwu execution cost fi perwu communication cost communication costs arise message startups dominate cost message passing communication cost suffered processor proportional number wus executes case e strictly convex made e models disk io increasing numbers wus processor leading marginally increasing io costs due fragmentation moment ignore additional communication costs synchronization costs time required mapping execute generation q wus n q viewed random cost associated initial wu thus theorem 33 tells us w q oe cas w q 0 suppose executing barrier synchronization costs processor b units time additional communication costs instance due contention modeled function c ir k ir workload distribution vector time required execute generation q wus function f w q closure f symmetric convex assumptions e convex c symmetric convex follows e f w q applies processing every generation q overall expected finishing times eg sum generational finishing times two mappings likewise bounded similar results obtained ts synchronization processors synchronize termination one models costs exactly except cost initial wu instead n q total size branching tree rooted wu mean branching distribution strictly less one es 1 case whenever expected finishing time larger 0 worth noting intuitive communication cost functions convex results apply instance every wu communicates every communication coresident wus free cost ex processor may better modeled form e actually concave x need even monotonic nonetheless seen theory applies wide variety communication cost functions another metric interest variation number wus assigned processors variance defined also symmetric convex generation q sa finite mean branching distribution ilr similar result holds true standard deviation square root variance time synchroniza tions standard deviation schurconvex 10 pp 71 42 functions queue length wu completes execution generates children places processors work list following another wu selected executed thus storage cost associated executing complex tasks generally show stochastic majorization applied objective functions based measuring queue lengths every time step simple example computations total spacetime product defined follows let qt vector enumerating number wus enqueued processor time let computations termination time assume cost processing one wu exactly 1 total spacetime product idea generalizedlet sj quantify cost holding j wus queue one unit time total spacetime cost respect show increasing convex synchronization expected spacetime cost respect worse 0 result also demonstrated gs synchronization branching distribution ilr model assumptions made probabilistic behavior processors queue completely independent queueing discipline used assume queueing discipline smallestgenerationfirst sgf whenever processor selects wu execution work list chooses one least generation index simplicity also assume execution wu takes unit time spacetime function rise usual spacetime product spacetime cost functions also intuitive example one might store wu states disk whenever queue length exceeds threshold l furthermore l exceeded cost might superlinear owing fragmentation costs candidate cost function would general assumptions spacetime cost function convex increasing zero empty queue lists seem us quite natural treatment spacetime costs ts synchronization hinges following observa tion processor k exactly w q k wu units generation q sgf queueing discipline point time processors queue exactly w q k wus partic ular instant first wu generation q executed queue consists entirely generation q wus contains show contribution expected space cost made processor k processing generation q wus sgf schedul ing increasing convex function w q k use fact find majorization vector expected contributions made processors processing generation q wus turn show total expected spacetime cost worse 0 expectations exist oe cas result applicable branching distribution suppose w q r processing th wu generation q produces random number x qi wu units join processors queue queue length instant th wu begins execution r r work units queue point first generation q wu executed executed one produced random number generation q wus therefore conditional expected spacetime cost suffered processing wu fixed oe convex r convex fl random variable z expectation convex assuming expectation exists expected spacetime cost processing r members generation q processor k r finally claim c r convex function r demonstrate suffices show c r2c r 2c r1 r since oe convex r fixed j oej r2oej r observation reduces problem demonstration fact sr increasing establishes oer proving convexity c r function r convex k whenever g convex ir convex ir k observe w q random spacetime cost respect generation q resulting assignment vector proven following result proposition 41 let increasing convex function suppose space cost holding k wus one processors queue one time unit sk define measure spacetime cost suffered executing generation q assignment given whenever ffl expected total spacetime cost using ts synchronization worse whenever expectation exists 21 analysis spacetime costs gs synchronization requires work assumption ilr branching distribution suppose w q k spacetime cost processor k interval time generation q wus executed two components already seen first cr k cost accumulated period length r k generation q wus executed second component spacetime cost suffered waiting heavily loaded processor finish processor k generates x generation q1 wus spacetime cost suffers waiting barrier max fr recalling definition oe equation 17 may write expected total spacetime cost processing generation q wus gs synchronization conditioned w q observe oer k branching random variable g schur convex k fact show using following characterization schurconvex functions k 3a2b 10 function ff k schurconvex ff symmetric increasing r 1 t2 fixed r fix r difference gr 1 always nonnegative condition tells us g schurconvex need examine two cases alternative assuming former straightforward algebra shows difference bounded two summations positive oei r increases r since oei r convex r r 1 r 2 also follows oei r every thus positive summation dominates negative summation desired inequality hold definition see oer r linear function r hence convex therefore inequality see desired bound hold oer convexity implies needed argument case r 1 6 almost exactly omitted schurconvexity g gives us stochastic majorization gs synchronization proposition 42 let increasing convex function suppose space cost holding k wus one processors queue one time unit sk suppose branching distribution ilr define measure spacetime cost respect executing generation q gs synchro nization processor r generation q wus g schurconvex k whenever oe 0 ffl expected total spacetime cost using gs synchronization worse gw q 0 whenever expectation exists 25 43 reliability yet another application majorization question whether hardware successfully execute entire computation suppose computation fails processor nonempty queue fails observe definition permits computation successfully complete even processor dies entire computation finished provided failing processor already finished show branching distribution ilr processors timetofailure distribution increasing hazard rate function probability failure greater 0 whenever conversely branching distribution ilr processor failure distribution decreasing hazard rate function reliability 0 better result proven ts synchronization suppose processor time failure random variable z monotone hazard rate function u well known z tu dsg 26 u nondecreasing u gamma u du concave say log prfz tg concave conversely u decreasing log prfz tg convex follows 3e1 10 u increases product schurconcave equivalently gammart decreases processor assigned wus initially ends processing wus total also processor processing time assumptions sgf scheduling ts synchronization unit execution cost per wu given equation 27 gives probability every processor executes wus without processor failure unconditional probability obtained taking expectation respect joint distribution prfevery processor executes wus lemma 22 asserts ilrc branching distribution ilr follows theorem 31 u increasing ers schurconcave function proves following proposition proposition 43 suppose hazard rate function u time processor failure increas ing suppose branching distribution ilr let flm probability every processor executes wus without processor failure ts synchronization sgf scheduling inequality reversed u decreasing 5 assignment processor pools last application stochastic majorization concerns problem large number n processors partitioned among smaller number k complex tasks parallel processing applied tasks accelerate execution time assume task requires generation wus executed generation generation wus may processed parallel overall system may use either ts gs synchronization let gx give time required processors execute x wus assume gx convex eg gx xm g0 0 suppose k initial wus may describe assignment n processors wus vector whose th component gives number processors assigned th wu also let n qi denote random number wus associated generation q task gs synchronization time required complete q th generation assumptions efl q symmetric convex function b4 proposition 10 showing efl q efl q 0 follows immediately overall expected finishing time gs synchronization worse 0 ts synchronization finishing time sum convex functions remains convex whence eaem symmetric convex assured expected finishing time ts synchronization worse using 0 6 summary paper explores application majorization problem assigning large number complex probabilistically identical tasks onto multiprocessor using model workload based branching processes show establish partial ordering among possible assignment tasks processors show quality initial assignment persists stochastic transformations workload ordering taken respect wide range objective functions including measuring finishing time spaceusage relia bility results developed two different models task synchronization finishing time results incorporate communication well synchronization costs also show theory applies processor partitioning problem utility theory lies generality objective functions considered fact optimal solutions identified even constraints placed potential assignments also pointed limitations assignments static work spawned wu stays processor stochastic structure tasks general would like appendix appendix prove claims made earlier paper ilrc condition upon oe scx results depend involves notion totally positive functions chapter 10 source following definition definition a1 totally positive function let b subsets real line function said totally positive order k denoted tp k 0 30 use following result 18a4a 10 lemma a1 k tpm l tp n oe oefinite measure convolution z tp minfmng relationship total positivity ilrc distributions direct given integervalued nonnegative probability mass function f may define function ff f theta 0 1 ff f tp 2 iff n equivalent saying f lr f j ie f ilrc reason interest ilrc distributions f convolution functions ff f satisfy three criteria required theorem 3j2 10 ffl ff f totally positive order 2 r measure v theorem 3j2s conclusion counting measure z schurconvex k theorem 31 restatement result 8u probability recognize flm expresses expected value oey oe cas results next consider oe cas ordering case able obtain analogue theorem 32 save oe cas result holds completely general branching distributions first must introduce little terminology develop intermediate result random vector said exchangeable components joint distribution invariant permutations components basic oe cas results rest following observation lemma a2 let xy nonnegative random variables random vector nonnegative exchangeable components assume x z independent rvs define proof let convex symmetric function define function since z exchangeable components also convex symmetric function u oe v follows result extends easily ir n lemma a3 let xy nonnegative random variables let random vector independent components z 1 z 2 distribution assume x components z mutually independent define proof symmetric convex function oe symmetric convex first two arguments therefore condition values z j 3 j k z j apply previous lemma obtain removal conditioning z j 3 j k yields desired result prepared prove theorem 33 let 0 mapping vector processors j 0 without loss generality may take let 00 mapping vector obtained 0 moving one wu processor 1 processor 2 apply lemma a2 interpret z fold convolutions initial wu costs x convolution 0 single initial wu cost z j convolution 0 initial wu workloads application lemma a3 yields incremental movement task heavily loaded processor lightly loaded processor corresponds general notion transfer 10 known whenever x oe x constructed finite number transfers transformed vector always dominated oe predecessor consequently 0 mapping vector one demonstrates w oe cas w 0 repeated application lemma a3 sequence transfers transmute 0 proves result r adaptive mesh refinement hyperbolic partial differential equa tions algorithmictheory practice new ordering stochastic majorization theory applications models dynamic load balancing heterogeneous multiple processor system computers intractability approximation processing time random graph model parallel computation local global analysis nearest neighbour balancing hypercube optimal partitioning randomly generated distributed programs allocating independent subtasks parallel processors evaluation parallel execution program tree structures optimal partitioning random programs across two processors dynamic remapping parallel computations varying resource demands communication efficient global load balancing stochastic majorization random variables proportional equilibrium rates optimality balanced workloads flexible manufacturing systems optimal static load balancing distributed computer sys tems analytic queueing network models parallel processing task systems optimal assignment customers parallel servers majorization arrangement orderings open queueing networks tr ctr philip j boland harshinder singh bojan cukic comparing partition random testing via majorization schur functions ieee transactions software engineering v29 n1 p8894 january