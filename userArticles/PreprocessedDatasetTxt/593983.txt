simple bayesian model bitmap compression bitmaps useful storage voracious component many information retrieval systems earlier efforts compress bitmaps based models bit generation particularly markov models permitted considerable reduction storage short memory markov models may limit compression efficiency paper accept state orientation markov models introduce bayesian approach assess statesemi analysis based data accumulating growing window paper describes details probabilistic assumptions governing bayesian analysis well protocol controlling window receives data find slight improvement best performing strictly markov models b introduction paper continues series papers bookstein et al 1992 1994 1997 apply advanced statistical models compress bitmaps earliest models assumed 1bits bitmap occur independently bookstein et al 1992 several environments however independence assumption valid example consider concordance built allow access large textual database concordance term occurring text associated bitmap bitsite region text obtained partitioning text logically coherent units bitmap representing term 1bit positions corresponding segment contains word within text discussions specific subjects clustered imposes clustering tendency 1bits within bitmaps belonging terms associated subjects since bit encoding determined probability given value example using arithmetic coding witten et al 1994 expect accurate modeling result improved compression paper focus problem modeling clustering rather details generating codewords first attempts represent clustering involved use variety markov models feller 1957 although significant improvement resulted using even simple markov models small number states bookstein et al 1997 ultimately limited finite memory models test alternative approach based principles bayesian statistical analysis press 1989 may correct limitation continue assumption made earlier papers bits generated within two states cluster state tends produce high density 1bits betweencluster state produces 1bits sparingly however longer model state transitions instead use bayesian reasoning assess likelihood either state given data data gathered within growing window closed decide state transition likely probability next bit taking simple bayesian model bitmap compression 3 value determined data currently window bit encoded accordingly next section describe general model remainder paper based illustrate ideas means simple version bayesian model elaborate subsequent sections describe protocol used control window data collected well describe experiments used test models 2 bayesian modeling conceptualize bitmap length bits generated manner similar hidden markov model described rabiner 1989 site bitmap generator assumed one two states cluster state c betweencluster state b states determine probability generating 1bit diverge markov assumption longer modeling state transitions rather collect data scan bitmap use data guess probability either state hence probability 1bit generated overall strategy encode bits using dynamic window accumulate data begin window contains zero bits scan encode bits window grows accumulate data window use bayesian methods improve estimate probability 1bit generated point sense state change close window begin process anew suppose opening window taking bayesian approach try determine probability distribution parameter governing data generation within window parameter p probability 1 bit starting prior distribution p based 4 bookstein klein raita information improve distribution window grows information accrues problem uncertainty p developing window primarily result knowing state though also possibly state may allow range probabilities generally assume cluster state c distribution probabilities given f c p corresponding distribution betweencluster state denoted f b p assume priori probability cluster state begin new window data base esti mate use prior unconditional distribution p p 0 p given assumptions accumulate data current window want integrate information contains improve estimate p 0 p denote density function p pjw indicating density function incorporates information window w scanning derive updated distribution use bayes formula get probability occurrence certain window fixed p simply respectively number 1s number zeros w contains n 0 n 1 values estimate probability 1bit given evidence w r integral simply p expected value p conditional evidence w simple bayesian model bitmap compression 5 proceed specify state based distributions consider two models increasing complexity 21 sharp probability distribution simple first model assume probability fully determined state let p c probability seeing 1 cluster p b probability seeing 1 clusters note allows us see 1s even cluster state vice versa since probability determined state ffix dirac ffifunction defined 0 everywhere whose integral satisfies z words ffip gamma p c probability distribution asserts equation 1 prior unconditional distribution p given mixture two dirac ffifunctions develop current window want integrate data contains prescribed equation 2 apply equation 4 evaluate p expected 6 bookstein klein raita value posterior distribution p p j w r 1 r 1 special values check reasonableness model useful examine several limiting values example cases state known formula gives appropriate probability producing 1bit also find window grows value n 1 increases indefinitely value n 0 remains fixed assuming grows relative n 0 evidence becomes increasingly strong c state appropriate probability approached next find initially surprising little thought shows result correct first note p n 1 hence p undefined must cluster state state permits 1bit similarly p p defined n 0 0 must betweencluster state requiring p indeed case similar comments could made cases unrealistic given probabilities represent finally effect single state probability 1 generated common probability simple bayesian model bitmap compression 7 22 betadistributed model relax requirement state completely determines probability parameter create much flexible models transcends principle hidden markov model hmm used earlier papers model examine still conceptualize generator one two states state influences doesnt fully determine probability generating 1bit gain additional flexibility admitting probability distributions density functions f c p f b p dispersion betadistribution johnson et al 1970 offers class models allows great deal flexibility controlling shape distribution time relatively convenient analytically include model example bayes approach allows us reach well beyond markov framework recognize possibility clusters identical clusters probability onebit greater others similar comment could made noncluster state although hybrid model using betamodel cluster state deltafunction betweencluster state possible betadistributed model assume parameters ff c 0 fi c 0 parallel distribution defines f b p ff note factorial operator noninteger values argument defined usual way terms substituting betafunctions f c f b equation 4 find integration 8 bookstein klein raita result analytically complex must evaluated numerically must adjust parameters ff c ff b fi c fi b optimize compression efficiency however easier interpret values parameters note expected value betadistribution given suppressing subscripts ff fi parameters ep j fi variance v reminiscent binomial distribution ff successes fi failures j ff tries terms model understandingly parameterized former indicating probability 1bit latter confidence parameter large model approaches sharp model described thus parameter role parallel p c sharp distribution similarly experiments indicate whether benefit flexible probabilities within clusters justify extra effort need extra two parameters 3 window dynamics preceding section assumed region state fixed know state parameters beta distribution parameters allow us make information free guess likelihood either state thereby probability next bit taking value 0 1 modify probability estimate accordance accretion data window grows preceding section indicated use data within window estimate probability 1bit generated bitmap generated shifting states take account simple bayesian model bitmap compression 9 must use protocol defines window grows closed begin window size 0 initial probability estimate eqn 1 bits scanned upgrade probability principle let window grow indefinitely would put us severe disadvantage state changes thus introduce parameter w max windowsize reaches w max stop growing window point begin shifting window dropping bits end introducing bits front practice finite w max limits size codeword bit inconsistent current state scanned choose value optimizes performance compression site unexpected bit value two scanned suggests state may changed estimate probabilities begin old window may longer informative point old window closed new one grown may possible estimate w max startover point theoretically ideal way assess predictive power current window would actually look ahead evaluate well able compress next several bits bits following already window given information within window performance expected probability prediction mechanism valid could computed using information theoretic arguments discussed cover et al 1991 hamming 1980 performance less expected indicates close window begin however decoder would able follow decision procedure know next incoming bits situation inherent many adaptive processes see eg vitter 1987 welch 1984 requires use less efficient delayed update strategy method adopted simulate strategy retrospectively bit encoded appended window w run consistency check next bit encoded tail history bits effect representation unencoded bitsfull window figure 1 schema window structure bits recently encoded backtrack turn one two maximum number bits backtrack parameter model every time backtrack ask whether bits reviewed consistent current window new window reviewed bits pass test backtracking step continue encoding following bit extension current window otherwise start new window though reviewed bits model retrospectively fails already encoded part preceding window necessary decoder considered data new window example first window closed basis inconsistent last three bits bits act information new window next bit encoded describe decision rule opening new window purpose calculation backtracking consider retrospectively current window called contain data point backtracked data begins treat backtracked data tail current window new data available encoder decoder illustrated figure 1 examine data w 1 consider whether hindsight continued extending w 1 begun new window w 2 suppose looking ward see consists n 1bits 0bits basis w 1 estimate probability corresponding probability w 2 simple bayesian model bitmap compression 11 simplicity consider probabilities unchanged see successively bits use standard bayesian argument estimate probabilities w 1 w 2 correct window given follows one two windows assumed correct probability w 1 correct given probability w thus odds favor new window p w 2 p w 1 given equation 7 expresses evidence infavor new window terms odds ratio used directly properly assessed tricky notice fixed evidence actually influenced fully expressed two factors thus empirically determine threshold change window provided product two righthand factors exceeds threshold instructive rewrite equation 7 note value nm number bits observed looking ahead fixed term brackets actually depends evidence isolation serve measure strong claim start new window factor value odds favoring new window provided bits tested zero suppose would result one windows say w selected 1bit modifies factor odds ratio p 2 collectively doesnt compensate initial weight use w else use window estimate p 1 used last probability estimate made evidence new evidence evaluated reasonable options available estimating p 2 example use priori probability assess whether evidence window point better evidence opposite bias obtained estimating p 2 actual evidence see highvalue estimate sufficiently larger based w 1 justify opening new window principle window update procedure could applied recursively resulting tail perhaps allowing even better shorter basis estimating probabilities upcoming bits however since restricted length 6 bits recursive procedure applied experiments next illustrate window update protocol example suppose encoded bit b 511 w consists bits b 500 next must test goodness window stages first stage first consistency check w 1 consist bits b 500 single bit b 511 basis w 1 compute 1bit given w 1 size codeword b 511 get corresponding quantity empty window call evidence closing window given else given equation 7 ignoring close window ratio larger threshold value fl beginning new window b 511 ratio smaller fl go next stage second stage first backtrack redefining w 1 b 500 probability 1bit given reconstituted w probability 1bit given empty window use equation 7 get weight new window based upon evidence evidence large enough close current window repeat process beginning simple bayesian model bitmap compression 13 window consisting bits b 510 b 511 else continue third stage suppose example last stage three compute p 1 basis new w assuming null window compute weight favoring new window based evidence new consisting bit sequence b 509 given equation 7 weight supports opening new window close current window begin cycle starting window made bits b 509 b 510 b 511 else accept old window make bits b 501 511 current window continue additional stages desired continue figure 2 example window development actual detailed example window development taken experiment described appears figure 2 numbers top figure denote bit positions specific bitmap studying actual bits shown beneath maximal length window w max set 10 figure depicts window dynamics encoding bits start window contains 1bits indices run 1bits gives high probability upcoming 1bit index 111 bit encoded inserted window update procedure notice window w 1 always better predictor corresponding tail empty window one 14 bookstein klein raita change made discard oldest bit window prevent overflow next two bits indices 112 113 repeated 0bit index 114 encoded note threshold value used consistency check allows one spurious bit inside run 1bits oldest bit discarded however next 0bit gives already enough evidence end run ones noticed stage two update process eight 1bits w 1 consistent pair zeroes tail open new window initialize contents way update procedure captures essential features bit sequence 0bits 1bit cluster vice versa satisfied since overall tendency clear also scanning truly heterogeneous bitmap region reason change window radically see eg situation encoding bit index 144 commences implies change state performed necessary ie boundaries characteristics bit string change considerably 4 experiments order compare new technique previous methods used test databases earlier papers namely king james version bible english chapters acting documents subset tresor de la langue francaise tlf tlf database 680 mb french language texts 112 million words 17 th 20 th centuries see bookstein et al 1992 details collection whose uncompressed concordance spans 345 mb excluding references 100 frequent words considered stopwords subset used study consisted 35070 terms belonging lexicographic range elle flaube models developing intended terms show considerable degree clustering clustering strength determined nature simple bayesian model bitmap compression 15 word choice define document represented single bit bitmaps investigation tlf chose document level immediately book level tlf hierarchy resulting bitmaps length bits chosen unit convenient construction concordance may obscure underlying clustering large size first task estimate parameters give best compression minimize function gamma log use fact encode bit log encoding based probability ith bit bitmap takes value probability determined evidence window performing minimization analytically hard reverted search methods may yield suboptimal parameter values parameters divide two sets model parameters p b p c sharp model three window parameters maximum window size w max maximal backtracking length b update process threshold fl controls restart window betamodel need recall ffs fis beta distribution expressed terms ps straightforward way simplify task finding optimal values parameters note number 1bits map sharp model requires yielding value betamodel fixed probability bit generation use formula p substituted p c initial estimate reasonable expect n 1 frequency occurrence 1bits satisfies constraint implies 0 1 set terms tested chose manually several combinations parameter values obeying restrictions searched local optima large number optimization runs performed showed overall shape function rippling high probability large minima maxima due safely fix input parameters without notable loss compression efficiency tuning may appropriate however 1bit densities clumping properties change considerably bitmap bitmap table descriptive statistics tested files summarizes compression performance various models tested bible words appearing least 60 chapters thus occurring 60 929 chapters considered tlf terms bitmaps partitioned three classes according density 1bits threshold values 78 388 1162 table statistics compression results bible tlf terms 623 2032 619 381 occurrences 131874 352522 402890 1387698 independent 2683 909 726 402 best 4state 2544 848 665 348 bayesbeta 2523 838 655 342 bayessharp 2556 844 665 352 simple bayesian model bitmap compression 17 upper part table 1 shows class number different terms total number occurrences lines lower part correspond various bitmap compression methods independence model 1state markov model used bookstein et al 1992 simple model take clustering account therefore performs relatively poorly bookstein et al 1997 studied performance different markov models values cited correspond choosing class model giving best compression among traditional 4state markov models line entitled hmm gives compression figures bookstein et al 1997 hidden markov model omission hmm result due enormous execution time needed run produce figure last lines correspond bayesian models described herein understand values table recall representing top level concordance follows term list sequentially documents term occurs list conceptualized bitmap bits length 1bit document term occurs measure compression list corresponding term compute number bits needed encode list methods divide value number documents term occurs table gives average quantity terms class words average per 1bit within uncompressed bitmap number bits needed encode 1bits bitmaps class earlier studies bookstein et al 1997 included cost storing necessary parameters negligible cases since seems varying w max threshold value fl large effect results fixed throughout experiments leaving three five parameters sharp beta models respectively compared four parameters hmm 4state models since high frequency terms compressed average number per term occurrence added bits caused overhead small parameter values define generating model 45 bitmaps able set equal zero implying single betadistribution suffices majority terms note however invalidate underlying multistate generating process rather intrinsic variability betadistribution seems adequate represent impact various states 55 terms useful incorporate services second betadistribution however cluster cluster distri butions probability 1bit low average value 0144 average standard deviations 0010 0096 respectively thus cases spread comparable size values seen tests bayesian model outperformed best 4state models 12 surprisingly new approach even improved upon compression obtained highly cpuintensive hmm model one cases thus conclude bayesian technique described paper gives good timespace tradeoff compressing better faster 4state models using significantly less processing time hmm 5 conclusion bitmaps convenient representing concordances realistic applications large storing expensive simplest techniques compressing bitmaps based models represent bitmap generation sequence independent events fact work quite well dealing large databases even small percent improvement cost yield large absolute benefits investigations directed applications size database justifies additional complexity simple bayesian model bitmap compression 19 typical design optimal systems found added complexity tends improve overall performance increment complexity yielding diminishing returns example best 4state models fact come quite close performance hidden markov models flexible expensive models considered bound ability improve performance using route simple probabilistic models paper interested assessing whether alternatives markov modeling relieved constraint markov models short memory could match improve performance used underlying state model applied bayesian reasoning rather markov transition probabilities assess state results comparable hmm comparably little cost convergence towards performance hmm agreement two rather different approaches two different databases leads us expect striking compression improvement unlikely without radical increase model complexity however examined one application representing concordance textual database applications clustering pronounced may lead quite different results variety models developed could strongly differentiated 6 r model based concordance compression storer j markov models clusters concordance compression systematic approach compressing full text retrieval system elements information theory introduction probability theory applications coding information theory distributions statistics continuous univeriate distributions2 exploiting clustering inverted file compres sion bayesian statistics tutorial hidden markov models selected applications speech recognition design analysis dynamic huffman codes technique high performance data compression managing gigabytes address offprints timo raita dept tr ctr justin zobel alistair moffat inverted files text search engines acm computing surveys csur v38 n2 p6es 2006