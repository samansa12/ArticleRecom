information dependencies paper uses tools information theory examine reason information content attributes within relation instance two sets attributes x information dependency measure ind measure characterizes uncertainty remaining values set values set x known variety arithmetic inequalities ind inequalities shown hold among ind measures ind inequalities hold relation instance numeric constraints ind constraints ind measures consistent ind inequalities applied relation instances remarkably functional multivalued dependencies correspond setting certain constraints zero armstrongs axioms shown consequences arithmetic inequalities applied constraints analog completeness set constraints consistent inequalities may construct relation instance approximates constraints within positive egr ind measures suggest many valuable applications areas data mining b introduction welldeveloped discipline information theory seemed little say information systems longstanding conundrum attempts use information theory measure information content relation blocked inability accurately characterize underlying domain answer mystery looking wrong place tools information theory dealing closely representation issues apply within relation instance various attributes instance traditional approach information theory based upon communication via channel instance fixed set messages one transmitted sender receiver via channel receiver gains certain amount information less likely message sent meaningful receipt formalized assigning message v probability p subject natural constraint defining information content v log 1 p logarithms paper base 2 another way viewing measure amount information message related surprising message isa weather report month july contains little information prediction hot prediction snow carries lot information issue surprise also related recipients state knowledge weather report example astonishment report snow directly related knowledge july january information content two reports would vastly different thus interdependence two sets messages highly significant two message sets independent intuitive statistical sense receipt message one set alter information content eg temperature wind speed two message sets independent receipt message first set may greatly alter likelihood receipt hence information content messages second set eg temperature form precipitation central concept information theory entropy h set messages weighted average message information definition 11 entropy given set messages probabilities g entropy entropy closely related encoding messages encoding v using log 1 p bits gives minimal number expected bits transmitting messages remark 1 suppose messages probability 0 contains single message database context information content measured terms selection specification specific value rather transmission avoids thorny problem seems say since database stored site transmission occurs information particular model looks instance single relation values arbitrarily selected tuple simplicity assume message source ergodicall tuples equally likely probability distribution could applied tuples less impact formalism intuition assumption tuples equally likely information required specify one particular tuple relation instance n tuples course log n minimal cost encoding requires uniformly log n bits treat attribute equivalent message source message set active domain value v probability c n v occurs c times thus single value carries log n bits drawn attribute n distinct values attribute key class standing code typical fouryear college approximately two bits information somewhat less extent attrition skewed enrollment gender vmi little information using entropy measure since information content value female high receipt unlikely major results paper use common definition information characterize information dependencies characterization three steps first extends use entropy measure information information dependency measure section 4 second derives number arithmetic inequalities must always hold particular measures relation instance section 5 third investigates consequences placing numeric constraints measures relation instance significantly functional multivalued dependency result constraining certain particular measures differences zero section 6 example weather report database month entropy 358 might discover condition entropy 19 fixed month condition entropy approximately 16 thus knowing value month contributes approximately 03 bits information knowledge condition 16 bits uncertainty hand personnel database empid provides entire information content salary 0 bits uncertain addition measureconstraint formulation exhibits analog completeness set numeric constraints consistent arithmetic inequalities positive ffl relation instance achieves constraints within ffl section 7 characterization information dependency many important theoretic practical im plications allows us carefully investigate notions approximate functional dependency help normalization opens whole realms data mining approaches preliminaries notations conventions relations relation instances nonempty multisets r denote instances 1 operators oe filter distinctiveness attributes r schema instance r xy z v w r xy denotes x equivalent fag 2 r xy z partition r values v equivalent hvi hvi 2 r enumerates instances distinct x r 1 similarly j wrt n z k wrt z probabilities countr r use consistent similarly two central notions entropy conditional probability statistical independence conditional probability allows us make possibly informed probability measure set values narrowing scope overall possibilities independence establishes bound informed conditional probability enables us definition 21 conditional probability conditional probability instance oe xx r symbols definition 22 independence xy independent p paper log function expressions form log10 convention continuity real number 0 lemma 21 log x lemma 22 let probability distribution log q p q null values considered 3 bounds entropy ease notation write hx hx understand h always associated nonempty instance r r clear context write h r x remainder section establish upper lower bounds entropy function lemma 31 upper lower bounds entropy 0 hx log consequently hx 0 suppose lemma 22 log 1 p intuitively entropy set x r equal zero signifies exists uncertainty information whereas equal log signifies complete uncertainty information consequence notation allows us find joint entropy sets xy r joint entropy xy written hxy lemma 32 bounds joint entropy xy r hx h independent proof first inequality x independent p thus inequality deduction fact equality second inequality observe mg j consequently log 1 p ij log 1 q thus symmetrically h well 4 ind measures information dependency measure ind measure x xy r attempts answer question much know provided know x using notation section 2 know possibly informed therefore recalculate entropy log amortizing different x values according respective probabilities gives entropy dependent x resulting following definition information dependency measure note measures metrics r e f e f c g ind measures h figure 1 left instance r right ind measures r observe h definition 41 information dependency measure information dependency measure ind measure given xis hxy normally drop word entropy referring measures value declaration dependency case fds measure dependency important keep mind characterize ind measure hxy terms ind measures hx hxy lemma 41 hxy note hxy measure information needed represent given x known information x contains latter quantity course measured 5 ind measure inequalities relationships among ind measures characterized inequalities expressions involving various measures formulae several named according corresponding functional dependency inference rules characterize special circumstances lemma 51 reflexivity x lm 41 lemma 52 111 111 figure 2 encodings b b given fig1 u contains portion bit string encodes similarly b u overlap shows portion encoding b contained within encoding surprise receiving aa witnessed fact although know receive first bit bf ie 0 need additional 14 bits second bit bf receipt abac ad hand poses surprise since bg completely contained therein illustrate situation two inds may interact little combine sum inds may interact strongly combination yields total dependencies putting restrictions left righthand sides constrains interactions hence tightens ind relationships lemma 53 union left hxy hxz hxy z equality p jji p kji independent lemma 54 hxy lemma 55 hxyz hxz hxyz hxz lemma 56 union right minhxz hxz hxyy lm 55 lemma 57 augmentation 1 hxzyz hxy hxy lm 55 lemma 58 transitivity hxy h z hxz hxxy hxyxz lm 57 lemma 59 union full hxy hwz hxwyz hxwyw hwyzy lm 57 hxwyz lm 58 lemma 510 decomposition z hxy hxz hxy hxz lemma 511 psuedotransitivity hxy hwyz hxwz hxwyw hwyz lm 57 hxwz lm 58 lemma 512 xy hwxy z proof lm 52 may assume wlog v w z let z h xz z h xz 6 fds mvds armstrongs axioms 61 functional dependencies functional dependencies fds longknown wellstudied 8 10 xy r x functionally determines written value yields single value proof recasting terms probabilities given x 2 x single p ij 0 consequently p singleton hence 62 armstrongs axioms armstrongs axioms 8 important functional dependency theory provide basis dependency inferencing system commonly three rules given armstrong axioms merely specializations inequalites 1 reflexivity x 2 augmentation 3 transitivity theorem 61 armstrong axioms derived directly ind inequalities proof reflexivity follows directly lm 51 augmentation lm 57 transitivity lm 58 additional three rules derived axioms often cited fundamental union psue dotransitivity decomposition also follow lm 53 lm 511 lm 510 respectively interestingly critical distinction armstrongs axioms ind inequalities former union derived original three axioms whereas latter union must derived first principles 621 fixed arity dependencies lemma 61 fds alternatively statement number distinct values x 2 x determines work example motivate case fds nonempty r practice however size often unity fds illsuited eg consider fparent childg relation r biologically countdistinct parent oe childc child c 2 child inds measures used model dependency easily h childparent 63 multivalued dependencies following xyz partition r multivalued dependencies mvds arise naturally database design intimately related natural join operator multivalued dependency intuitively see values z related wrt particular value x lemma 62 mvd count assume x r x proof definition mvds lemma 63 x jz holds iff hxy 62 conditional probabilities z wrt x must independent condition required lemma 53 equality hold lemma 53 equality conditional probabilities z wrt x independent hence lemma 62 x since acyclic join dependencies characterized set mvds clear ind inequalities characterize well though work really done characterization set mvds 64 additional ind inference rules three standard rules mvd inference 1 complementation 2 augmentation 3 transitivity complementation augmentation trivially true ind inequalies last rule tran sitivity rather interesting proof find alternative characterization mvds intuitively proof establishes hxy hxz hxy z lm 63 hxy hxz hxy hxyzlm 54 interestingly alternative characterization mvds case contribute information z lemma lemma 66 consequence lm 65 lemma 67 iw jv x xy iw jv lm 512 lemma 68 let hxr hxr lemma 69 transitivity mvds hxr 65 rules involving fds mvds pair rules allow mixing fds mvds 1 conversion 2 interaction rule conversion trivial interaction follows lm 64 section 62 stated critical difference armstrong axioms ind inequalities distinction axioms derivable rules additionally appear fundamental differences fds mvds ind inequalities example consider following problem let r schema set fds r let ir f set relation instances r satisfy f x r let g question whether exists set g fds x pi x ir f g known general g exist similar negative result holds mvds ind measures broader class fds mvds expectation theorem holds trivially since relation instances satisfy set ind inequalities 7 ind measure constraints summarize previous sections defined ind measures instance values reflect much information additionally required second set attributes given first set proved number arithmetic equalities inequalities various ind measures given schema inequalities must hold instance schema shown constraining certain ind measures simple expressions involving ind measures 0 imposes functional multivalued dependences instances generalize last step considering arbitrary numeric constraints upon ind measures eg hxy 49 relation instance r r fx g solution constraint h r xy 49 standard arithmetic formally definition 71 ind constraint system schema r theta n linear system constraint system characterized written ahx b transpose observe definition 71 sufficient describe ind measure inequality ind constraint systems simple requiring single fd extensive specifying entropies subsets r however every b x make sense applied relation instance either b may admit solutions eg solutions may violated ind measure constraints x eg definition 72 ind constraint system b x feasible provided linear system b plus ind measure constraints inferable x solvable observe solution extended system involves finding values hx 71 instances feasible constraint systems question naturally arises whether instance always exists feasible constraint system affirmative answer question whose proof sketched provides ind measures analog completeness venturing proof theorem prove simple result merely sake providing intuition comes two things observed reading following proof first duality instance counts approximate probabilities second way interpolation occurs lemma 71 given rational c 0 exists relation instance r single attribute jh r 1 intermediate value theorem since f continuous function interval 0 1 c value f0 f1 exists 2 0 1 probability distribution distribution approximate r constructing instance r fag distinct values sufficiently large countoe ai proof nonconstructive find suitable x example binary search theorem 71 instance existence feasible constraint system b x ffl 0 relation instance r satisfies b x within ffl 1 using observation definition 72 solve b x fixed values 2 pick 1ffl 3 give every attribute value large probability namely number attributes note highly probable attributes contribute negligible amount entropy since probabilities close 1 4 remaining probabilities attribute divided among b equal size buckets thus ha log b find b ff remark 2 wlog ordered decreasing entropy hence b b i1 add attributes order 1 5 stage construction included adding i1 already want construct also single distribution q corresponding i1 actually construct two distributions p pu p lower p upper upper case simple i1 independent theta b lower case found allocating q j among various ps b b i1 enough buckets go around small error nonzero correspond unique q 6 0 induction h p interpolate p p u match entropies conceptually similar lm 71 relies upon unusual structure pu caused almostunity cases p q another iteration 8 applications extensions presented formal foundation incorporating information theory relational databases many interesting valuable applications extensions work already pursuing 81 datamining datamining 3 search interesting patterns large databases motivated initial work interest establishing means interesting primary objective certainly find ind measures hxy ffi given instance r r search r takes place upon lattice h2 r hxy ffi checked every x ind inequalities facilitate search kivinen et al 4 considers finding approximate fds central notion violating instance r r xy r pair tuples define three normalized measures based upon number violating pairs number violating tuples number violating tuples removed achieve dependency respectively authors state problematically measures give different values particular relations therefore choosing measure bestif areis difficult feel ind measure shed light upon metrics connection measures ind measures illustrated three instances r 152 80 16 8 4 137 95 36 8 4 example shows hxy sometimes make finer distinctions g applications side kivinen et al done substantial work related approximate fds 4 paper important notion approximate dependency also brief discussion errors cast armstrong axiomlike inequalities 82 metrics rather considering information x lacks may look information contains normalized form ih interesting results maxixy x makes specification fds natural cannot used characterize mvds another interesting measure uses additional notions information theory rate language x countr average number bits required tuple projected x absolute rate ab logcountr difference ab gamma indicates redundancy x approaches r average tuple entropy increases reducing redundancy pertinant especially following section 83 connections relational algebra examining inds behave relational operators example lemma 81 let r fx zg r instance r r instance employing lossless decomposition ind measures rates change indicate decomposition indeed lossless 9 related work dearth literature area marrying information theory information systems closest work seems piateskyshapiro 2 proposes generalization functional dependencies called probabilitistic dependency pdep author begins using notation relate two sets attributes xy pdepx observe pdep approaches 1 x comes closer functionally determining since pdep inadequate author normalizes using proportion variation resulting known statistical measure x better fd x vice versa author describes expectation pdep effeciently sample values area artificial intelligence algorithm developed create decision trees means classification quinlan notably id3 5 c45 6 uses entropy dictate building proceed case supervised learning attribute selected target remaining attributes r gamma fag classifier algorithm works progressively selecting attributes intial set r gamma fag measuring classified properly acknowledgements authors would like thank dennis groth dirk van gucht chris giannella richard martin cm rood helpful suggestions r elements real analysis second edition probabilistic data dependencies data mining knowledge discovery overview approximate inference functional dependencies relations induction decision trees coding information theory foundations databases elements information theory principles database knowledgebase systems vol tr principles database knowledgebase systems vol coding information theory elements information theory c45 programs machine learning approximate inference functional dependencies relations data mining knowledge discovery bottomup computation sparse iceberg cube foundations databases data cube induction decision trees recovering information summary data ctr chris giannella edward robertson note approximation measures multivalued dependencies relational databases information processing letters v85 n3 p153158 14 february ullas nambiar subbarao kambhampati mining approximate functional dependencies concept similarities answer imprecise queries proceedings 7th international workshop web databases colocated acm sigmodpods 2004 june 1718 2004 paris france marcelo arenas leonid libkin informationtheoretic approach normal forms relational xml data proceedings twentysecond acm sigmodsigactsigart symposium principles database systems p1526 june 0911 2003 san diego california periklis andritsos rene j miller panayiotis tsaparas informationtheoretic tools mining database structure large data sets proceedings 2004 acm sigmod international conference management data june 1318 2004 paris france luigi palopoli domenico sacc giorgio terracina domenico ursino uniform techniques deriving similarities objects subschemes heterogeneous databases ieee transactions knowledge data engineering v15 n2 p271294 february marcelo arenas leonid libkin informationtheoretic approach normal forms relational xml data journal acm jacm v52 n2 p246283 march 2005 solmaz kolahi leonid libkin redundancy vs dependency preservation normalization informationtheoretic study 3nf proceedings twentyfifth acm sigmodsigactsigart symposium principles database systems june 2628 2006 chicago il usa bassem sayrafi dirk van gucht differential constraints proceedings twentyfourth acm sigmodsigactsigart symposium principles database systems june 1315 2005 baltimore maryland solmaz kolahi leonid libkin xml design relational storage proceedings 16th international conference world wide web may 0812 2007 banff alberta canada chris giannella edward robertson approximation measures functional dependencies information systems v29 n6 p483507 september 2004