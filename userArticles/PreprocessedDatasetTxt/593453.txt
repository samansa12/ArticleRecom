parallel algorithms discovery association rules discovery association rules important data mining task several parallel sequential algorithms proposed literature solve problem almost algorithms make repeated passes database determine set frequent itemsets subset database items thus incurring high io overhead parallel case algorithms perform sumreduction end pass construct global counts also incurring high synchronization costin paper describe new parallel association mining algorithms algorithms use novel itemset clustering techniques approximate set potentially maximal frequent itemsets set identified algorithms make use efficient traversal techniques generate frequent itemsets contained cluster propose two clustering schemes based equivalence classes maximal hypergraph cliques study two lattice traversal techniques based bottomup hybrid search use vertical database layout cluster related transactions together database also selectively replicated portion database needed computation associations local processor initial setup phase algorithms need communication synchronization algorithms minimize io overheads scanning local database portion twice setup phase processing itemset clusters unlike previous parallel approaches algorithms use simple intersection operations compute frequent itemsets maintain search complex hash structuresour experimental testbed 32processor dec alpha cluster interconnected memory channel network present results performance algorithms various databases compare well known parallel algorithm best new algorithm outperforms order magnitude b introduction recent progress automated data gathering availability cheap storage lot businesses routinely started collecting massive amounts data various facets organization eventual goal data gathering able use information gain competitive edge discovering previously unknown patterns data guide decision making highlevel inference process may provide host useful information customer groups buying patterns stock trends etc process automatic information inferencing commonly known knowledge discovery data mining kdd look one central kdd tasks mining associations discovery association rules important problem database mining prototypical application analysis sales basket data agrawal et al 1996 basket data consists items bought customer along transaction identifier besides retail sales example association rules shown useful domains decision support telecommunications alarm diagnosis predic tion university enrollments etc 11 problem statement problem mining associations basket data introduced agrawal imielinski swami 1993 formally stated let set distinct attributes also called items transaction database transactions unique identifier contains set items called itemset ie transaction itemset k items called kitemset subset length k called ksubset itemset said support transactions contain itemset association rule expression b itemsets b ae confidence association rule given bsupporta simply conditional probability transaction contains b given contains data mining task association rules broken two steps first step consists finding frequent itemsets ie itemsets occur database certain userspecified frequency called minimum support second step consists forming conditional implication rules among frequent itemsets agrawal srikant 1994 second step relatively straightforward support frequent itemsets known rules form generated frequent itemsets provided rules meet desired confidence hand problem identifying frequent itemsets hard given items potentially frequent itemsets form lattice subsets however small fraction whole lattice space frequent discovering frequent itemsets requires lot computation power memory disk io provided parallel computers efficient parallel methods needed discover relevant itemsets focus paper 12 related work sequential algorithms several algorithms mining associations proposed literature apriori algorithm mannila toivonen verkamo 1994 agrawal srikant 1994 agrawal et al 1996 shown superior performance earlier approaches agrawal imielinski swami 1993 park chen et al 1995 houtsma swami 1995 forms core almost current algorithms apriori uses downward closure property itemset support prune itemset lattice property subsets parallel association rules 3 frequent itemset must frequent thus frequent kitemsets used construct candidate 1itemsets pass database made level find frequent itemsets among candidates large disk resident databases algorithms incur high io overhead scanning iteration partition algorithm savasere omiecinski navathe 1995 minimizes io scanning database twice partitions database small chunks handled memory first pass generates set potentially frequent itemsets itemset locally frequent partition second pass global support obtained another way minimize io overhead work small random sample database toivonen 1996 zaki et al 1997a recently proposed new algorithms zaki et al 1997b scan database generating frequent itemsets new algorithms shown outperform previous apriori based approaches order magnitude zaki et al 1997b performance gains obtained using effective itemset clustering lattice traversal techniques paper presents efficient parallel implementations new algorithms parallel algorithms relatively less work parallel mining associations three different parallelizations apriori distributedmemory machine ibm sp2 presented agrawal shafer 1996 count distribution algorithm straightforward parallelization apriori processor generates partial support candidate itemsets local database parti tion end iteration global supports generated exchanging partial supports among processors data distribution algorithm partitions candidates disjoint sets assigned different proces sors however generate global support processor must scan entire database local partition remote partitions iterations thus suffers huge communication overhead candidate distribution algorithm also partitions candidates selectively replicates database processor proceeds independently local database portion still scanned every iteration count distribution shown superior performance among three algorithms agrawal shafer 1996 parallel algorithms improving upon ideas terms communication efficiency aggregate memory utilization also proposed cheung et al 1996b 1996a han karypis kumar 1997 pdm algorithm park chen yu 1995b presents parallelization dhp algorithm park chen yu 1995a ever pdm performs worse count distribution agrawal shafer 1996 recent work presented ccpd parallel algorithm sharedmemory machines zaki et al 1996 similar spirit count distribution candidate itemsets generated parallel stored hash structure shared among processors processor scans logical partition database atomically updates counts candidates shared hash tree ccpd uses additional optimization candidate balancing hashtree balancing shortcircuited subset counting speed performance zaki et al 1996 also presented new parallel algorithm eclat zaki parthasarathy li 1997 dec alpha cluster eclat uses equivalence class itemset clustering scheme along bottomup lattice traversal shown outperform count distribution order magnitude paper present parallelization results new clustering traversal techniques 13 contribution main limitation current parallel algorithms park chen yu 1995b zaki et al 1996 agrawal shafer 1996 cheung et al 1996b 1996a make repeated passes diskresident database partition incurring high io overhead furthermore schemes involve exchanging either counts candidates remote database partitions iteration results high communication synchronization overhead previous algorithms also use complicated hash structures entails additional overhead maintaining searching typically suffer poor cache locality parthasarathy work contrasts approaches several ways present new parallel algorithms fast discovery association rules based ideas zaki parthasarathy li 1997 zaki et al 1997b new parallel algorithms characterized terms clustering information used group related itemsets terms lattice traversal schemes used search frequent itemsets propose two clustering schemes based equivalence classes maximal uniform hypergraph cliques utilize two lattice traversal schemes based bottomup hybrid topdownbottomup search algorithms also use different database layout clusters related transactions together work distributed among processors way processor compute frequent itemsets independently using simple intersection operations interesting benefit using simple intersections algorithms propose implemented directly general purpose database systems holsheimer et al 1995 houtsma swami 1995 techniques eliminate need synchronization initial setup phase enable us scan database two times drastically cutting io overhead experimental testbed 32 processor dec alpha smp cluster 8 hosts 4 processorshost interconnected memory channel gillett 1996 network new parallel algorithms also novel utilize machine configuration information ie assume distributedmemory model across 8 cluster hosts assume sharedmemory model 4 processors host experimentally show new algorithms outperform well known count distribution algorithm also present extensive performance results speedup sizeup communication cost memory usage rest paper organized follows begin providing details sequential apriori algorithm section 3 describes previous apriori based parallel algorithms present main ideas behind new algorithms itemset transaction clustering lattice traversal techniques section 4 section 5 describes design implementation parallel association rules 5 new parallel algorithms experimental study presented section 6 conclusions section 7 2 sequential apriori algorithm section briefly describe apriori algorithm agrawal et al 1996 since forms core parallel algorithms agrawal shafer 1996 cheung et al 1996b 1996a han karypis kumar 1997 park chen yu 1995b zaki et al 1996 apriori uses downward closure property itemset support subset frequent itemset must also frequent thus iteration algorithm itemsets found frequent previous iteration used generate new candidate set pruning step eliminates candidate least one whose subsets frequent complete algorithm shown table 1 three main steps candidates kth pass generated joining l kgamma1 expressed c denotes ith item x j denotes items index j itemset x example let l bc bd deg c bdeg table 1 apriori algorithm 1 ffrequent 1itemsets 2 3 4 transactions 2 5 ksubsets 7 8 set frequent itemsets inserting itemset c k apriori tests whether frequent pruning step eliminate lot unnecessary candidates candidates c k stored hash tree facilitate fast support counting internal node hash tree depth contains hash table whose cells point nodes depth 1 itemsets stored leaves insertion procedure starts root hashing successive items inserts candidate leaf counting c k transaction database ksubsets transaction generated lexicographical order subset searched hash tree count candidate incremented matches subset compute intensive step algorithm last step forms l k selecting itemsets meeting minimum support criterion details 6 zaki parthasarathy ogihara li performance characteristics apriori refer reader agrawal srikant 1994 3 aprioribased parallel algorithms section look previous parallel algorithms algorithms assume database partitioned among processors equalsized blocks reside local disk processor count distribution algorithm agrawal shafer 1996 simple parallelization apriori processors generate entire candidate hash tree processor thus independently get partial supports candidates local database partition followed sumreduction obtain global counts note partial counts need communicated rather merging different hash trees since processor copy entire tree global l k determined processor builds c k1 par allel repeats process frequent itemsets found simple algorithm minimizes communication since counts exchanged among processors however since entire hash tree replicated processor doesnt utilize aggregate memory efficiently implementation count distribution used comparison experiments differs slightly description optimized testbed configuration one copy hash tree resides 8 hosts cluster 4 processors host share hash tree processor still local database portion uses local array gather local candidate support sumreduction accomplished two steps first step performs reduction among local processors host second step performs reduction among hosts also utilize optimization techniques hashtree balancing shortcircuited subset counting zaki et al 1996 improve performance count distribution data distribution algorithm agrawal shafer 1996 designed utilize total system memory generating disjoint candidate sets processor however generate global support processor must scan entire database local partition remote partitions iterations thus suffers high communication overhead performs poorly compared count distribution agrawal shafer 1996 candidate distribution algorithm agrawal shafer 1996 uses property frequent itemsets agrawal shafer 1996 zaki et al 1996 partition candidates iteration l processor generate disjoint candidates independent processors time database selectively replicated processor generate global counts independently choice redistribution pass involves tradeoff decoupling processor dependence soon possible waiting sufficient load balance achieved experiments repartitioning done fourth pass dependence processor processors pruning candidates processor asynchronously broadcasts local frequent set processors parallel association rules 7 iteration pruning information used arrives time otherwise used next iteration note processor must still scan local data per iteration even though uses problemspecific information performs worse count distribution agrawal shafer 1996 candidate distribution pays cost redistributing database scans local database partition repeatedly usually larger jjdjjp 4 efficient clustering traversal techniques section present techniques cluster related frequent itemsets together using equivalence classes maximal uniform hypergraph cliques describe bottomup hybrid itemset lattice traversal techniques also present technique cluster related transactions together using vertical database layout layout able better exploit proposed clustering traversal schemes also facilitates fast itemset support counting using simple intersections rather maintaining searching complex data structures 41 itemset clustering lattice subsets 12345 border frequent itemsets lattice subsets 1234 lattice subsets 345 sublattices induced maximal itemsets figure 1 lattice subsets maximal itemset induced sublattices motivate need itemset clustering means example consider lattice subsets set f1 2 3 4 5g shown figure 1 empty set omitted figures frequent itemsets shown dashed circles two maximal frequent itemsets frequent itemset maximal proper subset frequent itemset shown bold circles due downward closure property itemset support fact subsets frequent itemset must frequent frequent itemsets form border frequent itemsets lie border infrequent itemsets lie border frequent itemsets shown bold line figure 1 optimal association mining algorithm enumerate test frequent itemsets ie algorithm must efficiently determine structure border structure precisely determined maximal frequent itemsets border corresponds sublattices induced maximal frequent itemsets sublattices shown figure 1 given knowledge maximal frequent itemsets design efficient algorithm simply gathers support support subsets single database pass general cannot precisely determine maximal itemsets intermediate steps algorithm however approximate set itemset clustering techniques designed group items together obtain supersets maximal frequent itemsets potential maximal frequent itemsets present two schemes generate set potential maximal itemsets based equivalence classes maximal uniform hypergraph cliques two techniques represent tradeoff precision potential maximal itemsets generated computation cost hypergraph clique approach gives precise information higher computation cost equivalence class approach sacrifices quality lower computation cost 411 equivalence class clustering lets reconsider candidate generation step apriori let l f abc abd abe acd ace ade bcd bce bdeg assuming l kgamma1 lexicographically sorted partition itemsets l kgamma1 equivalence classes based common prefixes ie equivalence class 2 l kgamma2 given candidate kitemsets simply generated itemsets within class joining pairs class identifier prefix example obtain equivalence classes feg observe itemsets produced equivalence class namely set fabc abd abe acd ace adeg independent produced class b set fbcd bce bdeg class 1 member eliminated since candidates generated thus discard class idea partitioning l equivalence classes independently proposed agrawal shafer 1996 zaki et al 1996 equivalence partitioning used zaki et al 1996 parallelize candidate generation step ccpd also used candidate distribution agrawal shafer 1996 partition candidates disjoint sets parallel association rules 9 intermediate step algorithm set frequent itemsets l k 2 determined generate set potential maximal frequent itemsets l k note entire item universe maximal itemset however k 2 extract precise knowledge association among items larger value k precise clustering example figure 2 shows equivalence classes obtained instance 2 equivalence class potential maximal frequent itemset example class 1 generates maximal itemset 12345678 412 maximal uniform hypergraph clique clustering let set items denote vertex set hypergraph berge 1989 family edges subsets e 6 n simple hypergraph hypergraph simple graph simple hypergraph whose edges cardinality 2 maximum edge cardinality called rank j edges cardinality h called uniform hypergraph simple uniform hypergraph rank r called runiform hypergraph subset x ae subhypergraph induced x given ng runiform complete hypergraph vertices denoted k r consists rsubsets runiform complete subhypergraph called runiform hypergraph clique hypergraph clique maximal contained clique hypergraphs rank 2 corresponds familiar concept maximal cliques graph given set frequent itemsets l k possible refine clustering process producing smaller set potentially maximal frequent itemsets observation used given frequent mitemset k ksubsets must frequent graphtheoretic terms item vertex hypergraph ksubset edge frequent mitemset must form kuniform hypergraph clique furthermore set maximal hypergraph cliques represents approximation upperbound set maximal potential frequent itemsets true maximal frequent itemsets contained vertex set maximal cliques stated formally lemma hlk kuniform hypergraph vertex set edge set l k let c set maximal hypergraph cliques h ie let set vertex sets cliques c maximal frequent itemsets f 9t 2 f example uniform hypergraph clique clustering given figure 2 example case l 2 thus corresponds instance general clustering technique reduces case finding maximal cliques regular graphs figure shows equivalence classes maximal cliques within also shows graph class 1 maximal cliques seen immediately clique clustering accurate equivalence class clustering example equivalence class clustering produced potential maximal frequent itemset 12345678 hypergraph clique 12 13 14 15 16 17 18 23 25 27 28 34 35 36 45 46 56 58 68 78 frequent 2itemsets equivalence class graph maximal cliques equivalence class 1 maximal cliques per class equivalence classes figure 2 equivalence class uniform hypergraph clique clustering clustering produces refined set f1235 1258 1278 13456 1568g equivalence class 1 maximal cliques discovered using dynamic programming algorithm class x 2x said cover subset x given class c first identify covering set given fy 2 cjcovy 6 covy 6 covz z 2 c z yg recursively generate maximal cliques elements covering set class maximal clique covering set prefixed class identifier eliminat ing duplicates obtain maximal cliques current class see zaki et al 1997c details general graphs maximal clique decision problem npcomplete garey johnson 1979 however equivalence class graph usually sparse maximal cliques enumerated efficiently edge density increases clique based approaches may suffer factors affecting edge density include decreasing support increasing transaction size effect parameters presented zaki et al 1997b parallel association rules 11 42 lattice traversal equivalence class uniform hypergraph clique clustering schemes generate set potential maximal frequent itemsets potential maximal itemset induces sublattice lattice subsets database items traverse sublattices determine true frequent item sets goal devise efficient schemes precisely determine structure border frequent itemsets different ways expanding frequent itemset border lattice space possible present two schemes traverse sublattices one purely bottomup approach hybrid topdownbottomup scheme16 13 14 12 support potential maximal frequent itemset 123456 sort itemsets support topdown phase true maximal frequent itemsets 1235 13456 hybrid traversal figure 3 bottomup hybrid lattice traversal 421 bottomup lattice traversal consider example shown figure 3 shows particular instance clustering schemes uses l 2 generate set potential maximal itemsets lets assume equivalence class 1 one potential maximal itemset 123456 1235 13456 true maximal frequent itemsets supports 2itemsets class also shown like figure 1 dashed circles represent frequent sets bold circles maximal itemsets boxes denote equivalence classes potential maximal itemset 123456 forms lattice elements equivalence class 16g need traverse lattice determine true frequent itemsets pure bottomup lattice traversal proceeds breadthfirst manner generating frequent itemsets length k generating itemsets level k 1 ie intermediate step determine border frequent kitemsets example pairs elements 1 joined produce new equivalence classes frequent 3itemsets namely producing maximal itemset 1235 6g next step yields frequent class producing maximal itemset 13456 current algorithms use approach example process generating c k l kgamma1 used apriori agrawal et al 1996 related algorithms savasere omiecinski navathe 1995 park 1995a pure bottomup exploration lattice space since bottomup approach frequent subsets maximal frequent itemsets generated intermediate steps traversal 422 hybrid topdownbottomup search bottomup approach doesnt make full use clustering information uses cluster restrict search space may generate spurious candidates intermediate steps since fact subsets itemset frequent doesnt guarantee itemset frequent example itemsets 124 126 figure 3 infre quent even though 12 14 16 frequent envision traversal techniques quickly identify set true maximal frequent itemsets set known either choose stop point interested maximal itemsets gather support subsets well subsets known frequent definition paper restrict attention identifying maximal frequent subsets one possible approach perform pure topdown traversal cluster sublattice scheme may thought trying determine border infrequent itemsets starting top element lattice working way example consider potential maximal frequent itemset 123456 figure 3 turns frequent done case frequent check whether 5subsets frequent step ksubset turns frequent need check subsets approach doesnt work well practice since clusters approximation maximal frequent itemsets lot infrequent supersets true maximal frequent itemsets may generated example would generate 10 infrequent itemsets 123456 12345 12346 12356 12456 1234 1245 1236 1246 1256 using pure topdown scheme instead two infrequent itemsets generated pure bottomup approach 124 126 therefore propose hybrid topdown bottomup approach works well practice parallel association rules 13 basic idea behind hybrid approach quickly determine true maximal itemsets starting single element cluster frequent k itemsets extending one itemset till generate infrequent itemset comprises topdown phase bottomup phase remaining elements combined elements first set generate additional frequent itemsets important consideration topdown phase determine elements cluster combined approach first sort itemsets cluster descending order support start element maximum support extend next element sorted order approach based intuition larger support likely itemset part larger itemset figure 3 shows example hybrid scheme cluster 2itemsets sort 2itemsets decreasing order support intersecting 16 15 produce 156 extended 1356 joining 156 13 13456 finally find 123456 infrequent remaining element 12 simply join elements producing frequent itemset class 12 generates maximal itemset 1235 bottomup hybrid approaches contrasted figure 3 pseudocode schemes shown table 3 43 transaction clustering database layout kdd process consists various steps fayyad piatetskyshapiro smyth 1996 initial step consists creating target dataset focusing certain attributes via data samples database creation may require removing unnecessary information supplying missing data transformation techniques data reduction projection user must determine data mining task choose suitable algorithm example discovery association rules next step involves interpreting discovered associations possibly looping back previous steps discover understandable patterns important consideration data preprocessing step final representation data layout dataset another issue whether preliminary invariant information gleaned process two possible layouts target dataset association mining horizontal vertical layout 431 horizontal data layout format standardly used literature see eg agrawal srikant 1994 mannila toivonen verkamo 1994 agrawal et al 1996 dataset consists list transactions transaction transaction identifier tid followed list items transaction format imposes computation overhead support counting step particular transaction average length l iteration k generate test whether gamma l ksubsets transaction contained c k perform fast subset checking candidates stored complex hashtree data structure searching relevant candidates thus adds additional computation overhead furthermore horizontal layout forces us 14 zaki parthasarathy ogihara li items tids items tids horizontal layout vertical layout figure 4 horizontal vertical database layout scan entire database local partition iteration count candidate distribution must pay extra overhead entailed using horizontal layout furthermore horizontal layout seems suitable bottomup exploration frequent border appears extremely complicated implement hybrid approach using horizontal format alternative approach store potential maximal itemsets subsets data structure fast lookup eg hashtrees agrawal et al 1996 gather support single database scan plan explore later paper 432 vertical data layout vertical inverted layout also called decomposed storage structure holsheimer et al 1995 dataset consists list items item followed tidlist list transactions identifiers containing item example successful use layout found holsheimer et al 1995 savasere omiecinski navathe 1995 zaki parthasarathy li 1997 zaki et al 1997b vertical layout doesnt suffer overheads described horizontal layout due following three reasons first tidlist sorted increasing order support candidate kitemset computed simply intersecting tidlists two data structures need maintained dont generate ksubsets transaction perform search operations hash tree second tidlists contain relevant information itemset enable us avoid scanning whole database compute support count itemset layout therefore take advantage principle locality frequent itemsets cluster itemsets generated parallel association rules 15 moving next cluster third larger itemset shorter tid lists practically always true results faster intersections example consider figure 4 contrasts horizontal vertical layout simplicity shown null elements reality sparse storage used tidlist given 4g tidlist ab simply 4g immediately determine support counting number elements tidlist meets minimum support criterion insert ab l 2 intersections among tidlists performed faster utilizing minimum support value example lets assume minimum support 100 intersecting two itemsets ab support 119 ac support 200 stop intersection moment 20 mismatches ab since support abc bounded 119 use optimization called shortcircuited intersection fast joins inverted layout however drawback examination small itemsets tends costlier horizontal layout employed tidlists small itemsets provide little information association among items particular information present tidlists 1itemsets example database 1000000 1m transactions 1000 frequent items average 10 items per transaction tidlists average size 10000 find frequent 2itemsets intersect pair items requires operations hand horizontal format simply need form pairs items appearing transaction increment count requiring gamma number possible solutions problem 1 use preprocessing step gather occurrence count 2itemsets since information invariant performed lifetime database cost amortized number times data mined information also incrementally updated database changes time 2 store counts 2itemsets support greater user specified lower bound thus requiring less storage first approach 3 use small sample would fit memory determine superset frequent 2itemsets l 2 lowering minimum support using simple intersections sampled tidlists sampling experiments toivonen 1996 zaki et al 1997a indicate feasible approach superset determined easily verify true frequent itemsets among current implementation uses preprocessing approach due simplicity plan implement sampling approach later paper solutions represent different tradeoffs sampling approach generates l 2 onthefly extra database pass preprocessing approach requires extra storage items count storage requires om 2 disk space quite large large values however used experiments adds memory channel address receive transmit transmit receive receive figure 5 memory channel space lined region mapped transmit receive node 1 receive node 2 gray region mapped receive node 1 transmit node 2 small extra storage overhead using second approach reduce storage requirements may require extra scan lower bound support changed note also database requires amount memory horizontal vertical formats obvious figure 4 5 new parallel algorithms design implementation 51 dec memory channel digitals memory channel mc network gillett 1996 provides applications global address space using memory mapped regions region mapped process address space transmit receive virtual addresses regions map physical addresses located io space mcs pci adapter virtual addresses receive regions map physical ram writes transmit regions collected source mc adapter forwarded destination mc adapters hub transferred via dma receive regions global identifier see figure 5 regions within node shared across different processors node writes originating given node sent receive regions node loopback enabled region use loopback feature use writedoubling instead processor writes receive region transmit region processes host see modification made processes host though pay cost double writing reduce amount messages hub system unicast multicast processtoprocess writes latency 52 perlink transfer bandwidths mbs mc peak aggregate bandwidth also 32 mbs memory channel guarantees write ordering local cache parallel association rules 17 coherence two writes issued transmit region even different nodes appear order every receive region write appears receive region invalidates locally cached copies line 52 initial database partitioning assume database vertical format support counts 2itemsets available locally host assume database tidlists initially partitioned among hosts partitioning done offline similar assumption made count distribution agrawal shafer 1996 tidlists partitioned total length tidlists local portions host roughly equal achieved using greedy algorithm items sorted support next item assigned least loaded host note entire tidlist item resides host figure 6 shows original database resultant initial partition two processors 53 new parallel algorithms present four new parallel algorithms depending clustering lattice traversal scheme used ffl pareclat uses equivalence class clustering bottomup lattice traversal ffl parmaxeclat uses equivalence class clustering hybrid traversal ffl parclique uses maximal uniform hypergraph clique clustering bottomup lattice traversal ffl parmaxclique uses maximal uniform hypergraph clique clustering hybrid traversal algorithms using bottomup lattice traversal namely pareclat par clique generate frequent itemsets using hybrid traversal namely parmaxeclat parmaxclique generate maximal frequent itemsets noted earlier trivial modify hybrid traversal algorithms generate frequent itemsets interested examining benefits quickly identifying maximal elements hybrid scheme present parallel design implementation issues applicable four algorithms 54 parallel design implementation new algorithms overcome shortcomings count candidate distribution algorithms utilize aggregate memory system partitioning itemset clusters disjoint sets assigned different processors dependence among processors decoupled right beginning equivalence classes equivalence class weights 1 6 equivalence class assignment p1 2 3 partitioned database original database tidlists tidlist communication sublattices p1 2 3 sublattice p0 1 sublattice induced l2 figure 6 database partitioning cluster scheduling parallel association rules 19 table 2 pseudocode new parallel algorithms 1 begin parassociation 2 initialization phase 3 form l2 2itemset support counts 4 generate clusters l2 using 5 equivalence classes uniform hypergraph cliques 6 partition clusters among processors p 7 scan local database partition 8 transmit relevant tidlists processors 9 receive tidlists processors 10 asynchronous phase 11 assigned cluster c2 12 compute frequent itemsets bottomupc2 hybridc2 13 final reduction phase 14 aggregate results output associations 15 end parassociation redistribution cost amortized later iterations since processor proceed independently costly synchronization end iteration furthermore new algorithms use vertical database layout clusters relevant information itemsets tidlist processor computes frequent itemsets one cluster proceeding next local database partition scanned contrast candidate distribution must scan iteration algorithms dont pay extra computation overhead building searching complex data structures generate subsets transaction intersection performed itemset immediately inserted l k notice tidlists also automatically prune irrelevant transactions itemset size increases size tidlist decreases resulting fast intersections two distinct phases algorithms initialization phase responsible communicating tid lists among processors asynchronous phase generates frequent itemsets pseudocode new algorithms shown table 2 541 initialization phase initialization step consists three substeps first support counts 2itemsets preprocessing step read frequent ones inserted l 2 second applying one two clustering schemes l 2 equivalence class maximal hypergraph clique clustering set potential maximal frequent itemsets generated potential maximal clusters partitioned among processors suitable level loadbalancing achieved third database repartitioned processor local disk tidlists 1itemsets cluster assigned scheduling first partition l 2 equivalence classes using common prefix described using equivalence class clustering already potential maximal itemsets however using clique clustering generate maximal cliques within class see section 4 next generate schedule equivalence classes different processors manner minimizing load imbalance minimizing interprocess commu nication note may necessary sacrifice amount load balancing better communication efficiency reason whole equivalence classes including maximal cliques within assigned processor load balancing achieved assigning weighting factor equivalence class based number elements class since consider pairs next iteration assign weight class elements weights assigned generate schedule using greedy heuristic sort classes weights assign class turn least loaded processor ie one least total weight point ties broken selecting processor smaller identifier steps done concurrently processors since access global l 2 figure 6 shows example along equivalence classes weights assignment classes two processors notice entire sublattice induced given class assigned single processor leads better load balancing even though partitioning may introduce extra computation example 234 frequent 1234 cannot frequent either since belong different equivalence classes assigned different processors information used although size class gives good indication amount work better heuristics generating weights possible example could better estimate number frequent itemsets could derived equivalence class could use estimation weight believe decoupling processor performance right beginning holds promise even though may cause load imbalance since repartitioning cost amortized later iterations deriving better heuristics scheduling clusters minimize load imbalance well communication part ongoing research tidlist communication clusters partitioned among processors processor exchange information every processor read nonlocal tidlists memory channel network minimize communication aware fact configuration one local disk per host recall cluster 8 hosts 4 processors per host hosts take part tidlist exchange additional processes 8 hosts spawned asynchronous phase accomplish interprocess tidlist communication processor scans item tidlists local database partition writes transmit region mapped receive processors processors extract tidlist parallel association rules 21 receive region belongs cluster assigned example figure 6 shows initial local database two hosts final local database tidlist communication table 3 pseudocode bottomup hybrid traversal 1 input c equivalence 2 class maximal clique 3 clustering kitemsets 4 output frequent itemsets 2 c k 5 6 2 c k 7 c 8 9 10 nsup minsup 11 c 12 end 13 c k1 14 bottomupc k1 15 end 1 2 topdown phase 3 4 5 6 nsup minsup 7 8 else break 9 end 11 bottomup phase 12 2 s2 13 15 c3 16 end 542 asynchronous phase end initialization step relevant tidlists available locally host thus processor independently generate frequent itemsets assigned maximal clusters eliminating need synchronization processors cluster processed moving next cluster step involves scanning local database partition thus benefit huge io savings since cluster induces sublattice depending algorithm either use bottomup traversal generate frequent itemsets use hybrid traversal generate maximal frequent itemsets pseudocode two lattice traversal schemes shown table 3 note initially tidlists 1itemsets stored locally disk using tidlists 2itemset clusters generated since clusters generally small resulting tidlists kept memory bottomup approach tidlists 2itemsets clusters intersected generate 3itemsets cardinality resulting tidlist exceeds minimum support new itemset inserted l 3 split resulting frequent 3itemsets equivalence classes based common prefixes length 2 pairs 3itemsets within equivalence class intersected determine l 4 22 zaki parthasarathy ogihara li till frequent itemsets found l k determined delete thus need main memory space itemsets l kgamma1 within one maximal cluster topdown phase hybrid traversal maximal element seen far needs memoryresident along itemsets yet seen new algorithms therefore main memory space efficient experimental results memory usage algorithms presented next section pruning candidates recall count candidate distribution use pruning step eliminate unnecessary candidates step essential algorithms reduce size hash tree smaller trees lead faster support counting since subset transaction tested tree however vertical database layout found pruning step little help attributed several factors first additional space computation overhead constructing searching hash tables also likely degrade locality second extra overhead generating subsets candidate third extra communication overhead communicating frequent itemsets iteration even though may happen asynchronously fourth average size tidlists decreases itemsets size increases intersections performed quickly shortcircuit mechanism end asynchronous phase accumulate results processor print 55 salient features new algorithms section recapitulate salient features proposed algorithms contrasting count candidate distribution algorithms differ following respect ffl unlike count distribution utilize aggregate memory parallel system partitioning candidate itemsets among processors using itemset clustering schemes ffl decouple processors right beginning repartitioning database processor compute frequent itemsets independently eliminates need communicating frequent itemsets end iteration ffl use vertical database layout clusters transactions containing itemset tidlists using layout enables algorithms scan local database partition two times processor first scan communicating tidlists second obtaining frequent itemsets contrast count candidate distribution scan database multiple times iteration ffl compute frequent itemsets performs simple intersections two tid lists extra overhead associated building searching complex hash tree data structures complicated hash structures also suffer poor cache locality parthasarathy zaki li 1997 algorithms parallel association rules 23 available memory utilized keep tidlists memory results good locality larger itemsets generated size tidlists decreases resulting fast intersections shortcircuiting join based minimum support also used speed step ffl algorithms avoid overhead generating subsets transaction checking candidate hash tree support counting 6 experimental evaluation table 4 database properties database d1 d1 size d4 d4 size experiments performed 32processor 8 hosts 4 processorshost digital alpha cluster interconnected via memory channel network gillett 1996 system unicast multicast processtoprocess writes latency 52 perlink transfer bandwidths 30mbs alpha processor runs 233mhz theres total 256mb main memory per host shared among 4 processors host host also 2gb local disk attached less 500mb available us partitioned databases reside local disks processor used different synthetic databases generated using procedure described agrawal srikant 1994 used benchmark databases many association rules algorithms agrawal srikant 1994 holsheimer et al 1995 park chen yu 1995a savasere omiecinski navathe 1995 agrawal et al 1996 table 4 shows databases used properties number transactions denoted r r replication factor databases roughly 90mb size except sizeup experiments results shown databases replication factor could go beyond replication factor 6 used sizeup experiments since repartitioned database would become large fit disk average transaction size denoted jt j average maximal potentially frequent itemset size ji j number maximal potentially frequent itemsets 2000 number items refer reader agrawal srikant 1994 detail database generation experiments performed minimum support value 025 fair comparison algorithms discover frequent kitemsets k 3 using supports 2itemsets preprocessing step execution time sec count distribution pareclat execution time sec pareclat parclique parmaxeclat parmaxclique figure 7 parallel performance t10i4d2048k 61 performance comparison section compare performance new algorithms count distribution henceforth referred cd shown superior data candidate distribution agrawal shafer 1996 figures different parallel configurations represented hxpyt z denotes number hosts number processors per host z total number processors used experiments figures 7 8 9 show total execution time different databases different parallel configurations configurations arranged increasing order configurations arranged increasing order h first column compares pareclat cd second column compares new algorithms differences among apparent clearly seen pareclat outperforms cd almost configurations databases improvements high factor 5 look best new algorithm second column see improvement order magnitude even dramatic improvements possible lower minimum support zaki parthasarathy li 1997 interesting trend figures performance gap seems decrease larger configurations cd actually performing better h8p4t32 t10i4d2084k t15i4d1471k see consider figure 10 shows total number frequent itemsets different sizes different databases also figure 11 shows initial database repartitioning tidlist communication cost percentage parallel association rules 25 total execution time pareclat becomes clear enough work two databases sufficiently offset communication cost consequently 70 time spent initialization phase t20i6d1137k work pareclat still twice fast cd basic argument falls classic computation versus communication tradeoff parallel computing whenever ratio high expect pareclat outperform cd also expect relative improvements pareclat cd better larger databases unfortunately due disk space constraints able test algorithms larger databases except configurations local database partition less available memory cd entire database would cached first scan performance cd thus best case scenario since results include real hit cd would taken multiple disk scans mentioned section 55 pareclat designed scan database frequent itemset computation execution time sec count distribution pareclat total execution time sec pareclat parclique parmaxeclat parmaxclique figure 8 parallel performance t15i4d1471k second column figures 7 8 9 shows differences among new algorithms different databases parallel configurations several parameters affecting performance seen general parclique parmaxclique perform better pareclat parmaxeclat respectively use maximal hypergraph clique approach generates precise clusters axis general parmaxclique par maxeclat outperform parclique pareclat respectively hybrid lattice traversal scheme generates maximal frequent itemsets saving number intersections results also dependent number 26 zaki parthasarathy ogihara li execution time sec count distribution pareclat execution time sec pareclat parclique parmaxeclat parmaxclique figure 9 parallel performance t20i6d1137k frequent itemsets larger number frequent itemsets opportunity hybrid approach save joins example consider shows total number tidlist intersections performed four algorithms three databases t20i6d1137k largest number frequent itemsets see figure 10 parmaxclique cuts number intersections 60 pareclat reduction 20 parmaxeclat 35 parclique factors responsible trends indicated winner terms total execution time clearly parmaxclique improvements pareclat high 40 62 memory usage figure 12 shows total memory usage pareclat algorithm computation frequent itemsets progresses mean memory usage tidlists less 07mb databases even though database 360mb figure shows cases memory usage twice mean peaks graph usually due initial construction 2itemset tidlists within cluster since equivalence class clusters large observe maximum usage 35mb pareclat still less 10 database algorithms expect peaks lower since maximal clique clustering precise resulting smaller clusters hybrid traversal doesnt need entire cluster 2itemsets initially parallel association rules 271000300050007000 number frequent itemsets frequent itemset size k frequent itemsets number intersections pareclat parmaxeclat parclique parmaxclique figure 10 number frequent kitemsets b number intersections communication communication communication figure 11 communication cost pareclat 63 sensitivity analysis figures 13 14 15 first column show speedup different databases parallel configurations due disk constraints used replication 28 zaki parthasarathy ogihara li246810memory usage 2mean time t10i4d2084k memory usage mean055mb pareclat12345memory usage 2mean time t15i4d1471k memory usage mean043mb pareclat5152535memory usage 2mean time t20i6d1137k memory usage mean069mb pareclat figure 12 memory usage pareclat h1p1t1152535h1p1t1 h1p2t2 h2p1t2 h1p4t4 h2p2t4 h4p1t4 h2p4t8 h4p2t8 h8p1t8 h4p4t16 h8p2t16 h8p4t32 relative t10i4d2084k speedup pareclat parmaxeclat parclique total execution time sec replication factor t10i4d2084k sizeup pareclat parmaxeclat parclique parmaxclique figure 13 t10i4d2048k speedup sizeuph4p1t4 factor 4 database sizes approximately 360mb speedup numbers parallel association rules 2915253545 relative t15i4d1471k speedup pareclat parmaxeclat parclique parmaxclique50150250350 total execution time sec replication factor t15i4d1471k sizeup pareclat parmaxeclat parclique parmaxclique figure 14 t15i4d1471k speedup sizeuph4p1t4 impressive first glance however surprising example largest configuration h8p4t32 theres 11mb data per processor combined fact amount computation quite small see figure 50 70 time spent tidlist communication see figure 11 see maximum speedup 5 another reason communication involves 8 hosts additional processes host spawned initialization phase thus represents partiallyparallel phase limiting speedups take communication costs see maximum speedup 12 16 interesting trend stepeffect seen speedup graphs configurations number total processors ones hosts perform better also configurations total processors 4 configurations immediate preceding 1 processor per host performs better cases reason increasing number processors given host causes increased memory contention bus traffic increased disk contention processor tries access database local disk time sizeup sizeup experiments fixed parallel configuration h4p1t4 varied database replication factor 1 6 total database size ranging 90mb 540mb figures 13 14 15 second column show sizeup four algorithms different databases figures indicate almost linear sizeup slightly upward bend due relative computation versus communication cost larger database time spent communication intersection cost doesnt increase relative pareclat parmaxeclat parclique total execution time sec replication factor pareclat parmaxeclat parclique parmaxclique figure 15 t20i6d1137k speedup sizeuph4p1t4 pace moreover number frequent itemsets remains constant since use percentages minimum support opposed absolute counts replication factors 7 conclusions paper proposed new parallel algorithms discovery association rules algorithms use novel itemset clustering techniques approximate set potentially maximal frequent itemsets set identified algorithms make use efficient traversal techniques generate frequent itemsets contained cluster propose two clustering schemes based equivalence classes maximal hypergraph cliques study two lattice traversal techniques based bottomup hybrid search also use vertical database layout cluster related transactions together database also selectively replicated portion database needed computation associations local processor initial setup phase algorithms need communication synchronization algorithms minimize io overheads scanning local database portion two times setup phase processing itemset clusters algorithms use simple intersection operations compute frequent itemsets dont maintain search complex hash structures added benefit using simple intersections algorithms propose im parallel association rules 31 plemented directly general purpose database systems holsheimer et al 1995 using techniques presented four new algorithms pareclat equivalence class bottomup search parclique maximal clique bottomup algorithms discover frequent itemsets parmaxeclat equiv alence class hybrid search parmaxclique maximal clique hybrid search discover maximal frequent itemsets implemented algorithms processor dec cluster interconnected dec memory channel network compared well known parallel algorithm count distribution agrawal 1996 experimental results indicate substantial performance improvement obtained using techniques acknowledgments work supported part nsf research initiation award ccr 9409120 arpa contract f1962894c0057 r parallel mining association rules fast algorithms mining association rules fast discovery association rules mining association rules sets items large databases hypergraphs combinatorics finite sets fast distributed algorithm mining association rules efficient mining association rules distributed databases kdd process extracting useful knowledge volumes data computers intractability guide theory npcompleteness memory channel optimized cluster interconnect scalable parallel data mining association rules acm sigmod conf perspective databases data mining 11th intl efficient algorithms discovering association rules effective hash based algorithm mining association rules efficient parallel data mining association rules acm intl efficient algorithm mining association rules large databases sampling large databases association rules parallel data mining association rules sharedmemory multiprocessors evaluation sampling data mining association rules new algorithms fast discovery association rules new algorithms fast discovery association rules localized algorithm parallel association mining srinivasan parthasarathy currently doctoral student university rochester wei li received ph tr ctr jennifer seitzer james p buckley yi pan inded distributed knowledgebased learning system ieee intelligent systems v15 n5 p3846 september 2000 sukomal pal aditya bagchi association dissociation pragmatic considerations frequent itemset generation fixed variable thresholds acm sigkdd explorations newsletter v7 n2 p151159 december 2005 fast algorithm mining sequential patterns large databases journal computer science technology v16 n4 p359370 712001 euihong sam han george karypis vipin kumar scalable parallel data mining association rules ieee transactions knowledge data engineering v12 n3 p337352 may 2000 mohammed j zaki parallel distributed association mining survey ieee concurrency v7 n4 p1425 october 1999 congnan luo anil l pereira soon chung distributed mining maximal frequent itemsets data grid system journal supercomputing v37 n1 p7190 july 2006 jiawei han laks v lakshmanan jian pei scalable frequentpattern mining methods overview tutorial notes seventh acm sigkdd international conference knowledge discovery data mining august 2629 2001 san francisco california chenyong hu benyu zhang yongji wang shuicheng yan zheng chen qing wang qiang yang learning quantifiable associations via principal sparse nonnegative matrix factorization intelligent data analysis v9 n6 p603620 november 2005 yang srinivasan parthasarathy sameep mehta generalized framework mining spatiotemporal patterns scientific data proceeding eleventh acm sigkdd international conference knowledge discovery data mining august 2124 2005 chicago illinois usa gregory buehrer srinivasan parthasarathy shirish tatikonda tahsin kurc joel saltz toward terabyte pattern mining architectureconscious solution proceedings 12th acm sigplan symposium principles practice parallel programming march 1417 2007 san jose california usa assaf schuster ran wolff dan trock highperformance distributed algorithm mining association rules knowledge information systems v7 n4 p458475 may 2005 shengnan cong jiawei han jay hoeflinger david padua samplingbased framework parallel data mining proceedings tenth acm sigplan symposium principles practice parallel programming june 1517 2005 chicago il usa mohammed j zaki scalable algorithms association mining ieee transactions knowledge data engineering v12 n3 p372390 may 2000 aleksandar lazarevic zoran obradovic boosting algorithms parallel distributed learning distributed parallel databases v11 n2 p203229 march 2002 mohammed j zaki neal lesh mitsunori ogihara planmine predicting plan failures using sequence mining artificial intelligence review v14 n6 p421446 december 1 2000 john holt soon chung parallel mining association rules text databases journal supercomputing v39 n3 p273299 march 2007 vipin kumar mohammed zaki high performance data mining tutorial pm3 tutorial notes sixth acm sigkdd international conference knowledge discovery data mining p309425 august 2023 2000 boston massachusetts united states petr hjek martin holena formal logics discovery hypothesis formation machine theoretical computer science v292 n2 p345357 27 january andrzej skowron jan komorowski zdzislaw pawlak lech polkowski rough sets perspective data knowledge handbook data mining knowledge discovery oxford university press inc new york ny 2002 andrzej skowron rough sets boolean reasoning granular computing emerging paradigm physicaverlag gmbh heidelberg germany 2001 maniatty mohammed j zaki systems support scalable data mining acm sigkdd explorations newsletter v2 n2 p5665 dec 2000