data task parallel image processing environment paper presents data task parallel lowlevel image processing environment distributed memory systems image processing operators parallelized data decomposition using algorithmic skeletons image processing applications parallelized task decomposition based image application task graph way image processing application parallelized data task decomposition thus better speedups obtained validate method multibaseline stereo vision application b introduction image processing widely used many application areas including lm industry medical imaging industrial manufacturing weather forecasting etc areas size images large yet processing time small sometimes realtime processing required therefore last decade increasing interest developing use parallel algorithms image processing many algorithms developed parallelizing dierent image operators dierent parallel architectures parallel image processing algorithms either architecture dependent specically developed dierent applications hard implement typical image processing user without enough knowledge parallel computing paper present approach adding data task parallelism image processing library using algorithmic skeletons 3 4 5 image application task graph iatg skeletons algorithmic abstractions common series applications implemented parallel skeletons embedded sequential host language thus source parallelism program using skeletons create data parallel image processing framework easy use typical image processing user already known exploiting task data parallelism program solve large computational problems yields better speedups compared either pure task parallelism pure data parallelism 7 8 main reason data task parallelism relatively limited therefore using one limits achievable performance thus exploiting mixed task data parallelism emerged natural solution many applications eld signal image processing data set sizes limited physical constraints cannot easily increased cases amount available data parallelism limited example multibaseline stereo application described section 5 size image determined circuitry video cameras throughput camera interface increasing image size means buying new cameras building faster interface may feasible since data parallelism limited additional parallelism may come tasking coding image processing application using skeletons iatg obtain data task parallel environment paper organized follows section 2 brie presents description algorithmic skeletons survey related work section 3 presents classication lowlevel image operators skeletons parallel lowlevel image processing distributed memory system section 4 presents related work describes image application task graph used task parallel framework multibaseline stereo vision application together data parallel code using skeletons versus sequential code speedup results data parallel approach versus data task parallel approach presented section 5 finally concluding remarks made section 6 skeletons related work skeletons algorithmic abstractions encapsulate dierent forms parallelism common series applications aim obtain environments languages allow easy parallel programming user handle problems com munication synchronization deadlocks nondeterministic program runs usually embedded sequential host language used code hide parallelism application user concept algorithmic skeletons new lot research done demonstrate usefulness parallel programming skeletons polymorphic higherorder functions dened functional languages straightforward way reason skeletons build upon functional language 3 4 work also done using skeletons image processing 5 serot et al presents parallel image processing environment using skeletons top caml functional language paper develop algorithmic skeletons create parallel image processing environment ready use easy implementationdevelopment parallel image processing applications dierence previous approach 5 allow application implemented c programming environment allow possibility useimplement dierent scheduling algorithms obtaining minimum execution time 3 skeletons lowlevel image processing 31 classication lowlevel image operators lowlevel image processing operators use values image pixels modify image way divided point operators neighborhood operators global operators 1 2 disscus detail three types operators 1 point operators image point operators powerful functions image processing large group operators falls category main characteristics pixel output image depends corresponding pixel input image point operators used copy image one memory location another arithmetic logical operations table lookup image compositing discuss detail arithmetic logic operators classifying point view number images involved important issue developing skeletons arithmetic logic operations image alu operations fundamental operations needed almost imaging product variety purposes refer operations image constant monadic operations operations two images dyadic operations operations involving three images triadic operations monadic image operations monadic image operators alu operators image constant operations shown table 1 sx dx source destination pixel values location x k constant table 1 monadic image operations function operation add constant dx subtract constant dx multiply constant dx divide constant dx constant dx constant dx xor constant dx absolute value dx monadic operations useful many situations instance used add subtract bias value make picture brighter darker dyadic image operators dyadic image operators arithmetic logical functions pixels two source images producing destination image functions shown table two source images used create destination image dx table operations function operation add subtract multiply divide min dyadic operators many uses image processing example subtraction one image another useful studying ow blood digital subtraction angiography motion compensation video coding addition images useful step many complex imaging algorithms like development image restoration algorithms moddeling additive noise special eects image morphing motion pictures triadic image operators triadic operators use three input images computation output image example operation alpha blending image compositing useful function graphics computer imaging graphics compositing used combine several images one typically images rendered separately possibly using dierent rendering algorithms example images may rendered separately possibly using dierent types rendering hardware dierent algorithms image processing compositing needed product needs merge multiple pictures one nal image image editing programs well programs combine synthetically generated images scanned images need function computer imaging term alpha blend dened using two source images s2 alpha image destination image see formula 1 another example triadic operator squared dierence reference image two shifted images operator used multibaseline stereo vision application described section 5 table 3 triadic image operations function operation alpha blend dx squared di dx 2 local neighborhood operators neighborhood operators lters create destination pixel based criteria depend source pixel value pixels neighborhood surrounding neighborhood lters largely used computer imaging used enhancing changing appearance images sharpening blurring crispening edges noise removal also useful image processing applications object recognition image restoration image data compression dene lter operation changes pixels source image based values surrounding pixels may linear nonlinear lters linear ltering versus nonlinear ltering generally speaking lter imaging refers process produces destination image source image linear lter property weighted sum source images produces similarly weighted sum destination images contrast linear lters nonlinear lters somewhat dicult characterize output lter given input cannot predicted impulse response nonlinear lters behave dierently dierent inputs linear ltering using twodimensional discrete convolution imaging twodimensional convolution common way implement linear lter operation performed source image twodimensional convolution kernel produce destination image convolution kernel typically much smaller source image starting top image top left corner also origin image kernel moved horizontally image one pixel time moved one row moved horizontally process continued kernel traversed entire image destination pixel row column n kernel centered location source image mathematically twodimensional discrete convolution dened double summation given mn image fm n kl convolution kernel hk l dene origin top left corner assume fm n much larger hk l result convolving fm n hk l image gm n given formula formula assume kl odd numbers extend image k 12 lines vertical direction l 12 columns horizontal direction sequential time complexity operation omnkl observed time consuming operation well tted data parallel approach 3 global operators global operators create destination pixel based entire image information representative example operator within class discrete fourier transform dft discrete fourier transform converts input data set tempo ralspatial domain frequency domain vice versa lot applications image processing used image enhancement restoration compression image processing input set pixels forming twodimensional function already discrete formula output pixel x lm following x jk e 2i jl j k column coordinates 0 j n 1 0 k 1 also include class global operators operators like histogram transform image output another data structure 32 data parallelism lowlevel image operators operator description given previous section conclude point neighborhood global image processing operators parallelized using data parallel paradigm hostnode approach host processor selected splitting distributing data nodes host also processes part image node processes received part image host gathers assembles image back together figures 1 2 3 present data parallel paradigm hostnode approach point neighborhood global operators global operators send entire image corresponding nodes node process certain part image order avoid extra interprocessor communication due border information exchange neighborhood operators extend partition image showed figure 2 way node processor receives data needed applying neighborhood operator original image processed image master 0 master 0 node 1 node 2 node n1 master 0 fig 1 dcg skeleton point operators original image extended image master master node 1 node 2 node n1 master0processed image fig 2 dcg skeleton neighborhood operator original image processed area 0 processed area 1 processed area 2 processed area n1 processed image master master 0 0 fig 3 dcg skeleton global operators based observations identify number skeletons parallel processing lowlevel image processing operators named according type lowlevel operator number images involved operation headers skeletons shown based distribute compute gather dcg main skeleton previously known map skeleton 4 suitable regular applications lowlevel operators image processing implementation skeletons based ideea described paragraph see figures 1 2 3 skeleton run set processors set processors host processor selected split distribute images nodes node set receives part images image operator applied computation takes place result sent back host processor skeletons implemented c using mpipanda library 19 20 implementation transparent user void imagepointdist1iounsigned int nchar namevoidimop dcg skeleton monadic point operators one inputoutput void imagepointdist1iocunsigned int nchar name voidimopfloat ct dcg skeleton monadic point operators need constant value pararameter one inputoutput void imagepointdist1i1ounsigned int nchar name1char name2voidimop dcg skeleton monadicdyadic point operators one input one output void imagepointdist1io1iunsigned int nchar name1char name2voidimop dcg skeleton monadicdyadic point operators one inputoutput one input void imagepointdist2i1ounsigned int nchar name1char name2char name3voidimop dcg skeleton dyadictriadic point operators 2 inputs one output void imagepointdist2i2ounsigned int nchar name1char name2char name3char name4 inputs 2 outputs void imagepointdist3i1ounsigned int nchar name1char name2char name3char name4 dcg skeleton triadic point operators 3 inputs one output void imagewindowdist1iounsigned int nchar namewindow winvoidimop dcg skeleton neighborhood operators one inputoutput void imagewindowdist1i1ounsigned int nchar name1char name2window winvoidimop dcg skeleton neighborhood operators one input one output void imageglobaldist1iounsigned int nchar namevoidimop dcg skeleton global operators one inputoutput develop several types skeletons depend type lowlevel operator point neighborhood global number inputoutput images skeleton associate parameter represents task number corresponding skeleton used task parallel framework depending skeleton type one identiers images given parameters last argument point operator processing images skeleton used number lowlevel image processing operators perform similar way instance dyadic point operators take two input images combine process depending operator type produce output image depending operator type skeleton type might exist additional parameters necessary image operator point operators assigned imagepointdist skeletons neighborhood operators assigned imagewindowdist skeletons global operators assigned imageglobaldist skeletons skeletons modify input image imagepointdist 1io imagewindowdist 1io imageglob aldist 1io 1io stands 1 inputoutput image skeletons take number input images create new output image example imagepointdist 2i 1o skeleton point operators takes 2 input images creates new output image skeleton necessary dyadic point operators like addition subtraction etc see table 2 create new image processing two input images similarly skeleton imagepointdist 3i 1o point operators takes 3 input images creates new output image example lowlevel image processing operator suitable type skeleton squared dierence one reference image two disparity images operator used multibaseline stereo vision application see table 3 section 5 similar skeletons exist also local neighborhood global operators imagepointdist 1io c skeleton monadic point operators need constant value parameter processing input image see table 1 present example using skeletons code simple image processing application dataparallel way image processing application edge detection using laplace sobel operators first read input image create two output images 3 3 window apply laplace sobel operators num nodes number processors num nodes number nodes application run detected rst line partial code showed image name input image given input parameter skeletons image l image output parameters images skeleton used imagewindowdist 1i 1o skeleton perform operators last two parameters window used contains information size data window image operator applied via skeleton 4 task parallel framework recently shown exploiting task data parallelism program solve large computational problems yields better speedups compared either pure data parallelism either pure task parallelism 7 8 main reason task data parallelism relatively limited therefore using one bounds achievable performance thus exploiting mixed task data parallelism emerged natural solution show applying data task parallelism improve speedup application level considerable eort adding taskparallel support dataparallel lan guages fx 10 fortran 11 paradigm hpf 7 adding dataparallel support taskparallel languages orca 12 order fully exploit potential advantage mixed task data parallelism ecient support task data parallelism critical issue done compiler level also application level applications image processing eld suitable technique mixed task data parallel techniques use directed acyclic graph literature also called macro data ow graph mdg 7 data parallel tasks case image processing operators nodes precedence relationships edges purpose work change name graph image application task graph iatg 41 image application task graph model task parallel program modeled macro data ow communication graph 7 directed acyclic graph c v nite set nodes represents tasks image processing operators e set directed edges represent precedence constraints tasks w weight function gives weight processing time node task task weights positive integers c communication function c gives weight communication time edge communication weights positive integers image processing application task graph iatg fact mdg node stands image processing operator edge stands precedence constraint two adjacent operators case node represents larger entity mdg node simple instruction program important properties iatg weighted directed acyclic graph nodes represent image processing operators edges represent precedence constraints two distinguished nodes start precedes nodes stop succeeds nodes dene well balanced iatg application task graph type tasks image operators level example iatg multibaseline stereo vision application described section 5 figure 7 rst level squared dierence operator applied 3 images task second level error operator executed tasks moreover graph edges form regular pattern weights nodes edges iatg based concepts processing communication costs processing costs account computation communication costs data parallel tasks image processing operators corresponding nodes depend number processors allocated node communication costs account costs data communication nodes 42 processing cost model node iatg represents processing task image processing operator applied via dcg skeleton described section 32 runs nonpreemptively number processors task assumed computation cost denoted exec p function number processors computation cost function task obtained either estimation proling cost estimation use amdahls law according execution time task task number p number processors task executed tasks execution time single processor fraction task executes serially use proling tasks execution costs either tted function similar one described case data available processors proled values used directly table values simple determine measure execution times basic image processing operators implemented image processing library tabulate values 43 communication cost model data communication redistribution essential implementing execution scheme uses data task parallelism individual tasks executed data parallel fashion subsets processors data dependences tasks may necessitate changing set processors also distribution scheme figure 4 illustrates classical approach redistribution pair tasks task taska executed using seven processors reads data task taskb executed using four processors reads data necessitates redistribution data seven processors executing task taska four processors executing task taskb addition changing set processors could also change distribution scheme data instance two dimensional data taska might use block distribution whereas taskb might use rowstripe distribution processors executing taskb processors executing taska redistribution fig 4 data redistribution two tasks processors executing taskb processors executing taska master master b fig 5 image communication two host processors reduce complexity problem rst allowing one type distribution scheme rowstripe second sending images two processors selected host processors two sets processors shown figure 5 edge iatg corresponds precedence relationship associated communication cost denoted tcomm depends network characteristics latency bandwidth amount data transferred emphasized two types communication times first internal communication time represents time internal transfer data processors allocated task quantity part term execution time associated node graph secondly external communication time time transferring data ie images two processors two processors represent host processors two associated image processing tasks corresponding two adjacent graph nodes quantity actually communication cost edge graph case also use either cost estimation proling determine communication time stateoftheart distributed memory systems time send message containing l units data processor another processor modeled startup per byte cost pointtopoint communication l length message bytes run experiments distributed memory system consists cluster pentium pro200mhz pcs 64mb ram running linux connected myrinet 3dmesh topology dimension order routing 16 figure 6 shows performance pointtopoint communication operations predicted communication time reported time minimum time obtained 20 executions code reasonable select minimum value possible interference caused users trac network measurements perform linear tting extract communication parameters b figure 6 see predicted communication time based formula approximates good measured communication time measured predicted timemicroseconds message size 2 fig 6 performance pointtopoint communication das 44 iatg cost properties task input edges called entry task task output edges called exit task length path graph sum computation communication costs nodes edges belonging path dene critical path 7 cp longest path graph graph n nodes n last node graph represents nish time node exec p execution time task set p nodes critical path given formulas 7 pred set immediate predecessor nodes node dene average area 7 iatg n nodes tasks p processor system formula 8 p number processors allocated task p critical path represents longest path iatg average area provides measure processortime area required iatg based two formulas processors allocated tasks according results obtained solving following minimization problem subject solving allocation problem scheduler needed schedule tasks obtain minimum execution time classical approach wellknown list scheduling paradigm 13 introduced graham schedules one processor tasks tasks running one processor scheduling known npcomplete one processor tasks since several list scheduling algorithms proposed scheduling problem also extended multiple processor tasks tasks run nonpreemptively number processors 7 therefore multiple processor task scheduling also npcomplete heuristics used intuition behind minimizing equation 9 represents theoretical lower bound time required execute image processing application corresponding iatg execution time application neither smaller critical path graph less average area graph tsass convex programming algorithm 7 determining number processors task available used experimental part section 5 nonlinear solver based snopt 17 available internet 18 solving previous problem solving scheduling problem proposed scheduling algorithm presented 7 used another possibility use scheduling algorithms developed data task parallel graphs 8 9 5 experiments evaluate benets propose data parallel framework based skeletons also task parallel framework based iatg rst compare code multibaseline stereo vision algorithm without using skeletons without data parallelism compare speedups obtained applying data parallelism application speedups obtained data task parallelism multibaseline stereo vision application uses algorithm developed okutomi kanade 6 described webb al 14 15 gives greater accuracy depth use two cameras input consists three n n images acquired three horizontally aligned equally spaced cameras one image reference image two named match images 16 disparities 15 rst match image shifted pixels second image shifted 2d pixels dierence image formed computing sum squared dierences corresponding pixels reference image shifted match images next error image formed replacing pixel dierence image sum pixels surrounding 13 13 window disparity image formed nding pixel disparity minimizes error finally depth pixel displayed simple function disparity figure 7 presents iatg application observed computation dierence images requires point op erators computation error images requires neighborhood operators computation disparity image requires also point operator input ref m1 m2 reference two match images d015 task t1d m1 shifted pixels task t2d m2 shifted 2d pixels task t5 disparity image minimizes err image pseudocode multibaseline stereo vision application broadcast diff0 diff1 diff2 diff15 err0 err1 err2 reduce ref disparity image13171933 fig 7 multibaseline stereo vision iatg present sequential code application versus data parallel code application coding application combining number skeletons doesnt require much eort image processing user yet parallelizes application data task parallel code slightly dicult present sequential code imagepointdist3i1odimrefm1 dtpipe code based skeletons besides creating images host processor code nearly function headers dier skeleton parameters name images window image operator sequential version operator headers parameters images window skeletons implemented c using mpi 19 results data parallel approach compared results obtained using data task parallelism distributed memory system consists cluster pentium pro200mhz pcs 64mb ram running linux 16 connected myrinet 3dmesh topology dimension order routing task parallel framework use special mechanism register images processors rst created moreover skeleton associated task number corresponds use 1 2 4 8 16 32 64 processing nodes pool three articial reference images sizes 256 256 512 512 1024 1024 used code written using c mpi message passing library multibaseline stereo vision algorithm example regular well balanced application task parallelism applied without need allocator scheduler comparison reasons used algorithm described 7 obtained identical results divide number nodes number tasks obtain number nodes task run figure 8 show speedups obtained data parallel approach dierent image sizes figure 9 shows speedup application using data task parallel approach also dierent image sizes observe speedups become quickly saturated dataparallel approach speedups data task parallel approach perform good fact pure task parallelism 16 processors data task parallelism 16 pure task parallel speedups become attened 16 processors type application better rst apply task parallelism add data parallelism using data task parallelism ecient using data parallelism processors fig 8 speedup dataparallel approac processors81624324048 fig 9 speedup data task parallel approach 6 conclusions presented environment data task parallel image processing data parallel framework based algorithmic skeletons easy use image processing user task parallel environment based image application task graph computing iatg communication processing costs iatg regular well balanced graph task parallelism applied without need computations showed example using skeletons task parallel framework multibaseline stereo vision application multibaseline stereo vision example image processing application contain parallel tasks tasks simple image point neighborhood operator using data task parallelism ecient using data parallelism code data task parallel environment using c mpipanda library 19 20 easily ported parallel machines r parallel algorithms digital image processing parallel programming algorithmic skeletons structured management parallel computations skeletons structured parallel composition multiplebaseline stereo framework exploiting task data parallelism distributed memory multicomputers optimal use mixed task data parallelism pipelined compu tations cpr mixed task data parallel scheduling distributed systems new model integrated nested task data parallel program ming fortran language modular parallel programming task data parallel programming language based shared objects bounds multiprocessing timing anomalies implementation performance fast parallel multibaseline stereo vision distributed asci supercomputer das site users guide snopt 53 fortran package largescale nonlinear programming lucent technologies ampl site mpi complete reference vol1 mpi core experience portability layer implementing parallel programming systems tr algorithmic skeletons parallel algorithms fortran new model integrated nested task data parallel programming framework exploiting task data parallelism distributed memory multicomputers task dataparallel programming language based shared objects optimal use mixed task data parallelism pipelined computations mpithe complete reference multiplebaseline stereo cpr ctr development platform parallel image processing proceedings 6th wseas international conference signal speech image processing p3136 september 2224 2006 lisbon portugal antonio plaza david valencia javier plaza pablo martinez commodity clusterbased parallel processing hyperspectral imagery journal parallel distributed computing v66 n3 p345358 march 2006 frank j seinstra dennis koelma andrew bagdanov finite state machinebased optimization data parallel regular domain problems applied lowlevel image processing ieee transactions parallel distributed systems v15 n10 p865877 october 2004