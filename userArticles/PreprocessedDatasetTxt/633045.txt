scalable application layer multicast describe new scalable applicationlayer multicast protocol specifically designed lowbandwidth data streaming applications large receiver sets scheme based upon hierarchical clustering applicationlayer multicast peers support number different data delivery trees desirable propertieswe present extensive simulations protocol narada applicationlayer multicast protocol internetlike topologies results show groups size 32 protocol lower link stress 25 improved similar endtoend latencies similar failure recovery properties importantly able achieve results using orders magnitude lower control trafficfinally present results widearea testbed experimented 32100 member groups distributed 8 different sites experiments average group members established maintained lowlatency paths incurred maximum packet loss rate less 1 members randomly joined left multicast group average control overhead experiments less 1 kbps groups size 100 b introduction multicasting efficient mechanism packet delivery onemany data transfer applications eliminates redundant packet replication network also decouples size receiver set amount state kept single node therefore useful primitive scale multiparty applications however deployment networklayer multicast 11 beenwidely adopted commercial isps thus large parts internet still incapable native multicast decade protocols developed applicationlayer multicast protocols 10 12 7 14 15 24 18 change network infrastructure instead implement multicast forwarding functionality exclusively endhosts applicationlayer multicast protocols increasingly used implement efficient commercial content distribution networks paper present new applicationlayer multicast protocol developed context nice project university maryland 1 nice recursive acronym stands nice internet cooperative environment paper refer nice applicationlayer multicast protocol simply nice protocol protocol designed support applications large receiver sets applications include news sports ticker services suchas infogate httpwwwinfogatecom andespn bottomline httpwwwespncom realtime stock quotes updates eg yahoo market tracker popular internet radio sites applications characterized large potentially tens thousands receiver sets relatively low bandwidth soft realtime data streams withstand loss refer class large receiver set low bandwidth realtime data applications data stream applications data stream applications present unique challenge applicationlayer multicast protocols large receiver sets usually increase control overhead relatively lowbandwidth data makes amortizing control overhead difficult nice used implement large data stream applications since provably small con stant control overhead produces low latency distribution trees possible implement highbandwidth applications using nice well however paper concentrate exclusively low bandwidth data streams large receiver sets 11 applicationlayer multicast basic idea applicationlayer multicast shown figure 1 unlike native multicast data packets replicated routers inside network applicationlayer multicast data packets replicated end hosts logically endhosts form overlay network goal applicationlayer multicast construct maintain efficient overlay data transmission since appli network layer multicast application layer multicast figure 1 networklayer application layer multicast squarenodesare routers circular nodesare endhosts dotted lines represent peers overlay cationlayer multicast protocols must send identical packetsover link less efficient native multicast two intuitive measures goodnessfor application layer multicast lays namely stress stretch defined 10 stress metric defined perlink counts number identical packets sent protocol underlying link network stretch metric defined permember ratio pathlength source member along overlay length direct unicast path consider applicationlayer multicast protocol data source unicasts data receiver clearly multiunicast protocol minimizes stretch doesso cost stress links near source n number group members also requires oncontrol overhead single point however protocol robust sense number group member failures affect members group general applicationlayer multicast protocols evaluated along three dimensions ffl quality data delivery path quality tree measured using metrics stress stretch node degrees ffl robustnessof overlay since endhosts potentially less stable routers important applicationlayer multicast protocols mitigate effect receiver failures robustnessof applicationlayer multicast protocols measured quantifying extent disruption data delivery different members fail time takes protocol restore delivery members present first comparison aspect applicationlayer multicast protocols ffl control overhead efficient use network resources control overhead members low important cost metric study scalability scheme large member groups 12 nice trees goals nice develop efficient scalable distributed treebuilding protocol require underlying topology information specifically nice protocol reduces worstcase state control overheadat member olog n maintains constant degree bound group members approach olog n stretch bound possible topologyaware centralized algorithm additionally also show average member maintains state constant number members incurs constant control overheadfor topology creation andmain tenance nice applicationlayer multicast scheme create hier archicallyconnectedcontrol topology data delivery path implicitly defined way hierarchy structured additional route computations required along analysis various bounds also present simulationbased performance evaluation nice simula tions compare nice narada applicationlayer multicast protocol 10 narada first proposed efficient applicationlayer multicast protocol small group sizes extensions subsequently proposed 9 tailor applicability highbandwidth mediastreaming applications groups studied using simulations implementation lastly present results widearea implementation quantify nice runtime overheads convergence properties various group sizes 13 roadmap rest paper structured follows section 2 describe general approach explain different delivery trees built nice present theoretical bounds nice protocol section 3 present operational details pro tocol present performance evaluation methodology section 4 present detailed analysis nice protocol simulations section 5 widearea implementation section 6 elaborate related work section 7 conclude section 8 2 solution overview nice protocol arranges set end hosts hierarchy basic operation protocol create maintain hi erarchy hierarchy implicitly defines multicast overlay data paths described later section member hierarchy crucial scalability since members bottom hierarchy maintain state constant number members members top hierarchy maintain soft state olog n members logically member keeps detailed state members near hierarchy limited knowledge members group hierarchical structure also important localizing effect member failures nice hierarchy describedin paper similar member hierarchy used 3 scalable multicast group rekeying ever hierarchy 3 layered multicastcapable network constructed using network multicast services eg scoped expanding ring searches build necessary hierarchy unicast infrastructure provide multicastcapable network paper use endtoend latency distance metric hosts constructing nice hierarchy members close respect distance metric mapped part hierarchy allows us produce trees low stretch rest section describe nice hierarchy defined invariants must maintain describe used establish scalable control data paths 21 hierarchical arrangement members nice hierarchy created assigning members different levels layers illustrated figure 2 layers numbered sequentially lowest layer hierarchy layer zero denoted l0 hosts layer partitioned set clusters cluster size k 3k gamma 1 k constant consists set hosts close explain choice cluster size bounds later paper section 321 cluster cluster leader protocol distributedly chooses graphtheoretic center clus figure 3 control data delivery paths twolayer hierarchy hosts members l0 clusters b hosts members layers l0 l1 c host leader l1 cluster comprising b hosts clusterleaders clusterleaders layer 0 form layer 1 topological clusters joined layer 0 hosts g figure 2 hierarchical arrangement hosts nice layers logical entities overlaid underlying physical network ter leader ie cluster leader minimum maximum distance hosts cluster choice cluster leader important guaranteeing new joining member quickly able find appropriate position hierarchy using small number queries members hosts mapped layers using following scheme hosts part lowest layer l0 clustering protocol l0 partitions hosts set clusters cluster leaders clusters layer l join layer l i1 shown example figure 2 using 3 layer l0 clusters abcd efgh jklm 2 example assume c f centers respective clusters l0 clusters chosen leaders form layer l1 clustered create single cluster cfm layer l1 f center cluster hence leader therefore f belongs layer l2 well thenice clusters layers created using distributed algorithm described next section following properties hold distribution hosts different layers ffl host belongs single cluster layer ffl host present cluster layer l must occur one cluster layers fact clusterleader lower layers ffl host present layer l cannot present layer ffl cluster size bounded k 3k gamma 1 leader graphtheoretic center cluster ffl log k n layers highest layer single member denote cluster comprising hosts also define term supercluster host x assume host x belongs layers layer let xyz cluster belongs highest layer ie layer leader cluster supercluster x defined cluster next higher layer ie l leader belongs follows one supercluster defined every host except host belongs topmost layer supercluster supercluster layer immediately highest layer h belongs example figure 2 cluster cfm layer1 supercluster hosts b nice host maintains state clusters belongs one layer belongs supercluster 22 control data paths host hierarchy used define different overlay structures control messages data delivery paths neighbors control topology exchange periodic soft state refreshes generate high volumes traffic clearly useful structure higher connectivity control messages since cause protocol converge quicker figure 3 illustrate choices control data paths using clusters size 4 edges figure indicate peerings group members overlay topology set four hosts arranged 4clique panel 0 clusters layer l0 hosts c0 cluster leaders four l0 clusters andform single cluster layer l1 host c0 leader cluster layer l1 rest paper use denote cluster layer l j member x belongs defined x belongs layer l j control topology nice protocol illustrated figure 3 panel 0 consider member x belongs layers peers control topology members clusters x belongs layers ie members clusters x using examplefigure 3 panel 0 member a0 belongs layer l0 therefore control path peers members l0 cluster ie b0 contrast member b0 belongs layers l0 l1 therefore control path peers members l0 cluster ie a0 a1 a2 l1 cluster ie b1 b2 c0 control topology member cluster therefore exchanges soft state refreshes remaining members cluster allows cluster members quickly identify changes cluster membership turn enables faster restoration set desirable invariants described section 24 might violated changes delivery path multicast data distribution needs loop free otherwise duplicate packet detection suppression mecha clusters j end figure 4 data forwarding operation host h received data host p nisms need implemented therefore nice protocol choose data delivery path tree specifically given data source data delivery path sourcespecific tree implicitly defined control topology member executes instance procedure multicastdataforward given figure 4 decide set members needs forward data panels 1 2 3 figure 3 illustrate consequent sourcespecific trees sources members a0 a7 c0 respectively call basic data path summarize cluster layer control topology clique data topology star possible choose structures eg cluster ring control path balanced binary tree data path 23 analysis cluster hierarchy k 3k gamma 1 members control topology host belongs layer l0 peers ok hosts exchange control messages general host belongs layer l higher layer peers ok hosts eachof layers fore control overhead member ok hence clusterleader highest layer cluster host c0 figure 3 peers total ok log n neighbors worst case control overhead member follows using amortized cost analysis control overhead average member constant number members occur layer l higher layer boundedby onk therefore amortized control overhead average member log n log n asymptotically increasing n thus control overhead ok average member ok log n worst case holds analogously stress members basic data path 3 also number applicationlevel hops basic data path pair members olog n ok log n peers data path acceptableupper bound definedenhancementsthat reduce upperbound number peers member constant stress eachmember enhanceddata path created using local transformations basic data path thus reducedto constant number applicationlevel hops pair members still remain bounded olog n outline enhancement basic data path 4 24 invariants properties described analysis hold long hierarchy maintained thus objective nice protocol 3 note stress metric members equivalent degree members data delivery tree scalably maintain host hierarchy new members join existing members depart specifically protocol described next section maintains following set invariants ffl every layer hosts partitioned clusters size k 3k gamma 1 ffl hosts belong l0 cluster host belongs single cluster layer ffl cluster leaders centers respective clusters form immediate higher layer 3 protocol description section describe nice protocol using highlevel description detailed description protocol including packet formats pseudocode found 4 assume existence special host members know apriori using nomenclature developed 10 call host rendezvouspoint rp host intends join applicationlayer multicast group contacts rp initiate join pro cess ease exposition assume rp always leader single cluster highest layer hierarchy interacts cluster members layer control path bypassedon data path clearly possible rp part hierarchy leader highest layer cluster maintain connection rp belabor complexity application streaming media delivery rp could distinguished host domain data source nice protocol three main components initial cluster assignment new host joins periodic cluster maintenanceand refinement recovery leader failures discuss turn 31 new host joins new host joins multicast group must mapped cluster layer l0 illustrate join procedure figure 5 assume host a12 wants join multicast group first contacts rp join query panel 0 rp responds hosts present highest layer hierarchy joining host contacts members highest layer panel 1 identify member closest example highest layer l2 onemember c0 default closest member a12 amongst layer l2 members host c0 informs a12 three members b0 b1 b2 l1 cluster a12 contacts members join query identify closest member among panel 2 iteratively uses procedure find l0 cluster important note host h belongs layer l center l igamma1 cluster recursively approximation center among members l0 clusters part layered hierarchy hence querying layer succession top hierarchy layer l0 results progressive refinement joining host find appropriate layer l0 cluster join close joining member outline operation presented pseudocode procedure basicjoinlayer figure 6 assume hosts aware single wellknown host rp initiate join process therefore overheads due join queryresponse messages highest rp descreasesdown layers hierarchy rapid sequenceof joins rp need handlea large number join queryresponsemessages alternate andmore scalable join schemes join l0 l2 c0 join l0 b0b1b2 attach figure 5 host a12 joins multicast group j find st disth disth x x 2 decrement j endwhile join cluster cl j figure basic join operation member h join layer l new member 0 h already part layer l seeks membership information cl jgamma1 member queryrp gamma seeks membership information topmost layer hierarchy rp possible assume joining host aware nearbyhost already joined overlay fact pastry 19 tapestry 23 alleviate potential bottleneck rp rapid sequence joins based assumption 311 join latency joining process involves message overhead ok log n queryresponse pairs joinlatency depends delays incurred exchanges typically olog n roundtrip times protocol aggressively locate possible good peers joining member overhead locating appropriate attachments joining member relatively large reducethe delay betweena member joining multicast group receipt first data packet overlay allow joining members temporarily peer data path leader cluster current layer querying example figure 5 a12 querying hosts b0 b1 b2 closest point attachment temporarily peers c0 leader layer l1 cluster data path allows joining host start receiving multicast data group within single roundtrip latency join 312 joining higher layers important invariant hierarchical arrangement hosts leader cluster center cluster therefore members join leave clusters clusterleader may occasionally change considera changein leadership cluster c layer current leader c removes layers l j1 higher attached new leader chosen affected clusters example new leader h c layer l j chosen required join nearest l j1 cluster current supercluster definition cluster layer l j1 outgoing leader c joined ie new leader replaces outgoing leader supercluster ever supercluster information stale currently invalid new leader h invokes join procedure join nearest l j1 cluster calls basicjoinlayerhj routine terminates appropriate layer l j1 cluster found also note basicjoinlayer requires interaction member h rp rp therefore aids repairing hierarchy occasional overlay partitions ie entire supercluster information becomes stale periodic heartbeat messages exchanged cluster members rp fails correct operation protocol require capable recovery within reasonable amount time 32 cluster maintenance refinement member h cluster c sends heartbeat message every h seconds cluster peers neighbors control topology messagecontains distance estimate h member c possible h inaccurate estimate distance members eg immediately joins cluster clusterleader includes complete updated cluster membership heartbeat messagesto members allows existing members set appropriate peer relationships new cluster members control path cluster level l clusterleader also periodically sends immediate higher layer cluster membershipwhich supercluster members cluster l cluster cluster member state sent via unreliable messages kept cluster member softstate refreshed periodic heartbeat messages member h declared longer part cluster independently members cluster receive message h configurable number heartbeat message intervals 321 cluster split merge clusterleader periodically checks size cluster appropriately splits merges cluster detects size bound violation cluster exceeds cluster size upper bound two equalsized clusters correct operation protocol could chosen cluster size upper bound value 2k gamma 1 however waschosenas upperbound cluster would require split exceeds upper bound ie reaches size 2k subsequently equalsized split would create two clusters size k however single departure new clusters would violate size lower bound require cluster merge operation performed choosing larger upper bound eg 3k1 avoids problem cluster exceeds upper bound split two clusters size least 3k2 therefore requires least k2 member departures merge operation needs invoked cluster leader initiates cluster split operation given set hosts pairwise distances cluster split operation partitions subsetsthat meet size boundssuch maximum radius graphtheoretic sense new set clusters minimized similar kcenter problem known nphard additional size constraint use approximation strategy leader splits current cluster two equalsized clusters maximum radii among two clusters minimized also chooses centers two partitions leaders new clusters transfers leadership new leaders leadertransfer messages new clusters still violate size upper bound split new leaders using identical operations size cluster cl j layer k leader j initiates cluster merge operation note j belongs layer l i1 cluster cl i1 j j chooses closest clusterpeer k cl i1j k also leader layer l cluster initiates merge operation c sending clustermergerequest message k j updates members cl j merge information k similarly updates members cl k following merge j removes layer l i1 ie cluster cl i1 j 322 refining cluster attachments member joining layer may always able locate closest cluster layer eg due lost join query join response etc instead attaches cluster layer therefore eachmember h layer say l periodically probes members supercluster leaders layer l clusters identify closest member say j supercluster j leader l cluster h belongs inaccurate attachment detected case h leaves current layer l cluster joins layer l cluster j leader 33 host departure leader selection host h leaves multicast group sends remove message clusters joined gracefulleave however h fails without able send message cluster peers h detects departure nonreceipt periodic heartbeat message h h leader clus ter triggers new leader selection cluster remaining member j cluster independently select new leader cluster depending j estimates center among members multiple leaders reconciled single leader cluster exchange regular heartbeat messages using appropriate flag leadertransfer time two candidate leaders detect multiplicity present details operations 4 possible members inconsistent view cluster membership transient cycles develop data path cycles eliminated protocol restores hierarchy invariants reconciles cluster view members 4 experimental methodology analyzed performance nice protocol using detailed simulations widearea implementation simulation environment compare performance nice three schemes multiunicast native ipmulticast using core based tree protocol 2 narada applicationlayer multicast protocol given 10 internet experiments benchmark performance metrics direct unicast paths member hosts clearly native ip multicast trees least unit stress since link forwards single copy data packet unicast paths lowest latency consider unit stretch 4 provide us reference compare applicationlayer multicast protocols 41 data model experiments model scenario data stream source multicasting group chose single endhost uniformly random data source generating constant bit rate data packet data sequence effectively samples data path overlay topology time instant entire data packet sequence captures evolution data path time 42 performance metrics compare performance different schemes along following dimensions ffl quality data path measured three different metrics tree degree distribution stress links routers stretch data paths group members ffl recovery host failure hosts join leave multicast group underlying data delivery path adapts accordingly reflect changes experiments modeled member departures group ungraceful depar tures ie members fail instantly unable send appropriate leave messages existing peers topol ogy therefore transience particularly host failures path hosts may unavailable also possible multiple paths exist single host cycles develop temporarily study effects measured fraction hosts correctly receive data packets sent source group membership changed also recorded number duplicates host simulations applicationlayer multicast protocols number duplicates insignificant zero cases ffl control traffic overhead report mean variance distribution control bandwidth overheads routers end hosts 5 simulation experiments implemented packetlevel simulator four different protocols network topologies generated using transitstub graphmodel using gtitm topology generator 5 topologies simulations 10 000 routers average node degree 3 4 endhosts attached set routers chosen uniformly random among stub domain nodes number hosts multicast group varied 8 2048 different experiments simulations modeled lossless links thus data loss due congestion notion background traffic jit ter however data lost whenever applicationlayer multicast 4 recent studies 20 1 show may always case however use native unicast latency reference compare performance schemes protocol fails provide path source receiver duplicates received whenever one path thus simulations study dynamics multicast protocol effects data distribution implementation performance also affected factors additional link latencies due congestion drops due crosstraffic congestion comparisonwe haveimplemented entire narada protocol description given 10 narada protocol mesh first applicationlayer multicast approach designed primarily small multicast groups approach members distributedly construct mesh overlay topology multiple paths exists pairs members member participates routing protocol overlay mesh topology generate sourcespecific trees reach members narada initial set peer assignments create overlay mesh done randomly initial data delivery path may poor quality time narada adds good links discards bad links overlay narada 2 aggregate control overhead meshfirst nature requires host periodically exchange updates refreshes hosts protocol defined 10 number userdefined parameters needed set include link adddrop thresholds link adddrop probe frequency periodic refresh rates mesh degree etc present detailed description implementation narada protocol including impact different choices parameters 4 51 simulation results havesimulated widerange topologies group sizes member joinleave patterns protocol parameters nice set cluster size parameter k 3 experiments presented broadly findings summarized follows ffl nice trees data paths stretch comparable narada ffl stress links routers lower nice especially multicast group size increases ffl failure recovery schemes comparable ffl nice protocol demonstratesthat possibleto provide performance orders magnitudelower control overhead groups size 32 begin results representative experiment captures different aspects comparing various protocols 511 simulation representative scenario thisexperiment hastwo different phases join phaseand leave phase join phase set 128 members 5 join multicast group uniformly random simulated time 0 200 seconds hosts allowed stabilize appropriate overlay topology simulation time 1000 seconds leave phase starts time 1000 seconds hosts leave multicast group short duration 10 seconds repeated four times 100 second intervals remaining 48 members continue part multicast group end simulation member departures modeled host failures since damaging effect data paths experimented different numbers member departures single member 16 members leaving ten secondwindow sixteen departures agroup 5 show results 128 member case group size used experiments reported 10 nice performs increasingly better larger group sizes size 128 within short time window drastic scenario helps illustrate failure recovery modes different protocols better member departures smaller sizes cause correspondingly lower disruption data paths experimented different periodic refresh rates narada higher refresh rate recovery host failures quicker cost higher control traffic overhead narada used different values route update frequencies periods probing mesh members add drop links overlay sults report results using route update frequencies every 5 seconds labeled narada5 every seconds la beled narada30 second update period corresponds used 10 ran 5 second update period since heartbeat period nice set 5 seconds note could run much smaller heartbeat period nice without significantly increasing control overheadsince control messages limited within clusters traverse entire group also varied mesh probe period narada observed data path instability effect discussedabove results set narada mesh probe period 20 seconds data path quality figures 7 8 show average link stress average path lengths different protocols data tree evolves member join phase note figure shows actual path lengths endhosts stretch ratio average path length members protocol average path length members multiunicast protocol explained earlier join procedure nice aggressivelyfinds good points attachment members overlay topology nice tree converges quicker stable value within 350 seconds simulated time contrast narada protocols gradually improve mesh quality consequently data path longer duration average data path length converges stable value 23 hops 500 600 seconds simulated time corresponding stretch 218 narada path lengths improve time due addition good links mesh time stress tree gradually increases since narada decides add drop overlay links based purely stretch metric clusterbased data dissemination nice reduces average link stress general large groups nice converges trees 25 lower average stress experiment nice tree lower stretch narada tree however experiments narada tree slightly lower stretch value gen eral comparing results multiple experimentsover different group sizes see section 512 concluded data path lengths receivers similar protocols figures 9 10 plot cumulative distribution stress path length metrics entire member set 128 members time data paths converged stable operating point distribution stress links multiunicast scheme significantly large tail eg links close source stress 127 contrasted better stress distribution nice narada narada uses fewer number links topology nice since comparably aggressive adding overlay links shorter lengths mesh topology however due emphasis shorter path lengths stress distribution links hasa heaviertail nice 25 links stress four higher narada compared 5 nice distribution path lengths two protocols comparable 192123 100 200 300 400 500 600 700 800 900 average link stress time secs 128 endhosts joinjoin narada5 figure 7 average link stress simulation1525100 200 300 400 500 600 700 800 900 average receiver path length time secs 128 endhosts joinjoin narada5 ip multicast unicast figure 8 average path length number links link stress cumulative distribution link stress overlay stabilizes unicast truncated extends stress 127 narada5 unicast figure 9 stress distribution number hosts overlay path length hops cumulative distribution data path lengths overlay stabilizes unicast ip multicast narada5 figure 10 path length distribution simulation failure recovery control overheads investigate effect host failures present results second part scenario starting simulated time 1000 sec onds set 16 members leave group 10 second period repeat procedure four times members leave simulated time 1400 seconds group reduced 48 members members leave protocols heal data distribution tree continue send data partially connected topology figure 11 show fraction members correctly receive data packets duration narada5 nice similar performance average protocols restore data path remaining receivers within onds also ran experiment period narada lower refresh period caused significant disruptions tree periods 100 seconds 60 tree receive data lastly note data distribution tree used nice least connected topology possible expect failure recovery results much better structures alternate paths built atop nice figure 12 show byteoverheads control traffic access links endhosts dot plot represents sum control traffic kbps sent received eachmember group averaged 10 second intervals thus 10 second time slot two dots plot remaining host multicast group corresponding control overheads narada nice curves plot average control overhead protocol expected groups size 128 nice order magnitude lower average overhead eg simulation time 1000 seconds average control overhead nice 097 kbps versus 6205 kbps narada time instant narada30 shown figure average control overhead 1343 kbps note nice control traffic includes protocol messages including messages cluster formation cluster splits merges layer promotions leader elections 512 aggregate results present set aggregate results group size varied purpose experiment understand scalability different applicationlayer multicast protocols entire set members join first 200 seconds run simulation 1800secondsto allow topologies stabilize table 1 compare stress network routers links overlay path lengths group members average control traffic overheads network routers metric present mean standard deviation note narada protocol involves aggregate control overhead 2 n size group therefore simulation setup unable simu 0fraction hosts correctly received data time secs 128 endhosts join followed periodic leaves sets 16 leave narada5 figure 11 fraction members receiveddata packetsover duration member failures simulation103050700 200 400 600 800 1000 1200 1400 1600 1800 2000 control traffic bandwidth kbps time secs control traffic bandwidth access linksjoin leave narada5 avg figure 12 control bandwidth required endhost accesslinks group router stress link stress path length bandwidth overheads kbps size narada5 nice narada5 nice narada5 nice narada30 nice table 1 data path quality control overheads varying multicast group sizes simulation late narada groups size 1024 larger since completion time simulations order day single run one experiment 550 mhz pentium iii machine 4 gb ram naradaandnice tend converge trees similar path lengths stress metric network links routers however consistently lower nice group size large 64 greater interesting observe standard deviation stress changes increasing group size two protocols standard deviation stress increased narada increasing group sizes contrast standard deviation stress nice remains relatively constant topologybasedclustering nice distributes data path evenly among different links underlying links regardless group size control overhead numbers table different ones figure 12 column table average control traffic per network router opposed control traffic end host since control traffic gets aggregated inside network overhead routers significantly higher overhead endhost router overheads report values version route updatefrequency set onds recall narada30 version poor failure recovery performance much efficient specifically 5 times less overhead groups size 128 narada5 version heartbeat messages nice still sent 5 second intervals nice protocol worst case control overheads members increase logarithmically increase group size control overheads routers shown table 1 show similar trend thus although experimented upto 2048 members simulation study believe protocol scales even larger groups 6 widearea implementation implemented complete nice protocol experimented implementation onemonth period 100 member groups distributed across 8 different sites experimental topology shown figure 13 number members site varied 2 different experi ments example member experiment reported section 2 members sites b g h 4 c 8 unfortunately experiments much larger groups feasible testbed however implementation results protocol overheads closely match simulation experiments believe simulations provide reasonable indication nice implementation would behave larger group sizes 61 implementation specifics haveconductedexperiments data sourcesat different sites paper present representative set experimentswhere data stream source located site c figure 13 fig ure also indicate typical direct unicast latency millisec onds site c sites estimated oneway latencies obtained using sequence application layer udp probes data streams sent source host site c hosts using nice overlay topology implementa ghfedcba3944605333 source csucsbedu b asuedu c csumdedu f umbcedu g polyedu figure 13 internet experiment sites direct unicast latencies c tion experimented different heartbeat rates results presented section set heartbeat message period 10 seconds implementation estimate endtoend latency hosts various protocol operations including member joins leadership changes etc estimated latency two endhosts using lowoverhead estimator sent sequence applicationlayer udp probes controlled number probes adaptively using observed variance latency estimates instead using raw latency estimates distance metric used simple binning scheme map raw latencies small set equivalence classes specifically two latency estimates considered equivalent mappedto equivalence class resulted faster convergence overlay topology specific latency ranges class 01 ms 15 ms 510 ms 1020 ms 2040 ms 40100 ms 100200 ms greater 200 ms compute stretch endhosts internet experiments used ratio latency source host along overlay direct unicast latency host widearea implementation host receives data packet forwarded memberb along overlay tree immediately sends back overlayhop acknowledgment back b b logs roundtrip latency initial transmission data packet receipt acknowledgment entire experiment done calculated overlay roundtrip latencies data packet adding individual overlayhop latencies available logs host estimated oneway overlay latency half round trip latency obtained unicast latencies using lowoverhead estimator immediately overlay experiment terminated guaranteed measurements overlay latencies unicast latencies interfere 62 implementation scenarios internet experiment scenarios two phases join phase rapid membership change phase join phase set member hosts randomly join group different sites hosts allowed stabilize appropriate overlay de0750850951 fraction members stress cumulative distribution stress members members members figure 14 stress distribution testbed livery tree period rapid membership change phase starts host members randomly join leave group average member lifetime group phase set seconds like simulation studies member departures ungraceful allow us study worst case protocol behavior finally let remaining set members organize stable data delivery tree present results three different groups size 32 64 96 members data path quality figure 14 show cumulative distribution stress metric group members overlay stabilizes end join phase group sizes typical members unit stress 74 83 members experiments stress remaining members vary 3 9 members precisely cluster leaders different layers recall cluster size lower upperbounds experiments 3 9 respectively stress members reduced using highbandwidth data path enhancementsdescribed 4 larger groups number members higher stress ie 3 9 experiments since number clusters hence number cluster leaders ever expected increase logarithmic group size figure 15 plot cumulative distribution stretch metric instead plotting stretch value single host group sites located member hosts given site plot mean 95 confidence intervals apart sites c e sites near unit stretch however note source data streams experiments located site c hosts sites c e low latency paths source host actual endtoend latencies along overlay paths sites shown figure 16 sites c e latencies 35 ms 35 ms 30 ms respectively therefore primary contribution latencies packet processing overlay forwarding endhosts table 2 present mean maximum stretch different members direct unicast latency least 2 ms source ie sites b g h different sizes mean stretch sites low however cases see relatively large worst case stretches eg stretch sites distribution stretch 64 members figure 15 stretch distribution testbed515253545 overlay endtoend latency sites distribution latency 64 members figure latency distribution testbed 96member experiment one member stretch overlay path 463 failure recovery section describethe effects groupmembershipchanges data delivery tree observe successful overlay delivering data changesto overlay topology measured number correctly received packets different members rapid membershipchangephase experiment begins initial member set stabilized appropriate overlay topology phase lasts 15 minutes members join leave grou random average lifetime member group seconds figure 17 plot time fraction members successfully received different data packets total membership changes happened duration figure 18 plot cumulative distribution packet losses seen different membersover entire 15 minute duration themaximum number packet losses seen member 50 900 ie 56 and30 members encounterany packetlosses even rapid changes group membership largest continuous duration packet losses single host 34 sec onds typical members experienced maximum continuous0507090 100 200 300 400 500 600 700 800 900 fraction hosts correctly receive data time secs distribution losses packets random membership change phase members average member lifetime figure 17 fraction members received data packets group membership continuously changed testbed01030507090 fraction members fraction packets lost cumulative distribution losses members random membership change phase members average member lifetime figure cumulative distribution fraction packets lost different members entire sequence 900 packets rapid membership change phase testbed data loss two seconds true 4 members failure recovery statistics good enough use data stream applications deployed internet note experiment three individual packets 900 suffered heavy losses data packets times 76 620 819 received 51 36 31 members respectively control overheads finally present control traffic overheadsin kbps table 2 different group sizes overheads include control packets sent well received show average maximum control overhead member observed control traffic members lies 02 kbps 20 kbps different group sizes fact about80 members require less 09 kbps control traffic topology management terestingly average control overheads distributions change significantly group size varied worst case control overhead also fairly low less 3 kbps group stress stretch control overheads kbps size mean max mean max mean max table 2 average maximum values different metrics different group sizestestbed 7 related work number projects explored implementing multicast application layer classified two broad categories meshfirst narada 10 gossamer7 treefirst protocols yoid 12 almi 15 hostmulticast 22 yoid host multicast defines distributed tree building protocol endhosts almi uses centralized algorithm create minimum spanning tree rooted designated single source multicast data distribution overcast protocol 14 organizes set proxies called overcast nodes distribution tree rooted central source single source multicast distributed treebuilding protocol used create source specific tree manner similar yoid rmx 8 provides support reliable multicast data delivery endhosts using set similar proxies called reliable multicast proxies application endhosts configured affiliate nearest rmx architecture assumes existence overlay construction protocol using proxies organize appropriate data delivery path tcp used provide reliable communicationbetween pair peer proxies overlay recent projects chord 21 content addressablenet works 17 tapestry 23 pastry 19 havealso addressed scalability issue creating application layer overlays therefore closely related work definesa virtual ddimensional cartesian coordinate space overlay host owns part space 18 authors leveraged scalable structure define application layer multicast scheme hosts maintain od state path lengths odn 1d application level hops n number hosts net work pastry 19 selforganizing overlay network nodes logical peer relationships overlay based matching prefixes node identifiers scribe 6 largescale event notification infrastructure leverages pastry system create groups build efficient application layer multicast paths group members dissemination events based pas try similar overlay properties namely members olog 2 b n application level hops members 6 bayeux24 another architecture application layer mul ticast endhosts organized hierarchy defined tapestry overlay location routing system 23 level hierarchy defined set hosts share common suffix host ids technique proposed plaxton etal 16 locating routing named objects net work therefore hosts bayeux maintain ob log b n state endtoend overlay paths olog b n application level hops discussed section 23 proposed nice protocol incurs amortized ok state members endtoend paths members olog k n application level hops like pastry tapestry nice also chooses overlay peers based network locality leads low stretch endtoend paths summarize follows nice 6 b small constant multicast members maintain constant state members consequentlyexchangea constantamount periodic refreshes mes sages overhead logarithmic scribe bayeux overlay paths nice scribe andbayeuxhavea logarithmic number application level hops path lengths canmulticast asymptotically havea larger number application level hops nice canmulticast use single wellknown host rp nomenclature bootstrap join procedure members join procedure therefore incurs higher overhead rp higher layers hierarchy lower layers scribe bayeux assume members able find different nearby members overlay outofband mechanismsfrom bootstrap join procedure using assumption join overheads large number joining members amortized different nearby bootstrap members schemes 8 conclusions paper presented new protocol applicationlayer multicast main contribution extremely low overhead hierarchical control structure different data distribution paths canbe built results show possible build maintain applicationlayer multicast trees little head focus paper lowbandwidth data stream applications scheme generalizable different applications appropriately choosing data paths metrics used construct overlays believe results paper significant first step towards constructing large widearea applications applicationlayer multicast 9 acknowledgments thank srinivas parthasarathy implementing part narada protocol used simulation experiments also thank kevin almeroth lixin gao jorg liebeherr steven low martin reisslein malathi veeraraghavan providing us user accounts different sites widearea experiments thank peter druschel shepherding submission final version paper 10 r resilient overlay networks based trees cbt architecture scalable multicast routing scalable secure group communication ip mulitcast scalable application layer multicast model internetwork scribe largescale decentralized applicationlevel multicast infrastructure scattercast architecture internet broadcast distribution infrastructure service rmx reliable multicast heterogeneous networks enabling conferencing applications internet using overlay multicast architecture case end system multicast multicast routing datagram internetworks extended lans yoid extending multicast internet architecture steiner points tree metrics dont really help reliable multicasting overlay network almi application level multicast infrastructure accessing nearby copies replicated objects distributed environment scalable contentaddressable network case informed internet routing transport chord scalable peertopeer lookup service internet applications host multicast framework delivering multicast end users infrastructure faulttolerant widearea location routing bayeux architecture scalable faulttolerant widearea data dissemination tr multicast routing datagram internetworks extended lans accessing nearby copies replicated objects distributed environment case end system multicast keynote address steiner points tree metrics dont really help bayeux enabling conferencing applications internet using overlay muilticast architecture chord scalable contentaddressable network resilient overlay networks detour pastry applicationlevel multicast using contentaddressable networks scalable secure group communication ip multicast tapestry infrastructure faulttolerant widearea location scattercast ctr mohamed hefeeda ahsan habib boyan botev dongyan xu bharat bhargava promise peertopeer media streaming using collectcast proceedings eleventh acm international conference multimedia november 0208 2003 berkeley ca usa duc tran kien hua tai scalable media streaming large peertopeer networks proceedings tenth acm international conference multimedia december 0106 2002 juanlespins france dan rubenstein sneha kasera towsley jim kurose improving reliable multicast using active parity encoding services computer networks international journal computer telecommunications networking v44 n1 p6378 15 january 2004 mojtaba hosseini nicolas georganas design multisender 3d videoconferencing application end system multicast protocol proceedings eleventh acm international conference multimedia november 0208 2003 berkeley ca usa h q guo l h ngoh w c wong j g tan dincast hop efficient dynamic multicast infrastructure p2p computing future generation computer systems v21 n3 p361375 1 march 2005 vivek shrivastava suman banerjee natural selection peertopeer streaming cathedral bazaar proceedings international workshop network operating systems support digital audio video june 1314 2005 stevenson washington usa yang guo kyoungwon suh jim kurose towsley p2cast peertopeer patching video demand service multimedia tools applications v33 n2 p109129 may 2007 dejan kosti adolfo rodriguez jeannie albrecht amin vahdat bullet high bandwidth data dissemination using overlay mesh proceedings nineteenth acm symposium operating systems principles october 1922 2003 bolton landing ny usa suman banerjee seungjoon lee bobby bhattacharjee aravind srinivasan resilient multicast using overlays ieeeacm transactions networking ton v14 n2 p237248 april 2006 atul singh miguel castro peter druschel antony rowstron defending eclipse attacks overlay networks proceedings 11th workshop acm sigops european workshop beyond pc september 1922 2004 leuven belgium bryan horling victor lesser data dissemination techniques distributed simulation environments proceedings 36th conference winter simulation december 0508 2004 washington dc yuwei sung michael bishop sanjay rao enabling contribution awareness overlay broadcasting system acm sigcomm computer communication review v36 n4 october 2006 chris gauthierdickey virginia lo daniel zappala using ntrees scalable event ordering peertopeer games proceedings international workshop network operating systems support digital audio video june 1314 2005 stevenson washington usa suman banerjee seungjoon lee bobby bhattacharjee aravind srinivasan resilient multicast using overlays acm sigmetrics performance evaluation review v31 n1 june zhichen xu chunqiang tang sujata banerjee sungju lee rita receiver initiated justintime tree adaptation rich media distribution proceedings 13th international workshop network operating systems support digital audio video june 0103 2003 monterey ca usa suman banerjee chris kommareddy bobby bhattacharjee efficient peer location internet computer networks international journal computer telecommunications networking v45 n1 p517 15 may 2004 reza rejaie shad stafford framework architecting peertopeer receiverdriven overlays proceedings 14th international workshop network operating systems support digital audio video june 1618 2004 cork ireland suman banerjee seungjoon lee ryan braud bobby bhattacharjee aravind srinivasan scalable resilient media streaming proceedings 14th international workshop network operating systems support digital audio video june 1618 2004 cork ireland kunwadee sripanidkulchai aditya ganjam bruce maggs hui zhang feasibility supporting largescale live streaming applications dynamic application endpoints acm sigcomm computer communication review v34 n4 october 2004 kostas katrinis bernhard plattner bjrn brynjlfsson gsli hjlmtsson dynamic adaptation source specific distribution trees multiparty teleconferencing proceedings 2005 acm conference emerging network experiment technology october 2427 2005 toulouse france meng zhang jianguang luo li zhao shiqiang yang peertopeer network live media streaming using pushpull approach proceedings 13th annual acm international conference multimedia november 0611 2005 hilton singapore mohamed hefeeda bharat k bhargava david k yau hybrid architecture costeffective ondemand media streaming computer networks international journal computer telecommunications networking v44 n3 p353382 20 february 2004 ronaldo ferreira suresh jagannathan ananth grama locality structured peertopeer networks journal parallel distributed computing v66 n2 p257273 february 2006 himabindu pucha charlie hu z morley mao impact research network based testbeds widearea experiments proceedings 6th acm sigcomm internet measurement october 2527 2006 rio de janeriro brazil sencun zhu chao yao donggang liu sanjeev setia sushil jajodia efficient security mechanisms overlay multicast based content delivery computer communications v30 n4 p793806 february 2007 xing jin kanleung cheng sh gary chan scalable island multicast peertopeer streaming advances multimedia v2007 n1 p1010 january 2007 gabriel ghinita panos kalnis spiros skiadopoulos prive anonymous locationbased queries distributed mobile systems proceedings 16th international conference world wide web may 0812 2007 banff alberta canada chiping tang philip k mckinley topologyaware overlay path probing computer communications v30 n9 p19942009 june 2007 minh tran wallapak tavanapong wanida putthividhya ocs effective caching scheme video streaming overlay networks multimedia tools applications v34 n1 p2556 july 2007 meng zhang li zhao yun tang jianguang luo shiqiang yang largescale live media streaming peertopeer networks global internet proceedings acm workshop advances peertopeer multimedia streaming november 1111 2005 hilton singapore chuan wu baochun li optimal peer selection minimumdelay peertopeer streaming rateless codes proceedings acm workshop advances peertopeer multimedia streaming november 1111 2005 hilton singapore jauvane c de oliveira dewan tanvir ahmed shervin shirmohammadi performance enhancement mmogs using entity types proceedings 11th ieee international symposium distributed simulation realtime applications p2530 october 2226 2007 david gotz scalable adaptive streaming nonlinear media proceedings 14th annual acm international conference multimedia october 2327 2006 santa barbara ca usa guang tan stephen jarvis xinuo chen daniel p spooner performance analysis improvement overlay construction peertopeer live streaming simulation v82 n2 p93106 february 2006 yatin chawathe scattercast adaptable broadcast distribution framework multimedia systems v9 n1 p104118 july sergey gorinsky sugat jain harrick vin yongguang zhang design multicast protocols robust inflated subscription ieeeacm transactions networking ton v14 n2 p249262 april 2006 thorsten strufe jens wildhagen gnter schfer towards construction attack resistant efficient overlay streaming topologies electronic notes theoretical computer science entcs 179 p111121 july 2007 sonia fahmy minseok kwon characterizing overlay multicast networks costs ieeeacm transactions networking ton v15 n2 p373386 april 2007 abdolreza abdolhosseini moghadam saman barghi hamid reza rabiee mohammad ghanbari new scheme recovery failure nice overlay protocol proceedings 1st international conference scalable information systems may 30june 01 2006 hong kong aditya ganjam hui zhang connectivity restrictions overlay multicast proceedings 14th international workshop network operating systems support digital audio video june 1618 2004 cork ireland baochun li jiang guo mea wang ioverlay lightweight middleware infrastructure overlay application implementations proceedings 5th acmifipusenix international conference middleware october 1822 2004 toronto canada mojtaba hosseini nicolas georganas end system multicast protocol collaborative virtual environments presence teleoperators virtual environments v13 n3 p263278 june 2004 daria antonova arvind krishnamurthy zheng ravi sundaram managing portfolio overlay paths proceedings 14th international workshop network operating systems support digital audio video june 1618 2004 cork ireland sergey gorinsky sugat jain harrick vin yongguang zhang robustness inflated subscription multicast congestion control proceedings conference applications technologies architectures protocols computer communications august 2529 2003 karlsruhe germany kien hua duc tran range multicast video demand multimedia tools applications v27 n3 p367391 december 2005 mengkun yang zongming fei cooperative failure detection mechanism overlay multicast journal parallel distributed computing v67 n6 p635647 june 2007 anonymous overlay multicast journal parallel distributed computing v66 n9 p12051216 september 2006 shibsankar das jussi kangasharju evaluation network impact content distribution mechanisms proceedings 1st international conference scalable information systems p35es may 30june 01 2006 hong kong ying cai zhan chen wallapak tavanapong caching collaboration cache allocation peertopeer video systems multimedia tools applications v37 n2 p117134 april 2008 shudong jin azer bestavros smallworld characteristics internet topologies implications multicast scaling computer networks international journal computer telecommunications networking v50 n5 p648666 6 april 2006 rob sherwood seungjoon lee bobby bhattacharjee cooperative peer groups nice computer networks international journal computer telecommunications networking v50 n4 p523544 15 march 2006 himabindu pucha ying zhang z morley mao charlie hu understanding network delay changes caused routing events acm sigmetrics performance evaluation review v35 n1 june 2007 zongpeng li anirban mahanti progressive flow auction approach lowcost ondemand p2p media streaming proceedings 3rd international conference quality service heterogeneous wiredwireless networks august 0709 2006 waterloo ontario canada tackling grouptotree matching large scale group communications computer networks international journal computer telecommunications networking v51 n11 p30693089 august 2007 john r douceur jay r lorch thomas moscibroda maximizing total upload latencysensitive p2p applications proceedings nineteenth annual acm symposium parallel algorithms architectures june 0911 2007 san diego california usa karthik lakshminarayanan ananth rao ion stoica scott shenker endhost controlled multicast routing computer networks international journal computer telecommunications networking v50 n6 p807825 13 april 2006 zongpeng li baochun li lap chi lau achieving maximum multicast throughput undirected networks ieeeacm transactions networking ton v14 nsi p24672485 june 2006 xiaolong li aaron striegel case passive application layer multicast computer networks international journal computer telecommunications networking v51 n11 p31573171 august 2007 chunchao yeh lin siong pui frame forwarding peertopeer multimedia streaming proceedings acm workshop advances peertopeer multimedia streaming november 1111 2005 hilton singapore miguel castro peter druschel annemarie kermarrec animesh nandi antony rowstron atul singh splitstream highbandwidth multicast cooperative environments proceedings nineteenth acm symposium operating systems principles october 1922 2003 bolton landing ny usa v kalogeraki zeinalipouryazti gunopulos delis distributed middleware architectures scalable media services journal network computer applications v30 n1 p209243 january 2007 eli brosh asaf levin yuval shavitt approximation heuristic algorithms minimumdelay applicationlayer multicast trees ieeeacm transactions networking ton v15 n2 p473484 april 2007 algorithms tradeoffs multicast service overlay design simulation v82 n6 p369381 june 2006 beichuan zhang wenjie wang sugih jamin daniel massey lixia zhang universal ip multicast delivery computer networks international journal computer telecommunications networking v50 n6 p781806 13 april 2006 mohammad obaidat guest editorial recent advances modeling simulation network systems simulation v82 n6 p365367 june 2006 minseok kwon sonia fahmy pathaware overlay multicast computer networks international journal computer telecommunications networking v47 n1 p2345 14 january 2005 junhong cui mario gerla framework realistic systematic multicast performance evaluation computer networks international journal computer telecommunications networking v50 n12 p20542070 24 august 2006 zhi li prasant mohapatra investigating overlay service topologies computer networks international journal computer telecommunications networking v51 n1 p5468 17 january 2007 mehran dowlatshahi farzad safaei system architecture mobility management mobile immersive communications advances multimedia v2007 n1 p55 january 2007 optimized video peertopeer multicast streaming proceedings acm workshop advances peertopeer multimedia streaming november 1111 2005 hilton singapore patrick th eugster pascal felber rachid guerraoui annemarie kermarrec many faces publishsubscribe acm computing surveys csur v35 n2 p114131 june k k jack b lee parallel overlays high datarate multicast data transfer computer networks international journal computer telecommunications networking v51 n1 p3142 17 january 2007 hao yin chuang lin feng qiu xuening liu dapeng wu truststream novel secure scalable media streaming architecture proceedings 13th annual acm international conference multimedia november 0611 2005 hilton singapore ananth rao ion stoica overlay mac layer 80211 networks proceedings 3rd international conference mobile systems applications services june 0608 2005 seattle washington h saito k taura chikayama collective operations widearea message passing systems using adaptive spanning trees proceedings 6th ieeeacm international workshop grid computing p4048 november 1314 2005 yongjun li james z wang cost analysis optimization ip multicast group management computer communications v30 n8 p17211730 june 2007 suman banerjee christopher kommareddy koushik kar bobby bhattacharjee samir khuller omni efficient overlay multicast infrastructure realtime applications computer networks international journal computer telecommunications networking v50 n6 p826841 13 april 2006 e w biersack carra r lo cigno p rodriguez p felber overlay architectures file distribution fundamental performance analysis homogeneous heterogeneous cases computer networks international journal computer telecommunications networking v51 n3 p901917 february 2007 chae lee ho dong kim reliable overlay multicast trees private internet broadcasting multiple sessions computers operations research v34 n9 p28492864 september 2007 tetsuya kusumoto yohei kunichika jiro katto sakae okubo treebased application layer multicast using proactive route maintenance implementation proceedings acm workshop advances peertopeer multimedia streaming november 1111 2005 hilton singapore yang guo kyoungwon suh jim kurose towsley p2cast peertopeer patching scheme vod service proceedings 12th international conference world wide web may 2024 2003 budapest hungary panayotis fouliras spiros xanthos nikolaos tsantalis athanasios manitsaris lemp lightweight efficient multicast protocol video demand proceedings 2004 acm symposium applied computing march 1417 2004 nicosia cyprus zongming fei mengkun yang proactive tree recovery mechanism resilient overlay multicast ieeeacm transactions networking ton v15 n1 p173186 february 2007 yi cui baochun li klara nahrstedt achieving optimized capacity utilization application overlay networks multiple competing sessions proceedings sixteenth annual acm symposium parallelism algorithms architectures june 2730 2004 barcelona spain yair amir claudiu danilov stuart goose david hedqvist andreas terzis 1800overlays using overlay networks improve voip quality proceedings international workshop network operating systems support digital audio video june 1314 2005 stevenson washington usa jianguang lou hua cai jiang li interactive multiview video delivery based ip multicast advances multimedia v2007 n1 p1313 january 2007 jiantao kong karsten schwan kstreams kernel support efficient data streaming proxy servers proceedings international workshop network operating systems support digital audio video june 1314 2005 stevenson washington usa yuval shavitt tomer tankel hyperbolic embedding internet graph distance estimation overlay construction ieeeacm transactions networking ton v16 n1 p2536 february 2008 praveen rao justin cappos varun khare bongki moon beichuan zhang net unified datacentric internet services proceedings 3rd usenix international workshop networking meets databases p16 april 10 2007 cambridge andrea passarella franca delmastro usability legacy p2p multicast multihop ad hoc networks experimental study eurasip journal wireless communications networking v2007 n1 p3838 january 2007 zhang eugene ng animesh nandi rudolf riedi peter druschel guohui wang measurement based analysis modeling synthesis internet delay space proceedings 6th acm sigcomm internet measurement october 2527 2006 rio de janeriro brazil glen maclarty michael fry towards platform widearea overlay network deployment management computer networks international journal computer telecommunications networking v51 n8 p21442162 june 2007