clustering approximate similarity search highdimensional spaces paper present clustering indexing paradigm called clindex highdimensional search spaces scheme designed approximate similarity searches one would like find many data points near target point one tolerate missing near points searches scheme find near points high recall ios perform significantly better approaches scheme based finding clusters building simple efficient index analyze tradeoffs involved clustering building index structure present extensive experimental results b introduction similarity search generated great deal interest lately applications similar textimage search documentimage copy detection applications characterize objects eg images text documents feature vectors highdimensional spaces 13 23 user submits query object search engine search engine returns objects similar query object degree similarity two objects measured distance function feature vectors search performed returning objects nearest query object highdimensional spaces nearestneighbor search inherently expensive especially large number dimensions first search space grow exponentially number dimensions second simply way build index disk nearest neighbors query point physically adjacent disk discuss curse dimensional ity detail section 2 fortunately many cases sufficient perform approximate search returns many nearest neighbors 2 17 15 27 29 30 feature vector often approximate characterization object already dealing approximations anyway instance contentbased image retrieval 11 19 40 document copy detection 9 13 20 usually acceptable miss small fraction target objects thus necessary pay high price exact search paper present new similaritysearch paradigm cluster ingindexing combined scheme achieves approximate similarity search high efficiency call approach clindex clustering index ing clindex dataset first partitioned similar clusters improve io efficiency cluster stored sequential file mapping table built indexing clusters answer query clusters near query point retrieved main memory clindex ranks objects retrieved clusters distances query object returns top say k objects result clustering indexing intensively researched survey related work section 2 two subjects studied separately different optimization objectives clustering optimizes classification accuracy indexing maximizes io efficiency information retrieval different goals indexing schemes often preserve clusters datasets randomly project objects close hence similar highdimensional spaces onto 2d plane disk geometry analogous breaking vase cluster apart fit minimum number small packing boxes disk blocks although space required store vase may reduced finding boxes highdimensional warehouse restore vase requires great deal effort study show 1 taking advantage clustering structures dataset 2 taking advantage sequential disk ios storing cluster sequential file achieve efficient approximate similarity search highdimensional spaces high accuracy examine variety clustering algorithms two different data sets show clindex works well 1 dataset grouped clusters 2 algorithm successfully find clusters part study also explore natural algorithm called forming cf achieves preprocessing cost linear dimensionality polynomial dataset size achieve query cost independent dimensionality contributions paper follows ffl study clustering indexing effectively combined show rather natural clustering scheme using grids 1 lead simple index performs well approximate similarity searches ffl experimentally evaluate clindex approach wellclustered 30 000 image database results show clindex achieves high recall data well divided clusters typically return 90 call golden results ie best results produced linear scan entire dataset ios ffl dataset natural clusters clustering may ef fective fortunately real datasets rarely occupy largedimensional space uniformly 4 15 38 46 48 experiment set 450 000 randomly crawled web images welldefined categories shows clindexs effectiveness depend quality clusters nev ertheless one willing trade accuracy efficiency clindex still attractive approach ffl also evaluate clindex different clustering algorithms eg tsvq compare clindex traditional approaches eg tree struc tures understand gains achievable preprocessing data find clusters ffl finally compare clindex pacnn 15 latest approach conducting approximate nearest neighbor search rest paper organized follows section 2 discusses related work section 3 presents clindex shows similarity query conducted using scheme section 4 presents results experiments compares efficiency accuracy approach traditional index structures finally offer conclusions discuss limitations clindex section 5 related work section discuss related work three categories 1 treelike index structures similarity search 2 approximate similarity search 3 indexing clustering 21 treelike index structures many tree structures proposed index highdimensional data eg r tree 3 24 sstree 45 srtree 28 tvtree 31 xtree 6 mtree 16 kdbtree 36 tree structure divides highdimensional space number subregions containing subset objects stored small number disk blocks given vector represents object similarity query takes following three steps systems 21 1 performs whereami search find subregion given vector resides 2 performs nearestneighbor search locate neighboring regions similar vectors may reside search often implemented using range search locates regions overlap search sphere ie sphere centered given vector diameter 3 finally computes distances eg euclidean streetblock l 1 distances vectors nearby regions obtained previous step given vector search result includes vectors within distance given vector performance bottleneck similarity queries lies first two steps first step index structure fit main memory search algorithm inefficient large portion index structure must fetched disk second step number neighboring subregions grow exponentially respect dimension feature vectors number dimensions number neighboring subregions order o3 21 roussopoulos et al 37 propose branchandbound algorithm hjaltason samet 25 propose priority queue scheme reduce search space large reduced number neighboring pages access still quite large berchtold et al 5 propose pyramid technique partitions high dimensional space 2d pyramids cuts pyramid slices parallel basis pyramid scheme perform satisfactorily data distribution skewed search hypercube touches boundary data space addition copious ios random hence exceedingly expensive 1 example illustrate call random placement syndrome faced traditional index structures figure 1a shows 2dimensional cartesian space divided 16 equal stripes vertical horizontal dimensions forming 16 theta 16 grid structure integer cell indicates many points objects cell index structures divide space subregions equal points topdown manner suppose disk block holds 40 objects one way divide space first divide three vertical compartments ie left middle right divide left compartment horizontally left four subregions b c containing number points given query object residing near border b similarity query retrieve blocks b number subregions check neighboring points grows exponentially transfer 100 kbytes data modern disk 8kbyte block size sequential io ten times faster random io performance gap widens size data transferred increases respect data dimension clustering indexing object object z object x b random placement figure 1 shortcomings tree structures furthermore since highdimensional spaces neighboring subregions cannot arranged manner sequential possible query ob jects ios must random figure 1b shows 2dimensional example random phenomenon grid figure b represents subregion corresponding disk block figure shows three possible query objects x z suppose neighboring blocks query object four surrounding blocks instance blocks b e four neighboring blocks object x neighboring blocks objects x contiguous disk order must cfebad fcbeda reverse orders impossible store neighboring blocks query object z contiguously disk query must suffer random ios example suggests highdimensional spaces neighboring blocks given object must dispersed randomly disk tree structures many theoretical papers eg 2 27 29 studied cost exact search independent data structure used particular papers show n size dataset dimension log n nearestneighbor algorithm significantly faster linear search 22 approximate similarity search many studies propose conducting approximate similarity search applications trading small percentage recall faster search speed acceptable example instead searching neighboring blocks query object study 44 proposes performing whereami step similarity query returning objects disk block query object resides however approach may miss objects similar query object take figure 1a example suppose query object circled cell figure near border regions b return objects region query object resides miss many nearest neighbors arya mount 2 suggest approximate nearestneighbor searches denote function computing distance two points say approximate nearest neighbor q studies attempted devise better algorithms reduce search time storage requirements example indyk motwani 27 kushilevitz et al 30 give algorithms polynomial storage query time polynomial logn indyk motwani 27 also give another algorithm smaller storage requirements sublinear query time work however theoretical practical scheme implemented localitysensitive hashing scheme proposed indyk motwani 27 key idea use hash functions probability collision much higher objects close far apart approximate search also applied treelike structures recent work 15 shows one tolerate 0 relative error ffi confidence factor one improve performance mtree 12 orders magnitude although approximate nearestneighbor search reduce search space significantly recall low candidate space sampling nearest neighbor becomes exponentially larger optimal search space remedy problem followup study 27 builds multiple localitypreserving indexes dataset 26 analogous building n tree indexes dataset index distributes data data blocks differently answer query one retrieves one block following indexes combines results approach achieves better recall achieved one index addition n times preprocessing overhead replicate data times ensure sequential ios possible via every index 23 clustering indexing clustering clustering techniques studied statistics machine learning database communities work different communities focuses different aspects clustering instance recent work database community includes clarans 35 birch 48 dbscan 18 clique 1 waveclusters 39 techniques high degree success identifying clusters large dataset deal efficiency data search data retrieval late clustering techniques explored efficient indexing highdimensional spaces choubey chen rundensteiner 14 propose preprocessing data using clustering schemes bulkloading clusters rtree show bulkloading scheme hurt retrieval efficiency compared inserting tuples one time bulkloading scheme speeds inseration operation designed tackle search efficiency problem study focuses improving search efficiency presented rime system built 13 detecting replicated images using early version clindex 12 paper extend early work handling approximate similarity search conduct extensive experiments compare approach others bennett fayyad geiger 4 propose using em expecta tion maximization algorithm cluster data efficient data retrieval highdimensional spaces quality em clusters however depends heavily initial setting model parameters number clus ters data distribution follow gaussian distribution number clusters gaussian parameters initialized properly quality resulting clusters suffers initial parameters selected still open research problem 7 8 example difficult know many clusters exist em run em algorithm needs number cluster effectively also em algorithm may take many iterations converge may converge one many local optima 7 32 clustering algorithm clindex uses paper make assumption number clusters distribution data instead use bottomup approach groups objects adjacent space cluster similar grouping stars galaxias therefore cluster shape believe data uniformly distributed approach preserve natural shapes clusters addition clindex treats outliers differently placing separate files outlier close cluster similar objects clusters however outlier close outliers placing outliers separately helps search similar outliers efficiently 3 clindex algorithm since traditional approaches suffer large number random ios design objectives 1 reduce number ios 2 make ios sequential much possible accomplish objectives propose clusteringindexing approach call scheme clindex clustering indexing focus approach similar data disk minimize disk latency retrieving similar objects ffl build index clusters minimize cost looking cluster couple indexing clustering propose using common structure divides space grids show shortly grids use clustering used indexing direction insertdelete operation indexing structure cause regional reclustering grids clindex consists following steps 1 divides th dimension 2 stripes words dimension chooses points divide dimension describe adaptively choose dividing points section 332 stripes turn define cells dimension search space way using small number bits bits dimension encode cell feature vector belongs 2 groups cells clusters cell smallest building block constructing clusters different shapes similar idea calculus using small rectangles approximate polynomial functions degree finer stripes smaller cells finer building blocks approximate shapes clusters cluster stored sequential file disk 3 builds index structure refer clusters cell smallest addressing unit simple encoding scheme map object cell id retrieve cluster belongs one io remainder section divided four topics section 31 describes clindex works clustering phase indexing section 32 depicts structure built clindex index clusters tuning section 33 identifies tunable control parameters explains changing parameters affect clidnex performance finally show similar query conducted using clindex section 34 31 cf algorithm clustering cells perform efficient clustering highdimensional spaces use algorithm called clusterforming cf illustrate procedure figure 2a shows points distributed 2d evenlydivided grid cf algorithm works following way 1 cf first tallies height number objects cell 2 cf starts cells highest point concentration cells peaks initial clusters example figure 2a start cells marked 7 3 cf descends one unit height cells current height processed height cell one three conditions adjacent cluster adjacent one cluster adjacent one cluster corresponding actions cf takes cell adjacent cluster cell seed new cluster b cell adjacent one cluster join cell cluster c cell adjacent one cluster cf algorithm invokes algorithm called cliffcutting cc determine clustering b clustering figure 2 grid cf cluster cell belongs clusters combined 4 cf terminates height drops threshold called horizon cells belong cluster ie horizon grouped outlier cluster stored one sequential file figure 2b shows result applying cf algorithm data presented figure 2a contrast traditional indexing schemes split data shown figure 1a clusters figure 2b follow call natural clusters dataset formal description cf algorithm presented figure 3 input cf includes dimension number bits needed encode dimension threshold terminate clustering algorithm dataset p output consists set clusters phi heap structure h sorted cell id indexing clusters cell empty allocate structure c records cell cluster forming algorithm ffl input ffl output ffl phi cluster set ffl variables ffl execution steps 0 init 1 cell id 12 c heapfindh else new c cid 2 fcjc 2 hg temp array holding copy cells 3 sorts sort cells descending order cp 5 c 6 nil cp 51 psi findneighborclusterss cid psi contains cell cs neighboring clusters adjacent cluster new fi fi holds new cluster structure insert new cluster cluster set update cluster cell belongs else jpsij 1 cell adjacent one one cluster described section 312 new fi fic 0 group remaining cells outlier cluster 7 phi phi ffig 8 figure 3 cluster forming cf algorithm id cid number points cell cp cluster cell belongs cfi cells inserted heap cf twopass algorithm initialization step step 0 first pass step 1 tallies number points cell point p data set p maps point cell id calling function cell function cell divides dimension 2 regions value feature vector replaced integer 0 depends value falls range dimension quantized feature vector cell id object cf algorithm checks whether cell exists heap calling procedure heapfind found standard algorithm book cell exists algorithm increments point count cell otherwise allocates new cell structure sets point count one inserts new cell heap calling procedure heapinsert also standard textbooks second pass cf algorithm clusters cells step 2 figure cf copies cells heap temporary array steps 3 4 sorts cells array descending order point count cp step 5 algorithm checks cell adjacent existing clusters starting cell greatest height termination threshold cell adjacent existing cluster new cluster fi formed step 52a cf algorithm records centroid cell new cluster fic inserts cluster cluster set phi cell adjacent one cluster algorithm calls procedure cc cliff cutting step 52b determine cluster cell join procedure cc described section 312 cell joins identified new existing cluster finally steps 6 8 cells threshold grouped one cluster outliers 311 time complexity analyze time complexity cf algorithm let n denote number objects dataset number nonempty cells first takes od compute cell id object takes od time check whether two cells adjacent first pass cf algorithm use heap keep track cell ids heights given cell id takes od theta log time locate cell heap therefore time complexity first phase theta theta logm second pass time find neighboring cells mg reason three neighboring stripes given cell dimension either search neighboring cells 3 gamma 1 search nonempty cells highdimensional space 2 believe 3 therefore time complexity second phase od theta 2 total time complexity theta theta log od theta 2 since clustering phase done offline preprocessing time may acceptable light speed gained query performance 312 procedure cc cliffcutting cc procedure need decide cluster cell belongs many heuristics exist choose following two rules 1 new object adjacent one cluster add cluster 2 new object adjacent one cluster ffl merge clusters adjacent cell one cluster ffl insert cell cluster note rules may produce large clusters one avoid large clusters increasing either discuss sections 331 332 32 indexing structure second step cf algorithm indexing structure built support fast access clusters generated shown figure 4 indexing structure includes two parts cluster directory mapping table cluster directory keeps track information clusters mapping table maps cell cluster cell resides objects cluster stored sequential blocks disk objects retrieved efficient sequential ios cluster directory records information clusters cluster id name file stores cluster flag indicating whether cluster outlier cluster clusters centroid center cells cluster also stored information image databases example many image databases use feature vectors 100 dimensions clearly number images image database much smaller 3 100 mapping table cluster directory cell id cluster id00000011000001111111100162826 filenamefile158 figure 4 index structure cluster directory objects cluster retrieved disk know id cluster mapping table used support fast lookup cell cluster cell resides entry two values cell id cluster id number entries mapping table number nonempty cells empty cells take space worst case one cell object n cell structures id cell binary code size theta bits dimension number bits choose dimension suppose cluster id represented twobyte integer total storage requirement mapping table theta theta 8 2 bytes worst case total storage requirement mapping table order theta disk storage requirement mapping table comparable interior nodes tree index structure note cell ids sorted stored sequentially disk given cell id easily search corresponding entry binary search therefore number ios look cluster olog comparable cost whereami search tree index structure 33 control parameters granularity clusters controlled four parameters ffl number data dimensions object features ffl number bits used encode dimension number objects ffl horizon number cells determined parameters written 2 dtheta average number objects cell n 2 dtheta means dealing two conflicting objectives one hand want low point density low point density results large number cells relatively small number points cell hence tends create many small clusters similarity search want sufficient number points present good choice requester say least seven points cluster 33 hand want denselypopulated cells either since high point density results small number large clusters cannot help us tell objects apart cell id objects figure 5 clustering example 331 selecting value affects number size clusters figure 5 shows example onedimensional space horizontal axis cell ids vertical axis number points cells value set level threshold forms four clusters cells whose heights clustered outlier cluster threshold reduced cluster number cluster size may change size outlier cluster decreases outlier cluster relatively small number objects fit sequential disk blocks improve io efficiency hand might good outlier cluster relatively large keep clusters well separated selection proper value quite straightforward indirect way first one decides percentage data objects outliers add 5 th step cf algorithm figure 3 termination condition places remaining data outlier cluster number remaining data objects drops threshold 332 selecting due uneven distribution objects possible regions feature space sparsely populated others densely populated handle situation need able perform adaptive clustering suppose divide dimension 2 stripes divide regions points smaller substripes way may avoid large clusters desirable way approach similar extensible hashing scheme buckets many points extensible hashing scheme splits buckets case build clusters adaptively different resolutions choosing dividing points carefully based data distribution example image feature vectors since luminescence neither high low pictures makes sense divide luminescence spectrum coarsely two extremes finely middle adaptive clustering step done step 11 figure 3 cell procedure quantizes value dimension take statistical distribution dataset dimension consideration step however requires additional data analysis pass execute cf algorithm cell procedure information determine quantize dimension properly summarize cf bottomup clustering algorithm approximate cluster boundaries fine detail adaptive data distributions since cluster stored contiguously disk cluster accessed much efficient ios traditional indexstructure methods 34 similarity search given query object similarity search performed two steps first clindex maps query objects feature vector binary code id cell object resides looks mapping table find entry cell search takes form different actions depending whether cell found ffl cell found obtain cluster id cell belongs find file name cluster stored cluster directory read cluster disk memory ffl cell found cell must empty find cluster closest feature vector computing comparing distances centroids clusters query object read nearest cluster main memory high recall desirable read nearby clusters read candidate objects nearby clusters main memory sort according distances query object return nearest objects user remarks search return one cluster whose centroid close query object checking cluster directory since number clusters much smaller number objects dataset search nearest clusters likely done via inmemory lookup number clusters large one may consider treating cluster centroids points apply clustering algorithms centroids way one build hierarchy clusters narrow search space clusters group clusters nearest query object another alternative precompute knearest clusters store ids cluster show section 4 returning top clusters achieve high recall little time example x f figure figure 6 shows 2d example similarity search carried figure b c e five clusters areas covered clusters grouped outlier cluster suppose user submits x query object since cell object x belongs cluster return objects cluster sort according distances x high recall required return clusters case since clusters b c nearby also retrieve objects two clusters objects clusters b c ranked based distances object x nearest objects returned user query object falls outlier cluster first retrieve outlier cluster definition outlier cluster stores outliers hence number points outlier close small also find two closest clusters e return nearest objects two clusters evaluation experiments focused queries form find top k similar objects k nearest neighbors abbreviated knn knn query return top k nearest neighbors query object establish benchmark measure query performance scanned entire dataset query object find top 20 nearest neighbors golden results least three metrics interest measure query result recall x ios x ios performed fraction k top golden results retrieved b precision x ios x ios fraction objects retrieved among top k golden results c precision r top k golden objects found call rprecision quantifies much useful work done obtaining fraction golden results paper focus recall results comment briefly rprecision relative distance errors 47 environment believe precision useful metric since main overhead ios number nongolden objects seen performed experiments two sets images 30 000 450 000 image set first set contains 30 000 images corel cds extensively used testbed contentbased image retrieval systems computer vision image processing communities 30 000image dataset consists images different categories landscapes portraits buildings dataset set 450 000 randomlycrawled images internet 3 450 000image dataset variety content ranging sports cartoons clip arts etc difficult classify even manually characterize images converted image 48dimensional feature vector applying daubechies wavelet transformation 13 addition using cf clustering algorithm indexed feature vectors using five schemes equal partition ep treestructure vector quantization tsvq 22 r tree srtree mtree pacnn scheme 15 ffl ep understand role clustering plays clindex devised simple scheme ep partitions dataset sequential files without performing clustering partitioned dataset cells equal number images cell occupies contiguous region space stored sequential file since ep similar clindex cf except clustering performance differences schemes must due clustering 3 readers encouraged experiment prototype made available online 10 experimenting new approaches image characterization prototype may change time time differences significant mean dataset uniformly distributed clindex cf exploit clusters ffl tsvq evaluate effectiveness clindexs cf clustering algorithm replaced sophisticated algorithm stored clusters sequentially usual replacement algorithm used tsvq kmean algorithm 34 implemented clustering data highdimensional spaces widely used data compression lately indexing highdimensional data contentbased image retrieval 41 42 ffl r tree srtree tree structures often used similarity searches multidimensional spaces compare used r tree srtree structures implemented katayama satoh 28 studied io cost recall using treelike structures perform similarity search approximately note comparison clindex treelike structures fair since neither r tree srtree performs offline analysis ie clustering done advance thus results useful quantify potential gains offline analysis gives us compared traditional treebased similarity search ffl mtree pacnn also compare clindex ciaccia patellas pacnn implementation mtree 15 pacnn scheme uses two parameters ffl ffi adjust tradeoff search speed search accuracy accuracy ffl allows certain relative error results confidence ffi guarantees probability least ffl exceeded since pacnn implementation use supports 1nn search performed 20 1nn queries get 20 nearest neighbors 1 nn query nearest neighbor found removed mtree 20 th 1nn query 20 nearest neighbors removed dataset compared golden set compute recall test procedure reliable measuring recall pacnn however test procedure alters search cost well known cost exact 20nn query 20 times cost 1nn query much less approximate queries seems also case ensure fair comparison present pacnn experimental results separately section 42 discussed section 34 clindex always retrieves cluster whose centroid closest query object also added intelligence tsvq ep improve recall r tree srtree ever add optimization 4 measure recall used crossvalidation technique 34 commonly employed evaluate clustering algorithms ran test ten times time set aside 100 images test images used remaining images build indexes used setaside images query database built four different index structures produced average query recall run averaging recall 100 image queries took average 10 runs obtain final recall experiments intended answer following questions 1 clustering affect recall approximate search section 41 perform different algorithms two datasets different clustering quality find effects clustering 2 performance clindex compared pacnn scheme section 42 show pacnn scheme trades accuracy speed mtree structure 3 performance clindex terms elapsed time compared traditional structures compared sequentially reading entire file section 43 examine compare long five schemes take achieve target recall 4 block size affect recall section 44 cluster 30 000 image set using different block sizes answer question experiments collected recall 20nn queries section 45 present recalls knn 41 recall versus clustering algorithms figures 7 8 compare recalls cf tsvq ep r tree srtree discuss experimental results two data sets separately ffl 30 000image dataset figure 7 experiment schemes divided dataset 256 clusters given one io perform cf returns objects nearest cluster gives us average 62 recall ie return 62 top 20 golden results read three clusters accumulated recall cf increases 90 4 scheme like r tree srtree could modified preanalyze data build better structure results would presumably similar results ep keeps centroid information data block assist effective nearest neighbor search develop modified r tree srtree scheme hence verify hypothesis recall number ios clindex cf r tree sr tree figure 7 recalls five schemes 20nn 30 000 images read clusters recall still increases much slower pace read 15 clusters recall approaches 100 selectively read 6 15 6 data dataset obtain almost top 20 golden results ep scheme obtains much lower recall cf starts 30 recall first io completed slowly progresses 83 ios tsvq structure although better ep scheme still lags behind cf obtains 35 recall one io recall reach 90 10 ios recall cf tsvq converge 20 ios finally r tree srtree suffer lowest recall srtree performs better r tree consistent results reported 28 ffl 450 000image dataset figure 8 experiment schemes divided dataset 1 500 clusters given one io perform cf returns objects nearest cluster gives us average 25 recall read 10 clusters accumulated recall cf increases 60 read 90 clusters recall approaches 90 selectively read 6 90 6 data dataset obtain 90 top 20 golden results expected achieved recall image dataset good 30 000image set believe randomly crawled images may form recall number ios clindex cf r tree sr tree figure 8 recalls five schemes 20nn 450 000 images clusters well separated feature space clusters 30 000 image set nevertheless cases recall versus percentage data retrieved data set shows cf achieve good recall retrieving small percentage data ep scheme achieves much lower recall cf starts 1 recall first io completed slowly progresses 33 100 ios tsvq structure performs worse ep scheme achieves 30 recall 100 ios finally r tree srtree suffer lowest recall data display good clustering quality poor clustering algorithm exacerbates problem remarks using recall alone cannot entirely tell quality approximate results instance given recall say 60 approximate results different terms distances query point fairly similar query figure 9 uses relative distance error metric proposed 47 report quality approximation let q denote query point feature space k g average distance topk golden results q feature space k average distance topk actual results q relative distance error measured g g relative distance errors number ios clindex cf rsr tree figure 9 relative distance errors 20nn 30 000 images figure 9 shows relative distance errors clindex tsvq kept 20 ten ios relative distance errors treestructures still quite large 80 even ios 42 pacnn scheme conducted pacnn experiments 30 000image dataset observed value ffi plays important role value ffl determining search quality cost table 1 shows one representative set results set setting ffl values change tradeoff pattern search accuracy cost five different settings 0 001 002 005 01 table shows recall number ios wallclock query time 20 1nn queries mentioned previously cost 20 1nn queries substantially higher cost one 20nn query therefore one treat number ios elapsed time table loose bounds cost ffi increased zero 001 recall pacnn drops 953 40 number ios reduced substantially one eighth wallclock elapsed time reduced one fifth increase ffi recall cost continue drop remarks ffl quality approximation perspective performance metrics proposed measure quality approximation 47 pacnn scheme trades slight accuracy substantial search speedup multimedia applications one may care getting similar results finding exact topnn results pacnn scheme provides scalable effective way trade accuracy speed ffl io cost since pacnn scheme experimented implemented mtree treelike structure tends distribute nature cluster many noncontiguous disk blocks discussed problem section 2 takes mtree ios clindex achieve given recall observation consistent results obtained using treelike structures index images construction time took around 15 minutes build mtree building clindex cf takes 30 minutes 600mhz pentiumiii workstation 512 mbytes dram explained section 3 since clustering phase done offline preprocessing time may acceptable light speed gained query performance recall 953 400 253 122 59 number ios 13 050 wallclock time seconds 15 3 2 16 13 table 1 performance cost pacnn 43 running time versus recall shown clustering indeed helps improve recall dataset uniformly distributed space shown recall gap cf ep figures 7 8 also shown retrieving small fraction data cf achieve recall quite high eg 90 section shows time needed schemes achieve target recall compare elapsed time time takes sequentially read entire dataset report query time first use quantitative model compute io time since io time dominates query time model helps explain one scheme faster others compare wallclock time obtained computed io time show shortly two methods give us different elapsed times relative performance indexes let b denote total amount data retrieved achieve target recall n number ios tr transfer rate disk seek average disk latency elapsed time search estimated compute use parameters modern disk listed table 2 simplify computation assume io incurs one average seek 89 ms one half rotational delay 56 ms also assume tr 130 mbps image record including wavelets url address takes 500 bytes example compute elapsed time retrieve 4 clusters contains 10 30 000 images express 30 000 theta 10 theta 500 theta 8 ms figure 10a shows average elapsed time versus recall five schemes 30 000image dataset note average elapsed time estimated based number records retrieved ten rounds ex periments based number clusters retrieved horizontal line middle figure shows time takes read entire image feature file sequentially achieve 90 recall cf tsvq take substantially less time reading entire file traditional tree structures hand perform worse sequential scan would like achieve recall 65 figure 10b shows wallclock elapsed time versus recall collected dell workstation configured 600mhz pentiumiii processor 512 mbytes dram eliminate effect caching ran ten queries index structure rebooted system parameter name value disk capacity 26 gbytes number cylinders cyl 50298 min transfer rate tr 130 mbps rotational speed rpm 5400 full rotational latency time 1112 ms min seek time 20 ms average seek time 89 ms max seek time ms table 2 quantum fireball lct disk parameters query average running time schemes twice long predicted io time believe wallclock time includes index lookup time distance computation time io bus time memory access time addition io time relative performance indexes however unchanged figures clindex cf requires lowest running time reach every recall target short draw following conclusions 1 clindex cf achieves higher recall tsvq ep r tree cf adaptive wide variances cluster shapes sizes tsvq algorithms forced bound cluster linear functions eg cube 3d space cf constrained way thus odd cluster shape eg tentacles cf conform shape tsvq either pick big space encompasses tentacles select several clusters portion shape thereby inadvertently break natural clusters illustrated figure 1a surprising recall tsvq converges cf number ios tsvq eventually pieces together broken clusters 2 using cf approximate exact similarity search requires reading fraction entire dataset performance far better achieved performing sequential scan entire dataset 3 cf tsvq ep enjoy boost recall first additional io time recall clindex cf rsr tree read io time0515253510 20 wallclock time recall clindex cf rsr tree sequential b wallclock time figure 10 elapsed time vs recall 30 000 images ios since always retrieve next cluster whose centroid closest query object r tree conversely store centroid information cannot selectively retrieve blocks boost recall first ios discussed shortcomings tree structures section 2 detail 4 data set exhibit good clusters cfs effectiveness suffer poor clustering algorithm tsvq ep suffer even thus employing good clustering algorithm important clindex work well recall 60 70 80 90 100 golden objects retrieved 12 14 cf objects retrieved 115 230 345 460 rprecision 1044 609 464 391 116 r tree objects retrieved 2 670 3 430 4 090 4 750 5 310 rprecision 045 041 039 0379 0377 table 3 rprecision 20nn remarks precision figure 7 shows retrieving amount data cf higher recall r tree schemes put another way cf also enjoys higher rprecision retrieving number objects disk gives cf golden results since tree structure typically designed use small block size achieve high precision tested r tree method larger block size compared rprecision cf conducted experiment 30 000image dataset treelike structures ineffective 450 000image dataset took r tree 267 ios average complete exact similarity search table 3 shows rprecision cf ten times higher r tree different recall values result although surprising consistent many studies 28 43 common finding data dimension high tree structures fail divide points neighborhoods forced access almost leaves exact similarity search therefore argument using small block size improve precision becomes weaker data dimension increases 44 recall versus cluster size since tsvq ep treelike structures ineffective 450 000 image dataset conducted remaining experiments 30 000image dataset define cluster size average number objects cluster see effects cluster size recall collected recall values different cluster sizes cf tsvq cf r tree note cluster size cf determined selected values thus set four different combinations obtain recall values four different cluster sizes ep tsvq r tree selected cluster block sizes contain 50 900 objects figure 11 depicts recall yaxis four schemes different cluster sizes xaxis one io five ios performed figure 11a shows given cluster size cf higher recall tsvq ep r tree gaps clustering schemes ep r tree nonclustering schemes tsvq cf widen cluster size increases believe larger cluster size important role quality clustering algorithm plays cf enjoys l size objects clindex rtree one io02061 recall size objects clindex rtree b five ios figure 11 recall versus cluster size significantly higher recall tsvq ep r tree captures clusters better three schemes figure 11b shows cf still outperforms tsvq ep r tree five ios cf tsvq ep enjoy better improvement recall r tree cf tsvq ep always retrieve next cluster whose centroid nearest query object hand tree structure keep centroid information search neighborhood random thus suboptimal believe adding centroid information leaves tree structures improve recall 45 relative recall versus knn far measured recall returning top 20 nearest neigh bors section present recall top nearest neighbors returned one five ios performed ex periments partitioned dataset 256 clusters blocks schemes ie schemes cluster size figures 12a figures 12b present relative recall relative top 20 golden results four schemes one io five ios l top knn clindex cf rtree one io02061 recall top knn clindex cf rtree b five ios figure 12 recall knn performed respectively xaxis figures represents number nearest neighbors requested yaxis represents relative recall instance one nearest object requested cf returns nearest object 79 time one io 95 time five ios figures show recall gaps schemes insensitive many nearest neighbors requested believe nearest object found returned clusters conditional probability additional nearest neighbors also found retrieved clus high course conclusion hold number objects contained cluster must larger number nearest neighbors requested experiments cluster contains 115 objects average tested 20 golden results leads us following discussion tune cfs control parameters form clusters proper size 46 effects control parameters one clindexs drawbacks control parameters must tuned obtain quality clusters experiment different values form clusters show selected 30 000image dataset since size dataset much smaller number cells dtheta many cells occupied one object set 1 points fell outlier cluster set zero test values started 2 increased increments one checked effect recall figure 13 shows recall respect number ios decreases set beyond two dataset log n using large spreads objects apart thereby leads formation many small clusters dividing dataset finely one loses benefit large block size sequential ios02061 recall ios figure 13 recall versus although proper values dataset dependent empirically found following rules thumb useful finding good starting values ffl choose reasonable cluster size cluster must large enough take advantage sequential ios must also contain enough objects query object near cluster probability significant number nearest neighbors query object cluster high hand cluster large prolong query time unnecessarily according experiments increasing cluster size cluster contains 300 objects see figure 11 improve recall significantly therefore cluster 300 objects reasonable size ffl determine desired number clusters cluster size chosen one compute many clusters dataset divided number clusters large make cluster table fit main memory may either increase cluster size decrease number clusters consider building additional layer clusters adjust desired cluster size determined pick starting values value must least two points separated suggested one clusters separated running clustering algorithm initial setting average cluster smaller desired size set zero combine small clusters average cluster size larger desired size increase either obtain roughly desired cluster size 47 summary summary experimental results show 1 employing better clustering algorithm improves recall well elapsed time achieve target recall 2 providing additional information centroids clusters helps prioritize retrieval order clusters hence improves search efficiency 3 using large block size good io efficiency recall experimental results also show clindex effective uses good clustering algorithm data uniformly distributed dataset uniformly distributed highdimensional space sequentially reading entire dataset effective using clindex conclusions paper presented clindex paradigm performing approximate similarity search highdimensional spaces avoid dimensionality curse clindex clusters similar objects disk performs similarity search finding clusters near query object approach improves io efficiency clustering retrieving relevant information sequentially disk preprocessing cost linear dimensionality dataset polynomial n size dataset quadratic number nonempty cells query cost independent believe many highdimensional datasets eg documents images exhibit clustering patterns clindex attractive approach support approximate similarity search efficiently effectively experiments showed clindex typically achieve 90 recall retrieving small fraction data using cf algorithm clindexs recall much higher traditional index structures experiments also learned clindex works well uses large blocks finds clusters effectively searches neighborhood intelligently using centroid information design principles also employed schemes improve search performance example one may replace cf clustering algorithm paper one suitable particular dataset finally summarize limitations clindex future research plan ffl control parameter tuning discussed section 33 determining value straightforward determining good value requires experiments dataset provided parametertuning guidelines paper plan investigate mathematical model allows directly determine parameters regard believe pacnn 15 approach helpful ffl effective clustering clindex needs good clustering algorithm ef fective need investigate pros cons using existing clustering algorithms indexing highdimensional data ffl incremental clustering addition clustering algorithms offline algorithms clusters sensitive insertions deletions plan extend clindex perform regional reclustering supporting insertions deletions initial clusters formed ffl measuring performance using metrics study use classical precision recall measure performance similarity search plan employ metrics eg 47 compare performance different indexing schemes acknowledgments would like thank james ze wang comments draft help tsvq experiments would like thank kingshy gho marco patella assistance complete pacnn experiments mtree structure would also like thank tkde associate editor paolo ciaccia reviewers helpful comments original version paper r automatic subspace clustering high dimensional data data mining applications optimal algorithm approximate nearest neighbor searching fixed dimensions pyramidtechnique towards breaking curse dimensionality neural networks pattern recognition scaling em expectation maximization clustering large databases copy detection mechanisms digital docu ments toward perceptionbased image retrieval extended version searching near replicas images via clustering geberalized rtree bulkinsertion pac nearest neighbor queries approximate controlled search highdimensional metric spaces mtree efficient access method similarity search metric spaces algorithm approximate closestpoint queries densitybased algorithm discovering clusters large spatial databases noise query image video content qbic system safeguarding charging information internet database system implementa tion vector quantization signal compression visual information retrieval rtrees dynamic index structure spatial searching ranking spatial databases similarity search high dimensions via hashing approximate nearest neighbors towards removing curse dimensionality two algorithms nearestneighbor search high dimen sions efficient search approximate nearest neighbor high dimensional spaces em algorithm magical number seven machine learning efficient effective clustering methods spatial data mining nearest neighbor queries adaptive colorimage embedding database navigation multiresolution clustering approach large spatial databases fully automated contentbased image query system quantitative analysis performance study similaritysearch methods highdimensional spaces similarity indexing algorithms performance similarity indexing sstree database design second edition approximate similarity retrieval mtrees efficient data clustering method large databases tr ctr edward chang kwangting cheng lihyuarn l chang pbir perceptionbased image retrieval acm sigmod record v30 n2 p613 june 2001 jessica lin eamonn keogh stefano lonardi jeffrey p lankford donna nystrom visually mining monitoring massive time series proceedings tenth acm sigkdd international conference knowledge discovery data mining august 2225 2004 seattle wa usa edward chang kwangting cheng weicheng lai chingtung wu chengwei chang yileh wu pbir perceptionbased image retrievala system quickly capture subjective image query concepts proceedings ninth acm international conference multimedia september 30october 05 2001 ottawa canada weicheng lai kingshy goh edward chang scalability active learning formulating query concepts proceedings 1st international workshop computer vision meets databases june 1313 2004 paris france kingshy goh edward chang weicheng lai multimodal conceptdependent active learning image retrieval proceedings 12th annual acm international conference multimedia october 1016 2004 new york ny usa anicet kouomou choupo laure bertiquille annie morin optimizing progressive querybyexample preclustered large image databases proceedings 2nd international workshop computer vision meets databases june 1717 2005 baltimore md lijuan zhang alexander thomasian persistent clustered main memory index accelerating knn queries high dimensional datasets proceedings 2nd international workshop computer vision meets databases june 1717 2005 baltimore md sidahmed berrani laurent amsaleg patrick gros approximate searches kneighbors precision proceedings twelfth international conference information knowledge management november 0308 2003 new orleans la usa laurent amsaleg patrick gros sidahmed berrani robust object recognition images related database problems multimedia tools applications v23 n3 p221235 august 2004 nick koudas beng chin ooi kianlee tan rui zhang approximate nn queries streams guaranteed errorperformance bounds proceedings thirtieth international conference large data bases p804815 august 31september 03 2004 toronto canada simon tong edward chang support vector machine active learning image retrieval proceedings ninth acm international conference multimedia september 30october 05 2001 ottawa canada kingshy goh beitao li edward chang dyndex dynamic nonmetric space indexer proceedings tenth acm international conference multimedia december 0106 2002 juanlespins france edward chang beitao li megathe maximizing expected generalization algorithm learning complex query concepts acm transactions information systems tois v21 n4 p347382 october jessica lin eamonn keogh stefano lonardi visualizing discovering nontrivial patterns large time series databases information visualization v4 n2 p6182 july 2005 vassilis athitsos marios hadjieleftheriou george kollios stan sclaroff querysensitive embeddings proceedings 2005 acm sigmod international conference management data june 1416 2005 baltimore maryland keke chen ling liu clustermap labeling clusters large datasets via visualization proceedings thirteenth acm international conference information knowledge management november 0813 2004 washington dc usa vassilis athitsos marios hadjieleftheriou george kollios stan sclaroff querysensitive embeddings acm transactions database systems tods v32 n2 p8es june 2007 ertem tuncel hakan ferhatosmanoglu kenneth rose vqindex index structure similarity searching multimedia databases proceedings tenth acm international conference multimedia december 0106 2002 juanlespins france keke chen ling liu ivibrate interactive visualizationbased framework clustering large datasets acm transactions information systems tois v24 n2 p245294 april 2006 domenico cantone alfredo ferro alfredo pulvirenti diego reforgiato recupero dennis shasha antipole tree indexing support range search knearest neighbor search metric spaces ieee transactions knowledge data engineering v17 n4 p535550 april 2005