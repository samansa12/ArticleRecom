billiards related systems bulksynchronous parallel model two examples show suitability bulksynchronous parallel bsp model discreteevent simulation homogeneous largescale systems model provides unifying approach general purpose parallel computing addition efficient scalable computation ensures portability across different parallel architectures valuable feature approach simple cost model enables precise performance prediction bsp algorithms show theoretically empirically systems uniform event occurrence among components colliding hardspheres isingspin models efficiently simulated practice current parallel computers supporting bsp model b introduction parallel discreteevent simulation billiards related systems considered nonobvious algorithmic problem deserved attention literature 1 5 7 8 9 11 13 18 24 23 25 currently important class applications simulations computational physics 6 7 10 14 15 20 21 eg hardparticle fluids isingspin models disk packing problems however kind systems viewed general setting many moving objects 3 16 present everywhere real life eg big cities transport problems navigation systems computer games combat models hand systems considered sufficiently general computationally intensive enough used sort benchmark time warp simulation 5 23 25 whereas different simulation techniques shown efficient dealing large systems 8 9 11 13 18 similar parallel software development last decades prevalent approach simulation systems followed machine dependent exploitation inherent parallelism associated problem currently however one greatest challenges parallel computing establish solid foundation guide rapid process convergence observed field parallel computer systems enable architecture independent software developed emerging range scalable parallel systems 17 bulk synchronous parallel bsp model proposed provide foundation 22 wide range ap plications model already shown successful bridging role ie bridge hardware software direct analogy role played von neumman model sequential computing last fifty years present bsp model implemented different parallel architectures shared memory multiprocessors distributed memory systems networks workstations enabling portable efficient scalable parallel software developed machines 19 4 first step bsp implementation conservative optimistic parallel simulation algorithms far given 12 paper follow different approach using conservative algorithms designed purely bsp concepts evaluating performance two examples isingspin model hardparticle fluid note potentially largescale systems property random even distribution events among constituent elements believe however two examples exhibit sufficient generality complexity representative wide range related asynchronous systems eg instances multipleloop networks described 8 systems mentioned note synchronous nature bsp model algorithms reminiscent proposed 8 18 2 bsp model detailed description bsp model reader referred 22 17 bulksynchronous parallel bsp computer consists set processormemory pairs ii communication network delivers messages pointtopoint man ner iii mechanism efficient barrier synchronization subset processors specialized broadcasting combining facilities define time step time required single local operation ie basic operation addition multiplication locally held data values performance bsp computer characterized following four parameters p number processors ii processor speed ie number time step per second iii l synchronization periodicity ie minimal number time steps network l g ring op op 2d array p p p p butterfly ologp ologp hypercube ologp o1 table 1 bsp parameters parallel computers elapsed two successive barrier synchronizations processors iv g ratio total number local operations performed processors one second total number words delivered communication network one second ie g normalized measure time steps required sendreceive oneword message situation continuous traffic communication network see table 1 taken 17 shows bounds values g l different communication networks bsp computer operates following way computation consists sequence parallel super steps superstep sequence steps followed barrier synchronization processors point remote memory accesses takes effect superstep processor carry set programs threads following perform number computation steps set threads values held locally start send receive number messages corresponding nonlocal read write requests complexity superstep bsp algorithm determined follows let work w maximumnumber local computation steps executed processor let h maximum number messages sent processor h r maximum number messages received processor cost given time steps alternatively g cost bsp algorithm simply sum costs supersteps architecture independence achieved bsp model designing algorithm parameterized n size problem p number processors also l g resulting algorithms efficiently implemented range bsp architectures widely differing l g values example machine large g must provide algorithm sufficient parallel slackness ie v processor algorithm implemented p processor machine v p ensure every nonlocal memory access least g operations local data performed 3 basic bsp simulation algorithms kind systems relevant paper ie statistically homogeneous steady state systems event occurrences randomly evenly distributed among constituent elements simulated bsp computer using twophase conservative algorithms follows pprocessor bsp computer whole system divided p equalsized regions owned unique processor events involving elements located boundaries called border zone events bz events used synchronize parallel operation processors conservative less efficient version algorithm works iterations composed two phases parallel phase processor simultaneously allowed simulate sequentially asynchronously region ii synchronization phase occurrence one border zone event simulated one processor ones remain idle state improve efficiency algorithm exploiting opportunities simulate p border zone events parallel synchronization phase synchronization phase used cause barrier synchronization processors simulated time exchange state information among neighboring regions system examples studied state information refer states particular atoms particles located neighboring regions parallel phase every processor simulates events whose times less current global next bz event ie bz event least time among local next bz events held region processor thus global processor synchronization issued periodically variable time intervals driven chronological occurrence bz events see pseudocode figure 1 assume n elements evenly distributed throughout whole system regions made np theta np elements goal simulate occurrence events average assumed occur randomly evenly distributed among elements ie mn per ele ment goal achieved bsp algorithm iterations wherein iteration simulates total npe events parallel phase plus one event synchronization phase namely define f fraction bz events occur simulation show kind systems interested leading shows choosing regions sufficiently large always possible achieve degree parallelism strategy however actual gain running time due parallel phase processor simulates oap events sequentially crucially depends cost communication synchronization among processors synchronization phase cost depends particular parallel computer parallel prefix operation figure 1 realized follows virtual tary tree constructed among k processor owns region next border zone event nbze take place event scheduled occur time bz parallel prefix operation calculates minimum among set p local nbzes distributed p processors minimum stored processor parallel simulation processor initialisation end condition beginsuperstep simulate events time less bz processor k reads state neighboring regions endsuperstep processor k simulates occurrence nbze endwhile figure 1 hyperconservative simulation algorithm p processors leaves root partial calculated absolute minimum distributed among processors going root leaves cost operation value depends parameters g l eg small number processors p could convenient set efficiency algorithm figure 1 improved attempting simulate parallel border zone events per iteration explain procedure example let us assume situation next bz events e regions r r b respectively bg identifier element region r scheduled next bz event e occur time addition define time element 0 r scheduled bz event assume elements related due topology system simulated eg neighboring atoms isingspin model described note 0 necessarily time next bz event region however simulation e e 0 restricted order relation respective scheduled times 0 must simulate e e 0 otherwise first simulate thus simulate parallel two next bz events e e b otherwise must process sequentially bz events region lesser condition reached new bz event processed region r nonbz events time interval two consecutive bz events parallel simulation region r initialisation end condition beginsuperstep endsuperstep beginsuperstep simulate events e k time k k k 0 0 simulate next bz event e endif endsuperstep endwhile figure 2 conservative bsp simulation algorithm simulated well described pseudocode region r shown figure 2 operation reads value 0 element 0 stored region r b 4 isingspin systems isingspin system modeled n theta toroidal network every node network atom magnetic spin value gamma1 1 atom attempts change spin value discrete times given time atom currently simulated x random variable negative exponential distri bution new spin value decided considering current spin values four neighbors goal simulation process occurrence spin changes events sequential simulation system trivial since necessary deal one type event use efficient eventlist administer times ik1 cost c 1 processing event takes place sequential algorithm ologn even o1 calendar queue used 2 conjectured calendar queue o1 cost workload similar one produced isingspin system cost sequential simulation whole system n atoms case parallel simulation toroidal network divided p p theta p p regions np theta atoms region total 4 gamma 1 atoms border zone ie f region sequential eventlist algorithm applied parallel phase although executed smaller number atoms np cost c p processing every event parallel phase eventlist used iteration cost parallel phase determined maximum number events simulated processor period number hard determine optimistically assume average similar number events simulated processor going assume total simulated parallel phases executed simu lation total simulated processor introduces constant error since average maximum per iteration considered also assume analysis f bz bz events take place simulated sequentially hyperconservative algorithm figure 1 thus cost tp parallel simulation given tcs p g l cost communication g synchronization l among p processors generated iteration predict performance bsp algorithm need compare fastest sequential algorithm problem aim define speedup shows exists bsp algorithm total cost smaller cost ts sequential alternative ie bsp algorithm consider computation cost also cost communication synchronization among processors case isingspin model since c p c 1 replace c p c 1 obtain upper bound tcs required achieve 1 expressed shows effect cost tcs essentially absorbed f bz c 1 given particular machine characterized parameters g l always achieve speedup 1 sufficiently large problem characterized parameters f bz example extreme situation system low c 1 simulated inefficient chine say high g l way achieve increasing parallel slackness increasing andor reducing p enough reduce effect tcs p hyperconservative algorithm given figure 1 cost tcs dominated parallel prefix operation ie p isingspin system f exp table 2 results 8processor ibmsp2 efficient algorithm shown figure 2 cost depends number q different processors every processor communicate order decide whether simulate next bz event namely case 2d isingspin model q 2 isingspin model estimate bounds tcs ensure 1 end substitute obtain log p hyperconservative algorithm hand less conservative algorithm assume p bz events processed iteration namely obtain better bound note restricted case c leads bounds oa oa p respectively given bounds g l shown table 1 see restriction upper bound tcs possible satisfy practice example running hyper conservative algorithm 2d array computer would require one adjust p p table 2 show empirical results obtained using running time olog n sequential algorithm less conservative parallel algorithm figure 2 column 4 show fraction bz events per iteration value 10 means one bz event simulated processor best case column shows average among iterations 5 harddisk fluids second example complex consists twodimensional box size l theta l contains harddisks evenly distributed random assignment velocities nonobvious problem consists simulating total nddc elastic diskdisk collisions ddc events running time small possible section show similar bounds tcs although much higher constant factors required simulate systems bsp computer efficiently achieve efficient sequential running time whole box divided p n c theta p n c cells size oe theta oe diameter disk box periodical sense every time disk runs box boundary wall reenters box opposite point neighborhood disk whose center located cell c composed cell c eight cells immediately periodical adjacent c define average number disks per cell since oe disk collide 9 located neighborhood reduces ologn cost associated simulation every ddc event takes place since regarded constant use olog n eventlist administer pending events collisions disks move freely ddc events eventually cross neighboring cells regard instant disk crosses cell c neighboring cell c virtual wall collision vwc event time vwc event takes place necessary consider possible collisions among disks located new cells become part neighborhood ie cells immediately adjacent c adjacent c 3 disks considered consider effect events define average number vwc events take place two consecutive ddc events goal simulating nddc ddc events actually involves processing occurrence events note 1 represents probability next event take place given instant simulated time ddc event whereas probability next event vwc event perform simulation necessary maintain disk updated information time possible collisions disks j located neighborhood also necessary periodically update time crosses neighboring cell virtual wall w computations done pairwise manner considering positions velocities two objects involved event calculated outcome dynamic set eventtuples represents disk j virtual wall w e indicates ddc vwc event initialization first future events eventtuples predicted n disks system new future events successively calculated simulation advances end namely every time disk suffer ddc event notice subset events calculated disk ones really occur simulation obvious identify events advance different methods cope problem proposed literature 6 10 14 20 however common principle use efficient data structure maintain eventlist future events stored removed take place invalidated earlier events ddc event et 1 stored eventlist becomes invalidated another ddc event et place simulation assume one event e actually maintained disk one minimal time et eventlist event e becomes invalidated new event e 0 calculated considering complete neighborhood words every ddc vwc invalid event retrieved eventlist new collisions calculated considering 9 located neighborhood implies fairly slower sequential simulation also simplifies implementation analysis note fraction invalid events retrieved next event less 15 14 neglect effect analysis well initialization simulation enters basic cycle essentially composed following operations picking chronological next event event list ii updating states disks involved current event iii calculating new events disks one vwc event several ddc events iv inserting one event eventlist per disk involved current event operations cyclically performed end condition reached ie occurrence border zone event case parallel simulation running time sequential algorithm estimated follows constant factors neglected considering operations updating position velocity disk calculating one ddc event single operations cost o1 also every disk necessary consider disks located neighborhood calculating new ddc events calculating vwc takes time o1 well cost associated eventlist log n per event insertion whereas retrieving next event negligible selecting event minimal time disk also negligible since done new events calcu lated gives costs 2 3 log n simulation ddc vwc event takes place respectively overall cost simulation ddc takes place notice c 1 includes cost vwc events take place two consecutive ddc events total running sequential algorithm using theory harddisk fluids derivated following expression 13 diskarea density system calculating tddc obtained ex pressionm even extreme conditions eg ae 001 hand restriction oe imposes lower bound opt replacing practice choosing efficient simulation terms total running time space used cells parallel phase every processor simulates evolution disks located region total p processors average np disks every region logarithmic property average running time spent processor computing occurrence two consecutive ddc events region emphasize harddisk systems far difficult simulate parallel isingspin models particular necessary cope problem event scheduled particular disk may occur predicted time disk hit neighboring disk earlier simulated time necessarily leads one deal possibility rollbacks whole simulation restarted check point passed without error example algorithm figure 2 processor simulating region r fetches time 0 region r b superstep might occur processor simulating region r b changes value 0 next invalidating way work made processor simulating r parallel phase ie superstep 1 cope prob lem maintain additional copy whole state simulation state array one entry per disk entry keeps disks information position velocity local simulation time also use single linked list register position main state array modified complete iteration ie parallel phase plus synchronization phase described problem occurs use linked list make two state arrays identical repeat iteration better estimation 0 see 13 specific details bsp simulation harddisk fluids eg since bsp computer distributed memory system maintain region copy disks located border zone neighboring regions thus synchronization phase also involves transference information among neighboring regions properly updated states disk copies regions simulated processor made theta cells also define bz cells 4 gamma 1 cells located boundaries every region studying probabilities cases bz event takes place calculate f bz function parameters harddisk system general expression f bz given pddcbz pvwcbz probabilities ddc vwc event taking place border zone res pectively probabilities calculated considering given disk probability arbitrary cell direction also probability calculations bit involved many cases considered briefly expressions given obtained studying two types bz events ddc vwc positions disk ei involved probability disk located bz cell whereas probability disk located cell neighboring bz cell vwc event disk crosses cell probability 14 whereas ddc event disk collides disk located neighboring cell probability 18 disk located cell neighboring bz cell probability disk located corner box case probability bz event 58 ddc 24 vwc disk located corner last probabilities 38 14 respectively similar considerations used disk located inside bz cell vwc ddc probability bz event bz weighted sum cases obtained 2 2 pm represents probability ddc two disks located cell probability depends size cells oe purpose analysis enough say f bz running time tp parallel algorithm given tpp time spent simulating npe events ddc vwc parallel phase sp time spent synchronization phase simulating one event plus cost tcs associated communication synchronization among pro cessors note npe events total 11 npe ddc events events evenly distributed among p processors namely p processors simultaneously simulate 11 npe p ddc events tpp given f bz tsp given namely therefore last expression made exigent tcs assuming obtain log n tcs shows practical simulation 1a bound tcs similar isingspin system hand assume p border zone events simulated iteration less conservative algorithm f bz leads bound similar one ising spin model well important note calculations involved derivation speedup conservative sense mixing bsp cost units ones defined harddisk fluid f exp table 3 empirical results ibmsp2 basic unit cost updating disk state calculating new event disk much higher cost time step assumed g l bsp model table 3 show empirical results harddisk fluid simulated less conservative algorithm running ibmsp2 parallel computer 6 final comments paper derived upper bounds cost communication synchronization among processors order perform efficient conservative simulation two system examples conclude possible satisfy bounds current parallel computers empirical results confirm conclusion believe examples analyzed paper exhibit sufficient generality complexity considered representatives wide class systems events takes place randomly evenly distributed among constituent elements first example simple system links among neighboring regions processors maintained fixed whole simulation ever cost event processed system extremely low imposes harder requirements cost communication synchronization upper bounds much lower constant factors second example noticeably complex dynamic nature system links among regions change randomly simulation thus even hyperconservative algorithm figure 1 necessary cope problem rollbacks since scheduled events associated disk necessarily occur predicted time however simulated time progresses statistically rate region upper bounds communication synchronization similar first simpler example notice constant factors much higher second system example relaxes requirements bounds empirical results obtained less conservative algorithm shown figure 2 running ibmsp2 could obtain speedup 1 hyperconservative algorithm figure 1 running similar conditions emphasize ever results obtained particular machine fairly high g l values increasing slackness 5 disks per processor obtained algorithm figure 1 experiments described table 3 acknowledgements author supported university magallanes chile chilean scholarship r distributed simulation time warp part 1 design colliding pucks calendar queues fast o1 priority queue implementation simulation event set problem discrete event simulation object movement interactions green bsp library per formance colliding pucks simulation time warp operating system part 2 asynchronous behavior sectoring efficient algorithm hardsphere problem efficient parallel simulations dynamic ising spin systems efficient distributed eventdriven simulations multipleloop networks simulating colliding rigid disks parallel using bounded lag without time warp simulate billiards similars systems simulating billiards serially parallel direct bsp algorithms parallel discreteevent simulation eventdriven hardparticle molecular dynamics using bulk synchronous parallelism ef ficient algorithms manybody hard particle molecular dynamics empirical assessment priority queues eventdriven molecular dynamics simulation object oriented c approach discrete event simulation complex large systems many moving ob jects general purpose parallel comput ing parallel simulation billiard balls using shared variables event scheduling problem molecular dynamics simulation reduction eventlist molecular dynamic simulation bridging model parallel com putation distributed combat simulation time warp model performance im plementing distributed combat simulation time warp operating system case studies serial parallel simulation tr efficient parallel simulations dynamic ising spin systems calendar queues fast 01 priority queue implementation simulation event set problem implementing distributed combat simulation time warp operating system efficient distributed eventdriven simulations multipleloop networks bridging model parallel computation simulate billiards similar systems discrete event simulation object movement interactions general purpose parallel computing efficient algorithms manybody hard particle molecular dynamics efficient algorithm hardsphere problem parallel simulation billiard balls using shared variables object oriented c approach discrete event simulation complex large systems many moving objects ctr wentong cai emmanuelle letertre stephen j turner dag consistent parallel simulation predictable robust conservative algorithm acm sigsim simulation digest v27 n1 p178181 july 1997