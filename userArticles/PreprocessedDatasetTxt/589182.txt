twostep algorithms nonlinear optimization structured applications paper propose extensions trustregion algorithms classical step augmented second step insist yields decrease value objective function classical convergence theory trustregion algorithms adapted class twostep algorithms algorithms applied problem whose contribution objective function known functional form nonlinear programming package lancelot applied update slack variables variables introduced solve minimax problems leading enhanced optimization efficiency extensive numerical results presented show effectiveness techniques b introduction nonlinear optimization problems expensive function gradient evaluations desirable extract much improvement possible iteration algorithm objective function contains subset variables occurs predictable functional form second computationally relatively inexpensive update applied variables following classical optimization step additional step provides reduction objective function lead superior optimization eciency twostep algorithms successfully applied updating slack variables particular formulation minimax problems indicated numerical results variety problems instances subset variables slack variables variables introduced solve minimax problems appears xed known algebraic form objective function however since applied problem subset variables optimized relatively cheaply compared cost evaluating entire function example terms require simulation independent terms department mathematical sciences ibm j watson research center route 134 taconic room 33206 yorktown heights ny 10598 departamento de matematica universidade de coimbra 3000 coimbra portugal work started author visiting ibm j watson research center yorktown heights supported part centro de matematica da universidade de coimbra instituto de telecomunicacoes fct praxis xxi 221mat34694 ibm portugal z computer architecture design automation ibm j watson research center route 134 taconic room 33156 yorktown heights ny 10598 available analytically applicability really rather broad propose modications existing nonlinear optimization algorithms alternative approach feasible reformulate original problem eliminating subset variables apply algorithms remaining variables see example golub pereyra 17 paper deals twostep algorithms second step required yield decrease value objective function analysis given covers global convergence twostep trustregion algorithms presented unconstrained minimization problem continuously dierentiable function trust regions line searches one consider two versions twostep algorithms one called greedy called conservative greedy version exploits much possible decrease obtained second step whereas conservative approach calculates second step rst step conrmed satisfy traditional criteria required global convergence point conservative twostep linesearch algorithm new found books bertsekas 1 section 131 luenberger 19 section 710 second step called spacer step description greedy conservative twostep linesearch algorithms found 11 trust regions second step guaranteed decrease value objective function global convergence type lim inf k 1 krfy k immediately attained cases rst step would rejected sum rst second steps better chance accepted see remark 31 obtain lim k 1 krfy k either norm second step controlled trust region see condition 13 decrease objective function attained second step order magnitude norm step see condition 12 update slack variables referred motivated study local rate convergence twostep newtons method show second newton step variables retains qquadratic rate convergence traditional newtons method paper structured follows section 2 introduce twostep trustregion al gorithms section 3 analyze global convergence properties local rate twostep newtons method studied section 4 application twostep ideas update slack variables variables introduced solution minimax problems described section 5 section 6 presents numerical results obtained lancelot using updates analytic problems dynamicsimulationbased analytic statictimingbased circuit optimization problems finally conclusions drawn section 7 twostep trustregion algorithms rst consider trustregion framework presented paper 20 unconstrained minimization classical trustregion algorithm builds quadratic model form current point k h k approximation r 2 fy k note k k step k computed approximately solving trustregion subproblem subject ksk k k called trustregion radius kk arbitrary norm new point tested acceptance actual reduction fy k larger given fraction predicted reduction k k step k new point k1 accepted situation quadratic model k considered good approximation function fy region trust radius may increased otherwise considered good approximation function fy region case new point k1 rejected new trustregion subproblem form 2 solved smaller value trust radius simple trustregion algorithm described algorithm 21 trustregion algorithm 1 given 0 value fy 0 gradient rfy 0 approximation h 0 hessian f 0 initial trustregion radius 0 set 0 1 2 compute step k based trustregion problem 2 3 compute 4 case set compute h k1 select k1 satisfying k1 k otherwise set 5 increment k one go step 2 mechanism used update trust radius described algorithm 21 simple suces prove convergence results practice goal improving optimization eciency one uses updating schemes complex involving several subcases according value k propose paper modication trustregion algorithm motivated situation desirable update slack variables variables introduced solve minimax problems every iteration trustregion algorithm 7 implemented lancelot 9 see section 5 details practical applications twostep trustregion algorithm quite easy describe suppose computing step k based trustregion subproblem 2 know properties function fy enables us compute new step k guarantee fy k situation would certainly like test whether new point accepted modication requires careful redenition actual predicted reductions given algorithm 21 new actual predicted reductions propose aredy predy new predicted reduction predicted reduction obtained rst step plus actual reduction obtained second step choice predy k appropriate since second step k computed using model k k twostep trustregion algorithm given algorithm 22 twostep trustregion algorithm greedy 1 algorithm 21 2 compute step k based trustregion problem 2 3 possible nd another step k 4 compute predy 5 case set compute h k1 select k1 satisfying k1 k otherwise set 6 increment k one go step 2 twostep trustregion algorithm 22 evaluates new point acceptance steps k k computed call version greedy tries take much advantage possible decrease obtained second step k note although function f evaluated twice algorithm 22 reevaluation often computationally inexpensive context particularly interested involves relatively expensive evaluations k evaluations k involving subset variables cheap compute see section 5 could also consider twostep trustregion algorithm rst acceptable step k determined afterwards second step k computed algorithm outlined algorithm 23 twostep trustregion algorithm conservative 1 algorithm 21 2 repeat compute step k based trustregion problem 2 b compute c k set compute k1 satisfying k1 k set accepted true k set k accepted false accepted 3 possible nd another step k 4 set k 5 update h k increment k one go step 2 comments function evaluations apply algorithm 23 computation successful step k however case algorithm 23 function f evaluated twice iterations corresponding successful rst steps k 3 global convergence twostep trustregion algorithms analyze rst twostep trustregion algorithm 22 ie greedy version analysis conservative algorithm 23 similar section make assumption fh k g bounded sequence exists 0 require step k satisfy fraction cauchy decrease trustregion problem 2 words ask k satisfy 2 0 1 step c k called cauchy step dened solution scalar problem unknown subject ksk k variety algorithms compute steps satisfying condition see 3 22 23 25 26 proposition 31 fraction cauchy decrease krfy k k 6 5 respectively proof see powell 24 theorem 4 20 lemma 48 2 use proposition fact fy k predy krfy k k krfy k k inequality crucial prove global convergence twostep algorithm particular iteration k successful aredy ready prove rst convergence result theorem 31 consider sequence fy k g generated algorithm 22 k satises 6 f continuously dierentiable bounded fh k g bounded sequence lim inf krfy sequence fy k g bounded exists least one limit point rfy proof proof similar proof given 20 theorem 410 assume contradiction fkrfy k kg bounded away zero ie exists 0 krfy k k k 20 theorem 410 make direct use 9 rules update trust radius obtain lim k 1 next step show lim k note denitions 3 4 aredy turn using taylor series expansion ks k k k implies inequality 8 show converges zero rest proof follows classical argument trust regions k converges one rules update trust radius show k cannot converge zero contradiction attained proof completed 2 result theorem 31 require step k k may seem surprising result shows appropriateness denitions given 3 4 actual predicted reductions denitions allow us obtain conditions 9 11 crucial establish 10 remark 31 also important note denitions 3 4 improve acceptability step fact note k function strictly increasing k 1 words cases standard trustregion algorithm rejects step modied criterion always better usual one noted indicates successful iterations standard algorithm also successful modied twostep algorithm particular next step analysis prove additional conditions second step theorem 32 consider sequence fy k g generated algorithm 22 f continuously dierentiable bounded ly 0 fh k g bounded sequence rf uniformly continuous ly 0 either positive constants independent k lim krfy sequence fy k g bounded every limit point satises rfy proof proof similar proof given 20 theorem 414 see also thomas 27 show result contradiction assume therefore exists 1 2 0 1 subsequence indexed fm g successful iterates subsequence theorem 31 guarantees existence another subsequence indexed fl g krfy k k 2 k l krfy l k 2 fm g without loss generality subsequence previously mentioned 2 real number chosen converges zero k suciently large corresponding successful iterations holds 12 satised holds otherwise consider cases 12 13 separately cases make use sums consider indices corresponding successful iterations 12 holds use 15 obtain ks 13 holds appeal 16 write ks either case obtain since right hand side inequality goes zero left hand side since gradient f uniformly continuous suciently large number 0 1 inequality contradicts supposition 2 theorem required norm step k either k fy k former condition enforced step 2 algorithm 22 although might benecial could lead inferior decrease obtain global convergence point also satises necessary secondorder conditions optimality purpose require step k satisfy fraction optimal decrease trustregion problem 2 words ask k satisfy 2 0 1 k optimal solution 2 condition weakened several ways 20 step k satisfying fraction optimal decrease computed using algorithms proposed 22 25 case trustregion norm euclidean global convergence result following theorem 33 consider sequence fy k g generated algorithm 22 h satises 17 ly 0 compact f twice continuously dierentiable ly 0 exists least one limit point rfy positive semidenite proof proof basically proof theorem 47 22 2 obtain stronger global convergence results secondorder points instance results theorems 411 413 22 see also 21 theorem 417 c conditions required like ks k k k next results show second step preserve nice local properties behavior trust radius typical trustregion algorithms theorem 34 let fy k g sequence generated algorithm 22 addition assume step k satises either condition 12 condition 13 f twice continuously dierentiable bounded ly 0 fy k g limit point h positive denite fy k g converges iterations eventually successful f k g bounded away zero proof theorem 32 guarantee lim k 1 krfy k proof basically proof theorem 419 20 2 alternative result impose conditions 12 13 second step given however need assume fy k g converges theorem 35 let fy k g sequence generated algorithm 22 continuously dierentiable ly 0 fy k g converges point h positive denite iterations eventually successful f k g bounded away zero proof rst step k yields decrease quadratic model thus assumptions made h k h guarantee ks suciently large k turn using 8 implies predy ks k constants c 3 c 4 independent k taylor series expansion expression 11 gives fact fy k g converges result lim inf k 1 krfy k theorem 31 together imply lim k 1 krfy k thus 18 get lim k 1 ks k proof terminated typical argument trust regions 19 20 lim k 1 ks obtain limit lim predy shows appealing rules update trust radius iterations eventually successful trust radius uniformly bounded away zero 2 global convergence analysis algorithm 23 identical analysis given algorithm 22 point algorithm 23 well dened since nonstationary point always possible nd acceptable rst step also every k krfy k k krfy k k thus results given theorems 3135 hold algorithm 23 lim inftype result 10 obtained classical assumptions trustregion algorithms unconstrained optimization obtain limtype result 14 one two conditions 12 13 required case applications considered section 5 decrease obtained second step k always guaranteed satisfy moreover objective function strictly decreases along segment points k k case modify step 3 algorithms 22 23 way meet requirements theorem 32 modication given easy verify either 12 13 algorithm 31 step 3 algorithms 22 23 quadratic decrease case 3 compute step k g ks k k c 2 k k enlarged otherwise 12 holds c positive parameters c 2 set priori step 1 algorithms 22 23 course would like prove result theorem 32 case condition 12 replaced condition 21 however result unlikely true 4 local rate convergence twostep newtons method next section interested twostep algorithms second step calculated newtontype step variables section investigate local rate convergence algorithm step composed two newton steps second computed subset variables purpose let x suppose rst step k full newton step ie intermediate point k newton step applied variables u x k xed twostep newtons method described algorithm 41 twostep newtons method 1 choose 0 2 21 compute k 22 compute set k 23 set proof local convergence rate twostep newtons method requires modications standard proof newtons method 12 theorem 521 recall proof newtons method induction corollary 41 let f twice continuously dierentiable open set second partial derivatives lipschitz continuous fy k g sequence generated algorithm 41 converging point 2 rfy positive denite fy k g converges qquadratic rate proof k suciently close perturbation result 12 theorem 314 used prove nonsingularity hessian matrix r 2 fy k furthermore show r 2 first point r 2 uu fy lipschitz continuous r 2 uu fy positive denite thus inequality 22 perturbation lemma cited together imply nonsingularity r 2 hence method locally welldened second step yields since r u fy lipschitz continuous near use inequalities 22 23 write last inequality establishes qquadratic rate convergence 2 applications begin considering updating slack variables lancelot suppose problem trying solve form minimize fx subject c x positive integers technique implemented lancelot package 9 augmented lagrangian algorithm proposed conn gould toint 8 application augmented lagrangian algorithm problem reformulated minimize fx subject c x adding slack variables u algorithm considers following augmented lagrangian merit function estimate lagrange multiplier associated ith constraint positive penalty parameter ii positive scaling factor associated ith constraint solves sequence minimization problems simple bounds following subject u xed values ii twostep trustregion framework analysis described paper unconstrained minimization problems extended entirely straightforward way number algorithms minimization problems simple bounds particular algorithms 7 used lancelot solve problem 25 x xed function x slack variables u let us denote quadratic qu x dx ex depend x f constant dependency ii important since constants xed minimization process started key idea update slack variables every iteration k trustregion algorithm 7 used lancelot solve problem 25 trustregion algorithm computes current point k rst step k new point k compute step k updating slack variables u f represents objective function sections 14 note second step k exclusively components associated slack variables step computed u k1 optimal solution subject u due simple form quadratic solution explicit ii important remark updates require function gradient evaluations also considered codes npsol snopt 15 16 update slack variables application line search augmented lagrangian merit function prior solution next quadratic programming problem ways dealing slack variables studied literature see gould 18 references therein study impact slack variable update global convergence trustregion algorithm step variables required decrease quadratic qu u k u k u k case always guarantee decrease objective function larger ks k k 2 21 holds result shown following proposition drop x k q x k simplify notation proposition 51 exists positive constant c 5 whenever qu k u k qu k proof first write properties quadratic qu simple algebraic manipulations lead also since qu convex let c positive constant c min smallest eigenvalue f consider two cases 1 cku k k 2 case use 29 obtain 2 case appeal 28 get min proof completed setting c cg 2 another example application twostep algorithms arises one approach solution minimax problems consider following f realvalued function dened ir n one way solving minimax problem reformulate nonlinear programming problem adding articial variable z see 18 details leads minimize z subject z f x slack variables also introduced lancelot used solve nonlinear programming problem augmented lagrangian algorithm requires solution sequence problems simple bounds type subject u situation function x z variables u z xed values x constants variables problem 32 application twostep trustregion algorithm follows similar way hessian quadratic positive semidenite following form last row last column correspond variable z solution quadratic program minimize qz u subject u given ii z k1 solution equation ii ii right hand side ii equation 35 solved easily om oating point operations comparisons showing solution quadratic program 33 relatively inexpensive calculation several nonlinear optimization problems subset problem variables occur linearly example arrival times statictimingbased circuit optimization problems 6 problems also benet twostep updating 6 numerical tests 61 analytic problems modied lancelot release 9 include slack variable update 27 slack minimax variable updates 3436 updates incorporated lancelot using greedy twostep modication trustregion algorithm 7 minimization problems simple bounds implemented subroutine sbmin greedy twostep trustregion algorithm unconstrained minimization problems algorithm 22 tested following versions lancelot 1 lancelot release default parameter conguration specspc le except increased maximum number iterations 4000 2 version 1 slack minimax variable updates 27 3436 incorporated sbmin using greedy twostep trustregion algorithm 3 version 2 update variable z minimax problems ie z xed 3436 compared numerical performance three versions set problems 1 cute collection 2 set problems listed table 1 case minimax formulations table 2 mention number variables including slacks applicable minimax variable z number slack variables number equality inequality constraints excluding simple bounds variables note minimax problems reformulated nonlinear programming problems introduction additional minimax variable z shown 31 computational results presented tables 3 4 5 tests conducted ibm riscsystem 6000 model 390 workstation table 3 compare results versions 1 2 problems minimax problems table 4 present results versions 1 2 minimax problems table 5 include results versions 1 3 minimax problems tables 4 5 include majority minimax problems see section 63 numerical results remaining problems tables report value ag inform number iterations total cpu time determined values single value objective function values inform following meaning meaning norm projected gradient augmented lagrangian function become smaller 10 5 cases maximum number iterations 4000 reached cases norm step become small conclusion based sets problems version slack minimax variable updates exhibits superior numerical behavior fact version required average 15 fewer iterations version without updates problems hs109 haifam polak6 excluded calculation mainly comparison extraordinarily favorable case rst two worse last comparing tables 4 5 updating variable z addition twostep updates slacks seen yield signicant benet however minimax problems twostep algorithm performs poorly situation analyzed detail section 63 although cute contains 56 problems general constraints majority equality constrained problems excluded problems took 4000 iterations versions 1 2 included rest exception problems easy making total 56 problems minimax problems 26 nonminimax problems problem name variables slacks constraints core1 core2 157 26 134 corkscrw hadamard 769 512 648 hs85 26 21 21 table 1 nonminimax problems cute collection used 62 circuit optimization problems built extensive experience circuit optimization problems due expensive function evaluations modest numerical noise levels practical stopping criteria implementation designed terminate many asymptotic iterations taken algorithms described paper used dynamicsimulationbased circuit optimization tool called jiytune see 4 5 10 jiytune optimizes transistor wire sizes digital integrated circuits meet delay power area goals based fast circuit simulation timedomain sensitivity computation specs see 13 28 optimize multiple path delays highperformance circuit tuning often formulated minimax problem minimization problem nonlinear inequality constraints remark many analytic problems especially minimax problems rather small involve inexpensive function evaluations moreover clear twostep updating unlikely helpful asymptotically situations consequently also report numerical results circuit optimization problems indicative problems expensive function evaluations termination inherent noise practical considerations encouraged signicant asymptotic behavior numerical results presented table 6 version 1 second step consisted slack minimax variable updates 27 3436 however gradient constraint tolerances used 10 respectively problem name variables slacks constraints coshfun 81 20 20 goffin 101 50 50 haifal 9301 8958 8958 haldmads 48 42 42 minmaxbd table 2 minimax problems cute collection used safeguards related expected level numerical noise clearly observe table 6 twostep algorithm leads better nal objective function values practical applications simple function evaluation takes ten minutes cpu time eectiveness simple addition indeed signicant situations greedy twostep trustregion algorithm able take advantage decrease given slack variable updates algorithm accept steps otherwise would rejected see remark 31 also applied algorithms paper analytic statictimingbased circuit optimization problems see table 7 clear advantage twostep approach increasingly apparent larger problems problem name inform iterations total cpu obj function core1 00 953983 74117 911 core2 00 10481086 256257 729 corkscrw 00 4142 055054 116 hadamard 00 1709548 2290276 1141 tfi3 00 2334 038038 43 table 3 comparison versions 1 2 nonminimax problems lancelot outwith twostep updating 63 experiments minimax problems section consider minimax problems test set twostep algorithm improve numerically results obtained onestep case also makes considerably worse see rst part table 8 analyze reasons failure twostep updating minimax problems discuss ways enforce better numerical behavior consider general minimax problem 30 aim show types problems second step tendency make hessian illconditioned let us assume happens default rst lancelot major iteration circumstances using notation g x z following expressions elements twostep algorithms nonlinear optimization 20 problem name inform iterations total cpu obj function congigmz 00 3219 004005 28 coshfun 00 12769 131106 0773 goffin 00 144 103067 0 table 4 comparison versions 1 2 minimax problems lancelot withoutwith twostep updating gradient r z r similarly elements hessian matrix given magnitudes products r 2 small compared products r x hessian given problem name inform iterations total cpu obj function congigmz 00 3225 00401 28 coshfun 00 12792 131108 0773 goffin 00 148 103066 0 table 5 comparison versions 1 3 minimax problems lancelot withoutwith twostep updating slacks approximately i1 i1 i1 indices sums go 1 matrix clearly singular fact n 1st row negative sum last rows moreover rst n rows linear combination last rows result observations hessian projected hessian illconditioned 37 happens many indices j k key point analysis second step tendency produce iterates worsen property 37 produces decrease problem name variables ineq iterations total cpu obj function iomuxpower 102 42 2129 72309220 1510016000 coulman cold 33 17 2222 695683 271262 clkgen 22 5 255 35108 198182 coulman hot 33 17 1632 462100 283253 coulman delay 33 17 2624 726735 116111 minimax bultmann latch stall1 coulman cold minmax 34 17 6180 184229 694669 coulman hot minmax 34 17 6644 197134 744751 eischer northrop xor coulman delay minmax 34 17 100100 290306 674705 table updating dynamicsimulationbased circuit optimization problems ineq stands number inequality constraints values g x z u indices hessian might well illconditioned second steps applied doubt numerical results evidence claim second step problems worsens situation making hessian illconditioned presence nonzero lagrange multipliers formulae gradient hessian g x z u substituted g x z u similar conclusions could drawn second step may produce bad results minimax problems points towards set fx z hessian augmented lagrangian illconditioned eect uences negatively calculation rst step next iteration given undesirable feature hessian points close set one possible improvement twostep algorithm make sure calculation rst step accurate lancelot context could achieved choosing smaller tolerance stopping criterion conjugategradient technique another possible improvement reduce illconditioning hessian instance increasing value penalty parameter seen examples variables indeed modications improve bad numerical results presented second part table 8 compare results obtained following modications versions 1 2 4 version 1 initial value penalty parameter 100 default value 01 5 version 2 initial value penalty parameter 100 tolerance 10 12 problem name variables ineq iterations total cpu obj function symmetric 9 table 7 lancelot withoutwith twostep updating analytic minimax statictimingbased circuit optimization problems ineq stands number inequality constraints stopping criterion conjugate gradients study strategies make twostep updating eective minimax problems general subject future research 7 concluding remarks paper presented analyzed framework classical algorithms nonlinear optimization modied allow second computationally ecient steps generated conventional way guaranteed yield decrease objective function gave examples twostep algorithms update slack variables lancelot update variables introduced solve minimax problems however emphasize twostep algorithms generally applied example whenever functions dening problem known functional form variables considered trustregion algorithms proposed greedy conservative twostep algorithm analyzed convergence properties trustregion twostep algorithms see 11 linesearch twostep algorithms deriving conditions attain global convergence also showed twostep newtons method second step computed subset variables qquadratic rate convergence greedy twostep algorithms designed exploit much possible decrease attained second step trustregion framework allowed us design greedy twostep trustregion algorithm particularly well tailored achieve purpose finally included numerical evidence technique eective particularly problems expensive function evaluations twostep algorithms already found practical applications optimization highperformance custom microprocessor integrated circuits problem name inform iterations total cpu obj function minmaxbd 00 267952 134359 116 polak3 00 71125 0408 593 minmaxbd 00 4743 025022 116 table 8 rst part comparison versions 1 2 minimax problems lancelot withoutwith twostep updating second part comparison versions 4 5 minimax problems lancelot withoutwith twostep updating acknowledgments grateful n gould rutherford appleton laboratory comments suggestions earlier version paper led many improvements also grateful k scheinberg ibm j watson research center helping numerical results explanation section 63 would like thank elfadel ibm j watson research center providing analytic statictimingbased optimization circuit problems finally grateful referees useful comments suggestions r computer science applied mathematics cute constrained unconstrained testing environment approximate solution trustregion problem minimization twodimensional subspaces optimization custom mos circuits transistor sizing global convergence class trustregion algorithms optimization problems simple bounds circuit optimization via adjoint lagrangians numerical methods unconstrained optimization nonlinear equations sensitivity computation piecewise approximate circuit simulation practical methods optimization snopt sqp algorithm largescale constrained optimization users guide npsol 50 fortran package nonlinear programming solving three classes nonlinear programming problems via simple di linear nonlinear programming new algorithm unconstrained optimization minimization largescale quadratic function subject spherical con straint conjugate gradient method trust regions large scale optimization sequential estimation techniques quasinewton algorithms piecewise approximate circuit simulation tr ctr tong zhang dual formulation regularized linear systems convex risks machine learning v46 n13 p91129 2002 andreas wchter chandu visweswariah andrew r conn largescale nonlinear optimization circuit tuning future generation computer systems v21 n8 p12511262 october 2005 xiaoliang bai chandu visweswariah philip n strenski uncertaintyaware circuit optimization proceedings 39th conference design automation june 1014 2002 new orleans louisiana usa nicholas gould dominique orban philippe l toint cuter sifdec constrained unconstrained testing environment revisited acm transactions mathematical software toms v29 n4 p373394 december r conn elfadel w w molzen jr p r obrien p n strenski c visweswariah c b whan gradientbased optimization custom circuits using statictiming formulation proceedings 36th acmieee conference design automation p452459 june 2125 1999 new orleans louisiana united states andrew r conn ruud haring chandu visweswariah noise considerations circuit optimization proceedings 1998 ieeeacm international conference computeraided design p220227 november 0812 1998 san jose california united states andrew r conn chandu visweswariah overview continuous optimization advances applications circuit tuning proceedings 2001 international symposium physical design p7481 april 0104 2001 sonoma california united states