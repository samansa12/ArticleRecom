improved spill code generation software pipelined loops software pipelining loop scheduling technique extracts parallelism loops overlapping execution several consecutive iterations due overlapping iterations schedules impose high register requirements execution schedule valid requires number registers available target architecture register requirements reduced either decreasing iteration overlapping spilling registers memory paper describe set heuristics increase quality registerconstrained modulo schedules heuristics decide two previous alternatives define criteria effectively selecting spilling candidates heuristics proposed reducing register pressure applied software pipelining technique proposals evaluated using registerconscious software pipeliner workbench composed large set loops perfect club benchmark set processor configurations proposals paper compared previous proposal already described literature one processor configurations set loops fit available registers 32 speedup 168 reduction memory traffic factor 057 achieved affordable increase compilation time loops represents speedup 138 reduction memory traffic factor 07 b introduction software pipelining 9 instruction scheduling technique exploits instruction level parallelism ilp loop overlapping operations various successive loop iterations different approaches proposed literature 2 generation software pipelined schedules mainly focuses achieving high throughput 1 11 16 22 23 25 main drawback aggressive scheduling techniques high register requirements 19 21 using registers available requires actions reduce register pressure may also degrade performance either due additional cycles schedule due additional memory traffic reason proposals also focused attention minimization register requirements 12 15 20 27 register allocation consists finding final assignment registers loop variables variants invariants temporaries extensively studied framework acyclic schedules 3 5 6 7 based original graph coloring proposal 8 however software pipelining imposes constraints inhibit use techniques register allocation although proposals handle 13 14 24 none deals addition spill code scheduling needed reduce register pressure software pipelined loops software pipeliner fails generates schedule requires registers available target machine case additional actions performed order alleviate high register demand 24 one options reschedule loop reduced execution rate ie less iteration overlapping reduces number overlapped operations variables unfortunately register reduction may expense reduction performance another option spill variables memory occupy registers certain number clock cycles requires insertion store load instructions free use registers evaluation performed 18 shows reducing execution rate tends generate worse schedules spilling variables however authors show cases opposite situation may happen several aspects contribute quality spill code generated compiler first one deciding spill code applies uses variable subset second aspect relates selection spilling candidates implies deciding number variables uses selected spilling priority function used select among decisions need accurate estimates benefits selection spilling candidate produce terms register pressure reduction order motivate work table 1 shows two different spill algorithms average execution rate cycles initiation two consecutive iterations average memory traffic number memory accesses per iteration loops workbench section 24 whose schedule fit 32 registers one processor configurations used along paper table also includes ideal case ie infinite registers available spill needed notice gap two implementations one commercial described 26 experimental 18 ideal case large results motivated proposal new heuristics improve whole register pressure reduction process last column table shows results applying heuristics proposed paper represent 40 reduction execution rate memory traffic respect previous proposals paper use registerconscious pipeliner named hrms 20 schedule loops loops scheduled register allocation performed using wands strategy endfit adjacency ordering 24 register requirements decreased required paper contributes set heuristics 1 decide two different possibilities aforementioned adding spill code directly decrease execution rate 2 better selection spilling candidates terms assigning priorities selecting appropriate number paper also contributes analysis results spill variables uses performed different proposals compared ideal case upper bound performance metric ideal 18 26 paper avg execution rate 1201 2832 2943 2066 avg memory traffic 1538 5088 5213 3571 table 1 motivating example improving spill process proposals presented 18 workbench composed loops perfect 4 suitable software pipelining paper organized follows section 2 makes brief overview modulo scheduling register allocation spill code modulo scheduling section 3 focuses different steps proposals spilling variables memory section 4 presents different alternatives select spilling candidates effective way analyze tradeoff reducing execution rate adding spill code section 5 different alternatives heuristics evaluated terms dynamic performance taking account relative importance loop total execution time benchmark set finally section 6 states conclusions 2 basic concepts 21 modulo scheduling software pipelined loop schedule iteration divided stages execution consecutive iterations distinct stages overlapped number stages one iteration termed stage count sc number cycles initiation successive iterations software pipelined loop determines execution rate termed initiation interval ii execution loop divided three phases ramp phase fills software pipeline steady state phase maximum overlap iterations achieved ramp phase drains software pipeline steady state phase pattern operations executed stage achieved iterating piece code named kernel corresponds one stage steady state phase ii bounded recurrence circuits dependence graph loop recmii resource constraints target architecture resmii lower bound ii termed minimum initiation interval resmii reader referred 11 25 extensive dissertation calculate recmii resmii order perform software pipelining hypernode reduction modulo scheduling hrms heuristic 20 used hrms software pipeliner achieves mii large percentage workbench considered paper 974 loops addition generates schedules low register requirements registersensitive software pipelining technique used order overestimate necessity spill code scheduling performed two steps first step computes priority operations scheduled second step performs actual placement operations modulo reservation table 22 register allocation loop scheduled allocation values registers performed values used loop correspond either loopvariant loopinvariant variables invariants repeatedly used never defined execution loop invariant one value iterations loop therefore requires single register execution loop regardless scheduling machine configuration loop variants new value generated iteration loop therefore different lifetime lt 15 nature software pipelining lt values defined iteration overlap lt values defined subsequent iterations lt loop variants measured different ways depending execution model machine assume variable alive beginning producer operation start last consumer operation overlapping lt different iterations pattern length ii cycles indefinitely repeated obtained pattern indicates number values live given cycle maximum number simultaneously live values maxlive accurate approximation number registers required schedule 24 variants may lt values greater ii poses additional difficulty since new values generated previous ones used one approach fix problem provide form register renaming successive definitions value use distinct registers renaming performed compile time using modulo variable expansion mve 17 ie unroll kernel rename multiple definitions variable exist unrolled kernel rotating register files provide hardware solution solve problem without replicating code 10 ie renaming different instantiations loopvariant done execution time study implementation assume existence rotating register file use wandsonly strategy using endfit adjacency ordering 24 strategy usually achieves register allocation uses maxlive registers almost never requires maxlive registers however heuristics proposed paper applicable regardless hardware model register allocation strategy used 23 decreasing register requirements register allocation techniques software pipelined loops 24 assume infinite number registers name used registers ur number registers required execute given schedule available registers ar number registers available target architecture ur greater ar obtained schedule valid target processor case register pressure must decreased loop executed eg must obtain schedule ur ar different alternatives decrease register requirements outlined 24 1 reschedule loop larger ii 2 spill variables memory 3 split loop several smaller loops best knowledge loop splitting yet evaluated purpose decreasing register pressure two alternatives evaluated compared 18 used production compilers eg cydra5 compiler increases ii 11 mips compiler described 26 adds spill code next summarize main conclusions comparison ffl rescheduling loop bigger ii usually leads schedules less iteration overlapping therefore less register requirements unfortunately ur decrease directly expense reduction performance less parallelism exploited addition loops possible find valid schedule ur ar simply increasing ii ffl spilling variables memory makes available associated registers values spill requires use several load store operations may saturate memory units turning loop memorybounded loop case addition spill code leads increase ii degradation final performance increasing ii produces general worse schedules adding spill code however loops first option better suggests hybrid method cases adds spill code others increases ii produce better results instance 27 spills many uses possible without increasing ii ie tries saturate memory buses schedule obtained fit ar ii increased although heuristic always ends valid schedule care minimizing memory traffic fact may increase memory traffic loops require spill paper present heuristic allow bypassing step add spill code cases simply increase ii paper also proposes several new heuristics adding spill code heuristics allow better tuning final schedule performance degradation reduced well memory traffic overhead 24 experimental framework different proposals paper evaluated set architectures pimjlk defined follows number functional units used perform kind computations adders multipliers divsqr units j number loadstore units k latency adders multipliers configurations latency load store accesses two one cycles respectively divisions take 17 cycles square roots take cycles functional units fully pipelined except divsqr functional units particular four different configurations used p2m2l4 p2m2l6 p4m2l4 p4m4l4 registers order evaluate heuristics proposed total 1258 loops represent 80 total execution time perfect club 4 measured hppa 7100 scheduled first evaluate effectiveness proposals section 4 evaluation use loops ur ar number loops fulfills condition different processor configurations aforementioned shown table 2 notice 64 registers available number loops fit ar small therefore subject variance heuristics consequence main conclusions experimental evaluation drawn configurations however results 64 registers used confirm trend evaluate real impact performance taking account loops workbench section 5 ar p2m2l4 p2m2l6 p4m2l4 p4m4l4 table 2 number loops require registers available set processor configurations pimjlk metrics used evaluate performance following ffl sigmaii measures sum individual ii loops considered ffl sigmatrf measures sum individual number memory operations used scheduling ffl schedtime measures time schedule loops adding spill code initial algorithm use generating register constrained modulo schedules iterative algorithm shown figure 1 scheduling register allocation loop requires registers available set spilling candidates obtained ordered algorithm decides many candidates finally selected spilling introduces necessary memory accesses original dependence graph loop rescheduled modulo schedules tend compact goal saturate used resource difficult find empty slots allocate new memory operations modulo reservation table process repeated schedule requiring registers available found best knowledge previous spilling approaches based similar iterative algorithm 18 26 27 following subsections describe detail one aspects present solutions proposed previous researchers 31 variables uses lifetime variable spans definition last use lifetime variable divided several sections uses whose lifetime spans previous use priority regs allocation add spill select candidates sort candidates generation spill code scheduling rgs reqrm scheduling figure 1 flow diagram original spill algorithm current one example figure 2a shows producer operation followed four independent consumer operations case lifetime variable ranges beginning prod beginning cons4 four different uses defined u1 u4 shown right part figure disadvantage spill variables one variable several successors number associated spill memory operations suddenly increased may produce increase ii thus reduce performance addition loads added might actually contribute decrease register requirements spill uses allow finegrain control spill process alternatives used previous proposals spill variables used 18 26 spill uses 27 paper evaluate performance two alternatives combination heuristics proposed section 4 32 sorting spilling candidates criteria needed decide suitable spilling candidates ie decreasing register requirements smallest cost achieved assigning priority spilling candidate priority usually computed according lt candidate 18 ratio lt memory traffic introduced spilled 18 26 27 expected second heuristic always produces better results paper propose new criterion takes account criticality cycles spanned lifetime spilling candidate 33 quantity selection giving priorities spilling candidate algorithm decides many candidates actually spilled memory objective decrease register requirements ur ar minimum number spill operations requires estimation benefits candidate produce final schedule however new memory operations may saturate memory units lead increase ii increase ii reduces register pressure may lead situation unnecessary number candidates selected selection process done different ways instance 18 proposes spill one candidate time reschedule heuristic avoids overspilling expense unacceptable scheduling time avoid 26 performs several tries spilling poweroftwo number candidates process finishes new schedule fits ar found reduce number reschedulings effective way 18 selects much candidates necessary directly reduce ur ar order avoid overspilling time candidate selected lifetime subtracted current ur compute estimated number registers needed spilling another alternative used 27 generate schedules minimum register requirements consists selecting much candidates necessary saturate memory units current ii paper proposes new heuristic tries better foresee overestimation produced previous heuristics 34 adding memory accesses set candidates selected dependence graph modified order introduce necessary loadstore instructions rescheduled order guarantee spill effectively decreases ur spilled operations scheduled close possible producersconsumers accomplished scheduling spill operation associated producerconsumer single complex operation 18 spill variables store operation inserted producer load operation inserted consumer figure 2b shows modification dependence graph spilling variable figure 2a spill uses store lat fu cons1 cons2 cons3 cons4 store prod cons1 cons2 cons3 cons4 store prod prod b c cons1 cons2 cons3 u3 cons4 figure 2 original graph b graph spilling variable c graph spilling single use variable operation added producer single load operation added consumer ends corresponding use figure 2c shows modification graph use u3 selected one largest lifetime therefore releases registers 4 new heuristics spill code section describe issues gauges used control generation valid schedules spill code first control decides priority candidates selected spill idea behind proposal give priority ones contribute reduction register pressure effective way second control decides many candidates spilled rescheduling loop finally third control decides worth apply direct increase ii additional spill analysis also consider spilling candidates either variables uses 41 spill variables spill uses algorithms described section 3 decide candidates spill based lifetime ratio lifetime memory traffic spill would generate configuration p4m2l4 registers use 1932 4454 250 875 usecc useqf 1626 3791 229 787 usetf 1408 3139 132 593 useccqftf 1232 2895 126 606 table 3 improving performance metrics applying different heuristics make difference considering either spill variables uses variables instance table 3 shows sigmaii sigmatrf two register file sizes 32 64 registers processor configuration p4m2l4 either spill variables row labeled var spill uses row labeled use applied using lttrf criterion order spilling candidates table reports figures relative ideal case ie sigmaii sigmatrf ideal case subtracted values specific configuration notice general spill uses achieves schedules lower ii memory traffic initial results reader may conclude spill uses effective spill variables however see along paper proposals improve metrics tend reduce gap two alternatives addition behavior also depends architecture evaluated shown section 5 42 critical cycle first propose new criterion select candidates spill sometimes selection based lttrf may select candidates effectively reduce register pressure rationale behind spill reduces number simultaneous live candidates scheduling cycle number maximum thus deciding number registers needed critical cycle cc defined scheduling cycle number used registers ur maximum new selection criterion gives priority candidates cross cc criterion candidates selection may improve efficiency spill process shown table 3 processor configuration selected rows labeled varcc usecc show two performance metrics workload 43 number variables spill computation number candidates may accurate new spill code might increase ii schedule result saturation memory unit increase ii could reduce overall register pressure therefore would necessary use much spill initially expected proposal section tries foresee overestimation algorithm assumes register file available registers ar 0 actually adds actual number available registers number proportional gap ur ar follows ar 0 qf quantity factor notice corresponds spill one candidate time spill necessary candidates reduce ur ar qf parameter whose optimal value depends architecture characteristics loop paper conduct experimental evaluation parameter order determine range useful values analyze effects performance scheduling time schedtime figure 3 plots behavior sigmaii sigmatrf schedtime qf values range 1 0 lowest values qf leads worst results terms ii trf low schedtime large values qf lead better performance expense increase compilation time particular values qf larger 06 increase schedtime compensate increase perfor mance general medium values qf generate good schedules negligible increase compilation time table 3 rows labeled varqf useqf shows results 05 notice qf quantity factor18002200sumii a1 qf quantity factor40005000 b1 use qf quantity factor5001500 schedtime qf quantity factor250sumii a2 qf quantity factor8001000 b2 qf quantity factor100300500 schedtime c2 figure 3 behavior sigmaii b sigmatrf c total schedtime values qf 0 1 32 registers 1 64 registers 2 value increase much compilation time reduces considerably ii trf 44 traffic control previous techniques try improve performance spill process increasing effectiveness selection candidates situations better increase ii instead applying spill example ar ur adding spill code would lead saturation memory unit case ii memory traffic would increased order fit new memory operations however increase ii without adding spill memory traffic increase might also reduce ur figure 4 shows algorithm proposed control point decides better increase ii insert spill code order foresee previous situation algorithm performs estimation memory traffic number loads stores would introduced spill done newtrf maximum traffic maxtrf supported current value ii enough absorb newtrf algorithm directly increases ii without inserting spill process repeated particular new ii value might produce less spill code may required priority regs allocation add spill select candidates sort candidates trf absorbed ii generation spill code scheduling rgs reqrm scheduling figure 4 flow diagram proposed algorithm combines spill code traffic control maximum traffic architecture support multiplied tf traffic factor control saturation memory unit condition accepts addition spill code newtrf maxtrf tf done tradeoff applying spilling mechanism increasing ii tf parameter included obtain better tradeoff mechanisms moreover take always increasing ii take tf 1 always inserting spill tf take positive value experimentation observed best results obtained tf ranges 07 14 figure 5 plots behavior sigmaii sigmatrf schedtime values tf within range general observed best value ii obtained tf value close 095 want reduce traffic use smaller values tf notice time obtain schedules small variation table 3 rows labeled vartf usetf shows performance terms sigmaii sigmatrf tf set 095 notice registers spill variables performs better spill uses parameters qf tf tend reduce spill code may interfere positive way figure 6 plots combined effect parameters notice tuning parameters might lead better values sigmaii another observation tf used higher values qf needed results higher scheduling times particular 32 registers best results obtained qf set zero 64 registers best results obtained qf set 03 05 order summarize previous effects rows labeled varccqftf usecc qftf table 3 show performance cc qf tf used qf tf set value produce best performance results spill variables used sigmaii reduced 47 52 sigmatrf reduced 42 35 respect initial var 32 64 registers respectively similarly spill uses applied sigmaii reduced 36 50 sigmatrf reduced 35 30 14 13 12 11 10 09 08 07 tf traffic factor160020002400 a1 14 13 12 11 10 09 08 07 tf traffic factor300040005000 b1 use 14 13 12 11 10 09 08 07 tf traffic factor50150schedtime 14 13 12 11 10 09 08 07 tf traffic factor200300 a2 14 13 12 11 10 09 08 07 tf traffic factor600800sumtrf b2 14 13 12 11 10 09 08 07 tf traffic factor2060 schedtime c2 figure 5 behavior sigmaii b sigmatrf c schedtime tf ranges 07 14 32 registers 1 64 registers 2 first point corresponds tf 1 14 13 12 11 10 09 08 07 tf traffic factor160020002400 a1 14 13 12 11 10 09 08 07 tf traffic factor35004500sumtrf b1 14 13 12 11 10 09 08 07 tf traffic factor100300 schedtime 14 13 12 11 10 09 08 07 tf traffic factor200300 a2 14 13 12 11 10 09 08 07 tf traffic factor600800sumtrf b2 14 13 12 11 10 09 08 07 tf traffic factor2060 schedtime c2 figure behavior sigmaii b sigmatrf c schedtime several values qf tf ranges 07 14 32 registers 1 64 registers 2 first point given respect initial use 32 64 registers respectively increase performance expenses affordable increase scheduling time 32 registers scheduler requires 18 times original time 64 registers increase negligible performance evaluation effectiveness proposed mechanisms evaluated using static information ii trf evaluation demonstrated new heuristics effective obtaining better schedules however static evaluation show useful terms execution time dynamic memory traffic execution time estimated ii n n total number iterations e number times loop executed dynamic memory traffic estimated n number memory operations kernel code software pipelined loop results obtained p4m2l4 configuration shown figure 7 bar graphs upper part show execution time degradation relative ideal case ie assuming infinite number registers closer results 1 better performance notice 1 upper bound performance lower part figure b shows memory traffic mem relative ideal case ideal mem closer traffic 1 better schedules however case 1 lower bound memory traffic plots left side 1 correspond loops benchmark set plots right side 2 refer loops require spill code configuration registers figure 7a1 shows speedup 138 respect original proposal spill variables used 127 spill uses applied configuration figure 7b1 shows reduction memory factor close 07 cases 64 registers speedup reported less important close 106 memory traffic reduced factor close 09 figures 7a2 7b2 show performance subset loops require spill configuration registers performance loops increases factor 170 spill variables applied 152 spill uses applied 64 registers performance increases factor close 126 cases notice memory traffic registers 64 registers020610 cycles a1 use usecc useccqf useccqftf registers 64 registers13 mem b1 registers 64 registers020610 a2 registers 64 registers2610 menidealmem b2 figure 7 dynamic results different spill heuristics configuration p4m2l4 qf03 tf095 extraordinarily decreased 32 registers available memory traffic reduced factors 057 062 respect original proposals spill variables uses respectively 64 registers available memory traffic reduced factors 072 077 architecture notice spill uses performs better spill variables combination heuristics 32 64 registers critical cycle considered spill uses improves better spill variables however quantity factor traffic factor used performance tends level spill variables spill uses spill uses still performs slightly better results obtained processor configurations shown figure 8 first notice configurations heuristics proposed paper perform better however aspects need discussion example cases eg configuration p2m2l4 spill uses performs worse spill variables configurations eg p4m4l4 perform better spill uses applied 64 registers available spill variables performs better registers also contrary happens configurations p2m2l4 64 registers spill uses big performance degradation critical cycle considered finally parameters qf tf set different values config uration parameters give flexibility algorithm allow adapt registers 64 registers020610 cycles a1 registers 64 registers2610 mem b1 registers 64 registers020610 cycles a2 use usecc useccqf useccqftf registers 64 registers2610 mem b2 registers 64 registers020610 a3 registers 64 registers2610 mem b3 figure 8 dynamic results different spill heuristics configurations 1 p2m2l4 2 p2m2l6 3 p4m4l4 configuration however parameters tuned configuration order obtain good results instance used registers spill uses architecture spill variables obtains best performance performed extensive evaluations empirically obtain useful range values reasonably good results obtained particular qf range 00 03 tf range 09 10 addition values tuned specific applications even specific loops final performance much important compilation time like embedded applications 6 conclusions paper presented set heuristics improve efficiency process reduces register pressure software pipelined loops paper proposes new criteria decide two different alternatives contribute reduction decrease execution rate loop increase ii temporarily store registers memory spill code second alternative paper also contributes new criteria select spilling candidates many ones proposals evaluated using registerconscious software pipeliner however orthogonal could applied algorithm experimental evaluation done large collection loops perfect club benchmark impact different heuristics evaluated terms effectiveness efficiency terms effectiveness heuristics proposed reduce cases execution rate memory traffic respect original proposals terms efficiency reduction contributes real increase performance particular dynamic performance loops fill available registers increases factor ranges 125 168 memory traffic also reduced factor ranges 077 057 reduction execution time memory traffic achieved expenses reasonable increase compilation time worst case scheduler requires 18 times original time whole workbench dynamic performance increases factor ranges 107 138 memory traffic reduced factor ranges 09 07 scheduler manages compile loops less one minute configuration 64 registers less 35 minutes configuration although heuristics proposed contribute better registerconstrained schedules additional work needed tune several parameters like traffic quantity factors analyze real effect different architectural configurations also shown depending configuration spilling candidates either variables uses suggests dynamic process scheduler decides onthefly specific values parameters takes account variables uses may lead better schedules r realistic resourceconstrained software pipelining algorithm software pipelining spill code minimization techniques optimizing compilers perfect club benchmarks effective performance evaluation supercomputers coloring heuristics register allocation improvements graph coloring register allocation register allocation via hierarchical graph coloring register allocation spilling via graph coloring approach scientific array processing architectural design ap120bfps164 family overlapped loop support cydra 5 compiling cydra 5 stage scheduling technique reduce register requirements modulo schedule meeting new model loop cyclic register allocation register allocation using cyclic interval graphs new approach old problem circular scheduling new technique perform software pipelining systolic array optimizing compiler heuristics registerconstrained software pipelining quantitative evaluation register pressure software pipelined loops hypernode reduction modulo scheduling register requirements pipelined pro cessors software pipelining parisc compilers scheduling techniques easily schedulable horizontal architecture high performance scientific computing register allocation software pipelined loops iterative modulo scheduling algorithm software pipelining loops software pipelining showdown optimal vs heuristic methods production compiler software pipelining register allocation spilling tr overlapped loop support cydra 5 spill code minimization techniques optimizing compliers coloring heuristics register allocation register allocation via hierarchical graph coloring circular scheduling register allocation software pipelined loops register requirements pipelined processors lifetimesensitive modulo scheduling compiling cydra 5 improvements graph coloring register allocation iterative modulo scheduling software pipelining register allocation spilling software pipelining stage scheduling hypernode reduction modulo scheduling software pipelining showdown heuristics registerconstrained software pipelining quantitative evaluation register pressure software pipelined loops systolic array optimizing compiler conversion control dependence data dependence scheduling techniques easily schedulable horizontal architecture high performance scientific computing register allocation myampersandamp spilling via graph coloring ctr javier zalamea josep llosa eduard ayguad mateo valero software hardware techniques optimize register file utilization vliw architectures international journal parallel programming v32 n6 p447474 december 2004 alex alet josep codina antonio gonzlez david kaeli demystifying onthefly spill code acm sigplan notices v40 n6 june 2005 xiaotong zhuang santosh pande differential register allocation acm sigplan notices v40 n6 june 2005 xiaotong zhuang santosh pande allocating architected registers differential encoding acm transactions programming languages systems toplas v29 n2 p9es april 2007 javier zalamea josep llosa eduard ayguad mateo valero twolevel hierarchical register file organization vliw processors proceedings 33rd annual acmieee international symposium microarchitecture p137146 december 2000 monterey california united states josep codina josep llosa antonio gonzlez comparative study modulo scheduling techniques proceedings 16th international conference supercomputing june 2226 2002 new york new york usa javier zalamea josep llosa eduard ayguad mateo valero modulo scheduling integrated register spilling clustered vliw architectures proceedings 34th annual acmieee international symposium microarchitecture december 0105 2001 austin texas bruno dufour karel driesen laurie hendren clark verbrugge dynamic metrics java acm sigplan notices v38 n11 november javier zalamea josep llosa eduard ayguad mateo valero register constrained modulo scheduling ieee transactions parallel distributed systems v15 n5 p417430 may 2004