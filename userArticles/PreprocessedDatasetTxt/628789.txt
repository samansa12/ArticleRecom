combining belief networks neural networks scene segmentation concerned problem image segmentation pixel assigned one predefined finite number labels bayesian image analysis requires fusing together local predictions class labels prior model label images following work consider use treestructured belief networks tsbns prior models parameters tsbn trained using maximumlikelihood objective function em algorithm resulting model evaluated calculating efficiently codes label images number authors used gaussian mixture models connect label field image data paper compare approach scaledlikelihood method local predictions pixel classification neural networks fused tsbn prior results show higher performance obtained neural networks evaluate classification results obtained emphasize maximum posteriori segmentation also uncertainty evidenced eg pixelwise posterior marginal entropies also investigate use conditional maximumlikelihood training tsbn find gives rise improved classification performance mltrained tsbn b introduction concerned problem image segmentation pixel assigned one nite number classes work applied images outdoor scenes class labels sky road vegetation etc scenes typically complex involving many dierent objects objects highly variable eg trees means modelbased approaches readily applicable much work scene segmentation based approaches rst segment whole image regions classify region carry task successfully important classify region using attributes also take account context regions taking account context handled two ways either searching consistent interpretation whole scene taking account local context region nds 16 examples wholescene method 34 29 localcontext method used 50 51 major problem approaches process region creation unreliable leading oversegmentations alternative approach allows segmentation emerge along classication process formulated bayesian framework using prior model represents knowledge likely patterns labels image likelihood function describes relationship observations class labels 1 two main types prior model investigated called noncausal markov random elds mrfs causal mrfs statistical image modelling literature graphical models community two types models known undirected directed graphical models respectively 21 early work bayesian image modelling concentrated noncausal mrfs see eg 3 13 27 one disadvantage models suer high computational complexity example problem nding maximum posteriori map interpretation given image general nphard alternative causal mrf formulation uses directed graph commonly used form models treestructured belief network tsbn structure illustrated figure 1 image modelling standard dependency structure quadtree one attractive feature tsbn model hierarchical multiscale nature longrange correlations readily induced contrast noncausal mrfs typically nonhierarchical structure also shall see inference tsbns carried time linear number pixels using sweep tree leaves root back graphical models literature inference procedure known pearls messagepassing scheme 35 algorithm also known upwarddownward algorithm 40 24 generalisation trees standard baumwelch forwardbackward algorithm hmms see eg 37 one disadvantage tsbns random eld nonstationary example figure 1 common parent fourth fth pixels left x l root node whilst third fourth pixels share parent layer give rise blocky segmentations tsbn models used number authors image analysis tasks bouman shapiro 5 introduced model using discrete labels nodes image segmentation task perez et al 36 discussed map mpm maximum posterior marginal inference tsbns image processing 1 early work direction carried feldman yakimovsky 10 tasks laferte et al 19 20 extended model using multiscale feature pyramid image decomposition using em algorithm parameter estimation cheng bouman 6 investigated trainable multiscale models using decision trees compactly represent conditional probability tables cpts model tsbn models also used continuouslyvalued gaussian processes one two dimensions generalisation kalman lter models chains trees studied number groups notably prof willskys group mit developed theory derived fast algorithms problems optical ow estimation surface reconstruction texture segmentation 2 26 11 25 see also 17 also crouse et al 7 used multiscale tsbn model wavelet coecients debonet viola 9 used interesting treestructured network image synthesis using nongaussian densities however require prior classlabel images work tsbns discretevalued nodes germane purposes mentioned exact inference procedures noncausal mrfs general nphard however note important recent work approximate inference procedures graphs using local probability propagation schemes 46 local schemes guaranteed give correct answer graphs without loops see 35 give approximate answers gridstructured graphs typically used image analysis work messagepassing schemes loopy graphs includes decoding errorcorrecting codes 12 work 46 criticised basis ie nonhierarchical noncausal mrf models used although possible apply similar message passing schemes loopy hierarchical graphs see section 7 discussion work carried database images outdoor scenes see section 4 details colour images corresponding label images available paper makes number contributions investigate eects adaptation training parameters tree response data two methods considered rst tree trained maximise probability labelled images call maximum likelihood ml training similar work laferte et al 20 allow nonstationary parameterisation tree ects regularities image database second method tree trained maximise probability correct label image given raw input image call conditional maximum likelihood cml training quality mltrained tsbn model evaluated comparing well codes test set label images performance compared number coding schemes including jpegls lossless codec also provide analysis tsbn coding allows us quantify benets using higher levels tree correspond longerrange correlations tsbn comprises prior aspect bayesian model also require likelihood term whereby image data uences segmentation direct approach produce generative model probability density pixel features given image class example 5 class conditional densities modelled using gaussian mixture models compare approach alternative one neural networks used make local predictions given pixel features predictions combined prior principled manner using scaled likelihood method see section 22 details eects combining methods ml cml trained trees also investigated evaluate performance segmentation algorithms using pixelwise classication rates also analysis posterior pixelwise entropies conditional probabilities label images given colour images remainder paper organised follows section 2 describe tsbn model scaledlikelihood method detail explain inference carried section 3 derive equations training tsbn using maximum likelihood ml conditional maximum likelihood cml methods image coding using tsbns section 4 give details data training various models section 5 results presented concerning image coding estimation cpts section 6 analyse classication results network neural network models image segmenta tion 21 generative model model data illustrated figure 1 observed data assumed generated underlying process x x tsbn network arranged layers highest level level one node x 0 children level 1 lower levels denoted x l level l fundamental property belief networks encoding conditional independences 35 15 layered structure means distribution x n 1 n l given coarser scale nodes dependent x n 1 indeed treestructure network means node x n dependent single node x n 1 typically experiments parent node four children giving rise quadtreetype architecture node multinomial variable taking one c class labels labels used segmentation eg road sky vehicle etc 2 links nodes dened conditional probability tables cpts parent root node unconditional prior distribution instead cpt nodes x l onetoone correspondence observed data observed data case features derived blocks pixels rather individual pixels raw image model observations illustrated figure 1 observation dependent corresponding variable x l 2 note necessary hidden nodes cclass multinomial variables used convenience gives rise simple initialisation tsbntraining described section 45 figure 1 1d graphical model illustrating small treestructured belief network layers x denoted x l case denotes raw image information 22 likelihood model described fully generative model dened cpts root downwards since pixel composed number components feature space p density function one method use gaussian mixture density used 5 described section 44 however database using provides raw data labelled segmentations x l given information natural train classiers eg neural networks predict x l well known neural networks trained various error functions including crossentropy meansquared error approximate posterior probabilities p x l fusion predictions belief network x cannot achieved immediately requires p jx l terms however using bayes theorem obtain inference x data xed factors p need considered dening scaled likelihood lx l location obtain replacing lx l principled method fusion local predictions global prior model p x notation p denotes estimates desired probabilities method combining neural networks belief networks suggested hmms smyth42 morgan bourlard31 interesting connection scaled likelihood mutual information ix random variables x ix mutual information expected value log scaled likelihood described would need train separate neural network predict p x pixel clearly undesirable terms computational eort amount data required solution adopted train one neural network make position coecients pixel part inputs neural network use scaled likelihood require images turn p x depend position pixel know ensemble images regularities sky appearing top number ways approach estimation p x one train network predict class labels given position pixel alternative would use relationship p x approximate integral average appropriate feature vectors experiments see section 6 compared spatially one derived pixelwise marginals mltrained tsbn results obtained similar potential advantage scaledlikelihood method generative model p jx may quite complex although predictive distribution p x jy actually quite simple means generative approach may spend lot resources modelling details p jx particularly relevant task inferring x 23 inference given new image wish carry inference x l given probabilistic model computing posterior p x would highly expensive would require enumerating possible c k states x l two alternatives computationally feasible computation posterior marginals p x l giving rise segmentation based maximum posterior marginals mpm ii map interpretation data x achieved pearls message passing schemes described 35 schemes noniterative involve one upward one downward pass tree details given mpm computation appendix along method scaling calculation avoid ow computation marginal likelihood p x l 3 training tsbn assumed parameters used dene p x known fact estimated training data let denote parameters prior probabilities root node cpts tree let x il denote possible values x let pa ik set possible values taken pa parent x parameter ikl denotes cpt entry l 1 simplicity symbols x pa dropped probability written p x il jpa ik training prior model assumed number observation images associated labelled images x lm available index images training set let denote parameters likelihood model p yjx discuss turn maximum likelihood training x31 conditional maximum likelihood training x32 3 note nding conguration x likely equivalent nding conguration maximises p x x 31 maximum likelihood maximum likelihood training parameter vector estimated ml see likelihood model parameters tsbn model parameters estimated separately choosing likelihood model parameters maximise prior model parameters maximise assuming likelihood model xed obtain ml j optimisation carried using em algorithm uses bottomup topdown message passing infer posterior probabilities hidden nodes estep uses expected counts transitions reestimate cpts 40 24 21 reestimation formulas derived directly maximising baums auxiliary function q new estimated parameter vector x denotes hidden variables x nx lm pattern update entry cpt given joint probability obtained locally using message passing scheme see appendix details update gives separate update link tree given limited training data un desirable set variables sharing cpt denoted x em parameter update given information available one still carry maximum likelihood training model parameter sets would adapted case known unsupervised learning described tsbns 20 disadvantage scaledlikelihood method cannot used unsupervised learning p available 5 section iii c appears parameters reestimated test image unusual standard pattern recognition methodology model parameters estimated training data xed applied test images follow standard methodology 32 conditional maximum likelihood cml procedure objective predict correctly labels x l associated evidence parameters estimated maximising probability correct labelling given evidence cml analogy boltzmann machine observe computing conditional probability requires computation 1 probability p clamped phase ie x lm xed 2 probability py j freerunning phase xed 4 assume likelihood model p yjx objective function viewed function carry optimisation equation 8 take logarithms dene log log p used subscripts c f mean clamped free using decomposition simplied log log log nd cml equation 8 need maximise log p 4 use terminology clamped freerunning follows 18 unfortunately em algorithm applicable cml estimation cml criterion expressed rational function14 however maximisation equation 11 carried various ways based gradient l speech analysis 18 39 methods based gradient ascent used scaled conjugate gradient optimisation algorithm 30 4 used work use search method need calculate gradient l wrt letting ik jx lm shown see appendix b details ikl ikl ikl n ikl obtained propagating x lm respectively see equation 23 maximising l must ensured probability parameters remain positive properly normalised softmax function used meet constraints dene l 0 e z ikl 0 z ikl new unconstrained auxiliary variables ikl always sums one l index construction gradients wrt z ikl expressed entirely terms ikl n ikl n n ikl z ikl ikl ikl l 0 33 image coding tsbn provides generative probabilistic model label images evaluate quality model label process evaluating likelihood test set label images model calculating log 2 p x l labelled pixels image obtain coding cost bitspixel minimum attainable coding cost entropy bitspixel generating process computation p intractable mrf models compare tsbn results lossless jpegls codec 44 45 available httpwwwhplhpcomloco 331 coding cost tsbn model using tsbn model distribution images marginal likelihood label image x l calculated eciently root node x 0 tree see appendix a2 also consider eect truncating tree level root tree case instead one large tree image model consists number smaller trees correlations dierent trees ignored allows us quantify benets using higher levels tree correspond longerrange correlations priors smaller trees calculated propagating prior downward cpts obtain prior root likelihood image truncated model simply product likelihoods subimages computed smaller trees 4 experimental details 41 data colour images outdoor scenes sowerby image database 5 british aerospace used experiments database contains urban rural scenes feature variety everyday objects roads cars houses lanes elds various places near bristol uk scenes photographed using smallgrain 35mm transparency lm carefully controlled conditions image database digitised calibrated scanner generating high quality 24bit colour representation colour images corresponding label images provided database label images created oversegmenting images hand labelling region produced 92 possible labels organised hierarchical system combined labels produce seven classes namely sky vegetation road markings road surface building street furniture mobile object instance class street furniture combination many types road sign telegraph pole 5 database made available researchers please contact dr andy wright dr gareth rees advanced information processing department advanced technology centre sowerby bae systems ltd po box 5 filton bristol bs34 7qw uk email garethsreesbaesystemscom details undefined vegetation road marking road surface building furniture mobile object figure 2 rural urban scene handlabelled classication original images b handlabelled classication right key describing labels used bounding object figure 2a shows two scenes test image set database one rural one urban figure 2b shows handlabelled classications dierent greylevels label image correspond seven dierent possible classes original 104 images divided randomly independent training test sets size 61 43 respectively fullresolution colour images size 512 768 pixels downsampled 128 192 regions size 4 4 pixels label reduced region chosen majority vote within region ties resolved ordering label categories refer reduced label images label images original label images longer used 42 feature extraction important step classication feature selection initially forty features extracted region among six features based r g b colour components ie mean variance overall intensity region colour hue angle sine cosine 1 rb 2grb2 used 34 r g b indicate means red green blue components respectively texture features greylevel dierence vectors gldv textural features 48 47 contrast entropy local homogeneity angular second moment mean standard deviation cluster shade cluster prominence gldv features extracted based absolute dierence pairs gray levels distance apart four angles x location region also included feature space described section 22 feature normalised using linear transformation zero mean unit variance training set useful limit number features used increasing number features increases free parameters need optimised neural network training phase generalised linear model glm using normalised features inputs softmax outputs 4 used feature selection input sum absolute values weights coming input trained glm calculated twentyone features sum larger unity retained selection procedure based idea important features tend give rise larger weights cf automatic relevance determination idea mackay neal 32 43 mlp training multilayer perceptrons mlps used task predicting p x l explained section 22 probabilities estimated mlp takes input nonpositional feature vector position pixel retained features produced feature vector region fed mlp 21 input nodes 7 output nodes one hidden layer trained classify region one seven classes activation functions output nodes hidden nodes softmax function tanh sigmoid functions respectively error function used training process crossentropy multiple classes see 4 scaled conjugate gradient algorithm used minimise error function training performed using 51000 regions extracted training image dataset validating independent validation dataset 15000 regions validation dataset used order choose optimal number hidden nodes mlp eventually best performance validation set obtained mlp nodes training dataset mlp training formed choosing randomly 150 regions class single image tried use equal numbers regions class training set mlp aim rebalancing training set give net better chance learn infrequent classes see 4 p 224 probabilities class training set mlp denoted estimated simply evaluating fraction training set data points class corresponding probabilities pixels whole training set images denoted turned 00070 ordering classes sky vegetation road markings road surface building street furniture mobile object two sets prior probabilities dierent almost uniformly distributed classes biased towards classes two four corresponding vegetation road surface respectively since training set mlp reweights classes according necessary consider eect scaled likelihood fact following 4 p 223 nd z input network pixel network output class k p c k jy compensated network output z normalising factor used make one hence see scaled likelihood p equal unimportant constant call prediction p c k jy given equation 14 compensated mlp prediction segmentation obtained choosing probable class pixel independently call raw mlp compensated mlp segmentations using uncompensated compensated predictions respectively 44 gaussian mixture model training section 43 described mlps trained relate image features labels alternative approach build classconditional density estimators class use along bayes rule make predictions following 5 20 used gaussian mixture models gmms task specically cluster program available httpwwwecepurdueedubouman used training set used mlp considered three dierent feature sets average r g values region ii 21 features used train mlp iii 40 features addition two dierent settings cluster program used allowing either diagonal full covariance matrices gaussians program selects number mixture components automatically using mdl criterion recommended initialisation starting three times many components features used gmms class combined prior probabilities class p c k given section 43 produce pixelwise classications overall classication accuracies 6872 4976 7595 7138 7745 7192 3full 3diag 21full 21diag 40full 40diag models respectively test set gmm model highest pixelwise performance namely 40full used experiments see section 6 details number mixture components 40full model seven classes 9 9 4 9 8 7 6 respectively found trained gmm sometimes makes condent misclassications cause ow problems evaluating conditional probability p tsbn x l jy conditional probability ground truth labelling x l given image see section 64 reason replaced likelihood term p jx l minimum value needed avoid ow mltsbn 45 tsbn training tsbn used basically quadtree except six children root node take account 23 aspect ratio images downsampled image total 128 192 pixels took pixel downsampled image leaf node belief network built eightlevel tsbn total 32756 links nodes adjacent levels link separate cpt large training set would needed ensure cpts well determined turn implies huge computational resources could needed order nd suitable minimum cml objective function practice approach clearly impractical one technique dimensionality reduction case tie cpts experiments cpts level constrained equal except transition level 0 level 1 table separate exibility allows knowledge broad nature scenes eg sky occurs near top images learned network indeed ected learned cpts see section 5 training mltsbn network parameters initialised number dierent ways found highest marginal likelihood training data obtained initial values computed using probabilities derived downsampled version images sparsedata problem appeared initial values cpts pairings occur training data 6 dealt problem adding small quantity conditional probability pc k jc normalising modied probabilities used case least one 1c plot likelihood iteration number levelled iterations database pixels unlabelled assuming values missing random treated uninstantiated nodes easily handled belief network framework cml training initialised mltsbn solution plots conditional likelihood iteration number levelled 44 iterations cml training scaled conjugate gradient optimisation gmm mlp predictors upward propagation tree takes around 10s downward propagation around 40s sgi r10000 processor tree 30000 nodes made available c code tsbn training inference along matlab demonstration calls functions httpwwwdaiedacukdaidbpeoplehomesckiwcodecbnhtml 46 combining pixelwise predictions trees gmm mlp local predictors ml cml trained tsbns gives rise large number possible combinations pixelwise predictors trees ones investigated 6 reason important consider cpt entry set zero em algorithm move away zero training 1 raw gmm pixelwise predictions 2 compensated gmm pixelwise predictions spatiallyuniform compensation 3 compensated gmm pixelwise predictions using marginals mltsbn 4 gmm likelihood 5 gmm likelihood 6 raw mlp pixelwise predictions 7 compensated mlp pixelwise predictions spatiallyuniform compensation 8 compensated mlp pixelwise predictions using marginals mltsbn 9 tsbn methods calculated scaled likelihoods described section 43 map inference used pixelwise predictions entries 3 8 compensation using marginals mltsbn note dierent compensation probabilities used six regions image dened six cpts root level 1 performance methods investigated sections 5 6 5 results tsbn training section describe results training tsbn using ml cml training rst discuss labelimage coding results using mltrained tree inspect learned cpts mltrained tsbn cmltrained tsbn 51 image coding results section present results comparing mltrained tsbn lossless jpeg coding relevant theory described section 33 details tsbn training given section 45 06114truncation level bitspixel figure 3 bit rate bitspixel function truncation level tsbn average bit rate tsbn model 02307 bitspixel bpp comparison purposes jpegls codec gave average bit rate 02420 bpp also tried compressing label images using coding using unix utility gzip gave 03779 bpp fact similar level compression performance obtained jpegls tsbn suggests tsbn reasonably good model label images using truncated tree scheme discussed section 33 analyse tsbn results figure 3 shows bit rate bitspixel evaluated function truncating tree levels 0 7 time level 4 reached corresponding 8 8 block size almost benet attained 52 learned cpts cpts derived using ml training shown figure 4 note six separate cpts used transition root node level 1 explained section 45 also calculate prior marginals node tree simply taking prior root node passing relevant cpts path root node consideration 7 fact six cpts root level 1 transition means eect six dierent prior marginals levels 1 7 dened 23 aspect ratio image prior marginals shown figure 5 may easy interpret cptsmarginals permutation state labels 7 also achieved using pearls propagation scheme outlined appendix every leaf node uninstantiated node corresponding permutations incoming outgoing cpts would leave overall model unchanged however appears downsampling initialisation means large problem analysing figures 4 5 see 1 prior marginals level 7 ect overall statistics images sky vegetation road surface classes frequently occurring sky class likely found top half images road surface bottom half similar patterns detectable level 1 figure 5 although vegetation label less prevalent upper half level 2 trained cpts levels 1 7 exhibit strong diagonal structure implying children likely inherit parents class 3 level 0 level 1 cpts need read conjunction roots prior distribution provide good explanation level 1 prior marginals although laferte et al 20 carried em training tsbn note estimated cpts tied layerbylayer basis data figures 4 5 show relaxing constraint useful cpts prior marginals obtained cml training similar shown figures 4 5 respectively probably due fact cml training initialised mltsbn solution gmm mlp predictors 6 segmentation results performance evaluation turn classication testing images often classication performance evaluated pixelwise accuracies however complex realworld classication task tell whole story number factors concern us notably fact predicting labels pixels image spatial coherence important also note fractions pixels dierent classes tremendously dierent groundtruth labels used assessing performance 100 percent correct downsampling process also inaccuracies handlabelling process therefore dicult task assess quality classication derived various methods may also depend uses classication put early reference assessing quality segmentations 22 recently prior root node b root node level 1 c level 1 level 2 level 2 level 3 level 3 level 4 f level 4 level 5 g level 5 level level 6 level 7 figure 4 estimated prior root cpts ml training eightlevel belief network trained training images area black square proportional value relevant probability prior probabilities root node b six independent cpts links root node six children rst level ch cpts links adjacent levels level 1 level 7 respectively seven labels 1sky 2vegetation 3road marking 4road surface 5building 6street furniture 7mobile object cpts entry 11 top lefthand corner read level l indexing rows level l indexing columns level 0 level 1 level 2 level 3 level 4 level 5 level level 7 key vegetation road marking road surface building street furniture mobile object vegetation road marking road surface building street furniture mobile object vegetation road marking road surface building street furniture mobile object vegetation road marking road surface building street furniture mobile object figure 5 prior marginals training ml algorithm area black square proportional value relevant probability see text details realisation aim segmentation may return single segmentation multiple solutions 33 probability distribution segmentations p x l jy posterior distribution explored many ways describe two namely posterior marginal entropies ii evaluation conditional probability p jy x l ground truth image given input data section compare performance classication based smoothness segmented image section 61 pixelwise prediction accuracies section 62 marginal entropies section 63 conditional probability section 64 61 smoothness rural scene figure 2 figure 6 shows classications using combinations outlined section 46 classications obtained singlepixel methods typically lot highfrequency noise due locally ambiguous regions ml cmltrained trees tend smooth noise similar smoothing obtained using majority lter 23 one simply chooses common class within window centered pixel interest however one drawback majoritylter smoothing reasonablysized window tends remove ne detail road markings contrast seems tsbn methods yield something like adaptive smoothing depending strength local evidence also note majorityltering return probability distribution segmentations 62 pixelwise classication accuracy table 1 shows pixelwise classication accuracy class overall accuracy ten methods listed section 46 tsbn methods map segmentation result reported mpm results similar although generally worse tenths one percent noticeable feature performance obtained mlp methods superior gmm methods looking results detail notice raw results gmm mlp columns 1 improved compensation columns 2 7 resp compensated methods simply give figure classication rural scene raw gmm pixelwise predictions ii raw mlp pixelwise predictions iii compensated gmm pixelwise predictions iv compensated mlp pixelwise predictions v map segmentation segmentation mlp segmentation gmm tsbn viii map segmentation mlp weight frequently occurring classes seen comparing columns 1 2 6 7 small dierences spatially uniform compensation columns 2 7 mltsbn marginal compensation scheme columns 3 8 columns 4 8 combine pixelwise evidence mltsbn gmm mlp local models perhaps surprising performance decreases compared columns 3 7 respectively fair comparison methods columns 3 7 use marginals mltsbn correlation structure columns show performance gmm mlp local models combined trees trained using cml method relevant data cases performance better fusion mltsbn mlp method obtains best overall performance comparison note mccauley engel 28 compared performance bouman shapiros smap algorithm pixelwise gaussian classier remote sensing task found overall classication accuracy smap 36 higher 934 vs 898 reasons superior performance mlp experiments entirely clear however note test images relatively diverse set images although drawn distribution training images may features important mlp classier similar training test images features whose distribution modelled gmms vary training test sets contrast evaluations literature eg 28 use single test image training data drawn subset pixels case issue interimage variability arise also note comparison gmm mlp classiers carried using training set particular size composition dierent results might obtained factors varied 63 pixelwise entropy interested understanding uncertainty described p x l jy appears computation joint entropy conditional distribution intractable however posterior marginal entropies readily computable posterior marginals p x l table 1 performance 10 methods showing percentage correct class overall second column table gives overall percentage class test images class percentage vegetation 4012 6104 7892 7746 6792 7572 7932 9241 9040 8167 9045 road markings 017 5514 4226 4433 4309 2736 7861 6897 6791 7004 6791 road surface 395 6214 7818 8045 7010 7345 9452 9716 9626 9499 9685 building 611 4419 4698 4967 6370 7492 6769 4443 5273 7940 6460 street furniture 135 2858 1405 1377 2097 858 2489 396 462 1007 663 mobile object 057 5885 4305 4310 7276 7623 4913 2883 3215 7889 4474 overall 6371 7745 7794 7100 7585 8572 9016 8938 8738 9068 kjy 8 images displaying posterior marginal entropies shown figure 7 pertaining original image shown right figure 2 expected pixelwise entropy reduced use tsbn particularly eective cml trees notice pixels signicant posterior marginal entropy good indicators pixels misclassied especially true cml combination figure 7 property could well useful information later stage processing 64 conditional probability model developed good one p x l jy ascribe high probability ground truth labelling x l dierent image models compared terms relative values p jy particular compare tsbn image models independent pixel models ignoring spatial correlations obtain pmlp mlp local prediction similarly gmm local prediction tsbn p x l jy calculated follows 8 revision paper became aware calculation posterior marginal entropies proposed independently perez et al 36 determine condence maps c 00487 figure 7 posterior marginal entropies mlp predictor compensated pixelwise predictions b mltsbn c cmltsbn greyscale black denotes zero entropy white denotes 245 bits number underneath plot average pixelwise posterior entropy binary image showing misclassied labels bright correctlyclassied labels dark equation 16 follows equation 15 p jx l denominator evaluated methods outlined appendix a2 complexity arises calculation pixels label pmlp x jy pixels simply ignored p tsbn x l jy unlabelled pixels ignored numerator denominator equation 17 achieved setting vector appropriate nodes vector ones see appendix details figure 8 plot 1 log p x l jy various models panel b shows 43 test images posterior probability mlpcmltsbn method larger compensated mlp using independentpixel model panel shows similar comparison using gmm predictor cmltsbn method better 41 43 cases notice also relative scales plots especially gmm model makes condent mistakes pixels thereby dragging average posterior probability logp gmm logp gmm cmltsbn 05logp mlp logp mlp cmltsbn b figure 8 comparison log p x l jyn compensated gmm vs gmm compensated mlp large number similar plots made comparison posterior probabilities mltrained tsbn independent models comes roughly equal numbers better coded two models gmm mlp predictors cmltrained tsbn mlp prediction better methods 39 43 test images remaining four mltsbn mlp wins paper made number contributions used em algorithm train mltsbn observed learned parameters ect underlying statistics training images quality probabilistic model evaluated coding terms found comparable stateoftheart methods truncated tree analysis shows scales correlations important compared performance gmm mlp pixelwise classiers sizable realworld image segmentation task performance discriminativelytrained mlp found superior classconditional gmm model also shown scaledlikelihood method used fuse pixelwise mlp predictions tsbn prior compared conditional maximum likelihood cml training tree maximum likelihood ml training number dimensions including classication accuracy pixelwise entropy conditional probability measure p x l jy problem evaluating segmentations old one full answer may well depend decisiontheoretic analysis takes account enduse segmentation eg automated driving system however one attractive feature tsbn framework aspects posterior uncertainty computed eciently eg posterior marginal entropies discussed section 63 architectures ultimate image model know run generatively give rise blocky label images number interesting research directions try overcome problem bouman shapiro 5 suggest making complex crosslinked model problem inference becomes much complex one needs use junction tree algorithm see eg 21 one interesting idea suggested 5 retain pearlstyle message passing even though exact idea analysed 12 46 another approach inference use alternative approximation schemes recognition network used helmholtz machines 8 meaneld theory 41 alternative creating crosslinked architecture retain tsbn move away rigid quadtree architecture allow treestructure adapt presented image formulated bayesian fashion setting prior probability distribution treestructures initial results approach reported 49 43 believe general area creating generative models image data nding eective inference schemes fruitful area research appendix pearls probability propagation procedures describe pearls scheme probability propagation trees computation marginal likelihood p x l j scaling procedure algorithm avoid ow a1 pearls scheme rst consider calculation probability distribution p xje node x tsbn given instantiated nodes evidence e consider tree fragment depicted figure 9 based figure 414 35 p xje depends two distinct sets evidence evidence subtree rooted x denoted e x evidence rest tree denoted e x shall assume node nite number states c node dierent number states adds extra notation necessary application bayes rule together independence property tsbns yields product rule x partitioning e x e dened recursively assumed node x n children ik kth value node x known message sent node x child node x given recursively z z 0 2sx z 0 2sx z k kth state node z sx denotes siblings x ie children z excluding x normalising factor values x z sum 1 fact z e sx denotes evidence siblings x x z known message sent node x e e x x z figure 9 fragment causal network showing incoming message named message shown solid arrows outgoing message message broken arrows node x parent z propagation procedure completed dening boundary conditions root leaves tree vector root tree equal prior probabilities classes leaves tree vector vector ones node uninstantiated equal vector single entry 1 entries 0 corresponding instantiated state computation p xje node tree performed using upward phase message passing downward phase message passing nd maximum posteriori conguration hidden variables x given evidence e use similar message passing scheme described section 53 35 posterior marginal required em updates cml derivatives given set nodes siblings node x denotes message sent node pa node show partition e pa e sx rst calculate p using conditional independences described tree obtain pa ik computed dividing sides equation p e calculated a2 marginal likelihood consider procedure computing p ej assuming x 0 root node used root node e x0 empty a3 scaling pearls probability propagation order understand scaling required implementing message propagation consider two distinct message passing schemes separately firstly consider denition x equation 21 x probability given evidence x gives long normalising factor applied time calculation message case scaling needed x consider denition x equation 19 x equation 20 values node x product messages sent children child node forms weighted sum values form message sent parent elementwise multiplication messages equation 19 weighted sum calculation equation 20 cause numerical values vectors decrease exponentially distance leaves tree scale x three goals 1 keeping scaled x within dynamic range computer nodes tree 2 maintaining local propagation mechanisms pearls probability propagation 3 recovering true values end computation achieved recursive formulae children x equations initialised leaves x value gives reasonable scaling x used work unscaled value x computed using x x x product scaling coecients subtree rooted x including x fact interested calculation p xje necessary worry unduly scaling factors vector simply rescaled node required p xje calculated scaled vector vector requiring p xje sums 1 however scaling important wish calculate marginal likelihood p ej 9 referring back equation 26 nd dx0 product scaling factors used propagation procedure since dx0 could machine dynamic range compute log p log dx0 appendix b calculation derivatives cml optimisation appendix calculate gradient wrt section 32 suppress dependence p yjx notational convenience first note p j written sum possible values x tsbn using conditional independence relations p xj easily decomposed product transition probabilities links following ideas krogh18 hmms derivative l f wrt ikl ikl ikl ikl 9 scaling issues discussed perez et al 36 appears addressed issue scaling computation posterior marginals paper explicitly scaling computation p ej ikl ikl ikl ikl ikl step equation 30 equation 31 derived fact ikl appear product state l pa state k derivative term l c calculated similar manner except summation variables tree taken hidden variables x acknowledgements work funded epsrc grant grl03088 combining spatially distributed predictions neural networks epsrc grant grl78181 probabilistic models sequences authors gratefully acknowledge assistance british aerospace project making sowerby image database available us also thank dr andy wright bae helpful discussions dr ian nabney help netlab routines neural networks dr gareth rees bae discussions segmentation metrics dr john elgy introducing us work 5 prof kevin bowyer pointing work 33 also thank three anonymous referees associate editor prof charles bouman helpful comments advice considerably improved manuscript r computer vision modelling estimation multiresolution stochastic processes statistical analysis dirty pirtures neural networks pattern recognition multiscale random field model bayesian image segmentation trainable context model multiscale segmentation helmholtz machine decision theory arti revolution belief propagation graphs cycles stochastic relaxation inequality rational functions applications statistical estimation problems introduction bayesian networks statistical pattern recognition image analysis multiresolution gaussmarkov random eld models texture segmentation hidden markov models labeled sequences graphical models dynamic measurement computer generated image segmentations remote sensing image interpretation bayesian belief networks tool stochastic parsing likelihood calculation class multiscale stochastic models statistical methods automatic interpretation digitally scanned comparison scene segmentations smap neural networks statistical recognition continuous speech bayesian learning neural networks textured image segmentation returning multiple solutions probabilistic reasoning intelligent systems networks plausible inference tutorial hidden markov models selected applications speech recognition neural network classi hidden neural networks framework hmmnn hybrids parameter estimation dependence tree models using em algorithm hidden markov models fault detection dynamic systems dynamic positional trees structural image analysis locoi lossless image compression algorithm principles standardization jpegls correctness belief propagation gaussian graphical models arbitrary topology comparative study texture measures terrain classi dynamic trees image labelling neural network use neural networks region labelling scene un derstanding tr ctr neil lawrence andrew j moore hierarchical gaussian process latent variable models proceedings 24th international conference machine learning p481488 june 2024 2007 corvalis oregon todorovic michael c nechyba dynamic trees unsupervised segmentation matching image regions ieee transactions pattern analysis machine intelligence v27 n11 p17621777 november 2005 amos j storkey christopher k williams image modeling positionencoding dynamic trees ieee transactions pattern analysis machine intelligence v25 n7 p859871 july sanjiv kumar martial hebert discriminative random fields international journal computer vision v68 n2 p179201 june 2006 todorovic michael c nechyba interpretation complex scenes using dynamic treestructure bayesian networks computer vision image understanding v106 n1 p7184 april 2007 richard j howarth spatial models widearea visual surveillance computational approaches spatial buildingblocks artificial intelligence review v23 n2 p97155 april 2005 simone marinai marco gori giovanni soda artificial neural networks document analysis recognition ieee transactions pattern analysis machine intelligence v27 n1 p2335 january 2005