generalization ability folding networks abstractthe information theoretical learnability folding networks successful approach capable dealing tree structured inputs examined find bounds vc pseudo fat shattering dimension folding networks various activation functions consequence valid generalization folding networks guaranteed however distribution independent bounds generalization error cannot exist principle propose two approaches take specific distribution account allow us derive explicit bounds deviation empirical error real error learning algorithm first approach requires probability large trees limited priori second approach deals situations maximum input height concrete learning example restricted b introduction one particular problem connectionistic methods dealing structured objects find possibility makes processing data priori unlimited size possible connectionistic methods often use distributed representation objects vector space fixed dimension whereas lists trees logical formulas terms graphs etc consist unlimited number simple elements connected structured way possibly unlimited size admit direct representation finite dimensional vector space often structured data possesses recursive nature case processing structures possible standard neural networks enlarged recurrent connections mimic recursive nature structure 9 10 networks capable dealing trees lists arbitrary height length example dynamics proposed large number approaches dealing adaptive processing structured data raam lraam folding networks name 11 24 28 methods differ single processing step looks trained share method entire tree processed simple mapping applied recursively input tree according tree structure tree encoded recursively distributed representation code used standard connectionistic methods regarding folding networks encoding trained simultaneously classification trees learned using modification backpropagation 11 approach used successfully several areas application 7 11 21 22 25 raam lraam train encoding simultaneously dual decoding composition yields identity 24 28 classification encoded trees trained separately focus capability learning dynamics principle consider information theoretical learnability ie question whether finite number examples contains enough information learning given finite set data function dynamics identified mirrors underlying regularity decided underlying regularity cannot modeled function question whether valid generalization finite set examples underlying regularity possible function class standard feedforward networks question answered affirmative combinatorial quantity called vc dimension finite fixed architecture socalled pac learnability guaranteed moreover learning algorithm small empirical error generalizes well one find explicit bounds accuracy generalization depend number parameters number training patterns independent concrete distribution 3 order use folding networks learning mechanism necessary establish analogous results recurrent case question whether valid generalization architecture possible answered negatively none approaches learn principle question answered positively learning algorithm small empirical error good learning algorithm information theoretical point view course may exist differences efficiency algorithms algorithm may turn computationally intractable whereas another approach yields good solution short time however mainly difference empirical error optimization number examples necessary valid generalization least cases depends function class used learning learning algorithm unfortunately situation turns difficult recursive case standard feedforward networks exists work estimates vc dimension recurrent folding networks 14 19 combinatorial quantity finiteness characterizes distribution independent learnability arbitrary inputs dimension infinite due unlimited input length ie ability dealing inputs arbitrary size even leads ability storing arbitrary information finite number parameters since unlimited input space used purpose way consequence distribution independent bounds generalization error cannot exist situations order take specific distribution account modify two approaches literature guarantee learnability even infinite vc dimension adequate stratification function class instead 2 27 approaches formulated binary valued function classes consider generalization error algorithm zero empirical error generalize situation function classes arbitrary error applies folding networks standard learning algorithms well allows us establish information theoretical learnability folding networks first define dynamics folding networks formally mention facts learning theory add mentioned two formalisms allow us obtain concrete bounds deviation empirical error real error concrete situations estimations socalled vc pseudo fat shattering dimension folding architectures quantities play key role concerning learnability bounds tell us distribution independent learnability cannot guaranteed principle derive concrete distribution data dependent bounds generalization error folding networks completeness recall definition standard feedforward network feedforward neural network consists finite set neurons connected acyclic graph connection equipped weight w ij 2 r input neurons neurons without predecessor neurons called computation units nonempty subset computation units specified output units computation units output neurons called hidden neurons computation unit equipped bias activation function inputs n outputs computes function output units defined recursively neuron ae x input unit term called activation neuron architecture network weights biases specified allowed vary r obvious way architecture stands set networks results specifying weights biases consequence network computes mapping composed several simple functions computed single neurons activation functions f single computation units often identical drop subscript cases following activation functions considered identity id perceptron activation ae standard sigmoidal function polynomial activation functions feedforward networks handle real vectors fixed dimension complex objects trees labels real vector space assume following tree fixed fanout k means nonempty node exactly k successors consequently tree either empty tree consists root labeled value subtrees 1 k latter case denote tree set trees defined denoted r k one use recursive nature trees construct induced mapping deals trees inputs vector valued mapping appropriate arity assume r l used encoding labels taken r mapping g r theta r k deltal r l initial context 2 r induce mapping l defined recursively follows definition used formally define recurrent folding networks folding network consists two feedforward networks compute functions respectively initial context 2 r l computes mapping folding architecture given two feedforward architectures inputs l outputs l inputs n outputs respectively context specified either input neurons 1 k delta l g called context neurons g referred recursive part network h feedforward part input neurons folding network architecture neurons 1 g following assume network contains one output neuron h understand folding network computes function value one think recursive part encoding part tree encoded recursively real vector r l starting empty tree encoded initial context leaf encoded via g using code proceeding way subtree c e f c f recurrence context neurons recurrent part feedforward part input tree leads computation figure 1 example computation folding network specific tree serves input network unfolded according structure input tree output value simply computed unfolded network via g using already computed codes k subtrees 1 k feedforward part maps encoded tree desired output value see fig 1 example computation lists dealt folding networks reduce recurrent networks except separation feedforward recurrent part definition coincides standard definition partial recurrent neural networks literature 12 practice recurrent folding networks trained gradient descent method like backpropagation structure backpropagation time respectively 11 30 used successfully several areas application including time series prediction control search heuristics classification chemical data graphical objects 7 22 25 similar mechanism proposed processing structured data lraam possible define analogy encoding function g mapping deltal set ae r l induced decoding k ae means complementary dynamics decodes value recursively order obtain label root via g 0 codes k subtrees via g 1 g k definition 2 lraam consists two feedforward networks compute g r mk deltal r l respectively vector 2 r l set ae r l computes mapping frequently lraam used following way one chooses fixed architectures networks g g trains weights composition yields identity considered trees afterwards g g combined standard networks order approximate mappings trees real vector space vice versa second step feedforward architectures trained encoding decoding lraam remains fixed although lraam trained different way processing dynamics used classification structured data encoding part lraam combined standard network trained specific learning problem considering entire process obtain function class represented folding networks restrict learning functions trees real vector space hence following argumentation applies lraam mechanisms processing dynamics well however situation changes fix encoding learn feedforward network combined neural encoding lraam situation reduces learning standard feedforward networks trees identified fixed real input vectors 3 foundations learning theory learning deals possibility learning abstract regularity finite set data given fix input space x example set lists trees equipped oealgebra fix set f functions x 0 1 network architecture example unknown learned f purpose finite set independent identically distributed data drawn according probability distribution p x learning algorithm mapping x theta 0 1 selects function f pattern set function hopefully nearly coincides function learned write hm f x hm algorithm tries minimize real error p f hm f x p f z course error unknown general since probability p function f learned unknown concrete learning algorithm often simply minimizes empirical error dm f hm f x x dm f example standard training algorithm network architecture fits weights means gradient descent surface representing empirical error dependence weights first consider distribution dependent setting ie given fixed probability distribution p x algorithm called probably approximately correct pac sup holds ffl 0 weakest condition following holds bounds number examples guarantee valid generalization exist learning algorithm bounds independent unknown function learned practice somewhat stronger condition desirable existence one maybe inefficient algorithm satisfactory want use learning algorithm efficient yields small empirical error property learning algorithm empirical error representative real error captured property uniform convergence empirical distances uced short ie dm f holds ffl 0 f possesses uced property learning algorithm small empirical error highly probably good algorithm concerning generalization uced property desirable since allows us use algorithm small empirical error rank several algorithms dependence empirical errors quantity ffl referred accuracy probability explicitly bounded ffi refer ffi confidence exists equivalent characterization uced property allows us test property concrete classes f furthermore allows us derive explicit bounds number examples empirical real error deviate ffl confidence least ffi set pseudometric covering number nffl denotes smallest number n n points x 1 x n exist closed balls respect radius ffl center x cover characterization uced property possible uced property holds lim dm f jx denotes restriction f inputs x dm refers empirical distance explicit bound deviation empirical error real error given inequality fg2f jd p f dm f see 29example 55 corollary 56 theorem 57 need possibility estimating socalled empirical covering number f occurs inequality often done estimating combinatorial quantity associated f measures sense capacity function class first assume f concept class ie function values contained binary set f0 1g vapnikchervonenkis dimension vc dimension vcf f largest number points shattered f ie binary mapping points obtained restriction function f real valued function class f fflfat shattering dimension fat ffl f largest size set fflfat shattered f ie points x 1 x n exist reference points r 1 r n 2 r binary mapping x function f 2 f exists every quantity called pseudodimension psf holds arbitrary p 29 theorem 42 consequently finite vc pseudodimension respectively ensure uced property hold bounds terms lead bounds number examples guarantee valid generalization moreover ensure distribution independent uced property well ie uced property holds even prefixed sup p fat shattering dimension may smaller pseudodimension yields inequality even finite fat shattering dimension guarantees distribution independent uced property leads bounds generalization error following assume constant function 0 contained f usually case f function class computed folding architecture shown finiteness vc dimension f concept class finiteness fat shattering dimension f function class respectively even necessary f possess distribution independent uced property 1 18 general class socalled loss functions correlated f finite fat shattering dimension f possesses uced property however constant function 0 contained f class loss functions contains f f finite fat shattering dimension well central role combinatorial quantities estimate vc fat shattering dimension folding architecture next paragraph turn infinite general order ensure uced property argumentation needs refined fortunately input space x divided trees height case folding architecture vc dimension architecture finite restricted inputs x allows us derive bounds probability high trees restricted priori purpose use following theorem theorem 3 assume f function class inputs x outputs 0 1 assume measurable 2 n x ae x t1 assume p probability measure x ffl ffi 2 0 1 chosen p fg2f jd p f dm f finite holds even proof estimate deviation real error empirical error dm f m1 gamma ffl4 points x contained x p probability induced p x holds p differs p 3ffl8 fraction ffl4 dropped x dm changes ffl2 using chebychef inequality limit first term mentioned earlier second term limited expected covering number limited 512e entire term limited ffi oe respectively 2 however necessary know probability high trees priori order get bounds number examples sufficient valid generalization holds even maximum input height trees concrete training set restricted therefore likely larger trees occur luckiness framework 27 turns useful allows us substitute prior bounds probability high trees posterior bounds maximum input height since want get bounds uced property generalize approach 27 function classes following way assume f 0 1valued function class inputs x function socalled luckiness function function measures quantity allows stratification entire function class subclasses finite capacity l outputs small value lucky sense concrete output function learning algorithm contained subclass small capacity needs examples correct generalization case simply measure maximum height input trees concrete training set define corresponding function measures number functions least lucky f x note dealing real outputs consequently quantization according value ff outputs necessary ensure finiteness number f ff refers function class outputs f0 ff obtained outputs f 2 f kff gamma ff2 kff ff2 identified kff k 2 n luckiness function l smooth respect j phi mappings n theta r indicates fraction deleted x obtain x 0 0 technical condition need proof smoothness condition allows us estimate number functions least lucky g double sample xy large part know luckiness g first half sample x since lucky situation number functions considered limited hence good generalization bounds obtained condition characterizes kind smoothness enlarge sample set stronger condition smoothness requirement 27 consideration restricted functions g coincide x since want get results learning algorithms small empirical error necessarily consistent generalized possibility estimating luckiness double sample knowing first half appropriate case analogy 27 state following theorem guarantees kind uced property situation turned lucky concrete learning task purpose setting split different scenarios less lucky occur probability depending concrete scenario generalization bounds obtained theorem 4 suppose p 2 n positive numbers l luckiness function class f smooth respect j phi inequality valid learning algorithm h real values ffi ff 0 fflm ffi hence lucky situation phi limited 2 0 1 deviation real error empirical error limited term order high probability proof f 2 f bound probability fulfilled ffl defined 29 theorem 57 step 1 sufficient bound probability latter set single p ffi 2 intersecting set single set occurs definition smoothness l complement respectively obtain bound denote event consider uniform distribution u group permutations swap elements j thus r x oe vector obtained applying permutation oe 29 theorem 57 step 2 latter probability bounded sup xy length x 0 0 f ff denotes quantized version f outputs identified kff denote event probability u measured b define equivalence classes c permutations two permutations belong class map indices values unless x 0 0 contain index find restrict events c definitely consider permutations swap elements x 0 0 bound latter probability u 0 denotes uniform distribution swappings common indices x 0 0 latter probability bounded using hoeffdings inequality random variables values fsigmaerror x 0 term total therefore obtain desired bound choose ffl jm fulfilled r note bound ffl tends 0 decreasing j way becomes small furthermore obtained bounds difference real empirical error instead dealing consistent algorithms 27 considered functions instead concept classes causes increase bound ff due quantization decrease convergence used hoeffdings inequality function case furthermore dual formulation unluckiness function l 0 possible corresponds substitution definition l formulas hold manner use unluckiness framework later generalization ability folding networks want apply general results learning theory folding networks purpose first estimate combinatorial quantities vcf jx psf jx fat ffl folding architecture f restricted set x input trees height denote oe activation function architecture w number adjustable parameters ie weights biases components initial context n number neurons h depth feedforward architecture induces folding architecture ie maximum length path graph defining network structure upper bounds vc pseudodimension f jx obtained first substituting input tree equivalent input tree maximum number nodes unfolding network inputs applying bounds feedforward case unfolded networks details see 14 15 leads following bounds found 8 14 19 ow lnth oe linear ow th ln oe polynomial degree 2 w lnw 2 note bounds differ polynomial activation function respectively lower bounds found 8 14 19 oe nonlinear polynomial since standard feedforward perceptron architecture exists points sigmoidal function approximate perceptron activation arbitrarily well combine architecture sigmoidal case feedforward architecture obtain additional summand lower bounds sgd detailed construction described 14 theorem 11 following theorem yields slight improvement sigmoidal case theorem 5 input set r 2 sgd architecture exists shattering proof restrict argumentation case 2 consider tt trees depth contain binary numbers length first component labels leaves binary numbers length labels next layer numbers 0 1 first layer number 0 root tree ij 2 second component labels 0 except one layer 1 labels already defined coefficient 1 jth digit 21 tree 0 00 000 0 01 0 1 010 1 11 1 depth purpose definition coefficients enumerate binary strings used extract bits number 1 tt 12 efficient way context vector simply compare context numbers first bits correspond cut prefix subtracting number context obtain next bits next iteration step coefficient labels specify digit context vector responsible input tree ij namely digit definitions recursive architecture constructed outputs input ij responsible bit initial context therefore shatters trees appropriate choice initial context precise architecture induced mapping f 01 leads mapping computes third component responsible bit ij initial context 0 role first context neuron store remaining bits initial context recursive computation step context shifted multiplying 2 dropping first bits subtracting appropriate label tree corresponding layer second context neuron computes value 10 height remaining tree course substitute value scaled version contained range sgd third context neuron stores bit responsible obtain output 1 first bits appropriate context coincide binary number entry 1 position responsible tree position indicated x 2 f approximated arbitrarily well architecture sigmoidal activation function fixed number neurons shatters trees combine w architectures obtaining architecture shattering w tt trees ow weights proceeds first simulating initial context additional weights adding w architectures described 14 theorem 11 detail additional summand w ln w obtained described earlier 2 unfortunately lower bound still differs upper bound exponential term nevertheless interesting due following reason bounds linear polynomial case differ comparing 2 sigmoidal case real upper bound expected order wn lower bound obtained order 2 consequently capacity increases tree structured inputs sigmoidal case compared lists contrast linear polynomial case activation functions bounds become infinite arbitrary inputs allowed perceptron activation function prohibited restricting inputs lists trees nodes finite alphabet 19 general one could restrict absolute values weights inputs consider fat shattering dimension instead pseudodimension turns useful dealing svm ensembles networks example 4 13 27 unfortunately even activation function coincides identical function lower bound omegagammaun ln found fat shattering dimension restricted weights inputs 15 sigmoidal activation lower found fat shattering dimension 15 following theorem generalizes result large number activation functions theorem 6 activation function oe twice continuously differentiable nonvanishing second derivative neighborhood least one point obtain lower bound fat 01 recurrent architecture f 3 computation neurons activation function oe recursive part one linear neuron feedforward part input lists entries unary alphabet weights restricted constant depends activation function oe proof function property x therefore function class f 01shatters sets sequences mutually different length starting longest sequence set value 01 04 06 09 respectively corresponding desired output sequence one recursively choose inverse image 01 04 06 09 order get outputs shorter sequences finally appropriate initial context since even 0 1 entirely contained 09 holds continuous function differs f 01 oe twice continuously differentiable nonvanishing second derivative neighborhood least one point consequently points x 0 x 1 ffl 0 exist maximum deviation 0007 2 01 consequently gx differs fx 01 input 0 1 hence fg j 20 1g shatters sequences mutually different length well g implemented folding network without hidden layers 3 context neurons activation function oe recursive part one linear neuron feedforward part context closed set depends activation function oe holds g linear combination neurons activation function oe constant identity holds mappings f 1 appropriate arity vectors therefore linear mapping g integrated network structure except initial context chosen compact set weights architecture fixed set shattered depend activation function oe 2 hence distribution independent uced property hold fixed folding architecture distribution independent pac learnable realistic condition large number activation functions including standard sigmoidal function fact rely learning algorithm used characteristic function class possible deal inputs arbitrary size unlimited size used store sense dichotomies inputs regarding argumentation situation even worse architecture small uses different length inputs particular training sets typically occur time series prediction shattered ie tablelookup possible inputs however shown 14 distribution dependent pac learnability guaranteed arguments last section allow us derive bounds deviation empirical error real error learning algorithm corollary 7 denote f fixed folding architecture inputs x x set trees height assume p probability measure x assume chosen learning algorithm h jd p f hm f valid number examples chosen specified theorem 3 bound polynomial 1ffl 1ffi p x order fat ffl512 proof bounds follow immediately theorem 3 polynomial 1ffl 1ffi vc pseudo fat shattering dimension polynomial 1ffl 1ffi condition inequality derived 2 argumentation leads bounds limit probability p x furthermore bounds polynomial probability large trees tends 0 sufficiently fast necessary rate convergence depends folding architecture considered substitute prior information using luckiness framework learn concrete training sample derive bounds depend maximum height trees training sample capacity architecture corollary 8 assume f 0 1valued function class trees x p probability distribution x trees height finite every trees height x maximum height trees sample x proof want apply theorem 4 use notation theorem unlucki ness function l 0 x f maxfheight tree xg smooth respect phim l 0 x f ffi trees height l 0 x f jm l 0 x f ffi 2 seen follows number functions jfgjx 0 height l 0 x f bounded phim l 0 x f ffi ff number bounded quantity length x 0 0 latter probability equals z u uniform distribution swapping permutations 2m elements event want bound number swappings xy first half tree higher fixed value whereas second half least mj trees higher may swap mj indices arbitrarily obviously probability bounded 2 gammamj ffi j lg1ffim choose insert values inequalities obtained luckiness framework get bound sm chosen phim lx f ffi ff 2 t1 hence sufficient choose least 2 conclusions information theoretical learnability folding architectures examined purpose bounds vc pseudo fat shattering dimension play key role learnability cited improved respectively since fat shattering dimension infinite even restricted weights inputs cannot exist bounds number examples guarantee valid generalization independent special distribution since results depend concrete learning algorithm dynamics principle drawback many mechanisms proposed learning structured data list tree structure processed recursively according recursive structure data priori unlimited length height inputs offers possibility using space store desired dichotomy inputs way upper bounds vc pseudodimension given terms number parameters network maximum input height proposed two approaches allow stratification situation via input space output concrete learning algorithm concerning folding networks division input space sets trees restricted height fits first approach allows us derive bounds deviation empirical real error learning algorithm probability distribution probability high trees restricted priori second approach applied training situations height input trees restricted concrete learning example allows us derive bounds deviation empirical error real error depend concrete learning set means maximum height input trees note approaches bounds rather conservative yet tried improve constants occur consequence structural risk learning algorithm controlled folding networks methods processing dynamics well real error learning algorithm estimated empirical error network architecture number patterns additionally probability high trees maximum input height training set known although fact holds learning algorithm algorithms preferred compared others since algorithm small empirical error generalizes well needs number examples question arises whether learning algorithm capable minimizing empirical error efficient way algorithm preferred manages task efficiently folding architecture seems superior raam example used classification structured data try find encoding decoding appropriate classification encoding classification means function class considered dealing lraam instead folding networks difficult minimization task solved furthermore algorithms start prior knowledge available example form automata rules 23 highly probably find small empirical error faster algorithm start scratch starting point closer optimum value first case function class remains initialization training process adequate actually training recurrent networks gradient descend method proven particularly difficult 5 16 holds folding networks dealing high input trees well makes investigation alternative methods learning necessary already mentioned method start appropriate initialized network rather scratch 20 23 use appropriate modifications architecture training algorithm 6 16 algorithms bounds number training samples apply algorithm however complicated able yield valid generalization number examples independent underlying regularity r sufficient condition polynomial distributiondependent learnability probabilistic analysis learning artificial neural networks pac model variants valid generalization learning longterm dependencies gradient descent difficult credit assignment time alternatives backpropagation topological transformation hidden recursive models sample complexity learning recurrent perceptron mappings general framework adaptive processing data sequences adaptive processing sequences data structures learning taskdependent distributed representations backpropagation structure special issue recurrent neural networks sequence processing approximation learning convex superpositions learnability recursive data generalization elman networks long shortterm memory polynomial bounds vc dimension sigmoidal neural networks efficient distributionfree learning probabilistic concepts correspondence neural folding architectures tree automata inductive learning symbolic domains using structuredriven neural networks neural net architectures temporal sequence processing constructing deterministic finitestate automata recurrent neural networks recursive distributed representation relating chemical structure activity structure processing neural folding architecture experiments applicability folding architectures guide theorem proving structural risk minimization data dependent hierarchies labeling raam theory learning generalization roots backpropagation tr ctr k rahman wang pi yang tommy w chow sitao wu flexible multilayer selforganizing map generic processing treestructured data pattern recognition v40 n5 p14061424 may 2007 barbara hammer alessio micheli alessandro sperduti universal approximation capability cascade correlation structures neural computation v17 n5 p11091159 may 2005 barbara hammer peter tio recurrent neural networks small weights implement definite memory machines neural computation v15 n8 p18971929 august