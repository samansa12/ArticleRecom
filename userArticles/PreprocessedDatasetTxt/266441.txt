power optimization using divideandconquer techniques minimization number operations develop approach minimizing power consumption portable wireless dsp applications using set compilation architectural techniques key technical innovation novel divideandconquer compilation technique minimize number operations general dsp computations technique optimizes significantly wider set computations previously published techniques also outperforms performs least well techniques examples along architectural dimension investigate coordinated impact compilation techniques number processors provide optimal tradeoff cost power demonstrate proper compilation techniques significantly reduce power bounded hardware cost effectiveness techniques algorithms documented numerous reallife designs b introduction 11 motivation pace progress integrated circuits system design dictated push application trends pull technology improvements goal role designers design tool developers develop design methodologies architectures synthesis tools connect changing worlds applications technologies preliminary version paper presented 1997 acmieee international conference computeraided design san jose california november 1013 1997 authors addresses hong potkonjak computer science department university cali fornia los angeles ca 900951596 r karri department electrical computer engineering university massachusetts amherst 01003 permission make digital hard copies part work personal classroom use granted without fee provided copies made distributed profit direct commercial advantage copies show notice first page initial screen display along full citation copyrights components work owned others acm must honored abstracting credit permitted copy otherwise republish post servers redistribute lists use component work works requires prior specific permission andor fee recently new class portable applications forming new market exceptionally high rate applications products portable wireless market defined intrinsic demand portability flexibility cost sensitivity high digital signal processing dsp content schneiderman 1994 portability translates crucial importance low power design flexibility results need programmable platforms implementation cost sensitivity narrows architectural alternatives uniprocessor architecture limited number offtheshelf standard processors key optimization degree freedom relaxing satisfying set requirements comes properties typical portable computations computations mainly linear rarely 100 linear due either need adaptive algorithms nonlinear quantization elements computations well suited static compilation intensive quantitative optimization two main recent relevant technological trends reduced minimal feature size therefore reduced voltages deep submicron technologies introduction ultra low power technologies widely dominating digital cmos technologies power consumption proportional square supply voltage v dd effective techniques try reduce v dd compensating speed reduction using variety architectural compilation techniques singh et al 1995 main limitation conventional technologies respect power minimization also related v dd threshold voltage v traditional bulk silicon technologies voltages commonly limited range 07v however last years ultra low power silicon insulator soi technologies simox soi using separation oxygen bond etchback soi besoi silicononinsulatorwithactivesubstrate soias reduced v dd v well 1v elkareh et al 1995 ipposhi et al 1995 number reported ics values v dd v range low 005v 01v chandrakasan et al 1996 goal paper develop system synthesis compilation methods tools realization portable applications technically restated primary goal develop techniques efficiently effectively compile typical dsp wireless applications single multiple programmable processors assuming traditional bulk silicon newer soi technologies furthermore study achievable powercost tradeoffs parallelism traded power reduction programmable platforms 12 design methodology new design methodology briefly described follows given throughput power consumption cost requirements computation goal find costeffective powerefficient solutions single multiple programmable processor platforms first step find powerefficient solutions single processor implementation applying new technique described section 4 second step continue add processors reduction average power consumption enough justify cost additional processor step generates costeffective power efficient solutions straightforward design methodology produces implementations low cost low power consumption given design requirements main technical innovation research presented paper first approach minimization number operations arbitrary computa tions approach optimizes significantly wider set computations previously published techniques parhi messerschmitt 1991 srivastava potkonjak 1996 also outperforms performs least well techniques examples novel divideandconquer compilation procedure combines coordinates power enabling effects several transformations using well organized ordering transformations minimize number operations logical partition best knowledge first approach minimization number operations optimization intensive way treats general computations second technical highlight quantitative analysis cost vs power tradeoff multiple programmable processor implementation platforms derive condition optimization costpower product using parallelization beneficial 13 paper organization rest paper organized following way first next sec tion summarize relevant background material section 3 review related work power estimation optimization well program optimization using transformations particular minimization number operations sections 4 5 technical core paper present novel approach minimization number operations general dsp computations explore compiler technology impact powercost tradeoffs multiple processorsbased low power application specific systems present comprehensive experimental results analysis section 6 followed conclusions section 7 2 preliminaries delve technical details new approach outline relevant preliminaries section particular describe application computation abstractions selected implementation platform technology architectural level power estimation related background material 21 computational model selected computational model synchronous data flow sdf lee messerschmitt 1987 lee parks 1995 synchronous data flow sdf special case data flow number data samples produced consumed node invocation specified priori nodes scheduled statically compile time onto programmable processors restrict attention homogeneous sdf hsdf node consumes produces exactly one sample every execution hsdf model well suited specification single task computations numerous application domains digital signal processing video image processing broadband wireless communications control information coding theory multimedia syntax targeted computation defined hierarchical controldata flow graph cdfg rabaey et al 1991 cdfg represents computation flow graph nodes data edges control edges semantics underlying syntax cdfg format already stated synchronous data flow computation model relevant speed metric throughput rate implementation capable accepting processing input samples two consecutive iterations opted throughput selected speed metric since essentially dsp communication wireless computations latency limiting factor latency defined delay arrival set input samples production corresponding output defined specification 22 hardware model basic building block targeted hardware platform single programmable processor assume types operations take one clock cycle execution case many modern dsp processors adaptation software algorithms hardware timing models straightforward case multiprocessor make following additional simplifying assumptions processors homogeneous ii interprocessor communication cost time hardware assumption reasonable multiple processors placed single integrated circuit due increased integration although would realistic assume additional hardware delay penalty using multiple processors 23 power timing models conventional ultra low power technology well known three principal components power consumption cmos integrated circuits switching power shortcircuit power leakage power switching power given p switching dd f clock ff probability power consuming switching activity ie transition 0 1 occurs cl loading capacitance v dd supply voltage f clock system clock frequency ffc l defined effective switched capacitance cmos technology switching power dominates power consumption shortcircuit power consumption occurs nmos cmos transistors time leakage power consumption results reverse biased diode conduction subthreshold operation assume effective switched capacitance increases linearly number processors supply voltage lowered threshold voltage v use several different values 006v 11v conventional ultra low power technology also known reduced voltage operation comes cost reduced throughput chandrakasan et al 1992 clock speed follows following constant chandrakasan et al 1992 maximum rate circuit clocked monotonically decreases voltage reduced supply voltage reduced close v rate clock speed reduction becomes higher 24 architecturelevel power models single multiple programmable processors power model used research built three statistically validated experimentally established facts first fact number operations machine codelevel proportional number operations highlevel language hoang rabaey 1993 second fact power consumption modern programmable processors fujitsu sparclite mb86934 32bit risc microcontroller directly proportional number operations regardless mix operations executed tiwari lee 1995 tiwari lee 1995 report operations including integer alu instructions floating point instructions loadstore instructions locked caches incur similar power consumption since use memory operands results additional power overhead due possibility cache misses assume cache locking feature exploited far possible cache locking feature used target applications power consumption memory traffic likely reduced minimization number operations since less operations usually imply less memory traffic power consumption depends mix operations executed case intel 486dx2 tiwari et al 1994 detailed hardware power model may needed however obvious proposed power models programmable processors significant reduction number operations inevitably results lower power final empirical observation related power consumption timing models digital cmos circuits presented previous subsection based three observations conclude targeted implementation platform single programmable cmos processor reduction number operations key power minimization initial number operations n init optimized number operations n opt initial voltage v init scaled voltage v opt optimized power consumption relative initial power consumption vopt multiprocessors assuming communication overhead optimized power consumption n processors relative single processor vn vn scaled voltages single n processors respectively 3 related work related work classified along two lines low power implementation optimization particular minimization number operations using transformations relevant low power topics divided three directions power minimization techniques power estimation techniques technologies ultra low power design relevant compilation techniques also grouped three directions transformations ordering transformations minimization number operations last five years power minimization arguably popular optimization goal mainly due impact rapidly growing market portable computation communication products power minimization efforts across level design abstraction process surveyed singh et al 1995 apparent greatest potential power reduction highest levels behavioral algorithmic chandrakasan et al 1992 demonstrated effectiveness transformations showing order magnitude reduction several dsp computationally intensive examples using simulated annealingbased transformational script raghunathan jha 1994 goodby et al 1994 also proposed methods power minimization explore tradeoffs voltage scaling throughput power chatterjee roy 1994 targeted power reduction fully hardwired designs minimizing switching activity chandrakasan et al 1994 tiwari et al 1994 work power minimization programmable platforms targeted numerous power modeling techniques proposed levels abstraction synthesis process documented singh et al 1995 numerous efforts gate level higher level abstraction relatively efforts reported chandrakasan et al 1995 developed statistical technique power estimation behavioral level takes account components layout level including interconnect landman rabaey 1996 developed activitysensitive architectural power analysis approach execution units asic designs finally series papers established power consumption modern programmable processor directly proportional number operations regardless mix operations executed lee et al 1996 tiwari et al 1994 transformations widely used levels abstraction synthesis process eg dey et al 1992 however strong experimental evidence effective highest levels abstractions system particular behavioral synthesis transformations received widespread attention high level synthesis ku micheli 1992 potkonjak rabaey 1992 walker camposano 1991 comprehensive reviews use transformations parallelizing compilers state oftheart general purpose computing environments vlsi dsp design given banerjee et al 1993 bacon et al 1994 parhi 1995 respectively approaches transformation ordering classified seven groups local peephole optimization static scripts exhaustive searchbased generate test methods algebraic approaches probabilistic search techniques bottleneck removal methods enablingeffect based techniques probably widely used technique ordering transformations local peephole optimization tanenbaum et al 1982 compiler considers small section code time order apply one one iteratively locally available transformations advantages approach fast simple implement however performance rarely high usually inferior approaches another popular technique static approach transformations ordering order given priori often form script ullman 1989 script development based experience compilersynthesis software developer method least three drawbacks time consuming process involves lot experimentation random examples adhoc manner knowledge relationship among transformations implicitly used quality solution often relatively low programsdesign different characteristics ones used development script powerful approach transformation ordering enumerationbased generate test massalin 1987 possible combinations transformations considered particular compilation best one selected using branch 7andbound dynamic programming algorithms drawback large run time often exponential number transformations another interesting approach use mathematical theory behind ordering transformations however method limited several linear loop transformations wolf lam 1991 simulated annealing genetic programming probabilistic techniques many situations provide good tradeoff run time quality solution little information topology solution space available recently several probabilistic search techniques proposed ordering transformations compiler behavioral synthesis literature example backwardpropagationbased neural network techniques used developing probabilistic approach application transformations compilers parallel computers fox koller 1989 approaches combine simulated annealingbased probabilistic local heuristic optimization mechanism used demonstrate significant reductions area power chandrakasan et al 1995 behavioral logic synthesis several bottleneck identification elimination approaches ordering transformations proposed dey et al 1992 iqbal et al 1993 line work mainly addressing throughput latency optimization problems bottlenecks easily identified well quantified finally idea enabling disabling transformations recently explored number compilation whitfield soffa 1990 high level synthesis papers potkonjak rabaey 1992 srivastava potkonjak 1996 using idea several powerful transformations scripts developed one maximally arbitrarily fast implementation linear computations potkonjak rabaey 1992 joint optimization latency throughput linear computations srivastava potkonjak 1994 also enabling mechanism used basis several approaches ordering transformations optimization general computations huang rabaey 1994 key advantage class approaches related intrinsic importance power enablingdisabling relationship pair transformations transformations used optimization variety design program metrics throughput latency area power permanent temporal faulttolerance testability interestingly power transformations often focused secondary metrics parallelism instead primary metrics number operations compiler domain constant copy propagation common subexpression techniques often used easily shown constant propagation problem undecidable computation conditionals kam ullman 1977 standard procedure address problem use called conservative algorithms algorithms guarantee constants detected data declared constant indeed constant possible executions program comprehensive survey popular constant propagation algorithms found wegman zadeck 1991 parhi messerschmitt 1991 presented optimal unfolding linear computations dsp systems unfolding results simultaneous processing consecutive iterations computation potkonjak rabaey 1992 addressed minimization number multiplications additions linear computations maximally fast form throughput preserved potkonjak et al 1996 presented set techniques minimization number shifts additions linear computations sheliga sha 1994 presented approach minimization number multiplications additions linear computations srivastava potkonjak 1996 developed approach minimization number operations linear computations using unfolding application maximally fast procedure variant technique used conquer phase approach approach different two respects first technique handle restricted computations linear approach optimize arbitrary computations second approach outperforms performs least well technique linear computations 4 single programmable processor implementation minimizing number operations global flow approach presented subsection 41 strategy based divideandconquer optimization followed post optimization step merging divided sub parts explained subsection 42 finally subsection 43 provides comprehensive example illustrate strategy 41 global flow approach core approach presented pseudocode figure 1 rest subsection explains global flow approach detail decompose computation strongly connected componentssccs adjacent trivial sccs merged sub part use pipelining isolate sub parts sub part minimize number delays using retiming sub part linear apply optimal unfolding else apply unfolding isolation nonlinear operations merge linear sub parts optimize schedule merged sub parts minimize memory usage fig 1 core approach minimize number operations general dsp computation first step approach identify computations strongly connected componentssccs using standard depthfirst searchbased algorithm tarjan 1972 low order polynomialtime complexity pair operations b within scc exist path b path b illustrated example step shown figure 2 graph formed sccs acyclic thus sccs isolated using pipeline delays enables us optimize scc separately inserted pipeline delays treated inputs outputs scc result every output state scc depend inputs states scc addition constant multiplication functional delay state variable multiplication strongly connected component fig 2 illustrated example scc decomposition step thus sense scc isolated rest computation optimized separately number situations technique capable partition nonlinear computation partitions consist linear computations consider example computation consists two strongly connected components scc 1 scc 2 scc 1 operations additions multiplications constants scc 2 operations max operation additions obviously since computations additions multiplications constants max operations nonlinear however applying technique logical separation using pipeline states two parts linear note isolation affected unfolding define scc one node trivial scc trivial sccs unfolding fails reduce number operations thus adjacent trivial sccs merged together isolation step reduce number pipeline delays used x input output state vectors respectively b c constant coefficient matrices fig 3 statespace equations linear computations number delays sub part minimized using retiming polynomial time leisersonsaxe algorithm leiserson saxe 1991 note smaller number delays require smaller number operations since next states outputs depend previous states sccs classified either linear nonlinear linear computations represented using 2rgamma1r 2rgamma1r gives smaller value multiplications times unfolded system additions times unfolded system fig 4 closedform formula unfolding dense linear computation p inputs q outputs r states statespace equations figure 3 minimization number operations linear computations npcomplete sheliga sha 1994 adopted approach srivastava potkonjak 1996 optimization linear sub parts uses unfolding maximally fast procedure potkonjak rabaey 1992 note instead maximally fast procedure ratio analysis sheliga sha 1994 used srivastava potkonjak 1996 provided closedform formula optimal unfolding factor assumption dense linear computations provide formula figure 4 sparse linear computations proposed heuristic continues unfold improvement made simple heuristic efficient binary search based unimodality property number operations unfolding factor srivastava potkonjak 1996 iteration iteration i1 iteration i2 fig 5 example isolating nonlinear operations 2 times unfolded nonlinear sub part sub part classified nonlinear apply unfolding isolation nonlinear operations nonlinear operations isolated sub part remaining linear sub parts optimized maximally fast procedure arcs nonlinear operations linear sub parts considered inputs linear sub parts arcs linear sub parts nonlinear operations considered outputs linear sub parts process illustrated figure 5 arcs denoted considered inputs arcs denoted considered outputs unfolded linear sub part observe every output state nonlinear sub part depend nonlinear operations unfolding separation nonlinear operations ineffective reducing number operations fig 6 motivational example sub part merging sometimes beneficial decompose computation larger sub parts sccs consider example given figure 6 node represents sub part computation make following assumptions specifically clarifying presentation example simplifying example stress assumptions necessary approach assume sub part linear represented statespace equations figure 3 also assume every sub part dense means every output state sub part linear combinations inputs states sub part 0 1 1 coefficients number inside node number delays states sub part assume arc sub part x sub part every output state depends inputs states x separately optimizing sccs p 1 p 2 figure 6 costs 211 operations formula figure 4 otherhand optimizing entire computation entails 6367 operations reason separate optimization perform well example many intermediate outputs scc p 1 observation leads us approach merging sub parts reducing number operations since worthwhile explain sub part merging problem detail next subsection devoted explanation problem heuristic approaches since sub parts computation unfolded separately different unfolding factors need address problem scheduling sub parts scheduled memory requirements code data minimized observe unfolded sub parts represented multirate synchronous dataflow graph lee messerschmitt 1987 work bhattacharyya et al 1993 directly used note approach particular useful architectures require high locality regularity computation improves locality regularity computation decomposing sub parts using maximally fast procedure locality computation relates degree computation natural clusters operations regularity computation refers repeated occurrence computational patterns multiplication followed addition guerra et al 1994 mehra rabaey 1996 42 subpart merging initially consider merging linear sccs two sccs merged resulting sub part form scc thus general must consider merging adjacent arbitrary sub parts suppose consider merging sub parts j gain gaini j merging sub parts j computed follows number operations sub part cost j number operations merged sub part j compute gain cost j must computed requires constant coefficient matrices b c merged sub part j easy construct matrices using depthfirst search tarjan 1972 fig 7 times unfolded statespace equations gamma 1c gives smaller value opt states state group j outputs output group j inputs output group j depends inputs state group j depends states output group j depends states state group j depends fig 8 closedform formula unfolding two outputs depend set inputs states group true states times unfolded system represented statespace equations figure 7 equations total number operations computed times unfolded sub part follows let denote number multiplications number additions times unfolded system respectively resulting number operations n ini times unfolded system uses batch samples generate batch output samples continue unfold improvement achieved coefficients 1 gamma1 matrices b c closedform formula optimal unfolding factor opt number operations times unfolded system provided figure 8 improvement possible merging candidates compute gain merge pair highest gain fig 9 pseudocode greedy heuristic sub part merging generate starting solution set best solution determine starting temperature yet frozen yet equilibrium current temperature choose random neighbor 0 current solution else generate random number r uniformly 0 1 update temperature return best solution fig 10 pseudocode simulated annealing algorithm sub part merging evaluate possible merging candidates propose two heuristic algorithms sub part merging first heuristic based greedy optimization approach pseudocode provided figure 9 algorithm simple improvement merge pair sub parts produces highest gain heuristic algorithm based general combinatorial optimization technique known simulated annealing kirkpatrick et al 1983 pseudocode provided figure 10 actual implementation details presented following areas cost function neighbor solution generation temperature update function equilibrium criterion frozen criterion firstly number operations entire given computation used cost function secondly neighbor solution generated merging two adjacent sub parts thirdly temperature updated function old temperature 2000 ff chosen 01 high temperature regime every new state high chance acceptance temperature reduction occurs rapidly set 095 optimization process explores promising region slowly 10 ff set 08 quickly reduced converge local minimum initial temperature set 4000000 fourthly equilibrium criterion specified number iterations inner loop number fig 11 explanatory example iterations inner loop set 20 times number sub parts lastly frozen criterion given temperature temperature falls 01 simulated annealing algorithm stops heuristics performed equally well examples run times small examples sub parts used greedy simulated annealing based heuristics generating experimental results produced exactly results fig 12 simple example operations calculation 43 explanatory example putting together illustrate key ideas approach minimizing number operations considering computation figure 11 use assumptions made example figure 6 number operations per input sample initially 2081 illustrate number operations calculated maximally fast way potkonjak rabaey 1992 using simple linear computation 1 input x 1 output described figure 12 using technique srivastava potkonjak 1996 unfolds entire computation number reduced 725 unfolding factor 12 approach optimizes sub part separately separate optimization enabled isolating sub parts using pipeline delays figure 13 shows computation isolation step since every sub part linear unfolding performed optimize number operations sub part sub parts cost 12075 5391 11486 12975 1030 operations per input sample unfolding factors 3 10 6 7 2 respectively total number operations per input sample entire computation 52227 apply scc merging reduce number operations first consider greedy heuristic heuristic fig 13 motivational example isolation step considers merging adjacent sub parts initially possible merging candidate produce gains 5148 11206 5238 12287 11492 respectively scc p 3 scc p 4 merged unfolding factor 22 next iteration 4 sub parts 4 candidate pairs merging yield negative gains heuristic stops point total number operations per input sample decreased 3994 simulated annealing heuristic produced solution example approach reduced number operations factor 182 previous technique srivastava potkonjak 1996 achieved reduction factor 52 initial number operations single processor implementation since technique srivastava potkonjak 1996 new method yield higher throughput original supply voltage lowered extent extra throughput compensated loss circuit speed due reduced voltage initial voltage 33v technique reduces power consumption factor 260 supply voltage 148v technique srivastava potkonjak 1996 reduces factor 100 supply voltage 177v scheduling unfolded sub parts performed generate minimum code data memory schedule schedule period least common multiple unfolding factor1s 3036 let p 34 denote merged sub part simple minded schedule 759p 1 276p 2 132p 34 1012p 5 minimize code size ignoring loop overheads generates 9108 units data memory requirement schedule 759p 1 469p 2 33p 34 253p 5 minimizes data memory requirement among schedules minimizing code size generates 4554 units data memory requirement 5 multiple programmable processors implementation multiple programmable processors used potentially savings power consumption obtained summarize assumptions made section 2 processors homogeneous ii interprocessor communication cost time hardware iii effective switched capacitance increases linearly number processors iv addition multiplication take one clock cycle v supply voltage lowered threshold voltage v use several different values 006v 11v based assumptions using k processors increases throughput k times enough parallelism computation effective switched capacitance increases k times well reallife examples considered sufficient parallelism actually existed numbers processors used r e k r 2 fig 14 closedform condition sufficient parallelism using k processors dense linear computation r states observe next states ie feedback loops computed parallel note maximally fast procedure potkonjak rabaey 1992 evaluates linear computation first constantvariable multiplications parallel organizing additions maximally balanced binary tree since next states computed maximally fast procedure bottom binary computation tree exists paral lelism operations feedback loops computed parallel separated pipeline delays number processors becomes larger number operations outside feedback loops gets larger result parallelism dense linear computations provide closedform condition sufficient parallelism using k processors figure 14 note although formulae derived worst case scenario required number operations outside feedback loops small range number processors tried experiment exist operations outside feedback loops required full parallelism reallife examples considered one reduce voltage clock frequency k processors reduced factor k average power consumption k processors reduced single processor factor v1 scaled supply voltage k processor implementation v k satisfies equation et al 1992 observation always beneficial use processors terms power consumption following two limitations amount parallelism available limits improvement throughput critical path computation maximum achievable throughput ii supply voltage approaches close threshold voltage improvement power consumption becomes small cost adding processor justified mind want find number processors minimizes power consumption costeffectively standard cmos technology ultra low power technology since cost programmable processors high especially cost processors ultra low power platforms soi high elkareh et al 1995 ipposhi et al 1995 guidances costeffective design important need measure differentiate costeffective costineffective solutions propose pn product p power consumption normalized optimized single processor implementation n number processors number processors 11 50 50 090 093 098 104 110 117 124 130 137 40 100 108 117 128 138 149 159 170 180 30 114 133 151 170 188 206 224 242 260 20 142 182 222 260 298 335 371 408 444 07 33 33 089 091 095 100 106 112 119 125 131 20 112 128 145 162 179 196 212 228 245 10 163 222 280 338 394 450 505 560 615 03 13 13 092 096 102 108 116 123 130 138 145 07 124 149 175 200 224 248 272 295 319 table values pn products respect number processors various combinations initial voltage v init scaled voltage single processor v 1 threshold used smaller pn product costeffective solution pn smaller 10 using n processors decreased power consumption factor n depends power consumption requirement cost budget implementation many processors implementation use table provides values pn products respect number processors used various combinations initial voltage v init scaled voltage single processor v 1 threshold voltage v v init initial voltage implementation optimization note pn products monotonically increase respect number processors init new rp rp design ops sri96 method sri96 sri96 init ops init ops dist 48 473 364 130 230 132 242 chemical modem 213 213 14883 143 301 143 301 ge controller 180 180 10526 171 415 171 415 apcm receiver 2238 na 144419 na na 155 354 audio filter 1 154 na 760 na na 203 507 audio filter 2 228 na 920 na na 248 597 filter 1 296 na 15714 na na 188 468 filter 2 398 na 1845 na na 216 537 table ii minimizing number operations reallife examples improvement factor reduction percentage na applicable sri96 srivastava potkonjak 1996 table observe cost effective solutions usually use processors cases considered standard cmos ultra low power platforms also observe voltage reduction high single vnew prf dist 50 11 376 233 33 07 270 196 13 03 110 184 chemical 50 11 361 265 33 07 261 221 13 03 107 204 dac 50 11 381 272 33 07 250 275 13 03 100 269 modem 50 11 402 221 33 07 265 223 13 03 105 219 ge controller 50 11 365 321 33 07 239 325 13 03 096 316 table iii minimizing power consumption single programmable processor linear examples reduction factor processor case much room reduce power consumption using processors based observations developed strategy multiple processor implementation first step minimize power consumption single processor implementation using proposed technique section 4 second step increase number processors pn product given maximum value maximum value determined based power consumption requirement cost budget implementation strategy produces solutions processors many cases single processor reallife examples method minimization number operations significantly reduces number operations turn supply voltage single processor implementation adding processors usually reduce power consumption costeffectively method achieves costeffective solutions low power penalty compared solutions optimize power consumption without considering hardware cost 6 experimental results set benchmark designs include benchmark examples used sri vastava potkonjak 1996 well following typical portable dsp video communication control applications dac 4 stage nec digital analog converter dac audio signals modem 2 stage nec modem ge controller 5state ge linear controller apcm receiver motorolas adaptive pulse code vnew prf apcm receiver 50 11 385 262 33 07 253 264 13 03 101 258 audio filter 1 50 11 334 454 33 07 203 443 13 03 088 445 audio filter 2 50 11 303 676 33 07 185 654 13 03 08 658 filter 1 50 11 345 397 33 07 226 403 13 03 091 390 filter 2 50 11 324 515 33 07 212 524 13 03 085 504 vstol 50 11 343 410 33 07 225 415 13 03 090 402 table iv minimizing power consumption single programmable processor nonlinear ex amples prf power reduction factor modulation receiver audio filter 1 analog digital converter adc followed 14 order cascade iir filter audio filter 2 adc followed two adcs followed 10order two dimensional 2d iir two adcs followed 12order 2d iir filter vstol vstol robust observer structure aircraft speed controller dac modem ge controller linear computations rest nonlinear computations benchmark examples srivastava potkonjak 1996 linear include ellip iir5 wdf5 iir6 iir10 iir12 steam dist chemical table ii presents experimental results technique minimizing number operations reallife examples fifth seventh columns table ii provide improvement factors method srivastava potkonjak 1996 initial number operations respectively method achieved number operations srivastava potkonjak 1996 ellip iir5 wdf5 iir6 iir10 iir12 steam reduced number operations 23 103 dist chemical respectively examples srivastava potkonjak 1996 singleinput singleoutput linear computations except dist chemical twoinputs singleoutput linear computations since siso linear computations small vnew prf n prf n prf n prf dist 50 11 376 233 1 233 4 753 6 947 33 07 270 196 2 404 5 811 8 1054 13 03 110 184 2 371 4 631 7 874 chemical 50 11 361 265 1 265 3 687 5 939 33 07 261 221 2 449 5 886 7 1069 13 03 107 204 1 204 4 683 6 867 33 07 250 275 1 275 4 922 6 1171 13 03 100 269 1 269 3 705 5 967 modem 50 11 402 221 2 445 4 756 7 1045 33 07 265 223 2 456 5 907 7 1097 13 03 105 219 1 219 4 722 6 913 ge 50 11 365 321 1 321 3 839 5 1149 controller 33 07 239 325 1 325 4 1049 6 1320 13 03 096 316 1 316 3 805 5 1092 table v minimizing power consumption multiple processors linear examples pn threshold pn product n processors prf power reduction factor room improvement srivastava potkonjak 1996 method reduced number operations average factor 177 average 435 examples previous techniques either ineffective inap plicable tables iii iv present experimental results technique minimizing power consumption single programmable processor reallife examples various technologies method results power consumption reduction average factor 358 multiple processor implementations tables v vi summarize experimental results technique minimizing power consumption define threshold pn product pn value pn product stop increasing number processors pn ie power reduction addition processor must greater 2 cost effective almost cases single processor solution optimum pn gets larger number processors used increases solutions still use processors result order magnitude reduction power consumption results clearly indicate effectiveness new method 7 conclusion introduced approach power minimization using set compilation architectural techniques key technical innovation compilation technique minimization number operations synergistically uses several vnew prf n prf n prf n prf apcm 50 11 385 262 1 262 4 864 6 1092 receiver 33 07 253 264 2 529 4 895 7 1233 13 03 101 258 1 258 3 681 6 1032 audio 50 11 334 454 1 454 3 1112 4 1322 filter 1 33 07 203 443 1 443 2 799 4 1238 13 03 088 445 1 445 2 807 4 1256 audio 50 11 303 676 1 676 2 1188 4 1804 filter 2 33 07 185 654 1 654 2 1126 3 1445 13 03 08 658 1 658 2 1138 3 1464 filter 1 33 07 226 403 1 403 3 1032 5 1404 13 03 091 390 1 390 3 955 4 1134 filter 2 33 07 212 524 1 524 3 1282 4 1522 13 03 085 504 1 504 2 899 4 1379 33 07 225 415 1 415 3 1060 5 1440 13 03 090 402 1 402 3 976 4 1158 table vi minimizing power consumption multiple processors nonlinear examples pn threshold pn product n processors prf power reduction factor transformations within divide conquer optimization framework new approach deals arbitrary computations also outperforms previous techniques limited computation types furthermore investigated coordinated impact compilation techniques new ultra low power technologies number processors provide optimal tradeoff cost power experimental results number reallife designs clearly indicates effectiveness proposed techniques algorithms r compiler transformations high performance computing automatic program parallelization scheduling framework minimizing memory requirements multirate signal processing algorithms expressed dataflow graphs optimizing power using transformations energy efficient programmable computation design considerations tools lowvoltage digital system design synthesis low power dsp circuits using activity metrics performance optimization sequential circuits eliminating retiming bottlenecks silicon insulator emerging highleverage technology code generation generalized neural network microarchitectural synthesis performanceconstrained scheduling dsp programs onto multiprocessors maximum throughput maximizing throughput high performance dsp applications using behavioral transformations advanced 05 mu cmossoi technology practical ultrahighspeed lowpower circuits critical path minimization using retiming algebraic speedup monotone data flow analysis frameworks optimization simulated annealing synchronous dataflow dataflow process networks power analysis minimization techniques embedded dsp software retiming synchronous circuitry look smallest program exploiting regularity lowpower design journal vlsi signal processing static rateoptimal scheduling iterative dataflow programs via optimum unfolding maximally fast arbitrarily fast implementation linear computations multiple constant multi plications efficient versatile framework algorithms exploring common subexpression elimination fast prototyping data path intensive architectures behavioral synthesis low power personal communications global node reduction linear systems using ratio analysis power conscious cad tools methodologies perspective transforming linear systems joint latency throughput optimization power optimization programmable processors asic implementations linear systems transformationbased approach using peephole optimization intermediate code depth first search linear graph algorithms power analysis 32bit embedded microcontroller asia south pacific design automation conference power analysis embedded software first step towards software power minimization database knowledgebase systems survey highlevel synthesis systems acm transactions programming languages approach ordering optimizing transformations acm symposium principles practice parallel programming loop transformation theory algorithm maximize parallelism tr static rateoptimal scheduling iterative dataflow programs via optimum unfolding power analysis embedded software power optimization programmable processors asic implementations linear systems global node reduction linear systems using ratio analysis maximally fast arbitrarily fast implementation linear computations fast prototyping datapathintensive architectures ctr johnson kin chunho lee william h mangionesmith miodrag potkonjak power efficient mediaprocessors design space exploration proceedings 36th acmieee conference design automation p321326 june 2125 1999 new orleans louisiana united states luca benini giovanni de micheli systemlevel power optimization techniques tools proceedings 1999 international symposium low power electronics design p288293 august 1617 1999 san diego california united states luca benini giovanni de micheli systemlevel power optimization techniques tools acm transactions design automation electronic systems todaes v5 n2 p115192 april 2000