superlinear convergence interiorpoint method despite dependent constraints show interiorpoint method monotone variational inequalities exhibits superlinear convergence provided standard assumptions hold except wellknown assumption jacobian active constraints full rank solution show superlinear convergence occurs even constantrank condition jacobian assumed earlier work hold b introduction consider following monotone variational inequality closed convex set c ae find z 2 c z 1 set c defined following algebraic inequality mapping phi assumed c 1 continuously differentiable monotone component function g delta gdelta convex twice continuously differentiable introducing gdelta explicitly problem 1 obtain following mixed nonlinear complementarity ncp problem find vector triple z 2 ir n2m gammagz 2 c 1 function defined well known 3 suitable conditions g slater constraint qualification z solves 1 exists multiplier z solves 2 show superlinear local convergence methods nonlinear programs eusually makes several assumptions regard solution point recently assumptions included local uniqueness solution z uniqueness condition relaxed somewhat 6 allow several multipliers corresponding department mathematics university melbourne parkville victoria 3052 australia work author supported australian research council mathematics computer science division argonne national laboratory 9700 south cass avenue argonne illinois 60439 usa work supported mathematical information computational sciences division subprogram office computational technology research us department energy contract w31109eng38 locally unique solution z 1 introducing constant rank condition gradients constraints g active z point article show superlinear convergence holds previous setting 6 even constant rank condition hold result lends theoretical support numerical observations 6 section 7 moreover believe superlinear convergence result shown interiorpoint methods whose search directions asymptotically pure newton affinescaling direction defined 6 briefly stated assumptions make obtain superlinear results follows monotonicity differentiability mapping z fz gammagz partial derivative respect z lipschitz near z positive definiteness condition ensure invertibility linear system solved iteration interiorpoint method slater constraint qualification g existence strictly complementary solution secondorder condition guarantees local uniqueness solution z 1 formal statement assumptions details given section 22 superlinear convergence proved methods nonlinear programming without strict complementarity sumption results typically require jacobian active constraints full rank see pang 5 bonnans 1 facchinei fischer kanzow 2 possibly best known application 1 convex programming problem defined min z oez subject z 2 c doe easy show formulation 23 equivalent standard karushkuhntucker kkt conditions 4 constraint qualification holds solutions 4 correspond via lagrange multipliers solutions 23 addition solutions 1 coincide consider solution 1 interiorpoint algorithm ralph wright 6 turn natural extension safestepfaststep algorithm wright 7 monotone linear complementarity problems algorithm based restatement problem 2 set constrained nonlinear equations gammalambday e5 4 r f z r g z residuals r f r g defined obvious way iterates z satisfy positivity conditions strictly interiorpoint algorithm viewed modified newtons method applied equality conditions 5 search directions step lengths chosen maintain positivity condition near solution algorithm takes steps along pure newton direction defined by4 z f dg 0 delta r g z solution deltaz delta deltay system also known affinescaling direction duality measure defined used frequently analysis measure nonoptimality infeasibility extend superlinear convergence result 6 without constant rank condition active constraint jacobian show affinescaling step defined 6 size hence superlinearity result extended algorithms take nearunit steps along directions asymptotically affinescaling direction since extending work 6 much analysis earlier paper much analysis earlier work carries without modification present case omit many details focus instead main technical result needed prove fast local convergencethe estimate deltaz delta affinescaling stepand restate enough earlier material make current note selfcontained 2 algorithm section review notation assumptions statement algorithm ralph wright 6 also state main global superlinear convergence results differ corresponding theorems 6 absence constant rank assumption 21 notation terminology use denote solution set 2 z denote projection onto first particular z defined assumption 4 define partition f1 basic nonbasic index sets b n solutions z solution z strictly complementary use n b denote subvectors correspond index sets n b respectively similarly use dgb z denote jbj theta n row submatrix dgz corresponding b finally specify arguments functions g dg f understood appropriate components current point z notation dg refers dgz 22 assumptions give formal statement assumptions needed global superlinear convergence motivation given refer reader earlier paper 6 details first assumption ensures mapping f defined 3 monotone respect z therefore mapping z fz gammagz monotone assumption 1 phi component function second assumption requires positive definiteness certain matrix projec tion ensure coefficient matrix newtonlike system solved step interiorpoint algorithm nonsingular see 11 assumption 2 twosided projection matrix z fz onto ker dgz positive definite z 2 ir n basis z ker dgz matrix z z fz z invertible note assumption trivially satisfied nonnegativity condition z 0 incorporated constraint function gdelta assume slater condition holds constraint function g assumption 3 vector z 2 c gz 0 next assume existence uniqueness strictly complementary solution assumption 4 strictly complementary solution z strict complementarity condition essential superlinear convergence number contexts besides ncp nonlinear programming see example wright 8 chapter 7 analysis linear programming monteiro wright 4 asymptotic properties interiorpoint methods monotone linear complementarity problems next make smoothness assumption phi g neighborhood first component z strictly complementary solution assumption 4 show 6 lemma 42 assumption z first component solutions assumption 5 matrixvalued functions dphi 2 g lipschitz continuous neighborhood z finally make invertibility assumption projection hessian onto kernel active constraint jacobian assumption essentially secondorder sufficient condition optimality assumption 6 let z defined assumption 4 let b z defined section 2 2 twosided projection z fz onto kerdg statements results refer set standing assumptions define follows standing assumptions assumptions 16 together assumption algorithm ralph wright 6 applied problem 2 generates infinite sequence fz limit point along assumptions 16 superlinear convergence result ralph wright 6 requires constant rank constraint qualification hold specific analysis paper requires existence open neighborhood u z matrix sequences fh k g ae fdgb z index sets j ae however analysis 6 assumption invoked section 54 justified reusing many results earlier sections paper indeed also reuse results later sections 6 applying constant matrices certainly satisfy constant rank condition algorithm makes use family defined positive parameters fi follows particular kth iterate z belongs omegagamma chooses sequences ffl k g ffi k g satisfy given notation easy see since iterates z belong omegagamma since residual norms kr f k kr g k bounded terms vectors set justified using alone indicator progress rather merit function also takes account residual norms assume sequence iterates limit point denote 6 theorem 32 z particularly interested points inomega lie close limit point define nearsolution neighborhood sffi 23 algorithm major computational operation algorithm repeated solution 2mdimensional linear systems form4 z f dg 0 delta r g z centering parameter oe lies range 0 1 2 equations simply newton equations nonlinear system equality conditions 2 except oe term algorithm searches along direction deltaz delta deltay obtained 11 iteration algorithm performs fast step along direction obtained solving 6 equivalently 11 choose neighborhoodomega k1 strictly larger thanomega k appropriate choice fl k1 fi k1 thereby allowing nontrivial step ff k taken along direction without leavingomega k1 fast step achieves least certain fixed decrease accepted new iterate otherwise resetomega k1 omega k defne safe step solving 11 oe chosen range oe 1 constant perform backtracking line search along direction stopping identify value ff k achieves sufficient decrease without leaving setomega k1 algorithm parametrized following quantities whose roles explained fully 6 expdelta exponential function constants fi min fl max related starting point z main algorithm follows terminate solution z else although may calculate fast step safe step iteration coefficient matrix 11 steps coefficient matrix factored safestep procedure defined follows choose oe 2 oe 1 ff solve 11 find deltaz delta deltay choose ff first element sequence ff following conditions satisfied return zff ff yff fast step routine described next solve 11 find deltaz delta deltay set define choose ff first element sequence ff following conditions satisfied return zff ff yff 24 convergence algorithm algorithm converges globally according following theorem theorem 21 ralph wright 6 theorem 32 suppose assumptions 1 2 hold either limit points fz belong however focus following local superlinear convergence theorem simply restatement 6 theorem 33 without constant rank condition active constraint jacobian matrix 6 assumption 7 theorem 22 suppose assumptions 1 2 3 4 5 6 satisfied sequence fz infinite limit point z algorithm eventually always takes fast steps sequence f k g converges superlinearly zero qorder least 1 ii sequence fz converges superlinearly z rorder least 1 proof result follows earlier paper respects except estimate affinescaling step calculated 6 remainder section devoted proving estimate holds given assumptions 3 estimate affinescaling step strategy proving estimate 12 step 6 based partitioning righthand side 6 following vectors useful defining partition 13a 13c 13d z defined assumption 4 z projection current point z onto set z z solution components righthand side 6 partitioned as4 r f r g gammalambday e5 4 j f gammalambday e5 4 ffl f define corresponding splitting affinescaling step following linear systems4 z f dg 0 define third variant 6 follows4 z f dg gammadg c deltaz c delta deltay7 5 4 split step c deltaz c delta c deltay deltaz c delta c u v v gammadg u gammadg assumption 2 matrices 15 16 17 19 20 invertible systems unique solutions basic strategy proving estimate 12 follows 6 section 53 without assuming constant rank condition positive constant constant rank assumption however needed 6 prove step component u v also article obtain estimate without constant rank assumption proving deltaz c delta c deltay z 2 sffi first result proved earlier paper 6 collects bounds useful throughout section lemma 31 6 lemma 51 suppose standing assumptions hold constant c 1 following bounds hold z 2 s1 22a 22c lemma 31 implies limit point z defined 9 second result follows lemma 32 cf 6 lemma 52 suppose standing assumptions satisfied constants z 2 deltaz c delta c deltay 17 u v 19 satisfy respectively proof claim first righthandside components 17 19 6 equation 78 positive constants c 21 1 lipschitz continuity assump tion 5 definitions 8 10 fact fz defined 13 constants l denotes lipschitz constant assumption 5 radius ffi 3 chosen lies inside neighborhood assumption 5 second righthand side component simply possible adjustment c 22 remaining righthandside component 17 trivially consider system 17 proof 6 lemma 52 eliminating c deltay component obtain deltaz c delta reduce system eliminating vector c delta n obtain z f dg gammadg deltaz c one easily see righthandside vector expression 27 28 bounds 22 imply b n n z 2 using estimate gamma1 31 recalling notation limit point sequence z 2 sffi 3 z fz dg gammadg deltaz c f ok c ok c denotes righthandside vector 29 partitioning c deltaz components ker dg b ran dg assumption 6 c deltaz bounded norm size righthand side 30 hence constant c 23 z 2 sffi 3 choosing enough c 23 5 result 24 follows simple manipulation inequality proof 25 similar next result others following make use positive diagonal matrix defined lemma 31 constant c 3 z 2 s1 lemma 33 cf 6 lemma 53 suppose standing assumptions satisfied defined lemma 32 c 4 0 z 2 proof first let ffi defined lemma 32 adjusted necessary ensure proof closely follows 6 lemma 53 spell details analytical techniques also needed later result theorem 38 recall splitting 18 step c deltaz c delta c deltay components defined 19 20 respectively multiplying last block row 20 gamma12 gamma12 using 31 find e using 20 obtain z f since z f positive semidefinite assumption 1 hence taking inner products sides 35 obtain therefore u v third block row 19 implies therefore gammadg z f used monotonicity z f define constant c 41 25 27 32 34 z 2 ffi 28 32 substituting last two bounds 37 obtain follows inequality standard argument constant c 42 depending c 41 combining bound 18 36 obtain deltak first part 33 follows define c second part 33 follows likewise bounds components c delta c deltay follow easily lemma 33 theorem 34 cf 6 theorem 54 suppose standing assumptions satisfied positive constants ffi c 5 delta deltay proof let ffi defined lemma 33 definition 31 bounds 33 c 2 n hence using 22 obtain min proves k c delta obvious choice c 5 bound c deltay b derived similarly lemma 35 cf 6 lemma 510 let 6 j ae b 6 k ae n twosided projection z fz onto ker dg b positive definite 2 ir n j 2 ir jj j z f dg gammadg addition dimker z f dg gammadg 0 gammai deltak proof result differs 6 lemma 510 z replaces z argument dgdelta proof essentially unchanged assumptions 5 6 twosided projection z fz onto kernel dg positive definite z sufficiently close limit point z defined 9 follows lemma 35 23 set ae z fz dg gammadg oe constant column rank theorem 36 suppose standing assumptions hold positive constant ffi z 2 ffi c deltaz c deltay n solution following convex quadratic program min subject gammadg ub delta n deltab c deltay b moreover constant c 6 deltay deltay b k proof value ffi 39 ffi theorem 34 suffices prove result technique proof familiar follows proof 6 theorem 512 closely omit details point proved first estimate 21 summarize following theorem theorem 37 suppose standing assumptions hold constants 7 z 2 ffi deltaz c delta c proof let ffi defined theorem 36 theorem 34 27 28 z 2 deltay hence 41 also deltay follows 24 k c last result concerned second estimate 21 involving relationship u v c deltaz c delta c deltay theorem 38 suppose standing assumptions hold positive constants ffi 0 c 8 deltay z 2 sffi proof taking differences 15 17 obtain4 z f dg 0 c c c deltay delta deltaz 26 lipschitz continuity dgdelta assumption 5 theorem 37 radius ffi 4 2 0 delta deltaz3 remainder proof follows 6 lemma 57 applying technique used lemma 32 system 42 using estimate 43 constants next note technique used second half proof lemma 33 used prove deltay diagonal scaling matrix defined 31 modifications needed account different righthand side estimate 43 different estimate 44 k c omit details 32 45 follows immediately deltay final estimate c obtained substituting expressions 44 corollary 39 suppose standing assumptions hold constants 9 affinescaling step defined 6 satisfies proof theorems 37 38 u defined theorem 38 moreover follows directly 6 section 53 possibly adjustment ffi hence result follows 14 4 conclusions result proved explains numerical experience reported section 7 ralph wright 6 convergence behavior test problems seemed regardless whether active constraint jacobian satisfied constant rank condition speculated 6 possible relaxation constant rank condition verified article fact condition dispensed altogether results possibly first proofs superlinear convergence nonlinear programming without multiplier nondegeneracy uniqueness r local study newton type algorithms constrained problems accurate identification active constraints survey theory local convergence interiorpoint algorithms degenerate monotone lcp convergence splitting newton methods complementarity problems application sensitivity results superlinear convergence interiorpoint method monotone variational inequalities tr ctr hiroshi yamashita hiroshi yabe quadratic convergence primaldual interior point method degenerate nonlinear optimization problems computational optimization applications v31 n2 p123143 june 2005 lus n vicente stephen j wright local convergence primaldual method degenerate nonlinear programming computational optimization applications v22 n3 p311328 september 2002 huang defeng sun gongyun zhao smoothing newtontype algorithm stronger convergence quadratically constrained convex quadratic programming computational optimization applications v35 n2 p199237 october 2006