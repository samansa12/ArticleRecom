trace cache microarchitecture evaluation abstractas instruction issue width superscalar processors increases instruction fetch bandwidth requirements also increase eventually become necessary fetch multiple basic blocks per clock cycle conventional instruction caches hinder effort long instruction sequences always contiguous cache locations trace caches overcome limitation caching traces dynamic instruction stream instructions otherwise noncontiguous appear contiguous paper present evaluate microarchitecture incorporating trace cache microarchitecture provides high instruction fetch bandwidth low latency explicitly sequencing program higher level traces terms 1 control flow prediction 2 instruction supply spec95 integer benchmarks tracelevel sequencing improves performance 15 percent 35 percent otherwise equally sophisticated contiguous multipleblock fetch mechanism performance improvement due trace cache however one benchmark whose performance limited branch mispredictions performance gain almost entirely due improved prediction accuracy b introduction high performance superscalar processor organizations divide naturally instruction fetch mechanism instruction execution mechanism two mechanisms separated instruction issue buffers example issue queues reservation stations conceptually instruction fetch mechanism acts producer fetches decodes dispatches instructions buffer instruction execution engine consumer issues instructions buffer executes subject data dependence resource constraints instruction issue buffers collectively called instruction window window mechanism exposing instructionlevel parallelism ilp sequential pro grams larger window increases opportunity finding dataindependent instructions may issue execute parallel thus trend superscalar design construct larger instruction windows provide wider sueexecution paths exploit corresponding increase available ilp trends place increased demand instruction supply mechanism particular peak instruction fetch rate match peak instruction issue rate benefit aggressive ilp techniques diminished paper concerned instruction fetch bandwidth becoming performance bottleneck current fetch units limited one branch prediction per cycle therefore fetch one basic block per cycle previous studies shown however average size basic blocks integer codes small around four six instructions 30 3 fetching single basic block cycle sufficient implementations issue four instructions per cycle processors higher peak issue rates multiple branch prediction 30 3 4 26 used fetch unit least fetch multiple contiguous basic blocks cycle shown paper fetching multiple contiguous basic blocks important upper bound fetch bandwidth still limited due frequency taken branches therefore taken branch encountered necessary fetch instructions taken path cycle branch fetched 11 trace cache job fetch unit feed dynamic instruction stream decoder problem instructions placed cache compiled order storing programs static form favors fetching code infrequent taken branches large basic blocks neither cases typical integer programs figure 1a shows example dynamic sequence basic blocks stored instruction cache arrows indicate taken branches even multiple branch predictions per cycle four cycles required fetch instructions basic blocks abcde instructions stored noncontiguous cache locations instruction cache b trace cache figure 1 storing noncontiguous sequence instructions reason several researchers proposed special instruction cache capturing long dynamic instruction sequences 15 22 23 24 21 structure called trace cache line stores snap shot trace dynamic instruction stream referring figure 1 dynamic sequence blocks appear noncontiguous instruction cache contiguous trace cache figure 1b primary constraint trace maximum length determined trace cache line size may number implementationdependent constraints number type embedded control transfer instructions special terminating conditions tuning various performance factors 25 trace fully specified starting address sequence branch outcomes describe path fol lowed first time trace encountered allocated line trace cache line filled instructions fetched instruction cache trace encountered course executing program ie starting address predicted branch outcomes available trace cache fed directly decoder single cycle otherwise fetching proceeds normally instruction cache high bandwidth fetch mechanisms proposed based conventional instruction cache 30 4 3 26 every cycle instructions noncontiguous locations fetched instruction cache assembled predicted dynamic sequence typically requires multiple pipeline stages 1 level indirection special branch target tables generate pointers noncontiguous instruction blocks 2 moderate highly interleaved instruction cache provide simultaneous access multiple lines possibility bank conflicts 3 complex alignment network shift align blocks dynamic program order ready decod ingrenaming trace cache approach avoids complexity caching dynamic instruction sequences rather information constructing predicted dynamic sequence exists trace cache recreated fly instruction caches static representation cost approach redundant instruction storage instructions may reside primary cache trace cache redundancy among different lines trace cache 12 related prior work alternative high bandwidth fetch mechanisms four previous studies focused mechanisms fetch multiple possibly noncontiguous basic blocks cycle instruction cache branch address cache 30 subgraph predictor 4 collapsing buffer 3 multipleblock ahead predictor 26 trace cache development melvin shebanow patt proposed fill unit multinodeword cache 18 16 first work qualitatively describes performance implications smaller larger atomic units work instructionset architecture isa compiler hardware levels authors argue small compiler atomic units large execution atomic units achieve highest performance fill unit proposed hardware mechanism compacting smaller compiler units large execution units stored reuse decoded instruction cache evaluates performance potential large execution atomic units although work evaluates sizes single vax instruction basic block also suggests joining two consecutive basic blocks intervening branch highly predictable 17 software basic block enlargement discussed spirit trace scheduling 5 trace selection 11 compiler uses profiling identify candidate basic blocks merging single execution atomic unit hardware sequences level execution atomic units created compiler advantage approach compiler optimize schedule across basic block boundaries franklin smotherman 6 extended fill units role dynamically assemble vliwlike instruction words risc instruction stream stored shadow cache structure eases issue complexity wide issue processor applied fill unit decoded instruction cache improve decoding performance complex instructionset computer cisc 27 cases cache lines augmented store trees improve utilization line four works independently proposed trace cache complexityeffective approach high bandwidth instruction fetching johnson 15 proposed expansion cache addresses cache alignment branch prediction throughput instruction run merging expansion process also predetermines execution schedule instructions line unlike pure vliw cache schedule may consist multiple cycles via cycle tagging peleg weiser 22 describe design dynamic flow instruction cache stores instructions independent virtual addresses defining characteristic trace caches rotenberg bennett smith 23 24 motivate concept comparisons high bandwidth fetch mechanisms proposed literature defines trace cache design space patel friendly patt 21 expand upon present detailed evaluations design space arguing prominent role trace cache mispredict recovery cache proposed bondi nanda dutta 1 caches instruction threads alternate paths mispredicted branches goal work quickly bypass multiple fetch decode stages long cisc pipeline following branch mispredict nair hopkins 19 employ dynamic instruction formatting cache large scheduled groups similar spirit cycle tagging approach expansion cache also recent work incorporating trace caches new processing models vajapeyam mitra 29 sundararaman franklin 28 rotenberg jacobson sazeides smith 25 exploit data control hierarchy implied traces overcome complexity architectural hurdles superscalar processors jacob son rotenberg smith 14 propose control prediction model well suited trace cache called next trace pre diction discussed later sections friendly patel patt propose new processing model called inactive issue reducing effects branch mispredictions 7 dynamically optimizing traces storing trace cache reducing execution time significantly 8 microcode vliw blockstructured isas clearly concept traces exists software realm instructionlevel parallelism early work fisher 5 hwu chang 11 others trace scheduling trace selection microcode recognized problem imposed branches code optimization subsequent vliw architectures novel isa techniques example 12 10 promote ability schedule long sequences instructions containing multiple branches 2 trace cache microarchitecture section 11 introduced concept trace cache instruction cache captures dynamic instruction sequences traces present microarchitecture organized around traces 21 tracelevel sequencing premise proposed microarchitecture shown figure 2 provide high instruction fetch bandwidth low latency achieved explicitly sequencing program higher level traces 1 control flow prediction 2 supplying instructions cache instruction branch trace trace cache outstanding trace buffers execution engine branch outcomes update figure 2 microarchitecture next trace predictor 14 treats traces basic units explicitly predicts sequences traces traces unit prediction rather individual branches high branch prediction throughput implicitly achieved single trace prediction per cycle jacobson et al 14 demonstrated explicit trace prediction removes fundamental constraints number branches trace usually consequence adapting single branch predictors multiple branch predictor counterparts 23 also holds potential achieving higher overall branch prediction accuracy single branch predictors details next trace prediction presented section 23 output trace predictor trace identifier given trace uniquely identified starting pc outcomes conditional branches embedded trace trace identifier used lookup trace trace cache index trace cache derived starting pc combination pc branch comes using branch outcomes index advantage providing path associativity multiple traces emi nating start pc reside simultaneously trace cache even direct mapped 24 output trace cache one traces depending cache associativity trace identifier stored trace order determine trace cache hit analogous tag conventional caches desired trace present cache one cached trace identifiers matches predicted trace identifier trace predictor trace cache together provide fast tracelevel sequencing unfortunately tracelevel sequencing always provide required trace particularly true start program new region code reached neither trace predictor trace cache learned traces yet instructionlevel sequencing discussed next section required construct nonexistent traces repair trace mispredictions 22 instructionlevel sequencing outstanding trace buffers figure 2 used 1 construct new traces trace cache 2 track branch outcomes become available execution engine allowing detection mispredictions repair traces containing fetched trace dispatched execution engine outstanding trace buffer case trace cache miss trace prediction received allocated buffer trace prediction provides enough information construct trace instruction cache although typically requires multiple cycles due predictedtaken branches case trace cache hit trace dispatched buffer allows repair partially mispredicted trace ie branch outcome returned execution match path indicated within trace event branch misprediction trace buffer begins reconstructing tail trace trace start pc incorrect using corrected branch target instruction cache subsequent branches trace secondlevel branch predictor used make predictions advocate aggressive instruction cache design providing robust performance broad range trace cache miss rates instruction cache 2way interleaved full cache line fetched cycle independent pc alignment 9 secondlevel branch prediction mechanism simple 2bit counter branch target stored branch logically instructions counters targets stored instruction cache opposed separate cache branch target buffer allow fast parallel prediction number nottaken branches call instruction fetch mechanism seqn keeping terminology 24 number de noted n sequential basic blocks line size fetched single cycle trace buffer constructing trace written trace cache dispatched execution engine newly constructed trace result misprediction recovery trace identifier also sent trace predictor repairing path history 23 next trace prediction next trace predictor shown figure 3 based jacobsons work pathbased highlevel control flow prediction 13 14 index correlated prediction table formed sequence past trace identifiers hash function used generate index called dolc func tion depth specifies path history depth terms traces oldest indicates number bits selected trace identifier except two recent ones last path pc path pc path hash pc path prediction table path trace cache figure 3 jacobsons next trace predictor current indicate number bits selected secondmost recent recent trace identifiers respectively entry correlated prediction table contains trace identifier 2bit counter replacement predictor augmented several mechanisms 14 ffl hybrid prediction addition correlated table second smaller table indexed recent trace identifier second table requires shorter learning time suffers less aliasing pressure ffl return history stack call instructions path history pushed onto special stack corresponding return point reached path history call restored improves accuracy control flow following subroutine highly correlated control flow call alternate trace identifier entry correlated table may augmented alternate trace predic tion form associativity predictor trace misprediction detected outstanding trace buffer responsible repairing trace use alternate prediction consistent known branch outcomes trace trace buffer resort secondlevel branch predic instructionlevel sequencing avoided altogether alternate trace also hits trace cache 24 trace selection performance trace cache strongly dependent trace selection algorithm used divide dynamic instruction stream traces trace selection primarily affects average trace length trace cache hit rate turn affect fetch bandwidth interaction trace length hit rate however well stood preliminary studies indicate longer traces result lower hit rates may artifact naive trace selection policies sophisticated selection techniques conscious control flow constructs loop backedges loop fallthrough points call sites reconvergent points general may lead different conclusions reader referred 21 25 20 interesting controlflow conscious selection heuristics trace selection paper constrained maximum trace length 16 instructions indirect branches returns jumpcall indirects terminate traces 25 hierarchical sequencing figure 4a portion dynamic instruction stream shown solid horizontal arrow left right stream divided traces t5 sequence traces produced independent instructions come trace predictortrace cache trace predictorinstruction cache branch predictorinstruction cache mispredicted branch hierarchical mispredicted branch b nonhierarchical figure 4 two sequencing models example trace predictor mispredicts t3 trace buffer assigned t3 resorts instructionlevel se quencing shown diagram series steps depicting smaller blocks fetched instruction cache trace buffer strictly adheres boundary t3 t4 dictated trace selection even final instruction cache fetch produces larger block sequential instructions needed t3 call process hierarchical sequencing exists clear distinction intertrace control flow intratrace control flow intertrace control flow ie trace boundaries effectively predetermined trace selection unaffected dynamic effects trace cache misses mispredictions contrasting sequencing model shown figure 4b model trace selection reset point mispredicted branch producing shifted traces t3 0 t4 0 t5 0 sequencing model work well pathbased next trace prediction resolving branch misprediction trace t3 0 subsequent traces must somehow predicted however requires sequence traces leading t3 0 sequence available dicated question marks diagram potential problem hierarchical sequencing misprediction recovery latency explicit next trace prediction uses level indirection trace first predicted trace cache accessed implies extra cycle added latency misprediction recovery ever extra cycle exposed first consider case alternate trace prediction used primary alternate predictions supplied trace predictor time stored together trace buffer therefore alternate prediction immediately available accessing trace cache misprediction detected second alternate used secondlevel branch predictor instruction cache used fetch instructions correct path case instruction cache accessed immediately correct branch target pc returned execution engine evaluation assume trace must fully constructed instructions dispatched execution engine traces efficiently renamed unit 29 25 aggravates trace misprediction trace cache miss recovery latency want make clear however due fundamental constraint fetch model artifact dispatch model 3 simulation methodology 31 fetch models evaluate performance trace cache microar chitecture compare several constrained fetch models first determine performance advantage fetching multiple contiguous basic blocks per cycle conventional single block fetching benefit fetching multiple noncontiguous basic blocks isolated models next trace predictor used control prediction two reasons first next trace prediction highly accurate whether predicting one many branches time comparable better best single branch predictors literature second desirable common underlying predictor fetch models separate performance due fetch bandwidth due branch prediction section 32 differentiates following models trace selection algorithm ffl seq1 sequential 1 block trace single basic block 16 instructions length ffl seqn sequential n blocks trace may contain number sequential basic blocks instruction limit trace cache trace may contain number conditional branches taken nottaken instructions first indirect branch seq1 seqn models use trace cache interleaved instruction cache capable supplying trace single cycle 9 consequence sequential selection constraint therefore one may view seq1seqn fetch unit identical trace cache microarchitecture figure 2 except trace cache block replaced conventional instruction cache next trace predictor drives conventional instruction cache trace buffers used construct traces l2 cachemain memory present cache finally establish upper bound performance noncontiguous instruction fetching introduce fourth model tcperfect identical tc trace cache always hits 32 isolating trace predictortrace cache performance interesting sideeffect trace selection significantly affects trace prediction accuracy general smaller traces resulting constrained trace selec tion result lower accuracy determined least two reasons first longer traces naturally capture longer path history compensated using trace identifiers path history traces small good dolc function one trace length necessarily good another tc model dolc depth 7 traces consistently performs well benchmarks 14 seq1 seqn brief search design space shows depth 17 traces performs well observed however tuning dolc parameters enough trace selection affects accuracy ways graph figure 5 shows trace predictor performance using unbounded table ie using full unhashed path history make predictions graph shows trace mispredictions per 1000 instructions seq1 seqn tc trace selection history depth var ied go benchmark trace mispredictions seqn model dip 88 per 1000 instructions whereas tc model reaches 80 trace mispredictions per 1000 instructions unconstrained trace selection results creation many unique traces trace explosion generally negative impact trace cache performance hypothesize also creates many unique contexts making predictions large prediction table exploit additional context ideal trace prediction go913171 6 11 history depth traces trace misp1000 instr seqn figure 5 impact trace selection unbounded trace predictor performance conclude difficult separate performance advantage trace cache trace predictor show positive improvement longer traces nonetheless compare tc seqn seq1 would like know much benefit derived trace cache end developed methodology statistically adjust overall branch prediction accuracy given fetch model match another model trace predictor adjusted produces predictions normal fashion however making prediction predicted trace compared actual trace determined advance functional simulator running parallel timing simulator prediction correct actual trace substituted mispredicted trace probability words fraction mispredicted traces corrected probability injecting corrections chosen perbenchmark basis achieve desired branch misprediction rate methodology introduces two additional fetch mod els seq1adj seqnadj corresponding ad justed seq1 seqn models clearly models unrealizable useful performance comparisons adjusted branch misprediction rates match tc model 33 simulator benchmarks detailed fullyexecution driven superscalar processor simulator used evaluate trace cache microarchitec ture simulator developed using simplescalar platform 2 platform uses mipslike instruction set gccbased compiler create binaries datapath fetch engine shown figure 2 faithfully modeled next trace predictor 2 tries dolc functions compressing path history 16bit index described earlier section 32 tc seq models trace cache configuration size associativity indexing varied sufficient outstanding trace buffers keep instruction window full trace buffers share single port combined instruction cache secondlevel branch predic tor instruction cache 64kb 4way setassociative 2way interleaved line size 16 instructions cache hit miss latencies 1 cycle 12 cycles respectively secondlevel branch predictor consists 2bit counters branch targets assumed logically stored branch instruction cache instruction window 256 instructions used experiments processor 16way superscalar ie processor fetch issue 16 instructions cycle five basic pipeline stages modeled instruction fetch dispatch take 1 cycle issue takes least 1 cycle possibly instruction must stall operands 16 instructions including loads stores may issue cycle execution takes fixed latency based instruction type plus time spent waiting result bus instructions retire order loads stores address generation takes 1 cycle cache access 2 cycles hit data cache 64kb 4way setassociative line size 64 bytes miss penalty 14 cycles realistic aggressive memory disambiguation modeled loads may proceed ahead unresolved stores memory hazards detected store addresses become available recovery via selective reissuing misspeculated loads dependent instructions 25 seven spec95 integer benchmarks shown table simulated completion table 1 benchmarks benchmark input dataset dynamic instr count gcc o3 genrecogi 117m jpeg vigoppm 166m li queens 7 202m perl scrabblpl scrabblin 108m vortex persons250 101m 4 results 41 performance fetch models figure 6 shows performance six fetch models terms retired instructions per cycle ipc model section uses 64kb instruction storage 4way setassociative trace cache trace cache indexed using pc ie explicit path associativity except afforded 4 ways357go gcc jpeg li perl m88k vortex ipc tcperfect figure 6 performance fetch models draw several conclusions graph figure 6 first comparing seqn models seq1 models apparent predicting fetching multiple sequential basic blocks provides significant performance advantage conventional singleblock fetching graph figure 7 shows performance advantage seqn model seq1 model ranges 5 25 majority benchmarks showing greater 15 improvement similar results hold whether branch prediction accuracy adjusted seqn seq1 models first observation important seqn model requires sophisticated highlevel control flow predictor retains moreorless conventional instruction cache microarchitecture 0 5 10 15 20 30 go gcc jpeg li perl m88k vort improvement ipc seqn seq1 seqnadj seq1adj figure 7 speedup seqn seq1 second ability fetch multiple possibly noncontiguous basic blocks improves performance significantly sequentialonly fetching graph figure 8 shows performance advantage tc model seqn model ranges 15 35 speedup tc seqn 0 5 10 15 20 30 40 go gcc jpeg li perl m88k vort improvement ipc trace cache trace predictor figure 8 speedup tc seqn figure 8 also isolates contributions next trace prediction trace cache performance lower part bar speedup model seqnadj seqn since overall branch misprediction rate seqn adj adjusted match tc model part bar approximately isolates impact next trace prediction performance top part bar therefore isolates impact trace cache performance go suffers noticeably branch mispredictions benchmarks benefit model comes next trace prediction case longer traces tc model clearly valuable improving context used next trace predictor providing raw instruction bandwidth gcc however next trace prediction trace cache contribute equally performance five benchmarks benefit mostly higher fetch bandwidth finally figure 6 shows moderately large trace cache tc model nearly reaches performance upper bound established tcperfect within 4 table shows trace branchrelated measures average trace lengths tc range 124 li 158 jpeg instructions 16 2 times longer seqn traces table also shows predictor performance primary alternate trace mispredictions per 1000 instructions overall branch misprediction rates latter computed checking branch retirement see caused mis prediction whether originating trace predictor secondlevel branch predictor cases prediction improves longer traces tc 20 45 fewer trace mispredictions seq1 resulting 15 jpeg 41 m88ksim fewer total branch mispredictions note adjusted branch misprediction rates seq models nearly equal tc shorter traces however generally result better alternate trace prediction accuracy shorter traces result 1 fewer total traces thus less aliasing 2 fewer possible alternative traces given starting pc benchmarks except gcc go alternate trace prediction almost always correct given primary trace prediction incorrect predictions taken together result fewer 1 trace misprediction per 1000 instructions trace caches introduce redundancy instruction appear multiple times one traces table 2 shows two redundancy measures overall redundancy factor rf overall computed maintaining table unique traces ever retired redundancy ratio total number instructions total number unique instructions traces collected table rf overall independent trace cache configuration capture dynamic behavior dynamic redundancy factor rf dyn computed similarly using traces trace cache given cycle final value average cycles rf dyn measured using 64kb 4way trace cache rf overall varies 29 vortex 14 go rf dyn less rf overall ranges 2 4 fixed size trace cache limits redundancy perhaps temporally less redundancy 42 trace cache size associativity section measure performance tc model function trace cache size associativity figure 9 shows overall performance ipc 12 trace cache config urations direct mapped 2way 4way associativity four sizes 16kb 32kb 64kb 128kb associativity noticeable impact performance table 2 trace statistics model measure gcc go jpeg li m88k perl vort trace length 49 62 83 42 48 51 58 trace misp1000 88 145 52 69 35 34 15 seq1 alt trace misp1000 21 45 branch misp rate 50 110 77 37 22 22 11 adjusted misp rate 36 82 66 32 13 14 08 trace length 72 80 96 63 60 71 82 trace misp1000 73 127 46 69 33 31 12 seqn alt trace misp1000 27 54 05 09 06 03 03 branch misp rate 44 101 70 37 21 20 09 adjusted misp rate 36 81 67 31 13 14 08 trace length 139 148 158 124 131 130 144 trace misp1000 54 96 42 55 20 21 10 alt trace misp1000 27 53 09 13 05 03 03 branch misp rate 36 82 67 31 13 15 08 control instr per trace 28 23 13 29 25 25 23 rf overall 71 144 53 31 37 41 29 rf dyn 30 33 37 32 31 29 213545556575 trace cache size ipc jpeg perl vort go gcc li4 figure 9 performance vs sizeassociativity benchmarks except go go particularly large working set unique traces 25 total capacity important individual trace conflicts curves jpeg li fairly flat size little importance yet increasing associativity improves performance two benchmarks suffer general conflict misses otherwise size improve performance yet conflicts among traces start pc significant associativity allows simultaneously caching pathassociative traces performance improvement largest configuration 128kb 4way respect smallest one 16kb direct mapped ranges 4 go 10 gcc figure shows trace cache performance misses per 1000 instructions trace cache size varied along x axis six curves direct mapped dm 2 way 2w 4way 4w associative caches without indexing path associativity pa chose somewhat arbitrarily following index function achieving path associativity loworder bits pc form set index highorder bits index xored first two branch outcomes trace identifier gcc go benchmarks fit entirely within largest trace cache observed earlier go many heavilyreferenced traces resulting fewer 20 misses1000 instructions path associativity reduces misses substantially particularly direct mapped caches except vortex path associativity closes gap direct mapped 2way associative caches half often entirely misses1000 instr go253545 misses1000 instr li51525m88ksim2610misses1000 instr perl5152535 trace cache size trace cache size misses1000 instr dm dmpa 4wpa figure 10 trace cache misses 5 summary important design instruction fetch units capable fetching past multiple possibly taken branches cycle trace caches provide capability without complexity latency equivalentbandwidth instruction cache designs evaluated microarchitecture incorporating trace cache following major results ffl trace cache improves performance 15 35 otherwise equallysophisticated contiguous multipleblock fetch mechanism ffl longer traces improve trace prediction accuracy mispredictionbound benchmark go factor contributes almost entirely observed performance gain ffl moderately large associative trace cache performs well perfect trace cache go however trace mispredictions mask poor trace cache performance ffl overall performance sensitive trace cache size associativity one might expect due part robust instructionlevel sequencing ipc varies 10 wide range configurations ffl complexity advantage trace cache comes price redundant instruction storage gcc factor 7 redundancy among traces created corresponding factor 3 redundancy trace cache ffl instruction cache combined aggressive trace predictor fetch number contiguous basic blocks per cycle yielding 5 25 improvement singleblock fetching acknowledgments research trace caches genesis stimulating group discussions guri sohi students todd austin scott breach andreas moshovos dionisios pnevmatikatos n vijaykumar contribution gratefully acknowledged would also like give special thanks quinn jacobson valuable input providing access next trace prediction simulators work supported part nsf grants mip 9505853 mip9307830 us army intelligence center fort huachuca contract dabt63 95c0127 arpa order d346 eric rotenberg supported ibm fellowship r integrating misprediction recovery cache mrc superscalar pipeline evaluating future mi croprocessors simplescalar toolset optimization instruction fetch mechanisms high issue rates control flow prediction treelike subgraphs superscalar processors trace scheduling technique global microcode compaction alternative fetch issue policies trace cache fetch mechanism putting fill unit work dynamic optimizations trace cache microproces sors branch fixedpoint instruction execution units increasing instruction fetch rate via blockstructured instruction set ar chitectures trace selection compiling large c application programs microcode control flow speculation multiscalar processors expansion caches superscalar processors performance benefits large execution atomic units dynamically scheduled machines exploiting finegrained parallelism combination hardware software techniques hardware support large atomic units dynamically scheduled machines exploiting instruction level parallelism processors caching scheduled groups improving trace cache effectiveness branch promotion trace packing critical issues regarding trace cache fetch mechanism dynamic flow instruction cache memory organized around trace segments independent virtual address line trace cache low latency approach high bandwidth instruction fetch ing trace cache low latency approach high bandwidth instruction fetch ing trace processors improving cisc instruction decoding performance using fill unit multiscalar execution along single flow control improving superscalar instruction dispatch issue exploiting dynamic code se quences increasing instruction fetch rate via multiple branch prediction branch address cache tr ctr emil talpes diana marculescu power reduction work reuse proceedings 2001 international symposium low power electronics design p340345 august 2001 huntington beach california united states bartolini c prete proposal inputsensitivity analysis profiledriven optimizations embedded applications acm sigarch computer architecture news v32 n3 p7077 june 2004 emil talpes diana marculescu execution cachebased microarchitecture powerefficient superscalar processors ieee transactions large scale integration vlsi systems v13 n1 p1426 january 2005 emil talpes diana marculescu increased scalability power efficiency using multiple speed pipelines acm sigarch computer architecture news v33 n2 p310321 may 2005 michael behar avi mendelson avinoam kolodny trace cache sampling filter acm transactions computer systems tocs v25 n1 p3es february 2007 oliverio j santana ayose falcn alex ramirez mateo valero branch predictor guided instruction decoding proceedings 15th international conference parallel architectures compilation techniques september 1620 2006 seattle washington usa oliverio j santana alex ramirez josep l larribapey mateo valero lowcomplexity fetch architecture highperformance superscalar processors acm transactions architecture code optimization taco v1 n2 p220245 june 2004 sangjeong lee penchung yew augmenting trace cache highbandwidth value prediction ieee transactions computers v51 n9 p10741088 september 2002 xianglong huang stephen blackburn david grove kathryn mckinley fast efficient partial code reordering taking advantage dynamic recompilatior proceedings 2006 international symposium memory management june 1011 2006 ottawa ontario canada bartolini c prete optimizing instruction cache performance embedded systems acm transactions embedded computing systems tecs v4 n4 p934965 november 2005 yoav almog roni rosner naftali schwartz ari schmorak specialized dynamic optimizations highperformance energyefficient microarchitecture proceedings international symposium code generation optimization feedbackdirected runtime optimization p137 march 2024 2004 palo alto california michele co dee b weikle kevin skadron evaluating trace cache energy efficiency acm transactions architecture code optimization taco v3 n4 p450476 december 2006 independence trace processors proceedings 32nd annual acmieee international symposium microarchitecture p415 november 1618 1999 haifa israel roni rosner yoav almog micha moffie naftali schwartz avi mendelson power awareness selective dynamically optimized traces acm sigarch computer architecture news v32 n2 p162 march 2004 roni rosner micha moffie yiannakis sazeides ronny ronen selecting long atomic traces high coverage proceedings 17th annual international conference supercomputing june 2326 2003 san francisco ca usa alex ramirez oliverio j santana josep l larribapey mateo valero fetching instruction streams proceedings 35th annual acmieee international symposium microarchitecture november 1822 2002 istanbul turkey