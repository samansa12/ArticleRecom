spectral decomposition nonsymmetric matrices distributed memory parallel computers implementation performance class divideandconquer algorithms computing spectral decomposition nonsymmetric matrices distributed memory parallel computers studied paper presenting general framework focus spectral divideandconquer sdc algorithm newton iteration although algorithm requires several times many floating point operations best serial qr algorithm simply constructed small set highly parallelizable matrix building blocks within level 3 basic linear algebra subroutines blas efficient implementations building blocks available wide range machines illconditioned cases algorithm may lose numerical stability easily detected compensated forthe algorithm reached 31 efficiency respect underlying pumma matrix multiplication 82 efficiency respect underlying scalapack matrix inversion 256 processor intel touchstone delta system 41 efficiency respect matrix multiplication cmssl thinking machines cm5 vector units performance model predicts performance reasonably accuratelyto take advantage geometric nature sdc algorithms designed graphical user interface let user choose spectral decomposition according specified regions complex plane b introduction standard technique parallel computing build new algorithms existing high performance building blocks example lapack linear algebra library 1 writ department mathematics university kentucky lexington ky 40506 computer science division mathematics department university california berkeley ca 94720 z department computer science university tennessee knoxville tn 37996 mathematical sciences section oak ridge national laboratory oak ridge tn 37831 x department computer science university tennessee knoxville tn 37996 department mathematics university california berkeley ca 94720 computer science division university california berkeley ca 94720 ten terms basic linear algebra subroutines blas38 23 22 efficient implementations available many workstations vector processors shared memory parallel machines recently released scalapack 10beta linear algebra library 26 written terms parallel block blas pbblas 15 basic linear algebra communication subroutines blacs 25 blas lapack scalapack includes routines lu qr cholesky factorizations matrix inversion ported intel gamma delta paragon thinking machines cm5 pvm clusters connection machine scientific software library cmssl54 provides analogous functionality high performance cm5 work use high performance kernels build two new algorithms finding eigenvalues invariant subspaces nonsymmetric matrices distributed memory parallel computers algorithms perform spectral divide conquer ie recursively divide matrix smaller submatrices subset original eigenvalues one algorithm uses matrix sign function evaluated newton iteration 8 42 6 4 algorithm avoids matrix inverse required newton iteration called inverse free algorithm 30 10 44 7 algorithms simply constructed small set highly parallelizable building blocks including matrix multiplication qr decomposition matrix inversion describe section 2 using existing high performance kernels scalapack cmssl achieved high efficiency 256 processor intel touchstone delta system sign function algorithm reached 31 efficiency respect underlying matrix multiplication pumma 16 4000by4000 matrices 82 efficiency respect underlying scalapack 10 matrix inversion thinking machines cm5 vector units hybrid newtonschultz sign function algorithm obtained 41 efficiency respect matrix multiplication cmssl 32 2048by2048 matrices nonsymmetric spectral decomposition problem recently resisted attempts parallelization conventional method use hessenberg qr algorithm one first reduces matrix schur form swaps desired eigenvalues along diagonal group together order form desired invariant subspace 1 algorithm appeared required fine grain parallelism difficult parallelize 5 27 57 recently henry van de geijn32 shown hessenburg qr algorithm phase effectively parallelized distributed memory parallel computers 100 processors although parallel qr appear scalable algorithms presented paper may faster wide range distributed memory parallel computers algorithms perform several times many floating point operations qr nearly within level 3 blas whereas implementations qr performing fewest floating point operations use less efficient level 1 2 blas thorough comparison algorithms subject future paper parallel eigenproblem algorithms developed include earlier par allelizations qr algorithm 29 50 56 55 hessenberg divide conquer algorithm using either newtons method 24 homotopies 17 39 40 jacobis method 28 47 48 49 methods suffer use finegrain parallelism instability slow misconvergence presence clustered eigenvalues original problem constructed subproblems 20 three methods paper may less stable qr algorithm may fail converge number circumstances fortunately easy detect compensate loss stability choosing divide spectrum slightly different location compared approaches mentioned believe algorithms discussed paper offer effective tradeoff parallelizability stability algorithms closely related approaches used may found 3 9 36 symmetric matrices generally matrices real spectra treated another advantage algorithms described paper compute eigenvalues corresponding invariant subspace userspecified region complex plane help user specify region describe graphical user interface algorithms rest paper organized follows section 2 present two algorithms spectral divide conquer single framework show divide spectrum along arbitrary circles lines complex plane discuss implementation details section 3 discuss performance algorithms intel delta cm5 section 4 present model performance algorithms demonstrate predict execution time reasonably accurately section 5 describes design xwindow user interface section 6 draws conclusions outlines future work parallel spectral divide conquer algorithms spectral divide conquer sdc algorithms discussed paper presented following framework let jordan canonical form eigenvalues l theta l submatrix j eigenvalues inside selected region complex plane eigenvalues eigenvalues outside assume eigenvalues boundary otherwise reselect move region slightly invariant subspace matrix corresponding eigenvalues inside spanned first l columns x matrix 0 corresponding spectral projector let qrpi rank revealing qr decomposition matrix p q unitary r upper triangular pi permutation matrix chosen leading l columns q span range space p q yields desired spectral decomposition 11 12 eigenvalues 11 eigenvalues inside eigenvalues 22 eigenvalues outside substituting complementary projector gamma p p 22 11 eigenvalues outside 22 eigenvalues inside crux parallel sdc algorithm efficiently compute desired spectral projector p without computing jordan canonical form 21 sdc algorithm newton iteration first sdc algorithm uses matrix sign function introduced roberts 46 solving algebraic riccati equation however soon extended solving spectral decomposition problem 8 recent studies may found 11 42 6 matrix sign function signa matrix eigenvalues imaginary axis defined via jordan canonical form 21 eigenvalues j open right half plane eigenvalues j gamma open left half plane signa 0 easy see matrix spectral projector onto invariant subspace corresponding eigenvalues l number eigenvalues gamma spectral projector corresponding eigenvalues let qrpi rank revealing qr decomposition projector p q yields desired spectral decomposition 23 eigenvalues 11 eigenvalues eigenvalues 22 eigenvalues since matrix sign function signa satisfies matrix equation use newtons method solve matrix equation obtain following simple iteration iteration globally ultimately quadratically convergent lim j1 provided pure imaginary eigenvalues 46 35 iteration fails otherwise finite precision iteration could converge slowly close pure imaginary eigenvalues many ways improve accuracy convergence rate basic iteration 12 33 37 example may use called newtonschulz iteration avoid use matrix inverse although requires twice many flops efficient whenever matrix multiply least twice efficient matrix inversion newtonschulz iteration also quadratically convergent provided hybrid iteration might begin newton iteration ka 2 switch newtonschulz iteration discuss performance one hybrid later hence following algorithm divides spectrum along pure imaginary axis algorithm 1 sdc algorithm newton iteration let convergence j j max end compute compute 11 12 compute stopping criterion newton iteration say machine precision j max limits maximum number iterations say j return generally nonzero quantity measures backward stability computed decomposition since setting e 21 zero decoupling problem 11 22 backward error simplicity use qr decomposition column pivoting reveal rank although sophisticated rankrevealing schemes exist 14 31 34 51 variations newton iteration global convergence still need compute inverse matrix explicitly one form another dealing illconditioned matrices instability newton iteration computing matrix sign function subsequent spectral decomposition discussed 11 6 4 references therein 22 sdc algorithm inverse free iteration algorithm needs explicit matrix inverse could cause numerical instability matrix illconditioned following algorithm originally due godunov bulgakov malyshev 30 10 44 modified bai demmel gu 7 eliminates need matrix inverse divides spectrum along unit circle instead imaginary axis first describe algorithm briefly explain works algorithm 2 sdc algorithm inverse free iteration let convergence j j max r j end compute compute 11 12 compute algorithm 1 need choose stopping criterion inner loop well limit j max maximum number iterations convergence eigenvalues 11 eigenvalues inside unit disk eigenvalues 22 eigenvalues outside assumed eigenvalues unit circle algorithm 1 quantity measures backward stability illustrate algorithm works assume matrices want invert invertible inner loop algorithm see 22 j r j 22 j b j gamma1 22 therefore algorithm simply repeatedly squaring eigenvalues driving ones inside unit circle 0 outside 1 repeated squaring yields quadratic convergence analogous sign function iteration computing aa gamma1 2 equivalent taking cayley transform taking inverse cayley transform explanation algorithm works found 7 attraction algorithm equally well deal generalized nonsymmetric eigenproblem gamma b provided problem regular ie identically zero one simply start algorithm b regarding qr decomposition inner loop need form entire 2n theta 2n unitary matrix q order get submatrices q 12 q 22 instead compute qr decomposition 2n theta n matrix b h implicitly householder vectors lower triangular part matrix another n dimensional array apply q without computing 2n theta n matrix 0 obtain desired matrices q 12 q 22 show compute q rank revealing qr decomposition computing explicit inverse subsequent products yield ultimate inverse free algorithm recall purposes need unitary factor q rank turns using generalized qr decomposition technique developed 45 2 get desired information without computing fact order compute qr decomposition pivoting first compute qr decomposition pivoting matrix compute rq factorization matrix q h 27 28 r q desired unitary factor rank r 1 also rank matrix 23 spectral transformation techniques although algorithms 1 2 divide spectrum along pure imaginary axis unit circle respectively use mobius simple transformations input matrix divide along general curves result compute eigenvalues corresponding invariant subspace inside region defined intersection regions defined curves major attraction kind algorithm let us show use mobius transformations divide spectrum along arbitrary lines circles transform eigenproblem az z apply algorithm 1 fii split spectrum respect region 0 apply algorithm 2 split along curve example computing matrix sign function algorithm 1 split spectrum along circle centered radius r real choose real arithmetic real split spectrum along circle centered radius r real choose real arithmetic algorithm real x figure 1 different geometric regions spectral decomposition general regions obtained taking 0 polynomial function example computing matrix sign function divide spectrum within bowtie shaped region centered ff figure 1 illustrates regions algorithms deal assuming real algorithms use real arithmetic 24 tradeoffs algorithm 1 computes explicit inverse could cause numerical instability matrix illconditioned provides alternative approach achieving better numerical stability difficult problems algorithm 2 gives accurate answer algorithm 1 numerical examples found 7 however neither algorithm avoids accuracy convergence difficulties associated eigenvalues close boundary selected region stability advantage inverse free approach obtained cost storage arithmetic algorithm 2 needs 4n 2 storage space algorithm 1 certainly limit problem size able solve furthermore one step algorithm 2 6 7 times arithmetic one step algorithm 1 qr decomposition major component algorithm 2 matrix inversion main component algorithm 1 require comparable amounts communication per flop see table 4 details therefore algorithm 2 expected run significantly slower algorithm 1 algorithm 1 faster somewhat less stable algorithm 2 since testing stability easy compute use following 3 step algorithm 1 try use algorithm 1 split spectrum succeeds stop 2 otherwise try split spectrum using algorithm 2 succeeds stop 3 otherwise use qr algorithm 3step approach works trying fastest least stable method first falling back slower stable methods necessary paradigm also used parallel algorithms 19 fast parallel version qr algorithm32 becomes available would probably faster inverse free algorithm hence would obviate need second step listed algorithm 2 would still interest subset spectrum desired qr algorithm necessarily computes entire spectrum generalized eigenproblem matrix pencil gamma b 3 implementation performance started fortran 77 implementation algorithm 1 code built using blas lapack basic matrix operations lu decomposition triangular inversion qr decomposition initially tested software sun ibm rs6000 workstations cray preliminary performance data matrix sign function based algorithm reported 6 report focus implementation performance evaluation algorithms distributed memory parallel machines namely intel delta cm5 implemented algorithm 1 collected large set data performance primitive matrix operation subroutines target machines performance evaluation comparison two algorithms applications progress 31 implementation performance intel touchstone intel touchstone delta computer system 16 theta 32 mesh i860 processors wormhole routing interconnection network 41 located california institute technology behalf concurrent supercomputing consortium deltas communication characteristics described 43 order implement algorithm 1 natural rely scalapack 10 library beta version 26 choice requires us exploit two key design features package first scalapack library relies parallel block blas pbblas15 hides much interprocessor communication hiding communication makes possible express algorithms using pbblas thus avoiding explicit calls communication routines pbblas implemented top calls blas basic linear algebra communication subroutines blacs25 second scalapack assumes data distributed according square block cyclic decomposition scheme allows routines achieve well balanced computations minimize communication costs scalapack includes subroutines lu qr cholesky factorizations use building blocks implementation pumma routines 16 provide required matrix multiplication matrix order time scalapack 256 pes intel touchstone delta timing second gemm inv qrp 1000 2000 3000 4000 5000 6000 7000 800051525matrix order mflops per node scalapack 256 pes intel touchstone delta mflops per node gemm inv qrp figure 2 performance scalapack 10 beta version subroutines 256 16 theta 16 pes intel touchstone delta system matrix inversion done two steps lu factorization computed upper triangular u matrix inverted gamma1 obtained substitution l using blocked operations leads performance comparable obtained lu factorization implementation qr factorization without column pivoting based parallel algorithm presented coleman plassmann 18 qr factorization column pivoting much larger sequential component processing one column time needs update norms column vectors step makes using blocked operations impossible induces high synchronization overheads however see cost step remains negligible comparison time spent newton iteration unlike qr factorization pivoting qr factorization without pivoting post premultiplication orthogonal matrix use blocked operations figure 2 plots timing results obtained pumma package using blacs general matrix multiplication scalapack 10 beta version subroutines matrix inversion qr decomposition without column pivoting corresponding tabular data found appendix measure efficiency algorithm 1 generated random matrices different sizes whose entries normally distributed mean 0 variance 1 computations performed real double precision arithmetic table 1 lists measured results backward error number newton iterations total cpu time megaflops rate particular second column table contains backward errors number newton iterations parentheses note convergence rate problemdata dependent table 1 see 4000by4000 matrix algorithm reached 719231231 efficiency respect pumma matrix multiplica tion 71987082 efficiency respect underlying scalapack 10 beta matrix inversion subroutine performance model shows tables 9 10 11 12 14 confirm efficiency continue improve matrix size n increases table 1 backward accuracy timing seconds megaflops algorithm 1 256 node intel touchstone delta system timing mflops mflops gemmmflops invmflops iter seconds total per node per node per node 1000 2000 3000 4000 5000 6000 7000 8000 9000135matrix size gflops newton iteration based algorithm intel delta system gflops figure 3 performance algorithm 1 intel delta system function matrix size different numbers processors performance model explained section 4 figure 3 shows performance algorithm 1 intel delta system function matrix size different numbers processors table gives details total cpu timing newton iteration based algorithm summarized table 1 clear newton iteration sign function expensive takes 90 total running time compare standard sequential algorithm also ran lapack driver routine dgees computing schur decomposition reordering eigenvalues one i860 processor took 592 seconds matrix order 600 91 megaflopssecond assuming time scales like n 3 predict matrix order 4000 matrix able fit single node dgees would take 175000 seconds 48 hours compute desired spectral decomposition contrast algorithm 1 would take 1436 seconds 24 minutes 120 times faster however note dgees actually computes complete schur decomposition necessary reordering spectrum algorithm 1 decomposes spectrum along pure imaginary axis applications may users want decomposition along finer region complete schur decomposition desired cost newton iteration based algorithms increased though likely first step described table 2 performance profile 256 processor intel touchstone delta system time seconds 1000 1230691 6875 4275 13422 2000 4139592 18604 16134 44869 3000 7170490 36765 38375 79218 take time 13 32 implementation performance cm5 thinking machines cm5 introduced 1991 tests section run processor cm5 university california berkeley cm5 node contains fpu 64 kb cache four vector floating points units memory front end 33 hmz sparc 32 mb memory vector units peak 64bit floating point performance 128 megaflops per node 32 megaflops per vector unit see 53 details algorithm 1 implemented cm fortran cmf version 21 implementation fortran 77 supplemented arrayprocessing extensions ansi iso draft standard fortran 90 53 cmf arrays come two flavors distributed across cm processor memory user defined layout allocated normal column major fashion front end alone front end computer executes cm fortran pro gram performs serial operations scalar data stored memory sends instructions array operations cm receiving instruction node executes data necessary cm processors access others memory three communication mechanisms transparent cmf programmer 52 also used cmssl version 32 54 tmcs library numerical linear algebra rou tines cmssl provides data parallel implementations many standard linear algebra routines designed used cmf exploit vector units cmssls qr factorization available without pivoting uses standard householder transformations column blocking performed users discretion improve load balance increase parallelism scaling available avoid situations column norm close underflow overflow expensive insurance policy scaling used current cm5 code perhaps made available toolbox informed user qr pivoting qrp factorization routine shall use reveal rank half fast qr without pivoting due part elimination blocking techniques pivoting columns must processed sequentially gaussian elimination without partial pivoting available compute lu factorizations perform back substitution solve system equations matrix inversion matrix order time cmssl 32 32 pes vus cm5 timing second gemm inv qrp matrix order mflops per node cmssl 32 32 pes vus cm5 mflops per node gemm inv qrp figure 4 performance cmssl 32 subroutines 32 pes vus cm5 performed solving system lu factors obtained separately support balzers byers scaling schemes accelerate convergence newton require determinant computation routine estimating ka lu factors detect illconditioning intermediate matrices newton iter ation factorization inversion routines balance load permuting matrix blocking specified user used improve performance lu qr matrix multiplication routines outofcore counterparts support matricessystems large fit main memory current cm5 implementation sdc algorithms use outofcore routines principle algorithms permit outofcore solutions used figure 4 summarizes performance cmssl routines underlying implementation algorithm 1 tested newtonschulz iteration based algorithm computing spectral decomposition along pure imaginary axis since matrix multiplication twice fast matrix inversion see figure 4 entries random test matrices uniformly distributed gamma1 1 use inequality ka n switching criterion newton iteration 25 newtonschulz iteration 26 ie relaxed convergence condition newtonschulz iteration optimized performance test cases ran table 3 shows measured results backward accuracy total cpu time megaflops rate second column table backward error number newton iterations number newtonschulz iterations respectively table see comparing cmssl 32 matrix multiplication performance obtain 32 45 efficiency matrices sizes 512 2048 even faster cmssl 32 matrix inverse subroutine profiled total cpu time phase algorithm found 83 total time spent newton iteration 9 qr decomposition pivot 4actual predicted gemm inverse iter1 iter2 seconds seconds total per node per node per node table 3 backward accuracy timing seconds megaflops sdc algorithm newtonschulz iteration 32 pes vus cm5 ing 75 matrix multiplication newtonschulz iteration orthogonal transformations performance model model based actual operation counts scalapack implementation following problem parameters measured machine parameters matrix size p number processors b block size 2d block cyclic matrix data layout 20 lat time required send zero length message one processor another band time required send one double word one processor another dgemm time required per blas3 floating point operation models building blocks given table 4 model created counting actual operations critical path load imbalance cost represents discrepancy amount work busiest processor must perform amount work processors must perform models building blocks validated performance data shown appendix load imbalance increases block size increases based operation counts predict performance also estimate importance various suggested modifications either algorithm implementation hardware general predicting performance risky many factors control actual performance including compiler various library routines however since majority time spent algorithm 1 spent either blacs level 3 pbblas15 turn implemented calls blacs25 blas38 23 22 long performance blacs blas computation communication cost load imbalance cost task cost latency bandwidth gamma1 computation bandwidth gamma1 tri 4n 3 matrix multiply p lat 1 lg p householder application table 4 models building blocks inversions applications computation cost theta n 3 latency cost thetan lat 16020 lg p 3 lg p 16023lg p bandwidth cost theta n 2 imbalanced computation cost theta bn 2 imbalanced bandwidth cost thetabn band 2035 lg p 2035 lg p table 5 model algorithm 1 well understood input matrix small predict performance algorithm 1 distributed memory parallel computer table 5 predicted running time steps algorithm 1 displayed summing times table 5 yields using measured machine parameters given table 8 equation 49 yields predicted times table 7 table 3 get table 4 table 5 hence equation 49 made number simplifying assumptions based empirical results assume 20 newton iterations required assume time required send single message double words lat regardless many messages sent system although many patterns communication scalapack implementation majority communication time spent collective communica tions ie broadcasts reductions rows columns therefore choose lat band based programs measure performance collective communications assume perfectly square p pby processor grid assumptions allow us keep model simple understandable limit accuracy somewhat table performance newton iteration based algorithm algorithm 1 spectral decomposition along pure imaginary axis backward errors sec total sec total sec total 2000 3000 table 7 predicted performance newton iteration based algorithm algorithm 1 spectral decomposition along pure imaginary axis actual predicted actual predicted actual predicted time 2000 50257 4443 44869 3623 33634 3108 3000 103703 9947 79218 7568 57668 6104 tables 6 7 show model underestimates actual time delta 20 machine problem sizes timed table 3 shows model matches performance cm5 within 25 problem sizes except smallest ie main sources error model 1 uncounted operations small blas1 blas2 calls data copying norm computations 2 nonsquare processor configurations 3 differing numbers newton iterations required 4 communications costs fit linear model 5 matrix multiply costs fit constant costflop model 6 higher cost qr decomposition pivoting believe uncounted operations account main error model small n actual number newton iterations varies exactly 20 newton iterations needed nonsquare processor configurations slightly less efficient square ones actual communication costs fit linear model depend upon details many processors sending data simultaneously processors sending actual matrix multiply costs depend upon matrix model performance measured values parameter description limited cm5 dgemm blas3 peak flop rate 190 134 lat message latency comm software 150 157 table 8 machine parameters sizes involved leading dimensions actual starting locations matrices cost individual call blacs blas may differ model 20 however differences tend average entire execution data layout ie number processor rows processor columns block size critical performance algorithm assume efficient data layout specifically means roughly square processor configuration fairly large block size say 16 32 cost redistributing data input routine would tiny 2 p band compared total cost algorithm optimal data layout lu decomposition different optimal data layout computing u former prefers slightly processor rows columns latter prefers slightly processor columns rows addition lu decomposition works best small block size 6 delta example whereas computing u best done large block size 30 delta example difference significant enough believe slight overall performance gain maybe 5 10 could achieved redistributing data two phases even though redistribution would done twice newton step table 3 shows except n 512 model estimates performance algorithm 1 based cmssl reasonably well note table newtonshultz iteration scheme slightly efficient cm5 newton based iteration introduces another small error fact model matches performance cmssl based routine whose internals examined indicates us implementation matrix inversion cm5 probably requires roughly operation counts scalapack implementation performance figures table 8 measured independent program except cm5 blas3 performance communication performance figures delta table 8 report littlefield 1 43 communication performance figures cm5 measured whaley 2 58 computation performance delta linpack benchmark21 1 processor delta entry 1 processor cm5 linpack benchmark dgemm table 8 chosen experience graphical user interface sdc take advantage graphical nature spectral decomposition process graphical user interface gui implemented sdc written c based x11r5s 1 blacs use protocol 2 communication pattern closely resembles shift timings lat table 8 in58 band table 5 code xi code user parallel execution interface 7 routines figure 5 x11 interface xi sdc standard xlib library xt toolkit mits athena widget set nicknamed xi x11 interface xi paired code implementing sdc call union xsdc programmers interface xi consists seven subroutines designed independently specific sdc implementation thus xi attached sdc code present use cm5 cmfcmssl implementation fortran 77 version algorithm use real arithmetic figure 1 shows coupling sdc code xi library subroutines basically sdc code calls xi routine handles interaction user returns next request parallel computation sdc code processes request parallel engine necessary calls another xi routine inform user computational results user selected split spectrum point size highlighted region error bound computation along performance information reported user given choice confirming refusing split appropriate action taken depending choice process repeated user decides terminate program data structures pertaining matrix decomposition process managed xi binary tree records size status solvednot solved diagonal block corresponding spectral region error bounds split information x11 interface manage decomposition data frees sdc programmer responsibilities encapsulates decomposition process sdc programmer obtains useful information via interface subroutines figure 6 pictures sample session xsdc cm5 500 theta 500 matrix large central window called spectrum window represents region complex plane indicated axes title xsdc eigenvalues schur vectors indicates task compute eigenvalues schur vectors matrix analysis figure sample xsdc session lines spectrum window axes result spectral divide conquer shading indicates bowtie region complex plane currently selected analysis windows raisedlowered users request show details process described later buttons top control io appearance spectrum window algorithmic choices ffl file lets one save matrix one working start new matrix quit ffl zoom lets one navigate around complex plane zooming part spectrum window toggle turns features spectrum window example axes gershgorin disks eigenvalues ffl function lets one modify algorithm display less detail progress made buttons bottom used splitting spectrum example clicking right halfplane clicking point spectrum window split spectrum two halfplanes point right halfplane selected division would signal sdc code decompose matrix k 11 12 k eigenvalues 11 eigenvalues right halfplane eigenvalues 22 eigenvalues left halfplane button left halfplane works similarly except left halfplane would selected processing roles 11 22 would reversed manner inside circle outside circle divide complex plane boundary circle eastwest crosslines northsouth crosslines split spectrum lines 45 degrees real axis described split information window lower right corner figure 2 keeps track matrix splitting process reports two splits performed arrive current shaded spectral region first eastwest crossline split point 15 real axis divided entire complex plane four sectors drawing two lines sigma 45 degrees point 15 real axis sdc decomposed starting matrix 260 11 12 east west sectors correspond 11 block north south sectors correspond 22 block continuing eastwest sectors indicated previous split region divided two subregions separated boundary circle radius 4 centered origin circle drawn making sure boundary intersects east west sectors matrix reduced tob 106 154 240 106 11 12 13 shading indicates bowtie region corresponding interior circle 11 block currently selected analysis upper right corner figure 2 matrix information window displays status matrix decomposition process three entries corresponds spectral region square diagonal block 3 theta 3 block upper triangular matrix informs us blocks size whether eigenvalues eigenvectors schur vectors computed maximum error bound encountered along path decomposition process highlighted entry corresponds shaded region reports 11 block contains 106 eigenvalues solved error 144 theta 10 gamma13 eigenvalues listed window overlapping matrix information window plotted spectrum users request user may select region complex plane hence submatrix diagonal decomposition clicking pointer desired region click point 10 imaginary axis example would unhighlight current region shade north south sectors since region corresponds 33 block third entry matrixinformation window would highlighted splitinformation window would also updated detail single split performed arriving region spectrum block small enough user may choose solve via function button top spectrum window case eigenvalues schur vectors block would computed using qr per users request eigenvalues plotted spectrum current xi code supports real sdc extended handle complex case implementations complex sdc become available 6 conclusions future work written codes solve one hardest problems numerical linear algebra spectral decomposition nonsymmetric matrices implementation uses highly efficient matrix computation kernels available public domain distributed memory parallel computer vendors performance attained encouraging approach merits consideration numerical algorithms object oriented user interface xi developed paper provides paradigm us future design user friendly interface massively parallel computing environment note approaches discussed extended compute right left deflating subspaces regular matrix pencil gamma b see 4 7 details spectrum repeatedly partitioned divideandconquer fashion obviously task parallelism available independent submatrices arise well data parallellike matrix operations considered paper analysis 13 indicates task parallelism contribute small constant factor speedup since work root divideandconquer tree simplify implementation future work include implementation performance evaluation based algorithm comparison parallel qr extension algorithms generalized spectral decomposition problem integration 3step approach see section 23 object oriented user interface acknowledgements bai demmel supported part arpa grant dm28e04120 via subcontract argonne national laboratory demmel petitet supported part nsf grant asc9005933 demmel dongarra robinson supported part arpa contact daal0391c0047 administered army research office ken stanley supported nsf graduate student fellowship dongarra also supported part office scientific computing us department energy contract deac05 84or21400 work performed part using intel touchstone delta system operated california institute technology behalf concurrent supercomputing consortium access facility provided center research parallel computing r generalized qr factorization applictions parallelizable eigensolvers design parallel nonsymmetric eigenroutine toolbox block implementation hessenberg multishift qr eration design parallel nonsymmetric eigenroutine toolbox inverse free parallel spectral divide conquer algorithms nonsymmetric eigenproblems computational method eigenvalue eigenvectors matrix real eigenvalues divide conquer method tridiagonalizing symmetric matrices repeated eigenvalues circular dichotomy spectrum matrix numerical stability instability matrix sign function based algorithms solving algebraic riccati equation matrix sign function benefit mixed parallelism rank revealing qr factorizations pbblas set parallel block basic linear algebra subprograms pumma parallel universal matrix multiplication algorithms distributed memory concurrent computers note homotopy method linear algebraic eigenvalue problems parallel nonlinear leastsquares solver theoretical analysis numerical results trading parallelism numerical stability parallel numerical linear algebra performance various computers using standard linear equations soft ware set level 3 basic linear algebra subprograms extended set fortran basic linear algebra subroutines parallel algorithm nonsymmetric eigenvalue problem users guide blacs design linear algebra libraries high performance computers multishift qr algorithm worth trouble schur decomposition matrix parallel computation finding eigenvalues eigenvectors unsymmetric matrices using distributed memory multiprocessor problem dichotomy spectrum matrix efficient algorithm computing rankrevealing qr de composition parallelizing qr algorithm unsymmetric algebraic eigenvalue problem myths reality computing polar decomposition applications rank revealing qr svd sign matrix separation matrix eigenvalues parallel implementation invariant subspace decomposition algorithm dense symmetric matrices rational iteration methods matrix sign function basic linear algebra subprograms fortran usage solving eigenvalue problems nonsymmetric matrices real homotopies touchstone parallel algorithm computing eigenvalues unsymmetric matrix simd mesh processors characterizing tuning communication performance touchstone delta ipsc860 parallel algorithm solving spectral problems linear algebra aspects generalized qr factorization linear model reduction solution algebraic riccati equation jacobi jacobilike algorithms parallel computer parallel algorithm eigenvalues eigenvectors general complex matrix jacobilike algorithm computing schur decomposition nonhermitian matrix parallel implementation qr algorithm updating rankrevealing ulv decomposition cm fortran reference manual connection machine cm5 technical summary cmssl cm fortran cm5 edition implementing qr algorithm array processors efficient parallel implementation nonsymmetric qr algorithm shifting strategies parallel qr algorithm basic linear algebra communication subroutines analysis implementation across multiple parallel architectures tr ctr peter benner enrique quintanaort gregorio quintanaort statespace truncation methods parallel model reduction largescale systems parallel computing v29 n1112 p17011722 novemberdecember peter benner ralph byers rafael mayo enrique quintanaort vicente hernndez parallel algorithms lq optimal control discretetime periodic linear systems journal parallel distributed computing v62 n2 p306325 february 2002 leo chin sim graham leedham leo chin jian heiko schroder fast solution large n n matrix equations mimdsimd hybrid system parallel computing v29 n1112 p16691684 novemberdecember peter benner maribel castillo enrique quintanaort vicente hernndez parallel partial stabilizing algorithms large linear control systems journal supercomputing v15 n2 p193206 feb12000