privacypreserving data mining data mining attack privacy advocates misunderstanding actually valid concern generally done article shows technology security community change data mining better providing benefits still maintaining privacy b introduction explosive progress networking storage processor technologies led creation ultra large databases record unprecedented amount transactional information tandem dramatic increase digital data concerns informational privacy emerged globally tim97 eco99 eu998 off98 privacy issues exacerbated world wide web makes easy new data automatically collected added databases concerns massive collection data naturally extending analytic tools applied data data mining promise efficiently discover valuable nonobvious information large databases particularly vulnerable misuse cm96 the98 off98 ecb99 fruitful direction future research data mining development techniques incorporate privacy concerns agr99 specifically address following question since primary task data mining development models aggregated data develop accurate models without access precise information individual data records underlying assumption person willing selectively divulge information exchange value models provide wes99 example value provided include filtering weed unwanted information better search results less effort automatic triggers hs99 recent survey web users classified 17 respondents privacy fundamentalists provide data web site even privacy protection measures place ever concerns 56 respondents constituting pragmatic majority significantly reduced presence privacy protection measures remaining 27 marginally concerned generally willing provide data web sites although often expressed mild general concern privacy another recent survey web users wes99 found 86 respondents believe participation information forbenefits programs matter individual privacy choice resounding 82 said privacy policy would matter 14 said important long got benefit furthermore people equally protective every field data records wes99 cra99a specifically person ffl may divulge values certain fields ffl may mind giving true values certain fields ffl may willing give true values modified values certain fields given population satisfies assump tions address concrete problem building decisiontree classifiers bfos84 qui93 show possible develop accurate models respecting users privacy concerns classification one used tasks data mining decisiontree classifiers relatively fast yield comprehensible models obtain similar sometimes better accuracy classification methods mst94 related work extensive research area statistical databases motivated desire able provide statistical information sum count average maximum minimum pth percentile etc without compromising sensitive information individuals see excellent surveys aw89 sho82 proposed techniques broadly classified query restriction data perturbation query restriction family includes restricting size query result eg fel72 dds79 controlling overlap amongst successive queries eg djl79 keeping audit trail answered queries constantly checking possible compromise eg co82 suppression data cells small size eg cox80 clustering entities mutually exclusive atomic populations eg yc77 perturbation family includes swapping values records eg den82 replacing original database sample distribution eg lst83 lcl85 rei84 adding noise values database eg tyw84 war65 adding noise results query eg bec80 sampling result query eg den80 negative results showing proposed techniques cannot satisfy conflicting objectives providing high quality statistics time prevent exact partial disclosure individual information aw89 statistical quality measured terms bias pre cision consistency bias represents difference unperturbed statistics expected value perturbed estimate precision refers variance estimators obtained users consistency represents lack contradictions para doxes exact disclosure occurs issuing one queries user able determine exact value confidential attribute individual partial disclosure occurs user able obtain estimator whose variance given threshold share statistical database literature goal preventing disclosure confidential information obtaining high quality point estimates goal see sufficient us able reconstruct sufficient accuracy original distributions values confidential tributes adopt statistics literature two methods person may use system modify value field cs76 ffl valueclass membership partition values set disjoint mutuallyexhaustive classes return class true value x falls ffl value distortion return value x x r random value drawn distribution discuss methods level privacy provide next section use value dissociation third method proposed cs76 method value returned field record true value field record interestingly recent proposal ecb99 construct perturbed training sets based method hesitation approach global method requires knowledge values records problem reconstructing original distribution given distribution viewed general framework inverse problems ehn96 fjs97 shown smooth enough distributions eg slowly varying time signals possible fully recover original distribution nonoverlapping contiguous partial sums partial sums true values available us cannot make priori assumptions original distribution know distribution used randomizing values attribute rich query optimization literature estimating attribute distributions partial information bdf 97 olap literature work approximating queries subcubes higherlevel aggregations eg bs97 however works cope information intentionally distorted closely related orthogonal work extensive literature access control security eg sensitive information exchanged must transmitted secure channel stored securely purposes paper assume appropriate access controls security procedures place effective preventing unauthorized access system relevant work includes efforts create tools standards provide platform implementing system eg wor ben99 gwb97 paper organization discuss privacypreserving methods section 2 also introduce quantitative measure evaluate amount privacy offered method evaluate proposed methods measure section 3 present reconstruction procedure reconstructing original data distribution given perturbed distribution also present empirical evidence efficacy reconstruction procedure section 4 describes techniques building decisiontree classifiers perturbed training data using reconstruction proce dure present experimental evaluation accuracy techniques section 5 conclude summary directions future work section 6 consider numeric attributes section 6 briefly describe propose extend work include categorical attributes focus attributes users willing provide perturbed values attribute users willing provide even perturbed value simply ignore attribute users provide value training data treated containing records missing values effective techniques exist literature bfos84 qui93 privacypreserving methods basic approach preserving privacy let users provide modified value sensitive attributes modified value may generated using custom code browser plugin extensions products microsofts passport httpwwwpassportcom novells digitalme httpwwwdigitalmecom consider two methods modifying values cs76 valueclass membership method values attribute partitioned set disjoint mutuallyexclusive classes consider special case discretization values attribute discretized intervals intervals need equal width example salary may discretized 10k intervals lower values 50k intervals higher values instead true attribute value user provides interval value lies discretization method used often hiding individual values value distortion return value x x r random value drawn distribution consider two random distributions ffl uniform random variable uniform distribution gammaff ff mean random variable 0 ffl gaussian random variable normal distribution mean deviation oe fis63 fix perturbation entity thus possible snoopers improve estimates value field record repeating queries aw89 21 quantifying privacy quantifying privacy provided method use measure based closely original values modified attribute estimated confidence 50 95 999 discretization 05 theta w 095 theta w 0999 theta w uniform 05 theta 2ff 095 theta 2ff 0999 theta 2ff gaussian 134 theta oe 392 theta oe 68 theta oe table 1 privacy metrics estimated c confidence value x lies interval x 1 interval width defines amount privacy c confidence level table 1 shows privacy offered different methods using metric assumed intervals equal width w discretization clearly discretization provide amount privacy ff increases privacy also increases keep uniform discretization increase interval width hence reduce number intervals note interested high privacy use 25 50 100 200 range values attribute experiments hence discretization lead poor model accuracy compared uniform since values interval modified value gaussian provides significantly privacy higher confidence levels compared two methods therefore focus two value distortion methods rest paper 3 reconstructing original distribution concept using value distortion protect privacy useful need able reconstruct original data distribution randomized data note reconstruct distributions values individual records view n original data values x onedimensional distribution realizations n independent identically distributed iid random variables distribution random variable x hide data values n independent random variables 1 used distribution different random variable given x 1 realization cumulative distribution function fy would like estimate cumulative distribution function fx x reconstruction problem given cumulative distribution fy realizations n iid random samples estimate fx let value individual values x sum use bayes rule fis63 estimate posterior distribution function f 0 given assuming know density functions fx fy x respectively x1 z z using bayes rule density functions z expanding denominator r inner integral independent outer r independent r estimate posterior distribution function f 0 average distribution functions x r corresponding posterior density function f 0 x obtained differentiating f 0 1 given sufficiently large number samples expect x equation close real density function f x however although know know fx hence use uniform distribution initial estimate f 0 x iteratively refine estimate applying equation 1 algorithm sketched figure 1 using partitioning speed computation assume partitioning domain data values intervals make two approximations 1 example standard normal f 2e gammaz 2 2 repeat stopping criterion met figure 1 reconstruction algorithm ffl approximate distance z w w distance midpoints intervals lie ffl approximate density function f x average density function interval lies let ix denote interval x lies mi p midpoint interval p mx midpoint interval ix let fx p average value density function interval p ie r r ip dz applying two approximations equation 1 get let denote k intervals l p width interval p replace integral denominator sum since mz f x iz change within interval 2 compute average value posterior density function interval p z ip z ipn substituting equation 2 z ipn since r gaussian plateau b triangles2006001000 number records attribute value original randomized number records attribute value original randomized reconstructed uniform c plateau triangles2006001000 number records attribute value original randomized number records attribute value original randomized reconstructed figure 2 reconstructing original distribution let n p number points lie interval p ie number elements set fw jw g since points lie within finally let pr 0 x 2 p posterior probability x belonging interval p ie pr 0 x 2 p multiplying sides equation using prx 2 p substitute equation 3 step 3 algorithm figure 1 compute step 3 om 2 time 2 naive implementation equation 3 lead om 3 time however since denominator independent ip reuse results computation get om 2 time stopping criterion omniscience would stop reconstructed distribution statistically original distribution using say 2 goodnessoffit test cra46 alternative compare observed randomized distribution result randomizing current estimate original distribution stop two distributions statistically intuition two distributions close expect estimate original distribution also close real distribution unfortunately found empirically difference two randomized distributions reliable indicator difference original reconstructed distributions instead compare successive estimates original distribution stop difference successive estimates becomes small 1 threshold 2 test implementation empirical evaluation two original distributions plateau triangles shown original line figures 2a b respectively add gaussian random variable mean 0 standard rid age salary credit risk 43 40k high training salary 50k high high low age 25 b decision tree figure 3 credit risk example deviation 025 point distribution thus point value say 025 95 chance mapped value 026 074 999 chance mapped value 06 11 effect randomization shown randomized line apply algorithm partitioning figure 1 partition width 005 results shown reconstructed line notice able pick original shape distribution even though randomized version looks nothing like original figures 2c show adding uniform discrete random variable 05 05 point gives similar results 4 decisiontree classification randomized data 41 background begin brief review decision tree classifi cation adapted mar96 sam96 decision tree bfos84 qui93 class discriminator recursively partitions training set partition consists entirely dominantly examples class nonleaf node tree contains split point test one attributes determines data partitioned figure 3b shows sample decisiontree classifier based training shown figure 3a age 25 salary 50k two split points partition records high low credit risk classes decision tree used screen future applicants classifying high low risk categories decision tree classifier developed two phases growth phase prune phase growth partitiondata begin 1 points class 2 return 3 attribute evaluate splits attribute use best split partition 1 initial call partitiontrainingdata figure 4 treegrowth phase phase tree built recursively partitioning data partition contains members belonging class tree fully grown pruned second phase generalize tree removing dependence statistical noise variation may particular training data figure 4 shows algorithm growth phase growing tree goal node determine split point best divides training records belonging node use gini index bfos84 determine goodness split data set containing examples classes relative frequency class j split divides two subsets 1 index divided data gini split given gini split calculating index requires distribution class values partitions 42 training using randomized data induce decision trees using perturbed training data need modify two key operations treegrowth phase figure ffl determining split point step 4 ffl partitioning data step 5 also need resolve choices respect reconstructing original distribution ffl global reconstruction using whole data first partition data class reconstruct separately class ffl reconstruction root node reconstruction every node discuss issues pruning phase based minimum description length principle mar96 modification needed determining split points since partition domain intervals reconstructing distribu tion candidate split points interval bound aries standard algorithm every midpoint two consecutive attribute values candidate split point candidate splitpoint use statistics reconstructed distribution compute gini index partitioning data reconstruction procedure gives us estimate number points interval let 1 intervals n p estimated number points interval p associate data value interval sorting values assigning n lowest values interval 1 3 split occurs boundary interval pgamma1 p points associated go 1 points associated intervals go 2 retain association points intervals case split attribute different splitpoint lower tree reconstructing original distribution consider three different algorithms differ distributions reconstructed ffl global reconstruct distribution attribute beginning using complete perturbed training data induce decision tree using reconstructed data ffl byclass attribute first split training data class reconstruct distributions separately class induce decision tree using reconstructed data ffl local byclass attribute split training data class reconstruct distributions separately class however instead reconstruction reconstruction done node ie step 4 figure 4 avoid overfitting reconstruction stopped number records belonging node become small final detail regarding reconstruction concerns number intervals domain attribute partitioned use heuristic determine number intervals choose average 100 points per interval bound 10 100 intervals ie set 10 etc clearly local expensive algorithm terms execution time global cheapest 3 interval associated data value considered estimator original value data value algorithm byclass falls however closer global local since reconstruction done byclass root node whereas repeated node local empirically evaluate classification accuracy characteristics algorithms next section 43 deployment many applications goal building classification model develop understanding different classes target population techniques described directly apply applications applications classification model used predicting class new object without preassigned class label prediction accurate although able build accurate model using randomized data application needs access non perturbed data user willing disclose solution dilemma structure application classification model shipped user applied instance classification model used filter information relevant user classifier may first applied client side original data information presented filtered using results classification 5 experimental results 51 methodology compare classification accuracy global byclass local algorithms respect following benchmarks ffl original result inducing classifier unperturbed training data without randomization ffl randomized result inducing classifier perturbed data without making corrections randomization clearly want come close original accuracy possible accuracy gain randomized reflects advantage reconstruction used synthetic data generator agi experiments used training set 100000 records test set 5000 records equally split two classes table 2 describes nine attributes table 3 summarizes five classification functions functions vary quite simple decision surface function 1 complex nonlinear surfaces functions 4 5 functions 2 3 may look easy quite difficult distribution values age identical classes unless classifier first splits salary classifier exactly find five splitpoints salary 25 50 75 100 125 perfectly classify data width intervals less group group b function function function elevel function 4 067 theta salary function 5 067 theta salary hvalue theta maxhyears gamma 20 table 3 description functions attribute description salary uniformly distributed 20k 150k commission salary 75k uniformly distributed 10k 75k age uniformly distributed 20 80 elevel uniformly chosen 0 4 car uniformly chosen 1 20 zipcode uniformly chosen 9 zipcodes hvalue uniformly distributed k theta 50k k theta 150k k 2 f0 depends zipcode hyears uniformly distributed 1 loan uniformly distributed 0 500k table 2 attribute descriptions 20 range attribute function 2 also contains embedded xors known troublesome decision tree classifiers perturbed training data generated using uniform gaussian methods section 2 accuracy results involving randomization averaged runs experimented large values amount desired privacy ranging 25 200 range values attribute confidence threshold privacy level taken 95 experiments recall estimated 95 confidence value x lies interval interval width amount privacy 95 confidence level example 50 privacy salary cannot estimated 95 confidence closer interval width 65k half entire range salary similarly 100 privacy age cannot estimated 95 confidence closer interval width 60 entire range age 52 comparing classification algorithms figure 5 shows accuracy algorithms uniform gaussian perturbations privacy levels 25 100 xaxis shows five functions table 3 yaxis accuracy overall byclass local algorithms remarkably well 25 50 privacy accuracy numbers close original data even high 100 privacy algorithms within 5 absolute original accuracy functions 1 4 5 within 15 functions 2 3 advantage reconstruction seen graphs comparing accuracy algorithms randomized overall global algorithm performs worse byclass local algorithms deficiency global uses merged distribution classes reconstruction original distribution fares well functions 4 5 performance even randomized quite close original functions functions diagonal decision surface equal number points side diagonal surface hence addition noise significantly affect ability classifier approximate surface hyperrectangles stated beginning section though might look easy functions 2 3 quite difficult classifier find five splitpoints salary width interval 25k observe range randomizing function spreads 95 values 5 times width splits 100 privacy hence even small errors reconstruction result split points little accuracy drops poor accuracy original function 2 25 privacy may appear anomalous explanation lies privacy gaussian uniform8090100 accuracy original byclass global accuracy original byclass global randomized privacy gaussian uniform6080100 accuracy original byclass global accuracy original byclass global randomized figure 5 classification accuracy buried xor function 2 original reaches corresponding node stops find split point increases gini however due perturbation data randomization algorithms find false split point proceed find real split 53 varying privacy figure 6 shows effect varying amount privacy byclass algorithm omitted graph function 4 since results almost identical function 5 similar results obtained local algorithm xaxis shows privacy level ranging 10 200 yaxis accuracy algorithms legend byclassg refers byclass gaussian randomu refers randomized uniform etc two important conclusions drawn graphs ffl although uniform perturbation original data results much large degradation accuracy correction compared gaussian effect distributions quite comparable correction ffl accuracy classifier developed using perturbed data although identical comes fairly close original ie accuracy obtained using unperturbed data 6 conclusions future work paper studied technical feasibility realizing privacypreserving data mining basic premise sensitive values users record perturbed using randomizing function cannot estimated sufficient precision randomization done using gaussian uniform perturbations question addressed whether given large number users perturbation function 1 function 26080100 accuracy privacy level original accuracy privacy level original function 3 function 56080100 accuracy privacy level original accuracy privacy level original figure change accuracy privacy still construct sufficiently accurate predictive models specific case decisiontree classification found two effective algorithms byclass local algorithms rely bayesian procedure correcting perturbed distributions emphasize reconstruct distributions individual records thus preserving privacy individual records matter fact user perturbs sensitive value always return perturbed value estimate true value cannot improved successive queries found empirical evaluation ffl byclass local effective correcting effects perturbation 25 50 privacy levels accuracy numbers close original data even 100 privacy algorithms within 5 15 absolute original accuracy recall privacy measured 95 confidence 100 privacy means true value cannot estimated closer interval width entire range corresponding attribute believe small drop accuracy desirable tradeoff privacy many situations ffl local performed marginally better byclass required considerably computation investigation characteristics might make local winner byclass open problem ffl privacy level uniform perturbation significantly worse gaussian correcting randomization slightly worse correcting randomization hence choice applying uniform gaussian distributions preserve privacy based considerations gaussian provides privacy higher confidence thresholds uniform may easier explain users future work plan investigate effectiveness randomization reconstruction categorical tributes basic idea randomize categorical value follows retain value probability p choose one values random probability may derive equation similar equation 1 iteratively reconstruct original distribution values alternately may able extend analytical approach presented war65 boolean attributes derive equation directly gives estimates original distribution acknowledgments hallway conversation robert morris provided initial impetus work peter haas diligently checked soundness reconstruction procedure r privacy critics ui components safeguard users privacy tomasz imielin ski data mining crossing chasm security mechanism statistical databases truste online privacy seal program quasi cubes exploiting approximations multidimensional databases security privacy implications data mining auditing infrence control statistical databases suppression methodology statistical disclosure control mathematical methods statistics beyond concern understanding net users attitudes online privacy cranor editor special issue internet privacy selective partial access database tracker threat statistical database security secure statistical databases random sample queries cryptography data security computers security secure databases protection user influence data swapping balancing privacy precision mining logic rules economist regularization inverse problems european unions directive privacy protection question statistical confidentiality probability theory mathematical statistics recovering information summary data privacy marketplace net worth data distortion probability distribu tion privacy interfaces information management method system clientserver communications user information revealed function willingness reveal whether information required analytic approach statistical databases jorma ris sanen machine learning office information privacy com missioner internet security firewalls beyond practical dataswapping first steps survey world wide web security sprint scalable parallel classifier data mining statistical databases characteris tics design ldv multilevel secure relational database management system data mining privacy conflict making statistical security statistical database randomized response survey technique eliminating evasive answer bias privacy concerns freebies privacy net users think world wide web consortium study protection statistical databases tr ctr xun yi yanchun zhang privacypreserving distributed association rule mining via semitrusted mixer data knowledge engineering v63 n2 p550567 november 2007