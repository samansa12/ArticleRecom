faster numerical algorithms via exception handling attractive paradigm building fast numerical algorithms following 1 try fast occasionally unstable algorithm 2 test accuracy computed answer 3 recompute answer slowly accurately unlikely event necessary especially attractive parallel machines fastest algorithms may less stable best serial algorithms since unstable algorithms overflow cause exceptions exception handling needed implement paradigm safely implement efficiently exception handling cannot slow illustrate paradigm numerical linear algebra algorithms lapack library b introduction widely accepted design paradigm computer hardware execute common instructions quickly possible replace rarer instructions sequences common ones paper explore use paradigm design numerical algorithms exploit fact numerical algorithms run quickly usually give right answer well slower algorithms always right right answer mean algorithm stable computes exact answer problem slight perturbation input 12 reasonably ask algorithms take advantage faster occasionally unstable algorithms use following paradigm 1 use fast algorithm compute answer usually done stably 2 quickly reliably assess accuracy computed answer 3 unlikely event answer accurate enough recompute slowly accurately success approach depends large difference speed fast slow algorithms able measure accuracy answer quickly reliably important us floating point exceptions causing unstable algorithm abort run slowly last requirement means system must either continue past exceptions later permit program determine whether exception occurred else support userlevel trap handling paper assume first response exceptions available corresponds default behavior ieee standard floating point arithmetic 3 4 numerical methods drawn lapack library numerical linear algebra routines high performance computers 2 particular consider condition estimation error bounding linear systems computing eigenvectors general complex matrices symmetric tridiagonal eigenvalue problem singular value decomposi tion first two algorithms common need solve triangular systems linear equations possibly illconditioned triangular system solving one matrix operations found basic linear algebra subroutines blas 9 10 18 blas include related operations like dot product matrixvector multiplication matrixmatrix multiplication occur frequently scientific computing led standardization widespread implementation particular high performance machines highly optimized implementations blas good way write portable high performance code express ones algorithm sequence calls blas done systematically lapack numerical linear algebra leading significant speedups highly pipelined parallel machines 2 however linear systems arising condition estimation eigenvector computation often illconditioned means overunderflow completely unlikely since first distribution lapack portable many machines possible including exceptions fatal could take advantage speed optimized blas instead used tests scalings inner loops avoid computations might cause exceptions paper present algorithms condition estimation eigenvector computation use optimized blas test flags detect exceptions occur recover exceptions occur report performance results fast decstation 5000 slow decstation 5000 mips r3000 chip cpu 17 sun 4260 sparc chip cpu 15 dec alpha 11 crayc90 sparcstation 10 viking microprocessor slow dec 5000 correctly implements ieee arithmetic arithmetic nans 80 times slower normal arithmetic fast dec 5000 implements ieee arithmetic incorrectly operands involve denormals nans speed normal arithmetic otherwise two dec 5000 workstations equally fast 1 cray exception handling still compare speeds common case exceptions occur see speedup could exception handling available measure speedup ratio time spent old lapack routine time spent new routine speedups obtained condition estimation common case exceptions occur follows speedups ranged 143 650 either dec 5000 150 500 sun 166 928 dec alpha 255 421 cray results computing eigenvectors 108 quite attractive speedups would even higher machine optimized blas parallelized slower scaling code rare case exceptions occur speed depended strongly whether exception occurred early late triangular solve speed subsequent arithmetic nan notanumber arguments examples speedup high 541 fast dec 5000 13 times slower slow dec 5000 illustrates price implementing ieee nan arithmetic slowly discuss bisection algorithm finding eigenvalues symmetric tridiagonal matrices lapack sstebz routine takes special care inner loop avoid overflow division zero whereas algorithm takes advantage infinity arithmetic defined ieee standard report performance results sparcstation ipx weitek 8601 chip fpu well distributed memory multiprocessor cm5 speedups range 114 147 also discuss singular value decomposition algorithm used lapack routine sbdsqr careful scaling code avoided using exception handling speedups obtained cray ymp el2256 121 139 rest paper organized follows section 2 describes model exception handling detail section 3 describes algorithms solving triangular systems 1 normally buggy workstation would annoying case permitted us run experiments speed exception handling varied without exception handling section 4 describes condition estimation algorithms without exception handling gives timing results section 5 eigenvector computations section 6 compares bisection algorithms solving symmetric tridiagonal eigenvalue problem without exception handling section 7 describes benefit exception handling computing singular values matrix section 8 draws lessons value fast exception handling fast arithmetic nans infinity symbols section 9 suggests future research exception handling section review ieee standard arithmetic handles exceptions discuss relative speeds exception handling mechanisms affect algorithm design state assumptions made speeds paper also briefly describe exception handling interface decstation 5000 ieee standard classifies exceptions five categories overflow underflow division zero invalid operation inexact associated exception status flag trap five exceptions signaled detected signal entails setting status flag taking trap possibly flags sticky means raised remain set explicitly cleared flags tested saved restored altered explicitly software trap come user control sense user able specify handler although capability seldom implemented current systems default response exceptions proceed without trap deliver destination appropriate default value standard provides clearlydefined default result possible exception default values conditions produced summarized table 1 produced ieee default behavior sigma1 nan propagate computation without producing exceptions according standard traps sticky flags provide two different exception exception raised default value condition overflow underflow 0 sigma2 e min denormals e e min division zero sigma1 x0 finite x 6 0 invalid nan 1 gamma1 0 theta 1 00 11 etc inexact roundx true result representable table 1 ieee standard exceptions default values handling mechanisms utility depends quickly flexibly permit exceptions handled since modern machines heavily pipelined typically expensive impossible precisely interrupt exceptional operation branch execute code later resume computation even without pipelining operating system overhead may make trap handling expensive even though branching strictly needed merely testing sticky flags may somewhat expensive since pipelining may require synchronization event order update thus appears fastest use sticky flags instead traps test sticky flags seldom possible hand infrequent testing sticky flags means possibly long stretches arithmetic sigma1 nan arguments default ieee arithmetic slow compared arithmetic normalized floating point numbers clearly inadvisable wait long tests sticky flags decide whether alternate computations performed summary fastest algorithm depends relative speeds conventional unexceptional floating point arithmetic arithmetic nans sigma1 arguments testing sticky flags trap handling extreme case everything except conventional unexceptional floating point arithmetic terribly slow forced test scale avoid exceptions cpu denormal 1 nan measured mips r30003010 cc slow correct 120x slower full speed 80x slower fast buggy full speed full speed full speed 2 mips r40004010 120x slower full speed 32x slower cc full speed 10x slower f77 full speed 9x slower f77 full speed full speed full speed f77 parisc 68x slower full speed 42x slower cc rs6000 full speed full speed full speed cc full speed full speed full speed manual 387 486 pentium full speed full speed full speed manual i860 868x slower 432x slower 411x slower cc i960 full speed full speed full speed manual dec alpha 690x slower 343x slower 457x slower cc crayc90 na abort abort manual non ieee machine table 2 speed arithmetic denormal 1 nan arguments compared conventional arithmetic unfortunate situation introduction exception handling would unpleasant irony exception handling rendered unattractive use slow implementation paper design algorithms assuming userdefined trap handlers available testing sticky flags expensive enough done infrequently arithmetic nan sigma1 reasonably fast codes fact supply way measure benefit one gets making nan 1 arithmetic fast table 2 shows speed arithmetic denormalized numbers 1 nan compared conventional arithmetic machines table entries measured fortran c others architecture manuals dec alpha implement ieee defaults including infinities nans denormals precise interrupts causes significant loss speed compared normal arithmetic interface sticky flags via subroutine calls without special compiler support 2 returns first argument binary operations status flag set illustrate interfaces briefly one test machines decstation 5000 mips r3000 chip cpu decstation 5000 r3010 floatingpoint accelerator operates coprocessor r3000 processor chip extends r3000s instruction set perform floating point arithmetic operations fpa contains 32bit controlstatus register fcr31 designed exception handling readwritten instructions running user mode fcr31 contains five nonsticky exception bits one exception table 1 appropriately set cleared every floating point operation five corresponding trapenable bits used enable user level trap exception occurs five corresponding sticky bits hold accrued exception bits required ieee standard trap disabled operation unlike nonsticky exception bits sticky bits never cleared sideeffect floating point operation cleared writing new value controlstatus register nonsticky exception bits might used applications requiring finer grained exception handling parallel prefix 6 algorithms developed paper need manipulate trap enable bits set zero disable software traps sticky bits procedure exceptionreset clears sticky flags associated overflow division zero invalid operations suppresses exceptions accordingly function except returns true overflow division zero invalid sticky flags raised 3 triangular system solving discuss two algorithms solving triangular systems equations first one simpler faster two disregards possibility overunderflow second scales carefully avoid overunderflow one currently used lapack condition estimation eigenvector computation 1 solve lower triangular nbyn matrix use notation indicate submatrix l lying rows j columns k l l similarly li l following algorithm accesses l columns algorithm 1 solve lower triangular system endfor common operation standardized subroutine strsv one blas 9 10 18 algorithm 1 easily overflow even matrix l wellscaled ie rows columns equal moderate length example c gamma2 c gamma3 c gamma4 overflows ieee single precision even though row column l largest entry 1 magnitude terribly small entries similarly let l n c analogous nbyn matrix second 1st elements along main diagonal means l n c second algorithm scales carefully avoid overflow algorithm 1 algorithm works choosing scale factor 0 1 solving chosen whenever solution x would overflow case x would overflow even smallest positive floating point number set zero example consider single precision example li exactly l singular algorithm set compute nonzero vector x satisfying brief outline scaling algorithm see 1 details coarse bounds solution size computed follows algorithm begins computing c lower bound g values x step algorithm 1 finally lower bound g reciprocal largest intermediate final values computed anywhere algorithm 1 1in lower bounds x gamma1 computed instead upper bounds x j avoid possibility overflow upper bounds smallest floating point number safely inverted g un means solution computed without danger overflow simply call blas otherwise algorithm makes complicated series tests scalings algorithm 2 compare costs algorithms 1 2 algorithm 1 costs n 2 flops floating point operations half additions half multiplies also n divisions insignificant large n first step algorithm 2 computing c costs much algorithm 1 applications expect solve several systems coefficient matrix reuse c amortizes cost several calls best case g un simply call strsv makes overall operation count 15n 2 n 2 amortize worst rare case inner loop algorithm 2 scale step increasing operation count n 2 total 25n 2 2n 2 amortize updating x max costs another data accesses comparisons may may cheaper number floating point operations important operation counts algorithm 2 many data dependent branches makes harder optimize pipelined parallel architectures much simpler algorithm 1 borne results later sections algorithm 2 available lapack subroutine slatrs code handles upper lower triangular matrices permits solving input matrix transpose handles either general unit triangular matrices 300 lines long excluding comments fortran implementation blas routine strsv handles input options 159 lines long excluding comments details slatrs see 1 algorithm 2 solve lower triangular system compute described g un call blas routine strsv else else 0 else li compute null vector x else jxij 1 jxij endif endfor endif condition estimation section discuss ieee exception handling used design faster condition estimation algorithm compare first theoretically practice old algorithm used lapack new algorithm 41 algorithms solving nbyn linear system wish compute bound error true measure error using either onenorm jjxjj infinity norm jjxjj j usual error bound 12 pn slowly growing function n usually n ffl machine precision condition number ae pivot growth factor condition number defined computing gamma1 costs solving prefer estimate jja gamma1 jj 1 inexpensively lu factorization called condition estimation since jjajj 1 easy compute focus estimating jja pivot growth may defined jju jj 1 definitions possible close unity except pathological cases lapack library 2 set routines developed estimate reciprocal condition number k 1 estimate reciprocal k 1 call rcond avoid overflow k 1 inputs routines include factors l u factorization modification 14 hagers method 13 used estimate jja gamma1 jj 1 algorithm derived convex optimization approach based observation maximal value function attained one vectors e j jth column nbyn identity matrix algorithm 3 13 algorithm computes lower bound fl jja choose x repeat solve solving using algorithm 2 solve solving u using algorithm 2 quit else x e j j jz algorithm involves repeatedly solving upper lower triangular systems certain stopping criterion met due possibilities overflow division zero invalid exceptions caused illconditioning bad scaling linear systems lapack routine sgecon uses algorithm 2 instead algorithm 1 solve triangular systems like discussed section 3 details use scale factor returned algorithm 2 shown see routines sgecon slacon lapack 2 goal avoid slower algorithm 2 using exception handling deal illconditioned badly scaled matrices algorithm calls blas routine strsv property overflow occurs matrix extremely illconditioned case detect using sticky exception flags immediately terminate welldeserved estimate rcond0 merely replacing triangular solver used algorithm 3 strsv inserting tests overflow work seen choosing moderately illconditioned matrix norm near underflow threshold cause overflow solving though moderately illconditioned therefore modified logic algorithm follows comments indicate guaranteed lower bound k 1 exception leads early termination algorithm 4 algorithm estimates reciprocal rcond estimated reciprocal condition number call exceptionreset choose x repeat solve calling strsv except rcond 0 quit solve calling strsv except rcond 0 quit else delta ff except rcond 0 quit endif else solve calling strsv except rcond 0 quit endif else solve calling strsv except rcond 0 quit endif solve u calling strsv except rcond 0 quit else solve l calling strsv except rcond 0 quit endif else x e j jz endif behavior algorithm 4 described following lemma 1 algorithm 4 stops early exception true rounded reciprocal condition number satisfies rcond maxn 3 ae pivot growth factor proof algorithm seven places exceptions may occur analyze one one note x chosen jjxjj 1 exception occurs computing l gamma1 x therefore 2 exception occurs computing u rcond 1ov 3 exception occurs computing ff rcond 1ov 4 exception occurs computing u ff 5 exception occurs computing u 6 exception occurs computing u gammat ff therefore ov 7 exception occurs computing l gammat u gammat ff therefore rcond n 2 ov combining seven cases shown rcond maxn 3 ae ov exception occurs practice rcond ffl signals system illconditioned make error bound 1 large solution larger means computed solution digits guaranteed correct since maxn 3 ae ov ae enormous also mean error bound 1 enormous loss information stopping early algorithm 4 lemma 1 applicable linear systems partial complete pivoting gaussian elimination example lapack routines sgecon sgbcon strcon see section 42 descriptions routines complex counterparts symmetric positive definite matrices pivoting necessary analogous algorithm eg spocon developed analyzed though omitted paper due limitation length machine matrix dimension n 100 200 300 400 500 sgecon 200 152 146 144 143 spocon 283 192 171 155 152 strcon 333 178 160 154 152 sun 4260 sgbcon 200 220 211 277 271 sgecon 302 214 188 163 162 spocon 500 256 227 222 217 strcon 150 200 230 217 218 dec alpha sgbcon sgecon 266 201 185 178 166 spocon 225 246 252 242 235 strcon 300 233 228 218 207 crayc90 sgecon 421 348 305 276 255 table 3 speedups dec 5000sun 4260dec alphacrayc90 exceptions scaling occur sbw stands semibandwidth 42 numerical results compare efficiencies algorithms 3 4 rewrote several condition estimation routines lapack using algorithm 4 including sgecon general dense matrices spocon dense symmetric positive definite matrices sgbcon general band matrices strcon triangular matrices ieee single precision compare speed robustness algorithms 3 4 generated various input matrices yielding unexceptional executions without invocation scalings inside algorithm 2 well exceptional executions unexceptional inputs tell us speedup common case machines like cray measure performance lost lack exception handling first ran algorithms 3 4 suite wellconditioned random matrices exceptions occur scaling necessary algorithm 2 far common case practice experiments carried decstation 5000 sun 4260 dec alpha single processor crayc90 performance results presented table 3 numbers table ratios time spent old routines using algorithm 3 time spent new routines using algorithm 4 ratios measure speedups attained via exception handling estimated condition numbers output two algorithms always dense matrices matrices large bandwidth matrix dimension n increases time service cache misses constitutes larger portion execution time resulting decreased speedups ran sgbcon matrices small bandwidth whole matrix fit cache observed even better speedups second compared algorithms 3 4 several intentionally illscaled linear systems scalings inside algorithm 2 invoked whose condition numbers still finite sgecon alone matrices sizes 100 500 obtained speedups 162 333 decstation 5000 189 267 dec alpha third study behavior performance two algorithms exceptions occur generated suite illconditioned matrices cause possible exceptional paths algorithm 4 executed algorithms 3 4 consistently deliver zero reciprocal condition number algorithm 4 inside triangular solve computation involves numbers nan sigma1 indeed overflow produces sigma1 common situation subtract two infinities shortly thereafter resulting nan propagates succeeding operations words one exceptional operation common situation long succession operations nans compared performance fast slow decstation 5000 set problems 3 recall fast decstation nan arithmetic incorrectly speed conventional arguments whereas slow decstation computes correctly times slower table 4 gives speeds decstations slow dec 5000 goes 3 test matrices together software obtained via anonymous ftp 7 example 1 example 2 example 3 fast dec 5000 speedup 215 232 200 slow dec 5000 slowdown 1167 1349 900 sparcstation table 4 speeds examples exceptions matrix dimensions 500 times slower fast dec 5000 examples infinities nans occurred speedups ranged 35 60 machines table 4 also shows speedups observed sparcstation 10 1 nan arithmetic implemented correctly full speed 5 eigenvector computation consider another opportunity exploit ieee exception handling problem compute eigenvectors general complex matrices example contrast early ones requires recomputing answer slowly exception occurs paradigm let nbyn complex matrix nonzero vectors v u scalar satisfy conjugate transpose called eigenvalue v u called right left eigenvectors associated eigenvalue respectively lapack task computing eigenvalues associated eigenvectors performed following stages routine cgeev 1 reduced upper hessenberg form h zero first subdiagonal reduction written 2 h reduced schur form reduction written upper triangular matrix unitary 12 eigenvalues diagonal 3 ctrevc computes eigenvectors let v matrix whose columns right eigenvectors delta v right eigenvectors h q right eigenvectors similarly compute left eigenvectors let us first examine important stage calculating eigenvectors upper triangular matrix eigenvalues find right eigenvector v associated eigenvalue ii need solve homogeneous equation partitioned block form6 6 6 6 6 4 backward substitution v satisfying equation therefore problem reduced solving upper triangular system 3 dimension find n eigenvectors need solve triangular system 3 scalar multiple v also eigenvector always expect obtain answer scaling solution vector matter illconditioned badly scaled triangular system 3 purpose ctrevc calls triangular solve routine clatrs instead calling triangular solver ctrsv blas clatrs complex counterpart slatrs discussed section 3 using algorithm 2 common cases however scaling unnecessarily introduces overhead reimplemented part ctrevc containing triangular solve solving equation 3 first call ctrsv test exception flags exceptions occur go back call clatrs study efficiency modified ctrevc ran old code new one random upper triangular matrices various sizes observed speedups 149 165 decstation 5000 138 146 sun 4260 case overflow triangular solve invoked twice first using ctrsv yet throwing away solutions second using clatrs since ctrsv twice fast clatrs see section 3 performance loss 50 rare exception occurs see performance attained ctrevc alone effects performance whole process computing eigenvectors general complex matrices timed ctrevc context cgeev turns ctrevc amounts 20 total execution time cgeev therefore expect speed whole process increased 8 6 symmetric tridiagonal eigenvalue problem section consider problem finding eigenvalues real symmetric tridiagonal matrix let nbyn symmetric tridiagonal matrix form b b bisection method accurate inexpensive parallelizable procedure calculating eigenvalues inner loop method based integervalued function countoe real argument oe defined function countoe endfor return c thus count function counts number nonpositive ts iteration known number equals number eigenvalues less equal oe 12 suppose wish find eigenvalues b using bisection first evaluate counta countb difference two count values number eigenvalues b let b2 midpoint interval evaluate countoe deduce many eigenvalues lie intervals oe oe b recursively bisect two intervals interval containing single eigenvalue bisected repeated eigenvalue determined sufficient precision division involved recurrence may cause division zero overflow prevent occurrence exceptions careful scheme first developed w kahan 16 later used lapack sstebz routine 2 algorithm first computes threshold pivmin smallest number divide b 2 overflow inside inner loop divisor compared pivmin changed gammapivmin close zero algorithm 6 gives details method algorithm computes number eigenvalues less equal oe endfor return c machines ieee floatingpoint arithmetic may rewrite count function algorithm 7 even though b 2 may overflow whenever occurs default values sigma1 used continue computation algorithm 7 computes number eigenvalues less equal oe endfor return c signbitx extracts sign bit floatingpoint number x represented ieee format returned value either 0 1 depending whether x positive negative signbitx computed quickly logically shifting sign bit x rightmost bit position register leaving zeros bits correctness algorithm 7 relies fact arithmetic sigma1 signed zeros sigma0 obeys certain rules defined ieee standard merit algorithm 7 replaces two explicit conditional branches single straightline statement makes better use floatingpoint pipelines hardware requirement algorithm 7 attain good speed speed infinity arithmetic sparcstation ipx infinity arithmetic fast conventional arithmetic measured speed algorithm 6 7 various matrices sizes ranging 100 1000 algorithm 7 achieved speedups ranging 120 130 algorithm 6 also compared two bisection algorithms using algorithm 6 7 inner loops respectively find eigenvalues able get speedups ranging 114 124 due dominant role count function bisection algorithm also comparisons distributed memory multiprocessor thinking machines cm5 19 cm5 configuration contains 64 33mhz sparc 2 processors interconnected fattree network processing node 8 mbytes local memory coordination synchronization among processing nodes achieved via explicitly pass matrix size ts lapack bisect table 5 speedups parallel bisection algorithms cm5 ing messages floatingpoint arithmetic cm5 conforms ieee standard infinity arithmetic fast conventional arithmetic inderjit dhillon et al8 designed parallel bisection algorithm cm5 whole spectrum divided subintervals processing node responsible finding eigenvalues within one subinterval dynamic load balancing scheme incorporated eigenvalues evenly distributed table 5 report three types speedup numbers experiments algo stands running time algo single node cm5 p algo stands running time algo 64 node cm5 thus tsalgo represents parallel speedup algo two algorithms compared lapack bisect used algorithm 6 get count value ieee bisect used algorithm 7 get count value last column demonstrates speedup parallel ieee bisect parallel bisect see speedups attained using ieee arithmetic ranges 118 147 7 singular value decomposition section discuss using exception handling speed computation singular value decomposition matrix 12 important linear algebra computation many applications consists two phases phase 1 reduction bidiagonal form ie nonzero diagonal first superdiagonal costs 3 operations n matrix dimension phase 2 computing singular values bidiagonal matrix costs phase 2 take much longer phase 1 machines like crayc90 phase 1 readily vectorized parallelized whereas phase 2 consists nonlinear recurrences run scalar speeds example phase 1 21 floating point operations speed 594 megaflops time 0036 seconds whereas phase 2 16 floating point operations speed 69 megaflops time 023 seconds phase 2 takes longer phase 1 n 1200 section discuss using exception handling accelerate phase 2 phase 2 implemented slight modification lapack subroutine sbdsqr 2 describe suffices consider one main loops sbdsqr others similar addition 12 multiplies 4 addition two uses operation call rotf r takes f g inputs returns gr outputs simple formula subject failure inaccuracy either f g greater square root overflow threshold little larger square root underflow threshold therefore sbdsqr currently series tests scalings avoid failure difference sbdsqr routine routine inlines rot uses slightly different move accurate scaling algo rithm almost time tests indicate scaling needed impossible determine without running whole loop compare performance two versions sbdsqr one tests scales another call sbdsqr unsafe uses simple single line formulas r cs sn tested two routines cray ymp el2256 speedups depend somewhat matrix test bidiagonal matrix entries form dimensions ranging 50 1000 speedups 128 139 half 135 speedups 121 131 8 lessons system architects important lesson welldesigned exception handling permits common cases exceptions occur implemented much quickly alone makes exception handling worth implementing well trickier question fast exception handling must implemented three speeds issue speed nan infinity arithmetic speed testing sticky flags speed trap handling principle reason nan infinity arithmetic fast conventional arithmetic examples section 42 showed slowdown nan arithmetic factor 80 conventional arithmetic slows condition estimation factor 30 since exceptions reasonably rare slowdowns generally affect worst case behavior algorithm depending application may may important worst case important important system designers provide method fast exception handling either nan infinity arithmetic testing sticky flags trap handling making three slow force users code avoid exceptions first place original unpleasant situation exception handling designed avoid particularly important fast exception handling parallel computer following reason running time parallel algorithm running time slowest processor probability exception occurring least one processor p times great one processor p number processors 9 future work design paradigm numerical algorithms proposed paper quite general used develop numerical algorithms include rewriting blas routine snrm2 compute euclidean norm vector lapack routine shsein calls slatrs compute eigenvectors real upper hessenberg matrix complex division gradual underflow instead flush zero guarantee accurate result see 5 requires fast arithmetic denormalized numbers floating point parallel prefix useful operation various linear algebra problems robust implementation protection overunderflow requires fine grained detection handling exceptions 6 final comment concerns tradeoff speed nan infinity arithmetic granularity testing exceptions current approach uses large granularity since test exceptions complete call strsv approach fast nan infinity arithmetic must fast hand fine grained approach would test exceptions inside inner loop avoid useless nan infinity arithmetic however frequent testing clearly expensive compromise would test exceptions one several complete iterations inner loop strsv would require reimplementing strsv medium grained approach less sensitive speed nan infinity arithmetic effect granularity performance worth exploration software described report available authors 7 acknowledgements authors wish thank w kahan detailed criticism comments also wish thank inderjit dhillon providing us performance results bisection algorithms running cm5 r robust triangular solves use condition estimation underflow reliability numerical software specifications robust parallel prefix operations faster numerical algorithms via exception handling parallel algorithm symmetric tridiagonal eigenproblem implementation cm5 set level 3 basic linear algebra subprograms extended set fortran basic linear algebra subroutines sites editor alpha architecture reference manual matrix computations condition estimators algorithm 674 fortran codes estimating onenorm real complex matrix sparc international inc accurate eigenvalues symmetric tridiagonal matrix mips risc architecture basic linear algebra subprograms fortran usage connection machine cm5 technical summary tr extended set fortran basic linear algebra subprograms mips risc architecture set level 3 basic linear algebra subprograms sparc architecture manual alpha architecture reference manual lapacks users guide fortran codes estimating onenorm real complex matrix applications condition estimation matrix computations 3rd ed basic linear algebra subprograms fortran usage accurate eigenvalues symmetric tridiagonal matrix working note 36 robust triangular solves use condition estimation ctr technical report floatingpoint exception handling acm sigplan fortran forum v15 n3 p128 dec 1996 david bindel james demmel william kahan osni marques computing givens rotations reliably efficiently acm transactions mathematical software toms v28 n2 p206238 june 2002 inderjit dhillon beresford n parlett christof vmel design implementation mrrr algorithm acm transactions mathematical software toms v32 n4 p533560 december 2006 xiaoye li james w demmel david h bailey greg henry yozo hida jimmy iskandar william kahan suh kang anil kapur michael c martin brandon j thompson teresa tung daniel j yoo design implementation testing extended mixed precision blas acm transactions mathematical software toms v28 n2 p152205 june 2002 john r hauser handling floatingpoint exceptions numeric programs acm transactions programming languages systems toplas v18 n2 p139174 march 1996