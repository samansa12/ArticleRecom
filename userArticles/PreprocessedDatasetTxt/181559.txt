using virtual lines enhance locality exploitation spatial locality numerical codes significant potential performance improvements important however large cache lines cannot used current onchip data caches important pollution breed paper propose hardware design called virtual line scheme allows utilization large virtual cache lines fetching data memory better exploitation spatial locality actual physical cache line smaller currently found cache lines better exploitation temporal locality simulations show 17 64 reduction average memory access time obtained 20cycle memory latency also shown simple software informations used significantly decrease memory traffic flaw associated utilization large cache lines b introduction due constantly decreasing processor cycle time average memory access time observed processors rapidly increasing high network memory latency consequently cost cache miss become prohibitive monoprocessor multi processors urging development hardware software solutions numerical codes particularly sensitive cache performance working set usually large resulting numerous memory accesses array references numerical codes generally strong spatial temporal locality properties temporal locality exploited loop blocking 2 12 16 little support except cache line flexibility available exploitation spatial locality however experiments 15 tend show optimal cache line codes depending whether spatial temporal locality preferably exploited small directmapped caches popular 13 4 onchip space used cache hit time minimized cache hit time minimized determines processor cycle time work funded bra esprit iii projet ap part european agency dgxiii high performance computing division department computer science university leiden netherlands however caches sensitive cache conflicts pollution phenomena preventing use large cache lines paper propose cache design called virtual line scheme vls allows utilization large cache lines small directmapped caches avoiding flaws associated large cache lines increased cache conflicts cache pollu tion scheme proposed gives illusion variable line size used reference within code actual physical cache line size smaller usual cache line sizes order better exploit temporal locality large virtual cache lines used order better exploit spatial locality simulations show mean average memory access time observed benchmarks equal 17 cycles per reference 20cycle memory latency also shown simple software informations used significantly decrease memory traffic flaw common large physical large virtual lines section 2 tradeoff spatial locality temporal locality exploitation discussed section 3 principles virtual line scheme presented performance measurements provided section 4 shown assist virtual line scheme software informations decreasing memory traffic finally section 5 design implementation issues discussed 2 exploitation spatial temporal locality tradeoff spatial temporal locality principles caches exploit temporal spatial locality reside codes temporal locality exploited keeping data cache spatial locality exploited loading several consecutive data ie cache line memory request choice optimal line size delicate tradeoff temporal spatial locality exploitation respect temporal locality number cache entries large possible minimize cache pollution data loaded cache used cache conflicts data competing cache locations since number cache entries equal ratio cache size cache line size cache line size chosen small possible respect spatial locality number elements brought cache one time ie cache line size large possible order minimize number memory requests thereby reducing average memory access time element j11n1 j21n2 reg enddo enddo figure 1 example numerical loop nest reducing cache pollution vs exploiting spatial locality let us illustrate tradeoff example figure 2 references b exhibit flawless spatial locality columnwise storage assumed c apparently references larger line size smaller number cache misses note array b exhibits temporal locality therefore necessary minimize cache pollution array b kept cache however reference c exhibit spatial locality actually time reuse cache line equal one execution loop j 2 unlikely spatial locality exploited consequently cache line size equal l words l words array c loaded cache iteration j 2 l words loaded every l iterations references due spatial locality array c going pollute cache l times faster array besides reference c misses iteration j 2 reference misses one every l iterations ideally small line size used reference c references use large line size would reduce number misses array c would considerably decrease cache pollution due array thereby increase hit ratio array b note array also pollutes cache bringing data without temporal locality however gains obtained exploiting spatial locality array generally outperform cache pollution induces cache line size performance bottleneck numerical codes ability use large cache lines could significantly increase performance indeed array references often stride1 accesses 17 sides arrays without temporal locality also commonly found like matrix access matrixvector multiply primitive way reduce number cache misses references called coldstart misses use large line sizes true efficient algorithms exist exploiting temporal locality 2 12 16 misses physical line victim replacement main cache c1 secondary cache c2 virtual cache line physical cache line figure 2 virtual line scheme become performance bottleneck numerical loop nests assume instance loop nest figure 2 reference c n 2 small enough b fits cache number cache misses approximately equal l l first term corresponds c second term assuming spatial locality cannot exploited due reuse dis tance third term b doubling line size nearly decreases two number misses loop nest nonnumerical codes contrary nonnumerical codes like unix tools typically exhibit irregular pattern accesses memory therefore though exhibit spatial locality mostly favour small line sizes better exploitation temporal locality ie minimizing cache conflicts cache pollution 3 virtual line scheme virtual line scheme solution eliminating tradeoff small cache lines temporal locality large cache lines spatial locality cache miss large chunks elements fetched memory ie virtual cache line actually stored cache ie physical cache line therefore spatial locality exploited induce cache pollution 31 principles scheme physical line size ie actual cache line size l either order smaller current cache line sizes 8 bytes memory request large virtual line size made n physical lines fetched memory physical line containing requested word actually loaded cache remaining physical lines virtual line loaded small secondary cache way virtual line divided n sublines primary secondary caches divided n banks see figure 2 virtual line scheme confused subblock placement see 6 subblock place ment one tag subblocks virtual line scheme one tag physical line ie physical lines subblocks effective cache lines purpose subblock placement reduce miss penalty associated long cache lines onchip space used address tags purpose virtual line scheme allow flexible use cache space better temporal locality exploitation reducing cache line size virtually physically subblock place ment grouping together cache lines order exploit spatial locality group corresponds virtual line main flaw virtual line scheme additional onchip space required implement multibank design secondary cache case physical line equal 16 bytes example instead usual 32 bytes double number tags necessary increase cache size approximately 10 physical line 16 bytes compared physical line 32 bytes assuming 20bit tag per cache line secondary cache also induces onchip space overhead however two facts make good tradeoff first secondary cache used write buffer explained section 5 second also used victim cache discussed section 5 though victim caches yet implemented clear evidence 11 performance improvements bring secondary cache additional tags main overhead associated implementation virtual line scheme less significant hardware addons must included detailed section 5 311 workings virtual line scheme main parameters remainder paper main cache called secondary cache called characteristics several current onchip caches ie cache size equal 8192 bytes directmapped 13 4 secondary cache 1024 bytes large 4way associative parameters secondary cache determined experimentally sizes associativity used shown section 5 c 1 directmapped c 2 setassociative hit time c 1 considered equal one cycle hit time c 2 equal two cycles physical line size 16 bytes chosen virtual line size taken equal 64 bytes see section 5 ie caches divided 4 banks virtual line scheme algorithm processor request caches tested time answers one cycle later processor hits c 1 execution resumes reply c 2 ignored processor misses c 1 hits c 2 processor resumes execution two cycles mean physical line hit occurred transferred c 2 c 1 besides due cache role c 2 victim physical line c 1 transferred c 2 place hit line see figure 2 reason caches considered stall respect proces two cycles hit c 2 occurred processor misses caches memory request issued virtual line mentioned target physical line transferred c 1 physical lines virtual line stored c 2 nearly step mechanism aware virtual line rest time caches work much alike twolevel cache hierarchy except multibank mechanism line size equal l note cache miss latency increased one cycle standard cache delayed answer cache c 2 312 large physical line vs large virtual line observed memory latency due 2cycle access time c 2 benefit using large virtual line versus large physical line obvious accessing 4 physical lines virtual line case l costs l cycles l memory latency plus 1 cycle first physical line 3 theta cycles remaining 3 lines case used total l cycles physical line bytes total penalty equal l cycles nevertheless physical lines virtual line necessarily used actual average total latency accessing large physical virtual lines alike besides requested physical line comes back memory processor resume execution primary cache stall since remaining lines stored cache c 2 standard cache would also resume execution primary cache would stall several cycles new cache line loaded superscalar processor one request per cycle issued result processor stall cycles solutions avoiding processor stall large cache line reloads already proposed ibm rs6000 7 buffer used store incoming line processor needs wait cache reload performed virtual line acts buffer cache pollution however important asset large virtual lines large physical lines minimize primary cache pollution indeed physical line virtual line loaded c 1 requested processor hit ratio primary cache 16byte cache line nearly always equal standard cache 32byte cache line see figure 7 see section 321 details benchmarks note standard cache still performs better code strong spatial locality hand mm reduction pollution increases significantly hit ratio c 1 besides physical line loaded primary cache requested latter fact im portant though reference exhibit spatial locality time reuse long storing cache data used also form cache pollution scarce spatial locality case stride reference equal one virtual line scheme perform better large physical lines since among physical lines large virtual line used loaded cache c 1 virtual line scheme proves particularly useful references exhibit scarce spatial locality example figure 2 64byte physical line would load cache 8 64bit words cache7 corresponding cache pollution 64byte virtual line would load 2 words 16 bytes cache words staying cache performance comparison figure 3 shows performance comparison large physical lines large virtual lines increasing physical line size nearly always results performance reductions hand virtual line size scheme tolerates well large virtual lines numerical codes performance increased nonnumerical codes performance either slowly increases stable virtual line size increases degrades rapidly physical line size increases 32 performance virtual line scheme 321 overview experiments simulations performed cycle cycle order catch phenomena associated design bus utilization caches stall line trans fers slower response time cache collection 10 benchmarks used though difficult choose representative set bench marks tried combine different types codes illustrate maximum number behaviors numerical codes picked perfect club suite 1 benchmarks aparcbdnaws codes unix tools cc gnucc compiler cpr compress utility tex latex compiler codes numerical primitives lawrence livermore loop spmv sparse matrixvector multiply loop mm matrixmatrix multiply loop onemillion references traces extracted code first million references ignored order avoid nonrepresentative references corresponding initialization sections codes 322 reduction average memory access time ap arc bdna cc cpr mm spmv tex ws20406080 average memory access time standard cache virtual line scheme figure 4 performance comparison virtual line scheme average memory access time banks 4 banks 8 banks banks figure 5 influence physical virtual line sizes mm numerical nonnumerical codes significant performance improvement observed standard 8192byte cache line size 32 bytes see figure 4 reduction average memory access time varies 17 64 mean average access time equal 17 cycles 27 cycles standard cache performance improvement spectacular mm matrixmatrix multiply one two main references innermost loop nest non stride1 reference exhibits flawless spatial locality consequently standard cache pollution important cache line large virtual line scheme large virtual lines induce additional pollution far physical line kept small see figure 5 matrixmatrix multiply example exploits properties virtual line scheme increased exploitation spatial locality reduced cache pollution ap arc bdna cc cpr mm spmv tex ws20406080 average memory access time ls32 standard cache ls128 standard cache ls256 standard cache ap arc bdna cc cpr mm spmv tex ws20406080 average memory access time ls32 standard cache figure 3 performance large virtual physical lines otherwise noticed average memory access time smaller virtual line scheme standard cache codes see figure 4 means design safe ie degrade performance codes least benchmarks used memory latency value considered 323 increased performance reduction cache pollution exploitation spatial locality virtual line scheme profitable either limiting cache pollution allowing exploitation spatial locality temporal locality exploitation figure 8 fraction words prefetched ie fetched memory belong requested physical line effectively used indicated seen numerical code like arc benefits virtual line scheme uses negligible number prefetched data hand figure 7 shows hit ratio c 2 significant paradoxical behavior indicates reduced cache pollution due smaller physical line secondary cache allow better exploitation temporal locality note small physical cache line size also increases efficiency secondary cache type phenomenon expected nonnumerical codes see benchmark cpr rather numerical codes spatial locality exploitation naturally virtual line scheme also performs well simply exploiting spatial locality figure 8 seen benchmarks bdnallmmspmv ws use efficiently words prefetched efficiency increases physical line size decreases recall double number accesses c 2 necessary physical line size divided two therefore number hits c 2 artificially doubles number hits c 1 remains constant still secondary cache intensively used prefetch buffer andor victim cache hit ratio c 2 ap arc bdna cc cpr mm spmv tex ws10 hit hit ratio c1 vls hit ratio c2 vls hit ratio standard cache figure 7 hit ratio c 1 c 2 ap arc bdna cc cpr mm spmv tex ws010305 fraction fraction words prefetched fraction prefetched words used figure 8 efficiency virtual line scheme relatively high see figure7 benchmarks ccllmm efficiency mechanism decreases virtual line size increases see figure 6 benchmarks cprmm non stride1 accesses severely pollute secondary cache overall performance improvement nonnumerical codes nearly always due better exploitation temporal locality thanks smaller physical line size victim cache hand numerical codes benefit either large virtual lines andor decreased cache pollution 324 memory traffic whether physical virtual lines used memory traffic increased theory words loaded ap arc bdna cc cpr mm spmv tex ws103050 words fetched memory words referenced ls32 standard cache ls128 standard cache ls256 standard cache ap arc bdna cc cpr mm spmv tex ws103050 words fetched memory words referenced ls32 standard cache figure memory traffic large virtual physical lines used size line would impact memory traffic figure 8 shows though true codes llspmvws true codes arccccprmm selective miss requests however virtual line scheme designed minimize memory traffic cache miss issued physical lines virtual line present c 1 fetched memory indeed due loose connection physical lines virtual line possible miss occurred physical line physical lines virtual line still present cache fetching lines memory would induce coherence issues would wasteful therefore presence complementary physical lines c 1 tested issuing miss request note problem arises cache c 2 issue discussed section 5 implementation issues however argued facility may increase latency two solutions adopted avoid first physical lines tested simultaneously since necessarily belong different banks tag hand additional hardware necessary perform operation otherwise another cheaper solution applied miss request issued time necessary spend request first word significantly larger subsequent words therefore conceivable first word request processed cache tested presence physical lines solution advantage limiting amount hardware required reduction memory traffic using two methods choice transparent simulations observed memory traffic increases significantly less virtual line scheme large physical lines see figure 8 overall increase memory traffic remains reasonable l nonnumerical codes like cpr still exhibit memory traffic increase approximately 30 though average memory access time decreases larger virtual lines though average memory access time remains competitive memory traffic significantly increased 33 prefetching ap arc bdna cc cpr mm spmv tex ws20406080 average memory access time standard cache virtual line scheme prefetching figure 9 performance vlsprefetching ap arc bdna cc cpr mm spmv tex ws010305 fraction fraction words prefetched vls fraction prefetched words used vls fraction words prefetched vlsprefetching fraction prefetched words used vlsprefetching figure 10 memory traffic vlsprefetching though virtual line scheme allows utilization large line sizes excessively large sizes could induce severe bus contention secondary cache pollu tion reducing efficiency buffer victim cache see figures 3 6 besides larger line size smaller efficiency ie smaller ratio words fetched words used seen finally large line sizes prevent periodic cache misses vector access prefetching alternative large line sizes proposed 9 3 14 10 principle detect accesss stride reference predict next address used based address currently referenced stride prefetch corresponding data though simulations proved efficiency schemes require relatively heavy complex implementations besides array references within loop nest often stride1 see 17 making complex stride detection mechanism less necessary combining virtual line scheme prefetching virtual line scheme actually constitutes convenient architecture base introducing simple stride prefetching consider physical line virtual line stored one banks c 2 cause requested processor first word corresponds virtual address line used later physical line corresponding word l prefetched ie next physical line corresponding bank bank behaves independently banks con sequently even stride accesses exploited strides equal size virtual line note necessary physical line loaded instead whole virtual line respect implementation one bit necessary per cache line secondary cache indicate whether line prefetched victim replace ment otherwise buffer necessary store data address next physical line prefetched case memory busy prefetching request needs issued performance efficiency vector accesses miss occurs first virtual line next physical lines prefetched note regularity accesses disrupted one physical line excess loaded secondary cache indeed physical line fetched c 2 used next line never prefetched since prefetch occurs line c 2 never used transferred c 1 principle limits amount additional memory traffic wrong pre dictions besides mechanisms proposed appearing complex codes nonrectangular loops loops statements also exploited seen figure 9 prefetching increase performance codes strong spatial locality llmm degrades performance codes respect standard virtual line scheme respect standard cache performance degradations first due additional memory traffic increases observed memory latency case cache miss requests pending prefetch request locks memory also additional data fetched secondary cache degrade efficiency victim cache sometimes even prefetch buffer see figure 10 benchmark ap note however fraction prefetched words used nearly often increases see figure 10 benchmarks means prefetching potential improve performance far sideeffects increased memory traffic secondary cache pollution eliminated related work virtual line scheme combined prefetching also close multiway stream buffers proposed 11 mechanism time reference misses cache four cache lines plus requested cache line fetched memory four additional cache lines stored buffer first line buffer used three lines shifted upward next consecutive line prefetched design four physical lines prefetched instead one cost wrong prediction higher besides physical lines larger ones used virtual line scheme since correspond standard cache line sizes around bytes number interleaved accesses loop nest greater number stream buffers several references would compete buffer preventing exploitation prefetched data finally stride accesses smaller physical line could exploited flaws avoided virtual line scheme coupled prefetching ever clear cost secondary cache higher cost four stream buffers nevertheless tolerable case secondary cache developed purpose prefetching actu ally assuming virtual line scheme imple mented adding prefetching facility comes nearly cost 4 softwaredirected scheme previous sections shown virtual line scheme allows improved exploitation spatial locality without increased cache conflicts cache pollution usually induced large cache lines however well large physical cache lines large virtual lines increase significantly memory traffic could particularly troublesome within multiprocessor environment latency high besides somes cases important fraction physical lines loaded used thereby polluting secondary cache limiting efficiency buffer victim cache however least numerical codes relatively straightforward determine whether given ref standard cache vls vls prefetch soft vls soft vls pref2040average memory access time figure 11 performance softwaredirected vls example figure 1 standard cache vls vls prefetch soft vls soft vls fraction total words fetched memory total words referenced words prefetched vls prefetching words fetched words prefetched used words prefetched figure 12 memory traffic softwaredirected vls example figure 1 erence exhibits spatial locality information could used virtual line scheme decide whether large virtual line small physical line loaded corresponding reference possible data locality optimizing algorithms 2 12 16 sufficiently mature powerful detect temporal spatial locality numerical codes besides suggested 5 simple informations sufficient coordinate combined soft warehardware mechanism exploiting locality case one bit per loadstore instruction sufficient specify whether reference exhibits spatial locality example consider example figure 2 reference spatial locality reference dj 1 spatial locality therefore reference virtual line four physical lines loaded reference cj single physical line loaded first consequence reduction secondary cache pollution inducing reduction average memory access time shown figure 11 second important effect significant reduction memory traffic shown figure 12 note experiments restricted ex ample unavailability compiler instrumenting code spatial locality information underlying idea assisting virtual line scheme deciding whether physical virtual line fetched effectively implement variable cache line size mechanism give illusion cache c 1 mechanism used possible refine system specifying number physical lines brought instead choosing one physical line one virtual line general purpose provide necessary hardware support developing software optimization spatial locality hardware design current caches provides flexibility performing optimizations assisting temporal locality exploitation structure virtual line scheme also convenient assist exploitation temporal locality well cache pollution conflicts reduced data deprived temporal locality allowed reside primary cache least used underlying idea impose priorities data based temporal locality properties principle already investigated 5 performance scheme limited absence alternative cache store nonreusable data order avoid cache pollu tion fact temporal locality favoured spatial locality technique also compared bypassing even implemented processors like i860 8 however bypassing prevents exploitation spatial locality unless dedicated buffer used store bypassed cache lines secondary cache virtual line scheme could used purposes far information available temporal locality reference information used assist victim cache process reference temporal lo cality line transferred secondary cache victim replacement note information used bypass cache mentioned efficient since spatial locality reference cannot exploited consider example figure 2 reference dj 1 j 2 temporal locality strong spatial locality therefore reduction pollution obtained bypassing cache would probably compensate added memory accesses therefore cache line reference victim line transferred c 2 temporal locality line dj victim replacement turn discarded instead transferred c 2 result pollution cache c 2 remains minimum prefetching note software control also used prefetch ing seen section 3 sometimes prefetching bring performance improvements often severely increases memory traffic makes unsafe technique solution start prefetching reference spatial locality detected information provided spatial locality bit case unlikely many wrong predictions would occur figure 11 seen combination softwaredirected virtual line scheme prefetching performs significantly better standard virtual line scheme prefetching best performance actually obtained case seen figure 12 performance improvement due better efficiency prefetching consequently reduction memory traffic implementation issues informations provided ie presence spatial temporal locality reference loop nest extracted compiletime two bits necessary store information instruction set extended accommodate new loadstore informations compiler unable provide informations locality reference alternative set default data temporal locality experiments proved solution efficient temporal locality default regarding spatial locality also possible set spatial locality bit default since loading one physical line per memory request proved inefficient codes known exhibit little spatial locality would possible set default virtual line size smaller values like bytes 5 design implementation issues 51 physical line virtual line sizes size elementary physical line virtual line tightly coupled tradeoff choose physical line small enough maximize number cache entries ie cache size line size order better exploit temporal locality also find physical line large enough number accesses secondary cache frequent hit time c 2 longer similarly tradeoff virtual line find line size large enough sufficient spatial locality exploited small enough important fraction words fetched used thereby minimizing excessive memory traffic experiments proved relatively large virtual line sizes considered see figure 3 promising means spatial locality efficiently exploited still virtual line size 64 bytes selected induces moderate memory traffic increase hand choice physical line size restrictive optimal value seems equal 16 bytes though cases physical line sizes 8 bytes preferred however mostly codes without spatial locality benefit small lines performance codes spatial locality degraded numerous accesses secondary cache note 8 bytes corresponds one double precision floating point number 52 secondary cache ap arc bdna cc cpr mm spmv tex ws associativity cache c2 ls64ls16size c2102420average memory access figure 13 variation vls performance associativity cache c 2 ap arc bdna cc cpr mm spmv tex ws size cache c2 ls64ls16fullyassociative20average memory access figure 14 variation vls performance cache size associativity characteristics secondary cache determined size associativity finding optimal tradeoff difficult performance whole mechanism either stable varies nearly linearly secondary cache size associativity increases since secondary cache size small anyway surprising cache conflicts numerous therefore performance improves constantly associativity size creased implementation constraints performance reasons secondary cache size limited 1024 bytes chosen 4way associative write buffer mechanism ap arc bdna cc cpr mm spmv tex ws fraction write requests005 fraction standard cache virtual line scheme figure 15 amount write back requests mentioned section 3 secondary cache acts write buffer sparing onchip space mechanism relies replacement policy secondary cache lru policy used bank secondary cache physical line dirty reaches lowest priority respect lru written back memory therefore data used long time ie fair chance reused written back memory mechanism significantly reduce number memory write requests see figure 15 could decrease burden coherence protocols multiprocessor 53 coherence issues coherence secondary cache mentioned section 3 coherence issues arise twolevel cache hierarchy shown coherence maintained primary cache testing presence physical lines virtual lines fetching lines memory secondary cache problem arises two solutions adopted first technique used c 1 employed c 2 miss latency must increased one cycle access time secondary cache longer another solution send memory request without testing secondary cache beforehand cache c 2 tested request processed bank flag bit set physical line found data returns memory physical lines corresponding bank bit set discarded note would simple directly invalidate data secondary cache since dirty would always possible prefetching difficult problem maintain coherence prefetching used physical line prefetched stored c 2 possible test presence c 1 order check coherence test would stall c 1 could mean stalling processor especially processor superscalar cache often solicited since prefetching requests numerous tolerable solution adopted store physical line c 2 without checking c 1 since secondary cache acts write buffer coherence issues eliminated indeed dirty physical line c 1 needs go cache c 2 write buffer written back memory transferring dirty line c 1 c 2 tested presence physical line occurs line immediately invalidated line c 1 stored c 2 note impossible line c 2 dirty since writes occur c 1 mechanism increase swap time victim cache operation possible refine processor request hits c 1 hits redundancy detected data c 2 invalidated note cannot dirty 54 comparisons large associative caches ap arc bdna cc cpr mm spmv tex ws2040average memory access time cs8192assoc1ls32 standard cache cs8192assoc2ls32 standard cache cs16384assoc1ls32 standard cache cs16384assoc4ls128 standard cache figure comparison vls different cache architectures due onchip space required implement virtual line scheme necessary compare scheme performance larger caches seen figure 16 16kbyte directmapped cache either comparable nonnumerical codes smaller numerical codes performances virtual line scheme indicates available onchip space preferably dedicated hardware optimizations performance setassociative caches also investigated provide indication amount cache conflicts removed seen figure 16 16kbyte 4way associative cache 128byte cache line ibm rs6000 outperform virtual line scheme though 2way 8kbyte cache also increases significantly performance nonnumerical codes less efficient exploiting spatial locality therefore performs worse numerical codes note experiments hit time setassociative caches taken equal one cycle optimistic 6 conclusions paper hardware scheme using large cache lines without usual associated flaws proposed simulations showed efficiency scheme numerical codes nonnumerical codes large virtual lines allow better exploitation spatial locality small physical lines secondary cache allow better exploitation temporal locality inspite increased memory traffic code exhibited performance degradations simple software solution reducing memory traffic investigated though proved effi cient extensive testing still necessary fully validate concept underlying idea virtual line scheme propose cache design provides flexibility exploiting spatial temporal locality current cache architectures design intended architecture base hardware software optimizations developed provides convenient environment many solutions currently investigated cache bypass ing data priorities prefetching next goal show performance current data locality optimizing techniques significantly enhanced proper hardware support provided minimizing cache conflicts exploiting spatial locality r supercomputing performance evaluation perfect benchmarks strategy array management local memory directed prefetching scalar processors integrated hardwaresoftware solution effective management local storage highperformance systems computer architecture quantitative approach ibm risc system6000 technology intel i860 reference manual speculative prefetching improving directmapped cache performance addition small optimizing parallelism data locality sites editor alpha architecture reference manual prefetch unit vector operations scalar computers cache memories data locality optimizing algorithm empirical study array subscripts data de pendendencies tr data locality optimizing algorithm mips risc architectures effective onchip preloading scheme reduce data access penalty alpha architecture reference manual prefetch unit vector operations scalar computers optimizing parallelism data locality speculative prefetching supercomputer performance evaluation perfect benchmarks cache memories ctr toni juan toms lang juan j navarro differencebit cache acm sigarch computer architecture news v24 n2 p114120 may 1996 kuangchih liu chungta king performance study bounteous transfer multiprocessor sectored caches journal supercomputing v11 n4 p405420 dec 1997 edward h gornish alexander veidenbaum integrated hardwaresoftware data prefetching scheme sharedmemory multiprocessors international journal parallel programming v27 n1 p3570 feb 1999 weifen lin steven k reinhardt doug burger designing modern memory hierarchy hardware prefetching ieee transactions computers v50 n11 p12021218 november 2001