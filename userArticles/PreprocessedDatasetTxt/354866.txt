properties rescheduling size invariance dynamic reschedulingbased vliw crossgeneration compatibility abstractthe objectcode compatibility problem vliw architectures stems statically scheduled nature dynamic rescheduling dr 1 technique solve compatibility problem vliws dr reschedules program code pages firsttime page faults ie code pages accessed first time execution treating page code unit rescheduling makes susceptible hazards changes page size process rescheduling paper shows changes page size due insertion andor deletion nops code presents isa encoding called list encoding require explicit encoding nops code algorithms perform rescheduling acyclic code cyclic code presented followed discussion property reschedulingsize invariance rsi satisfied list encoding b introduction objectcode compatibility problem vliw architectures stems statically scheduled nature compiler vliw machine schedules code specific machine model machine generation precise cyclebycycle execution runtime machine model assumptions given code schedule unique semantics thus code scheduled one vliw guaranteed execute correctly different vliw model characteristic vliws often cited impediment vliws becoming generalpurpose computing paradigm 2 example illustrate shown figures 1 2 3 figure 1 shows example vliw schedule machine model two ialus one load unit one multiply unit one store unit execution latencies units indicated let machine generation known generation x figure 2 shows nextgeneration generation multiply load cycle latency 1 cycle latency ialu ialu mul gr8 r4 r3 1 cycle latency 1 cycle latency 3 cycle latency figure 1 scheduled code vliw generation x latencies changed 4 3 cycles respectively generation x schedule execute correctly machine due flow dependence operations b c h e f figure 3 shows schedule generation xn machine includes additional multiplier latencies fus remain shown figure 1 code scheduled new machine execute correctly older machines code moved order take advantage additional multiplier particular e f moved trivial way adapt schedule older machines case downward incompatibility generations situation different generations machines share binaries eg via file server compatibility requires either mechanism adjust schedule different set binaries generation one way avoid compatibility problem would maintain binary executables customized run new vliw generation would violate copyprotection rules would also increase diskspace usage alternatively program executables may translated rescheduled target machine model achieve compatibility done hardware software hardware approach adds superscalarstyle runtime scheduling hardware vliw 3 4 5 6 7 principle disadvantage approach adds complexity hardware may potentially stretch cycle time machine rescheduling hardware falls critical path software approach perform offline compilation scheduling program source code decorated object modules files code rescheduled manner yields better relative speedups technique cumbersome use due offline nature could also imply violation copy protection dynamic rescheduling dr 1 third alternative solve compatibility problem dynamic rescheduling program binary compiled given vliw generation machine model allowed run different vliw generation firsttime page fault pagefault occurs code page accessed first time program r4 ready cycle latency 1 cycle latency ialu ialu mul gr8 r4 r3 r6 ready r7 ready 1 cycle latency 1 cycle latency 43 cycle latency figure 2 generation incompatibility due changes functional unit latencies shown arrows old latencies shown parentheses operations produce incorrect results new latencies operations b e r4 ready cycle latency 1 cycle latency ialu ialu mul gr8 r4 r3 r6 ready 1 cycle latency 1 cycle latency 3 cycle latency mul r7 ready 3 cycle latency figure 3 generation xn schedule downward incompatibility due change vliw machine organization trivial way translate new schedule older generation execution page fault handler invokes module called rescheduler reschedule page host rescheduled code pages cached special area file system future use avoid repeated translations since dynamic rescheduling technique translates code perpage basis susceptible hazard changes pagesize due process reschedul ing changes machine model across generation warrant addition andor deletion nops tofrom page would lead page overflow underflow paper discusses technique called list encoding isa proves property reschedulingsize invariance rsi guarantees codesize change due dynamic rescheduling organization paper follows section 2 presents terminology used paper section 3 briefly describes dynamic rescheduling demonstrates problem codesize change example section 4 introduces concept reschedulingsize invariance rsi presents list encoding proves rsi properties list encoding section 5 presents concluding remarks directions future research terminology used paper originally rau 3 8 introduced discussion follows wide instructionword multiop vliw schedule consists several operations ops ops multiop issued cycle vliw programs latencycognizant meaning scheduled knowledge functional unit latencies architecture runs latencycognizant programs termed nonunit assumed latency nual architecture unit assumed latency ual architecture assumes unit latencies functional units superscalar architectures ual whereas vliws nual machine models discussed paper nual two scheduling models latencycognizant programs equals model lessthanorequals lte model 9 equals model schedules built operation takes exactly much specified execution latency contrast lte model operation may take less equal specified latency general equals model produces slightly shorter schedules lte model mainly due register reuse possible equals model however lte model simplifies implementation precise interrupts provides binary compatibility latencies reduced scheduler backend compiler dynamic rescheduler pagefault handler presented paper follow lte scheduling model purposes paper assumed program codes classified two broad categories acyclic code cyclic code cyclic code consists short inner loops program typically amenable software pipelining 10 hand acyclic code contains relatively large number conditional branches typically large loop bodies makes acyclic code unamenable software pipelining instead body loop treated piece acyclic code surrounded loop control ops examples cyclic code inner loops like counted doloops found scientific code examples acyclic code nonnumeric programs interactive programs distinction types code made scheduling rescheduling algorithms cyclic acyclic code differ considerably dynamic rescheduling technique treats separately also assumed program code structured form superblocks 11 hyperblocks 12 hyperblocks constructed ifconversion code using predication 13 14 support predicated execution ops also assumed superblocks hyperblocks single entry point block beginning block may multiple sideexits property useful bypassing problems introduced speculative code motion dr discussion found elsewhere see 15 overview reschedule buffer cache resume execution firsttime page fault context switch figure 4 dynamic rescheduling sequence events technique dynamic rescheduling performs translation code pages firsttime page faults stores translated pages subsequent use figure 4 shows sequence events take place dynamic rescheduling event 1 indicates firsttime pagefault pagefault os switches context fetches requested page next level memory hierarchy shown shown events 2 3 respectively events 1 2 3 standard case every page fault encountered os different case dr invocation module called rescheduler firsttime page fault rescheduler operates newly fetched page reschedule execute correctly host machine shown event 4 event 5 shows rescheduled page written area file system future use event 6 execution resumes facilitate detection vliw generation mismatch firsttime page fault program binary holds generationid header machine model binary originally scheduled boundaries identify pieces cyclic code program also stored program binary information made available rescheduler performs rescheduling page rescheduled code remains main memory displaced page memory time written special area file system called text swap subsequent accesses page lifetime program fulfilled text swap text swap may allocated perexecutable basis compile time allocated os systemwide global area shared active processes overhead rescheduling quantitatively expressed terms following factors 1 time spent firsttime pagefaults reschedule page 2 time spent writing rescheduled pages text swap area 3 amount disk space used store translated pages discussion overhead introduced dr investigation tradeoffs involved design textswap used reduce overhead beyond scope paper see 16 details 31 insertion deletions nops compiler schedules code vliw independent ops start execution machine cycle grouped together form single multiop op multiop bound execute specific functional unit often however compiler cannot find enough ops keep fus busy given cycle empty slots multiop filled nops machine cycles compiler cannot schedule even single op execution nops scheduled fus cycle instruction called empty multiop logically rescheduler dr thought performing following steps generate new code 1 matter type code first breaks multiop individual ops create ordered set ops second discards nops set ordered set ops thus obtained ual schedule third step depending upon resource constraints data dependence constraints rearranges ops ual schedule create new nual schedule fourth last step new nops empty multiops inserted required preserve semantics computation note number nops empty multiops newly inserted may old code may lead problem size code may change due rescheduling important note time change codesize due nops empty multiops example changes codesize illustrated figure 5 left portion figure old code shown assume execution latency ops e f g h 1cycle op b 3cycles load op c 2cycles ops e f dependent result op c hence begin execution op c finishes execution newer generation architecture shown right one ialu removed machine increasing latency loads 1 3cycles old code executed newer generation dr invokes rescheduler generates new code shown account new longer latency load unit inserts 1 terms new code old code necessarily mean code input rescheduler belongs older machine generation similar counterpart old code used means code input rescheduler new code means code output rescheduler ialu ialu fpadd fpmul load store cmpp br nop nop nop nop nop nop nop nop g nop nop nop nop nop nop h nop load latency increases one less ialu f dependent c c takes 2 cycles bytes total fpadd fpmul load store cmpp br ialu nop nop nop nop nop nop nop nop nop nop nop nop nop nop nop nop nop nop nop nop f nop nop nop nop nop nop nop g nop nop nop nop nop h nop 336 bytes total 10 extra nops figure 5 example illustrating insertiondeletion nops empty multiops due dynamic rescheduling empty multiop third cycle also old multiop consisting operations e f broken two consecutive multiops due reduction number ialus observe new code bigger old code assuming ops 64bits net increase size code 80 bytes corresponding 10extra nops inserted rescheduling page size computer system operates usually dictated hardware os nontrivial os handle changes page sizes run time previous work done area talluri hill attempts support multiple page sizes pagesize integral multiple base pagesize 17 18 19 enhanced vm hardware translationlookaside buffer tlb enhanced vm management policy must available support proposed technique possible help extra hardware multiple code page sizes used handle variations pagesize due dr would lead multitude problems first problem inefficient memory usage new page created accommodate spillover generated rescheduler remainder new page remains unused hand code page shrinks due dr leads hole memory second problem arises due control restructuring new page inserted must placed end code address space last multiop original page must modified jump new page last multiop new page must modified jump page lies original page code positioning optimization performed old code order optimize icache accesses process could violate ordering potentially leading performance degradation perhaps serious problem code movement within old page new page could alter branch target addresses merge points old code leading incorrect code may even possible repair code code jumps altered branch targets may visible rescheduler rescheduling time one solution avoid problem codesize change use specialized isa encoding hides nops empty multiops code since codesize changes dr due additiondeletion nops encoding circumvents problem encoding called list encoding ability discussed detail section 4 along rescheduling algorithms cyclic acyclic code rescheduling sizeinvariance empty multiops object code hence zeronop encoding property list encoding used support dr section presents formal definition list encoding followed introduction concept rescheduling sizeinvariance rsi also shown listencoded schedule code rescheduling sizeinvariant 41 list encoding rsi operation op defined 6tuple fh p n pred fu type opcode operandsg h 2 f0 1g 1bit field called headerbit p n nbit field called pause st pred stage predicate discussed section 43 futype uniquely identifies fu instance op must execute opcode uniquely identifies task op operands set valid operands defined op ops constant width 2 header op iff value headerbit field 1 2 definition 3 vliw multiop vliw multiop defined unordered sequence ops fo number hardware resources ops issued concurrently 1 header op 2 definition 4 vliw schedule vliw schedule defined ordered sequence multiops fm discussion list encoding order ops scheme encoding fixedwidth given vliw schedule new multiop begins header op ends exactly op next header op multiop fetch hardware uses rule identify fetch next multiop value p n field op referred pause used fetch hardware stop multiop fetch number machine cycles indicated p n mechanism devised eliminate explicit encoding empty multiops schedule futype field indicates functional unit op execute futype field allows elimination nops inserted compiler arbitrary multiop prior execution multiop member ops routed appropriate functional units based value futype field scheme encoding components vliw schedule termed list encoding since size every op size given list encoded schedule expressed terms number ops j number multiops j op j number ops given multiop definition 5 vliw generation vliw generation g defined set fr lg r set hardware resources g l 2 set execution latencies ops operation set g r set consisting pairs r resource type n r number instances r definition vliw generation model complex resource usage patterns op used 20 21 22 instead member set machine resources r presents higherlevel abstraction functional units found modern processors abstraction lowlevel machine resources registerfile ports operandresult busses required execution op functional unit bundled resource resources indicated manner assumed busy period time equal latency executing op indicated appropriate member set l definition 6 rescheduling sizeinvariance rsi vliw schedule said satisfy rsi property iff sizeofs gn versions original schedule prepared execution arbitrary machine generations g n gm spectively schedule said rescheduling sizeinvariant iff satisfies rsi property 2 proof list encoding rsi presented two parts first shown acyclic code program rsi list encoded followed proof cyclic code rsi list encoded since code assumed either acyclic cyclic result list encoding makes rsi follow remainder section algorithms reschedule types codes presented followed proofs 42 rescheduling sizeinvariant acyclic code algorithm reschedule acyclic code vliw generation g old generation g new shown algorithm reschedule acyclic code assumed old new schedules lte schedules see section 2 register file architecture compiler register usage convention algorithm reschedule acyclic code input old old schedule assumed n old cycles long old g machine model description old vliw new g machine model description new vliw output new new schedule var old length old new length new scoreboardnumber registers flag registers inuse old ru n new resource usage matrix r represents resource types g new n r number instances resource type r g new useinfon new number registers mark register usage new cycle op scheduled satisfying data dependence constraints cycle op scheduled satisfying data dependence resource constraints functions ru loopkup ffi returns earliest cycle later cycle ffi op scheduled satisfying data dependence resource constraints ru update oe marks resources used op cycle oe new dest register returns destination register op source register returns list source registers op latest use time oe returns latest cycle new register oe used recent writer ae returns id op modified register ae latest old begin multiop old c 2 old 0 c n old begin resource constraint check op ow 2 old completes cycle c begin new new cycle new new cycle j ow ru update rcffi ow update scoreboard op 2 old unfinished cycle c begin scoreboarddest register r reserved data dependence checks op r 2 old c begin r ffi 0 antidependence oe 2 dest register r latest use time oe pure dependence oe 2 source register r r ffi output dependence oe 2 dest register r r ffi rsi property list encoded acyclic code schedule proved theorem 1 arbitrary list encoded schedule acyclic code rsi proof proof presented using induction number ops arbitrary list encoded schedule let l arbitrary ordered sequence ops 1 occur piece acyclic code let f denote directed dependence graph ops l ie op l node f data controldependences ops indicated directed arcs f let sgn list encoded schedule l generated using dependence graph f designed execute certain vliw generation g n also let gm denote another vliw generation target rescheduling dr induction basis l 1 op sequence length 1 case sizeof gn dependence graph single node trivial case sgn rsi rescheduling generation gm number ops schedule remain 1 induction step l p op sequence length p p 1 assume sgn rsi words consider op sequence l p1 length p 1 l p1 obtained l p adding one op original program fragment let additional op denoted z op z thought borrowed original program correctness computation compromised l p ordered sequence ops op z must either prefix l p suffix also let tgn denote list encoded schedule sequence l p1 means sizeof gn 1 order prove current theorem must proved tgn rsi sgn rsi addition op z l p may change structure dependence graph f p two ways 1 op z adds one data dependence arcs f p 2 op z add data dependence arcs f p ffl op z adds dependences case corresponds fact op z control andor datadependent one ops l p vice versa following two subcases schedule constructed includes op z 1 construction tgn using dependence graph f 2 rescheduling tgn tgm cases dependences introduced op z must honored resource constraints must satisfied well done using wellknown list scheduling algorithm first subcase reschedule acyclic code algorithm second sub case appropriate nops empty multiops inserted schedule algorithms however schedules tgn tgm list encoded empty multiops made implicit using pause field header op previous multiop nops multiop made implicit via futype field ops thus source size increase schedules tgn tgm due newly added op z ffl op z add dependences case resource constraints would warrant insertion empty multiops argument similar previous case trivial see source size increase schedules tgn tgm newly added op z thus cases sizeof gn 1 equation 2 similarly cases sizeof gm equations 3 4 induction proved arbitrary list encoded schedule acyclic code rsi example transition code previously shown figure 5 application algorithm reschedule acyclic code shown figure 6 assuming original schedule belonged acyclic category observed size original code left rescheduled code right nops empty multiops eliminated list encoded schedules rescheduling algorithm merely rearranged ops adjusted values h p n pause fields within ops ensure correctness execution g new 43 rescheduling sizeinvariant cyclic code programs spend great deal time executing inner loops hence study scheduling strategies inner loops attracted great attention literature 23 24 25 8 26 27 28 29 inner loops typically small bodies relatively fewer ops header bit pause optype rest op dynamic rescheduling figure example list encoded schedule acyclic code rsi makes hard find ilp within loopbodies software pipelining wellunderstood scheduling strategy used expose ilp across multiple iterations loop 30 25 two ways perform software pipelining first one uses loop unrolling loop body unrolled fixed number times scheduling loop bodies scheduled via unrolling subjected rescheduling via reschedule acyclic code algorithm described section 42 code expansion introduced due unrolling however often unacceptable hence second technique modulo scheduling 30 employed moduloscheduled loops little code expansion prologue epilogue loop makes attractive paper moduloscheduled loops examined rsi property unrolledandscheduled loops covered acyclic rsi techniques presented previously first discussion structure moduloscheduled loops presented followed algorithm reschedule modulo scheduled code section ends formal treatment show listencoded moduloscheduled cyclic code rsi concepts rau 29 used vehicle discussion section assumptions hardware support execution modulo scheduled loops follows loops datum generated one iteration loop consumed one successive iterations interiteration data dependence also conditional code loop body multiple datadependent paths execution exist moduloscheduling loops nontrivial 2 paper assumes three forms hardware support circumvent problems first register renaming via rotating registers 29 order handle interiteration data dependencies loops assumed second convert control dependencies within loop body data dependencies support predicated 2 see 31 32 work area execution 14 assumed third support sentinel scheduling 33 ensure correct handling exceptions speculative execution assumed also preconditioning 29 counteddo loops presumed performed modulo scheduler necessary modulo scheduled loopomega gn consists three parts prologue gn kernel gn epilogue gn g n machine generation loop scheduled prologue initiates new iteration every ii cycles ii known initiation interval slice ii cycles execution loop called stage last stage first iteration execution kernel begins iterations various stages execution point time inside kernel loop executes steady state called kernel code branches back kernel multiple iterations simultaneously progress different stage execution single iteration completes end stage branch ops used support modulo scheduling loops special semantics branch updates loop counts enablesdisables execution iterations loop condition becomes false kernel falls epilogue allows completion stages unfinished iterations figure 7 shows example modulo schedule loop identifies prologue kernel epilogue row schedule describes cycle execution box represents set ops execute resource eg functional unit one stage height box ii loop stages belonging given iteration marked unique alphabet 2 fa b c e fg figure 7 also shows loop different form kernelonly ko loop 26 29 kernelonly loop prologue epilogue loop collapse kernel without changing semantics execution loop achieved predicating execution distinct stage modulo scheduled loop distinct predicate called stage predicate new stage predicate asserted loopback branch execution stage predicated newly asserted predicate enabled future executions kernel loop execution begins stages incrementally enabled accounting loop prologue stages enabled loop kernel execution loop steady state loop condition becomes false predicates stages reset thus disabling stages one one accounts iteration epilogue loop modulo scheduled loop represented ko form adequate hardware predicated execution software modulo scheduler predicate stages loop support assumed discussion ko loop schedules found 29 moduloscheduled loops represented ko form ko form thus potential encode modulo schedules classes loops property useful study dynamic rescheduling loops shown shortly size modulo scheduled loop larger original size loop modulo schedule explicit prologue kernel epilogue contrast ko loop schedule exactly one copy stage original loop body hence size original loop body provided original loop completely ifconverted 3 property ko loops useful performing dynamic rescheduling modulo scheduled 3 preconditioned counted doloops size size loop body preconditioning p1 p2 p3 p4 p5 f p6 f f f f f resources cycle prologue epilogue kernel1111 cycle figure 7 modulo scheduled loop left modulo scheduled loop prologue kernel epilogue marked shown schedule shown right collapsed kernelonly ko form stage predicates used turn execution ops given stage table shows values stage predicates would take loop loops algorithm reschedule ko loop details steps input algorithm modulo scheduled ko loop machine models old new generations g old g new briefly algorithm works follows identification predicates enable individual stages performed first order imposed allows derivation order execution stages single iteration ordering predicates may implicit predicateid used given stage increasing order predicateids alternatively order information could stored object file made available time dr invoked without substantial overhead order execution stages loop obtained reconstruction loop original unscheduled form performed time modulo scheduler invoked arrive new ko schedule new generation algorithm reschedule ko loop input omega old ko kernelonly modulo schedule number stages old g machine model description old vliw new g machine model description new vliw output schedule g new var old table n old buckets holding ops unique stage relative ordering ops bucket retained functions findstagepred returns stage predicate op enabled disabled puts op bucket b p orderbuckets b func sorts table buckets b according ordering function func stagepredordering describes statically imposed order stage predicates begin unscramble old modulo schedule multiops 2omega old op 2 begin findstagepred order buckets orderbuckets b stagepredordering perform modulo scheduling perform modulo scheduling sorted table buckets b using algorithm described rau 30 generate ko scheduleomega new rsi nature list encoded modulo scheduled ko loop proved theorem 2 arbitrary list encoded kernelonly modulo schedule loop rsi proof let l arbitrary ordered sequence ops 1 represents loop body let f denote directed dependence graph ops l ie op l node f data controldependences ops indicated directed arcs f note interiteration data dependences also indicated f letomega gn denote list encoded ko modulo schedule generation g n also let gm denote vliw generation rescheduling performed induction basis l 1 loop body length 1 case dependence graph single node trivial case thatomega gn rsi rescheduling generation gm number ops schedule remain 1 note loop degenerate case induction step l p loop body length p p 1 assume thatomega gn rsi words consider another loop body l p1 length p 1 let p st op denoted z also let theta gn denote list encoded ko modulo schedule l p1 means 1 order prove theorem hand must proved theta gn rsi ifomega gn rsi possible due op z l p1 nature graph f p could different graph f p1 two ways 1 op z data dependent one ops l p1 vice versa 2 op z independent ops l p1 cases data dependences resource constraints honored modulo scheduling algorithm via appropriate use nops andor empty multiops within schedule schedule list encoded nops empty multiops made implicit via use pause futype fields within ops hence words result equation 6 follows similarly cases sizeof theta gm equations 9 10 induction proved arbitrary list encoded ko modulo schedule rsi corollary 1 list encoded schedule rsi proof program codes divided two categories acyclic code cyclic code defined section 2 hence follows theorem 1 theorem 2 list encoded schedule rsi conclusions paper presented highlights solution crossgeneration compatibility problem vliw architectures solution called dynamic rescheduling performs rescheduling program code pages firsttime page faults assistance compiler isa os required dynamic rescheduling process reschedul ing nops must added todeleted page ensure correctness schedule additionsdeletions could lead changes page size codesize changes hard handle runtime would require extra support hardware tlb extensions software vm management extension isa encoding called list encoding encodes nops program implicitly presented list encoded isa fixedwidth ops header op first op multiop indicates number empty multiops following information eliminates need explicitly encode empty multiops schedule optype field encoded op eliminates need explicitly encoding nops within multiop decode hardware use information expand route op appropriate execution resource property list encoding called reschedulingsize invariance rsi proved acyclic cyclic kernelonly moduloscheduled codes schedule code rsi iff code size remains constant across dynamic rescheduling transformation study instruction fetch hardware icache organizations required support list encoding previously studied 34 work presented paper extended study encoding techniques may reschedulingsize invariant nonrsi encodings also study rescheduling algorithms operate nonrsi encodings conducted topics currently investigated authors r dynamic rescheduling technique object code compatibility vliw architectures dynamically scheduled vliw processors hardware support large atomic units dynamically scheduled machines architectural framework supporting heterege neous instructionset architectures fillunit approach multiple instruction issue architectural framework migration cisc higher performance platforms cydra 5 departmental supercomputer hpl playdoh architecture specification version 10 approach scientific array processing architectural design ap120bfps164 family superblock effective structure vliw superscalar compilation effective compiler support predicated execution using hyperblock conversion control dependence data dependence predicated execution optimization vliw compatibility systems employing dynamic rescheduling persistent rescheduledpage cache lowoverhead objectcode compatibility vliw architectures tradeoffs supporting two page sizes virtual memory support multiple page sizes surpassing tlb performance superpages less operating system support reduced multipipeline machine description preserves scheduling constraints optimization machine descriptions efficient use efficient instruction scheduling using finite state automata scheduling techniques easily schedulable horizontal architecture high performance scientific computing efficient code generation horizontal architec tures compiler techniques architectural support software pipelining effective scheduling technique vliw machines overlapped loop support cydra 5 realistic scheduling compaction pipelined architec tures new global software pipelining algorithm code generation schemas modulo scheduled doloop whileloops iterative modulo scheduling algorithm software pipelining loops modulo scheduling isomorphic control transformations software pipelining loops conditional branches sentinel scheduling model compilercontrolled speculative execution instruction fetch mechanisms vliw architectures compressed encodings tr ctr masahiro sowa ben abderazek tsutomu yoshinaga parallel queue processor architecture based produced order computation model journal supercomputing v32 n3 p217229 june 2005 jun yan wei zhang hybrid multicore architecture boosting singlethreaded performance acm sigarch computer architecture news v35 n1 p141148 march 2007