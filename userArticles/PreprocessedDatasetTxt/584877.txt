evaluation hierarchical clustering algorithms document datasets fast highquality document clustering algorithms play important role providing intuitive navigation browsing mechanisms organizing large amounts information small number meaningful clusters particular hierarchical clustering solutions provide view data different levels granularity making ideal people visualize interactively explore large document collectionsin paper evaluate different partitional agglomerative approaches hierarchical clustering experimental evaluation showed partitional algorithms always lead better clustering solutions agglomerative algorithms suggests partitional clustering algorithms wellsuited clustering large document datasets due relatively low computational requirements also comparable even better clustering performance present new class clustering algorithms called constrained agglomerative algorithms combine features partitional agglomerative algorithms experimental results showed consistently lead better hierarchical solutions agglomerative partitional algorithms alone b introduction hierarchical clustering solutions form trees called dendrograms great interest number application domains hierarchical trees provide view data different levels abstraction consistency clustering solutions different levels granularity allows flat partitions different granularity extracted data analysis making ideal interactive exploration visualization addition many times clusters subclusters hierarchical structure indeed natural constrain underlying application domain eg biological taxonomy phylogenetic trees 9 hierarchical clustering solutions primarily obtained using agglomerative algorithms 27 19 10 11 18 objects initially assigned cluster pairs clusters repeatedly merged whole tree formed however partitional algorithms 22 16 24 5 33 13 29 4 8 also used obtain hierarchical clustering solutions via sequence repeated bisections recent years various researchers recognized partitional clustering algorithms wellsuited clustering large document datasets due relatively low computational requirements 6 20 1 28 however common belief terms clustering quality partitional algorithms actually inferior less effective agglomerative counterparts belief based experiments low dimensional datasets well limited number studies agglomerative approaches outperformed partitional kmeans based approaches example larsen 20 observed group average greedy agglomerative clustering outperformed various partitional clustering algorithms document datasets trec reuters light recent advances partitional clustering 6 20 7 4 8 revisited question whether agglomerative approaches generate superior hierarchical trees partitional approaches focus paper work supported nsf ccr9972519 eia9986042 aci9982274 aci0133464 army research office contract dadaag559810441 doe asci program army high performance computing research center contract number daah0495 c0008 related papers available via www url httpwwwcsumnedukarypis compare various agglomerative partitional approaches task obtaining hierarchical clustering solution partitional methods compared use different clustering criterion functions derive solutions agglomerative methods use different schemes selecting pair clusters merge next partitional clustering algorithms used six recently studied criterion functions 34 shown produce highquality partitional clustering solutions agglomerative clustering algorithms evaluated three traditional merging criteria ie singlelink completelink group average upgma new set merging criteria derived six partitional criterion functions overall compared six partitional methods nine agglomerative methods addition traditional partitional agglomerative algorithms developed new class agglomerative algorithms introduced intermediate clusters obtained partitional clustering algorithms constrain space agglomeration decisions made refer constrained agglomerative algorithms algorithms generate hierarchical trees two steps first intermediate partitional clusters agglomerative algorithm builds hierarchical subtree second subtrees combined single tree building upper tree using subtrees leaves experimentally evaluated performance methods obtain hierarchical clustering solutions using twelve different datasets derived various sources experiments showed partitional algorithms always generate better hierarchical clustering solutions agglomerative algorithms constrained agglomerative methods consistently lead better solutions agglomerative methods alone cases outperform partitional methods well believe observed poor performance agglomerative algorithms errors make early agglomeration superiority partitional algorithm also suggests partitional clustering algorithms wellsuited obtaining hierarchical clustering solutions large document datasets due relatively low computational requirements also comparable better performance rest paper organized follows section 2 provides information documents represented similarity distance documents computed section 3 describes different criterion functions well criterion function optimization hierarchical partitional algorithms section 4 describes various agglomerative algorithms constrained agglomerative algorithms section 5 provides detailed experimental evaluation various hierarchical clustering methods well experimental results constrained agglomerative algorithms section 6 discusses important observations experimental results finally section 7 provides concluding remarks preliminaries throughout paper use symbols n k denote number documents number terms number clusters respectively use symbol denote set n documents want cluster denote one k clusters n 1 n 2 n k denote sizes corresponding clusters various clustering algorithms described paper use vectorspace model 26 represent document model document considered vector termspace particular employed weighting model document represented tf frequency th term document df number documents contain th term account documents different lengths length document vector normalized unit length tfidf 1 document vector unit hypersphere rest paper assume vector representation document weighted using tfidf normalized unit length given set documents corresponding vector representations define composite vector centroid vector c c vectorspace model cosine similarity commonly used method compute similarity two documents j defined cosd cosine formula simplified cosd document vectors unit length measure becomes one documents identical zero nothing common ie vectors orthogonal vector properties using cosine function measure similarity documents take advantage number properties involving composite centroid vectors set documents particular two sets unitlength documents containing n n j documents respectively j c corresponding composite centroid vectors following true 1 sum pairwise similarities documents document j equal j cosd q r q r q 2 sum pairwise similarities documents equal 2 q r cosd q r q r note equation includes pairwise similarities involving pairs vectors 3 hierarchical partitional clustering algorithm partitional clustering algorithms used compute hierarchical clustering solution using repeated cluster bisectioning approach 28 34 approach documents initially partitioned two clusters one clusters containing one document selected bisected process continues leading n leaf clusters containing single document easy see approach builds hierarchical agglomerative tree top ie single allinclusive cluster bottom document cluster rest section describe various aspects partitional clustering algorithm used study 31 clustering criterion functions key characteristic partitional clustering algorithms use global criterion function whose optimization drives entire clustering process partitional clustering algorithms clustering problem stated computing clustering solution value particular criterion function optimized clustering criterion functions used study classified four groups internal external hybrid graphbased internal criterion functions focus producing clustering solution optimizes function defined documents cluster take account documents assigned different clusters external criterion functions derive clustering solution focusing optimizing function based various clusters different graph based criterion functions model documents graph use clustering quality measures defined graph model hybrid criterion functions simultaneously optimize multiple individual criterion functions internal criterion functions first internal criterion function maximizes sum average pairwise similarities documents assigned cluster weighted according size cluster specifi cally use cosine function measure similarity documents want clustering solution optimize following criterion function maximize r 3 second criterion function used popular vectorspace variant kmeans algorithm 6 20 7 28 17 algorithm cluster represented centroid vector goal find clustering solution maximizes similarity document centroid cluster assigned specifically use cosine function measure similarity document centroid criterion function becomes following maximize r 4 comparing 2 criterion function 1 see essential difference criterion functions 2 scales withincluster similarity r term opposed n r term used 1 term r nothing squareroot pairwise similarity document r tend emphasize importance clusters beyond r 2 term whose documents smaller pairwise similarities compared clusters higher pairwise similarities also note similarity document centroid vector cluster defined dotproduct vectors get back 1 criterion function external criterion functions quite hard define external criterion functions lead meaningful clustering solutions example may appear intuitive external function may derived requiring centroid vectors different clusters mutually orthogonal possible ie contain documents share terms across different clusters however many problems criterion function trivial solutions achieved assigning first k 1 clusters single document shares terms rest assigning rest documents kth cluster reason external function discuss tries separate documents cluster entire collection opposed trying separate documents among different clusters external criterion function motivated multiple discriminant analysis similar minimizing trace betweencluster scatter matrix 9 30 particular external criterion function defined minimize c centroid vector entire collection equation see try minimize cosine centroid vector cluster centroid vector entire collection minimizing cosine essentially try increase angle much possible also note contribution cluster weighted based cluster size larger clusters weight heavier overall clustering solution equation 5 rewritten composite vector entire document collection note since 1d constant irrespective clustering solution criterion function restated see equation 6 eventhough initial motivation define external criterion function used cosine function measure separation cluster entire collection criterion function take account withincluster similarity documents due r term thus e 1 actually hybrid criterion function combines external internal characteristics clusters hybrid criterion functions study focus two hybrid criterion function obtained combining criterion 1 respectively formally first criterion function second 8 note since e 1 minimized h 1 h 2 need maximized inversely related e 1 graph based criterion functions alternate way viewing relations documents use similarity graphs given collection n documents similarity graph g obtained modeling document vertex edge pair vertices whose weight equal similarity corresponding documents viewing documents fashion number internal external combined criterion functions defined measure overall clustering quality study investigate one criterion function called minmaxcut proposed recently 8 minmaxcut falls category criterion functions combine internal external views clustering process defined 8 minimize cuts r ss r edgecut vertices r rest vertices graph ss r edgecut two sets vertices b defined sum edges connecting vertices vertices b motivation behind criterion function clustering process viewed partitioning documents groups minimizing edgecut partition however reasons similar discussed section 31 external criterion may trivial solutions reason edgecut scaled sum internal edges shown 8 scaling leads better balanced clustering solutions use cosine function measure similarity documents equations 1 2 criterion function rewritten since k constant criterion function simplified 32 criterion function optimization partitional algorithm uses approach inspired kmeans algorithm optimize one criterion functions similar used 28 34 details algorithm provided remaining section initially random pair documents selected collection act seeds two clusters document similarity two seeds computed assigned cluster corresponding similar seed forms initial twoway clustering clustering repeatedly refined optimizes desired clustering criterion function refinement strategy used consists number iterations iteration documents visited random order document compute change value criterion function obtained moving one k 1 clusters exist moves lead improvement overall value criterion function moved cluster leads highest improvement cluster exists remains cluster already belongs refinement phase ends soon perform iteration documents moved clusters note unlike traditional refinement approach used kmeans type algorithms algorithm moves document soon determined lead improvement value criterion function type refinement algorithms often called incremental 9 since move directly optimizes particular criterion function refinement strategy always converges local minima furthermore various criterion functions use refinement strategy defined terms cluster composite centroid vectors change value criterion functions result single document moves computed efficiently greedy nature refinement algorithm guarantee converge global minima local minima solution obtains depends particular set seed documents selected initial clustering eliminate sensitivity overall process repeated number times compute different clustering solutions ie initial clustering followed cluster refinement one achieves best value particular criterion function kept experiments used 10 rest discussion refer clustering solution mean solution obtained selecting best n potentially different solutions 33 cluster selection experimented two different methods selecting cluster bisect next first method uses simple strategy bisecting largest cluster available point clustering solution earlier experience approach showed leads reasonably good balanced clustering solutions 28 34 however limitation cannot gracefully operate datasets natural clusters different sizes tend partition larger clusters first overcome problem obtain natural hierarchical solutions developed method among current k clusters selects cluster leads k clustering solution optimizes value particular criterion function among different k choices experiments showed approach performs somewhat better previous scheme method used experiments presented section 5 34 computational complexity one advantages partitional algorithm similar partitional algorithms relatively low computational requirements twoclustering set documents computed time linear number documents cases number iterations required greedy refinement algorithm small less 20 large extend independent number documents assume bisection step resulting clusters reasonably balanced ie cluster contains fraction original documents overall amount time required compute n 1 bisections log n hierarchical agglomerative clustering algorithm unlike partitional algorithms build hierarchical solution top bottom agglomerative algorithms build solution initially assigning document cluster repeatedly selecting merging pairs clusters obtain single allinclusive cluster thus agglomerative algorithms build tree bottom ie leaves toward top ie root 41 cluster selection schemes key parameter agglomerative algorithms method used determine pairs clusters merged step agglomerative algorithms accomplished selecting similar pair clusters numerous approaches developed computing similarity two clusters27 19 16 10 11 18 study used singlelink completelink upgma schemes well various partitional criterion functions described section 31 singlelink 27 scheme measures similarity two clusters maximum similarity documents cluster similarity two clusters j given sim singlelink contrast completelink scheme 19 uses minimum similarity pair documents measure similarity sim completelink general single completelink approaches work well either base decisions limited amount information singlelink assume documents cluster similar completelink approach upgma scheme 16 also known group average overcomes problems measuring similarity two clusters average pairwise similarity documents cluster sim upgma 12 partitional criterion functions described section 31 converted cluster selection schemes agglomerative clustering using general framework stepwise optimization 9 follows consider ndocument dataset clustering solution computed performing l merging steps solution contain exactly n l clusters merging step reduces number clusters one given n lway clustering solution pair clusters selected merged next one leads n l 1way solution optimizes particular criterion function one n ln l 12 pairs possible merges evaluated one leads clustering solution maximum minimum value particular criterion function selected thus criterion function locally optimized within particular stage agglomerative algorithm process continues entire agglomerative tree obtained 42 computational complexity two main computationally expensive steps agglomerative clustering first step computation pairwise similarity documents data set complexity step general 2 average number terms document small independent n second step repeated selection pair similar clusters pair clusters best optimizes criterion function naive way performing recompute gains achieved merging pair clusters level agglomeration select promising pair lth agglomeration step require l 2 time leading overall complexity 3 fortunately complexity step reduced singlelink completelink upgma 1 2 pairwise similarities improvements value criterion function achieved merging pair clusters j change different agglomerative steps long j selected merged consequently different similarities gains value criterion function computed pair clusters inserted priority queue pair clusters j selected merged form cluster p priority queue updated gains corresponding cluster pairs involving either j removed gains merging rest clusters newly formed cluster p inserted lth agglomeration step involves l priority queue delete insert operations priority queue implemented using binary heap total complexity operations l logn l overall complexity n 1 agglomeration steps 2 log n unfortunately original complexity 3 naive approach cannot reduced h 1 h 2 criterion functions improvement overall value criterion function pair clusters j merged tends changed pairs clusters result cannot precomputed inserted priority queue 43 constrained agglomerative clustering one advantages partitional clustering algorithms use information entire collection documents partition dataset certain number clusters hand clustering decisions made agglomerative algorithms local nature advantages well disadvantages advantage easy group together documents form small reasonably cohesive clusters task partitional algorithms may fail may split documents across cluster boundaries early partitional clustering process especially clustering large collections however disadvantage documents part particularly cohesive groups initial merging decisions may contain errors tend multiplied agglomeration progresses especially true cases large number equally good merging alternatives cluster one way improving agglomerative clustering algorithms eliminating type errors use partitional clustering algorithm constrain space agglomeration decisions made document allowed merge documents part partitionally discovered cluster approach partitional clustering algorithm used compute kway clustering solution clusters treated separate collection agglomerative algorithm used build tree one finally k different trees combined single tree merging using agglomerative algorithm treats documents subtree cluster already formed agglomeration advantage approach able benefit global view collection used partitional algorithms local view used agglomerative algorithms additional advantage computational complexity constrained clustering log k k number intermediate partitional clusters k reasonably large eg k equals n original complexity 2 log n agglomerative algorithms reduced 23 log n 5 experimental results experimentally evaluated performance various clustering methods obtain hierarchical solutions using number different datasets rest section first describe various datasets experimental methodology followed description experimental results datasets well various algorithms available cluto clustering toolkit downloaded httpwwwcsumnedukarypiscluto 51 document collections experiments used total twelve different datasets whose general characteristics summarized table 1 smallest datasets contained 878 documents largest contained 4069 documents ensure diversity datasets obtained different sources datasets used stoplist remove common words words stemmed using porters suffixstripping algorithm 25 moreover term occurs fewer two documents eliminated data source documents terms classes fbis fbis trec 2463 12674 17 hitech san jose mercury trec 2301 13170 6 reviews san jose mercury trec 4069 23220 5 la2 la times trec 3075 21604 6 re0 reuters21578 1504 2886 13 re1 reuters21578 1657 3758 25 k1a webace 2340 13879 20 k1b webace 2340 13879 6 wap webace 1560 8460 20 table 1 summary data sets used evaluate various clustering criterion functions fbis dataset foreign broadcast information service data trec5 31 classes correspond categorization used collection hitech reviews datasets derived san jose mercury newspaper articles distributed part trec collection tipster vol 3 one datasets constructed selecting documents part certain topics various articles categorized based descript tag hitech dataset contained documents computers electronics health med ical research technology reviews dataset contained documents food movies music radio restaurants selecting documents ensured two documents share descript tag contain multiple categories la1 la2 datasets obtained articles los angeles times used trec5 31 categories correspond desk paper article appeared include documents entertainment financial foreign metro national sports desks datasets tr31 tr41 derived trec5 31 trec6 31 trec7 31 collections classes datasets correspond documents judged relevant particular queries datasets re0 re1 reuters21578 text categorization test collection distribution 10 21 divided labels two sets constructed datasets ac cordingly dataset selected documents single label finally datasets k1a k1b wap webace project 23 12 2 3 document corresponds web page listed subject hierarchy yahoo 32 datasets k1a k1b contain exactly set documents differ documents assigned different classes particular k1a contains finergrain categorization contained k1b 52 experimental methodology metrics one different datasets obtained hierarchical clustering solutions using various partitional agglomerative clustering algorithms described sections 3 4 quality clustering solution determined analyzing entire hierarchical tree produced particular clustering algorithm often done using measure takes account overall set clusters represented hierarchical tree one measure fscore measure introduced 20 given particular class c r size n r particular cluster size n suppose n r documents cluster belong c r fscore class cluster defined recall value defined n r n r pc r precision value defined n r n class c r cluster fscore class c r maximum fscore value attained node hierarchical clustering tree fscore entire clustering solution defined sum individual class fscore weighted according class size c c total number classes perfect clustering solution one every class corresponding cluster containing exactly documents resulting hierarchical tree case fscore one general higher fscore values better clustering solution 53 comparison partitional agglomerative trees first set experiments focused evaluating quality hierarchical clustering solutions produced various agglomerative algorithms partitional algorithms agglomerative algorithms nine selection schemes criterion functions tested including six criterion functions discussed section 31 three traditional selection schemes ie singlelink completelink upgma named set agglomerative methods directly name criterion function selection scheme eg 1 means agglomerative clustering method 1 criterion function upgma means agglomerative clustering method upgma selection scheme also evaluated various repeated bisection algorithms using six criterion functions discussed section 31 named set partitional methods adding letter p front name criterion function eg pi 1 means repeated bisection clustering method 1 criterion function overall evaluated 15 hierarchical clustering methods fscore results hierarchical trees various datasets methods shown table 2 row corresponds one method column corresponds one dataset results table provided primarily completeness order evaluate various methods actually summarized results two ways one looking average performance method entire set datasets comparing pair methods see method outperforms datasets first way summarizing results average fscore method twelve different datasets however since hierarchical tree quality different datasets quite different felt simple averaging may distort overall results reason used averages relative fscores follows dataset divided fscore obtained particular method largest fscore obtained particular dataset 15 methods ratios represent degree particular method performed worse best method particular series experiments note different datasets method achieved best hierarchical tree measured fscore may different ratios less sensitive actual fscore values refer ratios relative fscores since higher fscore values better relative fscore values less one method averaged relative fscores various datasets method average relative fscore close 10 indicate method best datasets hand average relative fscore low method performed poorly fbis hitech k1a k1b la1 la2 re0 re1 reviews tr31 tr41 wap 1 0592 0480 0583 0836 0580 0610 0561 0607 0642 0756 0694 0588 2 0639 0480 0605 0896 0648 0681 0587 0684 0689 0844 0779 0618 slink 0481 0393 0375 0655 0369 0365 0465 0445 0452 0532 0674 0435 clink 0609 0382 0552 0764 0364 0449 0495 0508 0513 0804 0758 0569 table 2 fscores different datasets hierarchical clustering solutions obtained via various hierarchical clustering methods results relative fscores various hierarchical clustering methods shown table 3 row table corresponds one method column table corresponds one dataset average relative fscore values shown last column labeled average entries boldfaced correspond methods performed best entries underlined correspond methods performed best among agglomerative methods partitional methods fbis hitech k1a k1b la1 la2 re0 re1 reviews tr31 tr41 wap average 1 0843 0826 0836 0927 0724 0775 0878 0801 0738 0847 0833 0824 0821 2 0910 0826 0868 0993 0809 0865 0919 0902 0792 0945 0935 0866 0886 slink 0685 0676 0538 0726 0461 0464 0728 0587 0519 0596 0809 0609 0617 clink 0868 0657 0792 0847 0454 0571 0775 0670 0590 0900 0910 0797 0736 table 3 relative fscores averaged different datasets hierarchical clustering solutions obtained via various hierarchical clustering methods number observations made analyzing results table 3 first repeated bisection method 2 criterion function ie pi 2 leads best solutions datasets entire set experiments method either best always within 6 best solution average pi 2 method outperforms partitional methods agglomerative methods 28 737 respectively second upgma method performs best among agglomerative methods followed 2 method two methods together achieved best hierarchical clustering solutions among agglomerative methods datasets except re0 average upgma 2 methods outperform agglomerative methods 530 2 27 respectively third partitional methods outperform agglomerative methods except pi 1 method one remaining five partitional methods average performs better nine agglomerative methods least 5 pi 1 method performs little bit worse upgma method better rest agglomerative methods fourth singlelink completelink 1 performed poorly among agglomerative methods pi 1 performed worst among partitional methods finally average h 1 h 2 agglomerative methods lead second best hierarchical clustering solutions among agglomerative methods whereas ph 2 pe 1 partitional methods lead second best hierarchical clustering solutions among partitional methods relative performance different methods close average relative fscores quite similar 2 8 upgma clink table 4 dominance matrix various hierarchical clustering methods hence make comparisons methods easier second way summarizing results create dominance matrix various methods shown table 4 dominance matrix 15 15 matrix row column corresponds one method value entry number datasets method corresponding row outperforms method corresponding column example value entry row 2 column e 1 eight means eight twelve datasets 2 method outperforms e 1 method values close twelve indicate row method outperforms column method similar observations made analyzing results table 3 first partitional methods outperform agglomerative methods looking left bottom part dominance matrix see entries close twelve except two entries row pi 1 means partitional method performs better agglomerative methods datasets second looking submatrix comparisons within agglomerative methods ie left top part dominance matrix see upgma method performs best followed 2 h 1 slink clink 1 worst set agglomerative methods third submatrix comparisons within partitional methods ie right bottom part dominance matrix see pi 2 leads better solutions partitional methods datasets followed ph 2 performed worse partitional methods datasets 54 constrained agglomerative trees second set experiments focused evaluating constrained agglomerative clustering methods results obtained using different criterion functions find intermediate partitional clusters using upgma agglomerative scheme construct final hierarchical solutions described section 43 upgma selected performed best among various agglomerative schemes results constrained agglomerative clustering methods shown table 5 dataset shown different subtable six experiments performed dataset corresponds row row labeled upgma contains fscores hierarchical clustering solutions generated upgma method one intermediate cluster criterion functions rows labeled 10 20 n40 n20 contain fscores obtained constrained agglomerative methods using 10 20 n40 partitional clusters constrain solution n total number documents dataset row labeled rb contains fscores hierarchical clustering solutions obtained repeated bisection algorithms various criterion functions entries boldfaced correspond method performed best particular criterion function whereas entries underlined correspond best hierarchical clustering solution obtained dataset number observations made analyzing results table 5 first datasets except tr41 constrained agglomerative methods improved hierarchical solutions obtained agglomerative methods alone matter partitional clustering algorithm used obtain intermediate clusters improvement achieved even small number intermediate clusters second many cases constrained agglomerative methods performed even better corresponding partitional methods finally partitional clustering methods improved agglomerative hierarchical results partitional clustering methods performed fbis hitech rb 0623 0668 0686 0641 0702 0681 rb 0577 0512 0545 0581 0481 0575 k1a k1b la1 la2 rb 0721 0758 0761 0749 0646 0801 rb 0787 0725 0742 0739 0634 0766 re0 re1 rb rb 0858 0892 0877 0873 0769 0893 rb 0743 0783 0811 0800 0753 0833 reviews wap table 5 comparison upgma constrained agglomerative methods 10 20 n40 n20 intermediate partitional clusters repeated bisection methods various criterion functions best terms generating whole hierarchical trees 6 discussion important observation experimental results partitional methods performed better agglomerative methods discussed section 43 one limitations agglomerative methods errors may introduced initial merging decisions especially cases large number equally good merging alternatives cluster without highlevel view overall clustering solution hard agglomerative methods make right decision cases since errors carried may multiplied agglomeration progresses resulting hierarchical trees suffer early stage errors observation also supported experimental results constrained agglomerative algorithms see case constrain space agglomeration decisions made even small number intermediate clusters early stage errors eliminated result constrained agglomerative algorithms improved hierarchical solutions obtained agglomerative methods alone since agglomerative methods better job grouping together documents form small reasonably cohesive clusters partitional methods resulting hierarchical solutions constrained agglomerative methods also better partitional methods alone many cases another surprising observation experimental results 1 upgma behave differently recall section 41 upgma method selects merge pair clusters highest average pairwise similarity hence extent via agglomeration process tries maximize average pairwise similarity documents discovered clusters hand 1 method tries find clustering solution maximizes sum average pairwise similarity documents cluster weighted size different clusters thus 1 considered criterion function upgma tries optimize however experimental results showed 1 performed significantly worse upgma looking fscore values individual class found classes 1 produce clusters similar quality upgma however 1 performed poorly large classes classes 1 prefers first merge loose subcluster different class merges tight subcluster class happens even subcluster class higher cross similarity subcluster different class observation explained fact 1 tends merge loose clusters first shown rest section definitions difference 1 upgma 1 takes account cross similarities well internal similarities clusters merged together let j two candidate clusters size n n j respectively also let j average pairwise similarity documents j respectively ie average cross similarity documents documents j ie merging decisions based j hand 1 merge pair clusters optimizes overall objective functions change overall objective function merging two clusters j obtain cluster r given equation 13 see smaller j values result greater 1 values makes looser clusters easier merged first example consider three clusters 1 2 3 2 tight ie 2 high class 1 whereas 3 loose ie 3 low different class suppose 2 3 similar size means value 1 determined mainly 2 possible 2 13 1 3 greater 2 even 2 closer 1 3 ie 12 13 result two classes close different tightness 1 may merge subclusters class together early stages fail form proper nodes resulting hierarchical tree corresponding two classes 7 concluding remarks paper experimentally evaluated nine agglomerative algorithms six partitional algorithms obtain hierarchical clustering solutions document datasets also introduced new class agglomerative algorithms constraining agglomeration process using clusters obtained partitional algorithms experimental results showed partitional methods produced better hierarchical solutions agglomerative methods constrained agglomerative methods improved clustering solutions obtained agglomerative partitional methods alone results suggest poor performance agglomerative methods may attributed merging errors make early stages eliminated extent introducing partitional constrains r merits building categorization systems supervised clustering document categorization query generation world wide web using webace principal direction divisive partitioning baysian classification autoclass theory results scattergather clusterbased approach browsing large document collections concept decomposition large sparse text data using clustering spectral minmax cut graph partitioning data clustering pattern classification cure efficient clustering algorithm large databases rock robust clustering algorithm categorical attributes webace web agent document categorization exploartion hypergraph based clustering highdimensional data sets summary results spatial clustering methods data mining survey data clustering review algorithms clustering data concept indexing fast dimensionality reduction algorithm applications document retrieval chameleon hierarchical clustering algorithm using dynamic modeling fast effective text mining using lineartime document clustering methods classification analysis multivariate observations web page categorization feature selection using association rule principal component clustering efficient effective clustering method spatial data mining algorithm suffix stripping automatic text processing transformation numerical taxonomy comparison document clustering techniques scalable approach balanced pattern recognition criterion functions document clustering experiments analysis tr algorithms clustering data automatic text processing transformation analysis retrieval information computer scattergather clusterbased approach browsing large document collections bayesian classification autoclass webace fast effective text mining using lineartime document clustering merits building categorization systems supervised clustering partitioningbased clustering web document categorization document categorization query generation world wide web using webace principal direction divisive partitioning chameleon scalable approach balanced highdimensional clustering marketbaskets efficient effective clustering methods spatial data mining pattern classification 2nd edition ctr mohammed attik shadi al shehabi jeancharles lamirel clustering quality measures data samples multiple labels proceedings 24th iasted international conference database applications p5865 february 1315 2006 innsbruck austria mihai surdeanu jordi turmo alicia ageno hybrid unsupervised approach document clustering proceeding eleventh acm sigkdd international conference knowledge discovery data mining august 2124 2005 chicago illinois usa rebecca cathey ling nazli goharian david grossman misuse detection information retrieval systems proceedings twelfth international conference information knowledge management november 0308 2003 new orleans la usa nachiketa sahoo jamie callan ramayya krishnan george duncan rema padman incremental hierarchical clustering text documents proceedings 15th acm international conference information knowledge management november 0611 2006 arlington virginia usa ying lai wai gen yee clustering highdimensional data using efficient effective data space reduction proceedings 14th acm international conference information knowledge management october 31november 05 2005 bremen germany tianming hu ying yu jinzhi xiong sam yuan sung maximum likelihood combination multiple clusterings pattern recognition letters v27 n13 p14571464 1 october 2006 jin soung yoo shashi shekhar john smith julius p kumquat partial join approach mining colocation patterns proceedings 12th annual acm international workshop geographic information systems november 1213 2004 washington dc usa amol ghoting gregory buehrer srinivasan parthasarathy daehyun kim anthony nguyen yenkuang chen pradeep dubey characterization data mining algorithms modern processor proceedings 1st international workshop data management new hardware june 1212 2005 baltimore maryland dina demnerfushman jimmy lin answer extraction semantic clustering extractive summarization clinical question answering proceedings 21st international conference computational linguistics 44th annual meeting acl p841848 july 1718 2006 sydney australia krishna kummamuru rohit lotlikar shourya roy karan singal raghu krishnapuram hierarchical monothetic document clustering algorithm summarization browsing search results proceedings 13th international conference world wide web may 1720 2004 new york ny usa tianming hu sam yuan sung consensus clustering intelligent data analysis v9 n6 p551565 november 2005 gne erkan language modelbased document clustering using random walks proceedings main conference human language technology conference north american chapter association computational linguistics p479486 june 0409 2006 new york new york gautam pant kostas tsioutsiouliklis judy johnson c lee giles panorama extending digital libraries topical crawlers proceedings 4th acmieeecs joint conference digital libraries june 0711 2004 tuscon az usa david cheng santosh vempala ravi kannan grant wang divideandmerge methodology clustering proceedings twentyfourth acm sigmodsigactsigart symposium principles database systems june 1315 2005 baltimore maryland richi nayak wina iryadi xml schema clustering semantic hierarchical similarity measures knowledgebased systems v20 n4 p336349 may 2007 david cheng ravi kannan santosh vempala grant wang divideandmerge methodology clustering acm transactions database systems tods v31 n4 p14991525 december 2006 jack g conrad khalid alkofahi ying zhao george karypis effective document clustering large heterogeneous law firm collections proceedings 10th international conference artificial intelligence law june 0611 2005 bologna italy ying zhao george karypis usama fayyad hierarchical clustering algorithms document datasets data mining knowledge discovery v10 n2 p141168 march 2005 rebecca j cathey eric c jensen steven beitzel ophir frieder david grossman exploiting parallelism support scalable hierarchical clustering journal american society information science technology v58 n8 p12071221 june 2007