generic grouping algorithm quantitative analysis abstractthis paper presents generic method perceptual grouping quality grouping method fairly general may used grouping various types data features incorporate different grouping cues operating feature sets different sizes proposed method divided two parts constructing graph representation available perceptual grouping evidence finding best partition graph groups first stage includes cue enhancement procedure integrates information available multifeature cues reliable bifeature cues stages implemented using known statistical tools walds sprt algorithm maximum likelihood criterion accompanying theoretical analysis grouping criterion quantifies intuitive expectations predicts expected grouping quality increases cue reliability also shows investing computational effort grouping algorithm leads better grouping results analysis quantifies grouping power maximum likelihood criterion independent grouping domain best knowledge analysis grouping process given first time three grouping algorithms three different domains synthesized instances generic method demonstrate applicability generality grouping method b introduction work proposes generic algorithm perceptual grouping paper presents new approach focuses analyzing relation information available grouping process corresponding grouping quality proposed generic algorithm may serve generate domain specific grouping algorithms different domains implement test three however analysis domain independent thus applies specific cases visual processes deal analyzing images extracting information one reason makes processes hard subsets data items contain useful information others relevant grouping processes rearrange given data eliminating irrelevant data items sorting rest groups corresponding certain object indispensable computer vision wt83 gri90 gestalt psychologists already noticed humans use basic properties may called grouping cues low85 recognize existence certain structures scene extract image elements associated structures even recognized meaningful object wer50 wt83 low85 gor89 field computer vision witkin tenenbaum wt83 suggested grouping processes part processing levels indeed grouping used many levels starting lowlevel processes smoothness based figureground discrimination su88 hh93 motion based aw94 jc94 sha94 grouping may considered midlevel processes highlevel vision processes object recognition hms94 zmfl95 proposed method separates two components grouping method grouping cues used grouping mechanism combines partition data set like grouping methods grouping mechanism used formally defined maximization consistency function group assignments given data maximization usually done various methods including dynamic programming su90 relaxation labeling pz89 tja92 simulated annealing hh93 graph clustering wl93 may also done hierarchically hms94 mn89 dr92 proposed algorithm also maximizes function likelihood data available relative grouping decision crucial difference exists however proposed algorithm previous work analysis provide predict grouping performance based reliability data specific grouping process based representing unknown partition groups form special graph vertices observed data elements edgespixels etc arcs contain grouping information estimated cues others eg hh93 su88 wl93 used graphs grouping algorithms use differently grouping task divided two parts constructing graph applying geometric knowledge data finding best partition graph groups stages implemented using known statistical tools walds sprt algorithm maximum likelihood criterion grouping cues building blocks grouping process shall treated source information available task used first stage algorithm constructing graph general domain specific rely assumed properties soughtfor groups choice essentially made taste intuition although rigorous statistical properties sometimes taken account low85 jac88 cle91 crh93 wellstudied task grouping edge points lying smooth boundary good example variety perceptual grouping cues typically used cues collinearity cocircularity sau92 curvature length su90 pz89 proximity combinations dr92 hh93 gm92 domains different assumptions cues used eg motion based cues jc94 aw94 symmetry hms94 3d symmetrybased invariance zmfl95 although good cues essential successful grouping finding aim instead consider cues given focus quantifying reliability relation expected grouping quality model cues random variables quantify reliability using properties corresponding distribution moreover suggest general method denoted cue enhancement improving reliability cue show tradeoff computational efforts achieved reliability enhanced cue although many grouping methods already suggested tested seems solid theoretical background established far performance grouping algorithms assessed implementing algorithm testing small number simulated real examples visually evaluating results methodology shows grouping methods perform well examples tested indeed succeed partitioning image elements seemingly correct subsets allow us however predict performance algorithms images compare algorithms whose assessment carried using varied examples proposed quantitative analysis expected grouping performance provides relations quality available data computational effort invested grouping performance quantified several measures grouping algorithm propose generic domain specific grouping algorithms may synthesized instances inserting appropriate cue topology specification form graph analysis applies generic abstract algorithm may useful predicting results specific domains main new contributions paper fairly general approach grouping applicable several domains tested three previous algorithms domain specific b quantification expected quality grouping results best knowledge analysis done c cue enhancement procedure capable significantly improve reliability many existing grouping cues rest paper organized follows starts formulation grouping task graph clustering problem graphbased grouping algorithm follows theoretical analysis described section 4 section 5 concerned cue enhancement procedure inluding short review walds sprt algorithm tested approach also experimentally providing three instances generic algorithm three domains comparisons theoretical predictions open questions research directions considered discussion 2 grouping task graph representation 21 grouping task set data elements data set may consist coordinates grey levels pixels image boundary points image etc naturally divided several groups disjoint subsets data elements group belong object lie smooth curve associated manner context grouping task data set given partition unknown inferred indirect information given form grouping cues often elements last l groups satisfy description elements first group 0 considered nonimportant background also mention according another grouping concept hypothesized groups necessarily disjoint consider different task believe least tools developed useful analysing 22 grouping cues grouping cues building blocks grouping process shall treated source information available task grouping cues domaindependent may regarded scalar functions ca defined subsets ae data feature set cue functions discriminative example high data features subset belong object low preferably also invariant change viewing transformation robust noise low85 grouping cues considered literature functions defined data subsets including two data elements exceptions convexity cue jac88 ushape cue mn89 later section 5 consider multifeature cues defined data subsets including three data features show integrate evidence available reliable bifeature cues stage however consider bifeature cues may either cues used common grouping processes result cue enhancement process described later following main goal work provide general domain independent framework grouping processes would like predict grouping performance relying detailed domaindependent knowledge cue using measure reliability reliability measure may defined considering cue function random variable distribution depends features set group binary cues dependency simply quantified two error prob abilities p miss probability cue ca indicates data features belong group fact p fa probability cue function indicates features belong group fact false alarm ideal cue general necessarily binary cue reliability quantified average log likelihood ratio cue characterization sometimes calculated using analytical models eg low85 always approximated using montecarlo experimentations jac88 23 representing groups cues using graphs approach grouping process based representing unknown partition groups data available cues using graphs nodes graphs observed data elements arcs may take different meanings unknown partition determined represented target graph composed several disconnected complete subgraphs cliques every clique represents different object group connection arcs nodes belong different cliques graph characterization called clique graph class graphs denoted g c nodes graph available grouping algorithm arcs contain grouping information hidden directly observable knowing g belongs class clique graphs g c grouping algorithm provide hypothesis graph g close possible g cue information described two graphs underlying graph g specifies arcs feature pairs evaluated group cue function available grouping algorithm second graph denoted measured graph gm specifies information provided cues arc belongs gm iff belongs g u result cue function indicates feature pair belongs group underlying graph specified designer depending domain computational effort limitations measured graph result cue evaluation process part grouping process 3 generic grouping algorithm generic grouping algorithm described section consists two main stages cue evaluation many feature pairs maximum likelihood graph partitioning two stages general depend particular domain grouping done except obvious choice domaindependent cue associated decisions made process 31 decisions made designer first thing choose grouping cue naturally depends domain assumed characterization soughtfor groups performance analysis described latter provide quantitative means choose alternatives cues may differ example tradeoff false alarm miss errors principle feature pairs corresponding complete underlying graph v e c v evaluated hypothesis graph g g u g g g set groups grouping cue underlying create graph data features set grouping graph clustering underlying graph measured graph graph clustering max likelihood decide edge desired target graph figure 1 proposed grouping process image set data features edgels every one represented node graph first step decide cue set featurepairs evaluated using cue set featurepairs specified arcs underlying graph g second step use grouping cues decide every feature pair g u data features belong group decisions represented measured graph every arc corresponds positive decision hence em e u known reliability decisions used last step find maximum likelihood partitioning graph represented hypothesized clique graph g h main issue considered paper relation hypothesis g h ground truth target graph g unknown cues meaningful however near adjacent data elements adequate evaluating every feature pair therefore cue evaluation restricted subset feature pairs specified spatial extent available cue example order detect long smooth curves using cocircularity proximity cues may test close data feature pairs hand testing global cues like affine motion feature pairs may tested contribute useful information another consideration affects choice underlying graph reliability grouping process computational effort invested shall see reliability increases density graph computational effort compromise made paper dont investigate optimal decisions stage assume cue associated adequate topology either given chosen intuitively 32 first stage evaluate grouping cues first stage grouping process feature pairs corresponding arcs g considered one arc time cue function used decide whether two data features belong group arc corresponding unobservable graph g simple decision may obtained binary cue sophisticated reliable process rely multiple evidence based features done cue enhancement procedure section 5 positive decisions represented measured graph decisions made em estimate projection target graph g underlying graph g u measured graph carries information accumulated first stage second one note also possible postpone decisions mark every arc underlying graph likelihood corresponding pair group maximum likelihood partition stage proceed similarly approach may yield better results due larger amount information carried second stage requires actual nonbinary cue distributions given rarely case considered 33 second stage maximum likelihood partition graph recall every decision made first stage modeled binary random variable statistics depends whether two data features belong group whether therefore likelihood decision indeed correct depends true unknown grouping therefore decisions made first stage represented measured graph gm specify likelihood every partition graph subgraphs choosing partition clique graph maximizes likelihood yields approximation required unknown target graph g one clique graphs context paper cue decisions assumed independent subject two types errors specified uniformly two error probabilities ffl miss error probability pair ffl miss ffl fa identical cue probability pair p miss common direct use bifeature cues equal error probability cue enhancement process see section 5 usually much better making probabilities nonuniform thus associating every arc g u individual pair error probabil ities may accurate model requires much accurate knowledge error mechanism likelihood measurement graph gm every candidate hypothesis e 2 g c given lfejeg 2 likelihood edge ffl miss e 2 enem propose use maximum likelihood principle hypothesize likely necessarily unique graph g2gc lfgm jgg 4 maximum likelihood criteria defined eq 4 specifies grouping result g h constructive algorithm moreover class optimization problems known high computational complexity exponential worst case therefore address theoretical aspect practical side separately theoretical point view shall assume hypothesis maximizes likelihood may found address main question relation result g h unknown target graph g question interesting concerned predicting grouping performance show two graphs close sense means algorithms use maximum likelihood principle predictable expected behavior even cant know g grouping hypothesis g h produces close enough true partitioning question considered next section practical point view one ask optimization problem solved reasonable time people use simulated annealing annealing methods solve similar problems hh93 others use heuristic algorithms vos92 developed heuristic algorithm based finding seeds groups form almost clique gm random graphs theory pal85 implies cliques certain size likely found inside object unlikely found elsewhere graph seeds found highest entries square adjacency matrix gm seeds iteratively modified making small changes moving one element one group another merging two groups etc using greedy policy local maximum likelihood function obtained experiments described section 6 algorithm performs nicely details found al95 4 analysis grouping quality section quantifies aspects similarity unknown scene grouping represented g hypothesized grouping suggested algorithm represented g h shall see dissimilarity depends error probabilities individual arcs ffl miss ffl fa connectivity density g u first result demonstrates good solutions rejected lfgm jg g2gc provided ffl miss proof every clique graph gv e 2 g lfgm jgg lfgm jg g ffl miss e2e u e ne arcs e u exist none two sets e e affect ratio therefore counted 2 borrowing terminology parameter estimation claim shows maximum likelihood partition consistent estimator arbitrarily reliable labeling underlying graph associated good cues leads correct decision assume ffl miss consistency ensured realistic case hypotheses regarding arcs underlying graphs may wrong shall show grouping performance degrades gracefully quality reliability cues performance may predicted general grouping performance good groups densely connected within underlying graph expected worse loosely connected groups example node data feature connected group one edge underlying graph may separated group hypothesized partition probability ffl miss may quite high turn proving fundamental claim results rely necessary condition satisfied partition selected according maximum likelihood principle consider two nodesdisjoint subsets graph denote cut jv g let l u denote cut width relative underlying graph similarly let l denote cut width relative measurement graph necessary condition let g maximum likelihood hypothesis satisfying eq 4 let logffl miss 1 disjoint partition group v 2 two groups l proof proof technique similar claim 1 proving first part consider likelihood ratio two hypotheses one g h denoted g h constructed g h separating v two different groups v 0 ffl miss l likelihood ratio nondecreasing function l figure 2 cut involved splitting group two proof claim 2 larger 1 l ffl u therefore claim satisfied g h likely g h contradicts assumption 4 holds second part claim proved similar mannerqualitatively claim shows maximum likelihood grouping must satisfy local conditions many pairs feature subsets implies grouping error either form adding alien data feature group deleting member requires single false alarm single miss provided connectivity underlying graph high enough addition error example merging group alien node v requires substantial fraction edges jv fv g none e included em requires many false alarms parameter ff specifying fraction cut edges required merge two subsets reflects expected error false alarm probability equal miss probability false alarm probability higher ff condition used show choosing sufficiently dense underlying graph significantly improve grouping performance shall consider two cases complete underlying graph locally connected underlying graph 41 complete underlying graphs complete underlying graph connects every data feature others provides maximal information graph clustering stage therefore may lead excellent grouping accuracy hand mentioned useful global grouping cues straight line consistent affine motion model etc many types grouping inaccuracies following claims consider true data feature group probability maximum likelihood process hypothesize group v containing k nodes particular additional node ikmin proof use claim 2 note l merging subsets requires least ffl u edges connecting included em event happens binomial distribution 2 true group maximum likelihood hypothesized group containing least k nodes probability v contains k 0 nodes alien gammaaliens proof use claim 2 v find probability particular data subset v merges take worst case approach sum probabilities subsets certain size j sizes higher k 0 2 k10203040 figure 3 two predictions analysis left kconnected curvelike group eg smooth curve likely brake number subgroups graph shows upper bound expected number subgroups versus minimal cut size group k eq 13 group size length 400 elements ffl miss 014 ffl typical values images like figure 8 shows increasing connectivity quickly reduces false division type groups right upper bound probability adding k 0 alien data features group size k using complete underlying graph claim 4 error probability negligible true group maximum likelihood hypothesized group containing maximal number data features probability v contains js data features gammadeletions ikmin miss proof particular deleted subset 0 ae use claim 2 note l split requires l ffl u event happens binomial distribution find probability subset size k 0 deleted sum subsets ignoring dependency events decrease probability 2 claims 345 simply state original group big enough miss false alarm probabilities small enough likely maximum likelihood partition include one group object containing aliens crude bound plotted figure 3right shows example even substantial cue errors probability hypothesizing highly mixed subsets small provided group large enough k 15 even practical performance measure calculated using approximations expected number addition deletion errors efk delete ikmin miss ikmin k group size k dffke experimental results two grouping error types given figure 9c figure 9d major difficulty see use complete underlying graph apply cues especially concerns cues meaningful locally cocircularity smooth curve detection therefore another option locally dense underlying graph also proposed 42 locally dense underlying graphs intuitive choice underlying graph less dense complete graph connect every data feature data features neighborhood either closest k data features data features certain radius specifying graph important keep substantial connectivity data features objects accidental deletion less likely connectivity demand quantified requiring projection every group underlying graph kconnected k gamma 1 nodes eliminated projected subgraph remains connected nice property kconnected graphs every cut contains least edges therefore deletion node requires least ffk miss errors alien data features either densely connected group implying incorrect addition group prevented high confidence connected enough considered candidates addition significant change case complete graph ffk miss errors cause deletion subgroup containing one data feature fact demonstrates relative weakness locally connected underlying graph aware weakness choose characterize grouping performance another measure expected number large subgroups group decomposes consider particular cut size k projection object underlying graph probability object divided cut two parts exactly divide ikmin miss suppose estimate number potential cuts denote number n cut expected number group separation simply n cut p divide kgammacut fortunately estimate may done interesting case curve like groups let kconnected curvelike group data features ordered along curve separation curve significant large parts associated cuts separate group consecutive curve points another group consecutive curve points number cuts n therefore guarantee number arcs every one cuts less k expected number parts curve decomposes higher divide ikmin miss number plotted figure generally decrease increasing cut size k due nonconstant nonmonotonic nature ratio kmin ff strictly monotonic locally connected underlying graphs used 2nd demonstrated instance algorithm considers grouping curve like groups based proximity smoothness 5 cue enhancement performance grouping algorithm depends much reliability cues available many situations reliability predetermined grouping algorithm designer prefer reliable cues available variety section however shows reliability grouping cue significantly improved using statistical evidence accumulation techniques method restricted grouping algorithm used also grouping algorithms two three domain specific grouping algorithm implement examples colinearity smoothness use procedure 51 cue enhancement procedure overview cue enhancement procedure considers one pair data features time tries use data features order estimate consistency pair shall say subset data features consistent subset true group idea behind following process evidence accumulation random data subset contains data pair may consistent necessarily e consistent therefore multifeature cues operating feature subset e 2 carries statistical information consistency e although bifeature cues easier calculate straightforward use cues test larger data subsets several significant advantages several useful cues simply defined one pair elements considered eg convexity bifeature cues usually corresponding multifeature cues associated improved reliability observe example accidental collinearity less likely points considered miss probability decrease slightly case generally reliability shapebased multifeature cue consistent instance particular object clearly increases number data features gh91 lin94 algorithm conceptually simple every data pair underlying graph algorithm draws several random data subsets 1 contain pair e corresponding multifeature cues ex tracted cue values deterministic functions subsets 1 may also considered instances random variable statistics depend data pair e particular consistency number random data subsets associated cues required conclusive reliable decision consistency e determined adaptively efficiently wellknown method statistical evidence integration walds sprt test 511 walds sprt algorithm application cue enhancement consider random variable x distribution depends unknown binary parameter takes value 0 1 every instance random variable carries statistical information parameter integrating information corresponding sequence random variable instances eventually lead reliable inference efficient accurate procedure integration statistical evidence sequential probability ratio test sprt suggested wald wal52 procedure quantifies evidence obtained trial log likelihood ratio function probability functions two different populations x value assigned random variable trial log likelihood ratio high value random variable x likely one hypothesis 1 likely 0 negative low situation reversed probabilities seeing x hypotheses close x carries little information hx 0 several trials taken log likelihood function composite event considered however trials independent composite log likelihood function equal sum individual log likelihood functions oe sum oe n serves statistics decision made walds procedure specifies two limits upper lower cumulative log likelihood function crosses one limits decision made otherwise trials carried formally denote decision made procedure let allowed probabilities decision error algorithm given simply iterative rule else test another subset upper lower limits depend allowed probability error defined eq 1 depend distribution random variable x calculate b using practical approximation proposed wald wal52 accurate ffl miss ffl fa small basic sprt algorithm terminates probability one optimal sense provides minimum expected number tests necessary obtain required decision error wal52 expected number tests given conditional expected amounts evidence single average case optimality worst case number trials required sprt algorithm bounded deal disadvantage modified truncated sprt wal52 uses predefined upper bound n 0 number tests used set n 0 times larger efng context cue enhancement procedure cue value regarded random variable apart specifying desired reliability ffl miss ffl fa using equation 16 calculate two thresholds b one must supply two distributions consistent inconsistent feature pairs loglikelihood ratio determined distribution evaluated carefully distributions cues taken consistent inconsistent populations denoted respectively p con ca p incon ca usually quite different important however observe even feature pair u v consistent random set including may therefore distributions modified follows random set containing feature pair fu vg ae additional randomly selected data features v consistent probability therefore modified cue distributions conditioned relative consistency first two feature unfortunately distributions similar difficult distinguish see figure 9a pair distributions considered experiments restricting binary cues distribution specified probabilities conditional distributions become log likelihood ratio th randomlyselected subset becomes log p log 1gammap sprt based cue enhancement procedure summarized figure 511 2 randomly choose data features x 3 calculate 4 update evidence accumulator every feature pair u v underlying 1 set evidence accumulator oe trials counter n 5 oe n n 0 oe 0 output u v consistent oe b n n 0 oe 0 output u v inconsistent else repeat 25 figure 4 cue enhancement algorithm success cue enhancement procedure relies validity statistical model particular following two assumptions assumption statistics cue values evaluated data subsets containing consistent inconsistent arc approximately assumption b cues extracted two random subsets including feature independent identically distributed random variables assumptions satisfied 6 cue enhancement procedure described identify consistency feature pair within specified error tolerance irrespective reliability basic cue provided assumptions b hold surprising conclusion seems contradict intuition according arbitrarily low identification errors impossible amount data image finite indeed arbitrarily high performance possible requires large number trials leading contradiction independence assumption therefore reliability basic cue important leads lower number trials computationally advantageous important validity statistical independence assumption indeed experiments show sprt significantly improves cue reliability achievable error rate arbitrarily small see experimental results next section constant specified reliability ffl miss ffl fa expected running time cue enhancement procedure constant total running time evaluating arcs underlying graph g u therefore linear number arcs emphasize enhancement method completely general may use cue satisfies benign assumptions stated section relies distributions cues calculated involve certain technicalities described full version al94 6 simulation experimentation section presents three different grouping applications implemented different domains instances generic grouping algorithm described best knowledge first time generic grouping algorithm used multiple domains implementation domain data features grouping cue different grouping mechanism computer program used see table 1 aim examples show useful grouping algorithms may obtained instances generic approach examine performance predictions experimental results expect general algorithm perform good domain specific algorithm tailored domain still tested domains got grouping results comparable obtained existing domain specific methods remarkable except choice cues associated underlying graph determined extent process depend domain moreover although analysis may help selecting different available cues focus choosing best cues testing approach using reasonable cues fore expect even better performance possible optimizing cues corresponding underlying graphs see results examples al94 al95 61 example 1 grouping points colinearity cues given set points r 2 algorithm partition data colinear groups one background set remove doubt intend propose grouping approach efficient even reasonable method detecting colinear clusters several common solutions eg hough transform ransac exist particular task table 1 three instances generic grouping algorithm 1st example 2nd example 3rd example data elements points r 2 edgels patches affine optical flow grouping cues colinearity cocircularity consistency proximity affine motion cues extent global local global enhanced cue subsets 3 points subsets 3 underlying graph complete graph locally connected graph complete graph grouping mechanism maximum likelihood graph clustering program chosen example characteristic example grouping tasks associated globally valid cues complete underlying graphs moreover provides convenient way measuring grouping performance quantification prediction main interest grouping cue defined data subsets containing k 2 data features second eigenvalue associated covariance matrix clearly eigenvalue small data subset closer linear see eg gm92 cue global hence underlying graph complete graph binarize cue simply check value lower threshold consider synthetic random images containing randomly drawn points eg figure 5a points drawn according distribution specified collection arbitrary straight lines objects associated given data additional uniformly distributed aliens data source easy automatically create many data sets known noise distributions grouping ground truth exception alien points located close objects typical grouping result shown explained figure 5 used colinearity example comprehensivly test performance grouping algorithm pre dictions first results show performance cue function cue enhancement procedure examine cue function estimate two cuevalue distributions differ consistency included pair fu vg ae done montecarlo process randomlyselected featuretriplets two distributions defined eq 18 tend quite similar shown figure 9a order make binary cue proceed selecting threshold binary cue decision specified threshold determines different binarycue errors p miss p fa one nondecreasing function nonincreasing function compromise done values p miss p fa effects efficiency sprt algorithm measured average number subsets efng needed reach specified error rate using eq 17 p 0 ca p 1 ca figure 9a one draw efng terms selected threshold shown figure 9b optimal threshold found cuevalue global minimum note selection effects resulted grouping quality computational time needed sprt reach desired error enhanced cue ffl miss ffl fa threshold also optimal sense provides maximum information evaluated featuretriplet measured average number subsets needed sprt efng given labels figure 9e 100 different prespecified ffl miss ffl fa values remarkably agrees predicted average eq 17 shown curves graph also shown enhanced cue reliability exceeds 95 ie ffl miss 5 ffl fa 5 even simple cue used low discrimination power next results show overall grouping quality regardless choice ffl miss 5 lines always detected 5 largest groups experiments selection affects however overall grouping quality measured counting addition errors deletion errors shown figure 9c 9d respectively note deletion error low expected addition error higher expected reason discrepancy alien data features close one lines erroneously added tradeoff grouping quality computational time cue enhancement procedure obtained three efng increases figure e errors decrease figures 9c 9d 62 example 2 grouping edgels smoothness starting image edgels data feature edge location gradient direction algorithm group edgels lie smooth curve useful grouping task considered many researchers see eg gm92 zmfl95 hh93 su90 crh93 crude cocircularity cue function operating edgel triples used calculated maximal angular difference gradient direction corresponding normal direction circular arc passing three points underlying graph locally connected constructed connecting every edgel k 2 10 50 nearest edgels k constant test procedure synthetic real images results good cases see figure 6 figure 7 synthetic images created detecting edges piecewise constant images contain grey level smooth blobs eg figure 6a synthetic example found perimeter two big blobs splits 34 groups see figure 6e happens places connectivity g u low minimal connectivity assumption fails split probability increases see figure 3 63 example 3 segmentation optical flow using affine mo tion third grouping algorithm based common motion data features pixel blocks grouped together motion obeys rule given optical flow consistent one affine motion model jc94 aw94 technically every pixel block represented location six parameters local affine motion model calculated using least squares grouping cue defined pairs blocks value sum optical flow errors block calculating using affine model block cue global hence complete underlying graph used cue enhancement used cue reliable typical error probabilities ffl miss 035 ffl 02 still results comparable obtained domain specific algorithm aw94 final clustering result shown figure 8f obtained postprocessing stage obtained grouping used calculate affine motion model every group used classify individual pixels image groups method used aw94 original image set points b associated data features original image c underlying graph gu complete graph pixel gray level indicates number arcs passing thru measured graph gm pixel gray level indicates number arcs passing thru one detected groups points fall wrong group f detected groups figure 5 example 1 grouping colinear points example images used experiments image associated five lines contains points vicinity 150 uniformally distributed additional data features grouping result nearoptimal surprise predictions demonstrates power complete underlying graph quantitative results experiments shown figure 9 original image b associated data features edgels c underlying graph g locally connected 40 nearest nbrs pixel gray level indicates number arcs passing thru brighter areas correspond denser regions g u measured graph gm pixel gray level indicates number arcs passing thru note bright groups measured graph longer correspond local density gu smoothness byproduct also serve saliency map one 14 detected groups f 14 detected groups figure example 21 grouping smooth curves synthetic image edge detection gradient calculated image 50 edge pixels randomly removed 10 background pixels added aliens uniformly distributed gradient directions total number edgels 5000 110000 arcs g u original image brain image b edge detection associated data features edgels c underlying graph g locally connected 40 nearest nbrs pixel gray level indicates number arcs passing thru measured graph gm pixel gray level indicates number arcs passing thru e five largest detected groups f detected groups superimposed original image figure 7 example 22 grouping smooth curves brain image underlying graph g u made 10400 edgels 230000 arcs processing time 10 minutes original image flowers sequence b associated data features optical flow blocks c underlying graph gu complete graph pixel gray level indicates number arcs passing thru measured graph gm pixel gray level indicates number arcs passing thru low number edges clouds area indicates optical flow area match affine motion model e 3 resulted groups different gray level black regions either eliminated high error affine model eg tree border grouped groups f postprocessing stage obtained grouping used calculate affine motion model every group used classify individual pixels image groups black pixels classified result shows even groups e visually nice still capture correct motion clustering image figure 8 example 3 image segmentation regions consistent affine motion parameters underlying graph complete graph 600 nodes 180000 arcs runtime 5 minutes cue value probability densety distribution colinearity cue values subsets include consistent feature casolid subsets include inconsistent feature pair p 1 ca dashed although two similar populations distinguished less 5 error shown e cue threshold b expected number trials needed cue enhancement procedure function selected cue threshold optimal cue threshold correspond minima curve experimental results shown efa20101100111111001100110210222111101111221121422411 c efa42234345556464546577866788981511111312141413816192011111916272115192820 emiss efa e tradeoff enhanced cue reliability computational effort invested clearly demonstrated figure showing error probabilities ffl miss ffl fa associated enhanced cue experimental average number trials en points labels solid lines show predicted error probabilities labels every point represents complete grouping process labeled resulting number deleted points deletion error 5 lines c added points addition error 5 linesd figure 9 quantitative results comparison analysis prediction experimental results example 1 grouping results tends reach nearperfect grouping using cue enhancement procedure goal work provide theoretical framework generic algorithm would apply various domains would predictable performance proposed generic grouping algorithm relies established statistical techniques sequential testing maximum likelihood maximum likelihood principle close previous grouping approaches like use densities evaluating evidence certain cues jac88 cumulative pairwise interaction score used figurefromground discrimination hh93 paper distinctive previous approaches provides first time analysis use principles relates expected grouping quality cue reliability connectivity used cases computational effort invested limit theoretical study three grouping appli cations every one based different cue implemented instances generic grouping algorithm demonstrate usefulness although made argument judging merits vision algorithm visually comparing action examples would like indicate results similar obtained domain specific methods eg su90 hh93 smoothness based grouping aw94 motion based grouping note gm may also used create saliency map saliency every data element degree gm saliency map eg figure 7d8d visually comparable proposed works eg shashua ullman su88 guy medioni gm92 suitablity figureground discrimination studied interesting conclusions arise analysis experimentation grouping algorithms apparent higher connectivity provided either complete underlying graph high degree locallyconnected graph enhance grouping quality fore selection cues grouping algorithm based maximizing reliability also extent cue extent determines connectivity valid underlying graph words amount information may extracted cue another consideration cue enhancement possibility cue satisfies independent random variable assumption reliable cue may obtained relatively low computational effort analysis computational complexity complete although requirements cue enhancement stage clearly stated even function quality required complexity results second stage finding maximum likelihood partition task known difficult simulated annealing used solve similar problems hh93 used heuristics based results random graph theory greedy search turned work surprisingly good design grouping algorithm one may either invest computational effort enhancing quality relatively small number cues use larger number unreliable cues merge higher connectivity underlying graph framework proposed paper makes choice explicit providing cue enhancement procedure independent maximum likelihood graph clustering making optimal choice interesting open question consider another research direction use methodology context different grouping notion different partitioning hypothesized groups necessarily disjoint acknowledgements would like thank john wang providing us flowers garden optical flow data r construction analysis generic grouping algorithm representing moving images layers bayesian mulitple hypothesis approach edge grouping contour segmentation computing curvilinear structure tokenbased grouping fast spreading metric based approximate graph partitioning algorithms algorithm finding best matches logarithmic expected time grimson daniel p perceptual grouping using global saliency enhancing operators theories visual perception extraction groups recognition use grouping visual object recognition finding structurally consistent motion correspondences amount data required reliable recognition perceptual organization visual recognition using perceptual organization extract 3d structures graphical evolution trace interface applications spatial data structures labeling curvilinear structure across scales token grouping affine analysis image sequences structural saliency detection globally salient structures using locally connected network grouping contours iterated pairing network supervised classification early perceptual structure dot patterns relational matching sequencial analysis laws organization perceptual forms optimal graph theoretic approach data clus tering theory application image segmentation role structure vision tr ctr shyjan mahamud lance r williams karvel k thornber kanglin xu segmentation multiple salient closed contours real images ieee transactions pattern analysis machine intelligence v25 n4 p433444 april p kammerer r glantz segmentation brush strokes saliency preserving dual graph contraction pattern recognition letters v24 n8 p10431050 may jens keuchel christoph schnrr christian schellewald daniel cremers binary partitioning perceptual grouping restoration semidefinite programming ieee transactions pattern analysis machine intelligence v25 n11 p13641379 november alexander berengolts michael lindenbaum performance connected components grouping international journal computer vision v41 n3 p195216 februarymarch 2001 jacob feldman perceptual grouping selection logically minimal model international journal computer vision v55 n1 p525 october anthony hoogs roderic collins robert kaucic joseph mundy common set perceptual observables grouping figureground discrimination texture classification ieee transactions pattern analysis machine intelligence v25 n4 p458474 april song wang joachim stahl adam bailey michael dropps global detection salient convex boundaries international journal computer vision v71 n3 p337359 march 2007 engbers arnold w smeulders design considerations generic grouping vision ieee transactions pattern analysis machine intelligence v25 n4 p445457 april song wang toshiro kubota jeffrey mark siskind jun wang salient closed boundary extraction ratio contour ieee transactions pattern analysis machine intelligence v27 n4 p546561 april 2005 bernd fischer joachim buhmann pathbased clustering grouping smooth curves texture segmentation ieee transactions pattern analysis machine intelligence v25 n4 p513518 april sudeep sarkar padmanabhan soundararajan supervised learning large perceptual organization graph spectral partitioning learning automata ieee transactions pattern analysis machine intelligence v22 n5 p504525 may 2000 stella x yu jianbo shi segmentation given partial grouping constraints ieee transactions pattern analysis machine intelligence v26 n2 p173183 january 2004 sudeep sarkar daniel majchrzak kishore korimilli perceptual organization based computational model robust segmentation moving objects computer vision image understanding v86 n3 p141170 june 2002