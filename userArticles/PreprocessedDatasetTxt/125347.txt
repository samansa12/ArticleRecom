distributed representations simple recurrent networks grammatical structure paper three problems connectionist account language considered1 nature linguistic representations2 complex structural relationships constituent structure represented3 apparently openended nature language accommodated fixedresource systemusing prediction task simple recurrent network srn trained multiclausal sentences contain multiplyembedded relative clauses principal component analysis hidden unit activation patterns reveals network solves task developing complex distributed representations encode relevant grammatical relations hierarchical constituent structure differences srn state representations traditional pushdown store discussed final section b introduction recent years considerable progress developing connectionist models language work demonstrated ability network models account variety phenomena phonology eg gasser lee 1990 hare 1990 touretzky 1989 touretzky wheeler 1989 morphology eg hare corina cottrell 1989 macwhinney et al 1989 plunkett marchman 1989 rumelhart mcclelland 1986b ryder 1989 spoken word recognition mcclelland elman 1986 written word recognition rumelhart mcclelland 1986 seidenberg mcclelland 1989 speech production dell 1986 stemberger 1985 role assignment kawamoto mcclelland 1986 miikkulainen dyer 1989a st john mcclelland 1989 clear connectionist networks many properties make attractive language processing time remain significant shortcomings current work hardly surprising natural language difficult domain poses difficult challenges paradigm challenges seen positive light test power framework also motivate development new connectionist approaches paper would like focus see three principal challenges successful connectionist account language hat nature linguistic representations ow complex structural relationships constituency ow apparently openended nature language accommodated fixedresource system interestingly problems closely intertwined representation press machine learning one approach addresses first two problems use localist represen tations localist networks nodes assigned discrete interpretations models eg kawamoto mcclelland 1986 st john mcclelland 1988 nodes may represent grammatical roles eg agent theme modifier relations eg subject daughterof may bound nodes represent wordtokens instantiate either spatial assignment kawamoto mcclelland 1986 miikkulainen dyer 1989b concurrent activation st john mcclelland 1989 various techniques eg smolensky press although localist approach many attractions number important drawbacks well first localist dictum one nodeone concept taken together fact networks typically fixed resources seems variance openended nature language nodes preallocated defined roles subject agent order process sentences multiple subjects agents case complex sentences must appropriate number type nodes one know types needed many provide situation becomes even troublesome one interested discourse phenomena generative theories language chomsky 1965 made much unbounded generativity natural language pointed rumelhart mcclelland 1986a reality language productions practice fact finite length number still even one accepts practical limitations noteworthy soft contextsensitive rather hard absolute way localist approach would predictfor instance consider difficulty understanding cat dog mouse saw chased ran away compared planet astronomer university hired saw exploded clearly semantic pragmatic considerations facilitate parsing structures otherwise hard process see also labov 1973 reich dell 1977 schlesinger 1968 stolz 1967 experimental demonstrations point thus although one might anticipate commonly occurring structural relations one would like limits processing soft rather hard way localist approach would second shortcoming use localist representations often underestimate actual richness linguistic structure even basic notion word one might assume straightforward linguistic primitive turns difficult define one might thought dramatic differences terms counts word across languages even within english morphological syntactic processes yield entities wordlike respects eg apple pie maninthestreet man seasons fact much linguistic theory today concerned nature role representation less focus nature operations thus localist approach certain positive aspects definite shortcomings well provides good solution problem account openended nature language commitment discrete welldefined representations may make difficult capture richness high dimensionality required language representations another major approach involves use distributed representations hinton 1988 hinton mcclelland rumelhart 1986 van gelder press together learning algorithm order infer linguistic representations models used localist approach typically made priori commitment linguistic representations agent patient etc networks explicitly trained identify representations input activating nodes correspond presupposes target representations theoretically valid also begs question real world corresponding teaching information might come alternative approach tasks must devised abstract linguistic representations play explicit role models inputs output targets limited variables directly observable environment naturalistic approach sense model learns use surface linguistic forms communicative purposes rather linguistic analysis whatever linguistic analysis done whatever representations developed internal network service task value approach need depend preexisting preconceptions abstract linguistic representations instead connectionist model seen mechanism gaining new theoretical insight thus approach offers potentially satisfying answer first question nature linguistic representations second advantage approach abstract representations formed hidden layer also tend distributed across highdimensional continuous space described analog hidden unit activation vec tors means larger much finergrained representational space work usually possible localist representations space infinite practical purposes may large approach may also provide better response third question apparently openended nature language accommodated fixedresource system rosy still left second question represent complex structural relationships constituency distributed representations far complex difficult understand localist representations tendency feel murkiness intractable distributed entails unanalyzable although fact exist various techniques analyzing distributed representations including cluster analysis elman 1990 hinton 1988 sejnowski rosenberg 1987 servanschreiber cleeremans mcclelland press direct inspec tion pollack 1988 principal component phase state analysis elman 1989 contribution 4analysissanger 1989 results studies limited analyses demonstrated distributed representations may posses internal structure encode relationships kinship hinton 1987 lexical category structure elman 1990 relationships static thus instance elman 1990 network trained predict order words sentences network learned represent words categorizing nouns verbs subcategorization nouns animateinanimate humannonhuman etc representations developed network explicitly taught lexical categories surely important language processing easy think sorts categorization seem different nature consider following sentences 1 boy broke window b rock broke window c window broke underlined words sentences nouns representations reflect nounhood category property belongs inalienably words true regardless appear nouns derivational processes may result nouns used verbs viceversa different level de scription underlined words also similar categorizable subjects sentences property however contextdependent word win dow subject sentence 1c two sentences object still another level description three underlined words differ 1a subject also agent event 1b subject instrument 1c subject patient theme sentence contextdependent property examples simple demonstrations effect grammatical structure structure manifest level utterance addition contextfree categorization words inherit properties virtue linguistic environment although distributed representations seem potentially able respond first last problems posed outset clear address question h ow complex structural relationships constituency represented fodor pylyshyn 1988 phrased need two degrees freedom specify thoughts intentional system entertaining time one parameter active vs inactive picks nodes express concepts system mind construction vs determines concepts system mind distributed propostions entertains pp 2526 point worth reminding ways complex structural relationships dealt symbolic systems contextfree properties typically represented abstract symbols np v etc contextsensitive properties dealt various ways theories eg generalized phrase structure grammar designate context explicit manner socalled slashcatego ries approaches use additional category labels eg cognitive grammar relational grammar government binding designate elements subject theme argu ment trajectory path etc addition theories may make use trees bracketing coin dexing spatial organization tiers arcs circles diacritics order convey complex relationships mappings processing implementation versions exist theories nearly require working buffer stack order account apparently recursive nature utterances rather formidable armamentarium required returning three questions posed outset although distributed representations characteristics plausibly may address need representational richness flexibility may provide soft rather hard limits processing must ask whether approach capture structural relationships sort required language question motivated work reported preliminary evidence encouraging regard hinton 1988 described scheme involve reduced descriptions complex structures represent partwhole hierarchies pollack 1988 press developed training regimen called recursive autoassociative memory raam appears compositional properties supports structuresensitive operations see also chalmers 1989 discussed earlier elmans 1990 use simple recurrent networks srn servanschreiber cleeremans mcclelland press provides yet another approach encoding structural relationships distributed form work described extends latter approach srn taught task involving stimuli underlying hierarchical recursive relation ships structure abstract sense implicit stimuli goal see network could infer abstract structure b represent compositional relationships manner support structuresensitive operations remainder paper organized follows first network architecture briefly introduced second stimulus set task presented properties task make particularly relevant question hand described next results simulation presented final discussion differences similarities approach traditional symbolic approaches language processing discussed network architecture time important element language question represent serially ordered inputs crucial various proposal advanced reviews see elman 1990 mozer 1988 approach taken involves treating network simple dynamical system previous states made available additional input jordan 1986 jordans work network state anyh point time function input current time step plus state output units previous time step work networks state depends current input plus internal state represented hidden units previous cycle hidden units taught assume specific values means develop represen tations course learning task encode temporal structure task words hidden units learn become kind memory taskspecific insert figure 1 type network used current work shown figure 1 network typical connections input units hidden units hidden units output units additional hidden layers input main hidden main hidden output may used serve transducers compress input output vectors additional set units called context units provide limited recurrence may called simple recurrent network context units activated oneforone basis hidden units fixed weight 10 linear activation functions result time cycle hidden unit activations copied context units next time cycle context combines new input activate hidden units hidden units therefore take job mapping new inputs prior states output constitute prior state must develop representations facilitate inputoutput mapping simple recurrent network studied number tasks elman 1990 gasser 1989 hare press task stimuli prediction task elman 1990 network similar figure 1 trained predict order words simple 2 3word sentences point time word presented network networks target output simply next word sequence lexical items inputs outputs represented localist form using basis vectors ie word randomly assigned vector single bit turned lexical items thus orthogonal one another form item encode information items category membership prediction made basis current input word together prior hidden unit state saved context units task chosen several reasons first task meets desideratum inputs target outputs limited observables environment net works inputs outputs immediately available require minimal priori theoretical 7analysis lexical items orthogonal arbitrarily assigned role external teacher minimized since target outputs supplied environment next moment time task involves might called selfsupervised learn ing second although language processing obviously involves great deal prediction prediction seem play role processing listeners indeed predict grosjean 1980 sequences words violate expectationsie un predictableresult distinctive electrical activity brain kutas 1988 kutas hill yard 1980 tanenhaus et al press third accept prediction anticipation plays role language learning provides partial solution called bakers paradox baker 1979 pinker 1989 paradox children apparently receive ignore negative evidence process language learning given frequent tendency initially overgeneralize positive data clear children able retract faulty overgeneralizations gold 1967 however suppose children make covert predictions speech hear others failed predictions constitute indirect source negative evidence could used refine retract scope generalization fourth task requires network discover regularities underlie temporal order words sentences simulation reported elman regularities resulted networks constructing internal representations inputs marked words form class nounverb well lexicosemantic characteristics animateinanimate humananimal largesmall etc results simulation however bore representation lexical category structure relevance grammatical structure unclear mono clausal sentences used shared basic structure thus question remains open whether internal representations learned architecture able encode hierarchical relationships necessary mark constituent structure stimuli stimuli simulation sequences words formed sentences addition monoclausal sentences large number complex multiclausal sentences sentences formed lexicon 23 items included 8 nouns 12 verbs relative pronoun endofsentence indicator period item represented randomly assigned 26bit vector single bit set 1 3 bits reserved another purpose phrase structure grammar shown table 1 used generate sentences resulting sentences possessed certain important properties include following insert table 1 agreement subject nouns agree verbs thus example 2a grammatical 2b training corpus consisted positive examples starred examples actually occur 2a john feeds dogs 2b boys sees mary words marked number singularplural form class verbnoun etc grammatical role subjectobject etc network must learn first items function would call nouns verbs etc must learn items examples singular plural must learn nouns subjects objects since agreement holds subject nouns verbs b verb argument structure verbs fall three classes require direct objects permit optional direct object preclude direct objects result sentences 3ad grammatical whereas sentences 3e 3f ungrammatical 3a girls feed dogs required 3b girls see boys optional 3c girls see optional 3d girls live precluded 3e girls feed 3f girls live dogs words represented orthogonal vectors type verb overtly marked input class membership needs inferred time cooccurrence facts learned c interactions relative clauses agreement verb argument facts become complicated relative clauses although direct objects normally follow verb simple sentences relative clauses subordinate clause direct object head clause cases network must recognize gap following subordinate clause verb direct object role already filled thus normal pattern simple sentences 3ad appears also 4a contrasts 4b 4a dog chases cat sees girl 4b dog cat chases sees girl hand sentence 4c seems conform pattern established 3 4a ungrammatical 4c dog cat chases dog sees girl similar complications arise agreements facts simple declarative sentences agreement involves n1 v 1 complex sentences 5a regularity vio lated straightforward attempt generalize sentences multiple clauses would lead ungrammatical 5b 5a dog boys feed sees girl 5b dog boys feeds see girl recursion grammar permits recursion presence relative clause expand noun phrases may introduce yet relative clauses etc leads sentences 6 grammatical phenomena noted ac may extended considerable distance dogs chase see hear viable sentences one literals inserted grammar occurs end sen tences endofsentence marker potentially occur anywhere string grammatical sentence might terminated thus sentence 7 carets indicate positions might legally occur data 47 examples sorts phenomena linguists argue cannot accounted without abstract representations precisely claimed abstract representations offer perspicacious account grammtical phenomena one example simply lists surface strings chomsky 1957 training data generated grammar summarized table 1 given point training training set consisted 10000 sentences presented network 5 times sentences concatenated input stream proceeded smoothly without breaks sentences however composition sentences varied time following training regimen used order provide incremental training network trained 5 passes following 4 corpora phase 1 first training set consisted exclusively simple sentences accomplished eliminating relative clauses result corpus 34605 words forming 10000 sentences sentence includes terminal phase 2 network exposed second corpus 10000 sentences consisted 25 complex sentences 75 simple sentences complex sentences obtained permitting relative clauses mean sentence length 392 minimum 3 words maximum 13 words phase 3 third corpus increased percentage complex sentences 50 mean sentence length 438 minimum 3 words maximum 13 words phase 4 fourth consisted 10000 sentences 75 complex 25 simple mean sentence length 602 minimum 3 words maximum staged learning strategy developed response results earlier pilot work work found network unable learn task given full range complex data beginning training however network permitted focus simpler data first able learn task quickly move successfully complex patterns important aspect earlier training constrained later learning useful way early training forced network focus canonical versions problems apparently created good basis solving difficult forms problems results conclusion fourth phase training weights frozen final values network performance tested novel set data generated way last training corpus task nondeterministic network unless memorizes sequence always produce errors optimal strategy case activate output units ie predict potential next words extent proportional statistical likelihood occurrence therefore rather assessing networks global performance looking root mean squared error ask closely network approximated probabilities technique described elman press used accomplish contextdependent likelihood vectors generated word every sentences vectors represented empirically derived probabilities occurrence possible predictions given sentence context point networks actual outputs compared likelihood vectors error used measure performance error quite low 0177 initial error 1245 minimal error equal activation units would 192 error also normalized computing mean cosine angle vectors 0852 sd 0259 measures indicate network achieved high level performance prediction gross measures performance however tell us well network done specific problem areas posed task let us look area turn agreement simple sentences agreement simple sentences shown figures 2a 2b insert figure 2 networks predictions following word boy either singular verb follow words three singular verb categories activated since basis predicting type verb else next word may relative pronoun conversely input word boys expectation verb plural follow else relative pronoun similar expectations hold nouns lexicon results follow performance sentences shown representative sentences similar structure b verb argument structure simple sentences figure 3 shows network predictions following initial noun verb three different verb types insert figure 3 verb lives networks expectation following item fact successor permitted grammar context verb sees hand may either followed optionally direct object may singular plural noun proper noun finally verb chases requires direct object network learns expect noun following verbs class c interactions relative clauses examples far involved simple sentences agreement verb argument facts complicated complex sentences figure 4 shows network predictions word sentence boys mary chases feed cats network generalizing pattern agreement found simple sentences might expect network predict singular verb following mary chases insofar predicts verb position conversely might confused pattern n1 n2 v1 fact prediction 4d correctly next verb singular order agree first noun found mechanism representing longdistance dependency main clause noun main clause verb despite presence intervening noun verb agreement relations relative clause insert figure 4 note sentence also illustrates sensitivity interaction verb argument structure relative clause structure verb chases takes obligatory direct object simple sentences direct object follows verb immediately also true many complex sentences eg boys chase mary feed cats sentence displayed however direct object boys head relative clause appears verb requires network learn items function nouns verbs etc b items fall classes c subclasses verbs different cooccurrence relations nouns corresponding verbdirect object restrictions verbs fall classes e expect direct object follow verb know already appeared network appears learned panel see expects chases followed verb main clause verb case rather noun even subtler point demonstrated 4c appearance boys followed relative clause containing different subject mary primes network expect verb follows must class requires direct object precisely direct object filler already appeared words network correctly responds presence filler boys knowing expect gap following chases also learns filler corresponds object position relative clause verb required appropriate argument structure network analysis natural question ask point network learned accomplish task success task seems constitute prima facie evidence existence 3of internal representations possessed abstract structure seemed reasonable believe order handle agreement argument structure facts presence relative clauses network would required develop representations reflected constituent structure argument structure grammatical category grammatical relations number least sort inference made case human language users based behavioral data one advantage working artificial system take additional step directly inspecting internal mechanism generates behavior course mechanism find necessarily used human lis teners may nonetheless surprised find solutions problems might guessed hierarchical clustering useful analytic tool helping understand internatl representations learned network contribute solving problem clustering diagrams hidden unit activation patterns good representing similarity structure representational space however certain lim itations one weakness provides indirect picture representational space another shortcoming tends deemphasize dynamics involved processing states may significance simply terms similarity states regard ways constrain movement subsequent state space recall examples 1 important part network learned lies dynamics involved processing word sequences indeed one might think network dynamics encoding grammatical knowledge certain sequences words move network welldefined permissible internal states sequences move network permissible states sequences permitted ungrammatical might therefore wish able directly inspect internal states represented hidden unit activation vectors network processes words sequence order see states trajectories encode networks grammatical knowledge unfortunately high dimensionality hidden unit activation vectors simulation 70 dimensions makes impractical view state space directly furthermore guarantee dimensions interest us sense pick regions importance networks solution task correlated dimensions coded hidden units indeed means representations distributed dimensions variation cut across degree dimensions picked hidden units however reasonable assume dimensions variation exist try identify using principal component analysis pca pca allows us find another set dimensions rotation axes along maximum variation occurs may additionally reduce number variables effectively removing linearly dependent set axes new axes permit us visualize state space way hopefully allows us see network solves task shortcoming pca linear however combination pca factors next level may nonlinear representation information may give incomplete picture actual computation dimension eigenvector associated eigen value magnitude indicates amount variance accounted di mension allows one focus dimensions may particular significance also allows post hoc estimate number hidden units might actually required task figure 5 shows graph eigenvalues 70 eigenvectors extracted insert figure 5 agreement sentences 8 presented network hidden unit patterns captured word processed sequence boy hears boys 8c boy boys chase chases boy 8d boys boys chase chase boy sentences chosen minimize differences due lexical content make possible focus differences grammatical structure 8a 8b contained training data 8c 8d novel never presented network learning examining trajectories state space along various dimensions apparent second principal component played important role marking number main clause subject figure 6 shows trajectories 8a 8b trajectories overlaid differences readily seen paths similar practical terms anaylsis involves passing training set trained network weights frozen saving hidden unit patterns produced response input covariance matrix resulting set hidden unit vectors calculated eigenvectors covariance matrix found eigenvectors ordered magnitude eigenvalues used basis describing original hidden unit vectors new set dimensions effect giving somewhat localized description hidden unit patterns new dimensions correspond location meaningful activity defined terms variance hyperspace since dimensions ordered terms variance accounted may wish look selected dimensions starting largest eigenvalues see flury 1988 detailed explanation pca gonzalez wintz 1977 detailed description algorithm diverge first word indicating difference number initial noun difference slight eliminated main ie second chase verb input apparently two sentences grammar number information relevance task main verb received insert figure 6 difficult imagine sentences number information may retained intervening constituent sentences 8c 8d examples sentences identical relative clause follows initial noun differs regard number two sentences material boys chase irrelevant far agreement requirements main clause verb trajectories state space two sentences overlaid shown figure 7 seen differences two trajectories maintained main clause verb reached point states converge insert figure 7 verb argument structure representation verb argument structure examined probing sentences containing instances three different classes verbs sample sentences shown 9 boy walks boy sees boy 9c boy chases boy first contains verb may take direct object second takes option direct object third requires direct object movement state space three sentences processed shown figure 8 insert figure 8 figure illustrates network encodes several aspects grammatical structure nouns distinguished role subject nouns three sentences appear upper right portion space object nouns appear principal component 4 shown encodes distinction verbs nouns collapsing across case verbs differentiated regard argument structure chases requires direct object sees takes optional direct object walks precludes object difference reflected systematic displacement plane principal components 1 3 relative clauses presence relative clauses introduces complication grammar representations number verb argument structure must clausespecific would useful network way represent constituent structure sentences trained network given following sentences 10a boy chases boy 10b boy chases boy chases boy 10c boy chases boy chases boy 10d boy chases boy chases boy chases boy first sentence simple three instances embedded sentences contained training data sentences 10c 10d 10e novel presented network learning phase trajectories state space four sentences principal components 1 11 shown figure 9 panel 9a shows basic pattern associated fact matrix sentences four sentences comparison figure panels 9b 9c shows trajectory matrix sentence appears follow matrix subject noun lower left region state space matrix verb appears left matrix object noun near upper middle region recall looking 2 70 dimensions along dimensions nounverb distinction preserved categorically relative clause appears involve replication basic pattern displaced toward left moved slightly ward relative matrix constituents moreover exact position relative clause elements indicates matrix nouns modified thus relative clause modifying subject noun closer relative clause modifying object noun closer trajectory pattern found sentences grammatical form pattern thus systematic insert figure 9 figure 9d shows happens multiple levels embedding successive embeddings represented manner similar way first embedded clause distinguished main clause basic patter clause replicated region state space displaced matrix material displacement provides systematic way network encode depth embedding current state however reliability encoding limited precision states represented turn depends factors number hidden units precision numerical values current simula tion representation degraded three levels embedding consequences degradation performance prediction task different different types sentences sentences involving center embedding eg 9c 9d level embedding crucial maintaining correct agreement adversely affected sentences involving socalled tailrecursion eg 10d latter sentences 7the syntactic structures principle involve recursion practice level embedding relevant task ie affect agreement verb argument structure way figure 9d interesting another respect given nature prediction task actually necessary network carry forward information prior clauses would sufficient network represent successive relative clause iteration previous pattern yet two relative clauses differentiated similarly servanschreiber cleeremans mcclelland press found simple recurrent network taught predict inputs generated finite state automaton network developed internal representations corresponded fsa states however also redundantly made finergrained distinctions encoded path state achieved even though information used task thus seems property networks able encode state way minimizes context far behavior concerned nonlinear nature allows remain sensitive context level internal representation discussion basic question addressed paper whether connectionist models capable complex representations possess internal structure productively estensible question particularly interest regards general issue useful connectionist paradigm framework cognitive models context nature representations interacts number closely related issues order understand significance present results may useful first consider briefly two issues first status rules whether exist whether explicit implicit second notion computational power whether sufficient whether appropriate sometimes suggested connectionist models differ classical models latter rely rules whereas connectionist models typically rule sys tems although first glance appears reasonable distinction actually clear distinction gets us far basic problem obvious meant rule general sense rule mapping takes input yields output clearly since many although neural networks function inputoutput systems bulk machinery implements transformation difficult see could thought rulesystems perhaps meant form rules differs classical models connectionist networks one suggestion rules stated explicitly former whereas implicit networks slippery issue unfortunate ambiguity meant implicit explicit one sense explicit rule physically present system form rule furthermore physical presence important correct functioning system however kirsh 1989 points intuitions counts physical presence highly unreliable sometimes contradictory seems really stake speed information made available true kirsh argues point persuasively quality explicitness belong data structures alone one must also take account nature processing system involved since information form may easily accessible one processing system inaccessible another unfortunately understanding information processing capacity neural networks quite preliminary strong tendency analyzing networks view traditional lenses suppose information contained form familiar computational systems information somehow bur ied inaccessible implicit consider instance network successfully learns complicated mapping say text pronunciation sejnowski rosenberg 1987 inspecting resulting network immediately obvious explain mapping works even characterize mapping precise way cases tempting say network learned implicit set rules really mean mapping complicated difficult formu late even unknown rather description failure understand mechanism rather description mechanism needed new techniques network analysis principal component analysis used present work contribution analysis sanger 1989 weight matrix decomposition mc millan smolensky 1988 skeletonization mozer smolensky 1989 successful analyses connectionist networks may provide us new vocabulary understanding information processing wemay learn new ways information explicit implicit may learn new notations expressing rules underlie cognition notation new connectionist rules may look different used example production rules may expect notation lend describing types regularity equal facility thus potential important difference connectionist models classical models whether one systems contains rules whether one system encodes information explicitly encodes implicitly difference lie nature rules kinds information count explicitly present potential difference brings us second issue computational power issue divides two considerations connectionist models provide sufficient computational power account cognitive phenomena provide appropriate sort computational power first question answered affirmatively important qualification shown multilayer feedforward networks one hidden layer squashing output arbitrary nonlinear activation function hidden layer capable arbitrarily accurate approximation arbitrary mappings thus belong class universal approximators hornik stinchcombe white press stinchcombe white 1989 pollack 1988 also proven turing equivalence neural networks principle networks capable implementing function classical system implement important qualification results sufficiently many hidden units provided case pollacks proof weights infinite precision currently known effect limited resources computational power since human cognition carried system relatively fixed limited resources question paramount interest limitations provide critical constraints nature functions mapped important empirical question whether constraints explain specific form human cognition context question appropriateness computational power becomes interesting given limited resources relevant ask whether kinds operations representations naturally made available likely figure human cognition one theory cognition requires sorting randomly ordered information eg word frequency lists forsters 1979 model lexical access becomes extremely important computational framework provide efficient support sort operation hand one believes information stored associatively ability system fast sort irrelevant instead important model provide associative storage retrieval 1 course things work directions availability certain types operations may encourage one build models type impractical frameworks need work inappropriate computational mechanism may blind us seeing things really let us return current work would like discuss first ways work preliminary limited discuss see positive contributions work finally would like relate work connectionist research general question raised outset discussion viable connectionist models understanding cognition results preliminary number ways first one imagine number example suggested norman additional tests could performed test representational capacity simple recurrent network memory capacity remains unprobed see servan schreiber cleeremans mcclelland press generalization tested limited way many tests involved novels sentences one would like know whether network inferentially extend knows types noun phrases encountered second simulation simple nouns relative clauses noun phrases different structures second true agreement verb argument structure facts contained present grammar important challenging barely scratched surface terms richness linguistic phenomena characterize natural languages third natural languages contain far complexity regard syntactic structure also semantic aspect indeed langacker 1987 others argued persuasively fruitful consider syntax semantics autonomous aspects language rather form meaning language closely entwined although may things learned studying artificial languages present one purely syntactic natural language processing crucially attempt retrieve meaning linguistic form present work address issue pdp models made progress problem eg st john mcclelland press current work contribute notion representational capacity connectionist models various writers eg fodor pylyshyn 1988 expressed concern regarding ability connectionist representations encode compositional structure provide openended generative capacity networks used simulations reported two important properties relevant concerns first networks make possible development internal representations distributed hinton 1988 hinton mcclelland rumelhart 1986 un bounded distributed representations less rigidly coupled resources localist representations strict mapping concept individual nodes also greater flexibility determining dimensions importance model second networks studied build sensitivity context important result current work suggest sensitivity context characteristic many connectionist models builtin architecture networks used preclude ability capture generalizations high level abstraction paradox sensitivity context precisely mechanism underlies ability abstract generalize fact networks exhibited behavior highly regular learned context 1insensitive rather learned respond contexts abstractly de fined recall even networks behavior seems ignore context eg figure 9d servanschreiber cleeremans mcclelland press internal representations reveal contextual information still retained behavior striking contrast traditional symbolic models representations systems naturally contextinsensitive insensitivity makes possible express generalizations fully regular highest possible level representation eg purely syntactic require additional apparatus account regularities reflect interaction meaning form contextually defined connectionist models hand begin task abstraction end continuum emphasize importance context interaction form meaning current work demonstrates characteristics lead quite naturally generalizations high level abstraction appro priate behavior remains everrooted representations contextually grounded simulations reported capitalize subtle distinctions con text ample demonstrations models eg kawamoto 1988 mcclelland kawamoto 1986 miikkulainen dyer 1989 st john mcclelland press finally wish point current approach suggests novel way thinking mental representations constructed language input conventional wisdom holds words heard listeners retrieve lexical rep resentations although representations may indicate contexts words acceptably occur representations contextfree exist canonical form constant across occurrences lexical forms used assist constructing complex representation forms serted one imagine complete result elaborate structure words visible also depicts abstract grammatical structure binds words account process building mental structures unlike process building physical structure bridges houses words whatever representational elements involved play role building blocks true bridges houses building blocks unaffected process construction different image suggested approach taken words processed separate stage lexical retrieval representations words isolation representations words internal states following input word always reflect input taken together prior state scenario words building blocks much cues guide network different grammatical states words distinct virtue different causal properties ametaphor captures characteristics approach combination lock metaphor role words analogous role played numbers combination numbers causal properties advance lock different states effect number dependent context entered correct sequence numbers move lock open state open state may said functionally compositional van gelder press sense reflects particular sequence events numbers present insofar responsible final state still physically present limitation combination lock course one correct combination networks studied complex causal properties words highly structuredependent networks allow many open ie gram matical states view language comprehension emphasizes functional importance representations similar spirit approach described bates macwhinney 1982 mcclelland st john taraban 1989 many others stressed functional nature language representations language constructed order accomplish behavior obviously behavior may range daydream ing verbal duels asking directions composing poetry representations propositional information content changes constantly time accord demands current task words serve guideposts help establish mental states support behavior representations snapshots mental states acknowledgments grateful many useful discussions topic jay mcclelland dave rumelhart elizabeth bates steve stich members ucsd pdpnlp research group thank mcclelland mike jordan mary hare ken baldwin two anonymous reviewers critical comments earlier versions paper research supported contracts n0001485k0076 office naval research contract daab0787ch027 army avionics ft monmouth requests reprints sent center research language 0126 university california san diego 920930126 author reached via electronic mail elmancrlucsdedu r sytntactic theory projection problem functionalist approaches grammar meaning structure language syntactic transformations distributed representations center research concepts cognition syntactic structures spreading activation theory retrieval sentence production symbolic schemata connectionist memories role binding evolution structure university california implementing connectionist production system using tensor products representation structure connectionist models finding structure time mental spaces connectionist models properties cognitive science frame semantics common principal components related multivariate models language thought connectionism cognitive architecture critical analysis levels processing structure language processor networks learn phonology syntax functionaltypological introduction spoken word recognition processes gating paradigm knowledge representation connectionist networks role similarity hungarian vowel harmony connectionist account connectionist perspective prosodic structure representing partwhole hierarchies connectionist networks technical report crgtr882 distributed representations transitivity grammar discourse serial order parallel distributed processing approach institute cognitive science report 8604 distributed representations ambiguous words resolution connectionist network functional syntax anaphora reading senseless sentences brain potentials reflect semantic incongruity foundations cognitive grammar theoretical perspectives usagebased model language learning cues rules temporal structure spoken language understanding analyzing connectionist model system soft rules ca morgan kaufmann publishers focused backpropagation algorithm temporal pattern recognition skeletonization technique trimming fat network via relevance assessment semantic constraints judged preference interpretations ambiguous sentences learnability cognitiion acquisition argument structure recursive autoassociative memory decising compositional distributed representations philosophical implications connectionism learning internal representations error propagation interaction knowledge sources spoken word identification contribution analysis technique assigning responsibilities hidden units connectionist networks linguistic competence parallel networks learn pronounce english text connectionist system rule based reasoning multiplace predicates variables proper treatment connectionism lexicon model language production universal approximation using feedforward networks nonsigmoid hidden layer activation functions study ability decode grammatically novel sentences journal verbal learning verbal behavior reconciling connectionism recursive nature stacks trees rules maps connectionist symbol processing technical report cmucs89158 many maps symbols among neurons details connectionist inference architecture connectionist implementation cognitive phonology graph network predictions following sequences boy lives graph network predictions word sentence boys mary chases feed dogs graph eigenvalues 70 ordered eigenvectors extracted simulation 2 trajectories state space sentences 8a 8b trajectories state space processing 8c 8d trajectories state space sentences 9a trajectories state space sentences 10ad boys mary chases feed cats tr ctr james henderson peter lane connectionist architecture learning parse proceedings 17th international conference computational linguistics august 1014 1998 montreal quebec canada james henderson segmenting state entities implication learning emergent neural computational architectures based neuroscience towards neuroscienceinspired computing springerverlag new york inc new york ny 2001 imran maqsood muhammad riaz khan ajith abraham intelligent weather monitoring systems using connectionist models neural parallel scientific computations v10 n2 p157178 june 2002 matthew h tong adam bickett eric christiansen garrison w cottrell 2007 special issue learning grammatical structure echo state networks neural networks v20 n3 p424432 april 2007 k rahman wang pi yang tommy w chow sitao wu flexible multilayer selforganizing map generic processing treestructured data pattern recognition v40 n5 p14061424 may 2007 james henderson neural network parser handles sparse data new developments parsing technology kluwer academic publishers norwell 2004 kathrine hammervold sentence generation neural networks proceedings first international conference natural language generation june 1216 2000 mitzpe ramon israel michael gasser acquiring receptive morphology connectionist model proceedings 32nd annual meeting association computational linguistics p279286 june 2730 1994 las cruces new mexico junghua wang yiwei yu jiahorng tsai internal representations product units neural processing letters v12 n3 p247254 dec 1 2000 marcin chady modelling higher cognitive functions hebbian cell assemblies emergent neural computational architectures based neuroscience towards neuroscienceinspired computing springerverlag new york inc new york ny 2001 heejin lim yoonsuck choe facilitating neural dynamics delay compensation prediction evolutionary neural networks proceedings 8th annual conference genetic evolutionary computation july 0812 2006 seattle washington usa peter c r lane james b henderson incremental syntactic parsing natural language corpora simple synchrony networks ieee transactions knowledge data engineering v13 n2 p219231 march 2001 hinrich schtze partofspeech induction scratch proceedings 31st annual meeting association computational linguistics p251258 june 2226 1993 columbus ohio symbolicconnectionist systemfor word sense disambiguation applied intelligence v7 n1 p526 january 1997 xindi cai nian zhang ganesh k venayagamoorthy donald c wunsch ii time series prediction recurrent neural networks trained hybrid psoea algorithm neurocomputing v70 n1315 p23422353 august 2007 stephen jos hanson michiro negishi emergence rules neural networks neural computation v14 n9 p22452268 september 2002 paul rodriguez simple recurrent networks learn contextfree contextsensitive languages counting neural computation v13 n9 p20932118 september 2001 imran maqsood ajith abraham weather analysis using ensemble connectionist learning paradigms applied soft computing v7 n3 p9951004 june 2007 samuel w k chan integrating linguistic primitives learning contextdependent representation ieee transactions knowledge data engineering v13 n2 p157175 march 2001 steve lawrence c lee giles sandiway fong natural language grammatical inference recurrent neural networks ieee transactions knowledge data engineering v12 n1 p126140 january 2000 ahmad emami frederick jelinek neural syntactic language model machine learning v60 n13 p195227 september 2005 bechtel compatibility complex systems reduction case analysis memory research minds machines v11 n4 p483502 november 2001 w f g haselager j f h van rappard connectionism systematicity frame problem minds machines v8 n2 p161179 november 1998 sheila garfield stefan wermter call classification using recurrent neural networks support vector machines finite state automata knowledge information systems v9 n2 p131156 february 2006 sheila garfield stefan wermter siobhan devlin spoken language classification using hybrid classifier combination international journal hybrid intelligent systems v2 n1 p1333 january 2005 stephan k chalup alan blair incremental training first order recurrent neural networks predict contextsensitive language neural networks v16 n7 p955972 september stefan c kremer spatiotemporal connectionist networks taxonomy review neural computation v13 n2 p249306 february 2001 stefan wermter knowledge extraction transducer neural networks applied intelligence v12 n12 p2742 januaryapril 2000