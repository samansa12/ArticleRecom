efficient image processing algorithms scan line array processor abstractwe develop efficient algorithms low intermediate level image processing scan line array processor simd machine consisting linear array cells processes images scan line fashion low level processing present algorithms block dft block dct convolution template matching shrinking expanding run realtime realtime mean required processing based neighborhoods size mm output lines generated rate om operations per line latency om scan lines best achieved model also develop algorithm median filtering runs almost realtime cost omrm logm time per scan line latency bf lfloorunderline 2rfloor scan lines intermediate level processing present optimal algorithms translation histogram computation scaling rotation also develop efficient algorithms labelling connected components determining convex hulls multiple figures run onrm logn onrm log2n time respectively latter algorithms significantly simpler easier implement already reported literature linear arrays b introduction 11 motivation realtime image processing understanding long regarded particularly demanding problem computer implementation computational complexity large io bandwidth required tasks involved consider example io bandwidth required perform realtime hdtv simula tion task typically involves handling 1k 1k frames rate 60 frames per second results bandwidth requirement approximately 500 mbytes per second progressively scanned image surprisingly problems generally lie well beyond capacity existing sequential processors consequently great deal effort devoted developing parallel architectures algorithms realtime image processing simplest category proposed architectures twodimensional array mesh examples class include mpp 1 clipp series 2 maspar 3 dap 4 gapp 5 general intent behind simd single instruction stream single data stream machines dimensions mesh match input image pixels assigned processors maintain spatial relationships image consequence machines perform local window operations typical low level image processing extreme efficiency course considerable cost usually associated large number processors furthermore nearestneighbor links might make local interprocessor communication quite fast communication two processors either end n theta n array require n time reduce cost global communication retaining advantages mesh second class architectures proposed category improves mesh linking sequence progressively smaller meshes mesh dimensions one half predecessor result pyramidal structure global communication costs n theta n array reduced thetan thetalog n steps thus pyramidal architecture appropriate intermediate level tasks require global exchange information examples pyramidal machine include papia 6 gam 7 systems course compared mesh increased cost associated pyramid due increased complexity general high level image processing requires simultaneous execution several independent tasks appropriate mimd multiple instruction stream multiple data stream machine disadvantage using purely mimd machine simulate simd operation considerable overhead control synchronization hence expected perform data parallel computations typical low level image processing efficiency displayed truly simd machines recognizing relative advantages simd mimd oper ations number machines proposed generally consist elaborate combination reconfigurable simd mimd modules basic idea behind creating hybrids enable programmer utilize efficient architecture whatever particular problem presented examples class architectures include image understanding architecture iua 8 associative string processor asp 9 netra 10 strategy architecture design probably offers best hope achieving optimal performance across spectrum processing tasks also expensive fortunately applications require levels image processing situations limited low intermediate level tasks either simd architectures already mentioned might well sufficient however one particularly simple example avoids cost constraints mentioned far still achieve optimal performance low intermediate level operations architecture proposed cases implemented variety names including scan line array processor slap 11 12 princeton engine 13 sarnoff engine 14 architecture referred slap basic topology simd machine linear array processors number processors corresponds number pixels row image incoming data loaded line line processor array distinct processor receiving image column course results high io bandwidth memory architecture entirely distributed among individual processors processor communicate directly two immediate neighbors whenever exist obvious disadvantage interconnection scheme small communication bandwidth would seem make global communication inefficient however number compensations compared alternatives anything else would involve either elaborate interconnection network shared memory accompanying increase cost complexity scaling slap simply requires adding processors end existing array unlike bus based systems obvious technical limitations many processors linked together without degrading performance hence seems reasonable suspect linear array architecture might offer one lowest cost options achieving optimal low intermediate level image processing however true necessary argue technology relatively cheap efficient algorithms written run architecture surprisingly design algorithms utilize linear array architecture received modest attention date literature specifically algorithms proposed sorting 15 matrix multiplication 16 hough transform 17 algorithms also proposed solve number graph 18 geometric problems 19 assuming image data already partitioned among processors according shuffled rowmajor distribution advantage shuffled rowmajor distribution significantly reduces cost global communication different contiguous regions image disadvantage rather time consuming implement appreciate recall image loaded line line slap hence pixel line initially assigned processor like index achieve shuffled rowmajor distribution interprocessor broadcasting would operations per processor n theta n image would make unsuitable realtime applications contrast initial distribution requires single load operation per processor scan line accordingly assume straightforward data distribution starting point developing efficient algorithms selection low intermediate level image processing tasks 12 computational model computational model used paper defined follows slap architecture linear array n processors image processed size n theta n processor indexed ascending order 0 n gamma 1 scan line received processor p k latches value corresponding column index k processor p k connected bidirectional communication link processors p kgamma1 p k1 whenever exist processors p kgamma1 p k1 referred left right neighbors processor p k respectively bandwidth communication link assumed word word defined olog n bits processor communicate constant number words immediate neighbors unit time processors operate together simd fashion processor general purpose sequential processor capacity conditional command execution local address generation unit time processor compute o1 basic logic arithmetic operations finally processor associated random access memory hold words unit time processor access local memory read write word 13 problem formulation paper problem developing algorithms low level image processing treated separately intermediate level tasks case low level operations assumed need achieve smallest possible latency consistent optimal running time end require processing scan line begin immediately received corresponding output line generated amount time independent image size formally suppose required processing based neighborhoods size theta require output lines generated rate om operations per line initial delay om lines algorithm achieves level performance runs real time case intermediate level operations entire image must available examination output line generated hence assume outset entire image already stored local memories processors way received column stored processor like index already noted straightforward input distribution requires time require subsequent preprocessing included time estimates intermediate level operations call algorithm optimal either runs time shown faster algorithm possible 14 results results summarized follows low level operations develop realtime algorithms block dft block dct convolution template matching shrinking expanding 20 also develop algorithm median filtering runs almost realtime cost om log time per scan line latency b mc scan lines 20 intermediate level operations develop optimal algorithms translation histogram computation scaling translation 20 although sake brevity omit discussion latter two algorithms also develop algorithms labelling connected components determining convex hulls multiple components run log n time log 2 n respectively 20 complexities last two algorithms compare favorably existing algorithms respectively require log n time also assume shuffled rowmajor distribution 19 addition algorithms significantly simpler easier implement thus expected perform better practice processing operations 21 median filtering median filtering process replacing pixel given image median pixels contained window centered around pixel filtering operation useful removing isolated lines pixels preserving spatial resolution specifically let x input image size n theta n let window size theta assumed odd convenience filtered image defined 1 1 straightforward approach solving problem slap compute median independently output pixel specifically produce row j output image processor p k first accumulates 2 pixel values necessary compute j k median 2 values computed best known sequential algorithm thetam 2 time next scan line received processor p k updates current set pixel values replacing least recent values current located either side processor p k clearly simple method yields desired latency om scan lines also operations per scan line however efficient procedure allows median filtering image performed rate om log operations per scan line desired latency procedure based observation window surrounding pixel j k window surrounding pixel j differ 2m pixels immediately suggests store 2 values contained window pixel j k appropriate data structure allows us 1 efficiently update data structure reflect window pixel j quickly find median values stored data structure one data structure orderstatistic tree 21 allows us dynamically 1 delete element 2 insert element 3 locate p th smallest element integer p time proportional logarithm tree size hence using data structure produce algorithm perform median filtering om log operations per scan line assume realtime implementation operation input j th row input image x initially processor p k broadcasts value input pixel xj k set gamma 1 processors fk gamma windows include xj k since essentially involves left shift input scan line mc positions followed right shift b mc positions clearly accomplished om time next processor p k deletes oldest values orderstatistic tree inserts value xj k values received neighbors already noted accomplished om log time finally olog time processor p k determines median updated orderstatistic tree outputs result value pixel j gamma b mc k output image thus shown following theorem theorem 1 given slap n processors median filtering operation n theta n image window size theta performed almost realtime cost om log operations per scan line latency b mc scan lines 22 block 2ddft 2ddct block transformation given type image involves partitioning image nonoverlapping blocks applying desired transformation resulting blocks two widely used transformations two dimensional discrete fourier transform 2ddft two dimensional discrete cosine transform 2ddct section present algorithm implement either transformation theta blocks image matrix realtime since two problems essentially analogous restrict discussion task computing block 2ddft let n theta n input image x partitioned nonoverlapping theta blocks assume convenience divides n evenly denote one block x ab g 2ddft x ab defined 1 straightforward approach apply transformation slap would independently evaluate contribution input line computation coefficient would line however 2ddft separable transform rewrite definition follows notice given row g input block given column output block single value 0 ab g computed om operations ab g calculated evaluate contribution g th input row th output column om operations thus taking advantage separability compute block dft realtime assume realtime implementation operation input j th row image x initially processor p k broadcasts value input pixel xj k set gamma 1 processors fbkmc theta bkmc theta 1 bkmc theta mm gamma 1g share block requires om time following processor p k computes k mod th coefficient 1ddft j mod th row block specifically processor p k evaluates expression clearly involves om operations next processor p k uses result update column partially computed 2ddft coefficients held array local memory precisely value c 0 c performs following operation clearly involves om operations last row block processed values held array partial gamma dft k k mod th column 2ddft block processed finally processor p k outputs bjmcgamma1bkmc j mod k mod pixel j gamma k output image hence following theorem theorem 2 given slap n processors block dft dct image size n theta n block size theta computed realtime cost om operations per scan line latency scan lines 23 convolution template matching convolution template matching fundamental image processing operations computationally demanding section present method performs convolution image size n theta n kernel size theta realtime rate om operations per scan line method extended perform template matching within time bounds given image x size n theta n kernel w size theta convolution x w image size n gamma 1 theta n gamma 1 defined assuming implicitly equal zero whenever j gamma r interval 0 n gamma 1 straightforward computation convolution would require operations therefore would require minimum omegagamma operations per scan line slap suppose instead employ overlapandadd strategy 22 specifically partition input image x nonoverlapping theta blocks referred x 0 indexed b convolve block x 0 ab theta kernel w obtain ab let easy verify value j k defined 6 obtained simply adding four entries 0 nothing apparent gained computationally redefining matrix convolution problem terms block convolution however zero pad block x according following rule ab ab 1 also zero pad kernel w analogous manner obtain w 00 done able obtain linear convolution x 0 ab w performing circular convolution zeropadded counterparts x 00 ab w 00 advantage circular convolution lies fact circular convolution two matrices obtained simply finding respective dfts multiplying identically indexed elements two dfts finally finding idft resulting product matrix expressed concisely follows let j denote circular convolution n denote elementwise multiplication ab already demonstrated previous section compute block dft n theta n image theta blocks om operations per scan line hence would also expect able obtain dft 2m theta 2m zeropadded blocks om time well resulting dft blocks distributed two columns per processor clear elementwise multiplication dft x 00 ab dft w would require om operations per processor well finally since computation idft essentially analogous computation dft would expect modification algorithm finding block dft would also yield idft dft x 00 ab operations per input line course would need complete computation dft x 00 ab could begin computation idft hence would actually overlap computation dft x 00 ab computation idft dft x 00 ab rather computation idft dft x 00 n dft w 00 thus see converting problem convolution one block convolution taking advantage properties fourier transforms reduce cost convolution om 2 operations per scan line om operations per scan line detailed presentation algorithm see 23 hence following theorem theorem 3 given slap n processors convolution image size n theta n kernel size theta computed realtime cost om operations per scan line latency 2m scan lines similarly template matching problem solved rate template size theta 24 shrinking expanding given positive integer mstep shrinking n theta n image x n theta n image defined recursively follows 1 similarly mstep expansion n theta n image x n theta n image e defined replacing minimum maximum definition easy verify mstep shrinking mstep expansion simply involve replacement pixel x respective minimum maximum 2m pixels contained 2m centered pixel essential similarity shrinking expansion problems discuss algorithm former assume realtime algorithm processor p k holds local memory queue referred queue k holds previous 2m input processor new pixel xj k received processor p k queue k updated minimum min k values queue k computed next processor p k broadcasts min k set processors whose windows encompass column k completed processor 2m represent minima columns contained window centered processor processor p k computes smallest 2m outputs pixel output image hence following theorem theorem 4 given slap n processors mstep shrinking expansion image size n theta n computed realtime cost om operations per scan line latency scan lines 3 intermediate level image processing operations 31 convex hull given set points distinguished common label convex hull defined smallest convex polygon contains points set extreme points defined corners smallest polygon section consider n theta n input image x pixel one possible labels pixels share particular label said belong set even though may spatially unconnected one another sets wish determine extreme points convex hull present algorithm compute convex hulls log 2 n time simplify presentation concentrate problem determining upper hulls since task finding lower hulls entirely analogous divideandconquer algorithm first divides input image two subimages one consisting leftmost n columns consisting rightmost n columns labels upper hulls two subimages recursively computed parallel novel strategy used merge two upper hulls merging procedure consists first concatenating sequence extreme points define right upper hull sequence extreme points define left upper hull done remove subsequences points fp points lie line connecting adjacent points p l p r algorithm performed making olog n lefttoright righttoleft sweeps across image label time eliminating fraction nonextreme points extensive pipelining used insure time sweep completed every one labeled sets spite simplicity algorithm analysis requires somewhat tricky geometric argument establish olog n sweeps sufficient hence first present algorithm provide proof correctness 311 algorithm begin algorithm two preprocessing steps reduces complexity task first preprocessing step processor p k sequentially examines correspondingly indexed column input image x local memory label encounters retains uppermost occurrence label potential candidate point upper hull set second preprocessing step label make lefttoright sweep across processor array pass processor p k identify candidate point processor candidate point processors 0 k maximum row index similarly label also make righttoleft sweep across processor array pass processor p k identify candidate point processor candidate point processors k maximum row index pipelining sweeps labels whole process completed time processor p k examines candidate points see lies line connecting respective left right maxima point eliminated possible candidate point label sequence candidate points left two preprocessing steps actually consists two characteristic subsequences one may empty first subsequence monotonically increasing subsequence monotonically decreasing significance property become apparent discuss proof algorithm computing extreme points upper hulls proceeds follows divide n theta n input image x two subimages x 1 x 2 x 1 consists columns consists columns n n gamma 1 labels recursively compute parallel uh 1 uh 2 respectively upper hulls sets points label found x 1 x 2 computation completed uh 1 uh 2 represented given label remaining candidate points columns 0 ngamma 1 columns n n gamma 1 respectively merge uh 1 uh 2 thereby obtain completed upper hull uh proceed follows first compute candidate point p coordinates immediate successor sp label accomplished making single righttoleft pass across processor array carrying along coordinates recently encountered candidate point label pipelining sweeps labels whole process completed time next olog n repetitions perform following computation label make lefttoright pass across processor array carrying along coordinates recently encountered candidate point label hence arrive candidate point p available coordinates current predecessor p p denote angle formed points p p opens away interior convex hull candidate point p check see p p corresponds asking whether p lies line segment connecting p p sp eliminate point consideration carry along coordinates p p new value predecessor sp complete lefttoright pass begin analogous righttoleft pass back across processor array however concerned deletions previous lefttoright pass may left successor values outdated hence bring along candidate point p coordinates immediate successor pipelining insures complete two passes labels time olog n repetitions candidate points remaining set extreme points upper hull set 312 proof made claim log 2 n time sufficient determine extreme points convex hulls arbitrary sets basis claim contention olog n passes across processor array sufficient merge two arbitrary upper hulls first prove latter claim demonstrate overall running time follows proof upper limit running time merging algorithm based geometric argument therefore clearly presented diagram shown figure 1 figure 1 illustrates process merging two arbitrary upper hulls defined subsequence extreme points fp 1 uh 2 defined subsequence extreme points fp j1 g order points subsequence corresponds relative order column indices assume uh 1 spans portion columns 0 ngamma 1 uh 2 spans portion columns n n gamma 1 upper hull defined n extreme points according way drawn uh 1 uh 2 figure 1 upper hull uh resulting merging uh 1 uh 2 line segment sake clarity allowed number distortions figure 1 first represented uh 1 uh 2 continuous curves rather concatenations line segments second allowed angle p j opening away interior convex hull less 90 ffi fact preprocessing makes impossible finally drawn uh 1 uh 2 merging eliminate entire subsequence g yet subsequence composed two smaller subsequences one shown monotonically decreasing shown monotonically increasing understand difficulty recall preprocessing procedure insures original se figure 1 merging two upper hulls quence candidate points monotonically increasing one two maximum points monotonically decreasing thereafter clearly maximum points must also extreme points completed convex hull implying subsequences candidate points lie either left right qualify elimination therefore subsequence nonextreme candidate points need eliminated must either monotonically increasing decreasing g however situation pictured figure 1 could still arise allow rotate image line segment p 1 p k parallel horizontal axis image grid permit changes described enhance clarity presentation without affecting validity proof consider arbitrary lefttoright pass across image described algorithm sweep extends tangent uh 2 secondto last point remaining uh 1 thereby eliminating intervening points fall line assume arbitrary number passes made across processor array time eliminating points end uh 1 pl last point remaining sequence next lefttoright pass extends line segment p p l p shown figure 1 following succeeding righttoleft pass draws tangent sp p two facts concerning point pa must hold first pa must lie left line segment p p l g perpendicular p 1 passing point p p l see recall subsequence candidate points requires elimination must either monotonically increasing monotonically decreasing consequence angle formed three points p u p v pw u must greater 90 ffi turn means perpendicular p 1 must divide subsequence exactly two parts thus every point remaining uh1 aside pl p p l must lie left line segment p p l g pa also require point pa lie line segment p c line segment parallel passing guaranteed fact angle formed three consecutive points upper hull must greater 180 ffi hence p u p v pw three consecutive points determine uh 1 slope p v pw defined respect p p l g must steeper p u p v consequently slope sp p defined respect p p l g must steeper must lie p c following righttoleft pass drawn tangent sp p succeeding left toright pass draws tangent p p p b notice reason point pa must lie line segment must lie line segment pa h line segment parallel passing pa moreover point pb must lie right line passing points p p l p since angle formed three consecutive points upper hull must greater 180 ffi finally define point f point line passing points p p l p distance p p p l divide region right line two halves indicated figure 1 enables us make following two observations 1 pb lies region 1 projection p p p b p 1 p k must greater equal projection p p l f since definition projection p p l f exactly twice projection p p l p follows projection second tangent p p p b must least twice projection previous tangent p p l p 2 since point pa must lie p c left p p l g follows slope p p p b defined respect p p l g must less slope dpb pb lies region 2 slope dpb must less slope df since definition slope df exactly half slope p p l f follows slope second tangent p p p b must less half slope previous tangent p p l p hence conclude tangent drawn lefttoright pass either half slope double projection tangent drawn previous lefttoright pass image digitalized easily shown smallest possible nonzero projection tangent either p 1 p k perpendicular omegagamma 1 course maximum possible projection p u p v either p 1 p k perpendicular obviously follows maximum minimum possible slopes p u defined respect 2 repectively therefore since tangent drawn lefttoright pass either half slope double projection tangent drawn previous lefttoright pass follows olog n passes sufficient compute upper bound overall running time determining possible convex hulls note solution merging problem two n theta psubimages 2 p n still requires thetan log ntime hence upper bound running time divideandconquer algorithm governed following recurrence log n whose solution log 2 n hence shown following theorem theorem 5 assume slap n processors input image size n theta n distributed one column per processor also assume pixel labelled one possible labels determine extreme points convex hulls labeled sets log 2 n time also use algorithm solve another problem involving convex hulls consider n theta n image pixel belong one 2 sets distinguished common label require pixels belong particular set must form connected component appropriate preprocessing use algorithm find extreme points convex hulls 2 sets log 2 n time justify claim consider arbitrary vertical line divides image two parts requirement members particular set belong connected component n sets members sides arbitrary line hence algorithm calls us make pass across processor array given set obviously need make pass across arbitrary line set one n sets therefore move one processor another coordinates candidate points labels need carried along hence log 2 n time suffices case well hence also following theorem theorem assume slap n processors input image size n theta n distributed one column per processor also assume pixel labelled one 2 possible labels restriction pixels share particular label must form connected component determine extreme points convex hulls labeled sets log 2 n time 32 connected components given binary image x two pixels called neighbors words two pixels said neighbors adjacent horizontally vertically diagonally two pixels p j 1 said connected exists sequence pixels p j h k h sharing property p j hgamma1 k hgamma1 neighbor p j h k h section present algorithm labels connected components n theta n input image slap log n time algorithm finding connected components employs divideandconquer strategy assume processor p k holds k th column input image divide n theta n input image x two subimages x 1 x 2 x 1 consists columns 0 ngamma 1 x 2 consists columns n n gamma 1 recursively parallel identify label connected components two subimages x 1 x 2 completed identify merge connected components span columns n easily done using existing sequential algorithms time either processor p ngamma1 processor p n broadcast label changes across two subimages since connected components spanning two columns hence label changes accomplished pipelining time clearly whole merging operation completed time compute upper bound running time labeling connected components note solution merging problem two n theta psubimages 2 requires thetan time hence upper bound running time divideandconquer algorithm governed following recurrence whose solution log n hence following theorem theorem 7 given slap n processors input image size nthetan distributed one column per processor connected components image labeled log n time 33 translation translation process mapping pixel location j k n theta n input image x new position j n theta n output image 1g assume simplicity b integers jaj jbj n2 since cases either jaj n2 jbj n2 reformulated jaj jbj n2 algorithm proceeds follows initially shift row jbj positions right correctly relocates jbj columns following wrap around remaining jbj columns end processor array pipelining columnbycolumn broadcast task translating pixels within particular column jaj positions easily accommodated insuring pixel reaches correct processor stored properly shifted memory location straightforward verify upper bound running time algorithm onjbj comparison lower bound derived noting problem requires us move n gamma jbj columns pixels distance jbj processors jbj columns distance n gamma jbj processors since processor perform constant number transfers per unit time n processors follows lower bound omegagamma njbj hence following theorem theorem 8 given slap n processors input image size n theta n distributed one column per processor translation image distance vertical direction distance b horizontal direction completed thetanjbj time bound optimal 34 histogram computation histogram n theta n input image x computation determines relative frequency occurrence various possible pixel values image develop algorithm assume simplicity possible pixel intensities constrained integer values ranging 0 gamma 1 nothing assumed relative sizes n algorithm proceeds follows first processor p k computes histogram k th column input image processor simply examines n pixels increments entry array called whose index corresponds pixel value next compute image matrix histogram adding together identically indexed values local histograms h k simply requires righttoleft passes across processor array leave completed histogram values array h 0 kept processor p 0 pipeline passes whole procedure completed om n time clearly optimal hence following theorem theorem 9 given slap n processors input image size n theta distributed one column per processor assume possible pixel values constrained integer values interval 0 gamma 1 histogram image computed time paper presented efficient algorithms slap variety low intermediate level image processing tasks low level operations algorithms run realtime clearly best achieved model intermediate level operations algorithms either optimal compare favorably existing algorithms moreover algorithms achieve performance without assuming input image already partitioned according shuffled rowmajor distribution taken together results suggest slap promising architecture realtime image processing r design massively parallel processor clip7a image processor design maspar mp1 cost effective massively parallel computer distributed processor array geometric arithmetic parallel processor papia gam pyramid image understanding architecture asp costeffective parallel microcomputer netra architecture large scale multiprocessor vision system realtime image processing scan line array pro cessors scan line array processors image computations princeton engine realtime video system simulator sarnoff engine massively parallel computer high definition system simulation optimal sorting algorithms parallel computers modular matrix multiplication linear ar ray computing hough transform scanline array processor optimal graph algorithms fixedsize linear array optimal geometric algorithms digitized images fixedsize linear arrays scanline arrays fundamentals digital image processing introduction algorithms efficient image processing algorithms scan line array proces sor tr ctr ronald greenberg finding connected components scan line array processor proceedings seventh annual acm symposium parallel algorithms architectures p195202 june 2426 1995 santa barbara california united states francesco gregoretti roberto passerone leonardo maria reyneri claudio sanso high speed vlsi architecture handwriting recognition journal vlsi signal processing systems v28 n3 p259278 july 2001 shorin kyo shinichiro okazaki tamio arai integrated memory array processor architecture embedded image recognition systems acm sigarch computer architecture news v33 n2 p134145 may 2005