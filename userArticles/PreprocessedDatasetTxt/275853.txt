scheduling blockcyclic array redistribution abstractthis article devoted runtime redistribution onedimensional arrays distributed blockcyclic fashion processor grid previous studies concentrated efficiently generating communication messages exchanged processors involved redistribution focus scheduling messages organize message exchanges structured communication steps minimize contention build upon results walker otto solved particular instance problem derive optimal scheduling general case namely moving cyclicr distribution pprocessor grid cyclics distribution qprocessor grid arbitrary values redistribution parameters p q r b introduction runtime redistribution arrays distributed blockcyclic fashion multidimensional processor grid difficult problem recently received considerable attention interest motivated largely hpf 13 programming style scientific applications decomposed phases phase optimal distribution data arrays onto processor grid typically arrays distributed according cyclicr pattern along one several dimensions grid best value distribution parameter r depends characteristics algorithmic kernel well communicationtocomputation ratio target machine 5 optimal value r changes phase phase one machine another think heterogeneous environment runtime redistribution turns critical operation stated 10 21 22 among others basically decompose redistribution problem following two subproblems message generation array redistributed efficiently scanned processed order build messages exchanged processors communication scheduling messages must efficiently scheduled minimize communication overhead given processor typically several messages send processors subset terms mpi collective operations 16 must schedule something similar mpi alltoall communication except processor may send messages particular subset receivers subset depending sender previous work concentrated mainly first subproblem message generation message generation makes possible build different message pair processors must communicate thereby guaranteeing volumeminimal communication phase processor sends receives data needed however question efficiently schedule messages received little attention one exception interesting paper walker otto 21 schedule messages order change array distribution cyclicr p processor linear grid cyclickr grid aim extend walker ottos work order solve general redistribution problem moving cyclicr distribution p processor grid cyclics distribution qprocessor grid general instance redistribution problem turns much complicated particular case considered walker otto however provide efficient algorithms heuristics optimize scheduling communications induced redistribution operation main result following values redistribution parameters p q r construct optimal schedule schedule whose number communication steps minimal communication step defined processor sendsreceives one message thereby optimizing amount buffering minimizing contention communication ports construction optimal schedule relies graphtheoretic techniques edge coloring number bipartite graphs delay precise mathematical formulation results section 4 need several definitions beforehand without loss generality focus onedimensional redistribution problems article although usually deal multidimensional arrays highperformance computing problem reduces tensor product individual dimensions hpf allow one loop variable align directive therefore multidimensional assignments redistributions treated several independent onedimensional problem instances rest article organized follows section 2 provide examples redistribution operations expose difficulties scheduling communications section 3 briefly survey literature redistribution problem particular emphasis given walker otto paper 21 section 4 present main results section 5 report mpi experiments demonstrate usefulness results finally section 6 state conclusions future work directions motivating examples consider array x0m gamma 1 size distributed according block cyclic distribution cyclicr onto linear grid p processors numbered goal redistribute x using cyclics distribution q processors numbered simplicity assume size x multiple qs least common multiple p r qs redistribution pattern repeats slice l elements therefore assuming even number slices x enable us without loss generality avoid discussing side effects let l number slices example 1 consider first example 5 note new grid q processors identical disjoint original grid p processors actual total number processors use unknown value 16 32 communications summarized table 1 refer communication grid note view source target processor grids disjoint table 1 even may actually case see source processor messages processor receives 7 messages hence need use full alltoall communication scheme would require 16 steps total 16 messages sent per processor precisely 15 messages local copy rather try schedule communication efficiently ideally could think organizing redistribution 7 steps communication phases step 16 messages would exchanged involving disjoint pairs processors would perfect oneport communication machines processor send andor receive one message time note may ask something try organize steps way step 8 involved pairs processors exchange message length approach interest cost step likely dictated length longest message exchanged step note message lengths may may vary significantly numbers table 1 vary 1 3 single slice vector vector x length lengths vary 1000 3000 times number bytes needed represent one datatype element schedule meets requirements namely 7 steps 16 disjoint processor pairs exchanging messages length provided section 432 report solution schedule table 2 entry position p q table denotes step numbered g clarity processor p sends message processor q table 3 compute cost communication step proportional length longest message involved step total cost redistribution sum cost steps elaborate model communication costs section 431 table 1 communication grid 5 message lengths indicated vector x size communication grid msg nbr msg 7 7 example 2 second example shows usefulness efficient schedule even processor communicates every processor illustrated table 4 message lengths vary ratio 2 7 need organize alltoall exchange steps way messages length communicated step able achieve goal see section 432 solution schedule given table 5 steps numbered p cost given table 6 check 16 steps composed messages length example 3 third motivating example shown table 7 communication scheme severely unbalanced processors may different number messages send andor receive technique able handle complicated situations provide section 44 schedule composed 10 steps longer possible messages length step instance processor messages length 3 send processor messages length 1 2 achieve redistribution communication steps processor sendsreceives one message per step number communication steps table 8 clearly optimal processor send cost schedule given table 9 table 2 communication steps communication steps 9 b f g e c table 3 communication costs communication costs step b c e f g total cost example 4 final example p 6 q show size two processor grids need see table 10 communication grid unbalanced solution schedule see section 44 composed 4 communication steps number optimal since processor messages receive note total cost equal sum message lengths processor must receive hence optimal 3 literature overview briefly survey literature redistribution problem particular emphasis given work walker otto 21 table 4 communication grid indicated vector x size msg nbr msg 31 message generation several papers dealt problem efficient code generation hpf array assignment statement like arrays b distributed blockcyclic fashion linear processor grid researchers see stichnoth et al17 van reeuwijk et al19 wakatani wolfe 20 dealt principally arrays distributed using either purely scattered cyclic distribution cyclic1 hpf full block distribution cyclicd n array size p number processors recently however several algorithms published handle general blockcyclic distributions sophisticated techniques involve finitestate machines see chatterjee et al 3 settheoretic methods see gupta et al 8 diophantine equations see kennedy et al 11 12 hermite forms lattices see thirumalai ramanujam 18 linear programming see ancourt et al 1 comparative survey algorithms found wang et al 22 reported powerful algorithms handle blockcyclic distributions efficiently simpler case pure cyclic fullblock mapping end message generation phase processor computed several different messages usually stored temporary buffers messages must sent set receiving processors examples section 2 illustrate symmetrically processor computes number length messages receive therefore allocate corresponding memory space summarize message generation phase completed processor table 5 communication steps communication steps 9 g table communication costs communication costs step b c e f cost prepared message processors must send data processor possesses information regarding messages receive number length origin 32 communication scheduling little attention paid scheduling communications induced redistribution operation simple strategies advocated instance kalns ni 10 view communications total exchange processors specify operation comparative survey wang et al 22 use following template executing array assignment statement 1 generate message tables post receives advance minimize operating systems overhead 2 pack communication buffers 3 carry barrier synchronization table 7 communication grid 5 message lengths indicated vector x size 14 nbr msg nbr msg 6 9 6 6 9 6 6 9 6 6 9 6 6 9 9 4 send buffers 5 wait messages arrive 6 unpack buffers although communication phase described precisely note explicit scheduling messages sent simultaneously using asynchronous communication pro tocol approach induces tremendous requirement terms buffering space deadlock may well happen redistributing large arrays scalapack library 4 provides set routines perform array redistribution described prylli tourancheau 15 total exchange organized processors arranged virtual caterpillar total exchange implemented succession steps step processors arranged pairs perform sendreceive operation caterpillar shifted new exchange pairs formed even though special care taken implementing total exchange attempt made exploit fact processor pairs may need communicate first paper devoted scheduling communications induced redistribution walker otto 21 review two main possibilities implementing communications induced redistribution operation wildcarded nonblocking receives similar strategy wang et al described asynchronous strategy simple implement requires buffering messages received hence total amount buffering high total volume data redistributed table 8 communication steps communication steps table 9 communication costs communication costs step b c e f g h j total cost synchronous schedules synchronized algorithm involves communication phases steps step participating processor posts receive sends data waits completion receive several factors lead performance degradation instance processors may wait others receive data hot spots arise several processors attempt send messages processor step avoid drawbacks walker otto propose schedule messages step processor sends one message receives one message strategy leads synchronized algorithm efficient asynchronous version demonstrated experiments written mpi 16 ibm sp1 intel paragon requiring much less buffering space walker otto 21 provide synchronous schedules special instances redistribution problem namely change array distribution cyclicr p processor linear grid cyclickr grid size main result provide schedule composed k steps step processors send receive exactly one message k smaller p size grid dramatic improvement traditional alltoall implementation table 10 communication grid message lengths indicated vector x size msg nbr msg 2 4 aim article extend walker ottos work order solve general redistribution problem moving cyclicr distribution p processor grid cyclics distribution qprocessor grid retain original idea schedule communications steps step participating processor neither sends receives one message avoid hot spots resource contentions explained 21 strategy well suited current parallel architectures section 431 give precise framework model cost redistribution 4 main results 41 problem formulation consider array x0m gamma 1 size distributed according blockcyclic distribution cyclicr onto linear grid p processors numbered goal redistribute x using cyclics distribution q processors numbered 1 equivalently perform hpf assignment cyclicr processor grid cyclics qprocessor grid 1 blockcyclic data distribution maps global index vector x ie element xi onto processor index p block index l item index x local block indices starting 0 mapping gamma p l x may written birc derive relation 1 general assignment dealt similarly table 11 communication steps communication steps table 12 communication costs communication costs step b c total cost similarly since distributed cyclics qprocessor grid global index j mapped get redistribution equation qs least common multiple p r qs elements li x initially distributed onto processor l multiple p r hence r divides l p divides l xi r similar reason two elements redistributed onto processor words redistribution pattern repeats slice l elements therefore restrict discussion vector x length l following let rqs bounds equation 3 become given distribution parameters r grid parameters p q redistribution problem determine messages exchanged find values p q redistribution equation 3 solution unknowns l x subject bounds equation 4 computing number solutions given processor pair p q give length message start simple lemma leads handy simplification lemma 1 assume r relatively prime proof redistribution equation 3 expressed equation 3 expressed solution given processor pair p q delta divides z z deltaz 0 deduce solution redistribution problem r 0 0 p q let us illustrate simplification one motivating examples back example 3 note need scale message lengths move redistribution operation r relatively prime one let us return example 3 assume know build communication grid table 7 deduce communication grid say keep messages scale lengths process makes sense new size vector slice deltal rather l see table 13 resulting communication grid course scheduling communications remain cost table 9 multiplied delta 42 communication pattern consider redistribution parameters r p q assume qs communication pattern induced redistribution operation complete alltoall operation proof rewrite equation 5 ps gamma p rl gamma qsm arbitrary multiple g since z lies interval 1 gamma whose length r guaranteed multiple g found within interval conversely assume g r exhibit processor pair p q exchanging message indeed desired processor pair see note pr gamma g divides p r hence multiple g added pr gamma qs lies interval 1 gamma therefore message sent p q redistribution 2 following aim characterize pairs processors need communicate redistribution operation case consider following function 2 another proof see petitet 14 table 13 communications indicated vector x size 14 nbr msg nbr msg 6 9 6 6 9 6 6 9 6 6 9 6 6 9 9 function f maps processor pair p q onto congruence class pr gamma qs modulo g according proof lemma 2 p sends message q fp modg let us illustrate process using one motivating examples back example 4 example proof lemma 2 receives message p receive message see table 10 check characterize classes introduce integers u v r theta extended euclid algorithm provides numbers relatively prime r following result proposition 1 assume r u mod g proof first see pq indeed integer note since g divides p r qs divides pq two different classes disjoint definition turns classes number elements see note k 2 0 integer 0 since g classes deduce number elements class pq next see p q mod finally p p r qs divide divides rs deduce pq divides hence processors pairs p distinct thus enumerated class0 definition 3 consider redistribution parameters r p q assume 1 let lengthp q length message sent processor p processor q redistribute single slice vector x size said earlier communication pattern repeats slice value reported communication grid tables section 2 single slice equal lengthp q interesting represent homogeneous communications processor pairs given class exchange message length proposition 2 assume qs length vector x redistributed let volk piecewise function given figure 1 k 2 1 gamma recall p q 2 classk sends message q volk volr figure 1 piecewise linear function vol proof simply count number solutions redistribution equation pr easily derive piecewise linear vol function represented figure 1 know build communication tables section 2 still derive schedule way organize communications efficiently possible 43 communication schedule 431 communication model according previous discussion concentrate schedules composed several successive steps step sender send one message symmetrically receiver receive one message give formal definition schedule follows definition 4 consider redistribution parameters r p q ffl communication grid p theta q table nonzero entry lengthp q position p q p send message q ffl communication step collection pairs lengthp communication step complete senders receivers active incomplete otherwise cost communication step maximum value entries words maxflengthp ffl schedule succession communication steps nonzero entry communication grid appears one one steps cost schedule may evaluated two ways 1 number steps ns simply number communication steps schedule 2 total cost tc sum cost communication step defined communication grid illustrated tables section 2 summarizes length required communications single slice vector vector size qs motivation evaluating schedules via number steps via total cost follows ffl number steps ns number synchronizations required implement sched ule roughly estimate communication step involving processors permutation measure unit number steps good evaluation cost redistribution ffl may try precise step several messages different lengths exchanged duration step likely related longest length messages simple model would state cost step ff ff startup time inverse bandwidth physical communication link although expression take hot spots link contentions account proven useful variety machines 4 6 cost redistribution according formula affine expression ff theta ns motivates interest number steps total cost 432 simple case simple characterization processor pairs class special case r q well p relatively prime proposition 3 assume respectively denote inverses r modulo g proof relatively prime qs hence g therefore inverse r modulo g well defined computed using extended euclid algorithm applied r g similarly inverse modulo g well defined condition easily translates conditions proposition simple case nice solution scheduling problem assume first 1 simply schedule communications class class class composed pq processor pairs equally distributed row column communication grid class exactly q sending processors per row p receiving processors per column direct consequence proposition 3 note g divide p q hypothesis gcdr schedule class want processor g send message processor equivalently look receiving side words processor position p 0 within block g elements must send message processor position q 0 within block g elements done maxpq complete steps messages instance five blocks senders three blocks receivers blocks senders send messages 3 blocks receivers use algorithm generating block permutation ordering communications blocks irrelevant alltoall communication scheme illustrated example 2 scheduling classes leads algorithm messages length given step 1 case simply regroup classes equivalent modulo g proceed summarize discussion following result proposition 4 assume scheduling class successively leads optimal communication scheme terms number steps total cost proof assume without loss generality p q according previous discussion number classes times p number steps class communication steps step schedule messages class k hence length volk times p communication steps composed messages length namely processing given class k 2 0 remark 1 walker otto 21 deal redistribution shown going r kr simplified going technique described section enables us retrieve results 21 44 general case gcds p entries communication grid may evenly distributed rows senders similarly entries communication grid may evenly distributed columns receivers back example 3 5 see table 7 rows communication grid 5 nonzero entries messages rows 10 similarly hence r 3 columns communication grid 6 nonzero entries columns 10 first goal determine maximum number nonzero entries row column communication grid start analyzing distribution class class classk k 2 0 1 processors pairs distributed follows ffl p 0 entries per column q 0 columns grid none remaining columns ffl q 0 entries per row p 0 rows grid none remaining rows proof first let us check since r relatively prime q 0 definition r 0 pq elements per class since classes obtained translation class0 restrict discussing distribution elements class formula lemma 1 states r mod mod p take values multiple 0 r mod q take values multiple r 0 hence result check total number elements note let us illustrate lemma 3 one motivating examples back example 3 elements class located p 0 columns processor grid let us check class1 instance indeed following lemma 3 shows cannot use schedule based classes considering class separately would lead incomplete communication steps rather build communication steps mixing elements several classes order use available processors maximum number elements row column communication grid obvious lower bound number steps schedule processor cannot send receive one message communication step proposition 5 assume otherwise communication grid full use notation lemma 3 1 maximum number mr elements row communication grid 2 maximum number mc elements column communication grid e proof according lemma 1 two elements classk classk row communication grid interval 0 pq necessarily 0 divides p relatively prime u fortiori 0 relatively prime u therefore 0 divides share rows processor grid congruent modulo 0 induces partition classes since exactly q 0 elements per row class since number classes congruent value modulo 0 either b rsgamma1 c rsgamma1 e deduce value mr value mc obtained similarly turns lower bound number steps given lemma 5 indeed achieved theorem 1 assume otherwise communication grid full use notation lemma 3 lemma 5 optimal number steps ns opt schedule proof already know number steps ns schedule greater equal g give constructive proof bound tight derive schedule whose number steps maxfmr mc g borrow material graph theory view communication grid graph set sending processors set receiving processors entry p q communication grid nonzero g bipartite graph edges link vertex p vertex q degree g defined maximum degree vertices g according konigs edge coloring theorem edge coloring number bipartite graph equal degree see 7 vol 2 p1666 berge 2 p 238 means edges bipartite graph partitioned g disjoint edge matchings constructive proof follows repeatedly extract e maximum matching saturates maximum degree nodes iteration existence maximum matching guaranteed see berge 2 p 130 define schedule simply let matchings iteration represent communication steps remark 2 proof theorem 1 gives bound complexity determining optimal number steps best known maximum matching algorithm bipartite graphs due hopcroft karp 9 cost ojv j 5 maxp q iterations construct schedule procedure ojp j 2 construct schedule whose number steps minimal 45 schedule implementation goal twofold designing schedule ffl minimize number steps schedule ffl minimize total cost schedule already explained view communication grid bipartite graph e accurately view edgeweighted bipartite graph edge edge p q length lengthp q message sent processor p processor q adopt following two strategies stepwise specify number steps choose iteration maximum matching saturates nodes maximum degree since free select matchings natural idea select among matchings one maximum weight weight matching defined sum weight edges greedy specify total cost adopt greedy heuristic selects maximum weighted matching step might end schedule ns opt steps whose total cost less implement approaches rely linear programming framework see 7 chapter 30 let jv j theta jej incidence matrix g ae 1 edge j incident vertex since g bipartite totally unimodular square submatrix determinant 0 1 gamma1 matching polytope g set vectors x 2 q jej ae intuitively selected matching polyhedron determined equation 7 integral rewrite set vectors x 2 q jej find maximum weighted matching look x c 2 n jej weight vector choose greedy strategy simply repeat search maximum weighted matching communications done choose stepwise strategy ensure iteration vertices maximum degree saturated task difficult vertex v maximum degree position replace constraint ax translates number maximum degree vertices 2 f0 1g jv j whose entry position 1 iff ith vertex maximum degree note either case polynomial method matching polyhedron integral solve rational linear problem guaranteed find integer solutions see fact greedy strategy better stepwise strategy terms total cost consider following example example 5 consider redistribution problem 3 communication grid given table 14 stepwise strategy illustrated table 15 number steps equal 10 optimal total cost 20 see table 16 greedy strategy requires steps namely 12 see table 17 total cost see table 18 table 14 communication grid message lengths indicated vector x size msg nbr msg 451 comparison walker ottos strategy walker otto 21 deal redistribution know going r kr simplified going apply results section 432 see remark 1 general case evenly distributed among columns communication grid r 1 necessarily among rows however rows total number nonzero elements 0 divides words bipartite graph regular since maximum matching perfect matching messages length lengthp p q communication grid consequence stepwise strategy lead optimal schedule terms number steps total cost note ns opt k hypotheses walker otto using notation lemma 5 note result applies graph regular entries communication grid equal following theorem extends walker otto main result 21 table 15 communication steps stepwise strategy stepwise strategy table communication costs stepwise strategy stepwise strategy step b c e f g h j total cost proposition 6 consider redistribution problem arbitrary p q schedule generated stepwise strategy optimal terms number steps total cost strategy presented article makes possible directly handle redistribution arbitrary cyclicr arbitrary cyclics contrast strategy advocated walker otto requires two redistributions one cyclicr cycliclcmrs second one cycliclcmrs cyclics 5 mpi experiments section presents results runs intel paragon redistribution algorithm described section 4 table 17 communication steps greedy strategy greedy strategy table communication costs greedy strategy greedy strategy step b c e f g h j k l total cost 51 description experiments executed intel paragon xps 5 computer c program calling routines mpi library mpi chosen portability reusability reasons schedules composed steps step generates one send andor one receive per processor hence used onetoone communication primitives mpi main objective comparison new scheduling strategy current redistribution algorithm scalapack 15 namely caterpillar algorithm briefly summarized section 32 run scheduling algorithm proceed follows 1 compute schedule steps using results section 4 2 pack communication buffers 3 carry barrier synchronization 4 start timer 5 execute communications using redistribution algorithm resp caterpillar algorithm 6 stop timer 7 unpack buffers maximum timers taken processors emphasize take cost message generation account compare communication costs instead caterpillar algorithm could used mpi alltoallv communication primitive turns caterpillar algorithm leads better performance mpi alltoallv experiments difference roughly 20 short vectors 5 long vectors use physical processors input output processor grid results sensitive grid disjoint grids senders receivers 52 results three experiments presented first two experiments use schedule presented section 432 optimal terms number steps ns total cost tc third experiment uses schedule presented section 44 optimal terms ns back example 1 first experiment corresponds example 1 5 redistribution schedule requires 7 steps see table 3 since messages length theoretical improvement caterpillar algorithm 16 steps 716 044 figure 2 shows significant difference two execution times theoretical ratio obtained small vectors eg size 1200 doubleprecision reals result surprising startup times dominate cost small vectors larger vectors ratio varies 056 064 due contention problems scheduler needs 7 step step generates 16 communications whereas 16 steps caterpillar algorithm generates fewer communications 6 8 per step thereby generating less contention back example 2 second experiment corresponds example 2 redistribution schedule requires 16 steps total cost 6 caterpillar algorithm requires 16 steps step least one processor sends message length proportional 7 hence total cost 112 theoretical gain 77112 069 expected long vectors startup times obtain anything better 086 contentions experiments ibm sp2 network workstations would likely lead favorable ratios back example 4 third experiment corresponds example 4 experiment similar first one redistribution schedule requires much fewer steps caterpillar 12 two differences however p 6 q algorithm guaranteed optimal terms total cost instead obtaining theoretical ratio 412 033 obtain results close 06 explain need take closer look caterpillar algorithm shown table 19 6 12 steps caterpillar algorithm indeed empty steps theoretical ratio rather 46 066 global size redistributed vector 64bit double precision500015000 microseconds caterpillar optimal scheduling figure 2 comparing redistribution times intel paragon table 19 communication costs caterpillar schedule caterpillar step b c e f g h j k l total cost 6 conclusion article extended walker ottos work order solve general redistribution problem moving cyclicr distribution p processor grid cyclics distribution qprocessor grid values redistribution parameters p q r constructed schedule whose number steps optimal schedule shown optimal terms total cost particular instances redistribution problem include walker ottos work future work devoted finding schedule optimal terms number steps total cost arbitrary values redistribution problem since problem seems difficult may prove npcomplete another perspective explore use heuristics like greedy algorithm introduced assess performances run experiments generated optimistic results one next releases scalapack library may well include redistribution algorithm presented article global size redistributed vector 64bit double precision40008000microseconds caterpillar optimal scheduling figure 3 time measurement caterpillar greedy schedule different vector sizes redistributed r linear algebra framework static hpf code distribution graphes et hypergraphes generating local addresses communication sets dataparallel programs portable linear algebra library distributed memory computers design issues performance software libraries linear algebra computations high performance computers matrix computations handbook combinatorics compiling array expressions efficient execution distributedmemory machines processor mapping techniques towards efficient data redistribution efficient address generation blockcyclic distributions lineartime algorithm computing memory access sequence dataparallel programs steele jr algorithmic redistribution methods block cyclic decompositions efficient blockcyclic data redistribution mpi complete reference generating communication array state ments design fast address sequence generation dataparallel programs using integer lattices implementation framework hpf distributed arrays messagepassing parallel computer systems redistribution blockcyclic data distributions using mpi redistribution blockcyclic data distributions using mpi runtime performance parallel array assignment empirical study tr ctr prashanth b bhat viktor k prasanna c raghavendra blockcyclic redistribution heterogeneous networks cluster computing v3 n1 p2534 2000 stavros souravlas manos roumeliotis pipeline technique dynamic data transfer multiprocessor grid international journal parallel programming v32 n5 p361388 october 2004 chinghsien hsu shihchang chen chaoyang lan scheduling contentionfree irregular redistributions parallelizing compilers journal supercomputing v40 n3 p229247 june 2007 hyungyoo yook myongsoon park scheduling genblock array redistribution journal supercomputing v22 n3 p251267 july 2002 chinghsien hsu sparse matrix blockcyclic realignment distributed memory machines journal supercomputing v33 n3 p175196 september 2005 minyi guo yi pan improving communication scheduling array redistribution journal parallel distributed computing v65 n5 p553563 may 2005 minyi guo ikuo nakata framework efficient data redistribution distributed memory multicomputers journal supercomputing v20 n3 p243265 november 2001 neungsoo park viktor k prasanna cauligi raghavendra efficient algorithms blockcyclic array redistribution processor sets ieee transactions parallel distributed systems v10 n12 p12171240 december 1999 chinghsien hsu yehching chung donlin yang chyiren dow generalized processor mapping technique array redistribution ieee transactions parallel distributed systems v12 n7 p743757 july 2001 chinghsien hsu yehching chung chyiren dow efficient methods multidimensional array redistribution journal supercomputing v17 n1 p2346 aug 2000 saeri lee hyungyoo yook misoo koo myongsoon park processor reordering algorithms toward efficient genblock redistribution proceedings 2001 acm symposium applied computing p539543 march 2001 las vegas nevada united states chinghsien hsu kunming yu compressed diagonals remapping technique dynamic data redistribution banded sparse matrix journal supercomputing v29 n2 p125143 august 2004 emmanuel jeannot frdric wagner scheduling messages data redistribution experimental study international journal high performance computing applications v20 n4 p443454 november 2006 peizong lee wenyao chen generating communication sets array assignment statements blockcyclic distribution distributed memory parallel computers parallel computing v28 n9 p13291368 september 2002 antoine p petitet jack j dongarra algorithmic redistribution methods blockcyclic decompositions ieee transactions parallel distributed systems v10 n12 p12011216 december 1999 jihwoei huang chihping chu efficient communication scheduling method processor mapping technique applied data redistribution journal supercomputing v37 n3 p297318 september 2006