factorial hidden markov models hidden markov models hmms proven one widely used tools learning probabilistic models time series data hmm information past conveyed single discrete variablethe hidden state discuss generalization hmms state factored multiple state variables therefore represented distributed manner describe exact algorithm inferring posterior probabilities hidden state variables given observations relate forwardbackward algorithm hmms algorithms general graphical models due combinatorial nature hidden state representation exact algorithm intractable intractable systems approximate inference carried using gibbs sampling variational methods within variational framework present structured approximation state variables decoupled yielding tractable algorithm learning parameters model empirical comparisons suggest approximations efficient provide accurate alternatives exact methods finally use structured approximation model bachs chorales show factorial hmms capture statistical structure data set unconstrained hmm cannot b introduction due flexibility simplicity efficiency parameter estimation algorithm hidden markov model hmm emerged one basic statistical tools modeling discrete time series finding widespread application areas speech recognition rabiner juang 1986 computational molecular biology krogh brown mian sjolander haussler 1994 hmm essentially mixture model encoding information history time series value single multinomial variablethe hidden statewhich take one k discrete values multinomial assumption supports efficient parameter estimation algorithmthe baumwelch algorithmwhich considers k settings hidden state time step however multinomial assumption also severely limits representational capacity hmms exam ple represent bits information history time sequence hmm would need distinct states hand hmm distributed state representation could achieve task binary state z ghahramani mi jordan variables williams hinton 1991 paper addresses problem constructing efficient learning algorithms hidden markov models distributed state representations need distributed state representations hmms motivated two ways first representations let model automatically decompose state space features decouple dynamics process generated data second distributed state representations simplify task modeling time series known priori generated interaction multiple looselycoupled processes example speech signal generated superposition multiple simultaneous speakers potentially modeled architecture williams hinton 1991 first formulated problem learning hmms distributed state representations proposed solution based deterministic learning 1 approach presented paper similar williams hintons also viewed framework statistical mechanics mean field theory however learning algorithm quite different makes use special structure hmms distributed state representation resulting significantly efficient learning procedure anticipating results section 3 learning algorithm obviates need twophase procedure boltzmann machines exact step makes use forwardbackward algorithm classical hmms subroutine different approach comes saul jordan 1995 derived set rules computing gradients required learning hmms distributed state spaces however methods applied limited class architectures hidden markov models distributed state representations particular class probabilistic graphical model pearl 1988 lauritzen spiegelhalter 1988 represent probability distributions graphs nodes correspond random variables links represent conditional independence relations relation hidden markov models graphical models recently reviewed smyth heckerman jordan 1997 although exact probability propagation algorithms exist general graphical models jensen lauritzen olesen 1990 algorithms intractable denselyconnected models ones consider paper one approach dealing issue utilize stochastic sampling methods kanazawa et al 1995 another approach provides basis algorithms described current paper make use variational methods cf saul jaakkola jordan 1996 following section define probabilistic model factorial hmms section 3 present algorithms inference learning section 4 describe empirical results comparing exact approximate algorithms learning basis time complexity model quality also apply factorial hmms real time series data set consisting melody lines collection chorales j bach discuss several generalizations probabilistic model factorial hidden markov models 247 t1 t1 2 2 t1 2 t1 b figure 1 directed acyclic graph dag specifying conditional independence relations hidden markov model node conditionally independent nondescendants given parents b dag representing conditional independence relations factorial hmm underlying markov chains section 5 conclude section 6 necessary details derivations provided appendixes 2 probabilistic model begin describing hidden markov model sequence observations modeled specifying probabilistic relation observations sequence hidden states fs g markov transition structure linking hidden states model assumes two sets conditional independence relations independent observations states given independent markov property using independence relations joint probability sequence states observations factored conditional independencies specified equation 1 expressed graphically form figure 1 state single multinomial random variable take one k discrete values kg state transition probabilities specified k theta k transition matrix observations discrete symbols taking one values observation probabilities fully specified k theta observation matrix continuous observation vector p js modeled many different forms gaussian mixture gaussians even neural network 2 present paper generalize hmm state representation letting state represented collection state variables z ghahramani mi jordan take k values refer models factorial hidden markov models state space consists cross product state variables simplicity assume k although results present trivially generalized case differing k given state space factorial hmm consists k combinations variables placing constraints state transition structure would result k theta k transition matrix unconstrained system uninteresting several reasons equivalent hmm k states unlikely discover interesting structure k state variables variables allowed interact arbitrarily time complexity sample complexity estimation algorithm exponential therefore focus factorial hmms underlying state transitions constrained natural structure consider one state variable evolves according dynamics priori uncoupled state variables graphical representation model presented figure 1 b transition structure system represented distinct k theta k matrices generalizations allow coupling state variables briefly discussed section 5 shown figure 1 b factorial hmm observation time step depend state variables time step continuous observations one simple form dependence linear gaussian observation gaussian random vector whose mean linear function state variables represent state variables k theta 1 vectors k discrete values corresponds 1 one position 0 elsewhere resulting probability density theta 1 observation vector ae oe w matrix theta k matrix whose columns contributions means settings c theta covariance matrix 0 denotes matrix transpose j delta j matrix determinant operator one way understand observation model equations 4a 4b consider marginal distribution obtained summing possible states k settings state variables thus factorial hidden markov models 249 k possible mean vectors obtained forming sums columns one column chosen w matrices resulting marginal density thus gaussian mixture model k gaussian mixture components constant covariance matrix c static mixture model without inclusion time index markov dynamics factorial parameterization standard mixture gaussians model interest right zemel 1993 hinton zemel 1994 ghahramani 1995 model considering current paper extends model allowing markov dynamics discrete state variables underlying mixture unless otherwise stated assume gaussian observation model throughout paper hidden state variables one time step although marginally independent become conditionally dependent given observation sequence determined applying semantics directed graphs particular dseparation criterion pearl 1988 graphical model figure 1 b consider gaussian model equations 4a4b given observation vector posterior probability settings hidden state variables proportional probability gaussian mean since function state variables probability setting one state variables depend setting state variables 3 dependency effectively couples hidden state variables purposes calculating posterior probabilities makes exact inference intractable factorial hmm 3 inference learning inference problem probabilistic graphical model consists computing probabilities hidden variables given observations context speech recognition example observations may acoustic vectors goal inference may compute probability particular word sequence phonemes hidden state problem solved efficiently via forwardbackward algorithm rabiner juang 1986 shown special case jensen lauritzen olesen 1990 algorithm probability propagation general graphical models smyth et al 1997 cases rather probability distribution hidden states desirable infer single probable hidden state sequence achieved via viterbi 1967 algorithm form dynamic programming closely related forwardbackward algorithm also analogues graphical model literature dawid 1992 learning problem probabilistic models consists two components learning structure model learning parameters structure learning topic current research graphical model machine learning communities eg heckerman 1995 stolcke omohundro 1993 current paper deal exclusively problem learning parameters given structure z ghahramani mi jordan 31 em algorithm parameters factorial hmm estimated via expectation maximization em algorithm dempster laird rubin 1977 case classical hmms known baumwelch algorithm baum petrie soules weiss 1970 procedure iterates step fixes current parameters computes posterior probabilities hidden states e step step uses probabilities maximize expected log likelihood observations function parameters step since e step em exactly inference problem described subsume discussion inference learning problems description em algorithm factorial hmms em algorithm follows definition expected log likelihood complete observed hidden data qoe new log q function parameters oe new given current parameter estimate oe observation sequence fy g factorial hmm parameters model consists computing q expanding 5 using equations 14b find q expressed function three types expectations hidden state variables hs hdeltai used abbreviate e fdeltajoe fy gg hmm notation rabiner juang 1986 hs corresponds fl vector state occupation probabilities hs corresponds k theta k matrix state occupation probabilities two consecutive time steps hs analogue single underlying markov model step uses expectations maximize q function oe new using jensens inequality baum petrie soules weiss 1970 showed iteration e steps increases likelihood p fy gjoe convergence local optimum hidden markov models exact step factorial hmms simple tractable particular step parameters output model described equations 4a4b found solving weighted linear regression problem similarly steps priors state transition matrices identical ones used baumwelch algorithm details step given appendix turn substantially difficult problem computing expectations required e step 32 exact inference unfortunately exact e step factorial hmms computationally intractable fact best shown making reference standard algorithms prob factorial hidden markov models 251 abilistic inference graphical models lauritzen spiegelhalter 1988 although also derived readily direct application bayes rule consider computations required calculating posterior probabilities factorial hmm shown figure 1 b within framework lauritzen spiegelhalter algorithm moralizing triangulating graphical structure factorial hmm results junction tree fact chain cliques size m1 resulting probability propagation algorithm time complexity otmk m1 single observation sequence length present forwardbackward type recursion implements exact e step appendix b naive exact algorithm consists translating factorial hmm equivalent hmm k states using forwardbackward algorithm time complexity otk 2m like models multiple denselyconnected hidden variables exponential time complexity makes exact learning inference intractable thus although markov property used obtain forwardbackward like factorizations expectations across time steps sum possible configurations hidden state variables within time step unavoid able intractability due inherently cooperative nature model gaussian output model example settings state variables one time step cooperate determining mean observation vector 33 inference using gibbs sampling rather computing exact posterior probabilities one approximate using monte carlo sampling procedure thereby avoid sum exponentially many state patterns cost accuracy although many possible sampling schemes review see neal 1993 present one simplestgibbs sampling geman geman 1984 given observation sequence fy g procedure starts random setting hidden states fs g step sampling process state vector updated stochastically according probability distribution conditioned setting state vectors graphical model useful node conditionally independent nodes given markov blanket defined set children parents parents children node sample typical state variable need examine states neighboring nodes sampled p sampling tm hidden variables model results new sample hidden state model requires otmk operations sequence overall states resulting pass gibbs sampling defines markov chain state space model assuming probabilities bounded away zero markov chain guaranteed converge z ghahramani mi jordan posterior probabilities states given observations geman geman 1984 thus suitable time samples markov chain taken approximate samples posterior probabilities first secondorder statistics needed estimate hs collected using states visited probabilities estimated sampling process used approximate e step em 4 34 completely factorized variational inference also exists second approximation posterior probability hidden states tractable deterministic basic idea approximate posterior distribution hidden variables p fs gjfy g tractable distribution qfs g approximation provides lower bound log likelihood used obtain efficient learning algorithm argument formalized following reasoning saul jaakkola jordan 1996 distribution hidden variables qfs g used define lower bound log likelihood log log qfs g log made use jensens inequality last step difference lefthand side righthand side inequality given kullbackleibler divergence cover thomas 1991 qfs g log complexity exact inference approximation given q determined conditional independence relations parameters thus chose q tractable structurea graphical representation eliminates dependencies p given structure free vary parameters q obtain tightest possible bound minimizing 6 refer general strategy using parameterized approximating distribution variational approximation refer free parameters distribution variational parameters illustrate consider simplest variational approximation state variables assumed independent given observations distribution written factorial hidden markov models 253 2 2 2 2 2 2 b figure 2 completely factorized variational approximation assumes state variables independent conditional observation sequence b structured variational approximation assumes state variables retain markov structure within chain independent across chains qs variational parameters g means state variables state variable represented kdimensional vector 1 k th position 0 elsewhere th markov chain state k time elements vector therefore define state occupation probabilities multinomial variable distribution q qs tk tk completely factorized approximation often used statistical physics provides basis simple yet powerful mean field approximations statistical mechanical systems parisi 1988 make bound tight possible vary separately observation sequence minimize kl divergence taking derivatives 6 respect setting zero obtain set fixed point equations see appendix c defined new ae oe residual error given predictions state variables including 6m z ghahramani mi jordan delta vector diagonal elements w 0 c fdeltag softmax operator maps vector vector b size elements log p denotes elementwise logarithm transition matrix p first term 9a projection error reconstructing observation onto weights state vector mthe particular setting state vector reduce error larger associated variational parameter second term arises fact second order correlation hs evaluated variational distribution diagonal matrix composed elements last two terms introduce dependencies forward backward time 5 therefore although posterior distribution hidden variables approximated completely factorized distribution fixed point equations couple parameters associated node parameters markov blanket sense fixed point equations propagate information along pathways defining exact algorithms probability propagation following may provide intuitive interpretation approximation made distribution given particular observation sequence hidden state variables markov chains time step stochastically coupled stochastic coupling approximated system hidden variables uncorrelated coupled means variational meanfield equations solve deterministic coupling means best approximates stochastically coupled system hidden state vector updated turn using 9a time complexity otmk 2 per iteration convergence determined monitoring kl divergence variational distribution successive time steps practice convergence rapid 2 10 iterations 9a fixed point equations converged expectations required e step obtained simple function parameters equations c6c8 appendix c 35 structured variational inference approximation presented previous section factors posterior probability state variables statistically independent contrast rather extreme factorization exists third approximation tractable preserves much probabilistic structure original system scheme factorial hmm approximated uncoupled hmms shown figure b within hmm efficient exact inference implemented via forwardbackward algorithm approach exploiting tractable substructures first suggested machine learning literature saul jordan 1996 factorial hidden markov models 255 note arguments presented previous section hinge form approximating distribution therefore structured variational approximations obtained using structured variational distributions q q provides lower bound log likelihood used obtain learning algorithm write structured variational approximation qs qs zq normalization constant ensuring q integrates one qs qs tk tk tk tk last equality follows fact vector 1 one position 0 elsewhere parameters distribution gthe original priors state transition matrices factorial hmm timevarying bias state variable comparing equations 11a11c equation 1 see k theta 1 vector h plays role probability observation p js 1 k settings example qs 1joe corresponds observation time state 1j intuitively approximation uncouples markov chains attaches state variable distinct fictitious observation probability fictitious observation varied minimize kl divergence q p applying arguments obtain set fixed point equations h minimize klqkp detailed appendix h new ae oe delta defined redefine residual error 6m z ghahramani mi jordan parameter h obtained fixed point equations observation probability associated state variable hidden markov model using probabilities forwardbackward algorithm used compute new set expectations hs fed back 12a 12b algorithm therefore used subroutine minimization kl divergence note similarity equations 12a12b equations 9a9b completely factorized system completely factorized system since hs fixed point equations written explicitly terms variational parameters structured approximation dependence hs h computed via forwardbackward algorithm note also 12a contain terms involving prior transition matrix p terms cancelled choice approximation 36 choice approximation theory em algorithm presented dempster et al 1977 assumes use exact e step models exact e step intractable one must instead use approximation like described choice among approximations must take account several theoretical practical issues monte carlo approximations based markov chains gibbs sampling offer theoretical assurance sampling procedure converge correct posterior distribution limit although means one come arbitrarily close exact e step practice convergence slow especially multimodal distributions often difficult determine close one convergence however sampling used e step em time tradeoff number samples used number em iterations seems wasteful wait convergence early learning posterior distribution samples drawn far posterior given optimal parameters practice found even approximate steps using gibbs samples eg around ten samples hidden variable tend increase true likelihood variational approximations offer theoretical assurance lower bound likelihood maximized minimization kl divergence e step parameter update step guaranteed decrease lower bound therefore convergence defined terms bound alternative view given neal hinton 1993 describes em terms negative free energy f function parameters oe observations posterior probability distribution hidden variables qs eq denotes expectation using distribution qs exact e step em maximizes f respect q given oe variational e steps used factorial hidden markov models 257 maximize f respect q given oe subject constraint q particular tractable form given view seems clear structured approximation preferable completely factorized approximation since places fewer constraints q cost tractability 4 experimental results investigate learning inference factorial hmms conducted two experi ments first experiment compared different approximate exact methods inference basis computation time likelihood model obtained synthetic data second experiment sought determine whether decomposition state space factorial hmms presents advantage modeling real time series data set might assume complex internal structurebachs chorale melodies 41 experiment 1 performance timing benchmarks using data generated factorial hmm structure underlying markov models k states compared time per em iteration training test set likelihoods five models ffl hmm trained using baumwelch algorithm ffl factorial hmm trained exact inference e step using straightforward application forwardbackward algorithm rather efficient algorithm outlined appendix b ffl factorial hmm trained using gibbs sampling e step number samples fixed 10 samples per variable 6 ffl factorial hmm trained using completely factorized variational approxima tion ffl factorial hmm trained using structured variational approximation factorial hmms consisted underlying markov models k states whereas hmm k states data generated factorial hmm structure state variables could take k discrete values parameters model except output covariance matrix sampled uniform 0 1 distribution appropriately normalized satisfy sumtoone constraints transition matrices priors covariance matrix set multiple identity matrix training test sets consisted 20 sequences length 20 observable fourdimensional vector randomly sampled set parameters separate training set test set generated algorithm run z ghahramani mi jordan fifteen sets parameters generated four problem sizes algorithms run maximumof 100 iterations em convergence defined iteration k log likelihood lk approximate log likelihood approximate algorithm used satisfied end learning log likelihoods training test set computed models using exact algorithm also included comparison log likelihood training test sets true model generated data test set log likelihood n observation sequences defined log p n obtained maximizing log likelihood training set disjoint test set provides measure well model generalizes novel observation sequence distribution training data results averaged 15 runs algorithm four problem sizes total 300 runs presented table 1 even smallest problem size standard hmm k states suffers overfitting test set log likelihood significantly worse training set log likelihood expected overfitting problem becomes worse size state space increases particularly serious factorial hmms log likelihoods three approximate em algorithms compared exact algorithm gibbs sampling appeared poorest performance three smaller size problems log likelihood significantly worse exact algorithm training sets test sets p 005 may due insufficient sampling however soon see running gibbs sampler 10 samples potentially improving performance makes substantially slower variational methods surprisingly gibbs sampler appears quite well largest size problem although differences methods statistically significant performance completely factorized variational approximation statistically significantly different exact algorithm either training set test set problem sizes performance structured variational approximation statistically different exact method three four problem sizes appeared better one problem sizes 005 although result may fluke arising random variability another interesting speculative explanation exact em algorithm implements unconstrained maximization f defined section 36 variational methods maximize f subject constrained distribution constraints could presumably act regularizers reducing overfitting large amount variability final log likelihoods models learned algorithms subtracted log likelihood true generative model trained model eliminate main effect randomly sampled generative model reduce variability due training test sets one important remaining source variance random seed used factorial hidden markov models 259 table 1 comparison factorial hmm four problems varying size negative log likelihood training test set plus minus one standard deviation shown problem size algorithm measured bits per observation log likelihood bits divided nt relative log likelihood true generative model data set 7 true true generative model log likelihood per symbol defined zero model measure hmm hidden markov model k states exact factorial hmm trained using exact e step gibbs factorial hmm trained using gibbs sampling cfva factorial hmm trained using completely factorized variational approximation sva factorial hmm trained using structured variational approximation k algorithm training test hmm 119 sigma 067 229 sigma 102 exact 088 sigma 080 105 sigma 072 gibbs 167 sigma 123 178 sigma 122 cfva 106 sigma 120 120 sigma 111 sva 091 sigma 102 104 sigma 101 hmm 076 sigma 067 981 sigma 255 exact 102 sigma 104 126 sigma 099 gibbs 221 sigma 091 250 sigma 087 cfva 124 sigma 150 150 sigma 153 exact 229 sigma 119 251 sigma 121 gibbs 325 sigma 117 335 sigma 114 cfva 173 sigma 134 207 sigma 174 exact 423 sigma 228 449 sigma 224 gibbs 363 sigma 113 395 sigma 114 cfva 485 sigma 068 514 sigma 069 260 z ghahramani mi jordan iterations em log likelihood bits iterations em log likelihood bits b iterations em log likelihood bits c iterations em log likelihood bits figure 3 learning curves five runs four learning algorithms factorial hmms exact b completely factorized variational approximation c structured variational approx imation gibbs sampling single training set sampled size used runs solid lines show negative log likelihood per observation bits relative true model generated data calculated using exact algorithm circles denote point convergence criterion met run ended three approximate algorithms dashed lines show approximate negative log likelihood 8 training run determined initial parameters samples used gibbs algorithm algorithms appeared sensitive random seed suggesting different runs training set found different local maxima plateaus likelihood figure 3 variability could eliminated explicitly adding regularization term viewed prior parameters maximuma posteriori parameter estimation alternatively bayesian ensemble methods could used average variability integrating parameter space timing comparisons confirm fact standard hmm exact extremely slow models large state spaces fig factorial hidden markov models 261 timeiteration hmm figure 4 time per iteration em silicon graphics r4400 processor running matlab ure 4 gibbs sampling slower variational methods even limited ten samples hidden variable per iteration em since one pass variational fixed point equations time complexity one pass gibbs sampling since variational fixed point equations found converge quickly experiments suggest gibbs sampling competitive timewise variational methods time per iteration variational methods scaled well large state spaces 42 experiment 2 bach chorales musical pieces naturally exhibit complex structure many different time scales furthermore one imagine represent state musical piece given time would necessary specify conjunction many different features reasons chose test whether factorial hmm would provide advantage regular hmm modeling collection musical pieces data set consisted discrete event sequences encoding melody lines j bachs chorales obtained uci repository machine learning originally discussed conklin witten 1995 event sequence represented six attributes described table 2 sixtysix chorales 40 events divided training set 30 chorales test set 36 chorales using first set hidden markov models state space ranging 2 100 states trained convergence 30 sigma 12 steps em factorial hmms varying sizes k ranging 2 6 ranging 2 also trained data 262 z ghahramani mi jordan table 2 attributes bach chorale data set key signature time signature attributes constant duration chorale attributes treated real numbers modeled lineargaussian observations 4a attribute description representation pitch pitch event int 0 127 fermata event fermata binary st start time event int 116 notes dur duration event int 116 notes approximate e step factorial hmms used structured variational ap proximation choice motivated three considerations first size state space wished explore exact algorithms prohibitively slow second gibbs sampling algorithm appear present advantages speed performance required heuristic method determining number samples third theoretical arguments suggest structured approximation general superior completely factorized variational approximation since dependencies original model preserved test set log likelihoods hmms shown figure 5 exhibited typical ushaped curve demonstrating tradeoff bias variance ge man bienenstock doursat 1992 hmms fewer 10 states predict well hmms 40 states overfit training data therefore provided poor model test data 75 runs highest test set log likelihood per observation gamma90 bits obtained hmm hidden states 9 factorial hmm provides satisfactory model chorales three points view first time complexity possible consider models significantly larger state spaces particular fit models 1000 states second given componential parametrization factorial hmm large state spaces require excessively large numbers parameters relative number data points particular saw evidence overfitting even largest factorial hmm seen figures 5 c finally approach resulted significantly better predictors test set likelihood best factorial hmm order magnitude larger test set likelihood best hmm figure 5 reveals factorial hmm clearly better predictor single hmm acknowledged neither approach produces models easily interpretable musicological point view situation reminiscent speech recognition hmms proved value predictive models speech signal without necessarily viewed causal generative models speech factorial hmm clearly impoverished representation factorial hidden markov models 263 log likelihood bits b size state space c figure 5 test set log likelihood per event bach chorale data set function number states hmms factorial hmms b dashed line line symbol represents single run lines indicate mean performances thin dashed line bd indicates log likelihood per observation best run factorial hmms trained using structured approximation methods true likelihood computed using exact algorithm musical structure promising performance predictor provides hope could serve step way toward increasingly structured statistical models music complex multivariate time series 5 generalizations model section describe four variations generalizations factorial hmm 51 discrete observables probabilistic model presented paper assumed realvalued gaussian observations one advantages arising assumption conditional density ddimensional observation p js 1 compactly specified mean matrices dimension theta k one theta covariance matrix furthermore step model reduces set weighted least squares equations model generalized handle discrete observations several ways consider single dvalued discrete observation analogy traditional hmms output probabilities could modeled using matrix however case factorial hmm matrix would theta k entries combination state variables observation thus compactness representation would entirely lost standard methods graphical models suggest approximating large matrices noisyor pearl 1988 sigmoid neal 1992 models interaction example softmax model generalizes sigmoid model 2 p js 1 multinomial mean proportional 264 z ghahramani mi jordan exp like gaussian model specification com pact using matrices size theta k lineargaussian model w overparametrized since model overall mean shown appendix nonlinearity induced softmax function makes e step step algorithm difficult iterative numerical methods used step whereas gibbs sampling variational methods used e step see neal 1992 hinton et al 1995 saul et al 1996 discussions different approaches learning sigmoid networks 52 introducing couplings architecture factorial hmms presented section 2 assumes underlying markov chains interact observations constraint relaxed introducing couplings hidden state variables cf saul jordan 1997 example depends equation 3 replaced following factorization similar exact variational gibbs sampling procedures defined architecture however note couplings must introduced caution may result exponential growth parameters example factorization requires transition matrices size k 2 theta k rather specifying higherorder couplings probability transition matrices one introduce secondorder interaction terms energy log probability function terms effectively couple chains without number parameters incurred full probability transition matrix graphical model formalism correspond symmetric undirected links making model chain graph jensen lauritzen olesen 1990 algorithm still used propagate information exactly chain graphs undirected links cause normalization constant probability distributionthe partition functionto depend coupling parameters boltzmann machines hinton sejnowski 1986 clamped unclamped phase therefore required learning goal unclamped phase compute derivative partition function respect parameters neal 1992 53 conditioning inputs like hidden markov model factorial hmm provides model unconditional density observation sequences certain problem domains observations better thought inputs explanatory variables factorial hidden markov models 265 others outputs response variables goal cases model conditional density output sequence given input sequence machine learning terminology unconditional density estimation unsupervised conditional density estimation supervised several algorithms learning hidden markov models conditioned inputs recently presented literature cacciatore nowlan 1994 bengio frasconi 1995 meila jordan 1996 given sequence input vectors g probabilistic model inputconditioned factorial hmm theta model depends specification p js conditioned discrete state variable possibly con tinuous input vector approach used bengio frasconis input output hmms iohmms suggests modeling p separate neural networks one setting decomposition ensures valid probability transition matrix defined point input space sumtoone constraint eg softmax nonlinearity used output networks using decomposition conditional probability k networks inference inputconditioned factorial hmms straightforward generalization algorithms presented factorial hmms exact forwardbackward algorithm appendix b adapted using appropriate conditional probabilities similarly gibbs sampling procedure complex conditioned inputs finally completely factorized structured approximations also generalized readily approximating distribution dependence input similar models probability transition structure decomposed complex dependence previous state variable input inference may become considerably complex depending form input conditioning maximization step learning may also change considerably general output transition probabilities modeled neural networks step longer solved exactly gradientbased generalized em algorithm must used loglinear models step solved using inner loop iteratively reweighted leastsquares mccullagh nelder 1989 54 hidden markov decision trees interesting generalization factorial hmms results one conditions input x orders state variables depends n 266 z ghahramani mi jordan 2 2 2 figure 6 hidden markov decision tree figure 6 resulting architecture seen probabilistic decision tree markovian dynamics linking decision variables consider probabilistic model would generate data first time step given top node 1 take k values stochastically partitions x space k decision regions next node hierarchy 2 subdivides regions k subregions output 1 generated input x 1 kway decisions hidden nodes next time step similar procedure used generate data model except decision tree dependent decision taken node previous time step thus hierarchical mixture experts architecture jordan jacobs 1994 generalized include markovian dynamics decisions hidden markov decision trees provide useful starting point modeling time series temporal spatial structure multiple resolutions explore generalization factorial hmms jordan ghahramani saul 1997 6 conclusion paper examined problem learning class generalized hidden markov models distributed state representations generalization provides richer modeling tool method incorporating prior structural information state variables underlying dynamics system generating data although exact inference class models generally intractable provided structured variational approximation computed tractably approximation forms basis expectation step em algorithm learning parameters model empirical comparisons several approximations exact algorithm show approximation efficient compute accurate finally shown factorial hidden markov models 267 factorial hmm representation provides advantage traditional hmms predictive modeling complex temporal patterns bachs chorales appendix step step equations parameter obtained setting derivatives q respect parameters zero start expanding q using equations 14b tr c tr log p hs log z a1 tr trace operator square matrices z normalization term independent states observations ensuring probabilities sum one setting derivatives q respect output weights zero obtain linear system equations w 0 a2 assuming dtheta1 vector let mk theta1 vector obtained concatenating vectors w theta mk matrix obtained concatenating w matrices size theta k solving a2 results moorepenrose pseudoinverse note model overparameterized since theta 1 means w matrices add single mean using pseudoinverse removes need explicitly subtract overall mean w estimate separately another parameter estimate priors solve q subject constraint sum one obtaining 268 z ghahramani mi jordan similarly estimate transition matrices solve qp subject constraint columns p sum one element new finally reestimation equations covariance matrix derived taking derivatives respect c gamma1 first term arises normalization gaussian density function z proportional jcj t2 jcjc substituting a2 reorganizing get equations reduce baumwelch reestimation equations hmms gaussian observables step presented case single observation sequence extension multiple sequences straightforward appendix exact forwardbackward algorithm specify exact forwardbackward recursion computing posterior probabilities hidden states factorial hmm differs straightforward application forwardbackward algorithm equivalent k state hmm depend k theta k transition matrix rather makes use independence underlying markov chains sum transition matrices size k theta k using notation fy g r mean observation sequence ff 1 ff factorial hidden markov models 269 obtain forward recursions end forward recursions likelihood observation sequence sum k elements ff similarly obtain backward recursions define obtain posterior probability state time obtained multiplying ff algorithm shown equivalent jensen lauritzen olesen algorithm probability propagation graphical models probabilities defined collections state variables corresponding cliques equivalent junction tree information passed forwards backwards summing sets separating neighboring clique tree results forwardbackwardtype recursions order otmk m1 using ff fi fl quantities statistics required e step z ghahramani mi jordan appendix completely factorized variational approximation using definition probabilistic model given equations 14b posterior probability states given observation sequence written z z normalization constant ensuring probabilities sum one similarly probability distribution given variational approximation 7 8 written expfgammah log using notation denoting expectation respect variational distribution using angular brackets hdeltai kl divergence three facts verified definition variational approximation diagf factorial hidden markov models 271 diag operator takes vector returns square matrix elements vector along diagonal zeros everywhere else kl divergence therefore expanded log c tr c trf log p taking derivatives respect obtain log c gammalog delta vector diagonal elements w 0 c c term arising log zq ensuring sum one setting derivative equal 0 solving gives equation 9a appendix structured approximation structured approximation hq defined log h using c2 write kl divergence tr c tr c diag log z d2 z ghahramani mi jordan since kl independent p first thing note parameters structured approximation remain equal equivalent parameters true system taking derivatives respect log h n get log h n log h 6m c log h n last term obtained making use fact log zq log h n cancels first term setting terms inside brackets d3 equal zero yields equation 12a acknowledgments thank lawrence saul helpful discussions geoffrey hinton support project supported part grant mcdonnellpew foundation grant atr human information processing research laboratories gift siemens corporation grant n000149410777 office naval research zoubin ghahramani supported grant ontario information technology research centre notes 1 related work inference distributed state hmms see dean kanazawa 1989 2 speech neural networks generally used model p jy probability converted observation probabilities needed hmm via bayes rule 3 columns w w n orthogonal every pair state variables n c diagonal covariance matrix state variables longer dependent given observation case explaining away state variable modeling variability observation along different subspace 4 bayesian treatment learning problem parameters also considered hidden random variables handled gibbs sampling replacing step sampling conditional distribution parameters given hidden variables example see tanner wong 1987 5 first term replaced log second term appear 6 samples used learning samples discarded beginning run although ten samples even approach convergence provides runtime roughly comparable variational methods goal see whether impatient gibbs sampler would able compete approximate methods factorial hidden markov models 273 7 lower values suggest better probabilistic model value one example means would take one bit true generative model code observation vector standard deviations reflect variation due training set test set random seed algorithm standard errors mean factor 38 smaller 8 variational methods dashed lines equal minus lower bound log likelihood except normalization term intractable compute vary learning resulting apparent occasional increases bound 9 since attributes modeled real numbers log likelihoods measure relative coding cost comparisons likelihoods meaningful whereas obtain absolute cost coding sequence necessary specify discretization level 10this analogous fullyconnected boltzmann machine n units hinton sejnowski 1986 every binary unit coupled every unit using 2 parameters rather o2 n parameters required specify complete probability table r maximization technique occurring statistical analysis probabilistic functions markov chains inputoutputhmm architecture mixtures controllers jump linear nonlinear plants multiple viewpoint systems music prediction elements information theory applications general propagation algorithm probabilistic expert systems model reasoning persistence causation maximum likelihood incomplete data via em algorithm neural networks biasvariance dilemma stochastic relaxation factorial learning em algorithm learning relearning boltzmann machines bayesian updating recursive graphical models local computations hierarchical mixtures experts em algorithm neural computation stochastic simulation algorithms dynamic probabilistic networks hidden markov models computational biology applications protein modeling local computations probabilities graphical structures application expert systems generalized linear models learning fine motion markov mixtures experts uci repository machine learning databases connectionist learning belief networks probabilistic inference using markov chain monte carlo methods technical report crgtr931 new view em algorithm justifies incremental variants statistical field theory probabilistic reasoning intelligent systems networks plausible inference ca morgan kaufmann introduction hidden markov models mixed memory markov models mean field theory sigmoid belief networks journal artificial intelligence research boltzmann chains hidden markov models exploiting tractable substructures intractable networks probabilistic independence networks hidden markov probability models hidden markov model induction bayesian model merging calculation posterior distributions data augmentation discussion bounds convolutional codes asymptotically optimal decoding algorithm mean field networks learn discriminate temporally distorted strings minimum description length framework unsupervised learning received tr probabilistic reasoning intelligent systems networks plausible inference model reasoning persistence causation learning relearning boltzmann machines elements information theory connectionist learning belief networks neural networks biasvariance dilemma hierarchical mixtures experts em algorithm probabilistic independence networks hidden markov probability models hidden markov model induction bayesian model merging minimum description length framework unsupervised learning ctr p xing michael jordan stuart russell graph partition strategies generalized mean field inference proceedings 20th conference uncertainty artificial intelligence p602610 july 0711 2004 banff canada ricardo silva jiji zhang james g shanahan probabilistic workflow mining proceeding eleventh acm sigkdd international conference knowledge discovery data mining august 2124 2005 chicago illinois usa andrew howard tony jebara dynamical systems trees proceedings 20th conference uncertainty artificial intelligence p260267 july 0711 2004 banff canada raul fernandez rosalind w picard modeling drivers speech stress speech communication v40 n12 p145159 april robert jacobs wenxin jiang martin tanner factorial hidden markov models generalized backfitting algorithm neural computation v14 n10 p24152437 october 2002 terry caelli andrew mccabe garry briscoe shape tracking production using hidden markov models hidden markov models applications computer vision world scientific publishing co inc river edge nj 2001 agnieszka betkowska koichi shinoda sadaoki furui robust speech recognition using factorial hmms home environments eurasip journal applied signal processing v2007 n1 p1010 1 january 2007 yunhua hu hang li yunbo cao dmitriy meyerzon qinghua zheng automatic extraction titles general documents using machine learning proceedings 5th acmieeecs joint conference digital libraries june 0711 2005 denver co usa yunhua hu hang li yunbo cao li teng dmitriy meyerzon qinghua zheng automatic extraction titles general documents using machine learning information processing management international journal v42 n5 p12761293 september 2006 tony jebara risi kondor andrew howard probability product kernels journal machine learning research 5 p819844 1212004 fine yoram singer naftali tishby hierarchical hidden markov model analysis applications machine learning v32 n1 p4162 july 1998 charles sutton khashayar rohanimanesh andrew mccallum dynamic conditional random fields factorized probabilistic models labeling segmenting sequence data proceedings twentyfirst international conference machine learning p99 july 0408 2004 banff alberta canada wang nanning zheng yan li yingqing xu heungyung shum learning kernelbased hmms dynamic sequence synthesis graphical models v65 n4 p206221 july jie tang hang li yunbo cao zhaohui tang email data cleaning proceeding eleventh acm sigkdd international conference knowledge discovery data mining august 2124 2005 chicago illinois usa cen li gautam biswas bayesian approach structural learning hidden markov models scientific programming v10 n3 p201219 august 2002 sophie deneve bayesian spiking neurons inference neural computation v20 n1 p91117 january 2008 lawrence k saul michael jordan mixed memory markov models decomposing complex stochastic processes mixtures simpler ones machine learning v37 n1 p7587 oct 1999 yong cao petros faloutsos frdric pighin unsupervised learning speech motion editing proceedings acm siggrapheurographics symposium computer animation july 2627 2003 san diego california hung h bui svetha venkatesh geoff west tracking surveillance widearea spatial environments using abstract hidden markov model hidden markov models applications computer vision world scientific publishing co inc river edge nj 2001 r anderson pedro domingos daniel weld relational markov models application adaptive web navigation proceedings eighth acm sigkdd international conference knowledge discovery data mining july 2326 2002 edmonton alberta canada tom ingliar milo hauskrecht noisyor component analysis application link analysis journal machine learning research 7 p21892213 1212006 martin v butz kernelbased ellipsoidal conditions realvalued xcs classifier system proceedings 2005 conference genetic evolutionary computation june 2529 2005 washington dc usa ying wu thomas huang robust visual tracking integrating multiple cues based coinference learning international journal computer vision v58 n1 p5571 june 2004 michael jordan zoubin ghahramani tommi jaakkola lawrence k saul introduction variational methods graphical models machine learning v37 n2 p183233 nov11999 andrea torsello antonio robleskelly edwin r hancock discovering shape classes using tree editdistance pairwise clustering international journal computer vision v72 n3 p259285 may 2007 charles sutton andrew mccallum khashayar rohanimanesh dynamic conditional random fields factorized probabilistic models labeling segmenting sequence data journal machine learning research 8 p693723 512007 h attias independent factor analysis neural computation v11 n4 p803851 may 15 1999 jinhai cai zhiqiang liu hidden markov models spectral features 2d shape recognition ieee transactions pattern analysis machine intelligence v23 n12 p14541458 december 2001 john binder daphne koller stuart russell keiji kanazawa adaptive probabilistic networks hidden variables machine learning v29 n23 p213244 novdec 1997 xiangdong dawn jutla nick cercone privacy intrusion detection using dynamic bayesian networks proceedings 8th international conference electronic commerce new ecommerce innovations conquering current barriers obstacles limitations conducting successful business internet august 1316 2006 fredericton new brunswick canada akio utsugi ensemble independent factor analyzers application natural image analysis neural processing letters v14 n1 p4960 august 2001 zoubin ghahramani introduction hidden markov models bayesian networks hidden markov models applications computer vision world scientific publishing co inc river edge nj 2001 cristian sminchisescu atul kanaujia dimitris metaxas conditional models contextual human motion recognition computer vision image understanding v104 n2 p210220 november 2006 hichem snoussi ali mohammaddjafari bayesian unsupervised learning source separation mixture gaussians prior journal vlsi signal processing systems v37 n23 p263279 junejuly 2004 inna stainvas david lowe generative probabilistic oriented wavelet model texture segmentation neural processing letters v17 n3 p217238 june russell greiner christian darken n iwan santoso efficient reasoning acm computing surveys csur v33 n1 p130 march 2001