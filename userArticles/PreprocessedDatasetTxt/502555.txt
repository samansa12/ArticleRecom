generalized clustering supervised learning data assignment clustering algorithms become increasingly important handling analyzing data considerable work done devising effective increasingly specific clustering algorithms contrast developed generalized framework accommodates diverse clustering algorithms systematic way framework views clustering general process iterative optimization includes modules supervised learning instance assignment framework also suggested several novel clustering methods paper investigate experimentally efficacy algorithms test hypotheses relation unsupervised techniques supervised methods embedded b introduction motivation although research machine learning focuses induction supervised training data many situations class labels available thus require unsupervised methods one widespread approach unsupervised induction involves clustering training cases groups reflect distinct regions decision space exists large literature clustering methods eg everitt 3 long history development increasing interest application yet still permission make digital hard copies part work personal classroom use granted without fee provided copies made distributed profit commercial advantage copies bear notice full citation first page copy otherwise republish post servers redistribute lists requires prior specific permission andor fee kdd2001 san francisco ca usa copyright 2001 acm 500 little understanding relation supervised unsupervised approaches induction paper begin remedy oversight examining situations supervised induction method occurs subroutine clustering algorithm suggests two important ideas first one able generate new clustering methods existing techniques replacing initial supervised technique different supervised technique second one would expect resulting clustering methods behave well eg form desirable clusters domains supervised components behave well provided latter labeled training data available pages follow explore ideas context iterative optimization common scheme clustering includes kmeans expectation maximization special cases reviewing framework section 2 describe approach embedding supervised algorithm learned classifier iterative optimizer section 3 examine four supervised methods taken step section 4 report experimental studies designed test hypotheses relations behavior resulting clustering methods supervised components closing review related work generative frameworks machine learning consider directions future research update class models assign instances classes clustered data data instances labeled data model parameters figure 1 iterative optimization procedure 2 generalized clustering many clustering systems rely notion iterative optimization figure 1 depicts system iterates two steps class model creation data reassignment reaching predetermined iteration limit changes occur reassignments many variations within general framework basic idea best illustrated wellknown example methods 21 kmeans em iterative optimizers two clustering algorithms popular simplicity flexibility kmeans 2 expectation maximization em 1 methods studied experimentally many problems used widely applied settings review algorithms briefly note key similarities show differences suggest general clustering framework kmeans algorithm represents class cen troid computes taking mean attribute instances belonging class geometric terms corresponds finding center mass cases associated class data reassignment involves assigning instance class closest centroid contrast em models class probability distribution extracts training data class model creation step data continuous class generally modeled ndimensional gaussian distribution consists mean variance attribute discrete case p extracted possible combination class ck attribute j attribute value v jl cases finding parameters contribution instance x weighted p ck jx data reassignment done recalculating p ck jx instance x class ck using new class models 22 general framework although clustering algorithms incorporate iterative optimization employ different methods developing class models thus view invoking different supervised learning technique distinguish among classes two algorithms also differ assign instances classes kmeans assigns instance single class whereas em uses partial signment instance distributed among classes refer absolute method strict paradigm partial method weighted observations lead general framework clustering involves selecting supervised learning algorithm selecting one assignment paradigms context kmeans em framework immediately suggests variants using weighted paradigm kmeans classifier obtain weighted kmeans algorithm similarly combining ems probabilistic classifier strict paradigm produces variant instance assigned entirely probable class variant explored name strict assignment em although partial assignment method commonly used although classifiers utilized kmeans em easily modified operate either assignment method supervised algorithms require sophisticated adaptations see shortly 3 supervised learning methods argued possible embed supervised learning method within generalized clustering framework however evaluation focused four simple induction algorithms limited representational power 6 clustering process aims generate disjoint decision regions powerful supervised methods designed produce describe algorithms detail including adaptations made use weighted paradigm adaptations involve altering model production take account weights instances revising instance reassignment generate class weights every instance used produce next generation class models 31 prototype modeler first supervised algorithm plays role k means creates prototype 13 centroid class extracting mean attribute training cases class prototype modeler classifies instance selecting class centroid closest ndimensional space distance metric sensitive variations scale version normalizes data values zero one creating prototypes weighted paradigm mean attribute becomes weighted average training cases relative proximity instance given centroid determines associated weight centroids class formally express jcj number classes new centroid composed weighted mean attribute mean attribute j cluster ck calculated value jth attribute instance x jxj total number instances 32 naive bayesian modeler selected naive bayes 2 second induction algo rithm described context em technique models class probability distribution described class ck attribute j attribute value v jl nominal attributes naive bayes represents discrete conditional probability distribution estimates counts training data estimates class probability p ck similar manner continuous attributes typically uses conditional gaussian distribution estimates computing mean variance attribute training data class calculate relative probability new instance belongs given class ck naive bayes employs expression assumes distribution values attribute independent given class operating normally strict classifier naive bayes returns class highest probability stance weighted case conditional distributions calculated using weighted sum rather strict sum expression determines weight used data reassignment process 33 perceptron list modeler another simple induction method perceptron algorithm 12 also combines evidence attributes classification uses expression ae threshold assign test case positive 1 negative 0 class weight wm specifies relative importance attribute taken together weights determine hyperplane attempts separate two classes learning algorithm invokes errordriven scheme adjust weights associated attribute 1 perceptron differentiate two classes employed ordered list perceptrons operates much like decision list algorithm first learns discriminate majority class others generating first percep tron instances majority class removed system trains distinguish new majority class rest producing another perceptron process continues one class remains treated default although perceptron traditionally assumes allornone assignment seems natural interpret scaled difference sum threshold likelihood weighted variant multiplies update attribute weight weight instance instance smaller weight smaller effect learning prevent small weights causing endless oscillations triggers updating cycle data incorrectly classified instance weight greater 05 although instances used actual update reassignment weighted method calculates difference instance value threshold scaled sigmoid produces bounds weight size instance evaluated perfectly threshold function would return 05 factor 5 exponent e distributes resulting weights larger range algorithm give weight close 05 instances otherwise sigmoid tight enough useful generally small range values 34 decision stump modeler final supervised learning algorithm selected decisionstump induction eg holte 4 differs others selecting single attribute classify instances end uses informationtheoretic measure frequency class ck training set jcj classes attribute continuous algorithm orders observed values considers splitting successive pair selecting split highest score method applies process recursively 1 purposes study used learning rate iterations training data well classification tasks values subset continuing divisions gain information measured training set tm given subset jp j number branches attribute nominal algorithm creates separate branch attribute value branch stump associated majority class training cases sorted branch accommodate weighted assignment adjust equations sum weights instances rather strict frequencies keep simple statistical information branch reassignment weight given instance class ck calculated jbj number instances associated branch instance sorted 4 experimental studies two intuitions clustering framework suggested corresponding formal hypotheses 2 first expected algorithm would exhibit preference one data assignment paradigms demonstrating better performance paradigm across different data sets second anticipated across data sets high low predictive accuracy supervised method would associated relatively high low accuracy corresponding clustering algorithm section describe designs experiments test hypotheses results obtained 41 experiments natural data test hypotheses ran generalized clustering system algorithmparadigm combination battery natural data sets also evaluated supervised algorithm independently training measuring predictive accuracy separate test set independent variables assignment paradigm clustering tests supervised learning algorithm data set number instances used training dependent variables classification accuracies unseen data used standard accuracy metric evaluate supervised classifiers clustering algorithms test set ffix classified correctly 0 otherwise evaluating accuracy trained classifier labeled data set test set removed clustering algorithms create classes added step completed cluster assigned actual naturally also expected single algorithm combination would outperform others data sets consistent general findings machine learning hardly deserves status hypothesis table 1 supervised accuracies four data sets prototype bayes perceptron stump promoters 860 870 760 700 iris 493 947 460 933 hayesroth 323 615 792 431 glass 848 790 390 976 class majority population example given cluster consists instances actually class 10 actually class b instances cluster declared members class accuracy 75 cluster approach loses detail let us evaluate clustering algorithm correct clusters selected four data sets uci repository pro moters iris hayesroth glass involved different numbers classes two seven different numbers attributes five 57 different attribute types nominal continuous mixed another factor selection led high classification accuracy one supervised methods typically lower accuracy others shown bold font table 1 differentiation supervised training data seemed prerequisite testing predicted correlation accuracies supervised learning clustering moreover remember four supervised methods restricted representational power generally limited one decision region per class result fact one method obtains high accuracy domains suggests classes maps onto single cluster lets us assume number classes data set corresponds number clus ters increasing chances meaningful results data set collected learning curve using tenfold crossvalidation recording results increment 25 data points typically clustering accuracy ceased improve early curve although supervised accuracy often continued increase results report involve accuracy measured last point curve table 2 unsupervised accuracies two alternative data assignment paradigms strictweighted prototype bayes perceptron stump promoters 620770 520410 490570 190260 iris 273513 833880 267320 553533 hayesroth 377392 300400 385385 346362 glass 848510 448619 262343 771743 recall first hypothesis predicted supervised method would construct accurate clusters combined preferred data assignment paradigm results table 2 shows classification accuracies methodparadigm combination four mains disconfirms hypothesis general supervised algorithm sometimes better one assignment scheme sometimes depending main naive bayes prototype learner showed supervised accuracy2060100 unsupervised accuracy decision stump perceptron list naive bayes prototype figure 2 supervised unsupervised accuracies using strict data assignment four algorithms four natural data sets large shifts sort though swings decisionstump learner less drastic perceptron list method showed support prediction favoring weighted assignment three data sets tied result fourth addressing first hypothesis proceeded test second claim relatively higher lower accuracy supervised mode associated relatively higher lower accuracy unsupervised data ie correlated positively original plan measure unsupervised accuracy learning algorithm combined preferred data assignment paradigm rejected notion preference resorted instead measuring relation supervised accuracy achieved clustering strict assignment followed separate measure accuracy supervised learning weighted assignment end computed correlation supervised accuracies using 16 algorithmdomain combinations table 1 analogous strict accuracies table 2 resulting correlation coefficient significant 001 level explained 55 percent variance figure 2 shows supervised accuracy reasonable predictor unsupervised accuracy thus generally supporting hypothesis also calculated correlation supervised accuracies weighted accuracies table 2 correlation also significant 001 level explained 43 percent variance 42 experiments synthetic data encouraging results natural data sets show framework relevance realworld clustering prob lems give limited understanding reasons underlying phenomena reason decided carry another study employed synthetic data designed reveal detailed causes effects one standard explanation induction methods outperforming others relies notion inductive bias reflects fact formalisms represent certain decision regions easily others since four supervised learning methods quite different inductive biases designed four separate learning tasks intended easily learned one methods others learning task incorporated two continuous variables three classes single contiguous decision region class thus domain designed decision stumps mind involved splits along one relevant attribute prototypefriendly domain involved three distinct proto types forth naive bayesian classifier difficult foil every supervised method least one domain relatively poorly domain devised generator produced 125 random instances either uniform bayes friendly domain gaussian distribution every class creating number instances one geometric metaphor clarifies one reason given method outperform others supervised unsupervised mode also suggests reason correlation behavior two tasks imper fect conventional wisdom states clustering easy clusters well separated difficult thus data generator also included parameter let us vary systematically separation boundaries class predictive variables domain ranged 0 1 varied separation distance although expected synthetic domains reproduce positive correlation observed natural data also predicted cluster separation influence effect particular thought correlation would lower gap small since iterative optimization would difficulty assigning instances right unlabeled classes whereas supervised learning would difficulty however correlation increase monotonically cluster distance since process finding wellseparated clusters dominated inductive bias supervised learning modules experimental runs synthetic data support predictions 3 despite attempts design data sets would distinguish among supervised learning methods correlations supervised unsupervised accuracies cluster separation considerably lower strict weighted studies natural domains though still marginally significant 01 level moreover experiments showed evidence correlation increases cluster separation giving strict weighted strict weighted figure 3 plots accuracies strict unsupervised learning supervised accuracy cluster separation suggests one reason negative result apparently correlations reduced ceiling effect supervised accuracies generally much higher results natural domains show little variation whereas unsupervised accuracies still range widely supervised methods typically learn accurate classifiers across four synthetic domains even though 3 study also revealed evidence preferred data assignment scheme best combinations shifting across domain separation level supervised accuracy2060100 unsupervised accuracy decision stump perceptron list naive bayes prototype figure 3 supervised unsupervised accuracies using strict data assignment four algorithms four synthetic data sets best design otherwise analogous plots higher values separation parameter show even stronger versions effect indicating supervised induction benefits cluster separation unsupervised clustering explains correlation increase predicted expectations rested intuition inductive bias cluster separation dominant factors determining behavior iterative optimizer negative results high correlations natural domains infer factors vary experiment play equal important role likely candidates include number relevant attributes number irrelevant attributes amount attribute noise number classes known affect predictive accuracy learned classifiers domain characteristics varied systematically future studies draw synthetic data explore relation clustering supervised learning 5 related work noted earlier exists large literature clustering others eg everitt 3 reviewed length much work relies iterative optimization group training cases exist many variants beyond kmeans expectationmaximization algorithms familiar readers instance michalski stepps 11 used logical rule induction characterize clusters assign cases recently zhang hsu dayal 15 described kharmonic means method operates like kmeans invokes different distance metric usually speeds convergence ever despite diversity researchers proposed either theoretical frameworks characterizing space iterative optimization methods software frameworks support rapid construction evaluation broader arena efforts link methods supervised unsupervised learning ex ample langley sage 8 adapted method inducing univariate decision trees operate unsupervised data thus generate taxonomy recently langley 6 liu et al 9 described similar sophisticated approaches relationship supervised unsupervised algorithms rule learning transpar martin 10 reported one approach adapts supervised techniques construct association rules unlabeled data research focused specific algorithms rather general generative frameworks however areas machine learning seen frameworks sort langley neches 7 developed prism flexible language productionsystem architectures supported many combinations performance learning algorithms later versions prodigy 14 included variety mechanisms learning searchcontrol knowledge classification problems kohavi et als 5 mlc supported broad set supervised induction algorithms one could invoke considerable flexibility generative abilities mlc apparent use feature selection support novel combinations existing algorithms effort comes closest spirit goals attempt provide flexible software infrastructure machine learning 6 concluding remarks paper presented framework iterative optimization approaches clustering lets one embed supervised learning algorithm modelconstruction com ponent approach produces familiar clustering techniques like kmeans em also generates novel methods appeared literature framework also let us evaluate hypotheses relation resulting clustering methods supervised modules tested using natural synthetic data first hypothesis supervised method preferred data assignment scheme produced accurate clusters borne experiments clustering practitioners continue combine prototype learning strict assignment giving kmeans naive bayes weighted assignment giving em found evidence combinations superior al ternatives however experiments support second hypothesis revealing strong correlations accuracy supervised algorithms natural data sets accuracy iterative optimizers em bedded augmented results experiments synthetic data gave us control decision regions separation clusters studies also produced positive correlations supervised unsupervised ac curacy failed reveal effect cluster separation clearly remains considerable room additional research framework supports variety new clustering algorithms interesting right also important testing hypotheses relations supervised unsupervised learning also carry experiments synthetic data vary systematically factors affect predictive accu racy irrelevant features attribute noise finally explore role cluster separation reason apparent influence studies although specific results intriguing attach importance framework supports new direction studies clustering mechanisms encourage researchers view existing techniques examples generative framework utilize framework explore space clustering methods reveal underlying relations supervised unsupervised approaches induction ultimately strategy produce deeper understanding clustering process role broader science machine learning 7 r maximum likelihood incomplete data via em algo rithm pattern classification scene analysis analysis simple classification rules perform well commonly used data sets elements machine learning prism users manual conceptual clustering discrimination learning clustering decision tree construction focusing attention observational learn ing importance context learning ob servation conceptual clustering principles neurodynamics perceptrons theory brain mechanisms categories concepts derivational analogy prodigy automating case acquisition tr simple classification rules perform well commonly used datasets derivational analogy prodigy elements machine learning clustering decision tree construction kharmonic means spatial clustering algorithm boosting ctr tadashi nomoto yuji matsumoto supervised ranking opendomain text summarization proceedings 40th annual meeting association computational linguistics july 0712 2002 philadelphia pennsylvania greg hamerly charles elkan alternatives kmeans algorithm find better clusterings proceedings eleventh international conference information knowledge management november 0409 2002 mclean virginia usa shi zhong joydeep ghosh unified framework modelbased clustering journal machine learning research 4 p10011037 1212003