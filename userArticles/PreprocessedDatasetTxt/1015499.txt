sizing router buffers internet routers contain buffers hold packets times congestion today size buffers determined dynamics tcps congestion control algorithm particular goal make sure link congested busy 100 time equivalent making sure buffer never goes empty widely used ruleofthumb states link needs buffer size average roundtrip time flow passing across link c data rate link example 10gbs router linecard needs approximately 250ms x 25gbits buffers amount buffering grows linearly linerate large buffers challenging router manufacturers must use large slow offchip drams queueing delays long high variance may destabilize congestion control algorithms paper argue ruleofthumb outdated incorrect backbone routers large number flows tcp connections multiplexed together single backbone link using theory simulation experiments network real routers show link n flows requires longlived shortlived tcp flows consequences router design enormous 25gbs link carrying 10000 flows could reduce buffers 99 negligible difference throughput 10gbs link carrying 50000 flows requires 10mbits buffering easily implemented using fast onchip sram b introduction motivation 11 background internet routers packet switches therefore buer packets times congestion arguably router buers single biggest contributor uncertainty inter net buers cause queueing delay delayvariance overflow cause packet loss underflow degrade throughput given significance role might reasonably expect dynamics sizing router buers well understood based wellgrounded theory supported extensive simulation experimentation router buers sized today based ruleofthumb commonly attributed 1994 paper villamizar song 1using experimental measurements eight tcp flows 40 mbs link concluded dynamics tcps congestion control algorithms router needs amount buering equal average roundtrip time flow passes router multiplied capacity routers network interfaces wellknown show later ruleofthumb indeed make sense one small number longlived tcp flows network operators follow ruleofthumb require router manufacturers provide 250ms buer ing 2 rule found architectural guidelines 3 requiring large buers complicates router design impediment building routers larger capacity example 10gbs router linecard needs approximately 250ms 10gbs 25gbits buers amount buering grows linearly linerate goal work find ruleofthumb still holds still true trac uses tcp number flows increased significantly today backbone links commonly operate 25gbs 10gbs carry 10000 flows 4 one thing sure well understood much buering actually needed buer size aects network performance 5 paper argue rule ofthumb outdated incorrect believe sig 25 3 35 4 45 window pkts figure 1 window top router queue bottom tcp flow bottleneck link figure 2 single flow topology consisting access link latency l acc link capacity cacc bottleneck link capacity c latency l nificantly smaller buers could used backbone routers eg removing 99 buers without loss network utilization show theory simulations experiments support argument least believe possibility using much smaller buers warrants exploration study comprehensive experiments needed real backbone networks paper isnt last word goal persuade one network operators try reduced router buers backbone network worth asking care accurately size router buers declining memory prices buer routers believe overbuering bad idea two reasons first complicates design highspeed routers leading higher power consumption board space lower density second overbuering increases endtoend delay presence congestion large buers conflict lowlatency needs real time applications eg video games device control cases large delays make congestion control algorithms unstable 6 applications unusable 12 ruleofthumb come ruleofthumb comes desire keep congested link busy possible maximize throughput network used thinking sizing queues prevent overflowing losing packets tcps sawtooth congestion control algorithm designed fill buer deliberately causes occasional loss provide feedback sender matter big make buers bottleneck link tcp cause buer overflow router buers sized tcp flows pass dont underflow lose throughput ruleofthumb comes metric use throughput goal determine size buer maximize throughput bottleneck link basic idea router packets buered outgoing link always busy outgoing link bottleneck want keep busy much time possible need make sure buer never underflows goes empty fact ruleofthumb amount buering needed single tcp flow buer bottleneck link never underflows router doesnt lose throughput ruleofthumb comes dynamics tcps congestion control algorithm particular single tcp flow passing bottleneck link requires buer size equal bandwidthdelay product order prevent link going idle thereby losing throughput give quick intuitive explanation rule ofthumb comes particular right amount buering router carried one longlived tcp flow section 2 give precise explana tion set stage theory buer sizing one flow multiple long shortlived flows later confirm theory true using simulation experiments sections 51 52 respectively consider simple topology figure 2 single source sends infinite amount data packets constant size flow passes single router senders access link much faster ceivers bottleneck link capacity c causing packets queued router propagation time sender receiver vice versa denoted tp assume tcp flow settled additiveincrease multiplicativedecrease aimd congestion avoidance mode sender transmits packet time receives ack gradually increases number outstanding packets window size causes buer gradually fill eventually packet dropped sender doesnt receive ack halves window size pauses 1 sender many packets outstanding network sent amount equal old win dow window size halved must therefore pause waits acks arrive resume transmitting key sizing buer make sure sender pauses router buer doesnt go empty force bottleneck link go idle determining rate buer drains determine size reservoir needed prevent going empty turns equal distance bytes peak trough sawtooth representing tcp assume reader familiar dynamics tcp brief reminder salient features found appendix window size show later corresponds worth asking tcp sawtooth factor determines buer size example doesnt statistical multiplexing sudden arrival short bursts eect particular might expect bursty tcp slowstart phase increase queue occupancy frequently fill buer figure 1 illustrates effect bursts queue size typical single tcp flow clearly queue absorbing short term bursts slowstart phase accommodating slowly changing window size congestion avoidance phase examine eect burstiness caused shortflows section 4 well find shortflows play small eect buer size fact dictated number long flows 13 buffer size influences router design seen ruleofthumb comes lets see matters particular complicates design routers time writing state art router linecard runs aggregate rate 40gbs one physical interfaces 250ms buering 10gbits 125gbytes buer memory buers backbone routers built commercial memory devices dynamic ram dram static ram sram 2 largest commercial sram chip today 36mbits means 40gbs linecard would require 300 chips making board large expensive hot instead try build linecard using dram would need 10 devices dram devices available 1gbit problem dram random access time 50ns hard use minimum length 40byte packet arrive depart every 8ns worse still dram access times fall 7 per year problem going get worse linerates increase future practice router linecards use multiple dram chips parallel obtain aggregate datarate memory need packets either scattered across memories adhoc statistical manner use sram cache refresh algorithm 7 either way large packet buer number disadvantages uses wide dram bus hundreds thousands signals huge number fast data pins network processors packet processor asics frequently 2000 pins making chips large expensive wide buses consume large amounts board space fast data pins modern drams consume lot power summary extremely dicult build packet buers 40gbs beyond given slowly memory speeds improve problem going get worse time substantial benefits could gained placing buer memory directly chip processes packets network processor asic case wide fast access single memory possible commercial packet processor asics built 256mbits embeddeddram memories 2 delaybandwidth product acceptable ie 98 smaller day singlechip packet processor would need external memories present evidence later buers includes devices specialized io ddr sdram rdram rldram fcram figure 3 schematic evolution router buer single tcp flow50150250 queue pkts figure 4 tcp flow single router buers equal delaybandwidth product upper graph shows time evolution congestion window w lower graph shows time evolution queue length qt small might make little dierence utilization backbone links 2 buffer size single longlived next two sections determine large router buers need tcp flows longlived start examining single longlived flow consider happens many flows multiplexed together starting single flow consider topology figure 2 single sender one bottleneck link schematic evolution routers queue source congestion avoidance shown figure 3 time t0 sender steadily increases windowsize fills buer buer drop first packet one roundtrip time later sender timesout waiting ack dropped packet immediately halves window size wmax wmax2 packets 3 window size limits number unacknowledged ie outstanding packets net work loss sender allowed wmax outstanding packets timeout allowed wmax2 outstanding packets thus sender many outstanding packets must pause waits acks wmax2 packets goal make sure router buer never goes empty order keep router fully utilized therefore buer must go empty sender pausing buer never goes empty router must sending packets onto bottleneck link constant rate c 3 tcp measures window size bytes count window size packets simplicity presentation queue pkts figure 5 tcp flow underbuered router50150250350 window pkts50150 queue pkts figure flow overbuered router turn means acks arrive sender rate c sender therefore pauses exactly wmax2c seconds wmax2 packets acknowledged resumes sending starts increasing window size key sizing buer make sure buer large enough sender pauses buer doesnt go empty sender first pauses 1 buer full drains period bc 2 shown figure 3 buer avoid going empty first packet sender shows buer hits empty ie wmax2c bc determine wmax consider situation sender resumed transmission window size wmax2 buer empty sender send packets rate c link underutilized well known sending rate tcp equation 7 appendixa since buer empty queueing delay therefore send rate c require rtt buer leads wellknown ruleofthumb widely known similar arguments made previously 8 9 result easily verified using ns2 10 simulation closedform analytical model ap pendix b figure 4 illustrates evolution single tcp reno flow using topology shown figure 2 buer size exactly equal ruleofthumb window size follows familiar sawtooth pattern increasing steadily loss occurs halving window size starting increase steadily notice buer occupancy almost hits zero per packet loss never stays empty behavior want bottleneck link stay busy appendix presents analytical fluid model provides closedform equation sawtooth closely matches ns2 simulations figures 5 6 show happens link buered overbuered figure 5 router buered buer size less rtt c congestion window follows sawtooth pattern suciently buered case however window halved sender pauses waiting acks insucient reserve buer keep bottleneck link busy buer goes empty bottleneck link goes idle lose throughput hand figure 6 shows flow buered behaves like correctly buered flow fully utilizes link however window halved buer completely empty queueing delay flows increased constant buer always packets queued summary b 2tp router buer never goes empty bottleneck link never go idle 3 many long tcp flows share link backbone router many flows share bottleneck link simultaneously single longlived flow realistic model example 25gbs oc48c link typically carries 10000 flows time 4 4 change model reflect buers required bottleneck link many flows consider two situa tions first consider case flows synchronized sawtooths march lockstep perfectly inphase consider flows synchronized least synchronized marching lockstep suciently desynchronized argue case practice amount buering drops sharply 31 synchronized long flows consider evolution two tcp reno flows bottleneck router evolution window sizes sending rates queue length shown figure 7 although two flows start dierent times quickly synchronize perfectly phase welldocumented studied tendency flows sharing bottleneck become synchronized time 9 11 12 13 set precisely synchronized flows buer requirements single flow aggregate behavior still sawtooth height sawtooth 4 shouldnt surprising typical user today connected via 56kbs modem fully utilized 25gbs simultaneously carry 40000 flows fully utilized buers barely used link isnt bottleneck size buers large number flows util pktss rate pktss500150025003500 util pktss queue pkts figure 7 two tcp flows sharing bottleneck link upper two graphs show time evolution rtt flows congestion window senders link utilization sending rate tcp senders bottom graph shows queue length router buer dictated maximum window size needed fill roundtrip path independent number flows specifically assume n flows congestion window w time endtoend propagation delay n window size maximum allowable number outstanding bytes qt buer occupancy time p average propagation delay solve buer size considering two cases packets dropped first move lock step flows largest window size wmax time buer full similarly window size smallest drop simultaneously 9 buer sized goes empty senders start transmitting pause 9500 10000 10500 11000 11500 12000 12500 probability packets link underutilized packets dropped pdf aggregate window normal distribution n11000400 figure 8 probability distribution sum congestion windows flows passing router approximation normal distri bution two vertical marks mark boundaries number outstanding packets fit buer sum congestion windows lower less packets outstanding link underutilized higher buer overflows packets dropped solving b find result holds number synchronized inphase flows 32 desynchronized long flows flows synchronized backbone router carrying thousands flows varying rtts small variations rtt processing time sucient prevent synchronization 14 absence synchronization demonstrated real networks 4 15 likewise found simulations experiments inphase synchronization common 100 concurrent flows rare 500 concurrent flows 5 although dont precisely understand synchronization tcp flows takes place observed aggregates 500 flows amount inphase synchronization de creases circumstances treat flows synchronized understand dierence adding synchronized desynchronized window size processes recall add together many synchronized sawtooths get single large sawtooth buer size requirement doesnt change hand sawtooths syn chronized flows add less sum look like sawtooth smooth distance peak trough aggregate window size get smaller hence given need much buer distance peak trough aggregate window size expect buer size 5 outofphase synchronization flows synchronized scale window dierent times cycle visible ns2 simulations 1000 flows however buer requirements similar outofphase synchronization synchronization time seconds buffer sum tcp windows pkts router queue pkts figure 9 plot p w tcp flows queue q oset 10500 packets requirements get smaller increase number flows indeed case explain demonstrate via simulation consider set tcp flows random indepen start times propagation delays well assume desynchronized enough window size processes independent model total window size bounded random process made sum independent sawtooths know central limit theorem aggregate window size process converge gaussian process figure 8 shows indeed aggregate window size converge gaussian process graph shows probability distribution sum congestion windows flows dierent propagation times start times explained section 51 window size process know queue occupancy time words outstanding bytes queue qt link 2tp c dropped represent number dropped packets buer large enough tcp operating correctly negligible compared 2tp c therefore distribution qt shown figure 9 given w normal distribution q distribution normal shifted constant course normal distribution restricted allowable range q useful pick buer size know immediately probability buer underflow lose throughput gaussian determine queue occupancy process know mean variance mean simply sum mean constituents find variance well assume convenience sawtooths average value dierent values would still yield results tcp sawtooth modelled oscillating uniform distribution around average congestion window size w minimum 2 maximum 4 since standard deviation uniform distribution 1 th length standard deviation single window size w thus equation 5 large number flows standard deviation sum windows w given equation 5 standard deviation qt know distribution queue occu pancy approximate link utilization given buer size whenever queue size threshold b risk guaranteed queue go empty lose link utilization know probability q b upper bound lost utilization q normal distribution use errorfunction 6 evaluate probability therefore get following lower bound utilization 1 6 numerical examples utilization using 10000 router buer size utilization util 9999988 means achieve full utilization buers delaybandwidth product divided squareroot number flows small multiple thereof number flows router increases amount required buer decreases result practical implications building routers core router currently 10000 100000 flows passing given time vast majority flows short eg flows fewer 100 packets flow length distribution heavy tailed majority packets given time belong long flows result router would achieve close full utilization buer sizes 1 1 delaybandwidth product verify result experimentally section 52 6 precise result could obtained using cherno bounds instead present guassian approximation clarity presentation 4 sizing router buffer tcp flows longlived fact many flows last packets never leave slowstart never reach equilibrium sending rate 4 weve considered longlived tcp flows well consider short tcp flows aect size router buer going find short flows tcp nontcp much smaller eect longlived tcp flows particularly backbone router large number flows define shortlived flow tcp flow never leaves slowstart eg flow fewer 90 packets assuming typical maximum window size 65kb section 53 see results hold short nontcp flows eg dns queries icmp etc consider topology figure 2 multiple senders separate access links widely reported measurement assume new short flows arrive according poisson process 16 17 slowstart flow first sends two packets four eight six teen etc slowstart algorithm sender increases windowsize one packet received ack access links lower bandwidth bottleneck link bursts spread single burst causes queueing assume worst case access links infinite speed bursts arrive intact bottleneck router model bursts arriving many dierent short flows bottleneck router flows sending burst two packets others might sending burst four eight sixteen packets distribution burstsizes large number flows consider burst independent bursts even bursts flow simplified model arrival process bursts opposed arrival flows assumed poisson one might argue arrivals poisson burst followed another burst one rtt later however low load many flows buer usually empty several times one rtt eectively memoryless time scale instance lets assume arrivals flows fixed length l doubling burst lengths iteration slowstart flow arrive n bursts size r remainder 1 therefore bursts arrive poisson process lengths iid random variables equally distributed among 2 4 2 n1 r router buer modelled simple mg1 queue fifo service discipline case job burst packets job size number packets burst average number jobs mg1 queue known eg 18 load link ratio amount incoming trac link capacity c ex ex 2 first two moments burst size model overestimate queue length bursts processed5152535450 average queue length length tcp flow pkts 200 mbits link model figure 10 average queue length function flow length 08 bandwidth impact buer requirement upper bound given mg1 model infinite access link speeds matches simulation data closely packetbypacket mg1 queue job dequeued whole job processed queue busy overestimate queue length half average job size exit interesting note average queue length independent number flows bandwidth link depends load link length flows validate model comparing simulations figure shows plot average queue length fixed load varying flow lengths generated using ns2 graphs three dierent bottleneck link bandwidths 40 80 200 mbs shown model predicts relationship closely perhaps surprisingly average queue length peaks probability large bursts highest necessarily average burst size highest instance flows size 14 generate larger queue length flows size 16 flow 14 packets generates bursts x largest burst size 8 probability 1 3 flow 16 packets generates bursts sizes maximum burst length 8 probability 1 4 model predicts bandwidth eect queue length measurements 40 80 200 mbs almost identical gap model simulation due fact access links bottleneck link space packets burst slower access links would produce even smaller average queue length determine buer size need probability distribution queue length average dicult closed form result exists general mg1 queue length distribution instead approximate tail using eective bandwidth model 19 tells us queue length distribution ex equation derived appendix c goal drop packets short flow drops packet retransmission significantly increases flows duration words want choose buer size b p q b small key observation short flows size buer depend linerate propagation delay flows number flows depends load link length flows therefore backbone router serving highly aggregated trac needs amount buering absorb shortlived flows router serving clients furthermore analysis doesnt depend dynamics slowstart burstsize distribution easily extended short unresponsive udp flows practice buers made even slower model simulation assumed access links faster bottleneck link evidence 4 20 highly aggregated trac slow access links cases lead bursts smoothed completely case individual packet arrivals close poisson resulting even smaller buers buer size easily computed md1 model setting summary shortlived flows require small buers mix short longlived flows see simulations experiments next section shortlived flows contribute little buering requirements buer size usually determined number longlived flows 7 5 simulation experimental weve described theoretical models long shortlived flows present results validate models use two validation methods simulation using ns2 network real internet routers simulations give us flexibility allow us explore range topologies link speeds numbers flows trac mixes hand experimental network allows us compare results network real internet routers tcp sources rather idealized ones ns2 less flexible limited number routers topologies results limited finite number dierent simulations experiments run cant prove results extend router internet 21 section 53 examine scope limitations results validation steps needed goal persuade network operator test results reducing size router buers approximately 99 checking utilization drop rates dont change noticeably time rely limited set experiments 51 ns2 simulations ran 15000 ns2 simulations one simulating several minutes network trac router verify model range possible settings limit 7 distribution flows define short flows long flows flows slowstart congestion avoidance mode respectively means flows may transition short long existence5015025035050 100 150 200 250 300 350 400 450 500 minimum required buffer number longlived flows 980 utilization 995 utilization 999 utilization figure 11 minimum required buer achieve 98 995 999 percent utilization oc3 155 mbs line 80ms average rtt measured ns2 longlived tcp flows simulations cases flows experience one congested link network operators usually run backbone links loads 1030 result packet drops rare internet backbone single point congestion rare unlikely flow encounter two congestion points assume router maintains single fifo queue drops packets tail queue full ie router use red droptail widely deployed scheme today expect results extend red results rely desynchronization tcp flows something likely red droptail used tcp reno maximum advertised window size least 10000 bytes 1500 1000 byte mtu average propagation delay tcp flow varied 25ms 300ms 511 simulation results longlived tcp flows figure 11 simulates oc3 155mbs line carrying longlived flows graph shows minimum required buer given utilization line compares buer size predicted model example model predicts 98 utilization buer rttc sucient number longlived flows small flows partially synchronized result doesnt hold however seen graph number flows exceeds 250 model holds well found order attain 999 utilization needed buers twice big model predicts found similar results hold wide range settings whenever large number flows little synchronization average congestion window two average congestion window smaller two flows encounter frequent timeouts buering required 22 simulations experiments looked three commonly used performance metrics see eect buer size minimum required buffer length tcp flow pkts 200 mbits link mg1 model p001 figure 12 minimum required buer increases average flow completion time afct 125 vs infinite buers short flow trac packet loss reduce buer size expect packet loss increase loss rate tcp flow function flows window size approximated see 22 equation 1 know sum window sizes rtt c b b made small window size halves increasing loss factor four necessarily problem tcp uses loss useful signal indicate tcps loss rate low one packet per multiple roundtrip times impor tantly show flows complete sooner smaller buers large buers one might argue applications use tcp adversely aected loss eg online gaming media streaming however applications typically even sensitive queueing delay goodput 100 utilization achievable goodput always 100 retransmissions increased loss goodput reduced small amount long buers equal greater rttc fairness small buers reduce fairness among flows first smaller buer means flows smaller roundtrip time sending rate higher large buers roundtrip times increase relative dierence rates decrease buering would increase fairness also increases flow completion times flows second eect timeouts likely small buers investigate timeouts aect fairness detail however ns2 simulations seemed minimally aected buer size 512 short flows use commonly used metric short flows flow completion time defined time first packet sent last packet reaches des tination particular measure average flow completion time afct interested tradeo buer size afct general link a50015000 50 100 150 200 250 300 350 400 450 500 minimum required buffer number longlived flows minimum buffer 95 utilization figure 13 buer requirements trac mix dierent flow lengths measured ns2 simulation load 1 afct minimized infinite buers packet drops therefore retransmissions take benchmark afct infinite buers find increase afct function buer size example figure 12 shows minimum required buer afct increased 125 experimental data ns2 experiments 40 80 200 mbs load 08 model p q plotted graph bound predicted mg1 model closely matches simulation results key result amount buering needed depend number flows bandwidth roundtrip time depends load link length bursts trac mix short flows future generation 1 tbs core router needs amount buering local 10 mbs router today 513 mixes short longlived flows practice routers transport mix short long flows exact distribution flow lengths varies network network time makes impossible measure every possible scenario give general idea flow mix influences buer size good news long flows dominate buer size suce large number flows better still well see afct short flows lower used usual ruleofthumb experiments short flows always slowdown long flows aggressive multiplicative increase causing long flows reduce windowsize figures 13 14 show mix flows 400 mbs link total bandwidth arriving short flows 80 mbs 20 total capacity number long flows varied 1 500 time experiment long flows attempted take bandwidth left available short flows practice never consumed 80 bandwidth rest would taken aggressive short flows expected small number flows flows partially synchronized 200 longlived flows synchronization largely disappeared average completion time apkt flow number longlived flows afct 14 packet flow rttbw buffers afct 14 packet flow rttbwsqrtn buffers figure 14 average flow completion times buer size rtt c n compared buer size rtt c graph shows long flows dominate flow size want 95 utilization need buer close means ignore short flows sizing buer course doesnt tell us shortlived flows faring might shutout long flows increased afct figure 14 shows case ns2 simulation average flow completion time much shorter rtt c n buers rtt c sized buers queueing delay lower reducing buer size still achieve 100 utilization decrease completion times shorter flows 514 pareto distributed flow lengths real network trac contains broad range flow lengths flow length distribution known heavy tailed 4 experiments used pareto distributions model define short flows still slow start pareto distributed flows congested router ie 1 model holds achieve close 100 throughput buers small multiple rtt c n 9 example ns2 simulation 155 mbs line rtt 100ms measured 100200 simultaneous flows achieved utilization 99 buer packets pointed 23 network low la tency fast access links limit tcp window size would concurrent flows network single heavy flow could hog bandwidth short period time terminate unlikely practice unless operator allows single user saturate network long backbone networks orders magnitude faster access networks users able saturate backbone anyway even could tcp capable utilizing link quickly 8 n number active long flows given time total number flows 9 number long flows n sizing buer found measuring number flows congestion avoidance mode instant visually selecting robust minimum due additive increase behavior certain window size trac transported highspeed routers commercial networks today 4 24 10s 1000s concurrent flows believe unlikely change future uncongested router ie 1 modeled using shortflow model presented section 4 often leads even lower buer requirements small buers may penalize long flows forced congestion avoidance early even though bandwidth still available want allow single flow take 1n bandwidth always need buers rtt c n even low link utilization found general result holds dierent flow length distributions least 10 trac long flows otherwise short flow eects sometimes dominate measurements commercial networks 4 suggest 90 trac long flows seems safe assume long flows drive buer requirements backbone routers 52 measurements physical router simulation captures characteristics router tcp interaction verified model running experiments real backbone router trac generated real tcp sources router cisco gsr 12410 25 4 x oc3 posengine 0 line card switches ip packets using pos ppp sonet 155mbs router input output queues although input queueing took place experiments total throughput router far maximum capacity switching fabric tcp trac generated using harpoon trac generator 26 linux bsd machines aggregated using second cisco gsr 12410 router gigabit ethernet line cards utilization measurements done using snmp receiving end compared netflow records 27 router buer link utilization flows rttbw pkts ram model sim exp 200 figure 15 comparison model ns2 simulation experimental results buer requirements cisco gsr 12410 oc3 linecard 521 long flows figure 15 shows results measurements gsr 12410 router router memory adjusted limiting length interface queue outgoing interface buer size given multiple rttc number packets size ram device would needed subtracted size internal fifo linecard see section 522 model lowerbound utilization predicted model sim exp utilization measured simulation ns2 physical router respectively 100 200 flows expect synchronization model predicts utilization correctly within measurement accuracy 01 ns2 sometimes predicts lower utilization found practice attribute synchronization flows simulations real network key result model simulation experiment agree router buer size equal approximately rttc opposed rtt c case would 1291 packets0011 queue length pkts fifo link underutilized exp cisco gsr ifqueue exp cisco gsr buffers model mg1 ps model mg1 fifo figure experimental simulation model prediction routers queue occupancy cisco gsr 12410 router 522 short flows section 4 used mg1 model predict buer size would need shortlived bursty tcp flows verify model generated lots shortlived flows measured probability distribution queue length gsr 12410 router figure 16 shows results comparison model match remarkably well 10 53 scope results future work results present paper assume single point congestion flows path dont believe results would change much percentage flows experienced congestion multiple links however investigated single point congestion means 10 results match closely assume router underreports queue length 43 packets learned manufacturer linecard undocumented 128kbyte transmit fifo setup 64 kbytes used queue packets internal fifo mtu 1500 bytes accounts exactly 43 packet dierence reverse path congestion would likely eect tcpbuer interactions 28 assump tions simplified network topology fairly general arbitrary network flows may pass routers bottleneck link however assume single point congestion packet loss little trac shaping occur previous links network focus tcp main trac type internet today constant rate udp sources eg online games single packet sources poisson arrivals eg dns modelled using short flow model results mixes flows still hold understand trac composed mostly nontcp packets would require study model assumes upper bound congestion window reality tcp implementations maximum window sizes low 6 packets 29 window sizes 64kbyte require use scaling option 30 rarely used results still hold flows limited window sizes require even smaller router buers 1 run simulations using random early detection 12 eect flow synchronization small number flows aggregates large number 500 flows varying rtts synchronized red tends little eect buer require ments however early drop slightly increase required buer since uses buers less eciently visible impact varying latency direct eect varying bandwidthdelay product congestion also caused denial service dos 31 attacks attempt flood hosts routers large amounts network trac understanding make routers robust dos attacks beyond scope paper however find direct benefit larger buers resistance dos attacks 6 related work villamizar song report rtt bw rule 1 authors measure link utilization 40 mbs network 1 4 8 longlived tcp flows dierent buer sizes find fifo dropping discipline large maximum advertised tcp congestion windows necessary buers rttc guarantee full link utilization reproduced results using ns2 confirm setup small number flows large congestion windows flows almost fully synchronized buer requirement single flow morris 32 investigates buer requirements 1500 longlived flows link 10 mbs 25ms latency concludes minimum amount buering needed small multiple number flows points bandwidthdelay product 217 packets flow fraction packet transit time many flows timeout adversely eects utilization fairness repeated experiment ns2 obtained similar results however typical router used carrier isp limited implications users fast access links need several packets outstanding achieve adequate performance users slow access links eg 32kbs modem users 96kbs gsm mobile access need additional buers network sucient packets outstanding however additional buer ends access link eg modem bank local isp gsm gateway cellular carrier believe overbuering core router serves dierent users would wrong approach overbuering increases latency everyone also dicult implement high linerates instead access devices serve slow lastmile access links 1mbs continue include packets worth buering link line speeds increasing mtu size staying constant would also assume issue become less relevant future avrachenkov et al 33 present fixed point model utilization long flows flow completion times short flows model short flows using mm1k model accounts flows bursts long flow model use analytical model tcp aected buer rtt model requires fixed point iteration calculate values specific settings one simulation result given directly compare results garetto towsley 34 describe model queue lengths routers load one similar model section 4 key dierence authors model bursts batch arrivals k m1 model opposed model models bursts varying job length mg1 model accommodates slowstart congestion avoidance mode however lacks closed form solution end authors obtain queue distributions similar 7 conclusion believe buers backbone routers much larger need possibly two orders magnitude results right consequences design backbone routers evidence buers made smaller havent tested hypothesis real operational network little dicult persuade operator functioning profitable network take risk remove 99 buers next step see results presented paper first step towards persuading operator try operator verifies results least demonstrates much smaller buers work fine still remains persuade manufacturers routers build routers fewer buers shortterm dicult competitive marketplace obvious router vendor would feel comfortable building router 1 buers competitors historical reasons network operator likely buy router larger buers even unnecessary eventually routers continue built using current ruleofthumb become dicult build linecards commercial memory chips end necessity may force buers smaller least results true know routers continue work fine network utilization unlikely aected 8 acknowledgments authors would like thank joel sommers professor paul barford university wisconsinmadison setting running measurements physical router wail testbed sally floyd frank kelly useful disucssions matthew hollimans feedback long flows led central limit theorem argument 9 r high performance tcp ansnet httpwww rfc 3439 internet architectural guidelines philosophy provisioning internet backbone networks support latency sensitive applications dynamics tcpred scalable control analysis memory architecture fast packet bu observations dynamics congestion control algorithm network simulator ns2 random early detection gateways congestion avoidance oscillating behaviour network tra understanding performance many tcp flows aggregate tra area tra data networks cascades investigating multifractal nature internet wan tra notes e internet tra scalable tcp congestion control personal communication stanford networking characteristics residential tra cisco 12000 series routers observations dynamics congestion control algorithm e rfc 1213 management information base network management tcpipbased internetsmibii framework classifying denial service attacks tcp behavior many flows modeling simulation measurements queueing delay longtail internet trac illustrated tr observations dynamics congestion control algorithm random early detection gateways congestion avoidance high performance tcp ansnet area traffic data networks cascades observations dynamics congestion control algorithm statistical bandwidth sharing difficulties simulating internet aggregate traffic performance active queue management drop tail understanding performance many tcp flows modeling simulation measurements queuing delay longtail internet traffic behavior many flows provisioning internet backbone networks support latency sensitive applications ctr gaurav raina oliver heckmann tcp local stability hopf bifurcation performance evaluation v64 n3 p266275 march 2007 amogh dhamdhere constantine dovrolis open issues router buffer sizing acm sigcomm computer communication review v36 n1 january 2006 performance analysis timerbased burst assembly slotted scheduling optical burst switching networks performance evaluation v63 n9 p10161031 october 2006 robert n shorten douglas j leith queue provisioning network efficiency transmission control protocol ieeeacm transactions networking ton v15 n4 p866877 august 2007 mahmoud elhaddad rami melhem taieb znati analysis transmission scheduling algorithm supporting bandwidth guarantees bufferless networks acm sigmetrics performance evaluation review v34 n3 p4863 december 2006 yeeting li douglas leith robert n shorten experimental evaluation tcp protocols highspeed networks ieeeacm transactions networking ton v15 n5 p11091122 october 2007 damon wischik nick mckeown part buffer sizes core routers acm sigcomm computer communication review v35 n3 july 2005 g vubrugier r stanojevic j leith r n shorten critique recently proposed buffersizing strategies acm sigcomm computer communication review v37 n1 january 2007 sangtae ha long le injong rhee lisong xu impact background traffic performance highspeed tcp variant protocols computer networks international journal computer telecommunications networking v51 n7 p17481762 may 2007 young eun xinbing wang performance analysis tcpaqm generalized aimd intermediate buffer sizes computer networks international journal computer telecommunications networking v51 n12 p36553671 august 2007 george varghese j andrew fingerhut flavio bonomi detecting evasion attacks high speeds without reassembly acm sigcomm computer communication review v36 n4 october 2006 yashar ganjali nick mckeown update buffer sizing internet routers acm sigcomm computer communication review v36 n5 october 2006 mihaela enachescu yashar ganjali ashish goel nick mckeown tim roughgarden part iii routers small buffers acm sigcomm computer communication review v35 n3 july 2005 joel sommers paul barford nick duffield amos ron improving accuracy endtoend packet loss measurement acm sigcomm computer communication review v35 n4 october 2005 aleksandar kuzmanovic power explicit congestion notification acm sigcomm computer communication review v35 n4 october 2005 zhenyun zhuang taeyoung chang raghupathy sivakumar aravind velayutham 3 applicationaware acceleration wireless data networks proceedings 12th annual international conference mobile computing networking september 2329 2006 los angeles ca usa elvis vieira michael bauer proactively controlling roundtrip time variation packet drops using smoothtcpq proceedings 3rd international conference quality service heterogeneous wiredwireless networks august 0709 2006 waterloo ontario canada allen b downey tcp selfclocking bandwidth sharing computer networks international journal computer telecommunications networking v51 n13 p38443863 september 2007 joel sommers paul barford nick duffield amos ron geometric approach improving active packet loss measurement ieeeacm transactions networking ton v16 n2 p307320 april 2008 kortebi l muscariello oueslati j roberts evaluating number active flows scheduler realizing fair statistical bandwidth sharing acm sigmetrics performance evaluation review v33 n1 june 2005 sumitha bhandarkar saurabh jain l narasimha reddy ltcp improving performance tcp highspeed networks acm sigcomm computer communication review v36 n1 january 2006