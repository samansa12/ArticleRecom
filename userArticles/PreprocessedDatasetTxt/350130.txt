stochastic grammatical inference text database structure document collection structural elements identified markup often necessary construct grammar retrospectively constrains element nesting ordering addressed others application grammatical inference describe approach based stochastic grammatical inference scales naturally large data sets produces models richer semantics adopt algorithm produces stochastic finite automata describe modifications enable better interactive control results experimental evaluation uses four document collections varying structure b introduction 11 text structure electronically stored text well known advantages identifying structural elements eg chapters titles paragraphs footnotes descriptive markup 4 5 12 commonly markup form labeled tags interleaved text following example referencethe art war chapter 3 paragraph 18reference know enemy know need fear result hundred battles sentence know enemy every victory gained also suffer defeat sentence know neither enemy succumb every battle sentence documents marked way updated interpreted much robustly structural elements identified codes specific particular system typesetting style markup also used support operations searches require words phrases occur within particular elements advantages gained using formal grammar specify elements used example instance conforms following grammar represented regular expression quotation reference sentence specifies quotation must contain reference followed one sentences use elements eg nesting sentence reference preceding reference sentence disallowed thus main benefit grammar new modified documents automatically verified compliance specification benefits include support queries structural conditions optimization physical database layout another important purpose grammar provide users general understanding texts organization overall grammar text database serves much purpose schema traditional database gives overall description data organized 18 widely used standard text element markup grammar specification sgml standard generalized markup language 24 recently xml 7 html represents specific application sgml ie uses single grammar set elements although grammar restrictive 12 automated document recognition unfortunately many electronic documents exist elements grammar implicitly defined typically layout typesetting conventions exam ple world wide web data laid according conventions must inferred use easily organized database 41 therefore pressing need convert structural information documents explicit form order gain full benefits online availability completely manual conversion would time consuming collection larger kilobytes therefore automated interactive methods needed two distinct subproblems element recognition grammar generation document represented procedural presentational markup first subproblem recognize mark individual structural elements based layout typesetting clues necessary know infer original conventions used map element types layout address task several existing approaches based interactive systems 14 26 learning systems 32 manual trial error construction finite state transducers 11 25 second subproblem extract implicit structural rules collection documents model grammar requires plausible model original intentions authors reconstructed extrapolating available examples appropriate way considered application grammatical inference general problem deals constructing grammars consistent training data 42 grammar inference text 3 note two problems may depend element recognition often requires ambiguities resolved considering elements used context however recognition usually considers usage rules isolation identifies ones really needed recognize element grammar considered single representation usage rules including ones relevant recognition may majority thus even certain components grammar need determined manually recognition phase grammar inference still useful automatically combining components filling ones needed recognition grammar inference problem especially applicable xml increasingly important variant sgml rich tagging allowed without requiring dtd grammar case recognition subproblem grammar generation comprises entire recognition problem benefits attaching grammar documents seen recent experience database system lore 31 lore manages semistructured data relationships among elements unconstrained place schemas dataguides automatically generated capture concise form relationships occur 17 used traditional schema roles query optimization aiding query formulation interestingly dataguide treestructured database analogous context free grammar 13 data describe approach problem using computerized version oxford english dictionary oed 39 large document complex structure 34 containing sixty types elements heavily nested two million element instances figure 1 lists common elements appear typical entry see guide berg 6 complete explanation structural elements used oed oed converted original printed form intermediate keyed form typesetting information computerized form explicitly tagged structural elements 25 text broken two hundred ninety thousand top level elements dictionary entries grammar inference problem considered one finding complete grammar describe top level entries broken many subproblems finding grammars lower level elements quotation paragraphs etymologies choice data oed good representative general problem inferring grammars text structure least complex text two ways amount structure information extremely high usage structural elements quite variable large amount structure information evidenced number different types tags also high density tagging compared text 35 sixty types tags density tagging headword group hg headword lemma hl lookup form lflf stressed form sfsf murray form mfmf end headword lemma hl murray pronunciation mprmpr ipa pronunciation ipripr part speech psps homonym number hoho end headword group hg variant form list vl variant date vdvd variant form vfvf end variant form list vl sense number definition defdef quotation paragraph qp earliest quote eqq date dd author aa work ww end earliest quote qeq latest quote lqqqlq obsolete entries end quotation paragraph qp subentry preceded byhence se bold lemma similar tags blbl following headword end subentry se end senses s0s1s8 end entry e figure 1 common elements oed corresponding tags order appear typical dictionary entry reproduced berg 5 represents humangenerated high level description data indentation denotes containment therefore headword group variant form list etymology senses top level elements entry children indented one step etc note one sense element occur top level grammar inference text 5 even element names abbreviated three fewer characters amount tagging comparable amount text text restrictions element usage tend vary one extreme element occur anywhere eg italics html elements always occur exactly way eg newspaper article always contains title followed byline followed body neither extremes interesting grammar inference element usage oed however constrained yet quite variable mainly consequence circumstances original publication compilation spanned 44 years filled 15000 pages seven different chief editors lifetime project completed 1933 well possibility computer support structure oed remarkably consistent variable enough appropriate problem test method regular enough make description grammar appropriate 14 text structure grammatical inference demonstrate use marked text oed training data grammatical inference process consider structure two short oed entries shown figure 2 represented derivation trees shown figure 3 nodes labeled corresponding tag names corresponding grammar representation shown figure 4 production left hand side corresponding nonleaf node right hand side formed immediate children number times production occurs data indicated preceding frequency nonterminal strings right hand sides productions considered training set single sub problem note generalize production regular expression overall grammar context free 28 standard choice modeling text structure three existing grammatical inference approaches text operate specifying rules used rewrite combine strings training set 10 14 38 following rule example generalizes grammar expanding language accepts greater equal given value applied first entry figure 2 example rules effect language simplify grammar combining productions applied two hg productions figure 2 example first rule gives mfsalamasdndrous mfhlbb psapshg lbrarelbsu1su etf l cf xlousxlxret s4s6defliving fire fiery hot passionate defqpqd1711d ag carya wphys phylw 29 tmy salamandrous spiritmy aetnous burning humourstqqps6 wexpos dom epist amp gospw wks 1629 76 tif salamandry spirit traduce godly labour silenced ministers haue wronged communion booke mfusdnderstrife mfhlbb hglbpoetlb xret s4s6defstrife carried upon earth defqpeqqdc 1611d achapmana wiliadw xx 138 twe soon shallsend heaven settle abode equals flying underdubhstrifest figure 2 two marked dictionary entries work ahonen mannila nikunen 1 2 3 uses classical grammatical inference approach generating language belonging characterizable subclass regular languages specifically use languages extension propose kcontextual languages contrast previous approaches grammar construction text structure use frequency information training set generate stochastic gram mar nonstochastic approaches mentioned inappropriate larger document collections complex structure way effectively deal inevitable errors pathological exceptions cases without considering frequency information stochastic grammars also better suited understanding exploration since express additional semantics provide natural way distinguishing significant components grammar thesis ahonen 1 partially address concerns example applying method finnish dictionary similar complexity oed first removed frequent cases training set grammar inference text 7 davinci v142 qp ps mf qp xr xr cf su ps mf davinci v203 qp ho xr mf figure 3 two parse trees also proposes adhoc methods separating final result high low frequency components generated result method consider frequency information assert better use inference method considers frequency information beginning stochastic grammatical inference algorithm chosen adopt application proposed carrasco oncina 9 modifications motivated shortcomings ability tune algorithm wish use interactively exploration rather black box produce single final result note understanding techniques figure 4 defacto grammar production frequencies needed support effective interaction primary technique used examples paper visualizing automata bubble diagrams bubble diagrams generated graph visualization program davinci performs node placement edge crossover minimization 15 remainder paper follows section 2 describes underlying algorithm section 3 explains modifications section 4 evaluates method real synthetic data section 5 concludes 2 algorithm provide overview algorithm carrasco oncina 9 enough detail explain modifications longer technical report includes detailed descriptions modified unmodified algorithms 43 many possible stochastic grammatical inference algorithms particular one used chosen several reasons first similar method ahonen et al uses finite automaton state merging paradigm since work represents indepth examination grammar inference text structure date reasonable use similar approach fact many results go beyond application algorithm rewriting automaton gram grammar inference text 9 mar consisting productions adapted outputs algorithm second reason choosing algorithm basic generalization operation merging states guided justifiable statistical test rather arbitrary heuristic bayesian model merging approach stolcke omohundro 40 probability estimation approach based forwardbackward algorithm 23 30 candidates satisfying characteristic final choice judged simplest elegant 21 alergia algorithm alergia carrasco oncina 9 adaptation nonstochastic method oncina garcia 33 algorithm produces stochastic finite automata sfas grammar constructs informally explained finite automata probabilities associated transitions probability assigned string therefore product probabilities transitions followed accepting note every state also associated termination probability included product state nonzero termination probability considered final state see book fu 16 formal complete description sfas properties inference paradigm used alergia common one first build defacto model accepts exactly language training set generalize generalization finite automata done merging states similar state merging operation used algorithm minimizing nonstochastic finite automaton 21 difference merges change accepted language allowed consider example productions et training data figure 4 represented prefix tree figure 5 primitive operation merging two states replaces single state labeled convention smaller two state identifiers incoming outgoing transitions combined frequencies associated transitions common added incoming termination frequencies figure 6 demonstrates effect merging states 2 4 2 5 note two states outgoing transitions symbol different destinations two destinations also merged avoid indeterminism thus merging two nonleaf states recursively require merging long string descendants algorithm based paradigm must define two things test whether two given states merged order pairs states tested alergia two states merged satisfy following equivalence criterion every symbol alphabet associated transition probabilities states equal termination probabilities states equal destination states two transitions symbol equivalent according recursive application criterion figure 5 defacto sfa prefix tree et element states labeled idnt id state identifier n incoming frequency termination frequency transitions labeled sf symbol f transition frequency final states nonzero termination frequencies marked double rings figure 6 figure 4 states 24 5 merged grammar inference text 11 whether two transitions probabilities equal decided statistical test observed frequencies let null hypothesis h equal alternative h differ let number strings arrive states f 1 number strings follow transitions question terminate using hoeffding bound 19 binomial distributions pvalue less chosen significance level ff test statistic greater expression case reject null hypothesis assume two probabilities different otherwise assume test ensures chosen ff represents bound probability incorrectly rejecting null hypothesis ie incorrectly leaving two equivalent nodes separate thus reducing ff makes merges likely results smaller models order pairs nodes compared defined follows nodes numbered breadthfirst order nodes given depth ordered lexically based string prefixes used reach node figure 5 example pairs nodes q tested varying j 1 number nodes 0 1 nonstochastic version algorithm ordering necessary prove identification limit 33 significance stochastic version unclear note worst case time complexity algorithm 3 occurs input merges take place thus requiring nn gamma 12 pairs nodes compared furthermore average recursive comparison continues depth practise expected running time likely much less carrasco oncina report experimentally time increases linearly number strings training set artificially generated fixed size model observed quadratic increase size model however since size normally chosen parameter adjustment small enough understanding running time problem experience 3 modifications section present modifications algorithm using pqp pseudoquotation paragraph element oed example 145289 instances element entire dictionary 90 unique arrangements subelements thus 90 unique strings appear right sides productions defacto grammar shown figure 7 along occurrence frequencies 4 elements occur pqp earliest quotation q quotation sq subsidiary quotation lq latest quotation usage elements known dictionary also deduced examples follows sq occur number times position q occur number times eq occur must occur first q lq occur must occur last q data simple intended illustrate modified learning algorithm give complex examples section 42 31 separation low frequency components original algorithm assumes every node frequency high enough statistically compared typically valid nodes low frequency always default null hypothesis equivalence resulting inappropriate merges characteristic result many low frequency nodes merge either root another low index node since comparisons made order index gives structure many inappropriate transitions pointing back low index nodes figure 8 shows example result pqp data transitions occur several parts model nodes 0 1 note form inappropriate merging problem remedied tuning single parameter ff usual hypothesis tests ff bound probability false reject null hypothesis ie failing merge two nodes fact equivalent complementary bound fi probability false accept unconstrained test alergia arbitrarily high low frequencies problem seen closely related small disjuncts problem discussed holte et al 20 rule based classification algorithms essentially rules covering cases training data perform relatively badly since inadequate statistical support holte et al give three general approaches improving situation 1 use exact significance tests learning algorithm test significance error rate every disjunct evaluating result whenever possible use errors omission instead default classifications note since training set includes positive examples second point apply first point however corresponds one modifications third agrees conclusions experiments several different treatments low frequency nodes led us conclusion single approach would always produce appropriate result certainly original action algorithm automatically merging first comparison understandable given frequencies question statistically insignificant therefore chose first incorporate significance test algorithm separate low frequency nodes automatically later decide alternative treatments nodes discussed section 34 following test standard one checking equivalence two binomial proportions considering significance see 13 example assume grammar inference text 13 21 3 qqsqq 524 eq 4 qqsqqq 5 eqlq 3 qqsqqqq 294 eqq 2 qqsqqqqq 1 eqqlq 58 qsq 30 eqqqqq 1 eqqqqqlq 9 qsqqqq 15 eqqqqqq 2 qsqqqqq 8 eqqqqqqq 3 qsqqqqqq 5 eqqqqqqqq 2 qsqqqqqqq 3 eqqqqqqqqq 1 qsqqqqqqqq 3 eqqqqqqqqqq 1 qsqqqqqqqqqq 1 eqqqqqqqqqqqqqqqq 2 qsqsq 2 eqsq 22 sq 28 lq 2 sqeq 20 qlq 5 sqeqqq 3 sqeqqqq 5 qqqlq 174 sqqq 6335 qqqq 177 sqqqq 1 qqqqlq 102 sqqqqq 579 qqqqqqq 9 sqqqqqqqq 293 qqqqqqqq 8 sqqqqqqqqq 124 qqqqqqqqq 2 sqqqqqqqqqq 93 qqqqqqqqqq 2 sqqqqqqqqqqq 2 sqqqqqqqqqqqq 4 qqqqqqqqqqqqqqqq 2 sqqqsq 2 qqqqqqqqqqqqqqqqqq 1 qqqqqqqqqqqqqqqqqqqqqqqqq 2 qqqqqsq 1 qqqqsqqqqqq 4 qqqsq 2 qqqsqqq 1 qqqsqqqq 1 qqqsqqqqqq 1 qqqsqqqqqqq 17 qqsq figure 7 pqp example strings davinci v142 30886455 q545 7142585 678455563 figure 8 result unmodified algorithm transitions characteristic inappropriate low frequency merges note termination transition frequencies f shown converted percentages representing f n subsequent figures simplify comparisons grammar inference text 15 true probabilities f 1 n 1 f 2 n 2 serve estimates sample sizes required satisfy following relationship ff fi bound probabilities false reject false accept null ae 2ffl z x denotes value cumulative probability standard normal distribution equal 1 gamma x value ffl additional parameter required bound fi representing minimum true difference p1 p2 null hypothesis rejected iff incorporate test algorithm following way associate boolean flag node initially false set flag true first time node involved comparison another node satisfies required relationship ff fi sample sizes nodes still false flags algorithm terminates classified low frequency components example result pqp data shown figure 9 low frequency nodes graph depicted rectangles 32 control level generalization important interactive operation control level generalization much finite language represented training set expanded one possible approach vary ff fi reducing ff increases generalization restricts possibility incorrectly leaving nodes separate therefore makes merges likely increasing fi also increases generalization increases allowable possibility incorrectly merging nodes note two completely equivalent since ff fi bounds probabilities respective errors unfortunately appropriate control generalization way since ff fi directly determine components data treated low frequency significant ie parts merged default using original algorithm classified low frequency according test section 31 therefore unless position arbitrarily vary amount available data according choice parameters another modification needed goal allow independent control division low high frequency components level generalization done changing 993333 954250 876333 30374473 q903 q643 q875 1921000 982500 94200 90200 823333 q583 5016250 2842310 figure 9 pqp inference result grammar inference text 17 hypothesis statistical test rather testing whether two observed proportions plausibly equal test whether plausibly differ less parameter fl modified test follows let 1 f 1 n 1 2 f 2 n 2 reject h larger value fl results null hypothesis easier satisfy therefore producing merges increase generalization example consider 3 results figures 10 11 12 constant ff fi values varying fl values low frequency components clipped moment higher fl values result fewer nodes larger languages less precise probability predictions 33 choosing algorithm parameters modified algorithm following parameters ffl fl maximum difference true proportions algorithm merge two states ffl ff probability bound chance making type error incorrectly concluding two proportions differ fl ffl fi probability bound chance making type ii error incor rectly concluding two proportions differ less fl true difference proportions least fl fourth parameter next describe effects changing parameters also explain useful interaction necessarily require separate control four choosing fl controls amount generalization setting 0 results states merged setting 1 always results output single state effectively 0context average frequency occurrence every symbol changes ff fi also affect level generalization main effect interest however define cutoff high low frequency components increasing either one decreases number nodes classified low frequency simplified interaction possible always equal davinci v203 q903 962963 grammar inference text 19 davinci v203 q903 davinci v203 figure 12 pqp inference result adjust together single parameter seriously reduce useful level control algorithms behavior ffl parameter determines difference fi probability applies must specified somewhere especially useful value control therefore fixed tied way size input value fl choose fix seen control really needed two major aspects inference process choosing combined value ff fi sets cutoff point significant data low frequency components choosing fl controls amount generalization example parameter adjustment consider inference result figure 9 examination reveals two possible changes first based observation nodes 1 3 similar accept lq sq number qs transition termination probabilities differ less ten percent unless slight differences deemed significant enough represent model better merge two nodes done increasing fl 01 thus allowing nodes test equal true probabilities differ ten percent second adjustment affects nodes 4 12 21 express fact strings starting sq much likely end two qs rule applies however five hundred one hundred grammar inference text 21 q903 30374473 876333 954250 993333 q875 q643 7142585 1921000 2842310 5016250 q583 823333 90200 94200 982500 figure 13 pqp inference result forty five thousand pqps dictionary choose simplify model expense small amount inaccuracy cases reduce ff fi reclassify nodes low frequency bisection search values ff fi 0 0025 reveals accomplished result application two adjustments described shown figure 13 34 low frequency components three possible ways treating low frequency components assume specific possible model leaving components separate leaving fi fixed allowing ff grow arbitrarily high merge low frequency components single garbage state approach adopted 36 merge low frequency nodes parts automaton many methods invented last approach observed general single method produce appropriate results components given model therefore propose tentative merging strategy first ordered list heuristics defined low frequency components merged positions model determined first heuristic list user identifies problem particular resulting tentative transition subtree remerged position determined next heuristic list heuristics designed based various grammatical inference learning approaches note problem choosing place merge low frequency component differs general problem stochastic grammatical inference two ways 1 rest high frequency model available source information 2 frequency information classified insignificant second point implies choose consider frequency information may use special techniques compensate could include laplace approximation probability bayesian approach using prior probability evidence measures developed recent work dfa rather sfa learning might also applicable 29 mention two heuristics use frequency information found work well guarantee model still able parse strings training set first merge every low frequency node immediate parent result terminals occurring low frequency subtree allowed occur number times order starting nearest highfrequency ancestor second heuristic locate position high frequency model entire lowfrequency subtree parsed subtree merged rest model replacing single transition identified position one possible position exists stepped proceeding next heuristic list example application second heuristic reconsider figure 13 merging every low frequency tree graph first lowest index node parse gives result figure 14 tentative transitions diagram grammar inference text 23 davinci v142 figure 14 figure 13 low frequency components merged parts graph marked dashed lines tentative transition node 1 0 input sq creates cycle allows one eq occur violates proper usage element outlined section 3 repointing transition node 1 alternate destination parses low frequency subtree gives acceptable result pqp element 4 evaluation section compare modified algorithm henceforth referred mod alergia alergia first using data drawn four different texts compare performance automatic searches use two specific examples illustrate points comparison 41 batch experiments use following four texts automatic search experiments ffl oed oxford english dictionary 39 500 mb exhibits complex sometimes irregular structure pharmaceutical database electronic version publication describes drugs available canada 27 exhibits mix simple complex structure ffl oald oxford advanced learners dictionary 22 17 mb exhibits complex structure regular oed designed beginning computerization ffl howto sgml versions howto documents linux operating system 10 mb exhibits relatively simple structure terminal structural elements nonterminals little substructure worth performing inference arbitrary cutoff discard give defacto automatons fewer 10 states leaves 24 elements oed 24 cps 23 oald 14 howto total 85 data sets procedure data set follows 1 randomly split strings equal size training validation test sets 2 let x size number states defacto automaton training set generate collection models various sizes using two methods test x parameter values using alergia enough find possible models test using modalergia behaves alergia test remaining x2 parameter varying ff fi fl merge low frequency components immediate parents 3 assess goodness model using metric values calculated validation set see unique model size number states keep best model generated method 4 recalculate metrics selected models test set compare results two methods two different metrics used first cross entropy also called kullbackliebler divergence quantifies probabilistic fit one model another measure previously used evaluate stochastic grammatical inference methods 37 given two probabilistic language models 1 2 cross entropy defined pm1 x probability set pm2 x probability assigned model estimate l first equal validation set later test set strings validation test sets recognized model find total probability call error corresponds probability rejecting valid string sfa stripped probabilities grammar inference text 25 model size b 543 figure 15 average percentage size interval covered example 543 percent interval 216 means average models interval column gives models generated methods b columns give models generated alergia modalergia searches used dfa use second metric also rather using infinite log value cases give infinite cross entropy use largest log value observed data set gives finite metric missing string adds penalty equal worst present string values ff fi fl chosen repeatedly scanning unit interval successively smaller increments chosen number points tested example first pass starts 05 increments 10 thus testing one point second starts 025 increments 05 testing two values fl used directly ff fi values squared first allow probability differences evenly spaced recall equivalence test used unmodified alergia since different ranges model sizes useful different purposes break results following size intervals 216 1732 3364 65128 size 1 excluded size 1 models given element also try ensure search method tests approximately equal number points interval done recording parameter intervals corresponding model size intervals avoiding parameters interval enough points tested sun supersparc runs averaged 11 minutes per element 85 elements total 2872 models different sizes generated 2263 found searches 27 found alergia search found modalergia search figure 15 shows average percentage size interval covered general fact modalergia finds significant number models alergia makes useful searching model particular size give example next section figure compares cross entropy error 2263 model sizes found methods across 85 data sets improvements diminish larger models larger models keep high frequency nodes defacto automaton high frequency nodes tend statistically significant model size cross entropy error figure 16 average percent improvements modalergia alergia cross entropy error models different size intervals bracketed numbers pvalues one sided test difference greater 0 avg node freq cross entropy error 1220 41 5e15 48 4e17 figure 17 average percent improvement modalergia alergia cross entropy error models generated defacto automata various average node frequencies intervals chosen break data four equal size sets bracketed numbers pvalues one sided test difference greater 0 therefore treated essentially algorithms leaving fewer low frequency components treat differently put another way fewer different ways modalergia generate model many states thus less differentiation alergia model addition target model size size variability training set affects relative performance two algorithms quantify amount calculating average node frequency defacto automaton sum string lengths training set divided number states defacto automaton sorting 2263 data points according value breaking four equal size sets gives results figure 17 expected modified algorithm better less frequency information available experiments demonstrate modalergia used produce models given size range even completely automatic procedure simple default treatment low frequency components used find probabilistically better models advantage modalergia greatest small models low frequency training sets grammar inference text 27 davinci v142 figure 18 inference result entry element oed 42 particular examples section gives two examples demonstrate advantages stochastic inference approach general modified algorithm particular first example use entry element oed create overgeneralized model compare prototypical entry presented figure 1 alergia produce models element size range 2 nodes even using bisection search narrows search interval way adjacent numbers double precision floating point representation contrast modalergia search described previous section generates model every size interval inspection smaller models found seven node graph shown figure 18 similar prototypical entry model highlights several interesting characteristics one level model rest high0011000000111111000000000111111111 figure 19 first three nodes monograph element cps data clipped components indicated scissors low frequency components paths hg vl et s4 correspond prototypical entry note semantics present nonstochastic description ffl variant form list vl optional actually omitted often ffl etymology et also omitted skipping directly senses often happen ffl element mentioned figure 1 status st frequently precedes headword group hg presence significantly increases chance et vl bypassed bypassed however label lb element normally inserted hg ffl number lbs also occur entry without st usually however many occur loop probability 0262 properties extremely useful comes exploring understanding data even disregarded standard grammar applications validating document instance furthermore stochastic properties grammar used exercise editorial control new entries introduced dictionary patterns rarely occur first trigger message operator doublecheck correctness asserted grammar inference text 29 intended entered yet flagged subsequent review approval higherlevel editorial authorities second example use monograph element cps data comment advantage separating low frequency components already done pqp example figure 19 shows first three high frequency nodes model data outgoing low frequency components shown clipped get final detailed model need expand examine subtrees low frequency components one time subtree option interactively stepping positions merge instance immediate parent nodes parsed deciding change inference parameters reclassify part high frequency deciding represents error data type interactive correction possible unmodified alergia 5 conclusions future work study concerned application grammatical inference text structure subject addressed 1 2 3 10 14 38 grammar generation important tool maintaining document database system useful creating grammars standard text database purposes also allows flexible view rather fixed grammar describes possible forms data grammar fluid evolves grammar change new data added many different forms grammar generated time overgeneralized highlevel view description subset data example thus generate grammars much summarize understand organization text serve traditional capacities like schema approach adds two things previous approaches extension stochastic grammatical inference algorithm greater freedom interactive tun ing advantages changing stochastic inference follows ffl stochastic inference effective since uses frequency information part inference process true learning method ffl stochastic models richer semantics therefore easier interpret interactively adjust demonstrated entry example section 42 note stochastic models easily converted nonstochastic ones dropping probability information therefore free use algorithm effective method learning nonstochastic models ffl stochastic inference framework allows parameterization used produce different models single data set flexibility used search single best model explore several models different generalization levels existing nonstochastic approaches problem work black boxes producing single untunable result additional tunability modified algorithm shown useful two ways experimental evaluation using four different texts two examples using specific elements texts possibilities exist improvement algorithm example state merging paradigm learning finite automata seen development since alergia first published particular control strategy compares merges nodes nonfixed order developed 29 gives freedom merge nodes order evidence supporting merges incorporating algorithm would straightforward especially view fact trivial convert result statistical test evidence measure another improvement would develop evidence measures assist user choosing merge destinations low frequency components possible starting points mentioned section 34 much future work exists integrating method system support traditional applications example semistructured database system lore 31 generate schemas use query planning optimization performs generalization effectively stopping prefix tree schemas therefore necessarily compact understandable addition traditional applications stochastic part grammar also suggests many novel applications example system could constructed assist authors creation documents flagging excessively rare structures process creation listing possible next elements partially complete entries order probability stochastic grammars could also used structural classifiers characterizing authoring styles two people use tag set acknowledgments financial assistance natural sciences engineering research council canada postgraduate scholarship information technology research center communications information technology ontario university waterloo gratefully acknowledged r generating grammars structured documents using grammatical inference methods forming grammars structured docu ments application grammatical inference generating grammars sgml tagged texts lacking dtd structured documents research potential electronic oed2 database university waterloo guide scholars guide oxford english dictionary essential companion users guide extensible markup language xml 10 learning stochastic regular grammars means state merging method grammar generation query processing text databases finite state transduction tools markup systems future scholarly text processing davinci 14 user manual syntactic pattern recognition applications enabling query formulation optimization semistructured databases mind grammar new approach modelling text probability inequalities sums bounded random variables concept learning problem small disjuncts introduction automata theory oxford advanced learners dictionary current english fourth edition hidden markov models speech recognition structuring text oxford english dictionary finite state transduction structured document database system krogh editor compendium pharmaceuticals specialties regular right part grammars parsers results abbadingo one dfa learning competition new evidence driven state merging algorithm estimation stochastic contextfree grammars using insideoutside algorithm database management system semistructured data wrapper induction semistructured inferring regular languages polynomial updated time oxford university press visualizing text learnability usage acyclic probabilistic automata statistical inductive learning regular formal languages creating dtds via gbengine fred oxford english dictionary inducing probabilistic grammars bayesian model merging grammatical inference introductory survey application stochastic grammatical inference method text struc ture tr markup systems future scholarly text processing learnability usage acyclic probabilistic finite automata lore regular right part grammars parsers introduction automata theory languages computation hidden markov models speech recognition grammatical inference learning stochastic regular grammars means state merging method forming grammars structured documents inducing probabilistic grammars bayesian model merging dataguides mind grammar statistical inductive learning regular formal languages ctr robert h warren frank wm tompa multicolumn substring matching database schema translation proceedings 32nd international conference large data bases september 1215 2006 seoul korea minos garofalakis aristides gionis rajeev rastogi seshadri kyuseok shim xtract learning document type descriptors xml document collections data mining knowledge discovery v7 n1 p2356 january enrique vidal franck thollard colin de la higuera francisco casacuberta rafael c carrasco probabilistic finitestate machinespart ieee transactions pattern analysis machine intelligence v27 n7 p10131025 july 2005 enrique vidal frank thollard colin de la higuera francisco casacuberta rafael c carrasco probabilistic finitestate machinespart ii ieee transactions pattern analysis machine intelligence v27 n7 p10261039 july 2005