atomic decomposition basis pursuit timefrequency timescale communities recently developed large number overcomplete waveform dictionaries stationary wavelets wavelet packets cosine packets chirplets warplets name decomposition overcomplete systems unique several methods decomposition proposed including method frames mof matching pursuit mp special dictionaries best orthogonal basis bobbasis pursuit bp principle decomposing signal optimal superposition dictionary elements optimal means smallest l1 norm coefficients among decompositions give examples exhibiting several advantages mof mp bob including better sparsity superresolution bp interesting relations ideas areas diverse illposed problems abstract harmonic analysis total variation denoising multiscale edge denoisingbp highly overcomplete dictionaries leads largescale optimization problems signals length 8192 wavelet packet dictionary one gets equivalent linear program size 8192 212992 problems attacked successfully recent advances linear programming interiorpoint methods obtain reasonable success primaldual logarithmic barrier method conjugategradient solver b introduction last several years explosion interest alternatives traditional signal representations instead representing signals superpositions sinusoids traditional fourier representation available alternate dictionaries collections parameterized waveforms wavelets dictionary best known wavelets steerable wavelets segmented wavelets gabor dictionaries multiscale gabor dictionaries wavelet packets cosine packets chirplets warplets wide range dictionaries available dictionary collection waveforms oe fl fl2gamma fl parameter envision decomposition signal approximate decomposition r residual depending dictionary representation decomposes signal pure tones fourier dictionary bumps wavelet dictionary chirps chirplet dictionary etc new dictionaries overcomplete either start way merge complete dictionaries obtaining new megadictionary consisting several types waveform eg fourier wavelets dictionaries decomposition 11 nonunique elements dictionary representations terms elements 11 goals adaptive representation nonuniqueness gives us possibility adaptation ie choosing among many representations one suited purposes motivated aim achieving simultaneously following ffl sparsity obtain sparsest possible representation object one fewest significant coefficients ffl superresolution obtain resolution sparse objects much higherresolution possible traditional nonadaptive approaches important constraint perhaps conflict goals ffl speed possible obtain representation order logn time 12 finding representation several methods proposed obtaining signal representations overcomplete dictionaries range general approaches like method frames 8 method matching pursuit 23 clever schemes derived specialized dictionaries like method best orthogonal basis 6 methods described briefly section 23 view methods advantages shortcomings principal emphasis proposers methods achieving sufficient computational speed resulting methods practical apply real data show computational examples methods either quite generally important special cases lack qualities sparsitypreservation stable superresolution 13 basis pursuit basis pursuit bp finds signal representations overcomplete dictionaries convex op timization obtains decomposition minimizes 1 norm coefficients occurring representation nondifferentiability 1 norm optimization principle leads decompositions different properties method frames particular much sparser based global optimization stably superresolve ways matching pursuit bp used noisy data solving optimization problem trading quadratic misfit measure 1 norm coefficients examples show stably suppress noise preserving structure wellexpressed dictionary consideration bp closely connected linear programming recent advances largescale linear programming associated interiorpoint methods applied bp make possible certain dictionaries nearlysolve bp optimization problem nearly linear time implemented primaldual log barrier interiorpoint method part computing environment called atomizer accepts wide range dictio naries instructions internet access atomizer given section 66 experiments standard timefrequency dictionaries indicate potential benefits bp experiments nonstandard dictionaries like stationary wavelet dictionary heaviside dictionary indicate important connections bp methods like mallat hwangs multiscale edge representation osher rudin fatemis total variationbased denoising methods number dictionaries existing methods overcomplete representation section 3 discuss principle basis pursuit relations existing methods ideas fields section 4 discuss methodological issues associated bp particular interesting nonstandard ways deployed section 5 describe basis pursuit denoising method dealing problem 12 section 6 discuss recent advances largescale linear programming resulting algorithms bp section 7 discuss number connections work representations n discretetime signal length n may also viewed vector r n interested reconstruction signal using superpositions elementary waveforms traditional methods analysis reconstruction involve use orthogonal bases fourier basis various discrete cosine transform bases orthogonal wavelet bases situations viewed follows given list n wave forms one wishes represent linear combination waveforms waveforms list viewed vectors r n linearly independent representation unique 21 dictionaries atoms considerable focus activity recent signal processing literature development signal representations outside basis setting use terminology introduced mallat zhang 23 dictionary collection parameterized waveforms gamma waveforms oe fl discretetime signals length n called atoms depending dictionary parameter fl interpretation indexing frequency case dictionary frequency fourier dictionary indexing timescale jointly case dictionary timescale dictionary indexing timefrequency jointly case dictionary timefrequency dictionary usually dictionaries complete overcomplete case contain exactly n atoms n atoms one could also continuum dictionaries containing infinity atoms undercomplete dictionaries special purposes containing fewer n atoms dozens interesting dictionaries proposed last years focus paper half dozen much applies cases well 211 trivial dictionaries begin overly simple examples dirac dictionary simply collection waveforms zero except one point course also orthogonal basis r n standard basis heaviside dictionary collection waveforms jump one particular point 1 ftflg atoms dictionary orthogonal every signal representation 212 frequency dictionaries fourier dictionary collection sinusoidal waveforms oe fl indexed angular frequency variable 2 f0 1g indicates phase type sine cosine detail standard fourier dictionary let fl run set cosines fourier sines fourier frequencies dictionary consists n waveforms fact basis simple one atoms mutually orthogonal overcomplete fourier dictionary obtained sampling frequencies finely let whole number 1 let collection cosines sines 1 fold overcomplete system also use complete overcomplete dictionaries based discrete cosine transforms sine transforms 213 timescale dictionaries several types wavelet dictionary fix ideas consider haar dictionary father wavelet dictionary collection translations dilations basic mother wavelet together translations father wavelet indexed scale location 2 f0 1g indicates gender detail standard haar dictionary let fl run discrete collection mother wavelets dyadic scales locations integer multiples scale b collection father wavelets coarse scale j 0 dictionary consists n waveforms orthonormal basis overcomplete wavelet dictionary obtained sampling locations finely one location per sample point gives socalled stationary haar dictionary consisting log 2 n waveforms called stationary since whole dictionary invariant circulant shift variety wavelet bases possible important variations smooth wavelet bases using splines using wavelets defined recursively twoscale filtering relations 9 although rules construction complicated boundary conditions 25 orthogonality versus biorthogonality 9 etc indexing structure standard haar dictionary paper use symmlet8 smooth wavelets ie daubechies nearly symmetric wavelets eight vanishing moments see 9 examples 214 timefrequency dictionaries much recent activity wavelet communities focused study timefrequency phenomena standard example gabor dictionary due gabor 1946 notation take frequency location phase ffit duration consider atoms oe atoms indeed consist frequencies near essentially vanish far away fixed ffi discrete dictionaries built timefrequency lattices delta delta chosen sufficiently fine complete discussions see eg 8 recently coifman meyer 5 developed wavelet packet cosine packet dictionaries especially meet computational demands discretetime signal processing 1d discrete time signals length n dictionaries contain n log 2 n waveforms wavelet packet dictionary includes special cases standard orthogonal wavelets dictionary dirac dictionary collection oscillating waveforms spanning range frequencies durations cosine packet dictionary contains special cases standard orthogonal fourier dictionary variety gaborlike elements sinusoids various frequencies weighted windows various widths locations paper often use wavelet packet cosine packet dictionaries examples overcomplete systems give number examples decomposing signals time frequency b phase plane frequency domain fftwaveletpacket337 frequency 0505c time domain time figure 21 timefrequency phase plot wavelet packet atom timefrequency dictionaries simple blockdiagram helps us visualize atoms appearing decomposition diagram adapted coifman wickerhauser 6 associates cosine packet wavelet packet rectangle timefrequency phase plane association illustrated figure 21 certain wavelet packet signal superposition several waveforms indicate waveforms appear superposition shading corresponding rectangles timefrequency plane 215 dictionaries always merge dictionaries create megadictionaries examples used include mergers wavelets heavisides 22 linear algebra suppose discrete dictionary p waveforms collect waveforms columns n p matrix phi say decomposition problem 11 written vector coefficients 11 dictionary furnishes basis phi n n nonsingular matrix unique representation atoms addition mutually orthonormal phi decomposition formula simple important trivial comment given dictionary waveforms one distinguish analysis synthesis synthesis operation building signal superposing atoms involves matrix n analysis involves operation associating signal vector coefficients attached atoms involves matrix p n phi synthesis analysis different linear operations must take care distinguish one avoid assuming analysis operator us coefficients used synthesize signal carbon b synthesis phase plane time frequency c analysis phase plane time frequency sorted coefficients order amplitude synthesis solid analysis dashed figure 22 analysis versus synthesis signal carbon overcomplete case interested p ae n phi invertible many solutions 22 given approach selects particular solution one uniquely automatically solve synthesis problem applying simple linear analysis operator illustrate difference synthesis panel 22a shows signal carbon panel 22b shows timefrequency structure sparse synthesis carbon vector ff yielding using wavelet packet dictionary visualize decomposition present phaseplane display shaded rectangles described panel 22c gives analysis carbon coefficients displayed phaseplane analysis synthesis large difference sparsity panel 22d compare sorted coefficients overcomplete representation synthesis analysis coefficients 23 existing decomposition methods several currently popular approaches obtaining solutions 22 231 frames method frames mof 8 picks among solutions 22 one whose coefficients minimum l 2 norm subject 23 solution problem unique label ff geometrically collection solutions 22 affine subspace r n mof selects element subspace closest origin sometimes called minimumlength solution matrix phi generalized inverse phi calculates minimumlength solution system linear equations signal hydrogen b ideal phase plane time frequency c phase plane mof time frequency figure 23 mof representation sparse socalled tight frame dictionaries mof available closed form nice example standard wavelet packet dictionary one compute vectors v kphi vk short phi notice phi simply analysis operator two key problems method frames first mof sparsity preserving underlying object sparse representation terms dictio nary coefficients found mof likely much less sparse atom dictionary nonzero inner product signal least potentially also usually member solution figure 23a shows signal hydrogen made single atom wavelet packet dic tionary result frame decomposition dictionary depicted phaseplane portrait figure 23c underlying signal synthesized single atom frame decomposition involves many atoms phaseplane portrait exaggerates greatly intrinsic complexity object second mof intrinsically resolutionlimited object reconstructed features sharper allowed underlying operator phi phi suppose underlying object sharply localized reconstruction ff instead phi phiff overcomplete case spatially spread figure 24 presents signal twinsine consisting superposition two sinusoids separated less socalled rayleigh distance 2n analyze 4fold overcomplete discrete cosine dictionary case reconstruction mof figure 24b simply convolution dirichlet kernel result synthesis coefficients broad oscillatory appearance consisting two many frequencies giving visual clue object may synthesized two frequencies alone 232 matching pursuit mallat zhang 23 discussed general method approximate decomposition 12 addresses sparsity issue directly starting initial approximation signal twinsine 050515frequencynyquist b mof coefs amplitude 050515frequencynyquist c mp coefs amplitude 050515frequencynyquist bp coefs amplitude figure 24 analyzing twinsine 4fold overcomplete discrete cosine dictionaries residual r builds sequence sparse approximations stepwise stage k identifies dictionary atom best correlates residual adds current approximation scalar multiple atom r steps one representation form 12 residual r r similar algorithm proposed gabor dictionaries qian chen 30 intrinsic feature algorithm stopped steps yields approximation using atoms dictionary orthogonal method works perfectly object made n atoms algorithm run steps recovers underlying sparse structure exactly dictionary orthogonal situation less clear algorithm myopic one expects certain cases might choose wrongly first iterations cases end spending time correcting mistakes made first terms fact seem happen see consider attempt superresolution figure 24a portrays signal twinsine consisting sinusoids two closely spaced frequencies mp applied case figure 24c using 4fold overcomplete discrete cosine dictionary initial frequency selected two frequencies making signal mistake mp forced make series alternating corrections suggest highly complex organized structure mp misses entirely doublet structure one certainly say case mp failed superresolve second one give examples dictionaries signals mp arbitrarily suboptimal terms sparsity somewhat artificial character different superresolution example devore temlyakovs example vladimir temlyakov talk ieee conference information theory statistics october 1994 described example straightforward greedy algorithm sparsitypreserving adaptation example based temlyakovs joint work ra devore 10 one constructs dictionary atoms first n dirac basis final atom involves linear combination first n decaying weights signal exact decomposition terms atoms greedy algorithm goes forever error size o1 steps illustrate decay figure 25a example set choose signal dictionary consists dirac elements c chosen normalize oe n1 unit norm shaobing chens example devoretemlyakov example applies original mp algorithm announced mallat zhang 1992 later refinement see also pati 29 involves extra step orthogonalization one takes terms entered stage solves least squares problem min ks coefficients one forms residual orthogonal terms currently model method called orthogonal matching pursuit omp pati 29 devoretemlyakov example apply omp shaobing chen found summer 1993 example similar flavor example special signal dictionary constructed following flavor dictionary composed atoms oe fl ng first atoms come dirac dictionary signal simple equiweighted linear combination first atoms dictionary atoms fl linear combination corresponding dirac ffi fl omp chooses atoms except first ever choosing one first result instead ideal behavior one might hope terminating steps one gets n steps convergence rate relatively slow illustrate behavior reconstruction error figure 25b chose 1024 dictionary oe oe one might hoped ideal behavior 233 best orthogonal basis certain dictionaries possible develop specific decomposition schemes customtailored dictionary wavelet packet cosine packet dictionaries examples special properties certain special subcollections elements dictionaries amount orthogonal bases one gets way wide range orthonormal bases fact orthogonal bases signals length n coifman wickerhauser 6 proposed method adaptively picking among many bases single orthogonal basis best basis sb denotes vector coefficients orthogonal basis b define entropy mp devore temlyakovs example number terms reconstruction reconstruction greedy dashed b omp chens example number terms reconstruction reconstruction greedy dashed figure 25 counter examples mp esb es scalar function scalar argument give fast algorithm solving algorithm cases delivers nearoptimal sparsity representations par ticular object question sparse representation orthogonal basis taken library one expects bob work well however signal composed moderate number highly nonorthogonal components method may deliver sparse representations demand bob find orthogonal basis prevents finding highly sparse representation example comes signal wernersorrows superposition several chirps sinusoids diracs see figure 26a analyzed cosine packet dictionary original coifman entropy bob finds nothing chooses global sinusoid basis best lack timevarying structure basis means chirp transient structure signal missed entirely see figure 26b 3 basis pursuit discuss approach problem overcomplete representations assume dictionary overcomplete general many representations principle basis pursuit find representation signal whose coefficients minimal 1 norm formally one solves problem subject 31 one point view 31 similar method frames 23 simply replacing 2 norm 23 1 norm however apparently slight change major consequences method frames leads quadratic optimization problem 226 signal werner sorrows b phase plane bob cw entropy time frequency c phase plane bob l1 entropy time frequency phase plane bp time frequency figure 26 analyzing signal wernersorrows cosine packet dictionary linear equality constraints involves essentially solution system linear equations contrast basis pursuit requires solution convex nonquadratic optimization problem involves considerably effort sophistication 31 linear programming explain last comment name basis pursuit develop connection linear programming lp linear program socalled standard form 7 16 constrained optimization problem defined terms variable x subject c x objective function collection equality constraints set bounds main question variables zero basis pursuit problem 31 equivalently reformulated linear program standard form 32 making following translations hence solution 31 obtained solving equivalent linear program equivalence minimum 1 optimizations linear programming known since 1950s see 2 connection basis pursuit linear programming useful several ways 311 solutions bases linear programming problem 32 suppose n matrix n suppose optimal solution exists well know solution exists n entries optimal x nonzero moreover generic case solution socalled nondegenerate exactly n nonzeros nonzero coefficients associated n columns columns make basis r n basis identified solution uniquely dictated basis thus finding solution lp identical finding optimal basis sense linear programming truly process basis pursuit translating lp results bp terminology decomposition waveforms oe linearly independent necessarily orthogonal collection general known advance instead depends problem data case selection waveforms therefore signaladaptive 312 algorithms bp optimization principle algorithm last forty years tremendous amount work done solution linear programs 1980s work focused variants dantzigs simplex algorithm many readers doubt studied last ten years spectacular breakthroughs made use socalled interiorpoint methods use entirely different principle point view free consider algorithm lp literature candidate solving bp optimization problem simplex interiorpoint algorithms offer interesting insights bp useful consider bp context particular algorithm indicate label either bpsimplex bpinterior bpsimplex standard implementations simplex method lp one first finds initial basis b consisting n linearly independent columns corresponding solution b gamma1 b feasible nonnegative one iteratively improves current basis step swapping one term basis one term basis using swap best improves objective function always exists swap improves maintains objective value except optimal solution moreover lp researchers shown one select terms swap way guarantee convergence optimal solution anticycling rules 16 hence simplex algorithm explicitly process basis pursuit iterative improvement basis improvement possible point solution achieved translating lp algorithm bp terminology one starts linearly independent collection n atoms dictionary one calls current decomposition one iteratively improves current decomposition swapping atoms current decomposition new atoms goal improving objective function application anticycling rules way select swaps guarantees convergence optimal solution assuming exact arithmetic bpinterior collection feasible points fx convex polyhedron r simplex simplex method viewed geometrically works walking around boundary simplex jumping one vertex extreme point polyhedron adjacent vertex objective better interiorpoint methods instead start point x 0 well inside interior simplex x 0 ae 0 go interior simplex since solution lp always extreme signal carbon b phase plane mof time frequency phase plane bob time frequency phase plane mp time frequency phase plane bp time frequency figure 31 analyzing signal carbon wavelet packet dictionary point simplex interiorpoint method converges current iterate x k approaches boundary one may abandon basic interiorpoint iteration invoke crossover procedure uses simplex iterations find optimizing extreme point translating lp algorithm bp terminology one starts solution overcomplete representation problem phia iteratively modifies coefficients maintaining feasibility phia applying transformation effectively sparsifies vector k iteration vector n significantly nonzero entries becomes clear correspond atoms appearing final solution one forces coefficients zero jumps decomposition terms n selected atoms general interiorpoint algorithms start 0 0 dont require feasibility phia 32 examples give computational examples bp action 321 carbon synthetic signal carbon composite 6 atoms dirac sinusoid 4 mutually orthogonal wavelet packet atoms adjacent timefrequency plane wavelet packet dictionary depth employed based filters symmlets 8 vanishing moments information problem sizes examples given table 1 figure 31 displays results phaseplane form comparison include phase planes obtained using mof mp bob first note mof uses basis functions orthogonal 6 atoms ie atoms times frequencies overlap atom appearing signal corresponding phase plane diffuse smeared second mp able relatively good job sinusoid dirac makes mistakes handling 4 close atoms third bob cannot handle nonorthogonality dirac cosine gives distortion coarsening signal fm time frequency f phaseplane bp time frequency time frequency time frequency time frequency figure 32 analyzing signal fmcosine cosine packet dictionary underlying phase plane picture finally bp finds exact decomposition sense four atoms quad dirac sinusoid correctly identified 322 twinsine recall signal twinsine figure 24a consists 2 cosines frequencies closer together rayleigh distance figure 24d analyze 4fold overcomplete discrete cosine dictionary recall example mp began choosing first step frequency two ideal ones never corrected error contrast bp resolves two frequencies correctly 323 fm signal figure 32a displays artificial signal fmcosine consisting frequencymodulated sinusoid superposed pure sinusoid figure 32b shows ideal phase plane figure 32cf analyze using cosine packet dictionary based primitive bell width 16 samples evident bob cannot resolve nonorthogonality sinusoid fm signal neither mp however bp yields clean representation two structures 324 gong figure 33a displays gong signal vanishes time 0 follows decaying sinusoid 0 figures 33b33d analyze cosine packet dictionary based primitive bell width samples bp gives finest representation decay structure visually somewhat interpretable bob mp results 0a signal gong c phase plane mof time frequency time frequency time frequency time frequency figure 33 analyzing signal gong cosine packet dictionary 33 comparisons briefly compare bp three main methods introduced section 23 331 matching pursuit first glance mp bp seem quite different mp iterative algorithm explicitly seek overall goal merely applies simple rule repeatedly contrast bp principle global optimization without specified algorithm contrast orthogonal mp specific algorithm bpsimplex may instructive orthogonal matching pursuit starts empty model builds signal model atom time step adding model important new atom among far model contrast bpsimplex starts full model ie representation object basis iteratively improves full model taking relatively useless terms model swapping useful new ones hence mp sort buildup approach bpsimplex sort swapdown approach make bp bob comparable suppose working cosine packet dictionary note 1 norm coefficients coifman 6 call additive measure information suppose apply two methods compare follows bob optimizing e orthogonal bases taken dictionary bp optimizing e bases formed dictionary last remark suggests might interesting apply bob procedure 1 norm entropy place standard coifmanwickerhauser entropy figure 26c try wernersorrows example section 233 signal analyzed cosine packet dictionary primitive bell width 16 1 entropy results timevarying basis reveals clearly underlying signal structure 1 entropy phase plane bp iteration time frequency time frequency time frequency time frequency time frequency time frequency figure 34 phase plane evolution bpinterior iteration improves performance bob bp better still figure 26d connection bp bob suggests interesting algorithmic idea standard implementation simplex method lp one starts initial basis iteratively improves basis swapping one term basis one term basis using swap best improves objective function initial basis seems natural bpsimplex use coifmanwickerhauser algorithm employ start best orthogonal basis choice starting basis bp seen method refining bob swapping nonorthogonal atoms place orthogonal ones whenever improve objective 333 method frames already discussed mof bp differ replacement l 2 objective function l 1 objective bpinterior interesting relation method frames bp interior initializes method frames solution hence one say bp sequentially improves method frames figure 34 shows movie bpinterior action fmcosine example using cosine packet dictionary six stages evolution phase plane shown one see phase plane improves clarity stepbystep variations recent development timefrequency dictionaries motivates done far however methods developed general applied dictionaries interesting results 41 stationary smooth wavelets usual orthonormal dictionaries periodized smooth wavelets consist wavelets scales indexed jth scale 2 j wavelets width wavelets scale circulant shifts shift n2 j samples authors 32 suggested scheme less satisfactory essentially shift adjacent wavelets large would say important features signal fortuitously aligned wavelets dictionary dictionary provide sparse representation signal however wavelets level j likely wavelets dictionary precisely aligned features interest dictionary may therefore provide diffuse representation stationary wavelet dictionary jth level n circulant shifts basic wavelet width n2 j since dictionary always contains wavelets aligned given feature hope dictionary provides superior representation panel 41a shows signal heavisine 41b shows result bp stationary symmlet8 dictionary mentioned section 21 coefficients displayed multiresolution fashion level j coefficients scale 2 j n plotted according spatial position surprisingly close agreement bp representation stationary wavelet dictionary ideas signal representation associated multiscale edges ideas mallat hwang 22 multiscale edge method analyzes continuous wavelet transform cwt scale 2 gammaj identifies maxima transform selects maxima important thresholding based amplitude im portant maxima identify important features signal mallat hwang proposed iterative method reconstructs object values cwt maxima almost quite thing saying one identifying important wavelets located corresponding maxima reconstructing object using maxima panel 41c shows cwt heavisine based symmlet8 wavelet multiresolution fashion panel 41d shows maxima cwt fine scales virtually 11 relationship maxima transform wavelets selected bp compare panel 41b stationary wavelet dictionary global optimization principle bp yields results close certain heuristic methods important contrast meyer counterexample multiscale edge approaches showing mallathwang approach may fail certain cases 26 counterexamples bp 42 dictionary mergers important methodological tool ability combine dictionaries make bigger expressive dictionaries mention two possibilities examples decompositions given section 5 jumpsine merge heaviside dictionary fourier dictionary either dictionary efficiently represent objects cannot example heavisides difficulty representing sinusoids sinusoids difficulty representing jumps combination might therefore able offer advantages signal heavisine 22 position logresolution b coefs bp heavisine 22 position logresolution c coefs cwt heavisine 22 position logresolution mutiscale edges representation heavisine figure 41 analyzing signal heavisine stationary wavelet dictionary jumpwavelet similar reasons one might want merge heavisides wavelets fact found sometimes preferable instead merge tapered heavisides wavelets step discontinuities start 0 jump time 0 level one unit higher later decay original 0 level denoising adapt bp case noisy data assume data form z standard white gaussian noise oe 0 noise level clean signal setting unknown known dont want get exact decomposition dont apply bp directly instead decompositions like 12 become relevant 51 proposal basis pursuit denoising bpdn refers solution min solution function parameter yields decomposition signalplusresidual size residual controlled 0 residual goes zero solution behaves exactly like bp applied 1 residual gets large r 0 recently michael saunders shaobing chen shown 51 equivalent following perturbed linear program subject ax perturbed linear programming really quadratic pro gramming retains structure similar linear programming hence similar classification algorithms bpdnsimplex bpdninteriorpoint types quadratic programming simplex like algorithms usually called active set algorithms label admittedly nonstandard 52 choice assuming dictionary normalized koe fl set value p cardinality dictionary motivated follows case dictionary orthonormal basis number papers 11 14 carefully studied approach denoising socalled softthresholding orthonormal basis detail suppose phi orthogonal matrix define empirical oecoefficients define soft threshold nonlinearity j define thresholded empirical coefficients soft thresholding empirical orthogonal coefficients papers cited show thresholding n number optimal nearoptimal properties regards meansquared error claim case orthobasis thresholding estimate ff also solution 51 observe soft thresholding nonlinearity solves scalar minimum problem note orthogonality phi rewrite 51 case min applying 52 coordinatewise establishes claim scheme suggested applied overcomplete well orthogonal settings therefore includes softthresholding orthobases special case formal arguments similar 13 used give proof meansquared error properties resulting procedure nearoptimal certain conditions recovered bob f recovered bp c recovered mof signal gong b noised figure 51 denoising noisy gong cosine packet dictionary 53 examples present two examples bpdn action timefrequency dictionaries compare bpdn three denoising methods adapted mof mp bob method offrames denoising mofdn refers minimizing least square fit error plus l 2 penalizing term min ks 2where penalizing parameter chose examples oe logp matching pursuit denoising mpdn runs matching pursuit coefficient associated selected atom gets threshold oe best orthogonal basis denoising bobdn thresholding scheme best orthogonal basis chosen bob algorithm special entropy 12 531 gong figure 51 displays denoising results signal gong signal noise ratio 1 using cosine packet dictionary panel displays noiseless signal panel b displays noisy version panels cf display denoising results mof bob mp bp respectively bp outperforms methods visually 532 twinsine figure 52 employs signal twinsine described earlier investigate superresolution noisy case panels b give noiseless noisy twinsine respectively using 4fold overcomplete discrete cosine dictionary reconstructions mof mp bpdn given mof gives reconstruction inherently resolutionlimited oscillatory noiseless case mp gives reconstruction goes wrong selects average two frequencies twinsine signal bp correctly resolves nonnegative doublet structure twinsine b noised twinsine c dct transform frequencynyquist mof coefs frequencynyquist frequencynyquist f bp coefs frequencynyquist figure 52 denoising noisy twinsine2 4fold overcomplete discrete cosine dictionary 54 total variation denoising recently rudin osher fatemi 28 called attention possibility denoising images using totalvariation penalized leastsquares specifically propose optimization problem min g2 tv g discrete measure total variation g solution problem denoised object li santosa 20 developed alternative algorithm problem based interiorpoint methods convex optimization 1dimensional case signals rather images possible implement amounts total variation denoising applying bpdn heaviside dictionary indeed arbitrary object unique decomposition heavisides recall 21 suppose object 0 decomposition total variation given moreover get approximate equality even objects obeying zeroboundary condi tions one normalize oe 0 appropriately consequently total variation denoising essentially special instance proposal 51 studied bpdn heaviside dictionary thereby obtaining essentially series tests tv denoising comparison considered also soft thresholding orthogonal wavelet dictionaries based s8symmlet smooth wavelet also constructed new dictionary based jumpwave merger s8symmlet wavelets smoothly tapered heavisides say atoms oe fl jump given point fl decay smoothly away discontinuity comparability heaviside dictionary normalized jumpwave dictionary every koe fl k tv 1 signal blocks bpdenoise heaviside f bpdenoise jumpwave c sorted coefs order amplitude dotted heaviside wave jumpwave bpfig54m 16may95 figure 54 tv denoise signal figure 53 denoising noisy blocks typical result object blocky presented figure 53 point view visual appearance total variation reconstruction panel far outperforms methods course object blocky sparse representation terms heavisides consider object like cusp piecewise smooth rather piecewise constant object longer sparse representation hand using jumpwave dictionary based merger wavelets tapered heavisides lead sparse representation see figure 54c one predict heaviside dictionary perform less well merged dictionary completely obvious comment translated statement total variation de noising becomes surprising prediction one expects lack sparse representation smooth objects heaviside dictionary translate worse performance tv denoising bpdn merged jumpwave dictionary test conducted experiments figure 54 compares denoising bpdn merged jumpwave dictionary tv denoising exhibits visually distracting stairstep artifacts dictionary jumpwave seems us behave much better 6 solutions largescale linear programs indicated section 31 optimization problem 31 equivalent linear program 32 also section 51 optimization problem 51 equivalent perturbed linear program 53 problems question largescale conducted decompositions signals length wavelet packet dictionary leading linear program size 8192 212 992 last ten years rapid expansion size linear programs successfully solved using digital computers good overview recent rapid progress field current state art afforded article lustig signal cusp c sorted coefs order amplitude dotted heaviside wave jumpwave bpfig56m 16may95 figure 56 dictionary merge signal figure 54 denoising noisy cusp marsten shanno 21 accompanying discussions bixby 1 saunders 31 todd 33 vanderbei 34 much rapid expansion size linear programs solved due interior point revolution initiated karmarkars proof pseudopolynomial time algorithm could based interiorpoint method 18 since wide array interiorpoint algorithms proposed considerable practical 21 theoretical 27 understanding available section describe algorithm experience 61 duality theory consider linear program standard form subject often called primal linear program primal linear program equivalent dual linear program subject x called primal variable z called dual variables term primal refers quantity kb gamma axk 2 term dual infeasibility refers kc gamma term duality gap refers difference primal objective dual objective c fundamental theorem linear programming states x solves linear program 61 primal infeasibility dual infeasibility duality gap zero therefore x nearly primal feasible nearly dual feasible duality gap offers good description accuracy x solution smaller duality gap closer x optimal solution 62 primaldual logbarrier lp algorithm mathematical work interiorpoint methods last ten years led large variety approaches names like projective scaling primaldual affine scaling pri maldual logarithmic barrier predictorcorrector cannot summarize ideas many mentioned 21 others covered references article approach based primaldual logbarrier algorithm order regularize standard lp gill et al 15 proposed solving following perturbed lp subject ax fl ffi normally small eg regularization parameters comment perturbed lp solves bpdn problem 51 main steps interior point algorithm follows 1 set parameters feasibility tolerance featol duality gap tolerance pdgaptol two regularization parameters fl ffi 2 initialize x 0 3 loop set x z diagonal matrices composed x z b solve 4y set c calculate primal dual step sizes ae p ae update variables increase k 1 4 following three conditions satisfied primal infeasibility featol b dual featol c duality fuller discussions related algorithms see 15 references principle could based approach interiorpoint schemes primaldual approach naturally incorporates several features found useful first iterates z feasible able choose starting point nearly feasible remain nearly feasible throughout sequence iterations second primal dual feasibility nearly achieved easy check closeness solution value limiting solution c x duality gap quantifies distance ideal 63 implementation heuristics primaldual log barrier algorithm described works fashion similar interiorpoint methods 21 starts initial feasible nearly feasible solution located near center feasible region iteratively improves current solution iterates x achieve desired accuracy requires relatively small number iterations example dozen iterations would common iteration requires solution system equations involving problem data like x z primaldual log barrier method system 64 thus numerical solution linear program interiorpoint methods amounts sequence several dozen solutions special systems linear equations leads slogan systems solved rapidly possible solve lp rapidly course general solving systems equations rapid general n n system takes order 3 time solve standard elimination methods modern stable factorization schemes 17 16 order practical algorithms based interiorpoint heuristic necessary able solve systems equations much rapidly one could solve general systems current state art linear programming 31 one attempts exploiting sparsity underlying matrix however optimization problems interested key difference successful largescale applications outlined 21 matrix deal sparse generally completely dense example generated fourier dictionary elements order magnitude density unlikely existing largescale interiorpoint computer codes could easily applied problems described paper application substitute sparsity consider dictionaries fast implicit algorithms phia phi therefore lead linear programs matrix admits fast implicit algorithms au v whenever one fast implicit algorithms natural think solving equations conjugategradient methods methods allow one solve equations using products bv various strategically chosen vectors v adapting ideas one develops fast implicit algorithms ada attempts solve central equations 64 iteratively avoiding costly step explicitly forming matrices ada application really need exact solution optimization problem moreover natural initial solution mof would viewed researchers already acceptable method atomic decomposition starting decomposition applying strategy based limited number iterations algorithm get view iterative improvement mof compare figure 34 cpu running time seconds figure signal problem size mof bob mp bp figure 24 twinsine 256 3500 6667 7517 figure 26 wernersorrows 1024 9500 1582 figure 31 carbon 1024 2000 2617 2650 1170 figure 32 fmcosine 1024 1050 9333 1829 1502 figure 33 gong 1024 1433 5683 5063 4482 figure 41 heavisine 256 2692 figure 51 noisy gong 1024 2117 6767 8600 1422 figure 52 noisy twinsine 256 4167 6833 5717 table 1 cpu running times examples stress strategy pursue optimal basis would like reach optimal basis make specific claims always reach reasonable time perhaps pursuit language help remind one fact believe pursuit process carried whatever length time willing invest makes useful improvement method frames 64 routine settings bp strategy routine signal processing bp follows ffl employ primaldual logarithmic barrier method perturbed lp 15 ffl suppose fast implicit algorithms aa ffl aim reach approximate optimum would usually suffice ffl barrier iteration involves approximate solution central equations 64 using conjugategradient method e g accuracy refer reader 4 detailed discussion implementation 65 complexity analysis table 1 displays cpu times seconds spent running various atomic decomposition techniques experiments computation done sun sparc20 workstation employ conjugategradient solver generalized inverse mof solution 24 resulting algorithm mof complexity order logn implement coifman wickerhausers bob algorithm 6 also complexity order logn observe bp typically slower mof bob bp also slower mp quasilinear complexity depending number chosen atoms except fmcosine signal figure 32 several factors influence running time basis pursuit 1 problem sizes complexity goes quasilinearly problem size increases 4 2 parameter settings complexity primaldual logarithmic barrier interiorpoint implementation depends accuracy solution accuracy conjugategradient solver accuracy solution determined two parameters featol pdtol controlling number barrier iterations parameter cgaccuracy decides accuracy cg solver consequently number cg iterations required solution accuracy goes complexity goes drasti cally recommend setting featol pdgaptol cgaccuracy 10 gamma1 routine signal processing recommend 10 gamma2 10 gamma3 one interested superresolution used setting 10 gamma1 computational experiments presented figures 26 31 32 33 51 51 figures 25 32 51 attempted superresolve two cosines close frequencies thus use setting 10 gamma2 figure 41 used setting 10 gamma3 3 signal complexity signal sparse representation algorithm converges quickly signal carbon contains 6 atoms wavelet packet dictionary takes 10 seconds whereas takes 7 minutes signal gong much complex 4 basis pursuit versus basis pursuit denoising employ interiorpoint implementation bp bpdn except difference value regularization parameter ffi ffi small eg 10 gamma4 bp bpdn choice regularizes central equations solved barrier iteration thus bpdn implementation seems converge quickly bp implementation example according experiments 4 takes 3 minutes perform bpdn noisy gong signal length 1024 cosine packet dictionary parameter setting takes 8 hours perform bp signal gong parameter setting 66 reproducible research paper written following discipline reproducible research described 3 complement article releasing underlying software environment placing internet access either anonymous ftp www browsers web browser httpplayfairstanfordeduschenatomizerhtml ftp client playfairstanfordedu file pubchensatomizer0600tarz reasons space refer reader 4 discussion related work statistics elsewhere r progress linear programming least absolute deviations theory wavelab reproducible research remarques sur lanalyze de fourier linear programming extensions ten lectures wavelets remarks greedy algorithms denoising orthonormal basis chosen library bases empirical atomic decomposition wavelet shrinkage asymptopia solving reduced kkt systems barrier methods linear quadratic programming numerical linear algebra optimization matrix computations new polynomialtime algorithm linear programming primaldual interior point algorithm linear programming affine scaling algorithm minimizing total variation image enhancement interior point methods linear program ming computational state art detection processing wavelets matching pursuit timefrequency dictionary finding primal dual optimal bases ondelettes sur lintervalle wavelets algorithms applications nonlinear totalvariationbased noise removal algorithms orthogonal matching pursuit recursive function approximation applications wavelet decomposition signal representation using adaptive normalized gaussian functions major cholesky would feel proud shiftable multiscale transforms theory practice interior point methods interior point methods algorithms formulations tr ctr anders la courharbo fast estimation optimal sparseness music signals proceedings 24th iasted international conference signal processing pattern recognition applications p205209 february 1517 2006 innsbruck austria e davies l daudet sparse audio representations using mclt signal processing v86 n3 p457470 march 2006 yuanqing li andrzej cichocki shunichi amari analysis sparse representation blind source separation neural computation v16 n6 p11931234 june 2004 zoltn szab andrs lrincz sparse representations generalized sparse approximation equivalent family svm tasks acta cybernetica v17 n3 p605614 january 2006 tong zhang approximation bounds sparse kernel regression algorithms neural computation v14 n12 p30133042 december 2002 prasanth b nair arindam choudhury andy j keane greedy learning algorithms sparse regression classification mercer kernels journal machine learning research 3 312003 shachar fleishman iddo drori daniel cohenor bilateral mesh denoising acm transactions graphics tog v22 n3 july peng xu dezhong yao two dictionaries matching pursuit sparse decomposition signals signal processing v86 n11 p34723480 november 2006 joel tropp algorithms simultaneous sparse approximation part ii convex relaxation signal processing v86 n3 p589602 march 2006 yaakov tsaig david l donoho breakdown equivalence minimal l1norm solution sparsest solution signal processing v86 n3 p533548 march 2006 chiranjib bhattacharyya second order cone programming formulations feature selection journal machine learning research 5 p14171433 1212004 gradient lasso feature selection proceedings twentyfirst international conference machine learning p60 july 0408 2004 banff alberta canada roger koenker quantile regression longitudinal data journal multivariate analysis v91 n1 p7489 october 2004 balaji krishnapuram lawrence carin mario figueiredo alexander j hartemink sparse multinomial logistic regression fast algorithms generalization bounds ieee transactions pattern analysis machine intelligence v27 n6 p957968 june 2005 bob l sturm laurent daudet curtis roads pitchshifting audio signals using sparse atomic approximations proceedings 1st acm workshop audio music computing multimedia october 2727 2006 santa barbara california usa pando georgiev panos pardalos fabian theis bilinear algorithm sparse representations computational optimization applications v38 n2 p249259 november 2007 f malgouyres image compression projection onto polyhedral set journal mathematical imaging vision v27 n2 p193200 february 2007 tong zhang dual formulation regularized linear systems convex risks machine learning v46 n13 p91129 2002 sayan mukherjee qiang wu estimation gradients coordinate covariation classification journal machine learning research 7 p24812514 1212006 yutaka ohtake alexander belyaev hanspeter seidel sparse surface reconstruction adaptive partition unity radial basis functions graphical models v68 n1 p1524 january 2006 shihao ji lawrence carin bayesian compressive sensing projection optimization proceedings 24th international conference machine learning p377384 june 2024 2007 corvalis oregon anna c gilbert muthukrishnan martin j strauss approximation functions redundant dictionaries using coherence proceedings fourteenth annual acmsiam symposium discrete algorithms january 1214 2003 baltimore maryland lorenzo peotta lorenzo granai pierre vandergheynst image compression using edge adapted redundant dictionary wavelets signal processing v86 n3 p444456 march 2006 chengen guo songchun zhu ying nian wu modeling visual patterns integrating descriptive generative methods international journal computer vision v53 n1 p529 june arthur e c pece computational rationale generative models computer vision image understanding v106 n1 p130143 april 2007 sayan mukherjee dingxuan zhou learning coordinate covariances via gradients journal machine learning research 7 p519549 1212006 yaakov tsaig david l donoho extensions compressed sensing signal processing v86 n3 p549571 march 2006 fabian j theis gonzalo garca use sparse signal decomposition analysis multichannel surface electromyograms signal processing v86 n3 p603623 march 2006 r gribonval r figueras ventura p vandergheynst simple test check optimality sparse signal approximation signal processing v86 n3 p496510 march 2006 alexander j smola bernhard schlkopf bayesian kernel methods advanced lectures machine learning springerverlag new york inc new york ny fabian j theis pando georgiev andrzej cichocki robust sparse component analysis based generalized hough transform eurasip journal applied signal processing v2007 n1 p8686 1 january 2007 charles micchelli massimiliano pontil feature space perspectives learning kernel machine learning v66 n23 p297319 march 2007 sylvain fischer rafael redondo laurent perrinet gabriel cristbal sparse approximation images inspired functional architecture primary visual areas eurasip journal applied signal processing v2007 n1 p122122 1 january 2007 l daudet b torrsani hybrid representations audiophonic signal encoding signal processing v82 n11 p15951617 november 2002 f malgouyres rank related properties basis pursuit total variation regularization signal processing v87 n11 p26952707 november 2007 alexander bronstein michael bronstein michael zibulevsky blind source separation using blockcoordinate relative newton method signal processing v84 n8 p14471459 august 2004 yee whye teh max welling simon osindero geoffrey e hinton energybased models sparse overcomplete representations journal machine learning research 4 1212003 yee whye teh max welling simon osindero geoffrey e hinton energybased models sparse overcomplete representations journal machine learning research v4 n78 p12351260 october 1 november 15 2004 cynthia dwork frank mcsherry kunal talwar price privacy limits lp decoding proceedings thirtyninth annual acm symposium theory computing june 1113 2007 san diego california usa masashi sugiyama klausrobert mller subspace information criterion infinite dimensional hypothesis spaces journal machine learning research 3 p323359 312003 mark plumbley samer abdallah thomas blumensath michael e davies sparse representations polyphonic music signal processing v86 n3 p417431 march 2006 pavel kisilev michael zibulevsky yehoshua zeevi multiscale framework blind separation linearly mixed signals journal machine learning research 4 1212003 alexander j smola sebastian mika bernhard schlkopf robert c williamson regularized principal manifolds journal machine learning research 1 p179209 912001 pavel kisilev michael zibulevsky yehoshua zeevi multiscale framework blind separation linearly mixed signals journal machine learning research v4 n78 p13391364 october 1 november 15 2004 gianluca monaci scar divorra escoda pierre vandergheynst analysis multimodal sequences using geometric video representations signal processing v86 n12 p35343548 december 2006 model michael zibulevsky signal reconstruction sensor arrays using sparse representations signal processing v86 n3 p624638 march 2006 mrio figueiredo adaptive sparseness supervised learning ieee transactions pattern analysis machine intelligence v25 n9 p11501159 september joseph f murray kenneth kreutzdelgado learning sparse overcomplete codes images journal vlsi signal processing systems v46 n1 p113 january 2007 joseph f murray kenneth kreutzdelgado learning sparse overcomplete codes images journal vlsi signal processing systems v45 n12 p97110 november 2006 alex j smola bernhard schlkopf tutorial support vector regression statistics computing v14 n3 p199222 august 2004 zhe chen simon haykin different facets regularization theory neural computation v14 n12 p27912846 december 2002 ivana radulovic pascal frossard multiple description coding redundant expansions application image communications journal image video processing v2007 n1 p88 january 2007 olga sorkine daniel cohenor dror irony sivan toledo geometryaware bases shape approximation ieee transactions visualization computer graphics v11 n2 p171180 march 2005 kjersti engan karl skretting john hkon husy family iterative lsbased dictionary learning algorithms ilsdla sparse signal representation digital signal processing v17 n1 p3249 january 2007 gunnar rtsch sebastian mika bernhard schlkopf klausrobert mller constructing boosting algorithms svms application oneclass classification ieee transactions pattern analysis machine intelligence v24 n9 p11841199 september 2002 hongxing zou dianjun wang xianda zhang yanda li nonnegative timefrequency distributions parametric timefrequency representations using semiaffine transformation group signal processing v85 n9 p18131826 september 2005