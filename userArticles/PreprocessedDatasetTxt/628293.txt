statistical method estimating usefulness text databases abstractsearching desired data internet one common ways internet used single search engine capable searching data internet approach provides interface invoking multiple search engines user query potential satisfy users number search engines interface large invoking search engines query often cost effective creates unnecessary network traffic sending query large number useless search engines searching useless search engines wastes local resources problem overcome usefulness every search engine respect query predicted paper present statistical method estimate usefulness search engine given query given query usefulness search engine paper defined combination number documents search engine sufficiently similar query average similarity documents experimental results indicate estimation method much accurate existing methods b introduction internet become vast information source recent years help ordinary users find desired data internet many search engines created search engine corresponding database defines set documents searched search engine usually index documents database created stored search engine term represents content word combination several usually adjacent content words index identify documents contain term quickly preexistence index critical search engine answer user queries efficiently two types search engines exist generalpurpose search engines attempt provide searching capabilities documents internet web webcrawler hotbot lycos alta vista wellknown search engines specialpurpose search engines hand focus documents confined domains documents organization specific interest tens thousands specialpurpose search engines currently running internet amount data internet huge believed end 1997 300 million web pages 15 increasing high rate many believe employing single generalpurpose search engine data internet unrealistic first processing power storage capability may scale fast increasing virtually unlimited amount data second gathering data internet keeping reasonably uptodate extremely difficult impossible programs ie robots used search engines gather data automatically may slow local servers increasingly unpopular practical approach providing search services entire internet following multilevel approach bottom level local search engines search engines grouped say based relatedness databases form next level search engines called metasearch engines lower level metasearch engines grouped form higher level metasearch engines process repeated one metasearch engine top metasearch engine essentially interface maintain index documents however sophisticated metasearch engine may maintain information contents metasearch engines lower level provide better service metasearch engine receives user query first passes query appropriate metasearch engines next level recursively real search engines encountered collects metasearch engine search search engine n search query q resulr r figure 1 twolevel search engine organization sometimes reorganizes results real search engines possibly going metasearch engines lower levels twolevel search engine organization illustrated figure 1 advantages approach user queries eventually evaluated smaller databases parallel resulting reduced response time b updates indexes localized ie index local search engine updated documents database modified although local updates may need propagated upper level metadata represent contents local databases propagation done infrequently metadata typically statistical nature tolerate certain degree inaccuracy c local information gathered easily timely manner demand storage space processing power local search engine manageable words many problems associated employing single super search engine overcome greatly alleviated multilevel approach used number search engines invokable metasearch engine large serious inefficiency may arise typically given query small fraction search engines may contain useful documents query result every search engine blindly invoked user query substantial unnecessary network traffic created query sent useless search engines addition local resources wasted useless databases searched better approach first identify search engines likely provide useful results given query pass query search engines desired documents examples systems employ approach include wais 12 aliweb 13 ggloss 6 savvysearch 9 dwise 27 challenging problem approach identify potentially useful search engines current solution problem rank underlying databases decreasing order usefulness query using metadata describe contents database often ranking based measure ordinary users may able utilize fit needs given query current approach tell user degree accuracy search engine likely useful second useful etc ranking helpful cannot tell user useful particular search engine paper usefulness search engine given query measured pair numbers nodoc avgsim nodoc number documents database search engine high potentials useful query similarities query documents measured certain global similarity function higher specified threshold avgsim average similarity potentially useful documents note global similarity function may may local similarity function employed local search engine threshold provides minimum similarity document considered potentially useful avgsim describes precisely expected quality potentially useful document database two numbers together characterize usefulness search engine nicely nodoc avgsim defined precisely follows d2dsimqdt simq 2 threshold database search engine simq similarity closeness query q document query simply set words submitted user transformed vector terms weights 22 term essentially content word dimension vector number distinct terms term appears query component query vector corresponding term term weight positive absent corresponding term weight zero weight term usually depends number occurrences term query relative total number occurrences terms query 22 26 may also depend number documents term relative total number documents database document similarly transformed vector weights similarity query document measured dot product respective vectors often dot product divided product norms two vectors norm vector normalize similarities values 0 1 similarity function normalization known cosine function 22 26 similarity functions see example 21 also possible practice users may know relate threshold number documents like retrieve therefore users likely tell metasearch engine directly number similar documents query like retrieve number translated threshold metasearch engine example suppose three databases d1 d2 d3 user query q 2 case user wants 5 documents used result 3 documents retrieved d1 2 documents d3 general appropriate threshold determined estimating nodoc search engine decreasing thresholds note knowing useful search engine important user determine search engines use many documents retrieve selected search engine example user knows highlyranked search engine large database useful documents searching large database costly user may choose use search engine even user decides use search engine cost search still reduced limiting number documents returned number useful documents search engine informed decision possible ranking information provided paper several contributions first new measure proposed characterize usefulness database search engine respect query new measure easy understand informative result likely useful practice second new statistical method subrange based estimation method proposed identify search engines use given query estimate usefulness search engine query show nodoc avgsim obtained process therefore little additional effort required compute comparison obtaining one method yields accurate estimates substantially better existing methods demonstrated experimental results also guarantees following property let largest similarity document query among documents search engine large sim suppose large sim large sim j two search engines j threshold retrieval set large sim large sim j based method search engine invoked search engine j query single term query consistent ideal situation documents examined descending order similarity since large portion internet queries single term queries 10 11 property approach means large percentage internet queries sent correct search engines processed using method addition new method quite robust still yield good result even approximate statistical data used method method improved adjacent terms query combined close optimal performance obtained rest paper organized follows section 2 reviews related work section 3 presents basic method estimating usefulness search engines section 4 discusses several issues proposed method applied practice experimental results presented section 5 section 6 describes adjacent query terms combined yield higher performance section 7 concludes paper related work able identify useful search engines query characteristic information database search engine must stored metasearch engine call information representative search engine different methods identifying useful search engines developed based representatives used several metasearch engines employed various methods identify potentially useful search engines 6 9 12 13 17 27 however database representatives used metasearch engines cannot used estimate number globally similar documents search engine 3 12 13 27 addition measures used metasearch engines rank search engines difficult understand result separate methods used convert measures number documents retrieve search engine another shortcoming measures independent similarity threshold number documents desired user result search engine always ranked regardless many documents desired databases search engines fixed conflict following situation given query search engine may contain many moderately similar documents zero highly similar documents case good measure rank search engine high large number moderately similar documents desired rank search engine low highly similar documents desired probabilistic model distributed information retrieval proposed 2 method suitable feedback environment ie documents previously retrieved identified either relevant irrelevant ggloss 6 database distinct terms represented pairs f number documents database contain ith term w sum weights ith term documents database usefulness search engine respect given query ggloss defined sum document similarities query greater threshold usefulness measure less informative measure example given sum similarities documents database cannot tell many documents involved hand measure derive measure used ggloss representative ggloss used estimate number useful documents database 7 consequently used estimate measure however estimation methods used ggloss different estimation methods employed 6 7 based two restrictive assumptions one highcorrelation assumption given database query term j appears least many documents query term k every document containing term k also contains term j disjoint assumption given database term j term k set documents containing term j set documents containing term disjoint due restrictiveness assumptions estimates produced two methods accurate note measure similarity sum used estimates produced two methods ggloss form lower upper bounds true similarity sum result two methods useful used together used separately unfortunately measure number useful documents estimates produced two methods ggloss longer form bounds true number useful documents 25 proposed method estimate number useful documents database binary independent case case document represented binary vector 0 1 ith position indicates absence presence ith term occurrences terms different documents assumed independent method later extended binary dependent case 16 dependencies among terms incorporated substantial amount information lost documents represented binary vectors result seldom used practice estimation method 18 permits term weights nonbinary however utilizes nonbinary information way different subrangebased statistical method described section 32 paper 3 new method usefulness estimation present basic method estimating usefulness search engine section 31 basic method allows values term weights nonnegative real numbers two assumptions used basic method 1 distributions occurrences terms documents independent words occurrences term documents effect occurrences nonoccurrences another term say term j documents 2 given database search engine documents term weight term two assumptions basic method accurately estimate usefulness search engine section 32 apply subrangebased statistical method remove second assumption first assumption also removed incorporating term dependencies covariances basic solution 18 problem incorporating term dependencies addressed section 6 see section 5 accurate usefulness estimates obtained even term independence assumption 31 basic method consider database search engine distinct terms document database represented vector weight significance ith term representing document 1 query similarly represented consider query weight query 1 term appear query corresponding weight zero query vector similarity q document defined dot product respective vectors namely simq um dm similarities often normalized 0 1 one common normalized similarity function cosine function 22 although forms normalization possible see example 21 basic method database represented pairs fp probability term appears document w average weights set documents containing given query um database representative used estimate usefulness without loss generality assume first r u nonzero therefore q becomes implies first r terms document need considered consider following generating function x dummy variable following proposition relates coefficients terms function probabilities documents certain similarities q proposition 1 let q defined terms independent weight term whenever present document w given database representative 1 r coefficient x function 3 probability document similarity q proof clearly must sum zero w u w u used different combinations w u may add without loss generality let us assume two combinations suppose probability q similarity document probability either exactly terms ft 1 exactly terms ft j1 j l g independence assumption terms probability exactly terms ft 1 first product v second product f1 2 rg g similarly probability exactly terms ft j1 j l g first product v second product f1 2 rg g therefore probability either exactly terms ft 1 exactly terms ft j1 j l g sum p q coefficient x function 3 example 1 let q query three terms weights equal 1 ie ease understanding weights terms query documents normalized suppose database five documents vector representations components corresponding query terms namely first document query term 1 corresponding weight 3 document vectors interpreted similarly five documents p 5 documents term 1 average weight term 1 documents 2 similarly p therefore corresponding generating function 06 consider coefficient x 2 function clearly sum p 1 former probability document exactly first query term corresponding similarity q w 1 2 latter probability document exactly last query term corresponding similarity w 3 2 therefore coefficient x 2 namely 0416 estimated probability document similarity 2 q generating function 3 expanded terms x combined obtain assume terms 5 listed descending order exponents ie proposition 1 probability document similarity b q words database contains n documents n expected number documents similarity b query q given similarity threshold let c largest integer satisfy b c nodoc measure query q based threshold namely number documents whose similarities query q greater estimated est note n b expected sum similarities documents whose similarities query b thus expected sum similarities documents whose similarities query greater therefore avgsim measure query q based threshold namely average similarity documents database whose similarities q greater estimated est since nodoc avgsim estimated expanded expression 5 estimating requires little additional effort comparison estimating one example 2 continue example 1 generating function 4 expanded 0048 formula 6 est nodoc3 q est avgsim3 q 0048 50192 interesting note actual nodoc nodoc3 q 1 since fourth document similarity similarity higher 3 q actual avgsim avgsim3 q 4 second third columns table 1 list true usefulness respect q different note table 1 nodoc avgsim values obtained estimated similarities strictly greater threshold value avgsim available case corresponding entry avgsim left blank remaining columns list estimated usefulness based different methods fourth fifth columns basic method sixth seventh columns estimation method based highcorrelation case eighth ninth columns estimation method disjoint case proposed 6 7 observed estimates produced new method approximate true values better given methods based highcorrelation disjoint assumptions true basic method highcorrelation disjoint table 1 estimated usefulness versus true usefulness furthermore distribution exact similarities q documents expressed following function analogous expression 8 probability document similarity 5 q zero probability document similarity 4 q 02 since document similarity 5 q exactly one document similarity 4 q terms interpreted similarly notice good match corresponding coefficients 8 9 32 subrangebased estimation nonuniform term weights one assumption used basic solution documents term weight term realistic subsection present subrangebased statistical method overcome problem consider term let w oe average standard deviation weights set documents containing respectively let p probability term appears document database based basic solution section 31 term specified query following polynomial included probability generating function see expression 3 u weight term user query expression essentially assumes term uniform weight w documents containing term reality term weights may nonuniform distribution among documents term let weights nonascending order magnitude w number documents term n total number documents database suppose partition weight range 4 subranges containing 25 term weights follows first subrange contains weights w 1 w second subrange contains weights w s1 w third subrange contains weights w t1 w v last subrange contains weights w v1 w k first subrange median 25 k2th weight term weights subrange wm1 similarly median weights second third fourth subranges median weights wm2 wm3 wm4 respectively k illustrated following figuret kthen distribution term weights may approximated following distribution term uniform weight wm1 first 25 k documents term another uniform weight wm2 next 25 k documents another uniform weight wm3 next 25 documents another uniform weight wm4 last 25 documents weight approximation query containing term polynomial 10 generating function replaced following polynomial probability term occurs document weight wmj 25 documents term assumed weight wmj j essentially polynomial 11 obtained polynomial 10 decomposing probability p document term 4 probabilities corresponding 4 subranges weight term first subrange instance assumed wm1 corresponding exponent x polynomial 11 similarity due term equals u wm1 taking consideration query term weight u since expensive find store wm1 wm2 wm3 wm4 approximated assuming weights term normally distributed mean w standard deviation oe constant looked table standard normal distribution noted constants independent individual terms therefore one set constants sufficient terms example 3 suppose average weight term presentation assume term weights normalized standard deviation weights term 13 table standard normal distribution c gamma115 note constants independent term thus suppose probability document database term 032 4 suppose weight term query 2 polynomial term generating function general necessary divide weights term 4 equal subranges example divide weights 5 subranges different sizes yielding polynomial form represents probability term weight ith subrange wmi median weight term ith subrange experiments report section 5 specific sixsubrange used special subrange highest subrange containing maximum normalized weight see section 5 normalized weight term document weight term document divided norm document maximum normalized weight database largest normalized weight among documents containing database probability highest subrange set 1 divided number documents database probability may underestimate however since different documents usually different norms therefore usually one document largest normalized weight estimated probability reasonable example 4 continuing example 3 term weights normalized facilitate ease reading suppose number documents database maximum weight term since probability term occurs documents 032 number documents use 5 subranges obtained splitting first subrange example 3 two subranges first new subrange covered maximum weight since assume one document largest term weight probability document largest term weight 001 among documents term percentage documents largest term weight 132 100 3125 document occupies first new subrange second new subrange contains term weights 75 percentile 100 gamma percentile probability associated second new subrange 007 median second new subrange 75 percentile looking standard normal distribution table c next 3 subranges identical last 3 subranges given example 3 c assume query term weight 2 polynomial term generating function note subrangebased method needs know standard deviation weights term result database terms represented triplets fp probability term appears document database w average weight term documents containing term oe standard deviation weights documents containing furthermore maximum normalized weight term used highest subrange database representative contain quadruplets fp maximum normalized weight term experimental results indicate maximum normalized weight critical parameter drastically improve estimation accuracy search engine usefulness following subsection elaborate maximum normalized weight critically important piece information correctly identifying useful search engines 321 singleterm query consider query q contains single term suppose similarity function widely used cosine function normalized query weight 1 term similarity document query q using cosine function w 0 dot product 1 normalized weight term document jdj norm w weight term document normalization consider database 1 contains documents term component database representative concerning term contain maximum normalized weight mw 1 mw 1 largest normalized weight term among documents database 1 discussion subsection highest subrange contains maximum normalized weight probability set 1 divided number documents database generating function query q database 1 number documents database different database 6 1 maximum normalized term weight mw term generating function query database obtained replacing mw 1 mw expression modified accordingly suppose mw 1 largest maximum normalized term weight term among databases mw 2 second largest mw 1 mw 2 suppose threshold retrieval set mw 1 estimated number documents similarities greater database 1 least p 1 j 6 1 2 estimated numbers documents similarities greater database 2 databases zero thus database 1 database identified estimation method documents similarities greater single term query identification correct documents normalized term weight mw 1 appear database 1 documents databases similarities less equal mw 2 general maximum normalized weights term databases arranged descending order number databases threshold set mw identified estimation method searched identification consistent ideal situation selected databases contain documents similarities greater databases desired documents similarities greater thus method guarantees correct identification useful search engines single term queries argument applies similarity functions 21 several recent studies indicate 30 higher percentages internet queries singleterm queries 10 11 thus large percentage internet queries method guarantees optimal identification maximum normalized weight term utilized 4 discussion applicability discuss several issues concerning applicability new method 41 scalability representative database used estimation method large size relative database estimation method poor scalability method difficult scale thousands text databases suppose term occupies four bytes suppose number probability average weight standard deviation maximum normalized weight also occupies 4 bytes consider database distinct terms subrangebased method probabilities average weights standard derivations maximum normalized weights stored database representative resulting total storage overhead 20 bytes following table shows several document collections percentage sizes database representatives based approach relative sizes original document collections collection name collection size distinct terms representative size percentage fr 33315 126258 1263 379 table sizes pages 2 kb statistics second third columns three document collections namely wsj wall street journal fr federal register doe department energy collected arpanist 8 table shows three databases sizes representatives range 385 740 sizes actual databases therefore approach fairly scalable also typically percentage space needed database representative relative database size decrease database grows new documents added large database number distinct terms either remains unchanged grows slowly comparison database representative used ggloss size database representative approach 67 larger due storing standard deviation maximum normalized weight term following methods used substantially reduce size database representative several ways reduce size database representative instead using 4 bytes number probability average weight standard deviation maximum normalized weight onebyte number used approximate follows consider probability first clearly probabilities interval 0 1 using one byte 256 different values represented based interval 0 1 partitioned 256 equallength intervals next average probabilities falling small interval computed finally map original probability average corresponding interval probability 015 example lies 39th interval 01484 01523 database representative probability represented number 38 using one byte suppose average probabilities 39th interval 01511 01511 used approximate probability 015 similar approximation also applied average weights maximum normalized weights standard deviations experimental results show see section 5 approximation negligible impact estimation accuracy database usefulness scheme used size representative database distinct terms drops 8 bytes 20 bytes result sizes database representatives databases 15 3 database sizes size reduction possible using 4 bits weight maximum normalized weight standard deviation experimental results show also section 5 good accuracy still obtained reduction 4 bits used weight maximum normalized weight standard deviation probability still uses one byte size representative database distinct terms drops 65 k bytes reducing percentages 123 24 mentioned larger databases database representatives likely occupy even lower percentages space 42 hierarchical organization representatives number search engines large representatives clustered form hierarchy representatives query first compared highest level representatives representatives whose ancestor representatives estimated large number similar documents examined result database representatives compared query similar idea also suggested others 6 suppose representatives v local databases 1 v higher level representative representatives hierarchy considered representative database combined 1 v union discuss obtain p assume databases 1 v pairwise disjoint let set terms v given term let p probability document contains w average weight documents contain mw maximum normalized weight documents standard deviation positive weights let pt wt mwt oet probability average weight maximum normalized weight standard deviation term new representative p respectively discuss obtain pt wt mwt oet simplify notation assume p term first three quantities namely pt wt mwt obtained easily number documents compute oet let k number documents containing note k denote weight jth document oe based definition standard deviation equation 12 derivations show quantity representative p computed quantities representatives next lower level 43 obtaining database representatives obtain accurate representative database used method need know following information 1 number documents database 2 document frequency term database ie number documents database contain term 3 weight term document database 1 2 needed compute probabilities 3 needed compute average weights maximum normalized weights standard deviations 1 2 usually obtained ease example query containing single term submitted search engine number hits returned document frequency term many proposed approaches ranking text databases also use document frequency information 3 6 27 recently starts proposal internet metasearching 5 suggests database source provide document frequency term discuss obtain information 3 internet environment may practical expect search engine provide weight term document search engine propose following techniques obtaining average term weights standard deviations maximum normalized term weights 1 use sampling techniques statistics estimate average weight standard deviation term query submitted search engine set documents returned result search term document term frequency ie number times appears computed starts proposal even suggests search engine provides term frequency weight information term returned document 5 result weight computed weights reasonably large number documents computed note one query may needed approximate average weight approximate standard deviation term obtained since returned documents query may contain many different terms estimation carried many terms time 2 obtain maximum normalized weight respect global similarity function used metasearch engine term directly follows submit single term query local search engine retrieves documents according local similarity function two cases considered case 1 global similarity function known similarity function search engine case search engine returns similarity retrieved document similarity returned first retrieved document maximum normalized weight term search engine return similarity explicitly first retrieved document downloaded compute similarity oneterm query similarity maximum normalized weight term case 2 global similarity function local similarity function different local similarity function unknown case first retrieved documents downloaded compute global similarities documents oneterm query largest similarity tentatively used maximum normalized weight may need adjusted another document search engine found higher weight term respect global similarity function although using sampling techniques introduce inaccuracy statistical data eg average weight standard deviation usefulness estimation method quite robust respect inaccuracy 4bit approximation value still produce reasonably accurate usefulness estimation furthermore recent study indicates using sampling queries capable generating decent statistical information terms 4 5 experimental results three databases d1 d2 d3 collection 6234 queries used experiment d1 containing 761 documents largest among 53 databases collected stanford university testing ggloss system 53 databases snapshots 53 newsgroups stanford cs department news host d2 containing 1466 documents obtained merging two largest databases among 53 databases d3 containing 1014 documents obtained merging 26 smallest databases among 53 databases result documents d3 diverse d2 documents d2 diverse d1 queries real queries submitted users sift netnews server 23 6 since user queries internet environment short 1 14 queries 6 terms used experiments approximately 30 6234 queries experiments singleterm queries documents queries noncontent words etc removed similarity function cosine function function guarantees similarity query document nonnegative term weights 0 1 result threshold larger 1 needed first present experimental results database representative represented set quadruplets w normalized weight probability standard deviation maximum normalized weight number original number ie approximation used results compared estimates generated method highcorrelation case previous method proposed 18 method 18 similar basic method described section 31 paper except utilizes standard deviation weights term documents dynamically adjust average weight probability query term according threshold used query please see 18 details experimental results method disjoint case 6 reported shown method highcorrelation case performs better disjoint case 18 present results database representative still represented set quadruplets original number approximated either onebyte number 4bit number investigate whether estimation method tolerate certain degree inaccuracy numbers used database representative experiments use six subranges subrange based method first subrange contains maximum normalized term weight subranges medians 98 percentile 931 percentile 70 percentile 375 percentile 125 percentile respectively note narrower subranges used weights large weights often important estimating database usefulness especially threshold large finally present results database representative represented set triplets w number original number words maximum normalized weight directly obtained estimated 999 percentile average weight standard deviation experimental results show importance maximum normalized weights estimation process medians using quadruplets original numbers consider database d1 query threshold four usefulnesses obtained first true usefulness obtained comparing query document database three estimated based database representatives estimation formulas following methods 1 method highcorrelation case 2 previous method 18 3 subrangebased method database representative represented set quadruplets number original number estimated usefulnesses rounded integers experimental results d1 summarized table 2 highcorrelation previous method subrangebased method u matchmismatch dn ds matchmismatch dn ds matchmismatch dn ds table 2 comparison different estimation methods using d1 table 2 threshold u number queries identify d1 useful d1 useful query least one document d1 similarity greater query ie actual nodoc greater 1 6234 queries identify d1 useful comparison different approaches based following three different criteria given threshold match reports among queries identify d1 useful based true nodoc number queries also identify d1 useful based estimated mismatch reports number queries identify d1 useful based estimated nodoc reality d1 useful queries based true nodoc example consider matchmismatch column using method highcorrelation case means 1474 queries identify d1 useful based true nodoc 296 queries also identify useful based estimated nodoc highcorrelation approach also queries identify d1 useful based highcorrelation approach reality d1 useful 35 queries clearly good estimation method match close u mismatch close zero threshold note practice correctly identifying useful database significant incorrectly identifying useless database useful database missing useful database harm searching useless database therefore estimation method much larger match component method b mismatch component significantly larger bs mismatch component considered better b table 2 shows subrangebased approach substantially accurate previous method 18 turn substantially accurate highcorrelation approach matchmismatch criteria fact thresholds 01 04 accuracy subrange based method 91 higher match category dn threshold dn difference nodoc column given estimation method indicates average difference true nodoc estimated nodoc queries identify d1 useful based true nodoc example average difference 1474 queries smaller number dn better corresponding estimation method table 2 shows subrangebased approach better previous method thresholds turn much better highcorrelation approach dn criteria ds threshold ds difference avgsim column given estimation method indicates average difference true avgsim estimated avgsim queries identify d1 useful based true nodoc smaller number ds better corresponding estimation method table 2 shows subrangebased approach substantially accurate two approaches thresholds experimental results databases d2 d3 summarized table 3 table 4 respectively tables 2 4 following observations made first subrangebased estimation method significantly outperformed two methods database criteria second match components best database d1 good database d2 database d3 probably due inhomogeneity data databases d2 d3 highcorrelation previous method subrangebased method u matchmismatch dn ds matchmismatch dn ds matchmismatch dn ds table 3 comparison different estimation methods using d2 highcorrelation previous method subrangebased method u matchmismatch dn ds matchmismatch dn ds matchmismatch dn ds table 4 comparison different estimation methods using d3 using quadruplets approximate numbers section 41 proposed simple method reduce size database representative approximating needed number average weight using one byte 4 bits accuracies parameter values reduced 4 bytes 1 byte essentially difference performance see table 5 relative table 2 table 6 lists experimental results numbers represented 4 bits except probability continues use 1 byte database d1 results compare tables 5 6 table 2 show drop accuracy estimation due approximation small criteria matchmismatch dn ds table 5 using one byte number matchmismatch dn ds table using 4 bits number probability uses one byte d1 similar slightly weaker results obtained databases d2 d3 see tables 7 8 relative table 3 d2 tables 9 10 relative table 4 d3 using triplets original numbers section 321 discussed importance maximum normalized weights correctly identifying useful databases especially single term queries used since single term queries represent large fraction queries internet environment expected use maximum normalized weights matchmismatch dn ds table 7 using one byte number matchmismatch dn ds table 8 using 4 bits number probability uses one byte d2 matchmismatch dn ds table 9 using one byte number d3 matchmismatch dn ds table 10 using 4 bits number probability uses one byte d3 significantly improve overall estimation accuracy queries among 6234 queries used experiments 1941 single term queries table 11 shows experimental results database d1 maximum normalized weights explicitly obtained instead assumed term normalized weights term set documents containing term satisfy normal distribution therefore maximum normalized weight estimated 999 percentile based average weight standard deviation comparing results table 2 table 11 clear use maximum normalized weights indeed improve estimation accuracy substantially nevertheless even estimated maximum normalized weights used results based subrangebased approach still much better based highcorrelation assumption obtained previous method 18 similar conclusion reached results table 3 table 12 compared results table 4 table 13 compared matchmismatch dn ds table 11 result d1 maximum weights estimated matchmismatch dn ds table 12 result d2 maximum weights estimated matchmismatch dn ds table 13 result d3 maximum weights estimated 6 combining terms subrangebased estimation method presented earlier terms assumed independently distributed documents database although overall experimental results reported section 5 good room improvement high retrieval thresholds see tables 2 3 4 threshold values 05 06 thus propose following scheme incorporate dependencies terms estimation process quite term dependency models information retrieval literature see example treedependency model bahadurlazarsfeld model generalized dependency model 26 18 employ bahadurlazarsfeld model incorporate dependencies estimation process model somewhat complicated addition make use maximum normalized term weight experimental results last section indicate maximum normalized term weight critical parameter thus following approach used instead consider distributions terms j database documents within set documents terms document largest sum normalized term weight normalized term weight j let largest sum called maximum normalized weight combined term denoted mnw ij terms j combined single term probability document database maximum normalized weight combined term mnw ij assumed 1n n number documents database pointed earlier unlikely another document database maximum normalized weight combined term two terms independently distributed documents database probability document database normalized sum term weights mnw ij two terms j estimated using subrangebased estimation method specifically polynomial representing probability document maximum normalized term weight followed probabilities document certain percentiles weights term written see example 4 similarly another polynomial written term j multiplying two polynomials together desired probability estimated criteria two terms j combined single estimated probability term independence assumption different 1n maximum normalized weight combined term higher maximum normalized weight two individual terms since aim estimate similarities similar documents latter condition ensure combined term used lead smaller similarities former condition implemented computing difference absolute value 1n estimated probability comparing preset threshold difference exceeds threshold two terms combined difference term pair j denoted ij stored together combined term ij two terms combined obtain documents containing terms distribution sum normalized weights two terms distribution apply subrangebased estimation combined term combined term ij store maximum normalized sum mnw ij average normalized sum standard deviation probability occurrence difference ij last quantity utilized determine term combined given term query explained later example 5 suppose users query q computer algorithm normalized term weight used example let maximum normalized weight terms computer algorithm mw respectively suppose polynomials two terms suppose maximum normalized weight combined term computer algorithm mnw greater mw 1 mw 2 multiplying polynomials probability document total normalized weight associated two terms mnw 12 higher 3878 10 gamma5 probability based assumption two terms independent actual probability 761 example since estimated probability actual probability differ substantially two terms combined combined term occurs 53 documents total 761 documents average normalized weight 0352 standard deviation normalized weights 0203 using subrangebased method combined term first subrange contains maximum normalized sum 0825 probability associated subrange 00013 second subrange median 98th percentile standard normal distribution table constant c 98th percentile 2055 thus median weight 0352 0769 note larger end point second subrange corresponds percentile since median 98th percentile width second subrange 2 thus probability normalized weight combined term document lies second subrange median 98th percentile 0000158 median weights probabilities subranges determined similarly thus polynomial combined term general om 2 term pairs need tested possible combination number terms large testing process may become time consuming order process easily carried restrict terms query terms ie terms appearing previously submitted queries pair terms adjacent locations query latter condition simulate phrases since components phrase usually adjacent locations given query need estimate distribution similarities query documents database taking consideration certain terms query may combined shall restrict combined term contain two individual terms essential decide given term query whether combined term combined term combined specifically consider three adjacent terms followed j followed k query term combined preceding term combined term j phrase usually consists two words simpler recognize phrases containing two words phrases containing three words otherwise check combined exists combined check combined term jk exists combined terms exist compare differences ij jk larger difference indicates term combined term j query example jk larger ij term j combined k distribution combined term used estimate distribution similarities documents query one combined term exists combined term used none two combined terms exists term j combined term using strategy combine terms perform experiments set queries three databases d1 d2 d3 results reported tables 14 15 16 clear combinedterm method better subrangebased method matchmismatch measure especially thresholds 05 06 large close optimal results obtained theoretically possible get better results 1 combining three terms together 2 modify polynomial representing two terms combined noted polynomial combined term take consideration situations exactly one two terms occurs possible include situations however would require storing information similarly process combining 3 terms one feasible would introduce complications since simple combinedterm method yields close optimal results clear whether worthwhile complicate estimation process subrangebased method combinedterm method u matchmismatch dn ds matchmismatch dn ds table 14 comparison different estimation methods using d1 subrangebased method combinedterm method u matchmismatch dn ds matchmismatch dn ds table 15 comparison different estimation methods using d2 subrangebased method combinedterm method u matchmismatch dn ds matchmismatch dn ds table comparison different estimation methods using d3 conclusions paper introduced search engine usefulness measure intuitive easily understood users proposed statistical method estimate usefulness given search engine respect query accurate estimation usefulness measure allows metasearch engine send queries appropriate local search engines processed save communication cost local processing cost substantially estimation method following properties 1 estimation makes use number documents desired user threshold retrieval unlike estimation methods rank search engines without using information 2 guarantees search engines containing similar documents correctly identified submitted queries singleterm queries since internet users submit high percentage short queries sent correct search engines processed 3 experimental results indicate estimation methods much accurate existing methods identifying correct search engines use estimating number potentially useful documents database estimating average similarity similar documents intend finetune algorithm yield even better results perform experiments involving much larger many databases current experiments based assumption term weights satisfy normal distribution however zipfian distribution 20 may model weights accurately also examine ways reduce storage requirement database representatives acknowledgment research supported following grants nsf iri9509253 cda9711582 hrd9707076 nasa nagw4080 nag55095 aro naah049610049 daah049610278 grateful luis gravano hector garciamolina stanford university providing us database query collections used 6 r characterizing world wide web queries probabilistic model distributed information retrieval searching distributed collections inference networks automatic discovery language models text databases starts stanford proposal internet metasearching generalizing gloss vectorspace databases broker hierar chies generalizing gloss vectorspace databases broker hier archies overview first text retrieval conference information data management research agenda 21st century real life information retrieval study user queries web information system corporate users wide area information servers information retrieval systems searching world wide web clustered search algorithm incorporating arbitrary term dependencies search broker determining text databases search internet estimating usefulness search engines information retrieval introduction modern information retrieval finding similar documents across multiple text databases estimation number desired records respect given query principles database query processing advanced applications server ranking distributed text resource systems internet tr ctr kinglup liu clement yu weiyi meng discovering representative search engine proceedings eleventh international conference information knowledge management november 0409 2002 mclean virginia usa kinglup liu adrain santoso clement yu weiyi meng discovering representative search engine proceedings tenth international conference information knowledge management october 0510 2001 atlanta georgia usa amir hosein keyhanipour behzad moshiri majid kazemian maryam piroozmand caro lucas aggregation web search engines based users preferences webfusion knowledgebased systems v20 n4 p321328 may 2007 clement yu prasoon sharma weiyi meng yan qin database selection processing k nearest neighbors queries distributed environments proceedings 1st acmieeecs joint conference digital libraries p215222 january 2001 roanoke virginia united states shengli wu fabio crestani distributed information retrieval multiobjective resource selection approach international journal uncertainty fuzziness knowledgebased systems v11 nsupplement p8399 september kingkup liu weiyi meng clement yu discovery similarity computations search engines proceedings ninth international conference information knowledge management p290297 november 0611 2000 mclean virginia united states clement yu weiyi meng wensheng wu kinglup liu efficient effective metasearch text databases incorporating linkages among documents acm sigmod record v30 n2 p187198 june 2001 zonghuan wu weiyi meng clement yu zhuogang li towards highlyscalable effective metasearch engine proceedings 10th international conference world wide web p386395 may 0105 2001 hong kong hong kong weiyi meng zonghuan wu clement yu zhuogang li highly scalable effective method metasearch acm transactions information systems tois v19 n3 p310335 july 2001 clement yu kinglup liu weiyi meng zonghuan wu naphtali rishe methodology retrieve text documents multiple databases ieee transactions knowledge data engineering v14 n6 p13471361 november 2002 robert losee lewis church jr information retrieval distributed databases analytic models performance ieee transactions parallel distributed systems v15 n1 p1827 january 2004 weiyi meng clement yu kinglup liu building efficient effective metasearch engines acm computing surveys csur v34 n1 p4889 march 2002