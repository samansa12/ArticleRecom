distributing chemical process optimization application gigabit network evaluate impact gigabit network implementation distributed chemical process optimization application optimization problem formulated stochastic linear assignment problem solved using thinking machines cm2 simd cray c90 vector computers psc intel iwarp mimd system cmu connected gigabit nectar testbed report experience distributing application across heterogeneous set systems present measurements show communication requirements application depend structure application use detailed traces build application performance model used estimate elapsed time application different computer system network combinations results show application benefits highspeed network need high network throughput increasing computer systems get faster also observed supporting high burst rates critical although structuring application communication overlapped computation relaxes bandwidth requirements b introduction highperformance networks made attractive distribute computeintensive applications across computer systems connected localarea widearea networks obvious benefit applications combine resources several systems reduce execution time heterogeneous computing special case distributed computing added benefit application component mapped onto appropriate architecture thus optimizing efficiency computation result heterogeneous computing result superlinear speed ie using n systems application runs n times faster individual systems eg 22 important question critical network performance success distributed computing many coarsegrain applications distributed successfully across relatively slow networks highspeed networks needed want apply distributed computing wide class applications including grand challenge national challenge applications first indication many applications computationally intensive also data intensive large data sets exchanged distributed tasks second observation computer systems get faster networks keep paper evaluate impact network bandwidth performance chemical process optimization application application economically important chemical process industry representative large class optimization problems fields use twostep evaluation strategy first distributed application across three systems connected gigabit nectar testbed intel iwarp system mimd cmu thinking machines cray c90 vector computers psc implementation shows feasibility distributed computing class problems allows us identify problems distributing large applications across heterogeneous systems evaluation however limited single set systems single network second step evaluation use detailed tracing information collected execution build performance model application model captures communication computation requirements application data dependencies different tasks model used estimate execution time different computational systems different networks remainder paper organized follows section 2 give overview application section 3 discuss mapping nectar testbed section 4 describe experimental set computational results present application performance model section 5 use model study impact network node performance application execution time section 6 summarize section 7 2 application overview modeling optimization applications primary tool decision making throughout chemical process industry typical engineering examples process design materials allocation optimal control realtime optimization process scheduling production capacity planning applications uncertainty inherent decision models processes ie model parameters estimates real values although failure account uncertainty key parameters decision problems lead nonoptimal solutions 2 deterministic optimization methods still predominant chemical engineering today 10 including explicit treatment uncertainty models increases computational requirements beyond capabilities todays computers optimal resource allocation problem expressed linear assignment problem lap stated follows given n resources eg raw materials n demands eg processing units set costs c ij assigning resource demand j find set one toone resourcedemand assignments minimizes total cost z primal lap defined follows lap subject x terms denote source terminal node sets corresponding resources demands respectively term denotes bipartite graph matching edge set 1 connecting source terminal nodes exclusive manner despite combinatorially large solution space variety polynomial time algorithms developed achieve efficiency exploiting special structure linear assignment problem see 20 21 14 17 details serial parallel algorithms lap problem stochastic costs c ij defined probabilistically ie represented probability distribution instead single value stochastic lap problems solved converting deterministic problems using certainty equivalent transformation 8 however transformation increases size lap problem exponentially furthermore transformation alters structure problem solution procedures specialized deterministic lap longer apply algorithm appropriate stochastic lap costs independently normally distributed variables presented 5 6 3 distributed stochastic lap solutions stochastic lap problem combine diversity tasks best suited different type computer system eg mpp fast scalar processor consequently application good candidate heterogeneous computing section describe mapped stochastic lap onto set heterogeneous systems connected nectar gigabit testbed discussion solution methods well mpp computing implementation results reported 5 6 7 31 application overview mapping figure 1 shows structure mapping distributed chemical process optimization application real world model ie production cost data simulated iwarp sampling normal distribution means ranging 0100000 standard deviations randomly set cost element production data sent c 90 analyzed used compute lap cost matrix means statistical model analysis statistical reduction c90 sends cost matrix cm2 lap solver initialization entire cost matrix received cm2 computes reduced cost matrix sends indices potentially optimal assignments c90 maximum cardinality matching computed c90 based initial zeroelement indices reduced cost matrix unweighted bipartite matching cm2 completes weighted matching portion lap solution 6 complete implementation application would include bayesian inference step generates probability manifold production data would also use means variances lap solver simplifications fundamentally alter structure mapping application production data process network market influences process disturbances raw materials selection stochastic linear assignment problem optimal initial matching model analysis bayesian inference real world material process j pq n q 1 n ts raw materials products pq l pyq pq dq figure 1 mapping stochastic linear assignment problem nectar testbed mapping application shown figure 1 driven performance suitability task system iwarp mimd system well suited independent complex function evaluations required simulation integration problem c90 used highperformance vector machine well suited statistical reduction computations serial initial matching procedure cm2 simd machine well suited row column scanning matrix arithmetic operations characteristic weighted matching portion lap solver indeed task could theoretically run systems none tasks well suited machines since singlemachine version application cannot measure speedups expect efficient processor utilization results superlinear speedup 32 implementation application developed adding iwarp sample generation phase existing highly optimized lap solver running c90 cm2 illustrates important advantage heterogeneous computing large applications built existing separatelydeveloped application components allowing fast application development without porting effort expect opportunity one main motivations heterogeneous computing highspeed networks implementation program control carried using pvm 32 9 cm2 acting master node reading key parameters cm2 lap started data generation job iwarp gen analysis job c90 ana simulated plant data generated iwarp sent using streams package 24 11 hippi network read c90 using socket interface communication c90 cm2 dhsc library 19 dhsc supports distributed computing across cm2 c90 using hippi network interconnect streams package dhsc used raw hippi actual communication fact variety tools used added considerable complexity distributing application moreover since tools support message passing hide details systems individuals expertise system help program development programming tools hide system details parallelizing compilers 26 clearly needed make distributed computing accessible users figure shows different computational tasks application interact figure scale horizontal bars represent computational tasks executed different systems arrows represent communication application delineated two main phases data generation analysis ii lap solution first phase consists data generation iwarp data analysis c90 lap initialization cm2 since time required lap initialization cm2 insignificant phase ends last packet cost matrix data reaches cm2 c90 generation analysis pipelined minimize execution time communication viewed separate stages pipeline pipelined computation stages execute concurrently execution time determined slowest stage pipeline lap solution phase application mainly performed cm2 remote procedure call rpc c90 perform unweighted initial matching phase operations including communication performed serially execution time sum execution time component data generation statistical analysis lap solver initialization convert reduced cost matrix unweighted bipartite matching weighted bipartite matching time data generation analysis lap solution figure application flowchart stochastic optimization problem 33 communication requirements application communication requirements summarized table 1 table gives source destination data format data size four data streams application component items table 1 refer application executable images residing compute nodes programs gen ana lap initialization comprise data generation analysis phase application programs lap fem denote lap solver initial matching lap solution phase application data source data type destination size plant data gen iwarp float ana c90 n 2 n ts stochastic model ana c90 int lap cm2 n 2 cost data lap cm2 int fem c90 n 2 initial matching fem c90 int lap cm2 n table 1 summary application communications requirements communication requirements application depend size n lap number samples n ts used simulate plant data number grid points n used map probability manifold number stochastic parameters n q stochastic lap application executed n n ts since probability manifold mapping ie bayesian inference problem omitted n q since costs assumed uncertain using mean variance lap solution instead mean would double communication requirements stochastic model data stream implementation reduction factor inversely proportional n discussed detail section 52 seen table data reduction analysis step ana c 90 first phase application significantly larger data requirements second data format changes several times computation resulting presentation layer overhead generated samples represented bit ieee floating point numbers iwarp transmitted c90 converted 64 bit cray floating point format statistical analysis c90 sample means converted bit integers transmitted cm2 data exchange cm2 c90 initial matching procedure use integer representation however result different data representations c90 cm2 transfers still require expensive transformation 16 27 18 structure application figure 2 four data streams different characteristics first stream sent continuous stream 64 kbyte packets since iwarp sends data generated second stream sent sequence bursts number bursts degree pipelining ie number blocks used computation data transfer c90 example degree pipelining 16 problem size 4k stream would consist 16 bursts 4 mbyte finally last two data streams sent single burst 4 experimental results distributed optimization application described executed nectar gigabit testbed may 9 1994 section describes execution environment presents analyzes measurements 41 nectar testbed nectar testbed one five national gigabit testbeds 23 funded arpa nsf cnri nectar testbed joint effort nectar group cmu bellcore pittsburgh supercomputer center psc bell atlantic goal testbed build gigabit metropolitan area network man demonstrate value applications testbed consists twentyfive dec alpha workstations iwarp parallel array 3 paragon 12 cmu campus cray c90 cray t3d 1 cm2 alpha cluster pittsburgh supercomputer center psc alpha workstations iwarp use network interfaces provide architectural support copy avoidance optimize throughput 24 25 15 paragon alpha frame buffer alpha file server cmu campus pittsburgh supercomputer center 26 km atmsonet file server alpha alpha alpha alpha file server parallel data lab vision lab offices figure 3 nectar testbed highperformance distributed computing system network figure connecting systems consists two hippi based lans linked 24 gbs atmsonet link 13 4 representing metropolitan area network man execution runs reported paper used hippi link runs parallel atmsonet link peak throughput link 100 mbsec however latency setting hippi connections across 26 km link packet maximum achievable throughput 73 mbsec packets size 64 kbyte 42 experimental set solved lap problem instances sizes number samples n ts fixed 64 another parameter experiment degree pipelining generation analysis phase block size used c90 analysis model data similarly sized run random number seeds used iwarp generation simulated plant data identical leading generation identical pseudorandom cost matrices done allow repeated identical runs help isolate elements system performance experiments cm2 run dedicated mode ie 32k processors attached users front end resulting repeatable runs lap component cm2 c90 application components run single node interactive mode variations elapsed times occurred repeated instances problem iwarp array run dedicated mode variations iwarp elapsed times occurred repeated instances problem due nonconstant load front end 43 measurements analysis experimental data problem size 4k plotted figures 4 6 problem sizes 1k 2k give similar results figure 4 shows total execution time application broken generation gen lap initial matching c90 fem reduction cost matrix plus lap weighted matching solution lap iwarp times dominate pipelined execution generation analysis phase time gen represents total time phase times c90 cm2 shown times shown function degree pipelining 500 0 750 012500 elapsed time sec pipelining c90 npassesnnrows fem gen figure 4 elapsed time versus degree pipelining generation analysis phase size 4k 4k problem generation component accounts 80 total execution time ie iwarp bottleneck observe first phase takes slightly longer highest lowest degree pipelining 8 128 effect small surprise since block sizes ranging 500 mbyte 32 mbyte iwarpc90 communication factor 64 smaller c90cm2 communication large enough allow efficient communication number blocks large enough pipeline fill drain times significant figure 5 shows cpu times model analysis data transfer conversion computations c90 4k case data read ana1readdata iwarp converted cray floating point format ana1cf132c statistical computations apost performed reduce data set cost means sent ana1dhscwrite cm2 shown figure read write cpu times relatively small vectorized format conversion c90 quite fast 10 nsec per floating point number nearly costly statistical computations indicates improvement conversion routines eliminating agreeing single representation highly desirable since cpu times elapsed times recorded program ana c90 data pipelining effect seconds pipelining c90 npassesnnrows ana1dhscwrite ana1readdata ana1cf132c apost figure 5 c90 cpu time versus degree pipelining analysis step 6 idle 22 0 22 2 22 4 22 6 22 initialmatching sec solution sec figure 6 initial matching cm2 idle versus lap solution times size 4k x axis figure 6 shows total execution time second phase application lap solver running cm2 lap c90 7 lap initial matching elapsed time c90 left black full curve cm2 idle right red dashed curve given versus total lap solution time different runs solver since cm2 ran fully dedicated mode cm2 times identical runs problem size change lap solution time result load changes c90 graph confirms close correspondence increasing initial matching times waiting idle time cm2 increasing overall lap solution times implication distributed computing systems load imbalance one machine affect utilization computing resources system running distributed applications efficiently require careful allocation system resources reasonably predictable response time systems used application 5 application performance model project behavior application result changes distributed computing system ie number compute nodes network bandwidth developed model application component next section use model examine sensitivity application execution time network computer system performance 51 computational models computational model represents execution time task application function problem size case iwarp cm2 size system parameters summarized table 2 models derived using data large series parameterized runs 5 6 7 models presented section sparse approach discuss tradeoffs sparse dense approach section 64 function parameter value problem size n 1k4k number data samples n tsrange cost matrix values r 10 5 number iwarp nodes p iwarp 64 measured number cm2 nodes p cm2 8k32k measured bandwidth iwarp c90 bw 1 hippi bandwidth c90 cm2 bw 2 hippi table model parameters used experiments data generation model gen running iwarp timing tests indicate 64 node iwarp array capable generating per second rate sample generation corresponds directly number iwarp nodes doubling number nodes doubles rate results following elapsedtime performance model generating samples iwarp ts 1 data analysis model ana running c90 using single node c90 elapsed times closely correlated data size n 2 model analysis c90 obtained regression experimental data sample means computed lap solver model lap running cm2 using data set parametric tests statistical model fit data giving following performance model lap solver constants 3 c natural scaling equation 3 number cost elements number cm2 nodes also time solve weighted matching problem weakly correlated degree precision characterized range r lap initial matching model fem running c90 using data set parametric tests statistical model fit data routine fem running c90 giving r constants c equation 4 first second order terms ie n n 2 respectively characterize lap size ratio nr represents natural indicator relative degree precision 6 52 traffic models traffic models shown table 3 simpler computational models linear dependence data time size data stream table 1 transfer time inverselyproportional sustainable network bandwidth note sustainable network bandwidth practice limited sending receiving hosts ability put data remove data network transfer model equation gen output iwarp c90 dt ts ana output c90 cm2 dt lap output cm2 c90 dt 4an 2 fem output c90 cm2 dt fem 4n table 3 application traffic models equation 7 requires explanation program lap running cm2 computes reduced cost matrix part initialization procedure 5 7 zero elements matrix potential optimal assignments lap program fem running c90 finds optimal initial matching given zero element indices reduced cost matrix simplest way transmit elements cm 2 transmit entire reduced cost matrix c90 selects zero elements called dense approach however use efficient way called sparse approach cm2 first locates indices zero elements sends vector indices c90 initial matching advantage sparse approach lower data transfer requirements ie number indices sent c90 general less n 2 however tradeoff cm 2 less efficient c90 constructing sparse incident matrix parameter equation 7 reduction factor indicating density incidence matrix setting 1 gives traffic model dense approach scalar application dependent lies general 1 n 1 depends relative precision problem defined ratio n r used experiments inversely proportional problem size 6 performance analysis equations 18 represent performance model distributed application section use model study performance two phases computation individually combined degrees freedom model lap problem size number nodes used iwarp cm2 machines sustainable network bandwidths also use model compare dense sparse transfer options cm2 c90 look different structure two application phases influences dependence execution time network bandwidth 61 data generation analysis performance model result pipelining generation analysis phase throughput system determined slowest component note iwarp c90 overlap communication computation communication phases independent stages pipeline elapsed time generation analysis phase maximum genx dt ana realistic parameters iwarp computation iwarp c90 communication limit performance phase figure 7 shows estimated generation analysis time function two limiting parameters iwarp nodes bandwidth time phase figure 7 estimated execution time sec generation analysis phase size 4k function network bandwidth mbsec iwarp system size number nodes experiment using 64 node iwarp system model predicts execution time 766 seconds closely matches measured time gen figure 4 note point flat part graph respect network bandwidth indicating generation phase limited computation iwarp network bandwidth scale iwarp system 256 nodes equivalent 66 node paragon system application 512 nodes 66 node paragon coprocessors used computation execution time drops almost linearly demonstrated sustained application throughputs 40 mbsec iwarp c90 11 network becomes bottleneck iwarp grows 460 nodes note paragon systems much larger 66 nodes built 40100 mbsec network bandwidth means excessive data analysis c90 becomes bottleneck systems size p iwarp 1540 nodes iwarp c90 bandwidth 62 lap solution performance model computation communication occur sequentially lap solution phase elapsed time sum lapx dt fem figure 8 shows execution time lap solver function network bandwidth number cm2 nodes shape graph different graph summarizing generation analysis phase specifically flat regions impact network bandwidth absolute execution time independent number cm2 nodes cm2 nodes bandwidth time phase figure 8 estimated execution time sec lap solver phase size 4k function network bandwidth mbsec size cm2 system number nodes sparse incidence matrix transferred cm 2 c90 experiment used cm2 32k nodes bandwidth c90 cm2 12 mbsec model predicts execution time 200 seconds close measured time lap plus fem figure 4 note c90cm2 bandwidth would need presentation layer format conversion cm2 18 higher throughput execution time would reduced 10 63 application performance model combining equations 18 gives overall application performance model accounting iwarp data generation transfer overlap discussed figure 9 shows sensitivity network bandwidth number iwarp nodes given 64k cm2 nodes bw results comparable figure 7 since data generation analysis phase application dominates execution time strongest sensitivity network bandwidth limits data flow iwarp c90 adequate number iwarp nodes avoid generation computation bottleneck relatively iwarp nodes generation computations become restrictive network shown graph observed experiments iwarp nodes bandwidth time figure 9 estimated execution time sec combined lap application size 4k function iwarpc90cm2 network bandwidth mbsec iwarp system size number nodes sparse approach figure 10a shows overall application performance sensitivity network bandwidth number cm2 nodes using 256 iwarp nodes assuming bw data generation computations iwarp limiting high bandwidths improvements occur increasing bandwidth data generation rate gain increasing number nodes cm2 base line time remains 200 seconds due data generation bottleneck figure 10b shows comparable sensitivities using 1024 iwarp nodes increasing number iwarp nodes data generation bottleneck removed overall performance continues improves network bandwidth increased cm2 nodes bandwidth time 256 iwarp nodes cm2 nodes bandwidth time b 1024 iwarp nodes figure estimated execution time sec lap application size 4k function iwarpc90cm2 network bandwidth mbsec size cm2 number nodes two iwarp system sizes 64 tradeoff dense sparse reduced matrix transfer two alternatives exist transferring initial reduced cost matrix zero element indices cm2 c90 initial matching computed see section 52 sparse approach cm2 locates zeroelement indices initial reduced cost matrix packs incidence vector sends vector c90 initial matching subproblem dense approach cm2 sends entire initial reduced cost matrix c90 forms sparse incidence matrix advantage sparse approach lower data transfer requirements since density term equation 7 bound 1 however tradeoff cm2 less efficient c90 constructing sparse incident matrix cm2 nodes bandwidth time phase figure estimated execution time sec lap solver phase size 4k function network bandwidth mbsec size cm2 system number nodes dense incidence matrix transferred cm 2 c90 figure 11 time lap solution phase plotted function network bandwidth number cm2 nodes dense matrix transfer method problem size 4k sensitivity network bandwidth minimal sparse method figure 8 expected dense transfer method shows higher sensitivity network bandwidth figure 11 since time pack incidence matrices cm2 relatively small compared overall lap solution sparse method superior performance dense method almost casesthe dense method sometimes faster small data sets however c90 times dense method could accelerated using single c90 node multiple c90 nodes used expect dense method would faster sparse method network bandwidth sufficiently high 65 impact network throughput execution time impact network speed overall application performance viewed terms percent time application communications bound vs compute bound interconnection compute speed number nodes network performance requirements lap application shown figures 12 13 application phase 1 figure 12 communications computing operations overlapped hence long network performance adequate application run completely compute bound increasing network bandwidth speed application decreasing network bandwidth immediately slows application complete stop limit phase 2 figure 13 communications computing operations serial case clearly key acceptable performance keep communications time significantly computational time increasing decreasing network performance direct effect elapsed time phase application communication times binding practically insignificant due smaller data sets transferred statistical reduction cost data percent elapsed time bound communications phase 1512128iwarp nodes figure estimate percent elapsed time application bound communications versus effective network bandwidth various iwarp sizes lap size 4k0 01 percent elapsed time bound communications phase 2 cm2 nodes figure phase 2 estimate percent elapsed time application bound communications versus effective network bandwidth various cm2 sizes lap size 4k sparse transfer 7 conclusions paper described distribution chemical process optimization application across heterogeneous system consisting intel iwarp cray c90 thinking machines cm2 computers connected nectar gigabit testbed implementation demonstrates several benefits heterogeneous computing efficient execution ability build applications connecting existing separatelydeveloped application components across network without port code note benefits eg efficient system utilization may realized different systems simultaneously dedicated application sufficient network bandwidth available creates organizational problem supercomputer centers implementation application also demonstrated difficulty distributing application across heterogeneous computers since expertise number different computer systems required better programming tools required time hide details systems integrate systems better measurements performance model application show sensitivity execution time network bandwidth depends strongly structure distributed application computation communication serialized rpcbased solution phase application network delay including increased latencies due physical distance increase execution time able burst high rates critical average bandwidth requirements application indicative communication requirements contrast communication computation overlapped pipelined generation analysis phase application network bandwidth requirements relaxed speed light latencies hidden specifically average bandwidth requirements application represent bandwidth sustained network acknowledgments would like thank michael hemy jamshid mahdavi todd mummert help distribution optimization application gregory j mcrae input conceptualization application also gratefully acknowledge use computational resources pittsburgh supercomputer center especially psc applications support group also grateful use computational resources sandia national labs final preparation manuscript research supported national science foundation defense advanced research projects agency cooperative agreement ncr8919038 corporation national research initiatives advanced research projects agency dod monitored space naval warfare systems command spawar contract n0003987c0251 r cray t3d system architecture overview models model value stochastic programming integrated solution highspeed parallel computing gigabitsec wide area computer networks potential applications technology challenges scheduling presence uncertainty probabilistic solution assignment problem scheduling presence uncertainty linear assignment problem solution largescale modeling optimization problems using heterogeneous supercomputing systems planning uncertainty using parallel computing pvm system supercomputer level concurrent computation heterogeneous network workstations recent developments evaluation optimization flexible chemical processes gigabit io distributedmemory systems architecture applications paragon xps product overview 25 gbs sonet datalink sts12c inputs hippi interface gigabit computer networks shortest augmenting path algorithm dense sparse linear assignment problems software support outboard buffering checksumming experiments gigabit neuroscience application cm2 combinatorial optimization networks matroids deployment hippibased distributed supercomputing environment pittsburgh supercomputing center deployment hippibased distributed supercomputing environment pittsburgh supercomputing center linear assignment problem implementation testing primaldual algorithm assignment problem running climate model heterogeneous gigabit network testbeds architecture evaluation highspeed networking subsystem distributedmemory systems host interface architecture highspeed networks programming task data parallelism multicomputer network supercomputing experiments cray2 cm2 hippi connection tr shortest augmenting path algorithm dense sparse linear assignment problems warp integrated solution highspeed parallel computing gigabit network testbeds exploiting task data parallelism multicomputer experiments gigabit neuroscience application cm2 architecture evaluation highspeed networking subsystem distributedmemory systems software support outboard buffering checksumming gigabit io distributedmemory machines host interface architecture highspeed networks ctr michael hemy peter steenkiste gigabit io distributedmemory machines architecture applications proceedings 1995 acmieee conference supercomputing cdrom p58es december 0408 1995 san diego california united states peter steenkiste highspeed network interface distributedmemory systems architecture applications acm transactions computer systems tocs v15 n1 p75109 feb 1997 peter steenkiste networkbased multicomputers practical supercomputer architecture ieee transactions parallel distributed systems v7 n8 p861875 august 1996