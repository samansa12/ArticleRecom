adaptively scheduling parallel loops distributed sharedmemory systems abstractusing runtime information load distributions processor affinity propose adaptive scheduling algorithm variations different control mechanisms proposed algorithm applies different degrees aggressiveness adjust loop scheduling granularities aiming improving execution performance parallel loops making scheduling decisions match real workload distributions runtime experimentally compared performance algorithm variations several existing scheduling algorithms two parallel machines ksr1 convex exemplar kernel application programs used performance evaluation carefully selected different classes parallel loops results show using runtime information adaptively adjust scheduling granularity effective way handle loops wide range load distributions prior knowledge execution used overhead caused collecting runtime information insignificant comparison performance improvement experiments show adaptive algorithm five variations outperformed existing scheduling algorithms b introduction oops richest sources parallelism widely used scientific application programs many scientific applications set independent tasks typically exists parallel loop called doall loop processing element iteration independent others performance loop scheduling algorithm mainly affected three overhead sources synchronization loop allocation load imbalance data communication although desirable efficient algorithm minimize three sources overhead usually impossible conflicts arise among exploiting processor affinity processor affinity refers certain data access dependence task specific processor precise definition given section 31 favors allocation loop iterations close data tends cause load imbalance load balance favors fine grain allocation loop iterations small number iterations allocated order minimize effects uneven assignment however fine grain allocation tends increase synchronization overhead loop allocation overhead different applications overhead source affects performance differently hence efficient loop scheduling algorithm optimize performance adaptively trading synchronization overhead loop allocation overhead load imbalance overhead data communication overhead moreover dynamic scheduling algorithm assume prior knowledge execution times loop iterations execution loop usually unpredictable practice far many novel dynamic scheduling algorithms proposed eg 2 4 5 6 8 9 11 10 algorithms fall two distinct classes central queue based distributed queue based central queue based algorithms 2 8 11 10 iterations parallel loop stored shared central queue processor exclusively grabs iterations central queue exe cute major advantage using central queue possibility evenly balancing workload keeping good load balance central queue based algorithms differ way reduce synchronization loop allocation overheads however three limitations associated use central queue iteration central queue likely dynamically allocated execute processor facilitate exploitation processor allocation processors one remotely access central work queue thereby generate heavy network traffic processors contend central queue central queue tends performance bottleneck resulting longer synchronization delay order exploit processor affinity inherent parallel execution many loops eliminate central bottleneck affinity scheduling algorithm proposed 6 distributes central queue local proc 10459219971000 1997 ieee yan x zhang high performance computing software laboratory university texas san antonio san antonio 782490664 c jin intervoice inc dallas texas email cjinintervoicecom manuscript received feb 19 1996 information obtaining reprints article please send email transpdscomputerorg reference ieeecs log number d96040 yan et al adaptively scheduling parallel loops sharedmemory systems 71 essor algorithm partitions iterations parallel loop statically local queues processor involved remote access load imbalance occurs markatos leblanc 6 show affinity scheduling almost achieves best performance tested cases compared central queue based algorithms enhance affinity scheduling algorithm presence large correlated imbalance loop execution time subramaniam eager 9 propose two loop partition methods dynamic partition wrapped partition two partition methods however improve affinity scheduling specific applications execute specific assumptions distribution loop execution time design distributed queue based algorithms reason prefer kinds loop partition methods uniform partition due uneven unpredictable execution time loop iterations hence crucial distributed queue based algorithm able dynamically efficiently schedule tasks runtime even load imbalance caused static partition existing affinity scheduling algorithms 6 9 processors schedule loop iterations local queues using allocation scheme time 1p remaining iterations local queue allocated p number processors iteration allocation scheme may efficient example initial loop partition balanced processors complete execution iterations local queues time processor grab iterations local queue allocation instead 1p remaining iterations hand initial loop partition balanced lightly loaded processors finish execution iterations local queue soon possible immediately turn help heavily loaded processors hence processors able dynamically increase decrease allocation granularity based runtime information reduce synchronization loop allocation overhead balance load evenly motivates us design adaptive scheduling algorithms improve existing affinity scheduling algorithms major objective paper exploit potential dynamic information reduce loop execution time propose adaptive scheduling algorithm five variations algorithms dynamically adjust allocation granularity according programs execution history classify parallel loop execution patterns fairly select set applications experimentally verify effectiveness algorithms comparison proposed affinity scheduling algorithms 6 9 experimental results show using runtime information adaptively adjust scheduling granularity effective way handle wide range load distributions prior knowledge used overhead caused collecting runtime information insignificant comparison benefit gained method proper algorithms designed experiments show aggressive algorithm using dynamic formation improves execution performance parallel loops adaptive algorithm outperformed existing scheduling algorithms experimentally organization paper follows section 2 describes detail main ideas design adaptive scheduling algorithm presents five variations different degrees aggressiveness adjustment loop scheduling granularities order assess precisely effectiveness proposed algorithms practice analyze classify program characteristics applications section 3 selected five representative kernel applications benchmarks performance evaluation section 4 report experimental results compari sons summarize paper section 5 similar affinity scheduling algorithm 6 adaptive affinity scheduling algorithm also constructed following three phases initial partition phase deterministic assignment policy used partition iterations parallel loop local queues processors ensures iteration always assigned processor start assignment scheme parallel loop executes repeatedly parallel iteration accesses data set different executions first execution parallel loop bring data locally processors subsequent execution parallel loop involves local data access local scheduling phase based local scheduling policy processor allocates part remaining iterations local queue execute local queue empty local scheduling cause remote access overhead local queue shared processors critical section used protect allocation loop iterations local queue local scheduling overheads mainly come synchronization overhead loop allocation overhead execution critical section reducing number allocations crucial improve performance local scheduling phase remote scheduling phase processor finishes execution iterations local queue remotely allocates portion iterations loaded processor system execute remote scheduling phase aimed dynamically balancing workload iteration reassigned avoids processor thrashing remote scheduling causes remote data access overhead well synchronization overhead loop allocation overhead instead relying preknowledge loops exe cution adaptive affinity scheduling algorithm exploits potential using dynamic execution history adaptively adjust iteration chunking size reduce synchronization loop allocation overheads algorithms also maintain better load balance main idea designs minimize local scheduling overhead phase dynamically balancing workload 72 ieee transactions parallel distributed systems vol 8 1 january 1997 speeded results reduction loop execution time initial phase loop n iterations partitioned chunks uniform size np p processors reason prefer partition methods absence precise prediction execution distribution loops iterations initial partition identical one 6 local scheduling phase processing speed variable termed ps variable set processor keeps track number iterations processor executed far initially set zero increased one time processor finishes execution iteration comparing local ps variable ps variables processor observe load distribution time processors smaller ps variable values executed iterations heavier workload executed processors larger ps variable values appli cations load distribution state certain steady dura tion feasible speculate load distribution near future current observation although applications really surge variance load distri bution prediction difference minimized dynamic readjustments order processors response spontaneously dynamic changes iteration work load necessary differentiate workload states processors fairness select average number iterations executed processors ie p pivot partition workload states processors following three types heavily loaded hlthe processors ps value smaller p normally loaded nlthe processors ps value within range p p lightly loaded llthe processors ps value equal larger p nonnegative range control variable adjusts distribution hl processors nl processors processors variations parameter would affect algorithmic performance dynamic features execution real applications loops make impractical impossible analytically determine optimal value discuss effect different values performance experiments giving empirical method determining performanceefficient order control chunk size allocation loop erations chunksize control variable k set processor processor always removes 1k remaining iterations local work queue execution beginning local scheduling phase chunksize control variables initialized value p total number processors adaptively independently adjusted chunksize control function p function p uses load state current value k processor two input parameters adjusts chunksize control variable processor heavily loaded p increases k aiming reducing chunk size iterations remaining heavily loaded processor executed lightly loaded processors therefore balancing workload efficiently processor normally loaded lightly loaded p decreases k aiming increasing chunk size normally loaded lightly loaded processor finish iterations local work queues soon possible immediately starts help heavily loaded processors processor completes execution iterations local queue turns remote scheduling phase affinity scheduling algorithm 6 processor exhausts local work queue starts help heavily loaded processors removes 1p remaining iterations heavily loaded processor allocation method may efficient processors turn help processors determine chunking size according current number lightly loaded normally loaded processors able help heavily loaded processors near future processor determines chunk control variable k follows n total number lightly loaded processors normally loaded processors n means include heavily loaded processor processor allocate remaining iterations p total number processors processor allocates 1k remaining iterations heavily loaded processor execute procedure repeat local work queues empty initially k smaller value p big chunk size used reduce number remote allocations experiments next section show selected big chunk size increase risk imbalancing load subsequently processors become lightly loaded normally loaded k increase reaches maximal value p following pseudocode description adaptive affinity scheduling algorithm given imple mentation code automatically inserted compiler application programs processor dynamically schedule execution loops without interference operating system existing work generate dynamic scheduling programs hand experiments order focus algorithmic study 1 initial partition phase initialpartitionn p n iterations uniformly partitioned p processors iterations processor yan et al adaptively scheduling parallel loops sharedmemory systems 73 2 local scheduling phase processor loop processor gets 1k local iterations execute adjusts k 1k iterations range load state processor adjust chunking granularity 3 remote scheduling phase processor loop locklocalqueuej iterations processor j iterations finished adaptively changing loop scheduling granularity major characteristic distinguishes adaptive affinity scheduling affinity scheduling algorithm 6 remotely reading ps variables processors overhead caused adaptive scheduling algorithm collecting execution history processors increased overhead nullifies benefit adaptively varying loop scheduling granularity adaptive affinity scheduling algorithm may exhibit performance improvement existing affinity scheduling algorithms different variations adaptive affinity scheduling algorithm constructed designing different chunk size control protocols function p propose four mechanisms adaptive algorithm let k chunk size control variable processor local scheduling phase exponentially adaptive ea mechanism ea mechanism processor increases decreases value chunksize control variable k factor time according current load state chunksize control function p formally defined state base base c h base integer constant choose two base initially k set p adaptive algorithm linearly adaptive la mechanism la mechanism processor increases decreases chunk size control variable k constant time interval according current load state chunksize control function p formally defined state con con con constant specified users choose 1 experiments la mechanism changes chunk size slower pace ea algorithm less risk imbalancing work load larger synchronization loop allocation overhead conservatively adaptive ca mechanism careful selection chunking size loop scheduling algorithm crucial find compromise synchronization overhead load imbal ance allocating bigger chunk iterations loop tends reduce synchronization loop allocation overhead increase risk imbalancing load previous work 5 shows order reasonable load imbalance synchronization head safe chunk size control variable k choose value p 2p ca mechanism constructed restricting varying range chunk size control variables la mechanism within p2 2p chunk size control function defined follows state min con constant 0 p use one experiments greedily adaptive ga mechanism ga mechanism employs twophase consensus method greedily enlarge chunking size non heavily loaded processors ga mechanism records previous load state processor processor finds nonheavily loaded state two consecutive allocations greedily reduces chunksize control variable 1 ie grabs remaining iterations local work queue exe cute otherwise processor increases decreases chunking size using conservation method ca mechanism pre record previous load state processor let c record current load state processor chunk size control function r pre c pre min c h hl hl hl hl keeping maintaining ps variable processor allows four adaptive mechanisms know exactly current workload processor thereby ps variables used adjust speed proc 74 ieee transactions parallel distributed systems vol 8 1 january 1997 essor consequence adjust workload among processors also introduces loop allocation head design heuristic variation denoted still adopts framework adaptive scheduling algorithm instead using ps variables determine workload distribution among processors use number iterations actually executed processor guide adjustments scheduling granularities initially parallel loop uniformly distributed proces sors processor repeats grabbing 1k remaining iterations local queue execute without adjustment k processor finishes iterations local work queue turns get iterations heavily loaded processor j processor said lightly loaded processor j heavily loaded processor increases scheduling granularity processor decreases scheduling granularity hence lightly loaded processors turn early possible help heavily loaded processors heavily load processors remain much workload possible even lightly loaded processors end execution parallel loop processors check whether executed approximately number iterations ie balanced workload processors increase scheduling granularities speed subsequent executions parallel loop comparing four variations adaptive algo rithm ea la ca ga ha variation differs several aspects instead determining load state time local scheduling used adaptive algo rithms ha variation updates load states processors remote scheduling phase end one execution parallel loop causes less scheduling overhead adaptive algorithm 2 ha variation works requiring parallel loop nested sequential loop execute repeatedly parallel loop executes ha variation becomes affinity algorithm 6 pseudocode heuristic variation adaptive affinity scheduling algorithm ha variation shown follows 1 initial partition phase initialpartitionn p n iterations uniformly partitioned p processors iterations processor 2 local scheduling phase processor loop locklocalqueuei remaining iterations 3 remote scheduling phase processor loop locklocalqueuej iterations processor j increase chunk size processor decrease chunk size processor j 4 program section end parallel loop barrierbarrier p tid execute code findmaximumandminimumofchunk kmax kmin p2 workload bal anced increase chunk size processor barrierbarrier p markatos leblanc 6 show affinity scheduling algorithm hereafter simplified ml algorithm outperforms algorithms exploit processor affinity hence focus comparing variations adaptive scheduling algorithm affinity scheduling algorithm two variations 9 scheduling algorithms evaluated compared 1 ml affinity scheduling algorithm 2 se dynamic initial partition affinity scheduling algorithm adaptive affinity algorithm exponential adaptive mechanism ea adaptive affinity algorithm linearly adaptive affinity mechanism la 5 adaptive affinity algorithm conservatively adaptive mechanism ca adaptive affinity algorithm greedily adaptive mechanism ga 7 heuristic adaptive variation ha experiments conducted two machines ksr1 hierarchicalringbased cache coherent sharedmemory system convex examplar crossbar ringbased cache coherent sharedmemory system address methods selecting application kernels evaluating algorithms 31 principles selecting application kernels considering effects program features scheduling algorithms characterize parallel loops three factors affinity loop iterations processors distribution loop execution time granularity loop iterations iterations parallel loop may exhibit affinity processors loop nested sequential loop executed repeatedly one fact parallel processing dominant overhead source many applications communications synchronization hence first classify parallel loops two classes potential affinity parallel loops nested sequential loop nonaffinity parallel loops executed strong iterations potential affinity parallel loop exhibits affinity processors significantly affected sizes data sets accessed iterations data locality iterations better data locality iteration means data set accessed iteration changes less significantly different executions iterations data locality determines affinity iterations processors hand sizes data sets accessed iterations determine benefit exploiting processor affinity parallel loops better data locality iterations small data sets eg one integer exploiting processor affinity improve execution times parallel loops balancing load reducing synchronization overhead let di data set iteration ith execution parallel loop let di size bytes data set di n fis average size data sets iteration n executions parallel loop n f f1 average size difference two data sets two consecutive loop executions iteration indicates approximately much data reloaded execution iteration parallel loop data locality iteration quantitatively evaluated following defined locality rate locality rate value 0 1 locality rate one means iteration always accesses set data larger locality rate represents better data locality strong iteration affinity processor quantitatively evaluated localityrate average number data sets accessed repeatedly data sets may always stored local cache selection potential affinity parallel loops use data locality data size differentiate affinity iterations processors unpredictable variance execution times parallel loops major obstacle loop scheduling algorithms work efficiently order show much parallel loop scheduling algorithms tolerate different distributions workload among iterations selected parallel loops load distribution cover three distinguished types loops balanced loops iteration amount computation time predictable imbalanced loops computation times iterations parallel loop vary predictable function loop control variable load distribution parallel loop fixed executes repeatedly unpredictable imbalanced loops computation times iterations change randomly depending initial input runtime variables eg execution time branch statement depends actual execution path ml algorithm handles load imbalance remote scheduling se algorithm improves ml algorithm performance predictable imbalanced parallel loops load distribution parallel loop changed multiple executions parallel loop execution times iterations loop increase decrease monotonically loop control variable adaptive algorithm variations dynamically adjust loop scheduling granularity speed load balance procedure based execution history processors following experiments show adaptive algorithm handle load imbalance efficiently wider range ml se algorithms besides affinity load distribution iteration granularity loop another important factor affecting performance loop scheduling algorithms parallel loops coarse granularity execution times loop iterations significantly larger overhead remote access delay balancing workload crucial reducing synchronization loop allocation heads parallel loops fine granularity execution times loop iterations much smaller overhead remote access delay important minimize scheduling overheads determination iteration granularity parallel loop depends interaction parallel loop underlying system difficult tell whether parallel loop coarsely grained finely grained execution instead classifying parallel loops granularity consider effect iteration granularity experiments based analyses classify parallel loops six types affinity load distributions loops potential affinity balanced workload ii loops potential affinity predictable workload iii loops potential affinity unpredictable workload iv loops nonaffinity balanced workload v loops nonaffinity predictable workload vi loops nonaffinity unpredictable workload order complete understanding well scheduling algorithm works area realworld applications select one application type loop nonaffinity unpredictable workload rare case practice therefore evaluate scheduling algorithms applications first five types loops 32 applications selected application kernels including potential affinity loops successive overrelaxation sor type jacobi iteration ji type ii transitive closure tc type iii matrix multiplication mm type iv adjoint convolution type v application kernels including non affinity loops type balanced affinity loops sor 76 ieee transactions parallel distributed systems vol 8 1 january 1997 iterations sor parallel loop take time execute iteration always accesses set data exploiting processor affinity may improve performance better balancing workload application parallel iteration locality rate one data set n array elements computational granularity parallel iteration type ii predictable affinity loops jacobi iteration ji iteration precision ajk ne 0andj ne ji program top 20 rows elements nonsingular matrix nonzero elements generated random number generator iterations parallel loop different workload determined distribution nonzero elements exploiting load imbalance would improve performance however workload parallel iteration changed executed repeatedly jth iteration parallel loop always accesses jth row matrices bj x0j jth iteration fixed executed repeatedly processor needs reload x0j cache x0j updated execution parallel loop hence application kernel exhibits good processor affinity iteration data set size n data locality close one average computational granularity iteration smaller sor kernel type iii unpredictable imbalanced affinity loops transitive closure tc kernel aji eq true aik eq true may exhibit serious load imbalance ji iteration parallel loop execution parallel iteration may computational granularity o1 depending input matrix iterations exhibit weaker affinity processors sor ji due random computation fea ture difficult quantify data locality affinity parallel iteration type iv balanced nonaffinity loops matrix multiplication mm cijcijaik mm program affinity exploit parallel iterations computational granularity reducing synchronization loop allocation overhead way improve performance application used investigate whether adaptive algorithm lower scheduling overhead ml algorithm type v predictable imbalanced nonaffinity loops adjoint convolution ac similar matrix multiplication application parallel loop ac kernel executes hence exhibit processor affinity however computational granularity ith parallel iteration 2 changing specific function control variable produce significant imbalanced load distribution triangular pat tern kernel used examine efficiently adaptive algorithms handle load imbalance caused uniform partition performance metric use evaluate algorithms execution time execution time measures differently scheduling algorithms work different types applications given problem size 41 comparisons loop scheduling algorithms first use np 2 value four adaptive scheduling variations ca la ea ga shall discuss effect value performance later section discuss np 2 costeffective next section fig 1 presents execution time seconds sor running two eight processors ksr1 convex exemplar since sor perfectly balanced application kernel dynamic partition se algorithm improve performance ml affinity algorithm hand introduced overhead ml algorithm result ml se perform worst among due overhead caused loop allocation synchronization steps adaptively increasing chunk size yan et al adaptively scheduling parallel loops sharedmemory systems 77 time processor accesses local work queue adaptive algorithms reduce times processors need accessing local work queues therefore scheduling synchronization overhead reduced five adaptive algorithms outperformed ml se algo rithms ea ga performed best among since take three steps adjust chunk size finish remaining iterations la variation needs allocation steps ea ga need ha ca variations change chunk size limited range therefore could get best benefit reducing synchronization loop allocation overhead perfectly balanced applications fig 2 plots execution time jacobi iteration ji different scheduling algorithms ksr1 convex exemplar ji application fits se algorithm best since workload distribution illustrates rectangular shapethe leftmost 20 heavy load remaining 80 almost zero workload se algorithm readjust initial partition balance workload processor improve execution time lower execution time curves se algorithm confirm instead readjusting initial partition adaptive algorithms reduced execution time adjusting chunk size processor lightly loaded processor took larger number iterations execute turned help heavily loaded processor heavily loaded processor took small number iterations execute might leave iterations processors finish solving linear system size ji kernel la ga ea ha variations perform well se ca variation performed slightly worse adaptive algorithms using ca variation processors zero workload still cannot take 2p iterations execute therefore need time finish lightly loaded jobs turn help heavily loaded processor meantime heavily loaded processor may already taken large number jobs execute leave enough jobs idle processors b fig 1 performance sor ksr1 exemplar b b fig 2 performance ji ksr1 exemplar b 78 ieee transactions parallel distributed systems vol 8 1 january 1997 fig 3 presents execution time transitive closure kernel random input graph 1024 nodes 10 edges uniformly presented execution parallel loop workload uniformly distributed among iterations however total workload increases next execution parallel loop fig 3a fig 3b show comparative performance seven tested algorithms respectively ksr1 machine exemplar se algorithm ml algorithm perform similarly case se algorithm little chance improve ml algorithm readjusting load distribution algorithms la ea ga performed best among adjusted scheduling granularity aggressively combining results experimental results sor conclude load balanced applications aggressively adjusting scheduling granularity efficient method reduce scheduling synchronization head thus improve performance well results also show overhead collecting state information significant comparing benefit gained adaptively adjusting scheduling granularity tested scheduling algorithm variations transitive closure kernel skewed input graph 640 nodes containing clique 320 nodes edges case load imbalance significant computation across iterations total load parallel loop increases one execution next execution times scheduling algorithm presented fig 4a fig 4b although authors 9 claim se algorithm assumes execution time particular iteration vary widely one execution loop another results show se algorithm still improve ml algorithm case studies adaptive algorithm capture variance load precisely se algorithm la ea ga ha performed better se algorithm variation performed similarly se algorithm b fig 4 performance tc skewed input ksr1 exemplar b b fig 3 performance tc random input ksr1 yan et al adaptively scheduling parallel loops sharedmemory systems 79 experimental results show adaptively adjusting scheduling granularity efficient way handle load imbalance unpredictable loop applications parallel loop embedded sequential loop call nonaffinity loop se algorithm heuristic variation ha chance improve ml affinity algorithm adjust initial partition adjust chunk size near end one execution parallel loop hope new partition new chunk size play role next execution parallel loop want see adaptive variations perform better ml algorithm nonaffinity loop fig 5a fig 5b present performance scheduling algorithms matrix multiplication mm algorithms ml se ha performed similarly algorithms ea la ga dynamically detect workload distribution conditions rapidly increase chunk size processors take remaining iterations execute accesses local work queue variation also increases chunk size limit 2p remaining iterations therefore involves less synchronization loop allocation overhead ml presents overhead ga la ea compared experimental results kernel sor adaptive variations improve ml algorithm mm significantly parallel loop executes one time fig 6a fig 6b present performance scheduling algorithms kernel adjoint convolution 128 se ha could improve ml algorithm since parallel loop embedded within sequential loop load imbalance across iterations significant since first iteration took time proportional 2 last iteration took time proportional o1 expected ml se ha performed similarly ea la ga performed best among ca variations performance 42 determine costefficient value previous section used np 2 value adaptive scheduling algorithm variations b fig 6 performance ac ksr1 exemplar b b fig 5 performance mm ksr1 exemplar b n number iterations parallel loop p number processors used execute parallel loop tested several values trying give optimal value evaluated adaptive scheduling algorithm variations different values five benchmark applications ksr1 exemplar values selected evaluate np np 2 32 4 respectively due space limitation present part results two adaptive scheduling variations ea ca respect one kernel applica tion remaining results specify follows also support conclusions going present fig 7a presents performance sor kernel ksr1 using ea adaptive variation different values also present performance ml algorithm running sor kernel comparison ea showed best performance ea presented worst performance since sor wellbalanced application processors normal workload large value like np guarantees workload state processor always normal processor increase chunk size reduce execution time although sor wellbalanced sometimes events cache misses page faults interprocessor communication delays bring execution time variance among iterations use small value 4 presence interference kinds events processors take workload states heavy therefore decrease chunk size factor two since give limit chunk size ea variation decrease chunk size exponential rate may cause processors take small chunk processor may take one iteration access local work queue similar selfscheduling ea spent much time ml number processors two four number processors increases six eight 4 becomes close np 2 reason holds 2 processors cannot determine workload states correctly due system interference small value number processors increases gets close value np 2 therefore good performance algorithms number processors four six eight fig 7b presents performance sor application ksr1 using ca adaptive variation different values also show curve ml algorithm curve ea variation 4 order compare fig 7b shows ca np performed best among ca shows small value comparison value np 2 may cause negative effect performance adaptive algorithm due system interfer ence ca performed worst among curves also notice curve much lower ea 4 reason limit range chunk size ca variation within p2 2p guarantees processor takes least 1 remaining iterations execute access local work queue 5 conclusion adaptively adjusting loop allocation granularity according workload execution speed proc essor loop scheduling algorithm demonstrates better performance affinity scheduling algorithm proposed markatos leblanc 6 dynamic partitioned affinity scheduling algorithm proposed subramaniam eager 9 authors shown two algorithms presented best performance among loop scheduling algorithms adaptive scheduling algorithm suitable wider range application programs reduce execution time well loadbalanced parallel loops also load unbalanced parallel loops experiments show overhead caused collecting state information sig b fig 7 performance sor ksr1 using ea different values using ca different values b yan et al adaptively scheduling parallel loops sharedmemory systems 81 nificant comparing benefit gained one important conclusion research efficiently using runtime information significantly improve efficiency loop scheduling algorithms among variations adaptive scheduling algo rithm ea la ga variations always demonstrate better performance ca ha variations although ea la ga higher risk ca terms causing loadimbalance terms much sensitive system interference observed worst performance phenomena case studies ping pong effect state processor often switched lightly loaded heavily loaded cause overwhelmed scheduling head addition negative effect ea la ga variations significantly reduced selecting appropriate workload control constant np 2 currently developing analytical model determine optimal values machine architecture may another important factor affects performance loop scheduling algorithms far able test adaptive algorithm variations ksr1 exemplar experimental results indicate algorithms performance quite independent sharedmemory architectures ever effectiveness adaptive algorithm significantly affected system size system size scales large cost collect runtime information increases advantages adaptive algorithm nullified increased overhead adaptive algorithm suitable scheduling parallel loops small number processors acknowledgments appreciate neal wagners samir das careful reading manuscript constructive comments wish thank anonymous referees helpful comments suggestions work supported part us national science foundation grants ccr 9102854 ccr9400719 us air force research agreement fd20409264157 air force office scientific research grant afosr9510215 r convex computer corp factoring practical robust method scheduling parallel loops dynamic scheduling method irregular parallel programs safe selfscheduling parallel loop scheduling scheme sharedmemory multiproces sors using processor affinity loop scheduling sharedmemory multiprocessors design tradeoffs process scheduling shared memory multiprocessor system guided selfscheduling practical selfscheduling scheme parallel supercomputers affinity scheduling unbalanced workloads processor selfscheduling multiple nested parallel loops trapezoid selfscheduling practical scheduling scheme parallel compilers tr ctr tatiana tabirca len freeman sabin tabirca laurence tianruo yang feedback guided dynamic loop scheduling convergence continuous case journal supercomputing v30 n2 p151178 november 2004 jose l aguilar ernst l leiss data dependent loop scheduling based genetic algorithms distributed shared memory systems journal parallel distributed computing v64 n5 p578590 may 2004 clemens grelck svenbodo scholz sac offtheshelf support dataparallelism multicores proceedings 2007 workshop declarative aspects multicore programming p2533 january 1616 2007 nice france yong yan xiaodong zhang zhao zhang cacheminer runtime approach exploit cache locality smp ieee transactions parallel distributed systems v11 n4 p357374 april 2000 sotiris ioannidis umit rencuzogullari robert stets sandhya dwarkadas craulcolon compiler runtime integration adaptation load1this work supported part nsf grants cda9401142 ccr9702466 ccr9705594semi external research grant compaq scientific programming v7 n34 p261273 august 1999 clemens grelck shared memory multiprocessor support functional array processing sac journal functional programming v15 n3 p353401 may 2005