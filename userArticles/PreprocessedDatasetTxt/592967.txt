cactus tools grid applications cactus open source problem solving environment designed scientists engineers modular structure facilitates parallel computation across different architectures collaborative code development different groups cactus code originated academic research community developed used many years large international collaboration physicists computational scientists discuss intensive computing requirements physics applications using cactus code encourage use distributed metacomputing detail design makes ideal application testbed grid computing describe development tools experiments already performed grid environment cactus including distributed simulations remote monitoring steering data handling visualization finally discuss grid portals already developed cactus open door global computing resources scientific users b introduction cactus 1 2 open source problem solving environment designed provide unied modular parallel computational framework physicists engineers cactus code originally developed provide framework numerical solution einsteins equations 3 one complex sets partial dierential equations physics equations govern cataclysmic events collisions black holes supernova explosions stars solution equations computers continues provide challenges elds mathematics physics computer science modular design cactus enables people institutes disciplines coordinate research using cactus collaborating unifying tool name cactus comes design central core connects application modules thorns extensible interface thorns implement custom developed scientic engineering applications einstein solvers applications computational uid dynamics thorns standard computational toolkit provide range capabilities parallel io data distribution checkpointing cactus runs many architectures applications developed standard workstations laptops seamlessly run clusters supercomputers parallelism portability achieved hiding driver layer features io system calling interface simple abstraction api cactus api supports cc f77f90 programming languages thorns thus thorn programmers work language nd convenient required master latest greatest computing paradigms makes easier scientists turn existing codes thorns make use complete cactus infrastructure turn used thorns within cactus cactus provides easy access many cutting edge software technologies developed academic research community globus metacomputing toolkit hdf5 parallel le io petsc scientic computing library adaptive mesh renement web interfaces advanced visualization tools 2 need grid many applications using cactus framework important one continues drive development solution einstein equations large varied computational requirements solving equations scenarios black hole neutron star collisions make good example demonstrating need grid computing ideal testbed developing new techniques developing cactus infrastructure make full use grid problems advances immediately available applications implementing full einstein equations nite dierence code amounts memory requirement around one hundred 3d arrays cpu requirement thousands oating point operations per grid point timestep considering suciently accurate solution full 3d black hole problem require least 1000 grid points spatial dimension implies computers analyze large data sets created simulation requires advanced techniques le management visualization date resources individual supercomputers available limited simulations around 200300 grid points spatial dimension even simulations generate huge amounts data negotiating curiosities dierent supercomputers batch queues le systems something physicists relish grid provides way access resources needed simulations provides uniform access layer supercomputers computing resources making resources far useful scientists want use simulations given appropriate permissions networks allocations grid enabling software scientist could principle run simulation set supercomputers connected grid thus able run far larger problems would possible routine basis without grid proper software tools grid provides necessary infrastructure connect machines deal resulting large data sets ultimately resources analyze volumes data dream physicists grid computing provide scientic programming environment similar shown figure 1 allowing working scenarios physicist sitting cafe berlin idea colliding black hole run maybe try new initial data set test new evolution method uses web portal pda select needed cactus thorns estimate required computer resources grid software takes selecting appropriate machine set machines use available grid software automatically creates transfers executables parameter les starts run remote resources several coees connects running cactus simulation using one remote access thorns sees things going better expected rings colleagues watch isosurfaces streamed cactus want save 3d data sets analyze later connect cactus run using web browser turn output grid functions interested futuristic scenario sounds pieces already exist prototype form developed integrated described 1 4 5 6 figure 1 dream grid computing grid infrastructure provides transparent exible working environment providing access global computing resources 3 grid computing cactus cactus designed grid grid applications mind provides layer top grid giving programming interface allows user completely ignorant nature machine machines simulation runs code provides access grid resources distributed io parallelization across number supercomputers precisely interface resources single machine 7 cactus thus provides convenient laboratory computer scientists develop metacomputing techniques tested real physics applications also real users without requiring changes physics application code new technique perfected immediately made available whole community cactus users grid computing developments experiments performed using cactus several years described sections capabilities developed connection cactus several projects dfnverein project 4 aei germany funded exploit high speed networks colliding black hole simulations concentrating remote visualization 8 remote steering distributed io 9 project funded socalled kdi program american national science foundation nsf joins institutes develop astrophysics simulation collaboratory 5 provide environment physicists utilize grid computing problems collisions neutron stars grads project 10 also funded nsf usa using cactus one applications developing grid based computing environment european grid forum 11 chosen cactus one applications running european gridtestbed technologies brought scientic engineering communities developed 31 distributed simulations grid actively working develop techniques allow researchers harness computational resources wherever may grid could include distributed set machines connected high speed networks allowing larger faster simulations would possible single machine supercomputing neutron star collision run cactus using globus 6 metacomputing toolkit split domain across two t3es dierent sides atlantic one munich germany one san diego california simulation neutron stars collided somewhere cyberspace atlantic ocean simulations launched visualized steered show oor orlando scaling across two machines used simulation roughly 50 believe excellent considering large amount communication required machines solve equations latencies transatlantic link latency bandwidth characteristic features determine speed network cactus aware features netuned order optimize communication example network high latency also high bandwidth many small messages coalesced fewer bigger ones running metacomputing environment one deal dier ent types networks sharedmemory distributedmemory highspeednetwork lan waninternet dierent latencybandwidth characteristics also possibility distinguish dierent types network connections one single distributed run tune cactus communication patterns adequately partly already achieved using mpichg2 nextgeneration mpiimplementation distinguish processors located one host native mpi installed processors separated lan wan according location mpichg2 choose dierent protocols tcp vendors mpi communication one single distributed parallel run cactus used new technology without problem demonstrated many metacomputing experiments last year including supercomputing 2000 dallas aspect loadbalancing since dierent architectures provide dierent types processors dierent speeds cactus provides ability decompose whole computational problem subproblems dierent size local processor power running metacomputing environment real productionlevel user portal built described section 5 making possible congure start cactus runs one machine via special webbased gui greatly simplies situation scientist since deal every detail local supercomputer batchsystems usernamepassword portal provides automatic staging compilation code local supercomputer distributed machines appearing single virtual machine 32 checkpointing distributed simulations grid grid computing events past often understood onetime well prepared attempts harness several machines time realistic setting compute resources considerable size expected available given time instead availability dynamic process true grid application capable dealing dynamic allocations resources cactus framework addresses challenge providing sophisticated crossplatform checkpointing mechanism general checkpointing technology allows user freeze state application writing checkpoint le disk application restored continued later time single mpp supercomputer cluster compute farm tightly coupled heterogeneous supercomputers figure 2 migration scenario distributed simulation simulation starts three tightly coupled supercomputers checkpointed migrated single machine computation migrates nish cluster cactus checkpoint memory image application written disk found several checkpointing systems total set user dened objects variables scalars etc memory images tend quite huge compatible within class operating systems architectures approach allows smaller architectureindependent check points crossplatform checkpoints cactus transferred arbitrary architectures operating systems numbers processors restarting continuing simulations checkpointing mechanism completely transparent user request checkpoints written regular timestep intervals end requested compute time allocation using steering interface immediately current timestep internal technicalities parallel io hidden user user control checkpoint behavior frequency parallel io means steerable parameters checkpoint mechanism allows output single global checkpoint le well multiple checkpoint les distributed machines mechanism makes use parallel io possible restarting multiple checkpoint les recombined single le used restart arbitrary set machines parallel restart operation multiple les currently restricted topology machines future developments add intelligent components immediately restart multiple checkpoint les across arbitrary machine topologies respect distributed simulations cactus user ability perform distributed run checkpoint simulation even though run heterogeneous machine set checkpoint le transferred new conguration machines continue simulation new pool machines dier previous one type number machines involved well number processors exible chain distributed simulations illustrated figure 2 initial simulation run across three tightly coupled supercomputers checkpointed checkpoint le transferred single mpp machine restarted second checkpointing event third stage simulation continued cluster system 4 gridenabled communication io techniques parallel driver layer cactus manages allocation domain decomposition grid variables well synchronization processor boundaries provided thorn means dierent thorns used implement dierent parallel paradigms pvm pthreads openmp corba etc cactus compiled many driver thorns required subject availability one actually used chosen user run time parameter le current standard driver thorn called pugh uses mpi provide parallelism order perform distributed cactus simulations grid pugh thorn simply linked gridenabled mpichg 12 implementation mpi available globus toolkit thus preparing gridenabled version cactus compilation choice completely transparent application thorn programmers add code gridenabled 9cactus using globus job submission tools cactus users start cactus runs grid environment easily single machine cactus io subsystem implemented similar generic manner esh provides runtime interface arbitrary io thorns register specic io methods methods turn invoked esh application thorn read external data cactus variables dump contents storage medium postprocessing analysis visualization purposes io thorns currently available computational toolkit provide methods write simulation data dierent formats 1d traceline plots 2d slices jpeg images full ndimensional arrays arbitrary hyperslabs ndimensional arrays reduction scalars eg minimummaximum values isosurface geometry data particle trajectories runtime standard output also using dierent io libraries flexio 13 hdf5 14 jpeg ascii methods libraries easily added thorn programmers following sections describe detail grid software techniques developed date allow cactus users easily perform postprocessing analysis data produced remote cactus simulation also monitor steer running cactus jobs remotely general overview nal proposed architecture gridenabled io system shown figure 3 hierarchical data format version 5 hdf5 plays key role overall picture hdf5 become widely accepted standard scientic computing community storing data denes exible le format provides ecient software library managing arbitrary multidimensional datasets various types raw data access accomplished via generic virtual file driver vfd layer hdf5 beneath abstraction layer exists set lowlevel io drivers provide dierent ways accessing raw data hdf5 le either located local disk storage media added drivers layer enable existing applications additional capability accessing remote data residing anywhere grid 41 direct remote file access hdf5 already gass driver global access secondary storage local file system virtual file driver layer amira data dpss gridftp virtual file driver layer gass gridftp unix dpss stream handling cactus simulation thorns thorn hyperslab io methods methods visualization amira amira modules steering gui steering dpss server gridftp server hdf5 io library hdf5 io library figure 3 general overview gridenabled io architecture automatically stages complete remote les rst local machine operates local copies via standard unix le io method feasible small medium sized data les however largescale computer simulations often generate largescale data sets single simulations may generate les containing several hundreds gbytes order tbyte machine resources increase conventional postprocessing analysis becomes prohibitively resourceintensive remote simulation data must staged local processing many cases example rstsight visualization purposes small fraction overall data really needed example simulation evolution two colliding black holes output may contain dozen variables representing state gravitational eld perhaps 1000 time steps evolution visualization one might want analyze rst time step one two variables order perform quick preanalysis highresolution data might sucient downsample array variables fetch data every grid point enhancing hdf5 vfd layer driver builds top data grid software components 15 globus toolkit enable existing io layers operate remote hdf5 les directly uniquely addressed url opening appropriate driver read write operations performed network transactions grid completely transparent application using data selection capabilities dening socalled hyperslabs arbitrary rectangular subregions multidimensional data sets optionally downsampling type conversion applied individual time steps zones interesting data read visualized ecient convenient way data grid client software supports remote partial le access distributed parallel storage systems dpss 16 supercomputing 1999 portland cebit 2000 hannover successfully demonstrated feasibility dpss data grid infrastructure demonstrations cactus simulation data residing remote dpss data servers visualized hdf5enabled version visualization package amira 17 illustrated figure 4 remote visualization amira cebit 2000 hannover aei potsdam hdf5 datasets dpss prototype rechenzentrum garching cactus simulation t3e infrastructure gigabit network german figure 4 remote le access visualization demo presented cebit 2000 remote access les located anywhere grid soon provided gridftp driver 18 supports standard ftp protocol enhanced partial le access parallel streaming capabilities grid security mechanisms another challenge occurs simulations carried distributed computer generate physically distributed les would occur exam ple order exploit parallel io methods desirable access transfer distributed data sets consistent single les using global address space pointers pieces locations plan also tackle problems data grid components organizing related les collections logical le instances datagrid project globus group investigating techniques 15 42 remote online data streaming visualization cactus also provides capability stream onlinedata running simulation via tcpip socket communications used many pur poses date common use live data streaming remote visualization focus however vision future grid sim ulations expect running simulations communicate migrate machine machine spawn additional processes grid etc hence expect data streaming fundamental enabling technology future grid simulations exploiting data streaming capabilities describe enable advanced grid simulations demonstrated cactus worm scenario running cactus simulation able migrate using data streaming techniques described site site across european egrid 19 simple example sophisticated types grid simulations based data streaming developing future remainder section focus data streaming use remote visualization multiple visualization clients connect running cactus executable via socket remote machine grid request arbitrary data running simulation display simulation results realtime visualizing example photons falling black hole isosurfaces gravitational waves emitted black hole collision data streaming integrated cactus several dierent ways one method access cactus output les written running simulation les registered http control interface described following section downloaded web browser example simple 1d data graphs viewed simply clicking download le ring example xgraph program twodimensional jpeg images viewed directly web browser continuous time sequences jpegs displayed using autorefresh option capable browsers another technique implements proprietary communication protocol sending specic geometry data isosurfaces particle trajectories raw socket connection visualization program 20 illustrated figure 6 precomputing data simulation side allows parallel rendering images also reduces amount data transferred remote visualization clients figure 5 online visualization amira visualization toolkit 17 allows user visualize slices complete 3d data set streamed running cactus simulation time display isosurface obtained online 3d eld generic approach streaming arbitrary data type based hdf5 io library vfd layer developed stream driver holds hdf5 data streamed cactus simulation inmemory hdf5 le ushclose operation entire le sent figure 6 trajectories freely falling particles vicinity rotating black hole particle positions streamed visualization tool realtime computation demonstrated cactus amira igrid 2000 yokohama 20 socket connected client client application driver used reconstruct inmemory le accessed usual read datasets since vfd layer hides lowlevel io operations upper layers hdf5 library application builds top applications use existing hdf5 lebased io methods immediately online remote data access without changing io interfaces demonstrated using dierent visualization toolkits including amira 17 ibm data explorer 21 lca vision 22 stream driver capable sending data simultaneously multiple clients one key component building collaborative visualization environment scientists dierent sites analyze results remote simulation either looking simultaneously data requesting dierent views working design sophisticated io request protocol implementation external data server handle multiple clients also serve requests individually integrating intelligent data management caching strategies server would relieve simulation communication overhead help reduce data trac general 43 remote monitoring steering cactus computational toolkit contains thorn httpd added cactus simulation provide inbuilt http server pointing web browsers url identifying running cactus job remote machine number collaborators connect monitor steer simulation online provided cactus web interface allows users query certain information run current iteration step list available thorns variables full description parameters current settings successful authorization user also interactively change parameters marked steerable simulation cycle parameters checked appropriate thorns may react changes individually io parameters steerable enables users selectively switch specic output runtime dynamically choosing variables output using io method io options hyperslabbing downsampling parameters may also modied order adjust online data streaming remote visualization clients web interface also used pause simulation optionally chosen condition satised advance simulation single timesteps web interface provided thorn httpd dynamically extensible thorn register update html pages runtime besides download page cactus output les also viewport available embeds dynamically generated jpeg images another steering interface builds top hdf5 stream driver described interface data streaming simply used bidirectional way cactus writes parameters hdf5 le streamed connected steering client user interaction client sends back modied version parameter le read evaluated cactus selfdescribing data format exibility add addi tional userdened information hdf5 also provides possibility build advanced steering clients graphical user interfaces example mini mummaximum values could assigned numerical parameters create sliders gfor convenient user interactions parameters belonging one thorn could sorted group hierarchy building menus features hdf5 make relatively easy implement dynamic graphical user interfaces arbitrary cactus parameter sets adapt current cactus conguration actively working including user interfaces existing visualization clients steering capabilities would limited exchanging hdf5 parameter les could also extended feed back kind data elds cactus instance add photons black hole simulation locate event horizon 5 portals onto grid grid useful concept services used create illusion resources centralized users workstation successful distributed applications grid paradoxically make user least aware fact operating distributed fashion motivation producing grid portal interface like cactus derived desire hide distributed applications immensely complex distributedparallel software architectures behind single point presence make accessible comparatively simple clientside interfaces portal single point presence typically hosted web customized particular user remembers particular aspects customizations regardless user accesses yahoo hotmail typical consumeroriented examples capability fact originators new meaning term portal doesnt matter login url portals get access view personalized environment data ie email placing pse like cactus within portal creates universally accessible interface scientic computing platform gui usercustomizable desktop application users workstation except customized environment accessible virtually location simply connecting url address science portal additional implied function automating entire work ow particular scientic application initial data generation selecting resources run application archival storage management analysis results simulations replaces series perhaps loosely usually poorly integrated tools comprehensive environment customized around particular application finally collaboratory provides additional facilities sharing information either online asynchronously among users portal cactus several key advantages make suitable basis portal design modular design supports dynamic assembly applications online simplied gui sophisticated multiplatform compilation makes simple run code available grid resource without complexities imake performance penalty virtual machine centralized revision control mechanism permits ecient sharing code software updates bug xes finally integrated visualization remote monitoring steering code web interface allows seamless integration capabilities portals webgui astrophysics simulation collaboratory portal 5 concrete use cactus within web portal gui leverages technology developed originally ecommerce applications architecture utilizes commercialgrade stronghold apache webserver oers ssl encryption using site certicate commercial certicate authority running side byside webserver tomcat jsp engine oers cleaner means manage automation elegant easily maintainable fashion jsp allows us directly execute methods serverside java beans rather typical cgiscript methodology parsing state form elements individually http post event java beans directly call java cog 6 pure java implementation globus toolkit extend automation grid resources userstate within system maintained backend database system openldap mysql allows simple replication portal state allowing web services scaled server replication science portals collaboratories play increasingly important role hpc grid evolves natural point organization user communities hpc environment particular eld science particular application code experimental laboratories bring together top researchers interested similar lines research internet provided us access enormous remotely located resources shifted center focus particular hpc site operating environment batch queues security policies rather science computed single point presence oered grid portal recreates gthe traditional laboratory environment scientists share similar interests applications brought together umbrella shared collaboratory portal distributed grid application specic community scientists rather generalpurpose resource unlike traditional webportals implicit oer go wwwmy portal locationorg well everything using compute resources grid portals business plan simply stated go wwwmy portal applicationorg everything need application regardless location compute resources returns focus scientic community scientic application rather location hpc resources grid really working another 5 years longer think example nsf supercomputing centers distinct sites like sdsc ncsa psc instead think particular application collaboratories set study dierent scientic applications resource merely name rather place summary cactus code computational toolkit large scale applications serves provide ideal laboratory developing testing new grid techniques working practices cactus easily congured run grid environment tools developed far already provide many capabilities exploiting global computing resources infrastructure tools developed immediately available user community testing many already successfully benecially used collaborations researching computationally intensive problems black hole neutron star collisions acknowledgements development cactus code highly collaborative eort indebted great many experts dierent institutions advice visions support original design cactus joan masso paul walker since extensively developed aei ncsa washington university pleasure us thank ian foster steve tuecke warren smith brian toonen joe bester globus team argonne national labs anl globus data grid work mike folk hdf5 development group ncsa helped us implementing requirements remote le access hdf5 code brian tierney lawrence berkeley labs dpss support jason novotny nlanr help globus graphical user interfaces michael russell university chicago portal work computing resources technical support provided aei anl ncsa rechenzentrum garchinggermany zib greatly acknowledge nancial support andre merzky thomas radke well provision gigabit network infrastructure course tiksl research project dfn german research network r httpwww numerical relativity tool computational astrophysics j dfn gigabit project toolkit httpwww grid adaptive development software grads httpwww european gridforum httpwww implementation httpwww flexio httpzeus hierachical data format version 5 httphdf distributed parallel storage system httpwwwdidc globus project gridftp universal data transfer grid geodesics kerr spacetime ibm data explorer httpwww httpzeus tr ctr michael russell gabrielle allen greg daues ian foster edward seidel jason novotny john shalf gregor von laszewski astrophysics simulation collaboratory science portal enabling community software development cluster computing v5 n3 p297304 july 2002 henri casanova francine berman thomas bartol erhan gokcay terry sejnowski adam birnbaum jack dongarra michelle miller mark ellisman marcio faerman graziano obertelli rich wolski stuart pomerantz joel stiles virtual instrument support gridenabled mcell simulations international journal high performance computing applications v18 n1 p317 february 2004 gabrielle allen david angulo ian foster gerd lanfermann chuang liu thomas radke ed seidel john shalf cactus worm experiments dynamic resource discovery allocation grid environment international journal high performance computing applications v15 n4 p345358 november 2001 james kohl torsten wilde david e bernholdt cumulvs interacting highperformance scientific simulations visualization steering fault tolerance international journal high performance computing applications v20 n2 p255285 may 2006 katarzyna rycerz alfredo tiradoramos alessia gualandris simon f portegies zwart marian bubak peter sloot regular paper interactive nbody simulations grid hla versus mpi international journal high performance computing applications v21 n2 p210221 may 2007 lingyun yang jennifer schopf ian foster conservative scheduling using predicted variance improve scheduling decisions dynamic environments proceedings acmieee conference supercomputing p31 november 1521 wes bethel john shalf griddistributed visualizations using connectionless protocols ieee computer graphics applications v23 n2 p5159 march ming wu xianhe sun grid harvest service performance system grid computing journal parallel distributed computing v66 n10 p13221337 october 2006