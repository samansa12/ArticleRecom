reflective newton method minimizing quadratic function subject bounds variables propose new algorithm reflective newton method minimization quadratic function many variables subject upper lower bounds variables method applies general indefinite quadratic function local minimizer subject bounds required particularly suitable largescale problem new method exhibits strong convergence properties global secondorder convergence appears significant practical potential strictly feasible points generated provide experimental results moderately large sparse problems based sparse cholesky preconditioned conjugate gradient linear solvers b introduction paper propose new algorithm solving box constrained quadratic programming problem matrix h symmetric general indefinite l denote feasible region strict ug h indefinite interested locating local minimizer problem 11 arises subproblem minimizing general nonlinear functions subject bounds problem right boxconstrained quadratic programming problem represents important class optimization problems subject considerable recent work eg 1 5 12 13 16 19 21 24 26 27 32 special subclass deserves mention boxconstrained leastsquares problem rectangular mbyn matrix n proposed algorithm course applied special case form b determination version algorithm involve formation matrix h open question propose new approach reflective newton algorithm algorithm generates sequence strictly feasible iterates fx k g converges standard assumptions local solution 11 x quadratic convergence rate coleman li 10 establish theoretical properties reflective newton approach applied general nonlinear boxconstrained problem indicate section 5 results apply directly reflective newton procedure proposed quadratic minimization problem 11 paper discuss nature reflective transformation section 2 discuss reflective newton approach applied problem emphasis specialized line search exploit special structure problem section 3 numerical experiments involving implementation reflective newton method applied boxconstrained quadratic minimization 11 discussed section 4 sequence fx k g generated algorithm strictly feasible therefore algorithm regarded interiorpoint algorithm however misleading classification algorithm differs markedly methods commonly referred interiorpoint algorithms example proposed algorithm use barrier function ensure feasibility algorithm generates descent directions q follows piecewise linear path reflecting constraints encoun tered interior point methods hand generate descent directions function restrict step along straightline direction ensure feasibility algorithm similar current proposal probably recent method due coleman hulbert 6 also strong connection previous work coleman li 7 8 9 20 various norm minimization problems driven nonlinear system equations representing firstorder optimality condi tions methods require piecewise quadratic minimization methods differ new algorithm general positive definiteness h required necessary finite upper lower bounds variables colemanhulbert method requires restrictive properties finally colemanhulbert method exteriorpoint method requiring strict decrease piecewise quadratic dual function whereas new method generates feasible iter ates requiring strict decrease original quadratic function q four key observations underpin new approach first possible change constrained problem 11 unconstrained problem without using penalty parameter replace 11 unconstrained problem min qy qy continuous piecewise quadratic function 2 r n unrestricted details transformation given next section including result theorem 1 proving equivalence 13 11 one view algorithm designed find local minimizer qy variables x related piecewise linear transformation reflective transformation feasible sequence fx k g moreover evaluation qy corresponds evaluation qry alternatively one view approach entirely original variables x instead describing method descent algorithm transformed problem qy method described method generates feasible iterates following piecewise linear path induced reflective mapping r discuss second key observation firstorder optimality conditions 11 equivalently 13 expressed single system nonlinear equations newton step system descent direction q neighbourhood local solution moreover neighbourhood local solution 13 full newton step 14 ie unit step size newton direction yields decrease qy important point suggests newton process 14 compatible 13 least neighbourhood solution suggests ultimate secondorder convergence achieved decreasing qy third observation leads globalization newton process turns newton equation 14 nonlinear system representing firstorder optimality conditions written form symmetric matrix 3 moreover turns positive definite neighbourhood minimizer interpreted loosely second derivative matrix qy suggests use ellipsoidal constraint ensure descent direction far solution specifically solve deltag positive diagonal scaling matrix delta positive discuss 10 good choice matrix dx ie diagonal matrix th diagonal component equal jv xj 1 2 vector defined figure 1 diagonal matrix plays important role paper henceforth reserve notation without superscript refer definition 17 course k refers 17 quantities defined current point x k iv g 0 l fig 1 definition vx solving 16 involves solving symmetric positive definite system suitable ds thus easy see 16 leads descent direction q current point may felt solving 16 expensive way determine descent direction largescale setting mind restricted version 16 used algorithm particular similar 2 usually restrict lowdimensional subspace 16 replaced lowdimensional subspace n provided ellipsoidal constraint inactive near solution lowdimensional subspace 3 function q piecewise quadratic function therefore r q always exist ever proposed algorithm generates points r q defined ultimately includes newton direction solution 19 eventually newton step 15 fourth major ingredient approach line search descent direction k determined onedimensional line search performed approximately locate minimizer q k structure q k onedimensional piecewise quadratic function efficient specialized line search procedure used alternative view onedimensional piecewise linear search performed along reflective path p k ff defined reflective transformation r beginning conclude introduction short review optimality conditions problem 11 firstorder optimality conditions written feasible point x local minimizer 11 let f ree denote set indices corresponding free variables point x f ree secondorder necessary conditions written 4 feasible point x local minimizer 11 2 ree submatrix h corresponding index set f ree conditions necessary sufficient state practical sufficiency conditions first need definition degeneracy definition 1 point x nondegenerate index definition state secondorder sufficiency conditions nondegenerate feasible point x satisfies 2 ree 0 x local minimizer 11 2 reflective transformation one interpretation approach solving boxconstrained quadratic programming problem 11 involves transformation unconstrained piecewise quadratic minimization problem 13 purpose section introduce transformation since ideas involved generally applicable begin discussion abstract level gradually work way back boxconstrained quadratic programming situation 4 notation matrix symmetric matrix write 0 mean positive definite means positive semidefinite consider problem f continuous c closed connected region n consider constrained problem 21 replaced unconstrained minimization problem form min r continuous onto mapping restrictions mapping r make acceptable transfor mation see continuity enough consider following onedimensional example let obviously one local solution global solution x 1 however let ry continuous function mapping onto 0 1 strict local maximizer 1 easy see local minimizer fry x clearly local solution original problem following property plays key role answering question r open mapping ffl 0 pair see munkres 25 example discussion open mappings answer question theorem 1 c continuous onto mapping assume r open mapping local minimizer 22 local minimizer 21 ii x local minimizer 21 local minimizer 22 moreover exists least one iii problem 21 unbounded problem 22 unbounded proof assume local minimizer 22 let local minimizer exists ffl 0 23 exists hence x 2 n therefore x local minimizer 21 ii assume x local minimizer 21 r onto mapping therefore exists local minimizer exists ffl 0 continuity exists therefore hence local minimizer 22 iii suppose fy k g sequence lim alternatively assume fx k g 2 c lim r onto mapping therefore x k exists k ry k lim therefore iii established illustrate consider problem 0g definition r clearly satisfies open mapping property multiplication note r differentiable jacobian r j r nonsingular ry 2 intc intc interior c specifically r x g diagonal matrix diagr x f diagonal matrix diagy note r 2 r tensor term r x fr 2 r matrix diagonal case therefore definition r leads unconstrained twicedifferentiable minimization problem standard techniques used solve 22 unfortunately numerical experience approach mixed particular problems become large illconditioning neardegeneracy increases number iterations required standard minimization algorithms achieve good accuracy becomes quite large feel due part fact transformation causes increase complexity function minimized eg quadratic function becomes quartic objection approach largely numerical illconditioning original problem accentuated problem transformed complex form note also transformed problem may many local minimizers theorem 1 problem however along increase number local minimizers comes increase negative curvature may cause optimization algorithms difficulty event experience simple differentiable transformation satisfactory subject paper alternative definition r problem 25 consider vector jvj denotes vector whose components absolute values vector v clear open mapping property holds note r everywhere differentiable particular r differentiable point ry 2 intc ie 6 0 case obviously nonsingular using transformation fry piecewise differentiable nature function example fx quadratic function fry piecewise quadratic function extend absolute value approach handle general situation index either u finite u similarly index either l finite l simplicity assume finite values u equal unity finite values l equal zero simple translation scaling achieve form 5 transformation propose x ry diagonal transformation ie 5 definition reflective transformation applied directly general problem given 10 fig 2 1dimensional reflective transformation finite upper lower bound index x depends transformation induces piecewise linear reflective path x example u figure 2 l absolute value function l r illustrated figure 3 finally u four cases described formally figure 4 easy verify r satisfies requirements theorem 1 therefore use r introduce extraneous local minimizers restrict set local minimizers using reflective transformation problem 11 replaced unconstrained piecewise quadratic minimization 13 principle problem 13 solved descent direction algorithm eg algorithm 1 figure 5 advantage using piecewise linear transformation r linear aspect transformation differentiable point local complexity local complexity fx q piecewise quadratic function apparent disadvantage piecewise nature fy lack differentiability means conventional nonlinear minimization methods cannot used particular order guarantee convergence restrictions nature descent direction must imposed see suppose k close descent direction q k nearly perpendicular hyperplane usual descent condition r result small step since r q continuous x 10 describe two properties constraintcompatibility consistency help guarantee sufficiently long steps consequently global convergence discuss oe x fig 3 1dimensional reflective transformation infinite upper bound briefly section 3 straightline direction k corresponds piecewise linear path x piecewise linear path described follows simplicity without loss generality assume k define vector 6 component vector br k records positive distance form x k breakpoint corresponding variable x k direction x k piecewise linear reflective path defined algorithm 2 figure 6 since single outer iteration considered include subscript k variables description algorithm 2 dependence k assumed given current point x k descent direction x denote piecewise linear path defined algorithm 2 fi note possible describe algorithm 1 entirely xspace without explicitly introducing either function q variables algorithm figure 8 difference algorithm 1 algorithm 3 notational view presented algorithm 3 advantage original space visualization 6 purpose computing br assume following rules regarding arithmetic infinities finite scalar case 1 l evaluate x differentiate r obtain th diagonal element diagonal jacobian matrix else j r case 2 l evaluate x differentiate r obtain th diagonal element jacobian matrix case 3 l evaluate x differentiate r obtain th diagonal element jacobian matrix case 4 l case constraints x x fig 4 reflective transformation r algorithm 1 choose 1 determine descent direction qy k 2 perform approximate line minimization k respect ff determine acceptable stepsize ff k ff k correspond breakpoint 3 fig 5 descent dirn algorithm f algorithm 2 let fi u finite upper bound number segments path determined 1 let fi distance nearest breakpoint along 2 3 reflect get new dirn update br b j b fig 6 determine linear reflective path p reflective process natural advantage first view algorithm 1 algorithm straight line descent direction algorithm familiar structure probably useful reader keep views mind paper primarily work xspace algorithm 3 simplicity drop superscripts x eg x becomes well known descent direction algorithm demands sufficient decrease every step order achieve reasonable convergence properties use conditions suggested goldfarb 18 use unconstrained setting given descent direction k ff k satisfies approximate line search conditions fig 7 reflective path algorithm 3 choose 1 determine initial descent dirn x k q x k 2 intf determine piecewise linear reflective path p k ff via algorithm 2 2 perform approximate piecewise line minimization qx k p k ff respect ff determine acceptable stepsize ff k ff k correspond breakpoint 3 x fig 8 reflective path algorithm piecewise linear path defined 28 condition 29 interpreted restricting step length large relative decrease f condition 210 interpreted restricting step length relatively small basic reflective path algorithm problem 11 stated algorithm 4 allow flexibility especially regard newton step always require 29 210 satisfied instead demand either conditions satisfied 29 satisfied ff k bounded away zero eg latter conditions used allow liberal use newton steps weaken global convergence results note algorithm 4 generates strictly feasible points ie since x 1 2 intf follows x k 2 intf algorithm 4 ae positive scalar choose 1 determine initial descent dirn k q x k note piecewise linear path p k defined x 2 perform approximate piecewise line minimization qx k p k ff respect ff determine ff k ff k correspond breakpoint b condition 29 satisfied c either ff k satisfies condition 210 ii 3 x fig 9 reflective path algorithm satisfying line search conditions 3 algorithm specifics framework reflective newton approach presented previous section algorithm 4 section specify precisely search directions generated well mechanics line search specialized quadratic problem 11 convergence analysis given 10 uses two important properties sequence search directions constraintcompatibility consistency constraint compatability needed guarantee sufficiently long step taken first constraint encountered usual descent condition g sufficiently negative enough context reflective algorithm condition takes account proximity constraints consistency standard notion capturing idea firstorder descent represented term g sisitent firstorder convergence following 17 define 2 definition 2 sequence vectors fw k g constraintcompatible sequence fd gamma2 bounded definition 3 sequence vectors fw k g satisfies consistency condition central approach terms achieving quadratic convergence satisfaction constraintcompatibility consistency frequent use reduced trust region problem determine k subspace r n k positive diagonal scaling matrix matrix k defined j v jacobian 7 v v defined figure 9 matrix g v diagonal matrix component defined extended gradient extended deal possible degeneracy particular g small positive constant clearly x nondegenerate point g sufficiently small implies x degenerate diagonal matrix dx used 31 defined 17 ie 8 choice yields welldefined trust region problem 31 see note using 34 31 becomes diagonal matrix k positive definite neighbourhood nondegenerate point satisfying secondorder sufficiency condi tions moreover unlike fm k g f k g bounded matrix k featured performer reflective newton algorithm newton step defined k positive definite final remark choice scaling matrix 34 assume 1is reasonable choice see suppose consider 7 matrix j v diagonal matrix diagonal component equal zero unity example components u v finite j finite lower bound infinite upper bound viceversa strictly speaking v differentiable point define j v point note v discontinuous point v delta g continuous 8 notation z vector jzj 1 2 denotes vector th component equal jz calculation dmd involves division jvxj 1gamma2p includes components go zero x x hand approaches singularity x x consider v specify subspace k important realize cardinality implementation therefore cost solving 31 negligible given k subspace trust region problem 35 approached following way let k defined k independent columns nbyt k orthonormalization columns gamma1 vector k therefore problem 35 becomes ks k set solution 37 negligible cost matrices small see appendix algorithm 5 figure 10 presents secondorder reflective path algorithm algorithm 5 choose 1 2 determine initial descent dirn k q k positive definite ks n k k positive definite choose choose subspace k solve 31 get k 3 determine ff k x k otherwise perform approximate piecewise line minimization qx k respect ff determine ff k ff k breakpoint b ff k satisfies 29 210 4 x fig 10 secondorder reflective path algorithm note ff accepted line search corresponds breakpoint modify ff k breakpoint 9 matrix denotes space spanned columns remains precise determination k k fully specify line search begin k k algorithm 6 figure 11 describes procedure algorithm 6 let positive constants case 0 k positive definite ks n case 1 k positive definite ks n krs n solve 31 get k else set case 2 k positive definite compute w k unit vector fw k g constraintcompatible z solve 31 get k else solve 31 get k fig 11 determination descent direction k remark case 2 determine appropriate negative curvature direction following way sparse cholesky factorization k complete k positive definite unit direction nonpositive curvature readily available easily computable eg 17 algorithm 6 make use sufficient negative curvature displayed fw constraintcompatible constraintcompatibility test implemented introducing large constant cp requiring either condition 39 condition 310 satisfied must rejected case turn lanczos process 10 get unit vector w k 310 satisfied interesting note extensive numerical experimentation results reported section 4 conditions 39 310 always satisfied partial cholesky factorization method backup lanczos procedure never required line search designed specialized approximate line search procedure efficiently exploit structure problem guarantee line search conditions algorithm 5 describing approximate procedure develop exact line search procedure possible problem find local minimizer quadratic function along piecewise linear path end use exact line search per se rather use truncated version subroutine improve within overall approximate strategy begin exact search exact line search initially concerned exact determination ff k ff k local minimizer qx k ff note convenient describe exact line search terms yvariables ie ry view straightline minimization piecewise quadratic function henceforth section omit major iteration subscript function continuous piecewise quadratic function ray divided intervals left right qff smooth interval denote restriction qff j th interval q j ff note q j ff quadratic function single variable exact line search algorithm visits intervals 1 2 lefttoright fashion attempt locate first local minimizer assume located local minimizer intervals 1 2 j gamma1 assume q j two possibilities either q j ff minimizer strictly within interval j ie ff j however admit minimizer within j must consider possibility fi j minimizer ff case q j1 inti j q j1 process repeated interval j1 algorithm 7 figure 12 presents compact description exact line search algorithm sketched step 12 algorithm 7 follows observation ff k direction infinite descent q beginning therefore theorem descent 11 algorithm 7 exact line search along direction beginning point 0 determine array breakpoints br according 27 fi 0 0 1 minimizer q k exists otherwise set ff k 1 problem 11 unbounded exit exit index according algorithm 2 fig 12 exact line search algorithm final concern regard exact line search efficient implementation step 11 theory computation straightforward assume q k k k unbounded k q k ff unbounded unless k case q k constant however challenge determine k 1 k efficiently note k 0 key efficiency observation reflective transformation r linear interval constant within interval interval k define oe k vector diagonal elements j r evaluated point interval follows follows therefore terminology used k k straightforward implementation determining k breakpoint k 0 needed however considerable structure exploited particular oe k1 vector component equal sigma1 differs oe k exactly one component exploit reduce work line search per breakpoint suppose determined k contain ff k hand following quantities k k x k value x k th breakpoint j index vector w k updated follows h j column j h coefficient k1 2 simply computed k1 coefficient k1 1 efficiently computed considering following equalities k1 finally x k1 computed summary coefficients k1 2 intermediate quantities x computed given k using 316317318 319 amounts approximately 4n work course initial quantities w 0must computed scratch requiring 2 work therefore k br denotes number breakpoints crossed total cost exact line search initialization w ii determination br iii ok br n steps 10 15 approximate line search exact line search described practical two reasons first exact minimizer along line may correspond exactly breakpoint ie boundary point algorithm requires strictly feasible points actually serious problem since small perturbation would yield strict feasibility reflective newton method sensitive boundary proximity serious objection exact line search economy despite efficient implementation described previous section relative cost high number breakpoints crossed k br large certainly large number tight variables solution say something close n total cost exact line search algorithm ultimately becomes 2 per line search unsatisfactory unnecessary since economical approximate line search effective section describe approximate line search henceforth refer subroutine improve uses exact line search described limited fashion beginning approximate minimizer subject bound k u number breakpoints permitted cross particular improve used cleanup role determining initial approximate minimizer bisection strategy improve called upon apply exact line search strategy thereby approximate minimizer improved cost ok u n k u typically chosen small eg k improve also impose approximate upper bound ff max size ff size improvement bounded ff subroutine improve following calling sequence precise description approximate line search algorithm given figure 13 algorithm 8 basic idea follows first direction k newton direction n k unit step attempted 29 satisfied full newton step accepted subject improvement subroutine improve possible slight adjustment avoid breakpoint second k corrspond newton direction unit step satisfy 29 bisection procedure executed interval 0 ff u ff u 1 upper bound step size point located satisfying 29 210 possibly improved subroutine improve algorithm 8 approximate line search along direction beginning point k unit step along k satisfies 29 ff k corresponds breakpoint set ff breakpoint else f k 6 n k unit step along n k satisfy 29g ffl use bisection find 210 ff k breakpoint set ff ff k corresponds breakpoint set ff breakpoint fig 13 approximate line search algorithm 4 numerical experiments implemented algorithm version matlab allows sparse matrix data structures 15 matlab 40 section present preliminary numerical results exception results reported table 12 experiments performed sun sparc workstations matlab environment 22 experiments reported table 12 performed heterogeneous environment involving intel ipsc860 32node multiprocessor backend sun sparc workstation frontend matrix factorizations solves performed backend c main matlab program executed frontend communication frontend frontend ethernet implemented use matlab mex files used environment facilitate solving large problems details heterogeneous environment given 3 starting stopping experiments reported paper starting value x ie x 1 follows component j upper lower bounds finite choose midpoint j upper lower bound corresponding component j infinite size choose choose note reflective newton approach particularly sensitive starting value example repeated many experiments reported using random strictly feasible starting point little difference behaviour detected choosing robust stopping rule optimization easy primary stopping rule based relative difference function value reasonable partly strict feasibility always maintained partly often real objective practical optimization achieve point relatively low function value specifically primary stopping rule choose matlab sun sparc workstation secondary stopping criteria well designed determine progress deemed slow secondary rule tends kick solving degenerate illconditioned problems flat region around solution entered parameter settings number parameters algorithm either large small category settings used experiments used determination scaling matrix see 33 used line search see 29 oe l 1 used line search see 210 oe bound number breakpoints crossed subroutine improve ffl ae lower bound stepsize see algorithm 8 line search produces unit step turns breakpoint point perturbed amount bounded ff kd k g k k see 38 used test constraintcompatibility see 310 used negative curvature test see 39 ffl used algorithm used algorithm 8 upper bound bisection process used algorithm 8 ff 41 positive definite problems generated number quadratic test problems certain properties first set results concentrate case h symmetric positive definite results reported use sparse matrices h sparsity patterns representing 3dimensional grid using 7 point difference scheme moretoraldo 24 qpgenerator adapted generate problems given sparsity pattern see also 6 review generator characteristics generator straightforward adaptation moretoraldo scheme sparse setting use several sparse matlab functions eg sprandsym sprand tables 13 dimension test problems case parameter pctbnd indicates percentage variables tight solution approximately evenly divided upper lower bounds parameter deg reflects degree solution nearly degenerate larger value deg greater amount near degeneracy technical details deg discussed 6 parameter cond reflects conditioning matrix h condition number h approximately 10 cond upper lower bound vectors u l generated follows approximately 75 components l chosen finite assigned value zero index assignment made random fashion similarly approximately 75 components u chosen finite assigned value unity index assignment made random fashion independent assignment l row tables 13 reflects results 10 independent runs parameter settings third column labelled max indicates maximum number iterations required set 10 independent runs achieve stopping criteria fourth column labelled avg records average number iterations required reach stopping criteria 10 problems last column labelled acc records number digits accuracy achieved function value true solution known table positive deg cond max avg acc 9 6 15 15 15 9 9 observations tables 13 first observe remarkable consistency reflective newton method problems terms iterations required achieve stopping criteria accuracy attained function value apparently little sensitivity degeneracy conditioning number variables tight solution course claim accuracy x independent conditiondegeneracy surely however usually acceptable optimization attain point nearly optimal function value quite successful test collection second absolute number iterations required obtain accurate solution terms function value q modest every case ie less 20 positive deg cond max avg acc 9 6 17 17 15 6 9 17 17 15 9 9 17 163 15 table positive deg cond max avg acc 9 3 17 163 15 9 6 positive definite problems timing breakdown 1000 encouraging considering dimension problems n 1000 spectrum problem characteristics considered important know algorithm spends time end generated larger problems structure broken timing information table 4 consider representative positive definite problem av erage characteristics ie vary problem dimension n sparsity structure remains second column labelled records number iterations required achieve stopping criteria totm records total number flops used partial cholesky factorization represents million totls records number flops used approximate line search algorithm 95 total flop count problems represented sum totm totls columns remaining work algorithm 2dimensional trust region solution negligible comparison see appendix detail solution trust region problems observations table 4 first significant growth number iterations problem dimension n increases high accuracy maintained larger values n well n increases sparse matrix factorization work totm increases relative lines search cost totls therefore speedup partial sparse factorization aspect algorithm eg use parallelism exploitation specific particular structure significant impact overall computing time conversely improving approximate line search terms cost crucial computing issue point largescale problems addition randomly generated structured positive definite problems experimented three specific test cases two problems literature eg 12 24 third example new tables 5 6 report obstacle problem first case lower bounds second case lower upper bounds defining specific example used chosen parameter settings specific functions used 24 table 7 reports elasticplastic torsion problem used parameters reported 24 define problem table 8 report linear spline approximation problem type problem arises example particle method approach turbulent combustion simulation 28 problem results large sparse leastsquares problem subject nonnegativity constraints variables set sample problem assume mbymbym 3dimensional grid within cell set particles randomly located use approximately 10 particles per cell experiments particle p known function value oep associate grid intersection point linear basis function determine best set coefficients x basis functions leastsquares sense subject nonnegativity constraints x function oe used experiments defined given point 3space table obstacle problem lower bounds norm 50 2500 14 13 100 10000 15 12 table obstacle problem lower upper bounds norm 50 2500 14 12 100 table elasticplastic torsion problem norm 100 10000 11 12 observations tables 58 noteworthy observation apparent insensitivity method problem size problems number linear spline approximation norm 22 10648 17 11 iterations grow given problem class dimension problem increases example linear spline problem 16 iterations required iterations required moreover number iterations always modest test set ie less 20 high accuracy achieved cases 42 indefinite problems adapted moretoraldo qp generation scheme combination sparse matrix functions matlab 40 generate large sparse indefinite matrices given sparsity pattern given approximate set approximate eigenvalues indefinite case chose finite upper lower bound vectors 1 avoid generation unbounded problems problems tables 912 roughly 10 eigenvalues h neg ative column labels however acc represent number accurate digits compared true solution since true solution unknown due indefiniteness h instead acc records number matching digits objective function q last 2 iterations case optimality conditions verified hold final point table indefinite problems deg cond max avg acc 9 3 23 193 15 9 6 26 217 15 9 9 observations tables 911 iteration counts indicate method quite consistent efficient indefinite problems compared performance positive definite problems still overall efficiency seems good average number indefinite problems deg cond max avg acc 9 3 9 6 19 177 15 3 9 14 113 15 9 9 25 183 15 table indefinite problems deg cond max avg acc 9 3 9 6 6 9 15 13 iterations required problem category always less 23 table 12 indicate algorithm spends time indefinite problems considering representative example increasing dimension table indefinite problems timing breakdown remark table 12 see apparent growth required iterations n increases clearly totm column dominates totls column n increases recall totm represents matrix factorization flop count totls represents number total flops required line search procedure therefore obtain improvements efficiency type approach best focus matrix factorization aspect overall procedure 5 theory conclusions numerical results obtained date strongly support notion reflective newton method represents efficient way accurately locate local minimizers largescale indefinite quadratic functions subject bounds variables theory supporting also reflective newton method globally quadratically convergent coleman li 10 present important theoretical properties reflective newton methods general nonlinear functions subject bounds variables method paper specialization general method quadratic case therefore general theory applies make compactness assumption formally stating main result compactness assumption given initial point x 1 2 f assumed level set theorem 2 let fx k g generated algorithm 5 fs k g generated algorithm 6 fff k g determined approximate line search algorithm algorithm 7 ffl every limit point fx k g firstorder point ae maximum spectral radius x g since ae continuous l compact set upper bound ae exists recall 3 constant used algorithm 6 ffl every nondegenerate limit point satisfies secondorder necessary conditions ffl nondegenerate limit point x satisfies secondorder sufficiency conditions sufficiently small fx k g convergent x convergence rate quadratic ie proof algorithm class algorithm described 10 assumptions theorem 20 10 satisfied result follows theorem 20 10 conclusion strong theoretical computational results indicate reflective newton method efficient reliable way solve problem 11 high accuracy computational results reported paper support claim 6 acknowledgement thank colleagues jianguo liu danny ralph many helpful discussions work danny ralph drew attention topology reference 25 7 appendix trust region problem trust region problem real symmetric matrix k delta k denotes 2norm purpose section review nature problem 71 discuss possible solution suitable lowdimensional problems context reflective newton method problem 11 matrix mx symmetric matix order 2 computational cost procedure describe solve 71 negligible compared required computations reflective newton algorithm propose larger problems approximate procedure usually preferred eg 14 23 30 31 much material section found elsewhere eg 2 4 11 14 23 29 30 31 diagonalization begin extremely useful characterization global solution 71 theorem 3 vector solves 71 exists scalar 0 positive semidefinite c ksk delta proof proof given example 30 usefulness result best revealed diagonalization suppose columns v orthonormal eigenvectors obviously equivalent b gamma 1 vectors satisfying b form vector fi arbitrary respect ab sometimes help respect satisfying cd basis algorithmic approach problem assume form given 74 strive satisfy cd choosing cases fi appropriately fi plays role value solution situation cd satisfied easily dispensed first half case 1 therefore primary task assuming form 74 determine sometimes fi satisfy divide approach three possibilities g case 1 case either newton step within sphere ksk 2 delta ka optimal solution case 2 figure obviously ksk 1 gamma 1 1 moreover ksk convex therefore ksk intersects delta exactly one place gamma 1 case 3 ks 1 gamma 1 k finite consider figures 34 two possibilities ks 1 gamma 1 solution 75 right gamma 1 chosen ensure 75 note jij 1 null space component may unique reciprocal secular equation theory build algorithm remarks given however better numerically replace condition 75 gammaksk 0 equation 76 linear shape equation 75 therefore equation 76 amenable solution via newtons method considering definition rsec n easy verify following 1 rsec convex gamma 1 1 lim 1 2 lim gamma rsec finite 3 ff 6 0 2 rsecgamma obviously single zero rsec exits right gamma 1 case 4 8i 2 ff algorithmic numerical concerns assume solution 71 sought willing able compute full eigenvalue information case perhaps due cost possible approximately solve 76 using iterative scheme involving cholesky factorization 14 method using appears straightforward case 1 newton step gammaa gamma1 g solution determine zero rsec 0 case 2 rsec admits single solution right gamma 1 case 3 ks 1 gamma 1 k delta zero rsec right gamma 1 otherwise solution 71 given 74 ks unfortunately situation quite clean numerical point view fuzziness second third cases particular ff 1 small equation illconditioned near gamma 1 quite difficult nigh impossible compute rsec small extreme illconditioning due following disagreement assume f1g simplicity note ff hand general limiting value 78 therefore nearby problems ff yield different solutions 76 cause illconditioning 76 solution illconditioning problem trustm first attempt find solution however jrsec j small computed zero returned zerofinder set solution 71 via 74 strategy works solution 76 ff small 2 close solution 71 corresponding ff zero see initially assume first consider case ff solution 71 otherwise result obvious define solution 71 written next consider ff small number write solution 71 implies therefore solution 71 ff near solution 71 ff general jij 1 solution 71 components ff near zero near solution 71 components set zero case several coefficients ff equal zero 2 problem 71 enjoy unique solution see 74 range space component unique r approximate solution trust region problem minimization twodimensional subspaces advanced computing research institute computing trust region step penalty function direct active set algorithm large sparse quadratic programs simple bounds quadraticallyconvergent algorithm linear programming problem lower upper bounds global convergence class trust region algorithms optimization simple bounds minimization quadratic functions subject box constraints minimization quadratic function many variables subject lower upper bounds computing optimal locally constrained steps sparse matrices matlab design implemen tation minimization subject bounds variables curvilinear path steplength algorithms minimization algorithms use directions negative curvature globally convergent method l p problems solving minimal least squares problem subject bounds variables generalized conjugate gradient algorithm solving class quadratic programming problems application velocitydissipation pdf model inhomogeneous turbulent flows family trustregionbased algorithms unconstrained minimization strong global convergence properties trust region methods unconstrained optimization conjugate gradient methods trust regions large scale optimization class methods solving large convex quadratic programs subject box constraints tr ctr kamin whitehouse david culler calibration parameter estimation sensor networks proceedings 1st acm international workshop wireless sensor networks applications september 2828 2002 atlanta georgia usa kamin whitehouse david culler macrocalibration sensoractuator networks mobile networks applications v8 n4 p463472 august c jamrog r tapia zhang comparison two sets firstorder conditions bases interiorpoint newton methods optimization simple bounds journal optimization theory applications v113 n1 p2140 april 2002 keiji yanai nikhil v shirahatti prasad gabbur kobus barnard evaluation strategies image understanding retrieval proceedings 7th acm sigmm international workshop multimedia information retrieval november 1011 2005 hilton singapore v g domrachev poleshuk regression model fuzzy initial data automation remote control v64 n11 p17151723 november