interdependence routing data compression multihop sensor networks consider problem broadcast communication multihop sensor network samples random field collected node network goal nodes obtain estimate entire field within prescribed distortion value main idea explore paper jointly compressing data generated different nodes information travels multiple hops eliminate correlations representation sampled field main contributions obtain using simple network flow concepts conditions ratedistortion function random field guarantee node obtain measurements collected every node network quantized within prescribed distortion value b construct large class physicallymotivated stochastic models sensor data able prove joint ratedistortion function data generated whole network grows slower bounds found truly novel aspect work tight coupling routing source coding explicitly formulated simple analytically tractable modelto best knowledge connection studied b appendix b minimum amount bits transported network solve transmission problem interested c contributions paper organization main ideas paper 1 dene model sensor data derive scaling law versus combining routing datacompression prevent congestion highly populated sensor networks main results proof existence routing algorithms source codes require bits transmissions joint rate distortion function samples eld would prove even decentralization constraints classical source codes still achieve optimal compression efciency furthermore attaining optimal performance requires number transmissions sublinear number nodes network b proof mild regularity conditions random eld average distortion per sample kept constant eld generates bounded amount information independently size total distortion kept constant growth logarithmic well total transport capacity network grows like argued section ia agreement result 9 paper shows performing routing data compression combined fashion prevent congestion however several questions still remain answered optimal strategy allows visit nodes minimize effective dataow optimum tradeoff routing delay trafc questions though practically important outside scope intended paper rest paper organized follows section iv compute bounds transport capacity network use bounds impose constraints total trafc ow network section v propose model generation sensor data based prove indeed amount data generated network well network capacity numerical examples concluding remarks presented section vii viii respectively ii independent encoders provide sought conditions rst thing need nd much trafc network generates clearly question depends statistics sensed data quantizationcompression technique used standard communication networks support transmission analog sources voice images video every information source independently encodes compresses data routed high speed core network destination see fig2 networking issues physical aspects information routing quantization compression fig 2 traditional network setting processing naturally kept separated traditional network structure fig 2 obvious ask would bits generated sensors increase network size sensors operate independent encoders following traditional structure discussed illustration purposes consider simple example suppose uniform range node uses scalar quantizer bits resolution ie quantization step distortion measured meansquare sense ie well known result basic quantization theory states particular source average distortion achieved quantizer also called operational distortionrate function 6 total distortion entire vector samples hence solving derive maintain total distortion entire network sample requires bits result total amount trafc generated whole network scales like network size interestingly even using optimal vector quantizers node compression node samples performed without taking consideration statistics sensors measurements scaling law still words one could certainly reduce number bits generated xed distortion level reduction would affect constants hidden bigoh notation scaling behavior network trafc would remain unchanged 3 examples analyses type found 14 sec 3 19 sec 7 fact even node utilizes dimesional vector quantizer compress optimally sequences subsequent samples say high resolution methods 6 show operational distortion rate function would differential entropy ie joint density samples collected node source specic constant hence node distortion rate function respect general source factor replacing example discussed dependence still exponential term determined much data particular coding strategy independent quantizers node produces need know network enough capacity transport data independent encoders answer 9 see consider partition network shown fig 3 fig 3 nodes spread uniformly unit square large take differential volume size small high probability number nodes differential volume number nodes strip shown gure since total number nodes must total area unit square product areas two strips shown gure one horizontal one vertical hence must number nodes strip shown sensor broadcast problem interest work nodes network must receive information measurements collected nodes result trafc generated left portion network must carried right trafc generated right portion network must carried left according calculation nodes within strip marked fig 3 must share load moving bits across network cut since links present stripe capacity capacity cut cannot larger maxowmincut theorem 4 ch 27 know value ow network upper bounded capacity network cut therefore total transport capacity network cannot higher see problem bits must go across cut capacity even optimal vector quantization strategies cannot compress data enough network carry itthe network encoder decoder fig 4 coding side information scale up1 correlated samples scaling analysis independent encoders presented ignores one fundamental aspect increasing correlations sensor data density nodes increases indeed data highly correlated sensors observe essentially value least intuitively seems clear almost exchange information needed node know values knowledge local sample global statistics already provide fair amount information remote samples naturally raises question coding strategies compress sensor data enough iii dependent encoders 13 20 14 rst introduced idea coding sensors data exploiting correlation among samples prevent network congestion specically 13 20 14 authors proposed compress separately correlated samples node mean distributed source coding techniques section rst discuss distributed source coding introduce novel approach propose consists combining multihop routing data compression distributed source coding idea distributed source coding rst introduced slepian wolf 18 quantied number bits necessary encode source receiver side information source see fig4 sensor network broadcast problem measurements sensors side information receiver available 13 20 14 hence node quantize data considering side information samples also utilized decoder 14 scenario described fig iiia shown one reduce amount bits per node per square meter result 14 provides rst theoretical evidence coding techniques exploit dependence among sensors samples key counteract vanishing throughput multihop networks fact even transport capacity per node per square meter vanishing number nonredundant bits node generates furthermore latter vanishing faster rate throughput techniques based ows cuts analyze information theoretic capacity problems networks proposed 1 5 ch 1410 context techniques provide alternative interpretation guptakumar results 9 correlated n nodes sensors multihop communication network n nodes central control fig 5 sensor network setting 14 even though multihop sensor networks appear killer application distributed source coding several reasons approach 13 20 14 improved 1 far theoretical evidence congestion avoided distributed source coding limited restrictive setting fact bounds 14 derived situation described fig 1 sensors one dimensional space relays two dimensional area suggests nodes physically separated even exactly ratio 2 approach distributed source coding requires complex encoders achieve signicant compression gains example proof developed 14 involves use codes problem ratedistortion side information 5 ch 149 efcient codewords nearly uniformly distributed true highly correlated data vector large sizes highdimensional vector quantizers practical shortblock settings gains obtained general less signicant last least true encoding performed without sharing one single bit data among nodes however real need impose constraint separation sourcecoding routing different layers communication system architecture reect physical separation functionalities multihop sensor network setting fig 1 trademark multihop networks power efcient transmission achieved data travel several intermediate closeby nodes reaching nal destination hence point view engineer engaged design practical sensor network fact creates opportunity using simpler compression techniques cannot missed fact neighboring nodes jointly compressing data queues forwarding remotely network dense eld smooth drastically reduce number bits per sample transmitting even greater precision accomplish gain nodes use variety techniques used compress sequences need resorting highly complex distributed source coding techniques truly novel interesting aspect paper combination classical source coding methods routing algorithms best knowledge explored authors b routing source coding scheme propose based idea described fig idea use classical source codes samples travel reencoding jointly data queues hop around network removing bits information redundant general th node transmit th sensor set samples th th node already received set encoder th node need pass encoder th node number fig 6 routing source coding bits least said effectively designing network ow using every data ow rateditortion function lower bound becomes tedious requires dening level distortion allowed every set data exchanged nontrivial function total distortion allowed much simpler approach refer joint entropy samples discretized quantizing nely every node quantizer cell entropy induced codewords quantization source denoted shortest codeword length represents uniquely set discrete data equal data entropy see appendix b determine number bits transferred every ow using joint entropy data ought transmitted chosen satisfy global distortion constraint obviously approach suboptimal entropy quantized data larger ratedistortion function since ratedisortion function denition lower bound number bits necessary represent data however see section vi worst case scenario case gaussian samples entropy grows scale ratedistortion function respect assume quantized samples entropycoded using optimum vector quantizer every ow transmitting data using shortest codeword length see appendix b equal data entropy practice entropy coding viable solution variety alternatives used sequence compression available fact contrary complicated vector quantizers required distributed source coding routing source coding combined processing node done using standard compression technique used normally compress sequences analog sources predictive encoding transformcoding jpeg etc difculty applying techniques resides difculty implementing algorithms distributed fashion interesting subject dealt elsewhere 15 16 section iv provide example possible network ow mean optimized yet allows us set study case establish formally condition trafc generated transported network condition satised data joint entropy suboptimum approach joint ratedistortion ideal optimum quantization takes place section v derive asymptotic rate distortion function data prove large class sensors data models condition found satised limit iv transport capacity using simple network ow concepts section ia argued upper bound transport capacity network size goal section construct one particular ow amount data ow needs push across network upper bound capacity network derive constraint amount data source generate broadcast whole network network flow case regular grid consider rst case regular grid naturally precedes construction general random grid anticipated construct conditions network ow using entropies quantized data start case four nodes generalize entire network recursive algorithm two examples among many possibilities transmission schemes shown fig 7 1 1 b 21 b 21 312 c 12 f 123 c 12 c 12 3 b 43 fig 7 consider network 4 nodes observes variable joint entropy two possible ways scheduling transmissions shown notation used gure rst transmission b second c third two transmissions letter label performed parallel means sample node encoded knowledge sample node available using chain rules entropies see transmission schedule left gure generate total trafc takes 8 time slots complete schedule right gure generate trafc bits require 4 time slots complete transmissions see examples fig 7 expense increased transmission delays communicate samples nodes generating amount trafc essentially one node collected samples encoded jointly information broadcast nodes alternatively sacricing compression efciency also possible incur lower transmission delays inherent tradeoff bandwidth use decoding delay two quantities linked together routing strategy employed entire network construct ow recursively simplicity assume integer trivial case network size nodes ie know value samples ie without transfer information consider partition nodes 4 groups containing nodes upperleft corner size nodes upperright corner lowerleft lowerright denotes set variables observed nodes set partition illustrated fig 8 ul ur fig 8 partition network size four subnets subnets size recursion get nodes within subnet know values quantized variables reduced problem problem four nodes considered fig 7 know exchanging total bits total 8 transmissions across cuts plus transmissions spread data within cuts enough ensure every node network size knows every quantized value construction illustrated fig 9 fig 9 since node boundary cut knowledge samples within subnet one encode samples jointly send th data across cut transmissions pieces spread throughout subnet reach nodes idealized system data quantized optimally without error propagation minimum number bits necessary represent network samples still must generate least total bits trafc still requires transmissions complete broadcast b network flow case random grid next address general case sensors located random positions case difference case regular grid considered fact random case nonzero probability recursive denition ow may encounter empty subnet case trivial modications take care problem basic idea divide network squares area probability tends 1 uniformly squares number points falling square 12 ch 2 therefore almost networks points problem random grid trivially reduced problem regular grid discussed plus local ood within square involving nodes details capacity random networks presented 11 c constraints network stability know following facts since bits must go across 4way cut dened capacity 4way cut using chainrule showed capable broadcasting data transferring minimum amount bits necessary represent data appendix b therefore minimum requirement need satisfy going show following sections requirement satised probability one wide class data models introduced section v v model sensor section iv saw appropriate routing reencoding along routes compress data generated entire network goal section verify reasonable models sensor data eqn 5 satised broadcast problem effectively solved able talk ratedistortion function data generated entire network need model data main idea would like capture models sensor data data corresponds measurements random process kind regularity conditions measurements become increasingly correlated density nodes becomes large propose end fairly general class models two assumptions data gaussian random variables b correlation among samples arbitrary spatially homogeneous function c let number nodes network grow correlation matrix converges smooth twodimensional function assumption worse case scenario far compression concerned consequence theorem 4 appendix b spatial stationarity even though totally general technical assumption common many statistical analysis captures well local properties random processes source model section establishes basic model upon base asymptotic analysis let denote random vector samples collected sensors time rst assumption ci ie spatially correlated random gaussian vector samples temporally uncorrelated samples temporally independent power spectrum bandlimited data sampled nyquist rate case restrictive assumption gains terms compression could obtained exploiting temporal dependence samples temporal independence focus one vector samples drop time index mean square error mse distortion measure constraint calculate assumption ci ratedistortion function network using reverse waterlling result 5 indicating ordered eigenvalues ratedistortion function otherwise exists therefore ratedistortion function function eigenvalues formed samples continuous multivariate function represents correlation samples taken two arbitrary points network eigenvectors entries satisfy following equation b asymptotic eigenvalues distribution derivations following try capture fact process cannot innite spatial degrees freedom asymptotic ratedistortion function obtained using following two basic steps 1 prove eigenvalues correlation matrix tend eigenvalues continuous integral equation provide model kernel continuous integral equation 13 bandlimited spatial frequencies allows us obtain asymptotic rate distortion bound said rst step rewrite 12 quadrature formula approximate integral equation 13 general integral quadrature coefcients approximation 13 holds since want explore convergence eigenvalues 12 set case rst side 14 equivalent right side 12 normalized therefore 14 valid left sides 12 normalized approximately equal left side 13 leads following approximation error approximation 14 determines error 15 two errors related following theorem derived 10 sec 54 lemma 1 denoting arbitrary eigenvalue 13 corresponding normalized eigenvector sufciently large exist eigenvalue denotes quadrature error ie assuming cii continuous grid quadrature error 16 implies condition cii lemma 1 operational condition distribution sensor nodes nodes distributed way quadrature error vanishes another interesting intuitively obvious consequence lemma 1 cii summarized following corollary easily proved using bounds lemma 1 triangular inequality corollary 1 eigenvalues corresponding two different grids hence cii satised grids eigenvalues asymptotically corollary 1 implies rely grid asymptotic behavior example regular lattice extrapolate asymptotic behavior eigenvalues latter c tractable case assume ciii correlation points 11 spatially homogeneous consequent structure regular grid also known doubly toeplitz ie blocks toeplitz assumption ciii implies useful aspect model empirical distribution eigenvalues sumptions 2d fourier spectrum 18 see next assumption ciii dene block toeplitz matrix converges mild adopting regular grid covering square area spacing likewise szegos theorem 7 establishes asymptotically eigenvalues toeplitz matrix converge spectrum correlation function essentially szegos theorem establishes eigenfunctions homogeneous kernel tend fourier basis complex exponentials result generalized two dimensional case matrix doubly toeplitz ie proceeding nal modelling assumption civ bandlimited respect bandwidth ie civ capture notion limit covariance function varies smoothly space rate distortion function asymptotic ratedistortion function obtained replacing summations 9 10 integrals eigenvalues become asymptotically continuous function 7 correspond points crosses threshold ie let us denote sets points ie let us also indicate set ratedistortion function civ areas smaller thus following lower bound also illustrated fig 10 fig 10 one dimensional illustration lower bound eqn 27 obtained bounding integrals dene eqn 25 together civ justies following upperbound ratedistortion function see total ratedistortion function entire network average distortion per sample kept xed total amount trafc generated network upper bounded constant irrespective network size alternatively keep total distortion xed considering increasingly large let average distortion even though rate distortion function asymptotic bound result signicant observe amount trafc generated entire network grows logarithmically well capacity bound proved eqn 5 vi quantization compression section iv constructed algorithm network ow based joint entropy samples discretized quantizing every nodes data les compressed using universal source coding algorithms huffman coding simpler suboptimal alternatives like lempelziv coding previously section iiib argued approach suboptimum growth entropy behavior ratedistortion function prove next highresolution analysis shows individually quantized uniform quantizer cell size joint entropy 6 joint differential entropy spatial samples whose denition analogous 4 gaussian dimensional multivariate process full rank covariance matrix determinant covariance matrix conditions ci civ shown becomes singular increasingly large nullspace dimensional gaussian process singular covariance matrix rank joint density samples expressed product densities auxiliary set independent gaussian random variables variance equal nonzero eigenvalues also called principal components whose number equal set dirac functions consequently denote product nonzero eigenvalues rank joint entropy gaussian multivariate density general written difcult prove high resolution approximation general case determine size agreement high resolution analysis consider quantization error nearly independent signal sample sample pessimistic assumption uniform distribution 6 thus total distortion order cell size therefore arguments similar used prove 28 show civ hence 32 words also grows logarithmically simple compression strategy downsampling routing simpleminded compression strategy nodes implement downsample appropriately sensor measurements spread network simple strategy allows us reach approximations conclusion asymptotic analysis fact even though spatially bandlimited process requires innite samples correctly reconstructed interpolation nyquist theorem indicates condition cii met sample eld frequency axis respectively network area equal one even oversample reduce interpolation error borders need samples fact nyquist criterion strictly requires samples per unit area border effects due fact area limited quantization errors propagate missing data interpolated reduced oversampling however important conclusion number samples grow hand constrain total distortion error average distortion per sample let us assume variance sample interpolated samples distortion greater equal distortion non interpolated ones implies non interpolated samples quantized rate least therefore total amount trafc produced network order qed vii numerical examples section provide numerical evidence validates asymptotic claims rst numerical example aimed corroborating corollary 1 assumed area network normalized one function dened 18 easily veried condition cii met g 11 show samples obtained regular grid g 12 show eigenvalues matrix whose entries one case random grid red line case regular lattice blue line observe nearly identical cases support non zero eigenvalues grow values increase proportionally finally g0505 fig 11 samples dened 38 obtained regular grid sensors300evdr150500 1 2 3 regular grid random grid eigenvalue index fig 12 eigenvalues various values 13 covariance model 38 total distortion show ratedistortion function calculated numerically using inverse waterlling 6 expected growth clearly logarithmic viii conclusions recent work transport capacity largescale wireless networks established pernode throughput networks vanishes number nodes becomes large result poses serious challenge design networks even argued large networks feasible precisely reason 9 previous work however pointed context sensor networks amount information fig 13 rate distortion function versus number nodes network generated node constant instead decays density sensing nodes increasesthis illustrated example based transmission samples brownian process arbitrarily low distortion even vanishing pernode throughput means using distributed source coding techniques 14 work shown alternative approach work around vanishing pernode throughput constraints 9 new approach based distributed coding techniques instead based use classical source codes combined suitable routing algorithms reencoding data intermediate relay nodes best knowledge rst results interdependencies routing data compression problems captured system model also analytically tractable key enabling step derivation construction family spatial processes satisfying fairly mild easily justiable physical point view regularity conditions able show amount data generated sensor network well transport capacity provides evidence largescale multihop sensor networks perfectly feasible even network model considered 9 entropy coding entropy random variable probability mass function dened multivariate random variables generalization straightforward note also entropy vector decomposed according called chain rule resulting iterated application chain rule probability used figure 7 importance denition entropy lies fact provides accurate answer following question regarding discrete data sources ie sources producing symbols drawn form discrete minimum number bits necessary represent data discrete source reconstructed without distortion answer given following theorem 5 ch5 theorem 1 expected length instantaneous dary code random variable proof theorem based existence coding technique huffman coding known achieve entropy within one bit one symbol encoded multiple symbols encoded together efciency huffman coding tends 100 5 theory cannot directly generalized handle case analog sources entropy innite even signal discretetime sequence fact source signal take real value sample still requires innite precision represented exactly however certain level distortion accepted minimum number bits necessary represent source calculated rigorously case discrete sources resorting parallel theory analog sources called rate distortion theory ii rate distortion theory nutshell rate distortion based seminal contribution claude shannon tried provided theoretical framework representation continuous source discrete symbols 17 suppose memoryless continuous source produces random sample density quantization problem boils representing discrete values measure distortion mapping quantify many bits needed represent 1959 paper shannon dened called rate distortion function average mutual information paper proved following two fundamental theorems theorem 2 minimum information rate necessary represent output discretetime continuous amplitude memoryless gaussian source variance based meansquare distortion measure per symbol single letter distortion measure theorem 3 exists encoding scheme maps source output code words given distortion minimum rate bits per symbol sample sufcient reconstruct source output average distortion arbitrarily close implication two theorems minimum number bits represent within prescribed meansquare error source gaussian words discrete code represents gaussian source meanquare distortion lower bound asymptotically achievable following important theorem proven berger 1971 2 generalizes results shannon theorem 4 ratedistortion function memoryless continuous amplitude source zero mean nite variance respect meansquareerror distortion measure upperbounded bergers theorem implies gaussian source one requires maximum encoding rate distortion function mse hence distortion metric meansquare error case gaussian source seen worse case scenario theorems extended multivariate sources analogous ones consider paper particular called inverse waterlling result used section v direct generalization theorem 2 multivariate gaussian source r network information flow rate distortion theory sphere packings introduction algorithms elements information theory vector quantization signal compression critical power asymptotic connectivity wireless networks capacity wireless networks maximum stable throughput problem random networks convergence stochastic processes distributed source coding symmetric rates applications sensor networks lattice quantization side information distributed signal processing algorithms sensor broadcast problem sensing lena coding theorems discrete source fidelity criterion institute radio engineers noiseless coding correlated information sources multiple description vector quantization lattice codebooks design analysis optimal code design lossless near lossless source coding multiple access networks tr introduction algorithms vector quantization signal compression elements information theory distributed source coding lattice quantization side information optimal code design lossless near lossless source coding multiple access networks ctr animesh kumar prakash ishwar kannan ramchandran distributed sampling smooth nonbandlimited fields proceedings third international symposium information processing sensor networks april 2627 2004 berkeley california usa tamer elbatt scalability hierarchical cooperation dense sensor networks proceedings third international symposium information processing sensor networks april 2627 2004 berkeley california usa bharath ananthasubramaniam upamanyu madhow virtual radar imaging sensor networks proceedings third international symposium information processing sensor networks april 2627 2004 berkeley california usa sundeep pattem bhaskar krishnamachari ramesh govindan impact spatial correlation routing compression wireless sensor networks proceedings third international symposium information processing sensor networks april 2627 2004 berkeley california usa yaron rachlin rohit negi pradeep khosla sensing capacity discrete sensor network applications proceedings 4th international symposium information processing sensor networks april 2427 2005 los angeles california g barriac r mudumbai u madhow distributed beamforming information transfer sensor networks proceedings third international symposium information processing sensor networks april 2627 2004 berkeley california usa abhinav kamra vishal misra jon feldman dan rubenstein growth codes maximizing sensor network data persistence acm sigcomm computer communication review v36 n4 october 2006 huiyu luo gregory j pottie designing routes source coding explicit side information sensor networks ieeeacm transactions networking ton v15 n6 p14011413 december 2007 mihaela enachescu ashish goel ramesh govindan rajeev motwani scalefree aggregation sensor networks theoretical computer science v344 n1 p1529 11 november 2005 answol hu sergio servetto asymptotically optimal time synchronization dense sensor networks proceedings 2nd acm international conference wireless sensor networks applications september 1919 2003 san diego ca usa bhaskar krishnamachari fernando ordez fundamental limits networked sensing flow optimization framework wireless sensor networks kluwer academic publishers norwell 2004 alexandre ciancio sundeep pattem antonio ortega bhaskar krishnamachari energyefficient data representation routing wireless sensor networks based distributed wavelet compression algorithm proceedings fifth international conference information processing sensor networks april 1921 2006 nashville tennessee usa hong luo jun luo yonghe liu energy efficient routing adaptive data fusion sensor networks proceedings 2005 joint workshop foundations mobile computing september 0202 2005 cologne germany christina peraki sergio servetto maximum stable throughput problem random networks directional antennas proceedings 4th acm international symposium mobile ad hoc networking computing june 0103 2003 annapolis maryland usa mehmet c vuran ian f akyildiz spatial correlationbased collaborative medium access control wireless sensor networks ieeeacm transactions networking ton v14 n2 p316329 april 2006 mehmet c vuran zgr b akan ian f akyildiz spatiotemporal correlation theory applications wireless sensor networks computer networks international journal computer telecommunications networking v45 n3 p245259 21 june 2004 xun su combinatorial algorithmic approach energy efficient information collection wireless sensor networks acm transactions sensor networks tosn v3 n1 p6es march 2007 micah adler collecting correlated information sensor network proceedings sixteenth annual acmsiam symposium discrete algorithms january 2325 2005 vancouver british columbia qun li michael de rosa daniela rus distributed algorithms guiding navigation across sensor network proceedings 9th annual international conference mobile computing networking september 1419 2003 san diego ca usa alexandra meliou david chu joseph hellerstein carlos guestrin wei hong data gathering tours sensor networks proceedings fifth international conference information processing sensor networks april 1921 2006 nashville tennessee usa tarik arici toygar akgun yucel altunbasak prediction errorbased hypothesis testing method sensor data acquisition acm transactions sensor networks tosn v2 n4 p529556 november 2006 himanshu gupta vishnu navda samir r das vishal chowdhary efficient gathering correlated data sensor networks proceedings 6th acm international symposium mobile ad hoc networking computing may 2527 2005 urbanachampaign il usa razvan cristescu baltasar beferulllozano martin vetterli roger wattenhofer network correlated data gathering explicit communication npcompleteness algorithms ieeeacm transactions networking ton v14 n1 p4154 february 2006 sergio servetto guillermo barrenechea constrained random walks random graphs routing algorithms large scale wireless sensor networks proceedings 1st acm international workshop wireless sensor networks applications september 2828 2002 atlanta georgia usa christopher sadler margaret martonosi data compression algorithms energyconstrained devices delay tolerant networks proceedings 4th international conference embedded networked sensor systems october 31november 03 2006 boulder colorado usa kaiwei fan sha liu prasun sinha scalable data aggregation dynamic events sensor networks proceedings 4th international conference embedded networked sensor systems october 31november 03 2006 boulder colorado usa j paradiso j lifton broxton sensate media multimodal electronic skins dense sensor networks bt technology journal v22 n4 p3244 october 2004