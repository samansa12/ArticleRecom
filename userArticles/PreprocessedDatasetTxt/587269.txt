efficient nonparametric density estimation sphere applications fluid mechanics application nonparametric probability density function estimation purpose data analysis well established recently methods applied fluid flow calculations since density fluid plays crucial role determining flow furthermore calculations involve directional axial data domain interest falls surface sphere accurate fast estimation probability density functions crucial calculations since density estimation performed iteration computation particular values fn x1 fn x2 fn xn density estimate sampled points xi needed evolve system usual nonparametric estimators make use kernel functions construct fn propose special sequence weight functions nonparametric density estimation especially suitable applications resulting method computational advantage kernel methods certain situations also parallelizes easily conditions convergence turn similar required kernelbased methods also discuss experiments different distributions compare computational efficiency method kernel based estimators b introduction esti mati oni problem esti mati values li ty ven samples associ ated di stri buti made type di stri buti whi ch samples drawn thi si si n contrast esti mati whi ch assumed come ven fami ly parameters esti mated ous stati sti cal methods early contri butors theory nonparametri c esti mati oni nclude smi rnov 21 rosenblatt 16 parzen 15 chentsov 3 descri pti ons ous approaches nonparametri c along wi th bi bli ography books lverman 23 nadaraya 14 recent developments presentedi n books scott 18 wand jones 27 results experi mental compari son dely used methods addi ti data analysi ani mportant appli cati nonparametri c onal flui mechani cs flow calculati ons per lagrangi framework set space evolved usi ng ng ons poi nts werei ni ti ally close move apart leadi ng mesh di storti cal di culti es problems th mesh di storti eli mi nated extent use smoothed cle hydrodynami cs ques 2 13 9 12 sph treats nts bei ng tracked samples comi ng unknown li ty di stri buti calculati ons often requi computati values unknown densi ty buti ts gradi ent well received editors august 16 1995 accepted publication revised form august 3 1999 published electronically june 13 2000 httpwwwsiamorgjournalssisc22129046html department computer science university california santa barbara ca 93106 omercs ucsbedu department mathematics indian institute technology bombay india ashokmath iitbernetin contrast appli cati ons concerned wi th di splay densi ty ci ent esti mate densi ty gri di n flui flow calculati ons requi red sample nt another di erencei n two types appli cati onsi deali ng th data analysi usually concerned th opti mal accuracy one get ven sample si ze flui flow calculati ons addi ti onal data ned wi thi ncreased di screti zati usually concerned wi th opti mal vari ati onal eort error appli cati ons examplei n ng di recti onal data 24 samples li e ci rcle 1 along surface al case di recti onal al whi ch c center ci rcle sphere ous methods proposed nonparametri c cal stati sti cs kernel 15 1 28 orthogonal seri es methods 17 11 kernel method extensi vely studi ed probably popular appli cati ons sph thi method value densi ty nt xi esti mated 1 ni esti mate ven sample posi ti ons samples drawn li ty di stri buti wi th unknown ki kernel hi ndow wi dth hi normali zati factor make f ni nto li ty ty one drawbacks kernel method onal costi nvolved even possi ble reduce onedi mensi onal case usi ng expansi polynomi al kernel ng strategy 19 thi strategy cannot ly extended hi gher di mensi ons 5 ng methods 5 usedi n di mensi however si nce evaluated gri methodi sui table flui flow calculati ons whi ch arei nterested requi red sample nt propose cosi nebased wei ght nonparametri c esti mati whi chi al case class esti mators form sequence 26 28 thi mi lar kernel esti mator ease evaluati seri es expansi role ndow wi dth parameter h kernel methodi replaced smoothi ng parameter method f ni form 2 choi ce c cularly sui table appli cati onsi n flui flow calculati ons values f n x 1 sampled di recti ons requi red stepi n flow show th esti mator red n values computed eci ently usi ng om 1d n operati ons di recti onal data om n operati ons al di mensi ons need large long wi thout bound th n thi si si n contrast 2 ons red kernel method computati oni n worst case expected complexi ty oh th ng bounded support howeveri n al case 1 kernel method reduced li near ng step gl u ashok srinivasan deri ons whi ch sequence esti mated ons f n fashi converge unknown experi mentally veri fy accuracy eci ency methodi n practi cal test cases experi ments cal analyses alsoi ndi cate vary th n opti mal accuracy paperi organi zed follows secti 2 define wei ght ons convergence ntegrated square error mise sample spacei 1 theorem 3 ons guarantee ef n also present correspondi ng results 2 secti 3 schemes eci ent computati esti mates 1 2 presented secti ons 4 5 descri experi mental results th esti mator wi th kernel method di stri buti ons practi ce experi mentsi mply net savi ngs number ons performed kernel methodsi n ons also veri fy formula found opti mal choi ce results show kernel method esti mator perform di erent setti ngs thus complement conclusi ons presentedi n secti 6 ns addi ti onal test results 2 cosine estimator convergence mise secti first menti related work done spheri cal data define esti mator deri ons ts convergence di recti onal data ci rcle correspondi ng results di recti onal al data sphere al data ci rcle kernel method nonparametri c esti mati di recti onal al di scussedi n 6 8 whi le deali ng th di recti onal data fi sher lewi embleton 6 recommend usi ng ng kernel exp al data recommend kernel 4 normali zes w li ty c ni reci procal h usedi n defini ti kernel esti mators x x cartesi represen tati nts p p respecti vely x x ii thei nner product two vectors w n role kx x 1 hall watson cabrera 8 analyze esti mators di recti onal data wi th x x replaced observe term x x ii cosi ne angle nts ii measure di stance along surface sphere nts p p inner product plays cruci al rolei n esti mators consi der esti mator terms powers nner product power playi ng role smoothi ng parameter thi enables us expand esti matori n seri es li tates fast computati c x c x ooc x32 b x fig 1 ns c 32 x c 21 case 1 first define esti mator 1 assume x 1 2 sequence ndependently andi denti cally di stri buted random ables observati ons di recti onal data th li ty wei mpose addi ti onal condi ti si nce random ables x j defined ci rcle 1 esti mator densi ty di recti onal data fx x der nonparametri c esti mator form ven 2 th cos 2m normali zati factor ven makes c xi ntegrate 1 cos 2m 2 dx maki ng use table ofi ntegrals gradshteyn ryzhi k 7 usi ng shown examples ons c x c 4 shown fi gure 1a thei nterval fi gure 1b wi sh find suci ent ons whi ch sequence esti mators f n converges fi n mise sense order thi first show convergence bi deri ons whi ch ance converges 0 shall use results prove convergence mise 1 fi rst show expected value esti mate f n x approaches actual uni formly ven n lemma 1 suppose f c 2 andl et f n x given 2 uniforml independentl n proof sfsds 7 gl u ashok srinivasan showni n lverman 20 whi ttle 29 change able x x usi ng peri odi ci ty c f along th 7 8 mean value theorem 2dy x nt x therefore 6 ntegral evaluates 1 nce yc yi odd secondi ntegral evaluates 0 let 2m ng esti mate cos 2m y2y 2 dy 0 yam cos 2m y2y 2 dy yam cos 2m y2y 2 dy yam cos 2m y2dy cos 2m 2 nce cos decreases yi ncreases thei nterval consi derati furthermore bounded 1 therefore order get bound wi choose functi take 0 thi term decays exponenti ally second 10i product thi term thus product approaches 0 si nce exponenti al decay domi nates order get good bound first term 10 wi sh choose ng condi ti 2i small possi ble choose arbi trari ly small thus mi asymptoti c bound furthermore ndependent x hence uni form lemma2 suppose f c 2 andl et f n x given 2 uniforml n provided n proof showni n whi ttle 29 consequence lemma 1 secondi ntegral approaches asymptoti cally hence second term approach 0 nce bounded thus suces show convergence firsti ntegral 0 x fx maki ng change able usi ng n expressi ri ghthand consequence pressi nce mn 2 abovei ntegral converges nce mi si ndependent x uni form therefore vari ance uni formly 0 ons lemma note bound bi cosi ne method ven lemma 1i form bound ance ven lemma therefore role played cosi ne methodi h 2 kernel based methods hi ndow wi dth kernel esti mator words bounds bi vari ance cosi ne esti mator arei n accordance wi th behavi kernel method lverman 23 si mi lari ty rates convergencei expected si nce cosi ne essenti ally li ke kernel esti mator though forms ons di er wi shown later advantage cosi ne esti mator li esi ni ts onal eci ency theorem 3 suppose f c 2 f n x given 2 n ef n proof ef showni n whi ttle 29 lemmas 1 2 thei ntegrals approaches 0 hence mise converges 0 gl u ashok srinivasan fact misei form c bounds constants shall explai n later exact asymptoti c constants mportant practi cal ons condi ti ons convergence esti mates ts deri vati ves real li nei nstead 1 consi der case di recti onal data li e along surface 22 case 2 let x 1 2 n sequence ables th values surface centered suppose li ty fx x j bounded second deri vati ves consi der nonparametri c esti mator form determi ned functi n c thi case follows xx denotes angle nts x x cos 2m normali zi ng factor ven deri vati along li nes case ci rcle ng theorem proved convergence esti mators theorem 4 suppose f c andl et f n x given 14 n ef analogous 13 form misei found expressi mise see asi n case hi ndow wi dth kernel esti mator deali ng th al data consi der ng al esti mator spheri cal data cos 2m xx also define correspondi ng esti mator ci rcle take cosi ne arc length two poi ntsi nstead cosi ne half arc length case di recti onal data cases ci rcle sphere vely 3 ecient evaluation density estimator secti shall descri eci ent algori thm computati esti mates f n x evaluated set n observed nts x 2 ci rcle 1 case also show f value f n arbi trary xi desi red ly accompli shed computed eci ency methodi based fact f n terms ons c x 15 suppose represent posi ti ons observed nts x 2 thei r cartesi coordi nates show x f n x expressed polynomi al total degree mi n coordi nates x coeci ents polynomi al determi nedi n turn coordi nates x moreover coeci ents sums contri buti ons due x ii ndependently fi rst consi der case di recti onal data 1 2 5 halfangle formula cosi ne get denote poi nts 1 correspondi ng angles x x 1 2 n cartesi coordi nates let represent standardi nner product r 2 cosx ng thi si nto 17 get expressi 18i polynomi al degree fixed compute coeci ents addi ng contri buti x follows usi ng al theorem 18 rsm x r thei nner th changi ng order summati rsm mr n i2 ami ven 6 use expressi onal ease 20 fies rsm gl u ashok srinivasan table co mputatio nal co mplexityo f co sine estimato r circle axial directional large consi der number ons requi red evaluati f n x ven ons x 2 powers x r i1 x r i2 fixed 1 2 computed th om multi pli cati ons ng thi 1 2 res omn multi pli cati ons conclusi thi step om 2 averages mr ven r computed wi th addi ti onal ons si nce total om 2 correspondi ng rs th 0 thi means coeci ents polynomi ali n 21 computed wi th total om 2 n ons coeci ents f n x computed evaluate f n x th calculate powers x r 1 x r 1 2 mi n om ons si nce coeci ents already avai lable remai ni ng res multi pli cati ons addi ti ons results di erent cases summari zedi n table 1 remark mise convergence condi ti 1 theorem 3 musti ncrease wi thout bound th n theoreti cally take ng li ke resulti mpli es computati densi ty sample poi nts accompli shed usi ng magni tude ves acceptable accuracy f n x problem th ng slowly magni tude controls error convergence proofs eci ent algori thm evaluati f n x di recti onal data constructed larly 2 observati ons 2 drawn unknown shown 4 rstm th mr n i3 thi ti coeci ents polynomi ali n 22 computed wi th total om 3 n ons preprocessi ng evaluati f n x res ons ng results deri ved al data summari zedi n table 1 also noted needed cartesi representati data data cal coordi nates wi addi ti onal overhead ng cartesi representati however thi overhead takes li near ti wi negli gi ble suci ently large data furthermorei shown 24 ani mportant class appli cati ons cartesi coordi nates preferable cal coordi nates latter systemi cally stable solvi ng di erenti al equati ons se subsequent parts secti shall compare onal e ci ency scheme wi th kernel method 31 parallelization one advantages onal strategy de bed abovei ease paralleli zati paralleli zati oni redi n many flui flow calculati ons due large si zes systems kernel methodi di cult paralleli ze use eci ent kerneli mplementati performs kernel evaluati ons nts whi ch di stance h ven sample eci enti mplementati paralleli zati res load ng decomposi ti poi nts close remai n processor processor roughly loadi n terms onal eort also communi cati pattern kernel method regular contrast paralleli zati cosi ne esti mator ly accompli shed global reducti operati whi ch eci ons usually avai lable thi method requi res onal eort poi nt loadi ly balanced havi ng number processor decomposi ti play ani mportant si nce poi nts processor 32 theoretical comparison kernel cosine estimators analyze onal eci ency kernel cosi ne esti mati methods ani mportant measure eci ency algori thmsi convergence rate error th sample ze n onal eort c red functi error e kernel esti mator te 23 hi smoothi ng parameter di mensi ni sample ze onal eort requi red nonparametri c esti mati expressed dependi ng detai ls algori thm used ven sample ves mal h h n 1d4 however si nce equati onal also depends h need der possi bi li ty value h smaller opti mal value may actually result lower onal eort let us consi der vari ati h th n form 25 24 23 mi ni mum error exponent terms ri ght otherwi se error due hi gher term wi domi nate thi leads whi chi value mal ven n let h optn represent opti mal h mi ni mi zi ng mise ven n h optc represent opti mal gl u ashok srinivasan mi ni mi zi ng onal eort functi error expressi deri ved necessari lyi mply h si nce relati would sti sfy expressi constant k k 1 choose subopti mal value hi n order toi mprove speed algori thm opti mal vari ati error th onal usi ng thi value ven 4 let us consi der cosi ne esti mator te asymptoti c mise follows ei mise mi smoothi ng parameter di di mensi ci rcle sphere ni sample ze onal eort requi red esti mator expressed n determi ned table 1 expressi c abovei 24 th recalli ng behaves h 2 mi lar previ ous case show opti mal vari ati error th onal ven examples cosi ne esti mator ci rcle th al data di recti onal data 2 onal complexi ty error related vely complexi ty kernel esti matori al di recti onal data however several di erent possi bi li ti es st dependi ng eci ent thei mplemen tati esti matori der esti mators form ven 3 4 howeveri f consi der kernel th bounded support use eci mentati algori thm computes kernel poi nts nonzero contri buti expected value data ci rcle note worst case remai ns 26 case consi der eci ent algori thm usi ng polynomi al kernels ng 19 whi ch uses li near ani ni ti al log n ng step thi case whi ch means kernel method better cosi ne kernel however appears natural generali zati thi update strategy hi gher di mensi ons 5 results di erent cases determi nedi n manner demonstrated presentedi n table 2 wi sh menti exact constantsi n theorem 3 qui te mportant compared wi th exponent e nce asymptoti cally slowdowni ncurred cache domi nates overall runni ng expect mpler memory access pattern esti mator wi makei advantageous kernel methodi n asymptoti c case table theo ptimal co mputatio nal eo rt versus mise numbers table represent relatio nship co mputatio nal es take acco unt initial rting step estimator circle cosine axial data 175 cosine directional data 225 kernel worst case 125 kernel expected case 125 si nce worst case es kernel method cosi ne esti mator di recti onal data sphere order eci enci es methods tested experi ments larly si nce worst case complexi ty cosi ne esti mator al data spherei expected case eci enti mplementati kernel esti mator need perform experi ments test meri ts two esti mators 4 experimental results performed cal experi ments al di recti onal data ci rcle spherei n order test eecti veness esti mator first plot esti mates known di stri buti ons demonstrate mise follows expected trends di stri buti ons finally compare onal eci ency esti mator wi th kernel methods ri cal results presentedi n appendi x consi der normali zes functi densi ty surface sphere si known functi u angles azi muth cal coordi nates thi soluti cular flui mechani cs fi gure 2a present cal esti mate versi taken thi figure take data di recti onal however nce th respect center ci rcle consi der data al use al esti mator see fi gure 2b res much smaller value fi gure 3a misei compared versus n onedi mensi onal di stri buti usi ng al cosi ne esti mator also compare th one case di recti onal esti matori n order show benefit usi ng al esti mator fi gure 3b misei compared versus n twodi mensi onal versi di stri buti surface sphere usi ng di recti onal esti mator next present results experi ments compari ng speed cosi ne kernel esti mators consi der opti mal vari ati onal eort th mise order get opti mal onal eort ven mise allow possi bi li ty may di erent sample si zes kernel cosi ne esti mators thi si fied calculati ons one ly change sample ze changi ng di screti zati system performed sons spheri cal data case data ci rcle dered asymptoti c analyses previ ous secti whi ch clearly cate li near kernel algori thmi n onedi mensi onal case wi outperform cosi ne esti mator howeveri n paralleli mplementati ng step li near kernel algori thm may slow one may wi sh consi der cosi ne esti mator gl u ashok srinivasan b co sine estimates theo nedimensio nal caseo f defined abo lid line represents true density dashed line represents directio nal estimate b dashed line represent axial estimate ng kernel chosen sons 1 2 0 otherwi se b n1000 n2000 fig 3 mise versusm n fo r density onedimensio nal n circle exp us cos 2 xa lid line sho ws results fo r axial co sine estimato r dashed line fo r directio nal estimato r two dimensio nal n exp us cos 2 defined abo mise fo r directio nal co sine estimato r ai normali zati constant rati di stance two nts along surface sphere hi ven argument kernel use thi kernel compari sons fied ts popular use gl u ashok srinivasan time fig 4 co mpariso f time seco nds versus mise fo r co sine kernel data sampled defined abo po ints marked represent kernel estimate po ints marked x represent co sine estimate flui mechani cs calculati ons 12 furthermore cannot expect kernel ficantly better performance ng reasons iti well known kernels equally good 23 table 31 th respect eci ency gi ven eci encyi consi derati oni onal eorti nvolved kernel takes 6 10 ng nt operati ons nonzero evaluati ncludi ng cost computi ng square di stance reasonable kernel would requi least 6 ng nt ons apart thi memory access ti mes zeroevaluati ons would add constant kernels fi gures 4 5 compare onal eort requi red cosi ne wei ght esti mator kernel esti mator es obtai ned data usi ng ng procedure performed esti mates ous values n h obtai ned mise ti calculati ons cosi ne kernel esti mates separately plotted data requi red calculati ons versus error chose lower envelope data curve cular esti mator si nce values n data lower envelope best nable speed ven mise thei mplementati kernel esti mator di vi ded spherei nto cells si des cells length least 2h nce kernel defined ndow dth 2h rather h placed samplei n ate cell computi ng densi ty cular cell need search cells expected complexi ty first der data esti mated usi ng al cosi ne esti mator al vari ant kernel esti mator fi gure 4 shows results twodi mensi onal di stri buti thi si example hi ghly nonuni form di stri buti see kernel cosi ne esti mators equally fast next consi der di stri buti ven azi muth results presentedi n fi gure 5a show cosi ne esti mator outperforms kernel esti mator order magni tude al forms esti mators used thi consi dered two es menti oned treated data di recti onal esti mated usi ng di recti onal vari ants kernel cosi ne esti mators di stri buti cosi ne esti mator performed poorly onal eci ency present results thi case fi gure 5b presents results di stri buti ven cos18 see cosi ne esti mator sti outperforms kernel esti mator though sli ghtly 5 discussion compari son esti mates wi th true densi tyi ndi cate cosi ne esti mator produces accurate results di stri buti ons tested plots mise versus n follow expected trends sample zei ncreases error decreases besi des opti mal value mi ncreases sample zei ncreases also seen number poi ntsi ncreases range whi ch esti mate performs well alsoi ncreases use thi advantage choosi ng subopti mal value whi ch decreases onal eort ficantly buti ncreases error sli ghtly experi ments compari ng onal eci enci es show cosi ne esti mator outperforms kernel esti mator al data di stri buti oni moderately uni form di stri buti oni hi ghly nonuni form two esti tors comparable performance al data cosi ne esti mator outperforms kernel esti mator sli ghtly di recti onal data di stri buti oni moderately uni form however ng results cosi ne esti mator poor hi ghly nonuni form di recti onal data general datai nonuni form smoother wei ght ons used thi ves low value whi chi mpli es fast evaluati usi ng cosi ne esti mator however thi leads hi gher h kernel esti mator whi chi mpli es samples contri bute kernel evaluati sample nt hence thi leads onal eort conversely di stri buti oni hi ghly nonuni form especi ally di recti onal data ker nel methodi preferred ri cal test results presentedi n demonstrate nt also analyzed experi mental data esti mate opti mum vari ati th n usi ng results experi ments perform least squares mate kn 125 onedi mensi onal esti mati whi chi expected based expressi mise ng appears reasonable esti mates densi ty surface sphere thi resulti also stent wi th cal predi cti ons magni tude k depends complexi ty functi es 1 10 di stri buti ons dered also noted values n h whi ch gave opti mal onal eort ven mise compared results kernel cosi ne esti mators observed values h close values whi ch gave mi ni mum mise ven sample si ze however values cantly lower values whi ch gave mi ni mum mise ven sample ze though errori nvolvedi tself much hi gher mi ni mum mise appears choose subopti mal smoothi ng parameteri n order ncrease speedi n case cosi ne esti mator gl u ashok srinivasan b time time fig 5 plo f time seco nds versus mise fo r co sine kernel estimatio f data sampled 18 po ints marked represent kernel estimate po ints marked x represent co sine estimate data treated axial b data treated directio nal elevation density fig 6 plo f density ns g dierent elevatio n lid line dashed line dashdo tted line tted line 6 conclu ion thi paper descri bed wei ght esti mator nonparametri c esti mati li ty ons based cosi nes provi ded ons whi ch esti mate ts deri vati ves converge actual ons developed scheme eci ent computati densi ty presented experi mental results check performance esti mator practi cal problems results cularly relevant flui mechan calculati ons andi n general ons sample si ze controlled example though refinement di screti zati also ven empi r cal formula choosi ng wei ght exponent parameter esti mator experi mental results suggest cosi ne esti mator outperforms kernel esti mator di recti onal al data moderately uni form ves performance comparable kernel esti mator hi ghly nonuni form al data whi le kernel methodi preferable hi ghly nonuni form di recti onal data potenti al theoreti cal study esti mator appendix test results present test resultsi n secti study eci enci es cosi ne kernel techni ques ed systemati cally bei ng vely uni form bei ng sharply peaked tests chose ons g si constant governs sharpness elevati normali zes thi li ty fi gure 6 shows densi ty functi elevati alone di erent values parameter thi c center sphere thus use al esti mators 4 alsoi gnore knowledge use general di recti onal esti mators gl u ashok srinivasan gl u ashok srinivasan gl u ashok srinivasan time b present results experi ments plots versus mise kernel esti mator versus cosi ne esti mator al di recti onal data kernel esti matori one usedi n ri cal 4 tests performed intel celeron 300mhz processor th 64 mb memory c code led wi th gcc compi ler opti mi zati level o3 see fi gures 7 8 9 10 11 sharp cosi ne esti mator outperforms kernel esti mator al di recti onal data densi ty becomes sharper kernel method starts outperformi ng cosi ne esti mator di recti onal data though latteri sti better al data densi ty becomes extremely sharp kernel method becomes better types data though al data two methods sti comparable extenti n terms speed results follow cally predi cted trends demonstrate two methods complement di erent types data appendix thank referees thei r detai led comments advi ce espe ci ally di recti ng attenti current li terature r glo bal measureso f deviatio nso f density functio n estimates estimatio f unkno wn pro bability density basedo bservatio ns fast implementatio nso f nparametric curve estimato rs statistical analysiso f spherical data kernel density estimatio n spherical data treesph unificatio f sph hierarchical tree metho estimatio f pro bability densities cumulatives fo urier series metho ds thed particle hydro dynamics estimatio f pro bability density functio n mo de remarkso n nparametric estimateso f density functio n estimatio f pro bability density ano rtho go nal series multivariate density estimatio n fast algo rithms fo r nparametric curve estimatio n kernel density estimatio n using fast fo urier transfo rm appro ximatio f pro bability densitieso f rando variables numerical lutio f partial di density estimatio n fo r statistics data analysis new co mputatio nal metho fo r lutio f flo w pro blemso f micro structured fluids pro bability density estimatio n astro pro bability density estimatio n using delta sequences kernel smo thing estimatio f pro bability density smo ns tr ctr jeff racine parallel distributed kernel estimation computational statistics data analysis v40 n2 p293302 28 august 2002