study reinforcement learning continuous case means viscosity solutions paper proposes study reinforcement learning rl continuous statespace time control problems based theoretical framework viscosity solutions vss use method dynamic programming dp introduces value function vf expectation best future cumulative reinforcement continuous case value function satisfies nonlinear first second order depending deterministic stochastic aspect process differential equation called hamiltonjacobibellman hjb equation well known exists infinity generalized solutions differentiable almost everywhere equation vf show gradientdescent methods may converge one generalized solutions thus failing find optimal controlin order solve hjb equation use powerful framework viscosity solutions state exists unique viscosity solution hjb equation value function use another main result vss stability passing limit prove convergence numerical approximations schemes based finite difference fd finite element fe methods methods discretize resolution hjb equation dp equation markov decision process mdp solved dp methods thanks strong contraction property initial data state dynamics reinforcement function perfectly known however rl approach consider system interaction priori least partially unknown environment learns experience initial data perfectly known approximated learning main contribution work derive general convergence theorem rl algorithms one uses approximations sense satisfying weak contraction property initial data result used modelbased modelfree rl algorithms offline online updating methods deterministic stochastic state dynamics though latter case described based fe fd discretization methods illustrated several rl algorithms one numerical simulation car hill problem b introduction paper reinforcement learning rl continuous statespace time case rl techniques see kaelbling littman moore 1996 survey adaptive methods solving optimal control problems partial amount initial data available system learns paper focus dynamic programming dp method introduces function emi munos called value function vf cost function estimates best future cumulative reinforcement cost function initial states rl continuous case difficult problem least two reasons since consider continuous statespace first reason value function approximated either using discretization grids triangulations general approximation neural networks polynomial functions fuzzy sets etc methods rl algorithms continuous statespace implemented neural networks see example barto sutton anderson 1983 barto 1990 gullapalli 1992 williams 1992 lin 1993 sutton whitehead 1993 harmon baird klopf 1996 bertsekas tsitsiklis 1996 fuzzy sets see nowe 1995 glorennec jouffe 1997 approximators based state aggregation see singh jaakkola jordan 1994 clustering see mahadevan sparsecoarsecoded functions see sutton 1996 variable resolution grids see moore 1991 moore atkeson 1995 however pointed several authors combination dp methods function approximators may produce unstable divergent results even applied simple problems see boyan moore 1995 baird 1995 gordon 1995 results using clever algorithms like residual algorithms baird 1995 particular classes approximation functions like averagers gordon 1995 lead convergence local global solution within class functions considered anyway difficult define class functions neural network suitable architecture within optimal value function could approxi mated knowing little prior knowledge smoothness properties second reason consider continuoustime variable indeed value function derived dp equation see bellman 1957 relates value state function values successor states continuoustime limit successor states get infinitely closer value point becomes function differential defining non linear differential equation called hamiltonjacobibellman hjb equation discretetime case resolution markov decision process mdp equivalent resolution whole statespace dp equation property provides us dp rl algorithms locally solve dp equation lead optimal solution continuous time longer case since hjb equation holds value function differentiable general value function differentiable everywhere even smooth initial data thus equation cannot solved usual sense leads either solution look classical solutions ie differentiable everywhere infinity solutions look generalized solutions ie differentiable almost everywhere fact illustrated simple 1dimensional example explains could many bad solutions gradientdescent methods rl indeed methods intend minimize integral hamiltonian generalized solutions hjb equation global optima problem reinforcement learning means viscosity solutions 3 gradientdescent methods may lead approximate among infinity generalized solutions giving little chance reach desired value function order deal problem integrating hjb equation use formalism viscosity solutions vss introduced crandall lions cran dall lions 1983 see users guide crandall ishii lions 1992 order define adequate class appears weak formulation solutions nonlinear first second order differential equation hjb equations main properties vss existence uniqueness fact value function vs thus large class optimal control problems exists unique vs hjb equation value function furthermore vss remarkable stability properties passing limit derive proofs convergence discretization methods approach consists defining class convergent numerical schemes among finite element fe finite difference fd approximation schemes introduced kushner 1990 kushner dupuis 1992 discretize resolution ffi hjb equation dp equation discrete markov decision process apply result convergence barles souganidis 1991 prove convergence value function v ffi discretized mdp value function v continuous problem discretization step ffi tends zero dp equation discretized mdp could solved dp method dp equation satisfies strong contraction property leading successive iterations converge value function unique fixed point equation data transition probabilities reinforcements perfectly known learner case rl approach thus propose result convergence rl algorithms use approximations data sense approximated dp equation need satisfy weak contraction property convergence occurs number iterations tends infinity discretization step tends zero result applies modelbased modelfree rl algorithms offline online methods deterministic stochastic state dynamics fe fd based discretization methods illustrated several rl algorithms one numerical simulation car hill problem follows consider discounted infinitetime horizon case description finitetime horizon case see munos 1997a deterministic state dynamics stochastic case see munos bourgine 1997 munos 1997a section 2 introduces formalism rl continuous case defines value function states hjb equation presents result showing continuity vf section 3 illustrates problems classical solutions hjb equation simple 1dimensional example introduces notion viscosity solutions section 4 concerned numerical approximation value function using discretization schemes finite element finite difference methods emi munos presented general convergence theorem whose proof appendix stated section 5 states convergence theorem whose proof appendix b general class rl algorithms illustrates several algorithms section 6 presents simple numerical simulation car hill problem 2 formalism reinforcement learning continuous case objective reinforcement learning learn experience influence behavior dynamic system order maximize payoff function called reinforcement reward function equivalently minimize cost function problem optimal control state dynamics reinforcement function priori least partially unknown paper concerned deterministic problems dynamics system governed controlled differential equation similar results stochastic case see munos bourgine 1997 munos 1997a related work using multigrid methods see pareigis 1996 two possible approaches optimal control pontryagins maximum principle theoretical work see pontryagin boltyanskii gamkriledze mischenko 1962 recently fleming rishel1975 study temporal differ ence see doya 1996 application control neural networks see bersini gorrini 1997 bellmans dynamic programming dp troduced bellman 1957 approach considered paper 21 deterministic optimal control discounted infinitetime horizon problems follows consider infinitetime horizon problems discounted framework case state dynamics depend time study finite time horizon case dependency time see munos 1997a let xt state system belongs statespace closure open subset ae ir evolution system depends current state xt control action ut 2 u u closed subset control space defined controlled differential equation dt control ut bounded lebesgue measurable function values u function f called state dynamics assume f lipschitzian respect first variable exists constant l f 0 initial state x 0 time choice control ut leads unique state dynamics 1 deterministic trajectory xt see figure 1 reinforcement learning means viscosity solutions 5 definition 1 define discounted reinforcement functional j depends initial data x 0 control ut 0 exit time xt trajectory always stays inside rx u current reinforcement defined rx boundary reinforcement defined boundary statespace fl 2 0 1 discount factor weights shortterm rewards longterm ones ensures convergence integral objective control problem find initial state x 0 control u optimizes reinforcement functional jx x figure 1 statespace initial state x 0 choice control ut leads trajectory xt 0 exit time statespace remark unlike discrete case continuous case need consider two different reinforcement functions r obtained accumulated running trajectory whereas r occurs whenever trajectory exits statespace formalism enables us consider many optimal control problem reaching target avoiding obstacles viability problems many optimization problems definition 2 define value function maximum value reinforcement functional function initial state time emi munos giving properties value function hjb equation continuity differentiability properties let us first describe reinforcement learning framework considered constraints implies 22 reinforcement learning approach rl techniques adaptive methods solving optimal control problems whose data priori least partially unknown learning occurs iteratively based experience interactions system environment current boundary reinforcement signals objective rl find optimal control techniques used dp however rl approach state dynamics fx u reinforcement functions rx u rx partially unknown system thus rl constructive iterative process estimates value function successive approximations learning process includes mechanism choice control deal exploration versus exploitation dilemma exploration provides system new information unknown data whereas exploitation consists optimizing estimated values based current knowl edge see meuleau 1996 mechanism integrating new information refining approximation value function latter topic object paper study numerical approximations value function great importance rl dp function deduce optimal feedback controller next section shows value function satisfies local property called hamiltonjacobibellman equation points relation optimal control 23 hamiltonjacobibellman equation using dynamic programming principle introduced bellman 1957 prove value function satisfies local condition called hamiltonjacobi bellman hjb equation see fleming soner 1993 complete survey deterministic case studied firstorder nonlinear partial differential equation stochastic case prove similar equation order two holds assume u compact set theorem 1 hamiltonjacobibellman value function v differentiable x let dv x gradient v x hamiltonjacobibellman u2u holds x 2 additionally v satisfies following boundary condition reinforcement learning means viscosity solutions 7 remark boundary condition inequality boundary point example x 1 2 figure 1 may exist control ut corresponding trajectory stays inside whose reinforcement functional strictly superior immediate boundary reinforcement rx 1 cases holds strict inequality remark using equivalent definition hjb equation 5 means v solution equation hamiltonian h defined differentiable function w u2u dynamic programming computes value function order find optimal control feedback control policy function x u optimal control u time depends current state xt u indeed value function deduce following optimal feedback control policy u2u pointed importance computing value function defining optimal control show properties v continuity dif ferentiability integrate sense hjb equation approximating 24 continuity value function property continuity value function may obtained following assumption concerning state dynamics f around boundary assumed smooth ie 2 c x 2 let gamma n x outward normal x example see figure 1 assume hypotheses mean point boundary ought trajectories tangential boundary state space theorem 2 continuity suppose 2 9 satisfied value function continuous proof theorem found barles perthame 1990 emi munos 3 introduction viscosity solutions theorem 1 know value function differentiable solves hjb equation however general value function differentiable everywhere even data problem smooth thus cannot expect find classical solutions ie differentiable everywhere hjb equation look generalized solutions ie differentiable almost everywhere find many solutions value function solve hjb equation therefore need define weak class solutions equation crandall lions introduced weak formulation defining notion viscosity solutions vss crandall lions 1983 complete survey see crandall et al 1992 barles 1994 fleming soner 1993 notion developed broad class nonlinear first second order differential equations including hjb equations stochastic case controlled diffusion processes among important properties viscosity solutions uniqueness results stability solutions approximated equations passing limit important result used prove convergence approximation schemes section 44 mainly fact value function unique viscosity solution hjb equation 5 boundary condition 6 first let us illustrate simple example problems raised one looks classical generalized solutions hjb equation 31 3 problems illustrated simple example let us study simple control problem 1 dimension let state xt 2 0 1 control ut 2 fgamma1 1g state dynamics dx consider current reinforcement everywhere boundary reinforcement defined r0 r1 example deduce value function hjb equation boundary conditions v 0 r0 v 1 r1 1 first problem classical solution hjb equation 03 corresponding value function plotted figure 2 observe v differentiable everywhere thus satisfy hjb equation everywhere classical solution hjb equation reinforcement learning means viscosity solutions 9 x optimal value function figure 2 value function differentiable everywhere 2 second problem several generalized solutions one looks generalized solutions satisfy hjb equation almost everywhere find many functions value function example function satisfying 11 almost everywhere boundary conditions illustrated figure 3 x optimal value function figure 3 many generalized solutions value function remark problem great importance one wants use gradientdescent methods general function approximator like neural networks approximate value function use gradientdescent methods may lead approximate generalized solutions hjb equation thus fail find value function indeed suppose use gradientdescent method finding function w minimizing error emi munos z x2o h hamiltonian defined section 23 method converge best case generalized solution v g 7 functions global optima minimization problem since error ev probably different value function v moreover control induced functions closed loop policy 8 might different optimal control defined v example function plotted figure 3 generates control given direction gradient different optimal control defined value function plotted figure 2 fact continuous time space problems exists infinity global minima gradient descent methods functions may different expected value function case neural networks usually use smooth functions differentiable everywhere thus neither value function v figure 2 generalized solution v g figure 3 exactly represented approximated let us denote v best approximations v v g network v local minima gradientdescent method minimizing e nothing proves v global minimum example could seem v smoother generalized solutions one discontinuity instead several ones sense e e true general case continuoustime case use smooth function approximator exists infinity local solutions problem minimizing error e nothing proves expected v global solution see munos baird moore 1999 numerical experiments simple one two dimensional problems time discretized problem disappears still careful passing limit paper describe discretization methods converge value function passing limit continuous case 3 third problem boundary condition inequality illustrate problem inequality boundary condition let 5 corresponding value function plotted figure 4 observe v 0 strictly superior boundary reinforcement r0 strict inequality occurs boundary point x 2 exists control ut trajectory goes immediately inside generates better reinforcement functional boundary reinforcement rx obtained exiting x give definition 4 follows weak viscosity formulation boundary condition 6 reinforcement learning means viscosity solutions 11 x value function figure 4 boundary condition may hold strict inequality condition 32 definition viscosity solutions section define notion viscosity solutions continuous functions definition discontinuous functions given appendix definition 3 viscosity solution let w continuous realvalued function defined ffl w viscosity subsolution 7 functions local maximum w gamma w ffl w viscosity supersolution 7 functions local minimum w gamma w ffl w viscosity solution 7 viscosity subsolution viscosity supersolution 7 33 properties viscosity solutions following theorem whose proof found crandall et al 1992 states value function viscosity solution theorem 3 suppose hypotheses theorem 2 hold value function v viscosity solution 7 emi munos order deal inequality boundary condition 6 define viscosity formulation differential type condition instead pure dirichlet condition definition 4 viscosity boundary condition let w continuous realvalued function defined ffl w viscosity subsolution 7 boundary condition 6 viscosity subsolution 7 functions local maximum w gamma w minfhxwdw ffl w viscosity supersolution 7 boundary condition 6 viscosity supersolution 7 functions local minimum w gamma w minfhxwdw ffl w viscosity solution 7 boundary condition 6 viscosity sub supersolution 7 boundary condition 6 remark hamiltonian h related optimal control problem case condition 13 simply equivalent boundary inequality 6 definition theorem 3 extends viscosity solutions boundary conditions moreover result uniqueness obtain following theorem whose proof crandall et al 1992 fleming soner theorem 4 suppose hypotheses theorem 2 hold value function v unique viscosity solution 7 boundary condition 6 remark important theorem shows relevance viscosity solutions formalism hjb equations moreover provides us useful framework illustrated next sections proving convergence numerical approximations value function study numerical approximations value function define approximation schemes discretizing hjb equation finite element finite difference methods prove convergence viscosity solution hjb equation thus value function control problem 4 approximation convergent numerical schemes 41 introduction main idea discretize hjb equation dynamic programming dp equation stochastic markovian decision process mdp resolution ffi solve mdp find discretized value function v ffi using dp techniques guaranteed converge since dp equation fixedpoint equation satisfying strong contraction property see puterman 1994 bertsekas 1987 also interested convergence properties discretized v ffi value function v ffi decreases 0 kushner 1990 kushner dupuis 1992 define two classes approximation schemes based finite difference fd section 42 finite element methods section 43 section 44 provides general theorem convergence deduced abstract formulation barles souganidis 1991 using stability properties viscosity solutions covers fe fd methods important required properties convergence monotonicity consistency scheme following assume control space u approximated finite control spaces u ffi ae u 42 approximation finite difference methods basis ir dynamics let positive negative parts f discretization step ffi let us consider lattices ffi z integers sigma frontier sigma ffi denote set points f 2 ffi z n least one adjacent point sigma see figure 5 let us denote jjyjj 1norm vector figure 5 discretized statespace sigma ffi dots frontier sigma ffi crosses emi munos fd method consists replacing gradient dv forward backward difference quotients v 2 sigma ffi direction e thus hjb equation approximated following equation knowing deltat ln fl approximation fl deltat gamma 1 deltat tends 0 deduce following equivalent approximation equation 2 sigma ffi p 0 j dp equation finite markovian decision process whose statespace space u ffi probabilities transition p 0 j u see figure 6 geometrical interpretation12 x x x x x x u x p u p u p u x x state control next state x prob figure 6 geometrical interpretation fd discretization continuous process left discretized resolution ffi mdp right transition probabilities p mdp coordinates vector 1 j gamma j projection onto segment direction parallel f u boundary condition define absorbing terminal states reinforcement learning means viscosity solutions 15 defining f ffi fd finite difference scheme dp equation fd equation states v ffi fixed point f ffi fd moreover f bounded value f fd satisfies following strong contraction since 1 exists fixed point value function unique computed dp iterative methods see puterman 1994 bertsekas 1987 computation v ffi convergence two standard methods computing value function v ffi mdp value iteration v ffi limit sequence successive iterations v ffi fd policy iteration approximations policy space alternative policy evaluation steps policy improvement steps see puterman 1994 bertsekas 1987 bertsekas tsitsiklis 1996 information dp theory section 5 describe rl methods computing iteratively approximated value functions following section study similar method discretizing continuous process mdp using finite element methods convergence two methods ie convergence discretized v ffi value function v tends 0 derived general theorem section 44 43 approximations finite element methods use finite element fe method linear simplexes based triangulation covering statespace see figure 7 value function v approximated piecewise linear functions v ffi defined values vertices fg triangulation sigma ffi value v ffi point x inside simplex linear combination v ffi vertices barycentric coordinates x inside simplex recall definition barycentric coordinates emi munos xd x figure 7 triangulation sigma ffi statespace v ffi x linear combination v ffi weighted barycentric coordinates x using finite element approximation scheme derived kushner 1990 continuous hjb equation approximated following equation j u point inside sigma ffi j require satisfies following condition positive constants k 1 remark interesting notice time discretization function u need constant depend state control u provides us freedom choice parameters assuming equation 19 still holds discussion choice constant time discretization function according space discretization size ffi order optimize precision approximations see pareigis 1997 let us denote 0 simplex containing j u v ffi linear inside simplex equation written dp equation markov decision process whose statespace set vertices fg probability transition state control u next states barycentric coordinates j u inside simplex 8 boundary states satisfy terminal condition reinforcement learning means viscosity solutions 17 x x x l h x xx x x x l l x x x x hstate control proba next state x l h x figure 8 finite element approximation consider vertex j 1 linear combination v weighted barycentric coordinates thus probabilities transition mdp barycentric coordinates defining f ffi fe finite element scheme approximated value function v ffi satisfies dp equation similarly fd scheme f ffi fe satisfies following strong contraction since 1 unique solution v ffi 23 21 computed dp techniques 44 convergence approximation schemes section present convergence theorem general class approximation schemes use stability properties viscosity solutions described barles souganidis 1991 obtain convergence another kind convergence result using probabilistic considerations found kushner dupuis 1992 results treat problem general boundary condition 9 fact important required properties convergence monotonicity property 27 consistency properties 30 31 corollary deduce fe fd schemes studied previous sections convergent emi munos 441 general convergence theorem let sigma ffi sigma ffi two discrete finite subsets ir assume x 2 lim ffi0 distx sigma operator space bounded functions sigma ffi concerned convergence solution v ffi dynamic programming equation boundary condition make following assumptions f ffl constant c ffl ffi exists solution v ffi 25 26 29 bounded constant v independent ffi consistency exists constant k 0 lim inf lim sup remark conditions 30 31 satisfied particular case lim theorem 5 convergence scheme assume hypotheses theorem satisfied assume 27 28 30 31 hold f ffi convergent approximation scheme ie solutions v ffi 25 lim gammax uniformly reinforcement learning means viscosity solutions 19 442 outline proof use procedure described barles perthame1988 idea define largest limit function v smallest limit function v prove respectively discontinuous sub super viscosity solutions proof based general convergence theorem barles souganidis 1991 given appendix use comparison result states 9 holds viscosity subsolutions less viscosity supersolutions thus v sup v inf definition limit function v viscosity solution hjb equation thus theorem 4 value function problem 443 fd fe approximation schemes converge corollary 1 approximation schemes f ffi fd f ffi fe convergent indeed finite difference scheme obvious since p 0 j u considered transition probabilities approximation scheme f ffi fd satisfies 27 28 17 dp equation mdp dp theory ensures 29 true check scheme also consistent conditions 30 31 hold fd satisfy hypotheses theorem 5 similarly finite element scheme basic properties barycentric coordinates x approximation scheme f ffi fe satisfies 27 19 condition holds dp theory ensures 29 true scheme consistent conditions 30 31 hold fe satisfies hypotheses theorem 5 45 summary previous results convergence given discretization step ffi strong contraction property 18 24 dp theory ensures values v ffi iterated dp algorithm converge value v ffi approximation scheme f ffi n tends infinity convergence scheme theorem 5 v ffi tend value function v continuous problem ffi tends 0 see figure 9 strong contraction property hjb equation dp equation figure 9 hjb equation discretized resolution ffi dp equation whose solution v ffi convergence scheme ensures v 0 thanks strong contraction property iterated values v ffi n tend v ffi emi munos remark theorem 5 gives result convergence hypothesis 9 continuity v satisfied however hypothesis satisfied value function continuous theorem still applies 9 satisfied value function discontinuous area still convergence value function continuous 5 designing convergent reinforcement learning algorithms order solve dp equation 14 20 one use dp offline methods value iteration policy iteration modified policy iteration see puterman 1994 synchronous asynchronous backups online methods like real time dp see barto bradtke singh 1991 bertsekas tsitsiklis 1996 example introducing qvalues q equation 20 solved successive backups indexed n states control u order provided every state control updated regularly values v ffi n algorithm converges value function v ffi discretized mdp n 1 however rl approach state dynamics f reinforcement functions unknown learner thus right side iterative rule 33 unknown approximated thanks available knowledge rl terminology two possible approaches updating values ffl modelbased approach consists first learning model state dynamics reinforcement functions using dp algorithms based rule 33 approximated model instead true values learning updating estimated qvalue q iteratively simulation trajectories online learning end exit time one several trajectories offline batch learning ffl modelfree approach consists updating incrementally estimated values n qvalue q n visited states without learning model follows propose convergence theorem applies large class rl algorithms modelbased modelfree online offline deterministic stochastic dynamics provided updating rule satisfies weak contraction property respect convergent approximation scheme fd fe schemes studied previously 51 convergence rl algorithms following theorem gives general condition rl algorithm converges optimal solution continuous problem idea reinforcement learning means viscosity solutions 21 updated values modelfree modelbased method must close enough convergent approximation scheme difference satisfies weak contraction property 34 theorem 6 convergence rl algorithms ffi let us build finite subsets sigma ffi sigma ffi satisfying properties section 44 consider algorithm leads update every state 2 sigma ffi regularly every state 2 sigma ffi least let f ffi convergent approximation scheme example 22 v ffi solution 25 26 assume values updated iteration n satisfy following properties sense following weak contraction property positive constant k function effi tends 0 sense positive constant kr ae 0 exists delta exists n n n sup result states hypotheses theorem applies mainly find updating rule satisfying weak contraction property 34 values v ffi computed algorithm converge value function v continuous problem discretization step ffi tends zero number iterations n tends infinity 511 outline proof proof theorem given appendix b condition 34 strong contraction property constant 1 convergence would obvious since 25 fact states updated regularly fixed would converge v ffi fact theorem 5 v ffi converges v could deduce v ffi emi munos case longer expect v ffi 34 holds prove object section b2 appendix b 0 exists small enough values ffi stage n result together convergence scheme leads convergence algorithm ffi 0 n 1 see figure 10 contraction property weak rl hjb equation dp equation figure 10 values v ffi iterated rl algorithm converge v ffi n 1 however weak contraction property satisfied v ffi tend v 512 challenge designing convergent algorithms general strong contraction property 36 impossible obtain unless perfect knowledge dynamics f reinforcement functions r r rl approach components estimated approximated learning phase thus iterated values v ffi imperfect may good enough satisfy weak contraction property 34 defining good approximations challenge designing convergent rl algorithms order illustrate method present section 52 procedure designing modelbased algorithms section 53 give modelfree algorithm based fe approximation scheme 52 modelbased algorithms basic idea build model state dynamics f reinforcement functions r r states local knowledge obtained simulation trajectories trajectory xn goes inside neighborhood defining neighborhood area whose diameter bounded kn ffi positive constant kn time n keep constant control u period n build model f u r u see figure 11 approximate scheme 22 following values using previous model qvalues q updated according function u satisfying 19 corresponds iterative rule 33 model f f n er instead f r x x x figure 11 trajectory goes neighborhood grey area state dynamics approximated n easy prove see munos moore 1998 munos 1997a assuming smoothness assumptions r r lipschitzian approximated v ffi n satisfy condition 34 theorem 6 applies remark using model build similar convergent rl algorithm based finite difference scheme 22 see munos 1998 thus appears quite easy design modelbased algorithms satisfying condition 34 remark method also used stochastic case model state dynamics average several trajectories yn gammax n n model noise variance see munos bourgine 1997 furthermore possible design modelfree algorithms satisfying condition 34 topic following section 53 modelfree algorithm finite element rl algorithm consider triangulation sigma ffi satisfying properties section 43 direct rl approach consists updating online qvalues vertices without learning model dynamics consider fe scheme 22 u j uf u projection onto opposite side simplex parallel direction f u see figure 12 suppose simplexes emi munos non degenerated ie 9k ae radius sphere inscribed simplex superior k ae ffi 19 holds let us consider trajectory xt goes simplex let input point output point control u assumed constant inside simplex x x x x figure 12 trajectory going simplex j u projection onto opposite side simplex ygammax good approximation j u gamma values u j u unknown system make following estimations thales ffl u approximated x gammabarycentric coordinate x inside simplex ffl j u approximated use knowledge state input output points x running time trajectory inside simplex barycentric coordinate x computed soon system knows vertices input side simplex besides r u approximated current reinforcement rx u input point thanks linearity v ffi inside simplex v ffi j u approximated algorithm consists updating quality q estimation system exits statespace inside simplex ie 2 update closest vertex simplex assuming additional regularity assumptions r r lipschitzian f bounded values v ffi 35 proves convergence modelfree rl algorithm based fe scheme see munos 1996 proof similar way design direct rl algorithm based finite difference scheme f ffi fd 16 prove convergence see munos 1997b 6 numerical simulation car hill problem description dynamics problem see moore atkeson 1995 problem statespace dimension 2 position velocity car experiments chose reinforcement functions follows current reinforcement rx u zero everywhere terminal reinforcement rx gamma1 car exits left side statespace varies linearly depending velocity car exits right side statespace best reinforcement 1 occurs car reaches right boundary null velocity see figure 13 control u 2 possible positive negative thrust thrust gravitation resistance goal position x r1 null velocity r1 max velocity reinforcement figure 13 car hill problem order approximate value function used 3 different triangulations composed respectively 9 9 17 17 33 33 states see figure 14 ran two algorithms follows ffl asynchronous real time dp based updating rule 33 assuming perfect model initial data state dynamics reinforcement functions ffl asynchronous finite element rl algorithm described section 53 based updating rule 37 initial data approximated parts trajectories selected random emi munos velocity triangulation 2 triangulation 3 position triangulation 1 figure 14 three triangulations used simulations order evaluate quality approximation methods also computed good approximation value function v plotted figure 15 using dp rule 33 dense triangulation 257 257 states perfect model initial data figure 15 value function car hill computed triangulation composed 257 257 states computed approximation error en discretization step triangulation k problem notice hypothesis 9 hold trajectories tangential boundary statespace boundary states zero velocity value function discontinuous frontier discontinuity happens point beginning frontier eventually get positive reward whereas point doomed exit left side statespace thus following remark section 45 order compute en k choseomega whole statespace except area around discontinuity figures 17 represent respectively 2 algorithms approximation error en k 3 triangulations function number iterations n observe following points approximation triangulation 1 triangulation 2 triangulation 3 figure 16 approximation error en k values computed asynchronous real time dp algorithm function number iterations n several triangulations approximation triangulation 1 triangulation 3 triangulation 2 figure 17 approximation error en k values computed asynchronous finite element rl algorithm ffl whatever resolution discretization ffi values v ffi computed rtdp converge n increases limit v ffi solution dp equation emi munos 20 moreover observe convergence v ffi value function v resolution ffi tends zero results illustrate convergence properties showed figures 9 ffl given triangulation values v ffi computed ferl converge discretization error approximation decreases rapidly oscillates within large range 2 error decreases slowly states updated oscillates within smaller range 3 dense discretization error decreases still slowly eventually gets close zero still oscillating thus observe illustrated figure 10 given discretization step ffi values converge however oscillate within range depending ffi theorem 6 simply states desired precision 8 exists discretization step ffi eventually 9n 8n n values approximate value function precision supjv ffi 7 conclusion future work paper proposes formalism study rl continuous statespace time case hamiltonjacobibellman equation stated several properties solutions described notion viscosity solution introduced used integrate hjb equation finding value function describe discretization methods using finite element finite difference schemes approximating value function use stability properties viscosity solutions prove convergence propose general method designing convergent modelbased modelfree rl algorithms illustrate several examples convergence result obtained substituting strong contraction property used prove convergence dp method cannot hold initial data perfectly known weak contraction property enables approximations data main theorem states convergence result rl algorithms discretization step ffi tends 0 number iterations n tends infinity practical applications method must combine learning dynamics n 1 structural dynamics operates discretization process example munos 1997c initial rough delaunay triangulation progressively refined adding new vertices according local criterion estimating irregularities value function munos moore 1999 kuhn triangulation embedded kdtree adaptively refined nonlocal splitting criterion allows cells take account impact cells deciding whether split future theoretical work consider study approximation schemes design algorithms based scheme adaptive variable resolution discretizations like adaptive discretizations munos moore 1999 munos 1997c partigame algorithm moore atkeson 1995 multi grid methods akian 1990 pareigis 1996 sparse grids griebel 1998 study rates convergence algorithms already exists cases see dupuis james 1998 study generalized control problems jumps generalized boundary conditions etc adequately address practical issues extensive numerical simulations comparison methods conducted order deal high dimensional statespaces future work concentrate designing relevant structural dynamics condensed function representations acknowledgments work funded dassaultaviation france carried laboratory engineering complex systems lisc cemagref france would like thank paul bourgine martine naillon guillaume guy barles andrew moore also grateful parents mentor daisaku ikeda best friend guylene appendix proof theorem 5 a1 outline proof use barles perthame procedure barles perthame 1988 first give definition discontinuous viscosity solutions define largest limit function v sup smallest limit function v inf prove following barles souganidis 1991 lemma 1 v sup respectively v inf discontinuous viscosity subsolution resp supersolution use strong comparison result lemma 2 states 9 holds viscosity subsolutions less viscosity supersolutions thus v sup v inf definition v sup v inf thus limit function v viscosity solution hjb equation thus value function problem a2 definition discontinuous viscosity solutions let us recall notions upper semicontinuous envelope w lower semicontinuous envelope w real valued function w definition 5 let w locally bounded real valued function defined emi munos ffl w viscosity subsolution hx wdw functions local maximum w lambda gamma w ffl w viscosity supersolution hx wdw functions local minimum w lambda gamma w ffl w viscosity solution hx wdw viscosity subsolution viscosity supersolution hx wdw a3 v sup v inf viscosity sub supersolutions lemma 1 two limit functions v sup respectively viscosity sub supersolutions proof let us prove v sup subsolution proof v inf supersolution similar let smooth test function v sup gamma maximum assumed strict x v sup n sequence converging zero v maximum n tends x tends 0 thus 2 sigma 27 theta 28 obtain theta theta tends 0 left side inequality tends 0 thus 31 thus v sup viscosity subsolution a4 comparison principle viscosity sub supersolutions assume 9 7 6 weak comparison principle ie viscosity subsolution w supersolution w 7 6 x 2 proof comparison result viscosity sub supersolutions see barles 1994 barles perthame 1988 barles perthame 1990 slightly different hypothesis fleming soner 1993 a5 proof theorem 5 proof lemma 1 largest limit function v sup smallest limit function v inf respectively viscosity subsolution supersolution hjb equation comparison result lemma 2 v sup v inf definition approximation scheme v ffi converges limit function v viscosity solution hjb equation thus value function problem 32 holds true appendix proof theorem 6 b1 outline proof know convergence scheme v ffi theorem 5 compact ae 1 0 exists discretization step ffi sup let us define seen section 511 strong contraction property 36 ffi e would converge 0 weak contraction property idea following proof 2 0 exists ffi stage n n n emi munos deduce 0 find sup b2 sufficient condition lemma 3 let us suppose exists constant ff 0 state updated stage n following condition hold exists n n ne ffi proof algorithm updates every state 2 sigma ffi regularly exists integer stage nm states 2 sigma ffi updated least since stage n thus b2 b3 thus exists n 1 8n sup moreover states 2 sigma ffi updated least thus exists n 2 ffi thus b4 b5 n lemma 4 1 0 exists delta 2 conditions b2 b3 satisfied proof let us consider convergence effi 0 0 exists delta 1 let us prove b2 b3 hold let e 34 b6 2and b2 holds 34 condition b3 holds b3 convergence algorithm proof let us prove theorem 6 ae 0 let us lemma 4 conditions b2 b3 satisfied lemma 3 exists n moreover convergence approximation scheme theorem 5 implies ae exists delta 2 sup thus finite discretized statespace sigma ffi sigma ffi satisfying properties section 44 exists n n n sup r methodes multigrilles en controle stochastique solutions de viscosite des equations de hamiltonjacobi vol volume17volume mathematiques et applications time problems optimal control vanishing viscosity solutions hamiltonjacobi equations comparison principle dirichlettype hamiltonjacobi equations singular perturbations degenerated elliptic equations convergence approximation schemes fully nonlinear second order equations neural networks control neuronlike adaptive elements solve difficult learning control problems dynamic programming simplification backpropagationthrough time algorithm optimal neurocontrol dynamic programming generalization reinforcement learning users guide viscosity solutions second order partial differential equations viscosity solutions hamiltonjacobi equations rates convergence approximation schemes optimal control controlled markov processes viscosity solutions fuzzy qlearning stable function approximation dynamic programming adaptive sparse grid multilevel methods elliptic pdes based finite differences reinforcement learning application control reinforcement learning applied differential game reinforcement learning survey numerical methods stochastic control problems continuous time reinforcement learning robots using neural networks automatic programming behaviorbased robots using reinforcement learning le dilemme explorationexploitation dans les systemes dapprentissage par renforcement variable resolution dynamic programming efficiently learning action maps multivariate realvalued statespaces partigame algorithm variable resolution reinforcement learning multidimensional state space general convergence theorem reinforcement learning continuous case gradient descent approaches neural netbased solutions hamiltonjacobibellman equation reinforcement learning continuous stochastic control problems barycentric interpolators continuous space time reinforcement learning variable resolution discretization highaccuracy solutions optimal control problems fuzzy reinforcement learning adaptive choice grid time reinforcement learning neural information processing systems mathematical theory optimal processes markov decision processes reinforcement learning soft state aggregation online learning random representations international conference machine learning simple statistical gradientfollowing algorithms connectionist reinforcement learning tr dynamic programming deterministic stochastic models numerical methods stochastic control problems continuous time connectionist learning control automatic programming behaviorbased robots using reinforcement learning simple statistical gradientfollowing algorithms connectionist reinforcement learning reinforcement learning application control numerical methods stochastic control problems continuous time reinforcement learning robots using neural networks partigame algorithm variable resolution reinforcement learning multidimensional statespaces rates convergence approximation schemes optimal control reinforcement learning continuous stochastic control problems adaptive choice grid time reinforcement learning barycentric interpolators continuous space myampersandamp time reinforcement learning markov decision processes neurodynamic programming finiteelement methods local triangulation refinement continuous reimforcement learning problems general convergence method reinforcement learning continuous case variable resolution discretization highaccuracy solutions optimal control problems dynamic programming ctr rmi munos andrew moore variable resolution discretization optimal control machine learning v49 n23 p291323 novemberdecember 2002