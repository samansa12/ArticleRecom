interiorpoint algorithm nonconvex nonlinear programming paper describes interiorpoint algorithm nonconvex nonlinear programming direct extension interiorpoint methods linear quadratic programming major modifications include merit function altered search direction ensure descent direction merit function obtained preliminary numerical testing indicates method robust numerical comparisons minos lancelot show method efficient promise greatly reducing solution times least classes models b introduction paper describe modifications used convert quadratic programming qp solver loqo general nonconvex nonlinear programming solver name code quadratic programming loqo implements interiorpoint method complete details qp implementation found 25 view dramatic success modern interiorpoint methods linear quadratic program ming guiding philosophy modify qp interiorpoint method loqo little possible make robust efficient general nonconvex nonlinear optimizer notational simplicity begin considering following nonlinear programming problem subject h x 0 1 x vector dimension n f x h x assumed twice continuously differentiable simplification general nonlinear programming problem include equality constraints bounds variables fact method developed paper implementation loqo handles cases efficiently exact way accomplished discussed detail later 1991 mathematics subject classification primary 90c30 secondary 49m37 65k05 key words phrases nonlinear programming interiorpoint methods nonconvex optimization research first author supported nsf grant ccr9403789 onr grant n000149810036 research second author supported afosr grant f496209510110 present consider version problem greatly simplifies terminol ogy extension general case quite straightforward interiorpoint approach taken paper described follows first add slack variables w constraints 1 reformulating problem subject hx w 0 2 hx w represent vectors elements respectively eliminate inequality constraints 2 placing barrier term resulting problem subject hx objective function classical fiaccomccormick 8 logarithmic barrier function lagrangian problem firstorder conditions minimum w w diagonal matrix elements vector ones hx jacobian matrix vector hx modify 5 multiplying second equation w producing standard primaldual system diagonal matrix elements note second equation implies nonnegative consistent fact vector lagrange multipliers associated originally inequality constraints basis numerical algorithm finding solution primaldual system 6 newtons method well known efficient linear convex quadratic programming order simplify notation time highlight connections linear quadratic programming introduce following definitions interiorpoint algorithm nonconvex nonlinear programming 3 newton system 6 x w 3 5 system 7 symmetric easily symmetrized multiplyingthe first equation 1 second equation w 1 yielding x w note measures primal infeasibility analogy linear programming refer dual infeasibility also note depend x w even though dont show dependence explicitly 8 small modification loqo solves iteration find search directions x w since second equation used eliminate w without producing offdiagonal fillin remaining system one normally elimination first hence w given resulting reduced kkt system given h x x algorithm proceeds iteratively initial point x 0 w 0 0 sequence points determined search directions described linear programming different steplengths k used primal dual directions whereas nonlinear programming common steplength employed 11 formulas search directions loqo solves reduced kkt system 12 nonetheless useful subsequent analysis give explicit formulas x w let denote dual normal matrix following theorem drop explicit indication dependence n f x w theorem 1 n nonsingular 8 unique solution particular 4 robert j vanderbei david f shanno remark formulas x w involve three terms cases first term viewed optimality direction second term centrality direction third term feasibility direction proof solving second block equations 12 eliminating first block equations yields system involving x whose solution using formula solve finally w resulting formula w definitions follows finally use formula eliminate 16 17 formulas 14 15 x w follow easily 12 critical features critical features algorithm choices k iteration modification system 8 order find local minimizer 1 linear convex quadratic programming modification linear system never required except possibly deal problems numerical accuracy step length iteration determined simple ratio test see example lustig marsten shanno 18 vanderbei 25 convex nonlinear programming linear system need modified method choosing k iteration becomes complex well known general convex nonlinear problems poor initial estimate newtons method may diverge order achieve convergence solution system 6 el bakry et al 7 introduced merit function showed proper choice exists sequence step lengths k 0 decreases monotonically iterates converge point provided jacobian system 6 remains nonsingular shanno simantiraki 21 tested variant algorithm hock schittkowski set test problems 16 found algorithm often efficient also often converged local maxima saddle points satisfy firstorder necessary conditions jacobian system 6 sometimes becomes singular causing algorithm fail thus became apparent better merit function way ensuring nonsingular jacobian would necessary successful general algorithm approach discussed detail next section 13 related work recently come attention lasdon et al 12 also studying basic algorithm algorithm differs many particulars general approach similar related work primaldual interiorpoint methods see 10 2 13 5 alternative barrier approach inequalityconstrained problems discussed 1 interiorpoint algorithm nonconvex nonlinear programming 5 2 algorithm modifications convex optimization mentioned previous section search directions given 8 together steplength selected simply ensure vectors w remain componentwise nonnegative yield efficient robust algorithm convex quadratic programming ever nonquadratic convex optimization problems reduction steplength may necessary guarantee convergence trivially illustrated unconstrained minimization univariate function f using initial estimate x 0 x 0 1 merit functions used guide one deciding much shorten steplength section describe specific merit function implemented 21 merit function merit functions equality constrained nonlinear programming subject great deal research past twentyodd years idea merit function ensure joint progress made toward local minimizer toward feasibility progress achieved shortening steplength along search directions defined 8 necessary sufficient reduction merit function made one possible merit function merit function exact means exists 0 0 minimizer 19 guaranteed feasible general conditions local minimizer original problem exactness useful property nondifferentiability 1 norms cause difficulties numerical algorithms merit function defined penalty function equality constrained nonlinear programming studied fiacco mccormick 8 differentiable everywhere however theoretical disadvantage requiring tend infinity ensure convergence feasible point hoped local minimizer original problem spite apparent disadvantage practice difficulty hence chosen use l 2 merit function 20 applied problems form 3 logw recall x hx following theorem shows among things large enough search directions defined 8 descent directions whenever problem strictly convex theorem 2 suppose dual normal matrix n positive definite search directions following properties w b x w 2 exists min 0 every min w x w cases equality holds x w satisfies 6 6 robert j vanderbei david f shanno proof easy see expressions x w given theorem 1 get w b x w assuming n positive definite see w b x w completes proof first property second property first note merit function barrier function plus constant times measure infeasibility address infeasibility term easy check w x w 2 explicit expression gradient infeasibility term follows x w combining inner products using barrier function infeasibility term get w x w suppose x w feasible 0 x w fail descent direction merit function necessary e case setting ensures x w descent direction every min interiorpoint algorithm nonconvex nonlinear programming 7 suppose x w feasible ie 0 case barrier functions inner product merit functions inner product agree furthermore either x w descent direction latter case use positive definiteness n conclude introducing dual variable e see last equation together assumed feasibility comprise precisely firstorder optimality conditions 5 barrier problem loqo initialized 0 unchanged long x w descent direction x w fails search direction calculated using algorithm proved quite satisfactory practice changed times calculations problems tried far many problems remains 0 may seem surprising recall search direction newton direction try find feasible firstorder point generally moves jointly toward minimizer feasible point ensure simply approach infeasible optimum slowly step along search vector becomes small increased factor 10 current version code starts next iteration new point increased value future work test efficiency gained simply using new old search directions thus step max chosen standard ratio test ensure nonnegative variables remain nonnegative context discussion section means must remain strictly positive interval 0 max searched successive halving using meritfunction evaluations value produces reduction merit function satisfying armijo condition conclude section two comments first discussion assumed n positive definite subsequent sections demonstrate algorithm modified relax assumption second method listed admittedly ad hoc convergence proof method kind certainly require carefully developed algorithm terms selection parameter yet preliminary testing method works surprisingly well attempts rigorous algorithms less successful thus viewed similarly recent method fletcher leyffer 9 essentially accepts point improves either optimality infeasibility much remains study experience date newtons method hindered little possible 3 algorithm modifications nonconvex optimization using merit function described previous section guide selection steplength together stabilization technique next section yields efficient robust algorithm convex optimization nonconvex optimization problems another important issue consider namely matrix nx 8 robert j vanderbei david f shanno xw may fail positive semidefinite section discuss impact indefiniteness describe algorithmic changes made address issue 31 search direction theorem 2 showed search directions x w desirable descent properties barrier function merit function provided n positive definite n indefinite algorithm might still converge something might local minimum problem one wishes solve consider example problem minimizing concave function 4x1 x subject bound constraint 0 x 1 algorithm presented far applied problem initialized 04 x 0 06 undesirable property converging global maximum 05 results suggest replacing h x definition search directions diagonal perturbation thereof chosen n positive definite theorem 2 applies ensures descent properties barrier merit functions course following question naturally arises impact perturbation important properties reduction primal dual infeasibility following theorem addresses question henceforth assume search directions computed using h instead h theorem 3 let proof begin last equality follows third block equations 8 analysis similar one must use first block equations 8 interiorpoint algorithm nonconvex nonlinear programming 9 finally analysis w follows closely analogous development linear pro gramming starts follows rewrite linear term follows substituting 24 23 get desired result linear programming usually chosen 0 1 therefore w decreases provided step length short enough however 0 theorem 3 shows dual infeasibility may fail decrease even arbitrarily small steps seems small price pay desirable properties even though dont proof convergence perturbed method empirical evidence suggests zero time using rules given shortly dual infeasibility eventually decrease zero iterates neighborhood optimal solution n positive definite nonlinear loqo pick 0 whenever necessary keep n positive defi nite guarantee algorithm convergent converges local minimum value computed follows first l dl factorization symmetric permutation reduced kkt matrix 12 permutation computed beginning based solely structurefillin considerations see 25 details h x positive definite reduced kkt matrix quasidefinite defined 24 hence shown 24 factorization guaranteed exist furthermore property every diagonal element associated original diagonal element must positive every element associated original diagonal element wy 1 must negative hence factoring scan diagonal elements looking elements wrong sign noting one largest magnitude let 0 denote largest magnitude diagonal elements correct sign must positive definite perturbation required otherwise initial perturbation trivial case 1 1 matrix perturbation guaranteed produce positive definite matrix however larger matrices guarantee therefore factor perturbed matrix check perturbation proves insufficient keep doubling perturbation found gives positive definite matrix hand initial perturbation sufficiently large successive halving find perturbation small finally double clear whether halving process necessary plan investigate issue future method described simple undoubtedly improved general worked well practice note w always positive h x ever needs modified 4 bounds ranges noted introduction general nonlinear programming problem may equality constraints well inequality constraints furthermore simple bounds variables may considered inequality constraints generally handled separately efficiency cases treated specifically quadratic solver loqo described 25 nonlinear modification continues accept formulations describe algorithmic issues briefly general form inequality constraint range constraint form easily converted system constraints equality constraints treated simply declaring range constraints r note increase number nonlinear constraints add linear constraint extra variable 23 vanderbei shows reduce resulting kkt system one identical size character 12 modifications diagonal matrix w 1 bounds variables handled similarly algorithm assumes variable x j possibly infinite upper lower bounds u j interiorpoint methods linear programming finite inequalities converted equalities adding slack variables resulting system equations reduced one exact form size 12 change addition diagonal matrix h x block reduced kkt matrix one note contrast lowerbound shift employed others treat lower bounds exactly way treat upper bounds symmetric treatment upper lower bounds first suggested gill et al 14 addition discussion section nonnegative variables added either range bound constraints must included logarithmic barrier term merit function extra linear equalities included penalty term thus final form reduced kkt system solved n n nonnegative diagonal matrix em positive diagonal matrix 1 2 appropriately modified righthand sides see 25 explicit expressions every variable either finite upper bound finite lower bound ie free variables e n positive diagonal matrix interiorpoint algorithm nonconvex nonlinear programming 11 41 free variables stabilization diagonal element nonnegative matrix e n introduced previous section positive corresponding variable finite upper lower bound zero variable free variable therefore things stand linear programming problems free variables reduced kkt matrix quasidefinite get quasidefinite matrix even case write free variable x j difference two nonnegative variables splitting free variables common trick next step usually eliminate problem instead retain free x j add constraint shown algebra reduce system usual reduced kkt system result exactly except e n strictly positive every component even associated free variables improves conditioning reduced kkt system thereby helps stabilize accuracy factorization course new nonnegative variables j g j must incorporated logarithmic barrier term merit function linear equations penalty term implicit handling free variables ensures matrix 25 quasidefinite whenever problem convex optimization problem particular whenever linear programming problem furthermore adding something positive h x increase likelihood system quasidefinite even problem nonconvex hence greatly reduce number times need add diagonal perturbations h x fact hock schittkowski test set report section 6 loqo took total 2174 newton iterations 3298 matrix factorizations average 152 factorizations per iteration previously stated search algorithm simplistic could lead significantly factorizations sophisticated version however results seem verify addition positive diagonal matrix e n semidefinite matrix xw greatly reduces number perturbations required full details computation e n modification righthand side well full details previous section reader referred 23 25 5 implementation details section provides implementation details nonlinear version loqo coded tested particular deal specific algorithmic choices next section give results computational testing comparison minos 20 lancelot 6 test set includes wellknown hock schittkowski problems 16 well several largescale realworld problems 51 choice barrier parameter far specified particular choice barrier parameter theoretical analyses interiorpoint algorithms choose 7 el bakry et al show choice algorithm converges zero merit function 18 computational experience shown algorithm performs best complementary products w approach zero uniform rate measure distance uniformity computing clearly w constant values far uniformity larger promotes uniformity next iteration consequently use following heuristic choice proven effective practice denotes steplength parameter default 095 settable scale factor defaults 01 52 initial point nonlinear programming traditionally requires starting point given part problem data comparative numerical testing done using traditional starting points interiorpoint code required however slack variables split variables added problem thus even initial values x j initial values w etc must determined program slack variables instance w given x 0 compute h two difficulties 26 first x 0 feasible 26 gives initial negative value w allowed interiorpoint methods second even x 0 feasible may lie boundary feasible region close initial w close zero progress hindered much interiorpoint methods linear programming necessary specify 0 initial values variables constrained nonnegative least large hence w initially set follows analogous method used set slacks range constraints used handle equality constraints also slack variables transform simple bounds equalities latter brings another interesting point standard test problems simple bounds variables give initial point violates bounds initial point seems unrealistic computationally dangerous sometimes bounds variables used ensure function value actually calculated enforcing appears somewhere calculations view added option code force bounds honored case initial point lies outside bounds altered lie inside bounds set exactly bound one bound upper lower bounds set 9010 mixture two bounds higher value placed nearer bound finally split parts associated free variable x set difference equal x j smaller two equal remaining point interest choice linear programming proved computationally efficient nonlinear problems tested date however proved far best choice good problemdependent choice remains topic study interiorpoint algorithm nonconvex nonlinear programming 13 53 issues issues stopping rules matrix reordering heuristics handled exactly manner documented 23 6 computational results loqo implementation described paper addition ability read industrystandard mps files expressing linear quadratic programming problems interfaced two popular mathematical programming languages ampl 11 gams 4 currently ampl provide solver secondorder information ie hessians therefore testing performed using ampl together loqo many solvers also interfaced ampl including minos 20 lancelot 6 tests described used loqo version 310 19971027 minos version 54 19940910 lancelot version 20031997 minos lancelot run using default options although increase number superbasics maximum number majorminor iterations larger problems tests performed r4600 sgi workstation 160 mbytes real mem kbytes data cache 133 mhz clock three components stopping rule loqo primal feasibility ii dual feasibility iii lack duality gap default rule declares solution primaldual feasible relative infeasibility less 10e 6 declares problem optimal addition 8 digits agreement primal dual objective function values 61 hock schittkowski suite table 1 shows solution times seconds hock schittkowski set test problems 16 table 2 shows many interiorpoint iterations loqo used solve problem solver run default parameters since loqo still development defaults might change future therefore list parameters might change give current default settings nonlinear problems mufactor called set 01 bndpush called set 100 honor bnds boolean flag indicating whether bounds variables enforced throughout iteration process opposed optimality set 1 ie true problems hock schittkowski set small linear pro gramming simplex method usually efficient interiorpoint methods small problems one would expect nonlinear programming expected minos simplexbased faster majority problems nonetheless pleasantly surprised see well loqo compared minos since times small direct time comparisons limited value summarized table 3 many times solver came first second third place finally computed total time loqo minos problems codes solved optimality total time loqo 219 seconds whereas minos 281 seconds noted minos uses first derivatives possible true newton variant minos might improve speed however experience date small dense problems coded ampl cost evaluating second derivatives significant may well offset improved algorithmic efficiency understanding default lancelot uses exact second derivatives 14 robert j vanderbei david f shanno time seconds time seconds time seconds name minos lancelot loqo name minos lancelot loqo name minos lancelot loqo 1 solution times hockschittkowski problems legend 1 could find feasible solution 2 erf available 3 step got small 4 many iterations 5 could code model ampl 6 unbounded badly scaled 7 core dump order interpret results table 1 note first hs067 run algorithm problem contains internal fixedpoint calculation difficult code ampl loqo failed converge one case remaining test set hs013 classic problem optimal point satisfy kkt condi tions loqo currently looks kkt points unable solve problem identifying optima satisfy kkt conditions well correctly determining unboundedness infeasibility remain topics study another point interest relatively high iteration count solution time loqo hs027 dramatically reduced increasing early iteration sequence methods tried date significantly help hs027 hurt enough problems less efficient overall thus optimal means choosing one hopes would part provably globally convergent algorithm also remains study interiorpoint algorithm nonconvex nonlinear programming 15 name iters name iters name iters name iters name iters 2 iteration counts hockschittkowski problems solver 1st 2nd 3rd loqo 44 3 rankings nonconvex problems often alternative optima certainly case problems hock schittkowski test set experiments loqo found alternative optima worse reported optima 8 problems hs002 hs016 better 7 problems hs047 hs088 hs089 finding optima worse generally ex pected finding optima better quite surprise reported hock schittkowski represent best optima known years experimentation test set suboptimal problems desired optimal solution could generally obtained altering thereby changing initial solution addition default value tried two values two problems noticeably less efficient others fails seven problems noticeably less efficient others name markowitz2 201 1200 201200 200 4m43s 1m56s 13m35s polygon2 195 42 766 880 07s 60m0s 1m1s sawpath 198 5 784 25 2s 5s 8s structure4 720 1536 5724 20356 2m40s 43m689s trafequil2 628 1194 5512 76 51s 53s 2m39s 4 preliminary computational results several application ar eas asterisk indicates solution obtained either infeasible suboptimal double asterisk indicates enough memory solve problem also run force variables remain within bounds ie honor bnds0 run three problems addition hs013 failed converge find surprising interiorpoint methods feel bounds honored beginning final note iteration counts table 2 represent total number times first second partial derivatives computed extra function evaluations used reducing merit function time taken calculate small compared components algorithm chose report execution times rather function counts 62 largescale realworld problems assembling collection realworld problems encoded ampl collection available first authors web site 22 table 4 gives brief summary problem statistics computational results larger problems encountered problems quite difficult table shows loqo efficient robust code large difficult problems compared solvers made naive attempts set parameters appropriate nondefault values codes clearly better able adjust parameters loqo codes note set time limit run one hour inability run lancelot due memory limitations structure4 result knowing adjust memory estimates made ampl invoking lancelot problems previously part standard test set give brief description interesting ones nondefault loqo parameters used noted problem descriptions 621 antenna array synthesis antenna important problem electrical engineering determine combine signals array antennas reinforce signal desired direction suppress signal undesired ones various formulations problem fall convex optimization details one formulation given 17 solve problem loqo set parameters follows sigfig5 bndpush2 convex inftol10e1 622 electrons sphere fekete2 given n electrons placed conducting sphere problem find distribution electrons minimizes total coulomb interiorpoint algorithm nonconvex nonlinear programming 17 potential problem nonconvex large number local minima problem solved 623 optimization markowitz2 markowitz model portfolio optimization seeks portfolio investments optimizes linear combination expected return expected risk risk modeled variance return problem markowitz2 separable convex quadratic formulation problem linear constraints expected reward risk computed using historical data 624 minimal surfaces minsurf given domain r 2 boundary data minimal surface problem find interpolation boundary data interior domain surface generated minimal surface area convex optimization problem 625 largest small polygon polygon2 given n problem find nsided polygon maximal area whose diameter exceed 1 problem sounds trivial graham showed 15 optimal hexagon regular hexagon solve polygon2 loqo set parameters follows convex 626 saw path tracking sawpath given list points describing center line wood piece problem fit best polynomial minimizes sum squares errors subject three sets side constraints 1 polynomial must go first point 2 polynomial must specified initial slope 3 radius curvature must never exceed given value problem comes f grondin 627 structural optimization structure4 structural optimization problem decide build structure minimize weight compliance subject constraint structure support one possible loadings various models convex others also models pinjointed trusslike structures others based finiteelement models problem structure4 find optimal bracket design obtained using convex finiteelement model corresponding trusslike model described 3 628 traffic equilibrium trafequil2 problem find flows network minimize nonlinear function called beckman objective 63 mittelmanns quadratic programming set service operations research community hans mittelmann done extensive testing optimization software summarize results 19 used ampl model generate number random feasible quadratic programming problems problems determined specifying values five parameters n p pf see 19 definitions used two versions random model generator one generates convex problems one results convex problems shown table 5 nonconvex problems results shown table 6 first table shows efficient interiorpoint methods compared methods convex quadratic programming problems regarding second table problems nonconvex inevitably different algorithms found different optima also encouraged fact loqo successfully solved six nine problems efficiently however loqo fail three problems reason algorithm eventually took small steps therefore failed make sufficient progress iteration limit reached merit reductions steplength incurred 100 20 200 11 34 28 4 3 293 98 14 500 100 1000 1 1 4304 811 85 1000 200 500 02 1 5247 1434 147 5 mittelmanns results convex quadratic programming numbers shown solvers solution times seconds loqo following parameter settings used bndpush100 honor bnds0 pred corr1 mufactor0 lp defaults next release loqo values defaults qps 3 37 19 500 100 1000 1 1 542 172 500 200 1000 1 1 462 1389 1000 200 2000 02 1 7106 1311 6 mittelmanns results nonconvex quadratic programming numbers shown solvers solution times seconds asterisk indicates solution obtained either infeasible sub optimal loqo following parameter settings used bnd push100 honor bnds0 pred corr1 mufactor0 short steps must arisen variables close bounds future research address identifying remedying phenomenon 7 conclusions aim work take existing interiorpoint code quadratic programming namely loqo modify little possible develop efficient code nonconvex nonlinear programming found essentially two changes needed 1 merit function ensure proper steplength control 2 diagonal pertubation hessian matrix ensure search directions descent directions problem convex interiorpoint algorithm nonconvex nonlinear programming 19 noted throughout paper algorithm documented herein represents first attempt interiorpoint code nonconvex nonlinear programming problems believe significant improvements made time however personally gratified somewhat surprised robust efficient initial code proved 8 acknowledgements wed like thank david gay much hard work providing automatic differentiation algorithms ampl compute gradients hessians 9 second author paper known olvi mangasarian approximately thirty years believe first met first four madison conferences mathematical programming held may 1970 four conferences organized first olvi ben rosen klaus ritter later olvi bob meyer steve robinson opinion contributed greatly development mathematical programming period considerable excitement great progress excitement perhaps greater olvi might wished first conference tear gas pervaded campus national guard omnipresent move sessions campus avoid antiwar protests since early days olvi remained close friends meeting regularly conferences wives shared many happy days visiting art museums attended sessions often wished wives around long enough taught olvis book new followed seminal work theorems alternative work student shi ping han sqp work proximal point algorithms complementarity recently detecting breast cancer name accomplishments great friend congenial host valued companion first author young new nonlinear programming wonderful way make acquaintance one icons field r globally convergent lagrangian barrier algorithm optimization general inequality constraints simple bounds primaldual algorithm minimizing nonconvexfunction subject bound linear equality constraints optimization methods truss geometry topology design gams users guide interior point algorithm large scale nonlinear programming lancelot fortran package largescale nonlinear optimization release formulation theory newton interiorpoint method nonlinear programming nonlinear programming sequential unconstrainted minimization tech niques nonlinear programming without penalty function ampl modeling language mathematical programming computational experience safeguarded barrier algorithm sparse nonlinear programming primaldual interior method nonconvex nonlinear pro gramming solving reduced kkt systems barrier methods linear quadratic programming largest small hexagon test examples nonlinear programmingcodes antenna array pattern synthesis via convex optimization interior point methods linear programming computational state art benchmarks optimization software minos 54 users guide loqo interior point code quadratic programming symmetric quasidefinite matrices linear programming foundations extensions tr formulation theory newton interiorpoint method nonlinear programming interior point potential reduction method constrained equations globally convergent lagrangian barrier algorithm optimization general inequality constraints simple bounds lancelot test examples nonlinear programming codes ctr alicia troncoso lora advances optimization prediction techniques realworld applications thesis ai communications v19 n3 p295297 august 2006 v adetola guay brief paper parameter convergence adaptive extremumseeking control automatica journal ifac v43 n1 p105110 january 2007 hande benson robert j vanderbei david f shanno interiorpoint methods nonconvex nonlinear programming filter methods merit functions computational optimization applications v23 n2 p257272 november 2002 sasan bakhtiari andr l tits simple primaldual feasible interiorpoint method nonlinear programming monotone descent computational optimization applications v25 n13 p1738 weipeng chen lui sha energyaware datacentric generic utility based approach wireless sensor networks proceedings third international symposium information processing sensor networks april 2627 2004 berkeley california usa hande benson arun sen david f shanno robert j vanderbei interiorpoint algorithms penalty methods equilibrium problems computational optimization applications v34 n2 p155182 june 2006 igor griva numerical experiments interiorexterior point method nonlinear programming computational optimization applications v29 n2 p173195 november 2004 richard h byrd jorge nocedal richard waltz feasible interior methods using slacks nonlinear optimization computational optimization applications v26 n1 p3561 october dapuzzo marino parallel computational issues interior point method solving large boundconstrained quadratic programming problems parallel computing v29 n4 p467483 01 april hande benson david f shanno exact primaldual penalty method approach warmstarting interiorpoint methods linear programming computational optimization applications v38 n3 p371399 december 2007 helmut maurer hans mittelmann optimization techniques solving elliptic control problems control state constraints part 1 boundary control computational optimization applications v16 n1 p2955 april 2000 hans mittelmann verification secondorder sufficient optimality conditions semilinear elliptic parabolic control problems computational optimization applications v20 n1 p93110 october 2001 l n vicente local convergence affinescaling interiorpoint algorithm nonlinear programming computational optimization applications v17 n1 p2335 oct 2000 silvia bonettini valeria ruggiero iterative methods solution symmetric indefinite kkt system computational optimization applications v38 n1 p325 september 2007 stefania bellavia benedetta morini global convergence enhancement classical linesearch interior point methods mcps journal computational applied mathematics v151 n1 p171199 1 february pa absil andr l tits newtonkkt interiorpoint methods indefinite quadratic programming computational optimization applications v36 n1 p541 january 2007 helmut maurer hans mittelmann optimization techniques solving elliptic control problems control state constraints part 2 distributed control computational optimization applications v18 n2 p141160 feb 1 2001 roman polyak nonlinear rescaling interior quadratic prox method convex optimization computational optimization applications v35 n3 p347373 november 2006 nicholas gould dominique orban philippe l toint galahad library threadsafe fortran 90 packages largescale nonlinear optimization acm transactions mathematical software toms v29 n4 p353372 december francisco facchinei giampaolo liuzzi stefano lucidi truncated newton method solution largescale inequality constrained minimization problems computational optimization applications v25 n13 p85122 silvia bonettini emanuele galligani valeria ruggiero inner solvers interior point methods large scale nonlinear programming computational optimization applications v37 n1 p134 may 2007 migdalas g toraldo v kumar nonlinear optimization parallel computing parallel computing v29 n4 p375391 01 april