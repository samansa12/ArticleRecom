cascade generalization using multiple classifiers increasing learning accuracy active research area paper present two related methods merging classifiers first method cascade generalization couples classifiers loosely belongs family stacking algorithms basic idea cascade generalization use sequentially set classifiers step performing extension original data insertion new attributes new attributes derived probability class distribution given base classifier constructive step extends representational language high level classifiers relaxing bias second method exploits tight coupling classifiers applying cascade generalization locally iteration divide conquer algorithm reconstruction instance space occurs addition new attributes new attribute represents probability example belongs class given base classifier implemented three local generalization algorithms first merges linear discriminant decision tree second merges naive bayes decision tree third merges linear discriminant naive bayes decision tree algorithms show increase performance compared corresponding single models cascade also outperforms methods combining classifiers like stacked generalization competes well boosting statistically significant confidence levels b introduction ability chosen classification algorithm induce good generalization depends appropriateness representation language express generalizations examples given task representation language standard decision tree dnf formalism splits instance space axisparallel hyperplanes representation language linear discriminant function set linear functions split instance space oblique hyper planes since different learning algorithms employ different knowledge representations search heuristics different search spaces explored diverse results obtained statistics henery 1997 refers rescaling method used classes overpredicted leading bias rescaling consists applying algorithms sequence output algorithm used input another algorithm aim would use estimated probabilities derived learning algorithm input second learning algorithm purpose produce unbiased estimate qc jw conditional probability class problem finding appropriate bias given task active research area consider two main lines research one hand methods try select appropriate algorithm given task instance schaf fers selection cross validation schaffer 1993 hand methods combine predictions different algorithms instance stacked generalization wolpert 1992 work presented near follows second line research instead looking methods fit data using single representation language present family algorithms generic name cascade generalization whose search space contains models use different representation languages cascade generalization performs iterative composition classifiers iteration classifier generated input space extended addition new attributes form probability class distributions obtained example generated classifier language final classifier language used high level generalizer language uses terms expressions language low level clas sifiers sense cascade generalization generates unified theory base theories generated earlier used form cascade generalization performs loose coupling classi fiers method applied locally iteration divideandconquer algorithm generating tight coupling classifiers method referred local cascade generalization implementation generates decision tree interesting relations multivariate trees brodley utgoff 1995 neural networks namely cascade correlation architecture fahlman generalization local cascade generalization described analyzed paper experimental study shows methodology usually improves accuracy decreases theory size statistically significant levels next section review previous work area multiple models section 3 present framework cascade generalization section 4 discuss strengths weaknesses proposed method comparison approaches multiple models section 5 perform empirical evaluation cascade generalization using uci data sets section 6 define new family multistrategy algorithms apply cascade generalization locally section 7 empirically evaluate local cascade generalization using uci data sets section 8 examine behavior cascade generalization providing insights works last section summarizes main points work discusses future research directions 2 related work combining classifiers voting common method used combine classifiers pointed ali pazzani 1996 strategy motivated bayesian learning theory stipulates order maximize predictive accuracy instead using single learning model one ideally use models hypothesis space vote hypothesis weighted posterior probability hypothesis given training data several variants voting method found machine learning literature uniform voting opinion base classifiers contributes final classification strength weighted voting base classifier weight associated could change time strengthens classification given classifier another approach combine classifiers consists generating multiple models several methods appear literature paper analyze biasvariance analysis kohavi wolpert 1996 methods mainly reduce variance bagging boosting methods mainly reduce bias stacked generalization metalearning 21 variance reduction methods breiman 1996 proposes bagging produces replications training set sampling replacement replication training set size original data examples appear others may appear replication training set classifier generated classifiers used classify example test set usually using uniform vote scheme boosting algorithm freund schapire 1996 maintains weight example training set reflects importance adjusting weights causes learner focus different examples leading different classifiers boosting iterative algorithm iteration weights adjusted order reflect performance corresponding classifier weight misclassified examples increased final classifier aggregates learned classifiers iteration weighted voting weight classifier function accuracy 22 bias reduction methods wolpert 1992 proposed stacked generalization technique uses learning two levels learning algorithm used determine outputs base classifiers combined original data set constitutes level zero data base classifiers run level level one data outputs base classifiers another learning process occurs using input level one data output final classification sophisticated technique cross validation could reduce error due bias chan stolfo 1995 present two schemes classifier combination arbiter combiner schemes based meta learning metaclassifier generated meta data built based predictions base classifiers arbiter also classifier used arbitrate among predictions generated different base classifiers training set arbiter selected available data using selection rule example selection rule select examples whose classification base classifiers cannot predict consistently arbiter together arbitration rule decides final classification based base predictions example arbitration rule use prediction arbiter base classifiers cannot obtain majority later chan stolfo 1995a framework extended using arbiterscombiners hierarchical fashion generating arbitercombiner binary trees skalak 1997 presents dissertation discussing methods combining classifiers presents several algorithms based stacked generalization able improve performance nearest neighbor classifiers brodley 1995 presents mcs hybrid algorithm combines single tree nodes univariate tests multivariate tests generated linear machines instance based learners node mcs uses set ifthen rules perform hillclimbing search best hypothesis space search bias given partition dataset set rules incorporates knowledge experts mcs uses dynamic search control strategy perform automatic model selection mcs builds trees apply different model different regions instance space 23 discussion results boosting bagging quite impressive using 10 iterations ie generating classifiers quinlan 1996 reports reductions error rate 10 19 quinlan argues techniques mainly applicable unstable classifiers techniques require learning system stable obtain different classifiers small changes training set analysis biasvariance decomposition error kohavi wolpert1996 reduction error observed boosting bagging mainly due reduction variance breiman 1996 reveals boosting bagging improve predictive accuracy learning algorithms unstable mentioned kohavi bauer 1998 main problem boosting seems robustness noise expected noisy examples tend mis classified weight increase examples present several cases performance boosting algorithms degraded compared original algorithms also point bagging improves datasets used experimental evaluation conclude although boosting average better bagging uniformly better bagging higher accuracy boosting bagging many domains due reduction bias boosting also found frequently higher variance bagging boosting bagging require considerable number member models rely varying data distribution get diverse set models single learning algorithm wolpert 1992 says successful implementation stacked generalization classification tasks black art conditions stacking works still unknown example currently hard fast rules saying level 0 generalizers use level 1 generalizer one use k numbers use form level 1 input space etc recently ting witten 1997 shown successful stacked generalization requires use output class distributions rather class predictions experiments mlr algorithm linear discriminant suitable 3 cascade generalization consider learning set multidimensional input vector yn output variable since focus paper classification problems yn takes values set predefined values yn 2 fcl 1 cl c g c number classes classifier function applied training set construct model generated model mapping input space x discrete output variable used predictor represented x assigns value example x traditional framework classification tasks framework requires predictor x outputs vector representing conditional probability distribution p1 pc p represents probability example x belongs class ie p jx class assigned example x one maximizes last expression commonly used classifiers naive bayes discriminant classify examples way classifiers eg c45 quinlan 1993 different strategy classifying example requires changes obtain probability class distribution define constructive operator x represents model training data x represents example example x operator concatenates input vector x output probability class dis tribution operator applied examples dataset 0 obtain new dataset 00 cardinality 00 equal cardinality 0 ie number examples example x 2 00 equivalent example 0 augmented c new attributes c represents number classes new attributes elements vector class probability distribution obtained applying classifier example x represented formally follows ad 0 represents application model data set 0 represents effect dataset dataset contains examples appear 0 extended probability class distribution generated model cascade generalization sequential composition classifiers generalization level applies phi operator given training set l test set two classifiers generalization proceeds follows using generates level 1 data level level learns level 1 training data classifies level 1 test data steps perform basic sequence cascade generalization classifier classifier 1 represent basic sequence symbol r previous composition could represented succinctly applying equations 2 3 equivalent simplest formulation cascade generalization possible extensions include composition n classifiers parallel composition classifiers composition n classifiers represented case cascade generalization generates n1 levels data final model one given n classifier model could contain terms form conditions based attributes build previous built classifiers variant cascade generalization includes several algorithms parallel could represented formalism run parallel operator returns new data set l 0 contains number examples l example l 0 contains n gamma 1 theta cl new attributes cl number classes algorithm set contributes cl new attributes 31 illustrative example example consider uci blake keogh merz 1999 data set monks2 monks data sets describe artificial robot domain quite well known machine learning community robots described six different attributes classified one two classes chosen monks2 problem known difficult task systems learn decision trees attributevalue formalism decision rule problem robot ok exactly two six attributes first value problem similar parity problems combines different attributes way makes complicated describe dnf cnf using given attributes examples original training data presented head body smiling holding color tie class round round yes sword red yes ok round round balloon blue ok using tenfold cross validation error rate c45 329 naive bayes 342 composite model c45 naive bayes c45rnaivebayes operates follows level 1 data generated using naive bayes classifier naive bayes builds model original training set model used compute probability class distribution example training test set level 1 obtained extending train test set probability class distribution given naive bayes examples shown earlier take form head body smiling holding color tie pok pnot ok class round round yes sword red yes 0135 0864 ok round round balloon blue 0303 0696 ok new attribute pok pnot ok probability example belongs class oknot ok c45 trained level 1 training data classifies level 1 test data composition c45rnaivebayes obtains error rate 89 substantially lower error rates c45 naive bayes none algorithms isolation capture underlying structure data case cascade able achieve notable increase performance figure 1 presents one trees generated c45rnaivebayes tree contains mixture original attributes smiling tie new attributes constructed naive bayes pok pnot ok root tree appears attribute pok attribute represents particular class probability class ok calculated naive bayes decision tree generated c45 uses constructed attributes given naive bayes redefining different thresholds two class problem bayes rule uses p ok threshold 05 decision tree sets threshold 027 decision nodes kind function given bayes strategy exam ple attribute pok seen function computes pclass okjx using bayes theorem branches decision tree performs one test class probabilities certain sense decision tree combines two representation languages naive bayes language decision trees constructive step performed cascade inserts new attributes incorporate new knowledge provided naive bayes new knowledge allows significant increase performance verified decision tree despite tie smiling smiling ok tie smiling ok pnot ok pnot ok pnot ok ok pnot ok pnot ok ok ok ok ok ok figure 1 tree generated c45rbayes fact naive bayes cannot fit well complex spaces cascade framework lower level learners delay decisions high level learners kind collaboration classifiers cascade generalization explores 4 discussion cascade generalization belongs family stacking algorithms wolpert 1992 defines stacking generalization general framework combining clas sifiers involves taking predictions several classifiers using predictions basis next stage classification cascade generalization may regarded special case stacking generalization mainly due layered learning structure aspects make cascade generalization novel ffl new attributes continuous take form probability class distribution combining classifiers means categorical classes looses strength classifier prediction use probability class distributions allows us explore information ffl classifiers access original attributes new attribute built lower layers considered exactly way original attributes ffl cascade generalization use internal cross validation aspect affects computational efficiency cascade many ideas discussed literature ting 1997 used probability class distributions level1 attributes use original attributes possibility using original attributes class predictions level 1 attributes pointed wolpert original paper stacked gener alization skalak 1997 refers schaffer used original attributes class predictions level 1 attributes disappointing results view could explained fact combines three algorithms similar behavior biasvariance analysis decision trees rules neuralnetworks see section 82 details point chan stolfo 1995a used original attributes class predictions scheme denoted classattribute combiner mixed results exploiting aspects makes cascade generalization succeed particular combination implies conceptual differences ffl stacking parallel nature cascade sequential effect intermediate classifiers access original attributes plus predictions low level classifiers interesting possibility explored paper provide classifier n original attributes plus predictions provided classifier ffl ultimate goal stacking generalization combining predictions goal cascade generalization obtain model use terms representation language lower level classifiers ffl cascade generalization provides rules choose low level classifiers high level classifiers aspect developed following sections 5 empirical evaluation 51 algorithms ali pazzani 1996 tumer gosh 1995 present empirical analytical results show combined error rate depends error rate individual classifiers correlation among suggest use radically different types classifiers reduce correlation errors criterion selecting algorithms experimental work use three classifiers different behaviors naive bayes linear discriminant decision tree 511 naive bayes bayes theorem optimally predicts class unseen example given training set chosen class one maximizes pc px attributes independent pxjci decomposed product px 1 show procedure surprisingly good performance wide variety domains including many clear dependencies attributes implementation algorithm required probabilities estimated training set case nominal attributes use counts continuous attributes discretized equal size intervals found produce better results assuming gaussian distribution domingos pazzani 1997 j dougherty r kohavi sahami 1995 number bins used function number different values observed training set lognr different values heuristic used dougherty et al 1995 good overall results missing values treated another possible value attribute order classify query point naive bayes classifier uses available attributes langley 1996 states naive bayes relies important assumption variability dataset summarized single probabilistic description sufficient distinguish classes analysis biasvariance implies naive bayes uses reduced set models fit data result low variance data cannot adequately represented set models obtain large bias 512 linear discriminant linear discriminant function linear composition attributes maximizes ratio betweengroup variance withingroup variance assumed attribute vectors examples class c independent follow certain probability distribution probability density function f new point attribute vector x assigned class probability density function f x maximal means points class distributed cluster centered boundary separating two classes hyperplane michie spiegelhalter taylor 1994 two classes unique hyperplane needed separate classes general case q classes needed separate applying linear discriminant procedure described get hyperplanes equation hyperplane given use singular value decomposition svd compute gamma1 svd numerically stable tool detecting sources collinearity last aspect used method reducing features linear combination linear discriminant uses almost available attributes classifying query point breiman 1996 states analysis biasvariance linear discriminant stable classifier achieves stability limited set models fit data result low variance data cannot adequately represented set models obtain large bias 513 decision tree dtree version univariate decision tree uses standard algorithm build decision tree splitting criterion gain ratio stopping criterion similar c45 pruning mechanism similar pessimistic error c45 dtree uses kind smoothing process usually improves performance tree based classifiers classifying new example example traverses tree root leaf dtree example classified taking account class distribution leaf also class distributions nodes path nodes path contribute final classification instead computing class distribution paths tree classification time done buntine 1990 dtree computes class distribution nodes growing tree done recursively taking account class distributions current node predecessor current node using recursive bayesian update formula pearl 1988 p e n probability one example falls node n seen shorthand p e 2 en e represents given example en set examples node n similarly p e n1 je n probability one example falls node n goes node n1 p e n1 je probability one example class c goes node n node n1 recursive formulation allows dtree compute efficiently required class distributions smoothed class distributions influence pruning mechanism treatment missing values relevant difference c45 decision tree uses subset available attributes classify query point breiman 1996 among researchers note decision trees unstable classifiers small variations training set cause large changes resulting predictors high variance fit kind data bias decision tree low 52 experimental methodology chosen 26 data sets uci repository previously used comparative studies estimate error rate algorithm given dataset use 10 fold stratified cross validation minimize influence variability training set repeat process ten times time using different permutation dataset 1 final estimate mean error rates obtained run cross validation iteration cv algorithms trained training partition data classifiers also evaluated test partition data algorithms used default settings comparisons algorithms performed using paired ttests significance level set 999 dataset use wilcoxon matchedpairs signedranks test compare results algorithms across datasets goal empirical evaluation show cascade generalization plausible algorithms compete quite well well established techniques stronger statements done extensive empirical evaluation table 1 data characteristics results base classifiers dataset classes examples dtree bayes discrim c45 c50 australian 2 690 1413sigma06 1448sigma04 1406sigma01 1471sigma06 1417sigma07 balance 3 625 2235sigma07 banding 2 238 2135sigma13 2324sigma12 2320sigma14 2398sigma18 2416sigma14 diabetes 2 768 2646sigma07 german 2 1000 2793sigma07 glass 6 213 3014sigma24 ionosphere iris 3 150 467sigma09 427sigma06 letter 26 20000 satimage 6 6435 1347sigma02 segment 7 vehicle 4 846 votes table 1 presents error rate standard deviation base classifier relative algorithm gamma sign first column means error rate algorithm significantly better worse dtree error rate c50 presented reference results provide evidence single algorithm better overall 53 evaluation cascade generalization table 2 3 presents results pairwise combinations three base classifiers promising combination three models column corresponds cascade generalization combination combination conducted paired ttests composite models compared components using paired ttests significance level set 999 gamma signs indicate combination eg c4rbay significantly better component algorithms ie c45 bayes results summarized tables 4 5 first line shows arithmetic mean across datasets shows promising combinations c45rdiscrim c45rnaive bayes c45rdiscrimrnaive bayes c45rnaive table 2 results cascade generalization composite models compared components bayrbay bayrdis bayrc45 disrdis disrbay disrc45 australian 1469sigma05 1361sigma02 balance 706sigma11 banding 2236sigma09 2199sigma08 1876sigma12 2328sigma14 2201sigma16 breast credit 1491sigma04 1335sigma03 1397sigma06 1422sigma01 1359sigma04 1434sigma03 diabetes german glass heart ionosphere 976sigma07 914sigma03 857sigma08 1338sigma08 iris letter segment sonar 2559sigma14 2372sigma11 2184sigma20 2481sigma12 vehicle votes 1000sigma03 bayesrdiscrim confirmed second line shows geometric mean third line shows average rank base cascading algorithms computed dataset assigning rank 1 accurate algorithm rank 2 second best remaining lines compares cascade algorithm toplevel algorithm fourth line shows number datasets toplevel algorithm accurate corresponding cascade algorithm versus number less fifth line considers datasets error rate difference significant 1 level using paired ttests last line shows pvalues obtained applying wilcoxon matchedpairs signedranks test statistics show promising combinations use decision tree highlevel classifier naive bayes discrim lowlevel classifiers new attributes built discrim naive bayes express relations attributes outside scope dnf algorithms like c45 new attributes systematically appear root composite models one main problems combining classifiers algorithms combine empirical evaluation suggests ffl combine classifiers different behavior biasvariance analysis table 3 results cascade generalization composite models compared components dataset c45rc45 c45rdis c45rbay c45rdiscrbay c45rbayrdisc stacked gen australian 1474sigma05 1399sigma09 1541sigma08 1424sigma05 1534sigma09 1399sigma04 balance banding 2377sigma17 2173sigma25 2275sigma18 2148sigma20 2218sigma15 2145sigma12 breast credit 1421sigma06 1385sigma04 1507sigma07 1484sigma04 1375sigma06 diabetes german glass 3202sigma24 3609sigma18 3360sigma16 3468sigma18 3511sigma25 3128sigma19 hepatitis ionosphere 1021sigma13 iris letter segment 321sigma02 sonar 2802sigma32 2475sigma29 2436sigma19 2445sigma18 2383sigma21 2481sigma10 votes table 4 summary results cascade generalization measure bayes bayrbay bayrdis bayrc4 disc discrdisc discrbay discrc4 arithmetic mean 1762 1729 1642 1529 1780 1772 1639 1794 geometric mean 1331 1272 1261 1071 1397 1377 1214 1482 average rank 967 946 663 752 906 877 723 1029 nr wins gamma 1412 test ffl low level use algorithms low variance ffl high level use algorithms low bias cascade framework lower level learners delay final decision high level learners selecting learners low bias high level able fit complex decision surfaces taking account stable surfaces drawn low level learners table 5 summary results cascade generalization measure c45 c45rc45 c45rbay c45rdis c45rdisrbay c45rbayrdis arithmetic mean 1598 1598 1344 1419 1309 1327 geometric mean 1140 1120 825 993 795 781 average rank 983 904 785 617 646 669 nr wins gamma 715 419 1115 818 818 test table 6 summary comparison stacked generalization c45rdiscrbay vs stackg c45rbayrdisc vs stackg number wins 11 significant wins 6 5 6 4 test given equal performance would prefer fewer component classifiers since train ing application times lower smaller number components larger number components also adverse affects comprehensibility study version three components seemed perform better version two components research needed establish limits extending scenario 54 comparison stacked generalization compared various versions cascade generalization stacked gener alization defined ting 1997 reimplementation stacked generalization level 0 classifiers c45 bayes level 1 classifier discrim attributes level 1 data probability class distributions obtained level 0 classifiers using 5fold stratified crossvalidation 2 table 3 shows last column results stacked generalization stacked generalization compared using paired ttests c45rdiscrimrnaive bayes c45rnaive bayesrdiscrim order gamma sign indicates dataset cascade model performs significantly better worse table 6 presents summary results provide evidence generalization ability cascade generalization models competitive stacked generalization computes level 1 attributes using internal crossvalidation use internal crossvalidation affects course learning times cascade models least three times faster stacked generalization cascade generalization exhibits good generalization ability computationally efficient aspects lead hypothesis improve cascade generalization 6by applying iteration divideandconquer algorithm hypothesis examined next section 6 local cascade generalization many classification algorithms use divide conquer strategy resolve given complex problem dividing simpler problems applying recursively strategy subproblems solutions subproblems combined yield solution original complex problem basic idea behind well known decision tree based algorithms id3 quinlan 1984 cart breiman et al 1984 assistant kononenko et al 1987 c45 quin lan 1993 power approach derives ability split hyper space subspaces fit subspace different functions section explore cascade generalization problems subproblems divide conquer algorithm generates intuition behind proposed method behind divide conquer strategy relations captured global level discovered simpler subproblems following sections present detail apply cascade generalization locally develop strategy decision trees although possible use conjunction divide conquer method like decision lists rivest 1987 61 local cascade generalization algorithm generalization composition classification algorithms elaborated building classifier given task iteration divide conquer algorithm local cascade generalization extends dataset insertion new attributes new attributes propagated subtasks paper restrict use local cascade generalization decision tree based algorithms however possible use divideandconquer algorithm figure 2 presents general algorithm local cascade generalization restricted decision tree method referred cgtree growing tree new attributes computed decision node applying phi operator new attributes propagated tree number new attributes equal number classes appearing examples node number vary different levels tree general deeper nodes may contain larger number attributes parent nodes could disadvantage however number new attributes generated decreases rapidly tree grows classes discriminated deeper nodes also contain examples decreasing number classes means tree grows number new attributes decreases order applied predictor cgtree must store node model generated base classifier using examples node classifying new example example traverses tree usual way input data set classifier output decision tree function cgtreed stopping return leaf class probability distribution else choose attribute maximizes splitting criterion 0 partition examples based values attribute generate subtree ree return tree containing decision node based attribute ai storing descendant subtrees ree endif end figure 2 local cascade algorithm based decision tree decision node extended insertion probability class distribution provided base classifier predictor node framework local cascade generalization developed cgltree uses phid adiscrimd operator constructive step internal node cgltree constructs discriminant function discriminant function used build new attributes example value new attribute computed using linear discriminant function decision node number new attributes built cgltree always equal number classes taken examples node order restrict attention well populated classes use following heuristic consider class number examples node belonging class greater n times number attributes 3 default n 3 implies different nodes different number classes considered leading addition different number new attributes another restriction use constructive operator ad error rate resulting classifier less 05 training data empirical study used two algorithms apply cascade generalization locally first one cgbtree uses constructive operator second one cgbltree uses constructive operator aspects algorithms similar cgltree one restriction application phid induced classifier must return corresponding probability class distribution x 2 0 classifier satisfies requisites could applied possible imagine cgtree whose internal nodes trees example small modifications c45 4 enables construction cgtree whose internal nodes trees generated c45 62 illustrative example bayes7 bayes11 3 bayes7 3 ok ok ok ok figure 3 tree generated cgtree using discrimrbayes constructive operator figure 3 represents tree generated cgtree monks2 problem constructive operator used phid discrimrbayesx root tree naive bayes algorithm provides two new attributes bayes 7 bayes 8 linear discriminant uses continuous attributes two continuous attributes built naive bayes case coefficients linear discriminant shrink zero process variable elimination used discriminant algorithm gain ratio criterion chooses bayes 7 attribute test dataset split two partitions one contains examples class ok leaf generated partition two new bayes attributes built bayes 11 bayes 12 linear discriminant generated based two bayes attributes built root tree attribute based linear discriminant chosen test attribute node dataset segmented process tree construction proceeds example illustrate two points ffl interactions classifiers linear discriminant contains terms built naive bayes whenever new attribute built considered regular attribute attribute combination built deeper nodes contain terms based attributes built upper nodes ffl reuse attributes different thresholds attribute bayes 7 built root used twice tree different thresholds 63 relation work multivariate trees respect final model clear similarities cgltree multivariate trees brodley utgoff langley refers multivariate tree topologically equivalent threelayer inference network constructive ability system similar cascade correlation learning architecture fahlman lebiere 1991 also final model cgbtree related recursive naive bayes presented langley interesting feature unifies single framework several systems different research areas previous work gama brazdil 1999 compared system ltree similar cgltree oc1 murthy et al lmdt brodley et al cart breiman et al focus paper methodologies combining classifiers compare algorithms methods generate combine multiple models 7 evaluation local cascade generalization section evaluate three instances local cascade algorithms cgbtree cgltree cgbltree compare local versions corresponding global models two standard methods combine classifiers boosting stacked generalization implemented local cascade generalization algorithms based dtree use exactly splitting criteria stopping criteria pruning mechanism moreover share many minor heuristics individually small mention collectively make difference decision node cgltree applies linear discriminant described cg btree applies naive bayes algorithm cgbltree applies linear discriminant ordered attributes naive bayes categorical attributes order prevent overfitting construction new attributes constrained depth 5 addition level pruning greater level pruning dtree table 7a presents results local cascade generalization column corresponds local cascade generalization algorithm algorithm compared similar cascade model using paired ttests example cgltree compared c45rdiscrim gamma sign means error rate composite model statistically significant levels lower higher correspondent model table 8 presents comparative summary results local cascade generalization corresponding global models illustrates benefits applying cascade generalization locally table 7 results alocal cascade generalization bboosting stacked c boosting cascade algorithm second row indicates models used comparison dataset cgbtree cgltree cgbltree c50boost stacked c5brbayes vs corresponding cascade models vs cgbltree vs c50boost adult 1346sigma04 1356sigma03 1352sigma04 gamma 1433sigma04 1396sigma06 1441sigma05 australian balance 532sigma11 banding 2098sigma12 2360sigma12 2069sigma12 credit 1535sigma05 1441sigma08 1452sigma08 1341sigma08 1343sigma06 1357sigma09 diabetes german glass ionosphere 962sigma09 1106sigma06 1100sigma07 iris 473sigma13 280sigma04 letter mushroom sonar 2623sigma17 vehicle votes 329sigma04 430sigma05 system cgbltree compared c50boosting variance reduction method 5 stacked generalization bias reduction method table 7b presents results c50boosting default parameter 10 aggregating trees stacked generalization defined ting 1997 described earlier section boosting stacked compared cgbltree using paired ttests significance level set 999 gamma sign means boosting stacked performs significantly better worse cgbltree study cgbltree performs significantly better stacked 6 datasets worse 2 datasets 71 step ahead comparing c50boosting cgbltree significantly improves 10 datasets loses 9 datasets interesting note 26 datasets 19 significant differences evidence boosting cascade different behavior improvement observed boosting applied decision table 8 summary results local cascade generalization cgbtree cgltree cgbltree c50boost stacked g c5brbayes arithmetic mean 1343 1398 1292 1325 1387 1163 geometric mean 870 946 820 881 1013 608 average rank 390 392 329 327 350 312 c45rbay c45rdis c4rbayrdis cgbltree cgbltree vs vs vs vs vs cgbtree cgltree cgbltree c50boost stacked g number wins 1016 1214 719 1313 1511 significant wins 33 35 38 109 62 test tree mainly due reduction variance component error rate cascade algorithms improvement mainly due reduction bias component table 7c presents results boosting cascade algo rithm case used global combination c50 boostrnaive bayes improves c50boosting 4 datasets loses 3 summary results presented table 8 evidence promising result intend near future boost cgbltree 72 number leaves another dimension comparisons involves measuring number leaves corresponds number different regions instance space partitioned algorithm consequently seen indicator model complexity almost datasets 6 cascade tree splits instance space half regions needed dtree c50 clear indication cascade models capture better underlying structure data 73 learning times learning time dimension comparing classifiers comparisons less clear results may strongly depend implementation details well underlying hardware however least order magnitude time complexity useful indicator c50 c50boosting run sparc 10 machine 7 algorithms run pentium 166mhz 32 mb machine linux table 9 presents average time needed algorithm run datasets taking time naive bayes reference results demonstrate cgtree faster c50boosting c50boosting slower generates 10 trees increased complexity also cgtree faster stacked generalization due internal cross validation used stacked generalization table 9 relative learning times base composite models bayes discrim c45 bayrdis disrdis c50 dtree bayrbay disrbay bayrc4 disrc4 c4rdis c4rc4 c4rbay cgbtree c4rdisrbay cgltree cgbltree c50boost stacked 41 455 481 670 685 772 1108 1516 1529 8 cascade generalization improve performance cascade generalization local cascade generalization transforms instance space new highdimensional space principle could turn given learning problem difficult one phenomenon known curse dimensionality section analyze behavior cascade generalization three dimensions error correlation biasvariance analysis mahalanobis distances 81 correlation ali pazzani 1996 shown desirable property ensemble classifiers diversity use concept error correlation metric measure degree diversity ensemble definition error correlation two classifiers defined probability make error definition satisfy property correlation object 1 prefer define error correlation two classifiers conditional probability two classifiers make error given one makes error definition error correlation lies interval 0 1 correlation one classifier 1 formula use provides higher values one used ali pazzani expected lowest degree correlation decision trees bayes decision trees discrim use different representation languages error correlation bayes discrim little higher despite similarity two algorithms use different search strategies results provide evidence decision tree discriminant function make uncorrelated errors classifier make errors different regions instance space desirable property combining classifiers table 10 error correlation base classifiers c4 vs bayes c4 vsdiscrim bayes vs discrim average 032 032 040 82 biasvariance decomposition biasvariance decomposition error tool statistics theory analyzing error supervised learning algorithms basic idea consists decomposing expected error three components x compute terms bias variance zeroone loss functions use decomposition proposed kohavi wolpert 1996 bias measures closely average guess learning algorithm matches target computed x 2 variance measures much learning algorithms guess bounces around different sets given size computed variance x 2 estimate bias variance first split data training test sets training set obtain ten bootstrap replications used build ten classifiers ran learning algorithm training sets estimated terms variance equation 7 bias 8 equation 6 using generated classifier point x evaluation set e terms estimated using frequency counts base algorithms used experimental evaluation different behavior biasvariance analysis decision tree known low bias high variance naive bayes linear discriminant known low variance high bias experimental evaluation shown promising combinations use decision tree high level classifier naive bayes linear discriminant low level classifiers illustrate results measure bias variance c45 naive bayes c45rnaive bayes datasets study results shown figure 4 summary results presented table figure 4 biasvariance decomposition error rate c45 bayes c45rbayes different datasets table 11 bias variance decomposition error rate bayes c45rbayes average variance 48 159 472 average bias 1153 1519 864 11 benefits cascade composition well illustrated datasets like balancescale hepatitis monks2 waveform satimage comparison bayes c45rbayes shows latter combination obtain strong reduction bias component costs increasing variance component c45rbayes reduces bias variance compared c45 reduction error mainly due reduction bias 83 mahalanobis distance consider class defines single cluster 9 euclidean space class centroid corresponding cluster defined vector attribute means x computed examples class shape cluster given covariance matrix using mahalanobis metric define two distances 1 withinclass distance defined mahalanobis distance example centroid cluster computed figure 5 average increase betweenclass distance x represents example attribute vector denotes centroid cluster corresponding class covariance matrix class 2 betweenclasses distance defined mahalanobis distance two clusters computed pooled denotes centroid cluster corresponding class pooled pooled covariance matrix using j intuition behind withinclass distance smaller values leads compact clusters intuition behind betweenclasses distance larger values would lead us believe groups sufficiently spread terms separation means measured betweenclasses distance withinclass distance datasets numeric attributes distances measured original dataset dataset extended using cascade algorithm observe withinclass distance remains almost constant classes distance increases example using constructive operator discrimrbay betweenclasses distance almost doubles figure 5 shows average increase betweenclass distance respect original dataset extending using discrim bayes discrimrbayes respectively 9 conclusions future work paper provides new general method combining learning models means constructive induction basic idea method use learning algorithms sequence iteration two step process occurs first model built using base classifier second instance space extended insertion new attributes generated built model given example constructive step generates terms representational language base classifier high level classifier chooses one terms representational power extended bias restrictions high level classifier relaxed incorporating terms representational language base classifiers basic idea behind cascade generalization architecture examined two different schemes combining classifiers first one provides loose coupling classifiers second one couples classifiers tightly 1 loose coupling base classifiers preprocess data another stage framework used combine existing classifiers without changes rather small changes method requires original data extended insertion probability class distribution must generated base classifier 2 tight coupling local constructive induction framework two classifiers coupled locally although work used local cascade generalization conjunction decision trees method could easily extended divideandconquer systems decision lists existing methods bagging boosting combine learned models use voting strategy determine final outcome although leads improvements accuracy strong limitations loss interpretability models easier interpret particularly classifiers loosely coupled final model uses representational language high level classifier possibly enriched expressions representational language low level classi fiers cascade generalization applied locally models generated difficult interpret generated loosely coupled classifiers new attributes built deeper nodes contain terms based previously built attributes allows us built complex decision surfaces affects somewhat interpretability final model using powerful representations necessarily lead better results introducing flexibility lead increased instability variance needs controlled local cascade generalization achieved limiting depth applicability constructive operator requiring error rate classifier used constructive operator less 05 one interesting feature local cascade generalization provides single framework collection different methods method related several paradigms machine learning example similarities multivariate trees brodley utgoff 1995 neural networks fahlman lebiere 1990 recursive bayes langley 1993 multiple models namely stacked generalization wolpert 1992 previous work gama brazdil 1999 presented system ltree combines decision tree discriminant function means constructive induction local cascade combinations extend work ltree constructive operator single discriminant function local cascade composition restriction laxed use classifier constructive operator moreover composition several classifiers like cgbltree could used unified framework useful overcomes superficial distinctions enables us study fundamental ones practical perspective users task simplified aim achieving better accuracy achieved single algorithm instead several ones done efficiently leading reduced learning times shown methodology improve accuracy base classifiers competing well methods combining classifiers preserving ability provide single albeit structured model data 91 limitations future work open issues could explored future involve ffl perspective biasvariance analysis main effect proposed methodology reduction bias component possible combine cascade architecture variance reduction method like bagging boosting ffl cascade generalization work classifiers could use neural networks nearest neighbors think methodology presented work type classifier intend verify empirically future problems involve basic research include ffl cascade generalization improve performance experimental study suggests combine algorithms complementary behavior point view biasvariance analysis forms complementarity considered example search bias one interesting issue explored given dataset predict algorithms complementary ffl cascade generalization improve performance datasets cascade able improve performance base classifiers characterize datasets predict circumstances cascade generalization lead improvement performance ffl many base classifiers use general preference smaller number base classifiers circumstances reduce number base classifiers without affecting performance ffl cascade generalization architecture provides method designing algorithms use multiple representations multiple search strategies within induction algorithm interesting line future research explore flexible inductive strategies using several diverse representations possible extend local cascade generalization provide dynamic control make step direction acknowledgments gratitude expressed financial support given feder praxis xxi project eco plurianual support attributed liacc esprit ltr project thanks also pedro domingos anonymous reviewers colleagues liacc valuable comments notes 1 except case adult letter datasets single 10fold crossvalidation used 2 also evaluated stacked generalization using c45 top level version used somewhat better using c45 top level average mean error rate 1514 3 heuristic suggested breiman et al 1984 4 two different methods presented ting 1997 gama 1998 5 preferred c50boosting instead bagging available us allows crosschecking results differences results previous published quinlan think may due different methods used estimate error rate 6 except monks2 dataset dtree c50 produce tree one leaf 7 running time c50 c50boosting reduced factor 2 suggested wwwspecorg 8 intrinsic noise training dataset included bias term 9 analysis assumes single dominant class cluster although may always satisfied give insights behavior cascade composition r reduction learning multiple descriptions empirical comparison voting classification algorithms bagging uci repository machine learning databases arcing classifiers classification regression trees wadsworth international group recursive automatic bias selection classifier construction multivariate decision trees multivariate analysis optimality simple bayesian classifier zeroone loss supervised unsupervised discretization continuous features recurrent cascadecorrelation architecture experiments new boosting algorithm combining classifiers constructive induction linear tree combining classification procedures induction recursive bayesian classifiers elements machine learning machine learning machine learning system induction oblique decision trees journal artificial intelligence research probabilistic reasoning intelligent systems networks plausible inference learning decision lists selecting classification method crossvalidation prototype selection composite nearest neighbor classifiers stacked generalization work correlation error reduction ensemble classifiers connection science stacked generalization tr probabilistic reasoning intelligent systems networks plausible inference recurrent cascadecorrelation architecture stacked generalization c45 programs machine learning bitechnical noteib multivariate decision trees elements machine learning recursive automatic bias selection classifier construction reduction learning multiple descriptions prototype selection composite nearest neighbor classifiers optimality simple bayesian classifier zeroone loss machine learning empirical comparison voting classification algorithms learning decision lists induction decision trees induction recursive bayesian classifiers combining classifiers constructive induction ctr robert munro daren ler jon patrick metalearning orthographic contextual models language independent named entity recognition proceedings seventh conference natural language learning hltnaacl 2003 p192195 may 31 2003 edmonton canada csar ferri peter flach jos hernndezorallo delegating classifiers proceedings twentyfirst international conference machine learning p37 july 0408 2004 banff alberta canada saddys segrera mara n moreno experimental comparative study web mining methods recommender systems proceedings 6th wseas international conference distance learning web engineering p5661 september 2224 2006 lisbon portugal ljupo todorovski sao deroski combining classifiers meta decision trees machine learning v50 n3 p223249 march joo gama functional trees machine learning v55 n3 p219250 june 2004 huimin zhao sudha ram entity identification heterogeneous database integration multiple classifier system approach empirical evaluation information systems v30 n2 p119132 april 2005 zeng dan pan jianbin prediction mhc iibinding peptides using rough setbased rule sets ensemble applied intelligence v27 n2 p153166 october 2007 b kotsiantis zaharakis p e pintelas machine learning review classification combining techniques artificial intelligence review v26 n3 p159190 november 2006