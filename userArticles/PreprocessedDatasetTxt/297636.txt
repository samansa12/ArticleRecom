convergence second order stationary points inequality constrained optimization propose new algorithm nonlinear inequality constrained minimization problem prove generates sequence converging points satisfying kkt second order necessary conditions optimality algorithm line search algorithm using directions negative curvature viewed nontrivial extension corresponding known techniques unconstrained constrained problems main tools employed definition analysis algorithm differentiable exact penalty function results theory lc1 functions b introduction concerned inequality constrained minimization problem p min fx three times continuously differentiable aim develope algorithm generates sequences converging points x satisfying together suitable multiplier 2 ir kkt first order necessary optimality conditions 1 kkt second order necessary optimality conditions z 0 2 sequel call point x satisfying 1 first order stationary point stationary point point satisfying 1 2 termed second order stationary point unconstrained case conditions 1 2 boil respectively standard algorithms unconstrained minimization usually generate sequences converging first order stationary points landmark paper 21 see also 23 22 16 20 references therein subsequent developments mccormick showed using directions negative curvature armijotype line search procedure possible guarantee convergence second order stationary points theoretical point view strong result since makes much likely limit points sequence generated algorithm local minimizers saddle points furthermore practical point view use negative curvature directions turns helpful minimization problems large nonconvex regions 16 20 convergence second order stationary points later established also trustregion algorithms 25 24 constitute one main reasons popularity class methods trustregion algorithms extended equality constrained box constrained problems mantain convergence second order stationary points 2 8 13 5 25 24 26 negative curvature line search algorithms proposed linearly inequality constrained case 22 17 however far aware algorithm solution complex nonlinearly inequality constrained minimization problem p exists generates sequences converging second order stationary points main purpose paper fill gap presenting negative curvature line search algorithm enjoys property basic idea behind approach easily explained follows reduce constrained minimization problem p equivalent unconstrained minimization problem using differentiable exact penalty function b apply negative curvature line search algorithm minimization penalty function although appealingly simple tackle difficulties make approach viable first establish connection unconstrained stationary points penalty function provided unconstrained minimization algorithm constrained second order stationary points problem p secondly must cope fact differentiable exact penalty functions although continuously differentiable never twice continuously differentiable everywhere cannot use offtheshelf negative curvature algorithm minimization furthermore even points second order derivatives exist explicit evaluation would require use third order derivatives functions f g willing calculate overcome difficulties develop negative curvature algorithm unconstrained minimization penalty function based theory lc 1 functions generalized hessians show using suitable approximation elements generalized hessian penalty function guarantee unconstrained minimization penalty function yields unconstrained stationary point matrix approximates element generalized hessian positive semidefinite suffices ensure point found also second order stationary point problem p believe algorithm proposed paper intereset first time able prove convergence second order stationary points general inequality constrained problems fairly natural extension negative curvature algorithms unconstrained constrained problems note computation negative curvature direction performed manner analogous cost unconstrained case also remark never require complementarity slackness assumption establish results finally think use non trivial nonsmooth analysis results analyze behavior smooth algorithms novel feature could fruitfully applied also cases paper organized follows next section recall known facts lc 1 functions generalized hessians asymptotic identification active constraints furthermore also introduce penalty function along relevant properties section 3 introduce analyze algorithm fourth section give hints practical realization algorithm finally last section outline possible improvements make remarks finally review notation used paper gradient function h indicated rh hessian matrix denoted r 2 h matrix rh given index set vector obtained considering components h indicate k delta k euclidean norm corresponding matrix norm subset ir n cos denotes convex hull square matrix min denotes smallest eigenvalue lagrangian problem p lx lx x lagrangian problem p evaluated x analogously indicate r x lx x r 2 xx lx x gradient hessian lagrangian respect x evaluated x background material section review results differentiability functions identification active constraints also recall definition basic facts differentiable exact penalty function problem p establish related new results 21 lc 1 functions said lc 1 function open set h continuously differentiable rh locally lipschitz functions first systematically studied 18 definition generalized hessian theorem reported also given gradient h locally lipschitz rh therefore differentiable almost everywhere generalized jacobian clarkes sense 3 defined precisely generalized hessian h whose definition follows lc 1 function open set let x belong define generalized hessian h x set 2 hx matrices defined differentiable x k r 2 hx k note 2 hx nonempty convex compact set symmetric matrices pointtoset map x 7 2 hx bounded bounded sets 18 lc 1 functions secondorder taylorlike expansion possible main result lc 1 function shall need paper theorem 21 let h lc 1 function open set let x two points x contained 22 identification active constraints section recall results identification active constraints stationary point x nonlinear program p refer interested reader 14 references therein detailed discussion issue recall results order stress fact neighborhood first order stationary point possible mild assumptions correctly identify constraints active solution first need terminology given stationary point x corresponding multiplier suppose unique denote 0 x set active constraints 0 x fij g x denotes index set strongly active constraints aim construct rule able assign every point x estimate ax lies suitably small neighborhood stationary point x usually estimates kind obtained comparing values g x value estimate multiplier example easily shown set phi x c positive constant multiplier function ie continuous function coincides set 0 x x sufficiently small neighborhood stationary point x satisfies strict complementarity condition see next section example multiplier function condition violated inclusions hold 14 stationary point x satisfy strict complementarity situation therefore complex recently shown nevertheless possible correctly estimate set 0 x 14 go details point identification active constraints strict complementarity hold possible mild assumptions simple way identification rule takes following form aex function take different forms according assumptions made stationary point x example neighborhood x f g analytic assumption met practical cases one define aex logrx choice set ax defined 4 coincide 0 x suitable neighborhood x 23 penalty function section consider differentiable penalty function problem p recall relevant known facts prove new results related differentiability issues dealt section 21 order define differentiable penalty function guarantee properties needed make following two assumptions assumption x 2 ir n gradients rg x 2 0 x linearly independent assumption b x 2 ir n following implication holds assumptions b together assumption c stated section 3 assumptions used establish results paper assumptions assumptions similar frequently encountered analysis constrained minimization algorithms however point considerably relaxed discussed section 5 chose use particular set assumptions order simplify analysis concentrate issues related main topic paper ie convergence second order stationary points start defining multiplier function mx theta matrix defined gx diagg x main property function continuously differentiable see x first order stationary point x corresponding multiplier assumption unique using function define following penalty function socalled penalty parameter theorem 22 following properties hold every ffl penalty function z continuously differentiable ir n gradient given e ith column theta identity matrix x diag x b every ffl function z lc 1 function ir n c let x first order stationary point problem p let ffl given exists neighborhoodomega x every x inomega following overestimate generalized hessian z evaluated x holds matrix write kka xk aex nonegative continuous function ae particular following overstimate holds x xx lx x ra xrg x proof point proved 11 point b follows expression gradient given taking account differentiability assumptions proof point c derived definition generalized hessian following way let x stationary point problem p consider point x neighborhoodomega x sequences points fx k g converging x gradient z existing x k happen either b g phi xg recalling expression gradient given previously write psi easy see case b hessian zx ffl x k obtained differentiating expression gives rg phi k phi rapresents sum terms always containing factor either phi taking account definition 2 zx ffl discussed previous section ifomega suitably small g phi x assertion point c follows facts definition following theorem gives sufficient condition terms matrices overestimate stationary point problem p second order stationary point theorem 23 let x first order stationary point problem p let ffl given matrix h exists positive semidefinite x second order stationary point problem p proof let h positive semidefinite suppose contradiction x satisfy kkt second order necessary conditions 2 vector z exists rg 0 recall x equals multiplier associated x hand theorem 22 c caratheodory theorem also integer fi 0 since 2 write taking account definition hx ffl 9 z hx ffl z immediately follows contradict assumption h positive semidefinite proof complete result turn fundamental approach since algorithm converge first order stationary stationary points least one element positive semidefinite remining part section consider technical results penalty functions used later help illustrate relation function z problem p proposition 24 let ffl 0 x 2 ir n given x unconstrained stationary point z x first order stationary point problem p b conversely x first order stationary point problem p every positive ffl rzx proof see 11 proposition 25 let ae ir n compact set exists every x 2 every ffl 2 0 ffl proof let 1 ae ir n compact subset ae intd 1 proof proposition 14 12 shown x feasible point intd 1 therefore find positive ffl x oex ae 0 x precisely 10 derives formula 24 12 discussion follows formula indicate maximum krgxk x note assumption 0 recalling krgx rzx fflk krgxkkrzx fflk easily deduce 10 set suppose theorem true sequences fx k g fffl k g exist since recalling expression rz gives thus assumption b x feasible get contradiction 12 11 concludes proof note proposition 24 proposition 25 imply given compact set ffl sufficiently small every stationary point problem p unconstrained stationary point z vice versa every unconstrained stationary point z stationary point problem p refer interested reader review paper 9 references therein detailed discussion properties differentiable penalty functions 3 convergence second order stationary points section consider line search algorithm minimization zx ffl yields second order stationary points problem p sake clarity break exposition three parts section 31 first consider line search algorithm algorithm converges fixed value penalty parameter ffl unconstrained stationary point penalty function z proposition 24 know penalty parameter sufficiently small would thus obtained first order stationary point problem p therefore section 32 introduce algorithm algorithm soc algorithm embedded simple updating scheme penalty parameter based proposition 25 show finite number reductions penalty parameter stays fixed every limit point algorithm soc first order stationary point problem p finally section 33 refine analysis algorithm soc show every limit point actually second order stationary point problem p order establish results sections 31 32 33 assume directions used algorithm satisfy certain conditions section 4 illustrate possible ways generating directions fulfil conditions order simplify analisys shall assume following assumption satisfied assumption c sequence fx k g points generated algorithms considered bounded 31 convergence fixed ffl unconstrained stationary points z algorithm first consider line search algorithm unconstrained minimization penalty function z generates fixed value ffl penalty parameter sequences converging unconstrained stationary points penalty function section ffl understood fixed positive constant algorithm generates sequence fx k g according following rule algorithm ff k compute linesearch procedure linesearch procedure step 2 set ff step 3 choose ff 2 oe 1 ff oe 2 ff go step 2 assume matrices h k depend sequence fx k g directions matrices h k bounded satisfy following conditions condition 1 directions k rzx k ffl k 0 0 condition 2 directions k rzx k ffl k 0 together matrices h k satisfy ae k positive semidefinite 0 condition 3 let fx k g fu k g sequences converging first order stationary point x problem p every sequence matrices fq k g sequence numbers converging 0 algorithm resembles classical line search algorithms using negative curvature directions force convergence second order stationary points unconstrained min imization apparent difference exponent tx k defined corresponding unconstrained algorithms usually tx k every k need change order able tackle fact penalty function everywhere twice continuously differentiable see example proof proposition 31 also assume directions k k matrices h k satisfy conditions 13 conditions 1 2 fairly standard similar employed unconstrained case condition 3 sequence matrices h k related nondifferentiability gradient z fact matrix h k supposed convey second order information penalty function therefore condition 3 imposes certain relation matrices h k generalized hessians z note function z twice continuously differentiable choice would satisfy continuity condition 3 following proposition shows linesearch procedure described well defined cases shall see interest us proposition 31 linesearch procedure well defined namely iteration test step 2 satisfied every ff sufficiently small point x k either unconstrained stationary point function z b first order stationary point problem p h k positive semidefinite proof assume contradiction assertion proposition false exists sequence fff j g ff z either condition condition b hold theorem 21 taking account rzx k ffl k 0 condition 2 find point z qu k symmetric matrix belonging 2 zu k ffl therefore 14 15 consider two cases condition holds krzx k fflk 6 0 dividing sides 16 ff 2 taking account making limit recalling sequence fqu k g bounded obtain contradiction condition b holds x k first order stationary point problem p h k positive semidefinite proposition 24 b rzx k rzx k ffl dividing sides 16 ff 2tx k making limit recalling sequence fqu k g bounded recalling condition 3 implies obtain 16 recalling h k positive semidefinite contradicts condition 2 proposition 31 shows algorithm possibly fail produce new point k rzx k supposing trivial case occur next theorem illustrates behaviour infinite sequence generated algorithm theorem 32 let fx k g infinite sequence produced algorithm every limit point x fx k g rzx proof since sequence fzx k fflg monotonically decreasing z continuous fx k g bounded assumption c follows fzx k fflg converges hence lim recalling acceptability criterion line search condition 1 condition 2 therefore 17 18 condition 1 condition 2 yield boundness k k condition 1 condition 2 19 20 imply turn suppose contradiction exists converging subsequence fx k gk 1 whose limit point x stationary point semplicity without loss generality rename subsequence fx k gk 1 fx k g condition 1 19 rzx ffl 6 0 imply 23 exists index k k z oe k 2 oe theorem 21 taking account rzx k ffl k 0 condition 2 find k k point z 24 25 follows dividing sides simple manipulations obtain 21 22 condition 1 since sequence fqu k g bounded 27 lim condition 1 implies rzx contradicts fact subsequence converge unconstrained stationary point proves theorem next sections given x k indicate mx k new point produced algorithm described 32 updating ffl guarantee convergence stationary points problem p algorithm soc section show possible update simple way value penalty parameter ffl minimizing penalty function z algorithm every limit point sequence points generated first order stationary point problem p accomplished algorithm soc next section shall show actually additional conditions limit points generated algorithm soc also second order stationary points problem p motivates name soc stands second order convergence algorithm soc step 0 select x 0 ffl 0 set step 2 else go step 3 step 2 maxgx k h k 6 0 go step 4 otherwise maxgx k step 3 krzx k go step 4 else go step 5 step 4 compute step 1 step 5 set x go step 1 algorithm soc related similar schemes already proposed literature see eg review paper 9 references therein core step step 3 iteration decision whether update ffl taken step obviously motivated propositions 25 proposition 24 theorem 33 algorithm soc well defined furthermore let fx k g fffl k g sequences produced algorithm soc either algorithm terminates p iterations first order stationary point x p problem p exist index ffl 0 every k k ffl every limit point sequence first order stationary point problem p proof algorithm well defined every time reach step 4 proposition 31 ensures mx k well defined algorithm stops finite number iterations instructions steps 1 2 rzx thesis follows proposition 24 therefore assume infinite sequence points generated assumption c theorem 25 guarantee ffl updated finite number times finite number times ffl algorithm soc reduces application algorithm zx ffl theorem 32 every limit point x fx k g rzx since test step 3 eventually always satisfied implies turn thesis follows theorem 24 33 algorithm soc second order convergence section prove additional suitable conditions every limit point sequence fx k g generated algorithm soc actually satisfies kkt second order necessary conditions establish result need two conditions condition 4 let fx k g sequence converging first order stationary point problem p directions k matrices h k satisfy condition 5 let fx k g sequence converging first order stationary point x problem p let ffl 0 given condition 4 mimics similar standard conditions unconstrained case h k hessian objective function roughly speaking requires direction k sufficiently good approximation eigenvector corresponding smallest eigenvalue h k condition 5 similarly condition 3 imposes connection matrices h k generalized hessian z following theorem establishes main result paper theorem 34 let fx k g sequence produced algorithm soc either algorithm terminates second order stationary point x p problem p produces infinite sequence fx k g every limit point x fx k g second order stationary point problem p proof algorithm soc terminates finite number iterations theorem 33 x p first order stationary point problem p hand instructions step 2 condition 5 h p positive semidefinite belongs therefore assertion follows theorem 23 pass case infinite sequence generated already know theorem 33 every limit point sequence first order stationary point problem p also know eventually ffl k updated ffl theorem 23 suffice show contains positive semidefinite element suppose contrary let fx k g converge x reasoning beginning proof theorem 32 21 still hold assume renumbering necessary 0 30 fact case 22 conditions 4 5 imply contradiction tends positive semidefinite element 30 repeating arguments used proof theorem 32 exists index k k k 26 holds 26 get recalling condition 3 taking account condition 1 rzx fact dividing sides 21 22 fqu k g bounded condition 5 renumbering necessary h condition 2 31 implies lim hence recalling condition 4 min h condition 5 contradicts fact subsequence fx k g converges kkt point every element ffl positive semidefinite 4 practical realization section show calculate directions k k matrices h k satisfying conditions 15 required previous sections let matrix h k defined ax estimate active set property neighborhood stationary point x section 23 discussed detail possible choices ax gave adequate references note also stationary point x matrix h k belongs given matrix wide range choices k k theoretically sound option take k gammarzx k b practical choice however could taking k solution linear system k diagonal matrix chosen h k k positive definite smallest eigenvalue sequence fh k k g bounded away 0 matrix k 0 matrix h k positive definite smallest eigenvalue greator small positive threshold value methods automatically constructing matrix k solving system h k well known used unconstrained case c another possible choice take k direction employed 10 1 15 chosen eigenvector associated smallest eigenvalue matrix h k sign possibly changed order ensure rzx k ffl k b suitable approximations direction point calculated indicated example 23 20 could also employed design algorithmically effective choice k k beyond scope paper wanted illustrate wide range options available choices certainly possible sequel sake concreteness shall assume k k chosen according options listed choices since supposing fx k g remains bounded set easy see also sequences bounded also standard show conditions 1 2 4 satisfied furthermore recall neighborhood stationary point x problem p easy see definition also condition 5 met choice matrix h k next proposition show also cumbersome condition 3 satisfied proposition 41 sequence matrices defined 32 satisfies condition 3 proof let sequences fx k g fu k g converging stationary point problem p given let fq k g sequence q k 2 2 zu k ffl every k theorem 22 c know assume without loss generality eventually x k sufficiently close point x matrix q k following form integer fae k g sequence converging 0 k fi k since k sufficiently large also recall b two theta r matrices write r j b j jth columns b respectively employing taylor expansion write r k rg k r 0 rg 0 r k rg k r 0 rg 0 fflb r k take account previous formula set k write relation thesis proposition readily follows setting 5 remarks conclusions presented negative curvature line search algorithm minimization nonlinear function subject nonlinear inequality constraints main novel feature method every limit point sequence generates satisfies kkt first second order necessary optimality conditions main tools employed obtain result continuously differentiable penalty function results theory lc 1 functions sake simplicity include equality constraints analysis easily handled results paper go one considers also equality constraints sufficient use analogous penalty function z equality constraints included see 12 another point deserves attention assumtions b c employ assumptions mainly dictated penalty function considered however relaxed sophisticated choice made penalty function chose use relatively simple function z concentrate main issues related second order convergence however continuously differentiable function proposed 7 employed instead z improve assumptions b c example assumption relaxed feasible x gradients rg x 2 0 x linearly independent significantly also assumptions b c considerably relaxed illustrate point introduce technical notation prefere omit refer reader 7 details point assumption c replaced natural mild assumptions problem data guarantee levels sets penalty function compact r constrained optimization lagrange multiplier methods trust region algorithm nonlinearly constrained optimization optimization nonsmooth analysis interior trust region approach nonlinear minimization subject bounds new trustregion algorithm equality constrained optimization global convergence class trust region algorithms optimization simple bounds continuously differentiable exact penalty function nonlinear programming problems unbounded feasible set convergence theory trustregion based algorithms equalityconstrained optimization algorithms continuous optimization system modelling optimization continuously differentiable exact penalty function nonlinear programming problems inequalty constraints exact penalty functions constrained optimiza tion convergence secondorder point trustregion algorithm nonmonotonic penalty parameter constrained optimization la sapienza globally quadratically convergent exact penalty based methods inequality constrained problems nonmonotone curvilinear line search methods unconstrained optimization newton methods largescale linear inequality constrained minimization generalized hessian matrix secondorder optimality conditions problems c 1 new results continuously differentiable exact penalty function la sapienza modification armijos stepsize rule negative curva ture nonlinear programming theory newtons method model trust region modification tr ctr giovanni fasano massimo roma iterative computation negative curvature directions large scale optimization computational optimization applications v38 n1 p81104 september 2007 immanuel bomze laura palagi quartic formulation standard quadratic optimization problems journal global optimization v32 n2 p181205 june 2005 x q yang x x huang partially strictly monotone nonlinear penalty functions constrained mathematical programs computational optimization applications v25 n13 p293311