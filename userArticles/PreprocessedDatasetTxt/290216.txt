simple fast parallel hashing oblivious execution hash table representation set linear size data structure supports constanttime membership queries show construct hash table given set n keys olg lg n parallel time high probability using n processors weak version concurrentread concurrentwrite parallel random access machine crcw pram algorithm uses novel approach hashing oblivious execution based probabilistic analysis algorithm simple following structure partition input set buckets random polynomial constant degree 1 olg lg n allocate mt memory blocks size kt let bucket select block random try injectively map keys block using random linear function buckets fail carry next iteration crux algorithm careful priori selection parameters mt kt algorithm uses olg lg n random words implemented workefficient manner b introduction let set n keys drawn finite universe u hashing problem construct following attributes injectiveness two keys mapped h value space efficiency space required represent h time efficiency every x 2 u hx evaluated o1 time single processor function induces linear space data structure perfect hash table representing data structure supports membership queries o1 time paper presents simple fast efficient parallel algorithm hashing problem using n processors running time algorithm olg lg n overwhelming probability superior previously known algorithms several respects computational models model computation use concurrentread concurrent parallel random access machine crcw pram family see eg 35 members family differ outcome event one processor attempts write simultaneously shared memory location main submodels crcw pram descending order power priority 29 lowestnumbered processor succeeds arbitrary 42 one processors succeeds known advance one collision 9 different values attempted written special collision symbol written cell collision 15 special collision symbol written cell tolerant 32 contents cell change finally less standard robust 7 34 two processors attempt write cell given step attempt cell obtain value 11 previous work hash tables fundamental data structures numerous applications computer science extensively studied literature see eg 37 40 survey 41 recent one particular interest perfect hash tables every membership query guaranteed completed constant time worst case perfect hash tables perhaps even significant parallel context since time executing batch queries parallel determined slowest query fredman komlos szemeredi 16 first solve hashing problem expected linear time universe size input set scheme builds 2level hash function level1 function splits subsets buckets whose sizes distributed favorable manner injective level2 hash function built subset allocating private memory block appropriate size 2level scheme formed basis algorithms dynamic version hashing problem also called dictionary problem insertions deletions may change dynamically algorithms given dietzfelbinger karlin mehlhorn meyer auf der heide rohnert tarjan 12 dietzfelbinger meyer auf der heide 14 dietzfelbinger gil matias pippenger 11 parallel setting dietzfelbinger meyer auf der heide 13 presented algorithm dictionary problem fixed ffl 0 n arbitrary dictionary instructions insert delete lookup executed ffl expected time n 1gammaffl processor priority crcw matias vishkin 39 presented algorithm hashing problem runs olg n expected time using lg n processors arbitrary crcw fastest parallel hashing algorithm previous work based 2level scheme makes extensive use counting sorting procedures known lower bounds parallel hashing given gil meyer auf der heide wigderson 27 rather general model computation required number parallel steps omegagamma1 n also showed restricted model one processor may simultaneously work key parallel hashing time isomegagamma3 lg n also gave algorithm yields matching upper bound function applications charged operations eg counting sorting free algorithm falls within realm mentioned restricted model matches omegagammae lg n lower bound charging operations concrete pram model 12 results main result linear static hash table constructed olg lg n time high probability space using n processors crcw pram algorithm following properties time optimality best possible result use processor reallocations shown 27 optimal speedup achieved small penalty execution time significant improvement olg n time algorithm 39 reliability time bound olg lg n obeyed high probability contrast time bound algorithm 39 guaranteed constant probability simplicity arguably simpler hashing algorithm previously published never theless analysis quite involved due tight tradeoffs probabilities conflicting events reduced randomness adapted consume olg lg n random words compared omegagamma n random words previously used work optimality work optimal implementation presented timeprocessor product running time increased factor olg n also requires olg lg n random words computational model allow lookup time olg lg n well algorithm implemented robust crcw model results summarized following theorem theorem 1 given set n keys drawn universe u hashing problem solved using space olg lg n time high probability using n processors ii olg lg n lg n time operations high probability algorithms run crcw pram reallocation processors keys employed use olg lg bits previous algorithms implementing 2level scheme either sequentially parallel based grouping keys according buckets belong require learning size bucket bucket allocated private memory block whose size dependent bucket size approach relies techniques related sorting counting require omegagammaeq n lg lg n time solved polynomial number processors implied lower bound beame hastad 4 lower bound holds even randomized algorithms recent results found involved ways circumvent barriers cf 38 3 26 30 circumvent obstacle learning buckets sizes purpose appropriate memory allocation technique oblivious execution sketched figure 1 1 partition input set buckets random polynomial constant degree 2 1 olg lg n allocate memory blocks size k b let bucket select block random try injectively map block using random linear function block selected another bucket injective mapping found bucket carries next iteration figure 1 template hashing algorithm crux algorithm careful priori selection parameters k iteration k depend expected number active buckets expected distribution bucket sizes iteration way makes desired progress possible rather likely execution oblivious following sense buckets treated equally regardless sizes algorithm make explicit attempt estimate sizes individual buckets allocate memory buckets based sizes case previous implementations 2level scheme attempt estimate number active buckets distribution sizes selection parameters k iteration made according priori estimates random variables estimates based properties level1 hash function well inductive assumptions behavior previous iterations remark hashing result demonstrates power randomness parallel computation crcw machines memory restricted linear size boppana 6 considered problem element distinctness given n integers decide whether distinct showed solving element distinctness nprocessor priority machine bounded memory requires omegagammaeq n lg lg n time bounded memory means memory size arbitrary function n range input values easy see memory size bounded omegagamma element distinctness solved o1 expected time using hash functions fact 22 however hold linear size memory parallel hashing algorithm implies incorporating randomness element distinctness solved expected olg lg n time using n processors collision weaker priority model linear memory size 13 applications perfect hash table data structure useful tool parallel algorithms matias vishkin 39 proposed using parallel hashing scheme space reduction algorithms large amount space required communication processors algorithms become space efficient preserve number operations penalties introducing randomization increase time using hashing scheme time increase may substantially smaller algorithms using scheme 39 resulting time increase olg n using new scheme time increase lg lg n lg n case construction suffix trees strings 2 17 naming assignment procedure substrings large alphabets 17 algorithms time increase 39 olg lg n lg lg n 2 algorithm leaves expected time unchanged case integer sorting polynomial range 33 superpolynomial range 5 39 applications discussed conclusion section 14 outline rest paper organized follows preliminary technicalities used algorithm analysis given section 2 algorithm template presented greater detail section 3 two different implementations based different selections k given subsequent sections section 4 presents implementation fully satisfy statements theorem 1 relatively simple analysis improved implementation main algorithm involved analysis presented section 5 section 6 show reduce number random bits section 7 explains algorithm implemented optimal number operations model computation discussed section 8 also give modified algorithm weaker model section 9 briefly discusses extension hashing problem input may consist multiset finally conclusions given section 10 preliminaries following inequalities standard see eg 1 markovs inequality let random variable assuming nonnegative values chebyshevs inequality let random variable 0 chernoffs inequality let binomial variable 0 terminology probabilities say event occurs ndominant probability occurs probability gammaomegagamma21 usage notation essentially follows polylogarithmic number events one occurs ndominant probability conjunction occurs ndominant probability well therefore usually satisfied demonstrating algorithmic step succeeds ndominant probability fact 21 let independent binary random variables let 2 dominant probability 3 proof recall well known fact 1in pairwise independent 1 inequality 2 2 2 e 2 inequality 2 inequality 1 3 follows immediately hash functions remainder section let u fixed splits set buckets bucket subset fx size collides bucket singleton function injective perfect element collides let 0im r function injective number collisions pairs keys generally b r number rtuples keys collide h polynomial hash functions let prime class degreed polynomial hash functions 1 mapping u 0 mod c rest section consider probability space h selected uniformly random h following fact corollary shown fredman komlos szemeredi 16 carter wegman 8 original proof case however generalization 1 straightforward fact 22 e corollary 23 hash function h injective probability least 1 proof function h injective fact 22 markovs inequality probability h injective prob b following shown 11 fact 24 3 r 0 let r rth moment distribution 0im r easy see shown completely random function r linear n high probability fixed r 2 polynomial hash functions dietzfelbinger et al 12 proved following fact fact 25 let r 0 n r exists constant oe r 0 depending r tighter estimates distribution r given 11 completeness proofs attached appendix b fact 26 let r 2 r 1jr r phi r psi stirling number second kind 1 fact 27 let ffl 0 constant probability stirling number second kind number ways partitioning set k distinct elements j nonempty subsets eg 31 chapter 6 3 framework hashing oblivious execution 31 algorithm template input algorithm set n keys given array hashing algorithm works two stages correspond two level hashing scheme fredman komlos szemeredi 16 first stage level1 hash function f chosen function selected random class h sufficiently large constant selected analysis hash function f partitions input set buckets bucket first stage easily implemented constant time main effort implementation second stage described next second level hash table built second stage algorithm bucket private memory region called block assigned address memory block allocated bucket recorded cell designated array ptr size also bucket level2 function constructed function injectively maps bucket block descriptions level2 functions written ptr let us call bucket active appropriate level2 function yet found inactive otherwise beginning stage buckets active algorithm terminates buckets become inactive second stage consists olg lg n iterations executing constant time iterative process rapidly reduces number active buckets number active keys iteration new memory segment used segment partitioned blocks size k k set analysis bucket key associated one processor operation active bucket iteration given figure 2 allocation bucket selects random one memory blocks block selected another bucket bucket remains active participate next step hashing bucket selects random two functions h 1 tries hash block separately functions either one functions injective description memory address block written appropriate cell array ptr bucket becomes inactive otherwise bucket remains active carries next iteration figure 2 two steps iteration based oblivious execution last iterations may become necessary iteration repeat body constant number times precise conditions number repetitions given section 5 hash table constructed algorithm supports lookup queries constant time given key x search begins reading cell ptrfx contents cell defines level2 function used x well address memory block x stored actual offset block x stored given injective level2 hash function found hashing step 32 implementations algorithm template described constitutes framework building parallel hashing algorithms execution algorithms oblivious sense iterative process finding level2 hash functions require information number size active buckets successful termination performance dependent priori setting parameters k effectiveness allocation step relies sufficiently many memory blocks effectiveness hashing step relies sufficiently large memory blocks requirement keeping total memory linear imposes tradeoff two parameters challenge finding balance k achieve desired rate decay number active buckets number active keys deduced number active buckets based characteristics level1 hash function determined show two different implementations algorithm template leading analysis different nature first implementation given section 4 parameters selected way iteration number active buckets expected decrease constant factor although iteration may fail constant probability geometrically decreasing series bounds number active buckets iteration olg lg n iterations expected number active keys active buckets becomes nlg n omegagamma1 remaining keys hashed additional constant time using different approach employing olg lg n time procedure technical point view analysis implementation imposes relatively modest requirements level1 hash function since uses firstmoment analysis ie markovs inequality moreover requires simpler version hashing step one hash function h 1 used expected running time olg lg n running time guaranteed arbitrary small constant probability second implementation given section 5 implementation characterized doublyexponential rate decrease 2 number active buckets keys olg lg n sequence v0 decreases exponential rate v v01 sequence decreases doublyexponential rate v v02 1ffl ffl 0 iterations keys hashed without processing implementation superior several respects time performance high probability key handled original processor forms basis improvements reducing number random bits technical point view analysis implementation subtle imposes demanding requirements level1 hash function since uses secondmoment analysis ie chebyshevs inequality achieving doublyexponential rate decrease required careful selection parameters done using symbolic spreadsheet approach together implementations demonstrate two different paradigms fast parallel randomized algorithms involving different flavor analysis one requires exponential rate decrease problem size relies reallocation processors items subsequent works use paradigm extensions mentioned section 10 paradigm relatively easy understand difficult analyze using framework probabilistic induction analysis expectations analysis shows iteration succeeds constant probability implies overall constant success probability contrast second implementation shows iteration succeeds ndominant probability implies overall ndominant success probability analysis significantly subtle relies powerful techniques second moment analysis second paradigm consists doublyexponential rate decrease problem size hence require wrapup step 4 obtaining exponential decrease section presents first implementation algorithm template using rather elementary analysis expectations show iteration problem size decreases constant factor constant probability general framework described section 41 shows implies problem size decreases overall exponential rate olg lg n iterations number keys reduced nlg nomegagamma1 simple load balancing algorithm allocates lg nomegagamma21 processors remaining key using excessive number processors key finally hashed constant time 41 designing expectation consider iterative randomized algorithm iteration measure problem decreases random amount companion paper 22 showed iteration one actually assume previous iterations algorithm far expected behavior paradigm suggested design iteration successful constant probability assumption least constant fraction previous iterations successful justified following lemma lemma 41 probabilistic induction 22 consider iterative randomized process 0 following holds iteration probability least 12 provided among first iterations least t4 successful probabilityomegagammail every 0 number successful iterations among first iterations least t4 42 parameters setting analysis let level1 function taken h 10 set let fact 25 simplify analysis allow parameters k assume nonintegral values actual implementation must rounded nearest integer increase memory requirements constant factor performance measures improved memory usage memory space used lemma 42 let v number active buckets beginning iteration proof assume level1 function f satisfies fact 25 11 holds probability least 12 proof continued using lemma 41 iteration successful v t1 v 2 thus number active buckets j successful iterations m2 gammaj probabilistic inductive hypothesis among first iterations least t4 successful probabilistic inductive step show iteration parameters k chosen achieve constant deactivation probability buckets size distinguish following three types events failures may cause bucket remain active end iteration allocation failure bucket may select memory block also selected buckets probability fixed bucket successfully reserve block allocation step since v buckets selecting random one memory blocks ae 1 v 12 10 ii size failure bucket may large current memory block size result probability find level2 hash function high enough number buckets beginning iteration larger fi 11 therefore 13 without loss generality assume v t1 v 2 v buckets needed become inactive still considered active thus purpose analysis iii hash failure bucket may fail find injective level2 hash function even though sufficiently small uniquely selected block let ae 3 probability bucket size fi successfully mapped block size k hashing step corollary 23 13 bucket size fi successfully reserves block size k successfully mapped becomes inactive expected number active buckets beginning iteration therefore bounded markovs inequality proving inductive step lemma follows lemma 43 let n number active keys beginning iteration constants c ff 0 proof follows 11 using simple convexity argument n maximal active buckets beginning iteration size q case 11 therefore lemma 42 lemma follows lemma 43 lemma 42 exponential decrease number active keys number active buckets probabilityomegagamma329 number active keys becomes nlg n c constant c 0 olg lg n iterations 43 final stage execution second stage parameter setting described number available resources memory cells processors factor lg nomegagamma1 larger number active keys resource redundancy makes possible hash remaining active keys constant time described remainder section keys hashed iterative process hashed auxiliary hash table size consequently implementation lookup query consist searching key hash tables auxiliary hash table built using 2level hashing scheme level1 function maps set active keys array size n function selected random class hash functions presented dietzfelbinger meyer auf der heide 14 definition 41 property ndominant probability bucket size smaller lg n 14 theorem 46b remainder section assume event indeed occurs alternatively use n ffl universal class hash functions presented siegel 43 active key allocated 2 lg n processors active bucket allocated 4lg n 3 mem ory allocation done mapping active keys injectively array size lg n mapping indices buckets injectively array size onlg n 3 mappings done olg lg n time ndominant probability using simple renaming algorithm 20 remaining steps take constant time independently select 2 lg n linear hash functions store designated array hash functions used buckets memory allocated bucket partitioned 2 lg n memory blocks size 2 lg 2 n bucket mapped parallel 2 lg n blocks 2 lg n selected linear hash functions mapping tested injectiveness carried 2 lg n processors allocated key bucket one injective mappings selected level2 function selection made using simple leftmost 1 algorithm 15 buckets mappings injective construction auxiliary hash table fails lemma 44 assume number keys remain active iterative process nlg n 3 construction auxiliary hash table succeeds ndominant probability proof recall bucket size lg n mapping bucket memory block size 2lg n 2 injective probability least 12 corollary 23 probability bucket injective mapping therefore 1n 2 probability least 1 gamma 1n every bucket least one injective mapping easy identify failure algorithm fails terminate within designated time restarted hash table therefore always constructed since overall failure probability constant expected running time olg lg n 5 obtaining doublyexponential decrease implementation algorithm template presented previous section maintains exponential decrease number active buckets throughout iterations section presents implementation number active buckets decreases doubly exponential rate intuitively stochastic process behind algorithm template potential achieving doublyexponential rate memory block sufficiently large comparison bucket size probability bucket remain active inversely proportional size memory block corollary 23 consider idealized situation case iteration active buckets allocated memory block size k iteration k active buckets could allocated memory block size k 2 iteration k 3 active buckets allocated memory block size k 4 less idealized setting buckets deactivate large current value k number buckets bounded using properties level1 hash function must guaranteed fraction large buckets also decreases doublyexponential rate illustrative crude calculation given assumes memory evenly distributed active buckets make doublyexponential rate possible failure probability allocation step hence ratio must also decrease doublyexponential rate establishing bound number large blocks showing large fraction buckets allocated memory blocks also concern previous section however enough show constant bounds probabilities allocation failure size failure hash failure parameter setting establishes balance required doublyexponential rate presented following analysis algorithm performance section concludes description parameters selected 51 parameters setting let level1 function taken h set let 52 memory usage proposition 51 total memory used algorithm proof 17 memory used first stage memory used iteration second stage total memory used second stage therefore mostx 53 framework time performance analysis defined runtime analysis second stage carried showing lemma 52 ndominant probability number active buckets beginning iteration lemma proved induction lg lg n lg induction base follows fact n active buckets subsequent subsections prove inductive step deriving estimates number failing buckets iteration assumption beginning iteration active buckets specifically show induction ndominant probability number active buckets end iteration bucket may fail find injective level2 hash function estimating number buckets fail find injective level2 function iteration assume bucket uniquely selected memory block bucket size large relatively current block size accordingly section 42 distinguish following three types events failures may cause bucket remain active end iteration allocation failure bucket may select memory block also selected buckets ii size failure bucket may large current memory block size result probability find level2 hash function high enough iii hash failure bucket may fail find level2 hash function even though sufficiently small uniquely selected block provide estimates number buckets remain active due either reasons lemma 55 case lemma 56 case ii lemma 57 lemma 58 case iii estimates shown hold ndominant probability induction step follows adding estimates wrap let therefore infer proposition 53 ndominant probability number iterations required deactivate buckets lg lg n lg 54 failures uniquely selecting block lemma 54 let ffl fixed suppose either 12ffl let random variable representing number buckets fail uniquely select block proof bucket probability buckets select memory block selected therefore stochastically smaller binomially distributed random variable obtained performing independent trials probability success say 3 gammaomegagamma26 otherwise situation e 1 since integer valued 2m 2 1 25 setting covered lemma 12gammaffl occurs constant number iterations throughout algorithm requires following special treatment body iterations repeated thus providing second allocation attempt buckets failed uniquely select memory block first trial random variables representing number buckets fail uniquely select block first second attempts respectively j 1 25 gammaomegagamma27 therefore dominant probability second attempt falls within conditions equation lemma 55 let lg lg n lg number buckets fail uniquely select block ndominant probability t1 4 proof lemma 54 number buckets fail uniquely select memory block dominant probability 1923 20 20 24 holds also ndominant probability since 19 20 55 failures hashing considering buckets uniquely selected block fail find injective level2 function draw special attention buckets size lemma 56 number buckets larger fi ndominant probability t1 4 proof let incorporating appropriate values stirling numbers second kind fact 26 get 17 therefore fact 27 ndominant probability 6 follows number buckets bigger fi ndominant probability 3116 18 20 20 20 24 analysis hashing failures buckets small enough split two cases lemma 57 suppose 2k n number buckets size fi fail hashing step iteration ndominant probability t1 4 proof without loss generality may assume exactly active buckets size fi participate step 2 bucket mapped memory block size k probability mapping noninjective corollary 23 fi 2 probability bucket fails hashing attempts therefore 12k let total number failing buckets fact 21 1823 20 20 2024 note since 2k n holds ndominant probability done lemma 58 suppose n repeating hashing step iteration constant number times get proof thus therefore 18 36 constant ffi 0 recall proof lemma 57 bucket fails hashing step probability 12k 37 iteration body repeated d2ffie failure probability bucket becomes 2k lemma follows markov inequality 6 reducing number random bits section show reduce number random bits used hashing algorithm algorithm described previous section consumes thetan lg u random bits first iteration already uses thetan lg u random bits subsequent iteration number random words u used constant factor larger memory used iteration resulting total thetan lg u random bits sequential hashing algorithm fredman komlos szemeredi 16 implemented olg lg u bits 11 show parallel hashing algorithm implemented olg lg u bits first show algorithm modified reduce number random bits olg u lg lg n first stage requires o1 random elements u construction level1 function remains unchanged iteration second stage required om random elements u modified follows allocation step bucket independently selects random memory block om lg random bits consumed reduced olg making use polynomial hash functions lemma 61 using 6 lg random bits set r mapped constant time array size 3m number colliding elements 2m 2 proof let selected random image bucket defined algorithmically h 1 first applied elements h 2 applied elements collided h 1 colliding elements g collided h 1 h 2 r 0 set elements collide h 1 clearly jr 16 consider following three cases corollary 23 prob r 0 6 2 follows fact 24 probability 2 jr 3 12gammaffl fact 22 2 markovs inequality therefore dominant probability jr case corollary 23 injective r 0 invoking procedure block allocation increase total memory consumption algorithm constant factor hashing step implementation hashing part iteration body using independent hash functions active buckets consumes om lg u random bits reduced olg u using hash functions pairwise independent technique application context hash functions essentially due 10 11 modification step follows hashing attempt executed step four global parameters selected random algorithm hash function attempted bucket hashing attempts bucket fully independent thus proof lemma 58 unaffected modification recall fact 21 assumes pairwise independence since pairwise independent proof lemma 57 remains valid well leads reduction number random bits used algorithm olg u lg lg n number random bits reduced follows employ preprocessing hashing step input set injectively mapped range 0 done applying hash function selected appropriate class map universe u range algorithm described used build hash table set lookup key x done searching x hash table simple class hash functions h 3 appropriate universe reduction application shown 11 class h 3 following properties 1 selection random function class requires olg lg u bits 2 selection made constant time single processor 3 function injective ndominant probability 4 computing x x 2 u done constant time preprocessing tantamount reduction size universe application algorithm requires olg n lg lg n bits total number random bits used therefore olg lg u 7 obtaining optimal speedup description algorithm section 3 assumed number processors n thus timeprocessor product lg lg n objective section workoptimal implementation product p number processors maximized array bucket array divided p sectors one per processor parallel step algorithm executed processor traverse sector execute tasks included key active bucket active let n number active keys beginning iteration assume implemented algorithm reached point assume active elements gathered array size lg lg n applying nonoptimal algorithm section 3 p n lg lg n processor responsible np lg lg n problem instances gives running time lg lg n np workoptimal first show problem size reduced sufficiently application nonoptimal algorithm olg lg lg lg n iterations lemma 71 exists n n n ndominant probability proof number active buckets decreases doublyexponential rate seen lemma 52 see number keys decreases doublyexponential rate well show ndominant probability inequality 32 r 6n clearly holds summation active buckets convexity argument total number keys active buckets maximized active buckets equal size number active buckets bounded therefore inequality 40 obtained 41 replacing definition 23 substituting numerical values parameters using 16 20 lemma follows choosing appropriate value 0 respect 23 40 remains exhibit workefficient implementation first 0 steps algorithm implementation outputs active elements gathered array size lg lg n rest section dedicated description implementation algorithm progresses number active keys number active buckets de crease however decrease number active elements different sectors necessarily identical time implementing one parallel step proportional number active elements largest sector therefore crucial occasionally balance number active elements among different sectors order obtain work efficiency let load sector number active elements tasks load balancing algorithm takes input set tasks arbitrarily distributed among p sectors using p processors redistributes set load sector greater average load constant factor suppose load balancing algorithm whose running time using p processors lb p ndominant probability load balancing applied step size sector p describe simple workoptimal implementation load balancing applied first 0 parallel steps parallel step executes time order total time implementation order decreases least exponential rate total time order onp using load balancing algorithm 20 runs lb time conclude ndominant probability running time pprocessor machine load balancing algorithm applied consumes op lg lg p random bits bits used random mapping step similar allocation step hashing algorithm thus similar approach mapping procedure lemma 61 may established number random bits load balancing algorithm reduced olg p lg lg p finally remark using load balancing efficient yet simple way describe 23 yields faster workefficient implementation technique based carefully choosing appropriate times invoking load balancing procedure applies algorithm problem size exponential rate decrease hence applies implementation section 4 well implementation load balancing algorithm used olg n times resulting parallel hashing algorithm takes onplg lg n lg n time ndominant probability 8 model computation section give closer attention details implementation pram study type concurrent memory access required algorithm first present implementation collision extension weaker tolerant model proceed presenting implementation even weaker robust model hashtable constructed implementation supports searches olg lg n time finally examine concurrent read capability needed implementations 81 implementation collision tolerant describe implementation collision implementation also valid tolerant since step collision simulated constant time tolerant provided case linear memory used 32 initialization selection level1 hash function done single processor since level1 function polynomial constant degree selection done single processor read processors constant time using singe memory cell dmax flg lg u lg nge bits concurrentwrite operation required implementation stage bucket representatives algorithm template assumes bucket act single entity operations eg selecting random block selecting random hash function since usually several keys belong bucket necessary coordinate actions processors allocated keys simple way based fact linearly many buckets bucket uniquely indexed value f level1 hash function members processor whose index determined bucket index acts bucket representative performs actions prescribed algorithm bucket allocation hashing steps processor representing active bucket selects memory block level2 hash function records selections designated cell processors keys bucket read cell use selected block hashing step participating processor whose key belongs active bucket writes key cell determined level examines cell contents see write operation successful processor write failed attempt write key position array ptr number bucket processor belongs processors belonging bucket learn level2 function selected bucket injective reading content ptri change value collision symbol indicate noninjectiveness complete process array ptr restored next hashing attempt restoration done constant time since array linear size summary proposition 81 algorithms theorem 1 implemented tolerant 82 implementation robust describe implementation expense slowing lookup operation makes assumption result concurrentwrite cell specifically present implementation robust model lookup query may take olg lg n time worst case o1 expected time keys table difficulty robust model letting processors bucket know whether level2 hash function bucket injective main idea modified implementation allowing iterations proceed without determining whether level2 hash functions injective whenever key written memory cell hashing step deactivated bucket size decreases modified algorithm performs least well implementation bucket deactivated keys mapped injectively total memory used modified algorithm size representation hash table change allocation step first note algorithm carried without using bucket representatives allocation memory blocks done using hash functions lemma 61 processor individually compute index memory block evaluating function g function selected designated processor representation 6 lg bits read constant time processors modify algorithm hashing step carried active buckets even buckets collided allocation step participate hashing step modification serve improve performance algorithm since even sharing block another bucket probability bucket finds injective function block zero modification eliminates concurrent memory access needed detecting failures allocation step hashing step selection level2 hash function done hashing step described section 6 seen 39 four global parameters selected made available processors done constant time remains eliminate concurrent memory access required determining level2 function single bucket injective whenever key successfully hashed function deactivated even keys bucket successfully hashed thus keys bucket may stored hash table using different level2 hash functions two steps iteration hashing algorithm summarized figure 3 let x active key bucket fx processor assigned x executes following steps allocation compute index memory block selected bucket x g defined 38 hashing determine h level2 hash function selected bucket x h defined 39 write x cell h x memory block g read contents cell x written key x becomes inactive figure 3 implementation iteration hashing algorithm robust lookup algorithm search key x done follows let read position h x memory block g appropriate array random bits used hash table construction algorithm assumed recorded available search terminated either x found else exceeds number iterations construction algorithm lookup algorithm requires olg lg n iterations worst case however key x 2 expected lookup time random selections made hashing algorithm o1 alternative simplified implementation curiously sequence modifications algorithm described section lead 1level hashing scheme ie elimination indirect addressing see observe iteration active key x written memory cell g x function g x dependent n random selections made algorithm input even simpler implementation 1level hashing algorithm delineated next iteration new array size 3m used defined 19 addition function g defined 38 selected random processor representing active key x iteration tries write x g x reads cell x successfully written g x x deactivated otherwise x remains active processor representing carries next iteration see algorithm terminates olg lg n iterations observe operation keys iteration operation buckets allocation step section 6 therefore analysis section 6 reused substituting keys buckets ignoring failures hashing step 2level algorithm hash table consists collection arrays easily verified linear size lookup query given key x executed olg lg n time reading g x 83 minimizing concurrent read requirements algorithms construction hash table tolerant robust modified use concurrentread single cell allowing preprocessing stage olg n time concurrent read eliminated implying ercw model sufficient modifications parallel lookups still require concurrent read execution time increases olg lg n worst case nevertheless expected time lookup single key x 2 o1 details described next 831 concurrent read tolerant implementation two types concurrent read operations required modified algorithm first sequence olg lg n functions g alternatively g simplified implementation must agreed upon processors since functions represented olg u bits selection broadcasted beginning iteration concurrentread cell single cell concurrent read requirement broadcasting eliminated adding olg ntime preprocessing step broadcasting special case simulating crcw pram erew pram kind concurrentread operation occurs processors read memory cell verify hashing cell succeeded operation replaced following procedure memory cell processor standing whenever pair hx ji written cell processor assigned cell sends acknowledgement processor j writing memory cell j designated array lookup algorithm requires concurrentread capabilities sense lookup operation demanding construction hash table similar phenomenon observed karp luby meyer auf der heide 36 context simulating random access machine distributed memory machine main challenge design parallelhashing based simulation algorithm execution read step congestions execution write step resolved attempting write several locations using first write succeeded difficult resolve read congestions since cells values stored already determined indeed read operation constitutes main runtime bottleneck algorithm 832 concurrent read robust implementation simplified 1level hashing algorithm construction hash table robust modified follows eliminate step processor key x reads contents cell trying write cell instead use acknowledgement technique described processor j handling active key x writes hx ji cell g x processor standing cell g x hx ji written sends acknowledgement processor j note implementation introduces new type failures due unpredictability concurrent write operation robust acknowledgement successful hash may received consider example following situation let j processor whose key x collide let 0 two processors colliding keys 0 ie g two processors concurrently write pairs hy ii hy cell g result concurrent write arbitrary particular pair hx 0 ji would cause processor standing cell g garble acknowledgement sent processor j recall acknowledgement processor j implemented writing memory location associated j number new failures described half number colliding keys easy verify analysis remains valid since number new failures number hashing failures accounted section 55 occur implementation 9 hashing multisets conclude technical discussion briefly considering variation hashing problem input multiset rather set first note analyses exponential doublyexponential rate decrease problem size affected possibility multiple occurrences key result relying estimates number active buckets rather number active keys number distinct keysnot number keysdetermines probability bucket find injective function predictable decrease number active keys essential obtaining optimal speedup algorithm unfortunately analysis section 7 regard implementation section 5 hold understand difficulty consider case substantial fraction input consists copies key nonnegligible probability key may belong large bucket probability bucket deactivates first iterations memory blocks sufficiently large small allow global decrease number keys high probability consequently rapid decrease number buckets may accompanied similar decrease number keys contrast nature analysis section 4 makes susceptible easy extension multiple keys leads optimal speedup algorithm albeit expected performance using probabilistic induction lemma required show copy active key stands constant positive probability deactivation iteration since analysis based expectations concerns regarding correlations copies key dependencies different iterations details left reader also note model computation required multiset collision since must possible distinguish case multiple copies key written memory cell case distinct keys written also extensions hashing algorithms require concurrent read single memory cell used hashing multiset input collision model opposed robust must assumed finally observe hashing problem multiset input reduced ordinary hashing problem input consists set procedure known leaders election procedure selects single representative among processors share value using olg lg ntime linearwork leaders election algorithm runs tolerant 24 theorem 2 given multiset n keys drawn universe u hashing problem solved using space olg lg n time high probability using n processors ii olg lg n lg n time operations high probability algorithms run tolerant conversely note hashing algorithm run arbitrary solves leaders election problem particular simple 1level hashing algorithm robust implemented arbitrary multiset input gives simple leaders election algorithm consider another variant multiset hashing problem data record associated key natural semantics problem multiple copies key inserted hash table data records identical processors representing copies key conflicting data records terminate computation error code collision model makes easy enough extend implementations discussed accommodate variant sophisticated semantics data records consolidated requires different treatment eg applying integer sorting algorithm hashed keys see 39 conclusions presented novel technique hashing oblivious execution using technique algorithms constructing perfect hash table fast simple efficient made possible running time obtained best possible model keys handled original processors number random bits consumed algorithm thetalg lg u n open question close gap number thetalg lg u bits consumed sequential hashing algorithm 11 program executed processor extremely simple indeed coordination processors computing function testing injectiveness implementation robust model even coordination eliminated large constants hidden oh notation analysis may render described implementations still far practical believe constants substantially improved without compromising simplicity algorithm careful tuning parameters tightened analysis may interesting subject separate research usefulness oblivious execution approach presented paper limited hashing problem alone adopted 24 simulations among submodels crcw pram hashing algorithm keys partitioned subsets however partition arbitrary given input subset maximum key must computed subsequent work oblivious execution technique hashing section 3 implementation section 4 presented preliminary form 21 subsequently oblivious execution technique used several times obtain improvements running time parallel hashing algorithms matias vishkin 38 gave olg n lg lg n expected time algorithm gil matias vishkin 26 gave tighter failure probability analysis algorithm 38 yielding olg n time high similar improvement olg n lg lg n expected time olg n time high probability described independently bast hagerup 3 olg n time hashing algorithm used building block parallel dictionary algorithm presented 26 parallel dictionary algorithm supports parallel batches operations insert delete lookup oblivious execution technique important role implementation insertions dictionary dictionary algorithm runs olg n time high probability improving ffl dictionary algorithm dietzfelbinger meyer auf der heide 13 dictionary algorithm used obtain space efficient implementation parallel algorithm cost slowdown olg n time high probability hashing algorithms use logstar paradigm 38 relying extensively processor reallocation simple algorithm presented paper moreover require substantially larger number random bits karp luby meyer auf der heide 36 presented efficient simulation pram distributed memory machine doublylogarithmic time level improving previous simulations logarithmic time level use fast parallel hashing algorithm essential result algorithm presented sufficient obtain goldberg jerrum leighton rao 28 used techniques paper obtain oh lg lg n randomized algorithm hrelation problem optical communication parallel computer model gibbons matias ramachandran 18 adapted algorithm presented obtain lowcontention parallel hashing algorithm qrqw pram model 19 implies efficient hashing algorithm valiants bsp model hence hypercubetype noncombining networks 44 acknowledgments thank martin dietzfelbinger faith e fich providing helpful comments also wish thank uzi vishkin avi wigderson early discussions part research done visits first author att bell laboratories second author university british columbia would like thank institutions supporting visits many valuable comments made two anonymous referees gratefully acknowledged r probabilistic method parallel construction suffix tree fast reliable parallel hashing optimal bounds decision problems crcw pram improved deterministic parallel integer sorting optimal separations concurrentwrite parallel machines observations concerning synchronous parallel models computation universal classes hash functions new simulations crcw prams power twopoint based sampling polynomial hash functions reliable relations concurrentwrite models parallel computation storing sparse table o1 worst case access time data structures algorithms approximate string matching efficient lowcontention parallel algo rithms qrqw pram accounting contention parallel algorithms fast load balancing pram fast hashing pramdesigning expectation designing algorithms expectations effective load balancing policy geometric decaying algorithms fast efficient simulations among crcw prams simple fast parallel hashing towards theory nearly constant time parallel algorithms doubly logarithmic communication algorithms optical communication parallel computers universal interconnection pattern parallel computers optimal parallel approximation algorithms prefix sums integer sorting concrete mathematics incomparability parallel computation towards optimal parallel bucket sorting every robust crcw pram efficiently simulate priority pram introduction parallel algorithms sorting searching converting high probability nearlyconstant timewith applications parallel hashing parallel hashing integer sorting data structures algorithms sorting searching data structures olg n parallel connectivity algorithm universal classes fast high performance hash functions general purpose parallel architectures tr