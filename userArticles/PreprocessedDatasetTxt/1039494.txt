fast montecarlo algorithms finding lowrank approximations consider problem approximating given n matrix another matrix specified rank k smaller n singular value decomposition svd used find best approximation however takes time polynomial n prohibitive modern applications article develop algorithm qualitatively faster provided may sample entries matrix accordance natural probability distribution many applications sampling done efficiently main result randomized algorithm find description matrix rank k holds probability least 1 verbarverbarf frobenius norm algorithm takes time polynomial k1epsi log1 independent n particular implies constant time determined given matrix arbitrary size good lowrank approximation b introduction realworld data often large number attributes featuresdimensions natural question whether fact generated small model ie much smaller number parameters number attributes one way formalize question problem finding lowrank approximation given n matrix find matrix rank k f small possible matrix frobenius norm f defined f alternatively view rows points r n problem finding kdimensional linear subspace minimizes sum squared distances points problem arises many contexts partly matrix algorithms ecient lowrank matrices also interesting consider norms eg 2norm see section 6 mostly focus frobenius norm traditional singular value decomposition svd used solve problem time ominmn 2 nm 2 many applications motivated information retrieval web slow one needs linear sublinear algorithm speed svdbased lowrank approximation papadimitriou et al 2000 suggested random projection preprocessing step ie project rows olog ndimensional subspace find svd subspace reduces worstcase complexity omn log n small loss approximation quality still high fast problem solved first sight seems mn lower bound single nonzero entry one examine entries find good approximation suppose sample entries probability proportional magnitudes constantsized sample would suce case paper show rank k approximation found time polynomial k 1 error parameter provided sample entries natural probability distribution sampling assumptions made explicit shortly also discussed context applications main result following theorem 1 given mn matrix k randomized algorithm finds description matrix rank k drankdk f f holds probability least 1 algorithm takes time polynomial k 1 log1 independent mn complex computational task find first k singular values randomly chosen submatrix matrix explicitly constructed description okmn time consequence polyk 1 time determine high probability rank k approximation error f error probability boosted standard techniques prove theorem fixed error probability central idea approach described follows pick p rows fast lowrank approximation 3 independently random according probability distribution satisfying assumption a1 see section 11 suppose rows form pm matrix rows scaled form matrix step 1 algorithm section 4 relatively easy lemma 2 show approximately equals intuition jth entry dot product ith jth columns indeed since random sample rows entry ij estimates scaling done make estimate unbiased standard linear algebra get svd spectral decomposition sd 1 therefore approximately sd repeating sd read svd turn obtained sd ss since ss pp matrix problem reduced computing svd constant sized matrix still leaves computation ss apply sampling trick second time pick sample p columns form pp matrix w step 2 algorithm ww approximates ss sd ww needed svd w suces central computational task algorithm present algorithm section 4 besides lemma 2 key step analysis showing go approximate left singular vectors approximate right singular vectors small loss key insight paper basis algorithm existence good lowrank approximation subspace spanned small sample rows state formally constant c defined assumption a1 theorem 2 let n matrix sample rows distribution satisfying assumption 1 let v vector space spanned probability least 910 exist orthonormal set vectors 1 2 k v drankdk f cs f 1 useful note k restriction linear transformation subspace spanned j namely x subspace x x orthogonal subspace elementary linear algebra matrix k rows span vectors 1 k therefore rank k describe 1 right singular vectors precisely eigenvectors alan frieze et al approximation suces give vectors 1 k algorithm vectors computed multiplying submatrix set vectors r p thus matrix recovered set k pdimensional vectors set p numbers indicating submatrix spanned corresponding rows also follows first k singular values computed within cumulative additive error f section 3 give proof existence theorem theorem directly gives omnkpolyk algorithm suggests algorithm whose running time linear small polynomial k 1 algorithm developed following paper drineas et al 2004a subsequent developments discussed section 6 throughout paper denotes ith row matrix row vector j denotes jth column column vector ij entry ith row jth column also positive integer r r denotes set 1 2 r 11 sampling assumptions describe sampling assumptions detail assumption 1 sample rows row chosen probability f constant c 1 independent mn denotes euclidean length known us dont need know p assumption 2 row sample entry j probability q ji satisfying f q ji known us need know values matrix known sparsity structure might able set sampling little preprocessing particular matrix dense ie j f constant c take general matrix making one pass entire matrix set data structures let us sample entries fast onwards time per sample satisfy assumptions 1 2 pass several things suppose j fast lowrank approximation 5 create olog bins pass put lth bin entries j 2 l1 also keep track number entries bin treat entries bin value sample pick bin probability proportional total sum squares bin pick entry uniformly set entries bin pass also set similar data structures row 12 applications section discuss algorithm context applications rely lowrank approximation show several situations satisfy sampling assumptions algorithm thus obtain svd approximation eciently applications discuss include face recognition picture compression 121 latent semantic indexing general technique processing collection documents give cursory description broad area discuss relation main problem see berry et al 1995 deerwester et al 1990 dumais et al 1988 dumais 1991 details empirical results suppose documents n terms occur documents terms may words occur documents key words occur model hypothesizes small number k unknown topics documents topic modelled probability distribution n terms ie nvector nonnegative reals summing 1 model hand shown additional assumptions subspace spanned k best topics close span top k singular vectors socalled documentterm matrix papadimitriou et al 2000 latter n matrix ij frequency jth term ith document alternatively one define ij 0 1 depending upon whether jth term occurs ith document argue practice assumptions algorithm satisfied used place full svd algorithm easy see allowed one pass document set data structures sampling ideally creator document could supply vector squared term frequencies otherwise frequency large unreasonable since words occur often socalled buzz words removed analysis need precompute length l document typically available say file pick document probability proportional length easily seen satisfy assumption 1 without squares ie sample ith entry probability l assumption squares satisfied frequencies small assumption 2 similarly implemented given document pick word uniformly random ie q 122 web search model kleinberg kleinberg 1999 proposed algorithm problem finding important documents set documents returned web search works analyzing matrix entries ij equal 1 0 depending upon whether ith alan frieze et al document points jth algorithm sets find two unitlength mvectors x x ay maximized course problem finding singular vectors keyword multiple meanings top singular vectors large singular values interesting interest find largest k singular vectors small k worthwhile consider assumptions case assumption 1 sucient sample documents roughly according number hypertext links assumption 2 sucient able follow random link document 123 lowrank approximations regularity lemma fundamental regularity lemma szemeredis graph theory szemeredi 1978 gives partition vertex set graph pairs parts nearly regular give details lemma host applications see komlos simonovits 1996 graph theory lemma nonconstructive asserted existence partition give algorithm find alon duke lefmann rodl yuster finally able give algorithm find partition polynomial time alon et al 1994 frieze kannan 1996 frieze kannan 1999a lowrank approximations adjacency matrix graph related regular partitions szemeredis lemma algorithm constructing partition derived connection directly relevant results point one case lowrank approximations useful direct application eigenvector computation szemeredis partition given frieze kannan 1999b 2 singular value decomposition matrix expressed r u form orthonormal set column vectors v also u av called singular value decomposition r rank theorem eckart young golub van loan 1989 matrix k minimizes df among matrices rank k less given implies f k 2 f r use notation throughout paper fast lowrank approximation 7 3 small sample induces good approximation goal section prove theorem 2 namely subspace spanned sample rows chosen according assumption 1 contains approximation nearly best possible chance v 1 v k belong subspace would done since k would provide required approximation show vectors w defined shortly subspace approximate scaled versions respective v let random sample rows chosen distribution satisfies assumption 1 define vectorvalued random variable note general multiset ie rows might picked multiple times vectors w clearly subspace generated first compute expectation w view average iid random variables following distribution probability p taking expectations 2 ew since p c f since u ew f 2 w exactly equal v instead expectation would sucient prove theorem wish carry approximately end define 2 2 expectation variance vector valued random variable taken separately component alan frieze et al let 1 2 n orthonormal basis r n l dimension v 1 let l ay matrix f candidate approximation span bound error using f note k j l thus f ay fy f 3 also f w taking expectations using 2 get ea f f 4 f rank k k best rank k approximation f thus f nonnegative random variable 4 implies 3 follows f next observe good lowrank approximation respect frobenius norm implies good lowrank approximation respect 2norm fast lowrank approximation 9 approximations since obtained 2norm see section 6 theorem 3 f f proof let suppose b unit eigenvector x eigenvalue f see f f 5 rank matrix k bxx k 1 f f since 2 f contradicts 5 4 sampling algorithm section present main constanttime algorithm produce approximation theorem 1 first pick set p rows distribution satisfying assumption 1 form matrix rows scaling pick p columns probability distribution satisfying assumption 2 scale columns get p p matrix w find singular vectors matrix show get good lowrank approximation reader might want consult discussion statements theorems 1 2 introduction intuitive idea algorithm works description parameter c one assumptions alan frieze et al algorithm input matrix integer k 0 error parameter 0 set 1 sample rows independently choose rows according distribution assumption 1 ie f let p n matrix rows 1 scaling amounts normalizing rows length 2 sample columns independently choose columns according distribution p f show using assumption 2 let w p p matrix columns j 3 compute svd compute top k singular vectors u 1 u k column space w 8k output v low rank approximation tt next discuss implementation algorithm particular carry step 2 first pick row row probability 1p suppose chosen row ith row pick j 1 2 n probabilities q ji assumption 2 defines probabilities p row f f f f fast lowrank approximation 11 last step implied next lemma lemma 1 w chosen algorithm probability least f 1 f proof routine calculation f f next observe row f cp random variable 2 f sum p independent random variables therefore f f f first part lemma follows using chebychevs inequality proof second part similar 5 analysis next lemma asserts sample n rows matrix provides good approximation sense n n close key tool analysis lemma 2 let b matrix let probability distribution 1 2 f 0 1 let sequence p independent samples chosen according distribution q let n p b matrix 0 f alan frieze et al proof f en en f f thus em f en f f f result follows markovs inequality introduce notation rest analysis matrix vectors x define x x f x orthogonal unit vectors represents norm projection onto subspace spanned x case fast lowrank approximation 13 thus x k top k singular vectors lemma 3 let matrices number columns f 1 pair unit vectors z z row space f set unit vectors z 1 z 2 z k row space f proof first part lemma easy second using fact f matrix n see ax equals traz z z az z az z az z az z az exactly similar reasoning z z using first part lemma second part follows ready prove main theorem proof theorem 1 prove conclusion theorem holds probability least 34 apply lemma 2 twice row sample induced column sample follows lemma 2 probability least 910 following events hold f ss f cp 500k c 32 assume events occur throughout proof approximate constants arise somewhat crudely convenient rationals follows theorem 2 probability least 910 unit vectors x k row space alan frieze et al applying second part lemma 3 vectors x see f f using 7 singular values exist unit vectors k column space f applying theorem 2 w see probability least 910 unit vectors z k column space w f applying second part lemma 3 w vectors z see f therefore vectors u k computed algorithm satisfy f note highest possible value ax k k 2 f remains show fact w large implies large construct suitable set vectors algorithm since u k orthonormal singular vectors f applying lemma 3 time w vectors u follows f next crucial step switch u column space v row space achieved following claims whose proof defer section 51 f 2 v follows lemma 3 f assuming 16 thus f rearranging terms get conclusion theorem fast lowrank approximation 15 51 proof claims 1 2 observe first f f f consider v furthermore 2 f 10 similarly using 8 2 using bounds 10 11 9 f f f f f using bounds lemma 1 next vector u matrix ss u u 4 observe first part lemma 3 implies f 12 immediately ss u alan frieze et al f completes proof claim 1 since 9k 2 2 2 16 value p chosen satisfy 12 6 recent work several developments problem lowrank approximation since preliminary version paper frieze et al 1998 appeared drineas et al drineas et al 2004a give algorithm whose running time omr although theoretically much slower due dependence practice better dependence k 1 might make practical alternative samplingbased algorithm given achlioptas mcsherry 2001 comparable bounds frobenius norm significantly better bounds 2norm main idea sparsify matrix randomly sampling entries one one compute svd sparse matrix faster baryossef 2003 lower bound lowrank approximation given essentially matches bound drineas et al 2004a also shown algorithm complexity possible using uniform sampling drineas et al 2004a current paper get implicitly defined lowrank approximation explicit approximation given drineas kannan 2003 shows c matrix columns picked sampling r n matrix rows picked random cur u matrix computed c thus explicit approximation preserving sparsity structure paper drineas kannan 2001 applied sampling idea basic problem multiplying two matrices b showed sample columns according probability distributions similar paper take corresponding rows b product approximates whole product ab finally improvement analysis terms number rows need sampled obtained drineas et al 2004b r fast computation low rank approximations algorithmic aspects regularity lemma sampling lower bounds via information theory using linear algebra intelligent information retrieval indexing latent semantic analysis fast montecarlo algorithms approximate matrix multiplica tion pass ecient algorithms approximating large matrices clustering large graphs via singular value decomposition fast monte carlo algorithms matrices ii computing lowrank approximations matrix using latent semantic analysis improve information retrieval improving retrieval information external sources regularity lemma approximation schemes dense prob lems quick approximations matrices applications simple algorithm constructing szemeredis regularity parti tion fast montecarlo algorithms finding lowrank ap proximations matrix computations authoritative sources hyperlinked environment szemeredis regularity lemma applications graph ory latent semantic indexing probabilistic analysis regular partitions graphs tr ctr petros drineas michael w mahoney muthukrishnan sampling algorithms l2 regression applications proceedings seventeenth annual acmsiam symposium discrete algorithm p11271136 january 2226 2006 miami florida sariel harpeled get close median shape computational geometry theory applications v36 n1 p3951 january 2007 sariel harpeled get close median shape proceedings twentysecond annual symposium computational geometry june 0507 2006 sedona arizona usa dimitris achlioptas frank mcsherry fast computation lowrank matrix approximations journal acm jacm v54 n2 p9es april 2007 w fernandez de la vega marek karpinski ravi kannan santosh vempala tensor decomposition approximation schemes constraint satisfaction problems proceedings thirtyseventh annual acm symposium theory computing may 2224 2005 baltimore md usa amit deshpande luis rademacher santosh vempala grant wang matrix approximation projective clustering via volume sampling proceedings seventeenth annual acmsiam symposium discrete algorithm p11171126 january 2226 2006 miami florida