impact exploiting instructionlevel parallelism sharedmemory multiprocessors abstractcurrent microprocessors incorporate techniques aggressively exploit instructionlevel parallelism ilp paper evaluates impact processors performance sharedmemory multiprocessors without latencyhiding optimization software prefetching results show ilp techniques substantially reduce cpu time multiprocessors less effective removing memory stall time consequently despite inherent latency tolerance features ilp processors find memory system performance larger bottleneck parallel efficiencies generally poorer ilpbased multiprocessors previous generation multiprocessors main reasons deficiencies insufficient opportunities applications overlap multiple load misses increased contention resources system also find software prefetching change memory bound nature applications ilp multiprocessor mainly due large number late prefetches resource contention results suggest need additional latency hiding reducing techniques ilp systems software clustering load misses producerinitiated communication b introduction sharedmemory multiprocessors built commodity microprocessors increasingly used provide high performance variety scientific commercial ap plications current commodity microprocessors improve performance using aggressive techniques exploit high levels instructionlevel parallelism ilp techniques include multiple instruction issue outoforder dy namic scheduling nonblocking loads speculative ex ecution refer techniques collectively ilp techniques processors exploit techniques ilp processors previous studies sharedmemory multiproces work supported part ibm partnership award intel corporation national science foundation grant ccr9410457 ccr9502500 cda9502791 cda9617383 texas advanced technology program grant 003604025 sarita adve also supported alfred p sloan research fellowship vijay pai fannie john hertz foundation fellowship parthasarathy ranganathan lodieska stockbridge vaughan fellowship paper combines results two previous conference papers 11 12 using common set system parameters aggressive mesi versus msi cachecoherence protocol aggressive compiler better sparc sc 42 gcc 272 application rather gcc 258 full simulation private memory references sors however assumed simple processor single issue inorder scheduling blocking loads specu lation multiprocessor architecture studies model stateoftheart ilp processors 2 7 8 9 analyze impact ilp techniques fully exploit recent advances uniprocessor technology sharedmemory multiprocessors detailed analysis ilp techniques affect performance systems interact previous optimizations systems required paper evaluates impact exploiting ilp performance sharedmemory multiprocessors without latencyhiding optimization software prefetching evaluations study five applications using detailed simulation described section ii section iii analyzes impact ilp techniques performance sharedmemory multiprocessors without use software prefetching applications see performance improvements use current ilp techniques improvements vary widely partic ular ilp techniques successfully consistently reduce cpu component execution time impact memory stall time lower application dependent consequently despite inherent latency tolerance features integrated within ilp processors find memory system performance larger bottleneck parallel efficiencies generally poorer ilpbased multiprocessors previousgeneration multiproces sors deficiencies caused insufficient opportunities application overlap multiple load misses increased contention system resources frequent memory accesses softwarecontrolled nonbinding prefetching shown effective technique hiding memory latency simple processorbased shared memory systems 6 section iv analyzes interaction software prefetching ilp techniques sharedmemory multiprocessors find compared previousgeneration systems increased late prefetches increased contention resources cause software prefetching less effective reducing memory stall time ilpbased systems thus even adding software prefetching applications remain largely memory bound ilpbased system results suggest compared previousgeneration 2sharedmemory systems ilpbased systems greater need additional techniques tolerate reduce memory latency specific techniques motivated results include clustering load misses applications increase opportunities load misses overlap techniques producerinitiated communication reduce latency make prefetching effective section v ii methodology simulated architectures determine impact ilp techniques multiprocessor performance compare two systems ilp simple equivalent every respect except processor used ilp system uses stateoftheart ilp processors simple system uses simple processors section ii a1 compare ilp simple systems suggest architectural tradeoffs rather understand aggressive ilp techniques impact multiprocessor per formance therefore two systems identical clock rates include identical aggressive memory network configurations suitable ilp system section iia2 figure 1 summarizes system parameters a1 processor models ilp system uses stateoftheart processors include multiple issue outoforder dynamic scheduling nonblocking loads speculative execution simple system uses previousgeneration simple processors single issue inorder static scheduling blocking loads represents commonly studied sharedmemory systems since access compiler schedules instructions inorder simple processor assume singlecycle functional unit latencies also assumed previous simpleprocessor based sharedmemory stud ies processor models include support softwarecontrolled nonbinding prefetching l1 cache a2 memory hierarchy multiprocessor configuration simulate hardware cachecoherent nonuniform memory access ccnuma sharedmemory multiprocessor using invalidationbased fourstate mesi directory coherence protocol 4 model release consistency previous studies shown achieves best performance 9 processing nodes connected using twodimensional mesh network node includes proces sor two levels caches portion global sharedmemory directory network interface splittransaction bus connects network interface directory controller rest system node caches use writeallocate writeback policy cache sizes chosen commensurate input sizes ap plications following methodology described woo et al 14 primary working sets applications fit l1 cache secondary working sets fit l2 cache caches nonblocking use miss processor parameters clock rate 300 mhz fetchdecoderetire rate 4 per cycle instruction window order buffer sizememory queue size outstanding branches 8 functional units 2 alus 2 fpus 2 address generation units 1 cycle latency memory hierarchy network parameters mshrs 64byte line l2 cache 64 kb 4way associative 1 port 8 mshrs 64byte line pipelined memory 4way interleaved ns access time bus 100 mhz 128 bits split transaction network 2d mesh 150mhz 64 bits per hop flit delay 2 network cycles nodes multiprocessor 8 resulting contentionless latencies processor cycles local memory 45 cycles remote memory 140220 cycles cachetocache transfer 170270 cycles fig 1 system parameters status holding registers mshrs 3 store information outstanding misses coalesce multiple requests cache line multiprocessor results reported paper use configuration 8 nodes b simulation environment use rsim rice simulator ilp multipro cessors model systems studied 10 rsim executiondriven simulator models processor pipelines memory system interconnection network detail including contention resources takes application executables input speed simulations assume instructions hit instruction cache assumption reasonable since applications small instruction footprints c performance metrics addition comparing execution times also report individual components execution time cpu data memory stall synchronization stall times characterize performance bottlenecks systems ilp processors unclear assign stall time specific instructions since instructions execution may overlapped preceding following instructions use following convention similar previous work eg 5 account stall cycles every cycle calculate ratio instructions retired instruction window cycle maximum retire rate processor attribute fraction cycle busy time remaining fraction cycle attributed stall time first instruction could retired cycle group busy time functional unit nonmemory stall time together cpu time henceforth use term memory stall time denote data memory stall component execution time first part study key metric used application input size lu luopt 256x256 matrix block 8 fft fftopt 65536 points mp3d 50000 particles water 512 molecules fig 2 applications input sizes evaluate impact ilp ratio execution time simple system relative achieved ilp system call ilp speedup detailed analysis analogously define ilp speedup component execution time applications figure 2 lists applications input sets used study radix lu fft splash suite 14 water mp3d splash suite 13 five applications input sizes chosen ensure reasonable simulation times since rsim models aggressive ilp processors detail 10 times slower simpleprocessorbased sharedmemory simulators luopt fftopt versions lu fft include ilpspecific optimizations potentially implemented compiler specifically use function inlining loop interchange move load misses closer overlapped ilp processor impact optimizations discussed sections iii v versions lu also modified slightly use flags instead barriers better load balance since sparc compiler ilp system ex ist compiled applications commercial sun sc 42 gcc 272 compiler based better simulated ilp system performance full optimization turned compilers deficiencies addressing specific instruction grouping rules ilp system partly hidden outoforder scheduling ilp processor 2 iii impact ilp techniques performance section analyzes impact ilp techniques multiprocessor performance comparing simple ilp systems without software prefetching overall results figures 3 4 illustrate key overall results application figure 3 shows total execution time three components simple ilp systems normalized total time simple system ad ditionally bottom figure also shows ilp speedup application figure 4 shows parallel efficiency 3 ilp simple systems expressed percentage figures show three key trends ffl ilp techniques improve execution time ap plications however ilp speedup shows wide vari 2 best knowledge key compiler optimization identified paper clustering load misses implemented current superscalar compiler 3 parallel efficiency application system n processors defined execution time uniprocessor execution time multiprocessor theta 1 ation 129 mp3d 354 luopt average ilp speedup original applications ie including luopt fftopt 205 ffl memory stall component generally larger part overall execution time ilp system simple system ffl parallel efficiency ilp system less simple system applications next investigate reasons trends b factors contributing ilp speedup figure 3 indicates important components execution time cpu time memory stalls thus ilp speedup shaped primarily cpu ilp speedup memory ilp speedup figure 5 summarizes speedups along total ilp speedup figure shows low variable ilp speedup applications attributed largely insufficient variable memory ilp speedup cpu ilp speedup similar significant among applications ranging 294 380 detailed data shows applications memory stall time dominated stalls due loads miss l1 cache therefore focus impact ilp l1 load misses load miss ilp speedup ratio stall time due load misses simple ilp systems determined three factors described first factor increases speedup second decreases third may either increase decrease ffl load miss overlap since simple system blocking loads entire load miss latency exposed stall time ilp load misses overlapped useful work reducing stall time increasing ilp load miss speedup number instructions behind load miss overlap however limited instruction window size load misses longer latencies instructions instruction window fore load miss latency normally completely hidden behind load misses thus significant load miss ilp speedup applications multiple load clustered together within instruction window enable load misses overlap contention compared simple system ilp system see longer latencies increased contention due higher frequency misses thereby negatively affecting load miss ilp speedup ffl change number misses ilp system may see fewer misses simple system speculation reordering memory ac cesses thereby positively negatively affecting load miss ilp speedup applications except lu see similar number cache misses simple ilp case lu sees 25x fewer misses ilp reordering accesses otherwise conflict number misses change ilp system sees 1 load miss ilp speedup load miss overlap exploited ilp outweighs additional latency contention illustrate execution time ilp simple ilp415 241x simple ilp390 256x simple ilp340 294x simple ilp282 354x simple ilp782 129x simple ilp753 simple ilp449 230x memory cpu speedup fft fftopt lu luopt mp3d radix water fig 3 impact ilp multiprocessor performance parallel efficiency 86 82 fft lu water simple ilp fig 4 impact ilp parallel efficiency fig 5 ilp speedup total execution time cpu time memory stall time multiprocessor system effects load miss overlap contention using two applications best characterize luopt radix figure 6a provides average load miss latencies luopt radix simple ilp systems normalized simple system latency latency shown total miss latency measured address generation data arrival including overlapped part ilp exposed part contributes stall time difference bar lengths simple ilp indicates additional latency added due contention ilp applications see significant latency increase resource contention ilp however luopt overlap additional latency well large portion base simple latency thus leading high memory ilp speedup hand radix cannot overlap additional latency thus sees load miss slowdown ilp configuration use data figures 6b c investigate causes load miss overlap contention related latencies applications causes load miss overlap figure 6b shows ilp systems l1 mshr occupancy due load misses luopt radix curve shows fraction total time least n mshrs occupied load misses possible n x axis figure shows luopt achieves significant overlap load misses 8 load miss requests outstanding simultaneously various times contrast radix almost never 1 outstanding load miss time difference arises load misses clustered together instruction window luopt typically separated many instructions radix causes contention figure 6c extends data figure 6b displaying total mshr occupancy load store misses figure indicates radix large amount store miss overlap overlap contribute increase memory ilp speedup since store latencies already hidden simple ilp systems due release consistency store miss overlap however increases contention memory hi erarchy resulting ilp memory slowdown radix luopt contentionrelated latency comes primarily load misses effect mitigated since overlapped load misses contribute reducing memory stall time c memory stall component parallel efficiency using analysis see ilp system generally sees larger relative memory stall time component figure generally poorer parallel efficiency figure simple system since memory ilp speedup generally less cpu ilp speedup memory component becomes greater fraction total execution time ilp system simple system understand reduced parallel effi ciency figure 7 provides ilp speedups uniprocessor configuration reference uniprocessor also generally sees lower memory ilp speedups cpu ilp speedups however impact lower memory ilp speedup higher multiprocessor longer latencies remote misses increased contention result larger relative memory component execution time relative uniprocessor additionally dichotomy local remote miss latencies multiprocessor often tends decrease memory ilp speedup relative uniprocessor load misses must overlapped load misses load misses similar latencies 4 thus overall multiprocessor system less able exploit ilp features corresponding uniprocessor system applications 4 fft fftopt see better memory ilp speedups multiprocessor uniprocessor overlap multiple load misses similar multiprocessor remote latencies section code exhibits overlap greater impact multiprocessor longer remote latencies incurred section miss latency simple ilp simple ilp radix2092 overlapped stall utilization number l1 mshrs number l1 mshrs utilization number l1 mshrs number l1 mshrs effect ilp average l1 miss latency b l1 mshr occupancy due loads c l1 mshr occupancy due loads stores fig 6 load miss overlap contention ilp system fig 7 ilp speedup total execution time cpu time memory stall time uniprocessor system consequently ilp multiprocessor generally sees lower parallel efficiency simple multiprocessor iv interaction ilp techniques software prefetching previous section shows ilp system sees greater bottleneck memory latency simple system softwarecontrolled nonbinding prefetching shown effectively hide memory latency sharedmemory multiprocessors simple processors section evaluates software prefetching interacts ilp techniques sharedmemory multiprocessors followed software prefetch algorithm developed mowry et al6 insert prefetches applications hand one exception algorithm 6 assumes locality maintained across synchronization schedule prefetches across synchronization ac cesses removed restriction beneficial consistent comparison experiments reported prefetches scheduled identically simple ilp prefetches scheduled least 200 dynamic instructions corresponding demand accesses impact scheduling decision discussed including impact varying prefetch distance overall results figure graphically presents key results experiments fft fftopt similar performance fftopt appears figure figure shows execution time components application simple ilp without software prefetching pf indicates addition software prefetching execution times normalized time application simple without prefetching figure 9 summarizes key data software prefetching achieves significant reductions execution time ilp 13 43 three cases lu mp3d water reductions similar greater simple applications ever software prefetching less effective reducing memory stalls ilp simple average reduction 32 ilp ranging 7 72 vs average 59 range 21 88 simple net effect even prefetching applied ilp average memory stall time 39 ilp range 11 65 vs average 16 range 1 29 simple ap plications ilp system remains largely memorybound even software prefetching b factors contributing effectiveness software prefetching next identify three factors make software prefetching less successful reducing memory stall time ilp simple two factors allow ilp additional benefits memory stall reduction available simple one factor either help hurt ilp focus issues specific ilp systems previous work discussed nonilp specific issues 6 figure 10 summarizes effects exhibited applications studied negative effects first two important applications increased late prefetches last column figure 9 shows number prefetches late completely hide miss latency increases applications moving simple ilp one reason increase multipleissue outof order scheduling speed computation ilp decreasing computation time prefetch lapped simple also stalls load misses prefetched incur late prefetch thereby allowing outstanding prefetched data arrive cache ilp provide similar leeway increased resource contention shown section iii ilp processors stress system resources simple prefetches increase demand resources resulting contention greater memory latencies resources stressed configuration cache ports mshrs alus address generation units negative interaction clustered misses optimizations cluster load misses ilp system luopt potentially reduce effectiveness software prefetching example addition prefetching dexecution time lu memory cpu normalized execution time luopt memory cpu normalized execution time fftopt memory cpu normalized execution time mp3d memory cpu normalized execution time radix memory cpu normalized execution time water memory cpu fig 8 interaction software prefetching ilp duces execution time lu 13 ilp system contrast luopt improves 3 simple system lu luopt improve 10 prefetching luopt prefetching slightly better lu prefetching ilp 3 clustering optimization used luopt reduces computation successive misses contributing high number late prefetches increased contention prefetching overlapped accesses ilp accesses difficult prefetch may overlapped nonblocking loads outoforder scheduling prefetched lines lu luopt often suffer l1 cache conflicts resulting lines replaced l2 cache used demand accesses l2 cache latency results stall time simple overlapped processor ilp since prefetching ilp needs target accesses already overlapped ilp appear effective ilp simple fewer early prefetches early prefetches prefetched lines either invalidated replaced corresponding demand accesses early prefetches hinder demand accesses invalidating replacing needed data caches without providing benefits latency reduction many applications number early prefetches drops ilp improving effectiveness prefetching applications reduction occurs ilp system allows less time prefetch subsequent demand access decreasing likelihood intervening invalidation replacement speculative prefetches ilp prefetch instructions speculatively issued past mispredicted branch speculative prefetches potentially hurt performance bringing unnecessary lines cache bringing needed lines cache early speculative prefetches also help performance initiating prefetch needed line early enough hide latency appli cations prefetches issued past mispredicted branches lines also accessed correct path app reduction execution time reduction memory stall time maining memory stall time prefetches late ple ple ple ple mp3d 43 43 78 59 29 62 1 12 water average 14 14 59 fig 9 detailed data effectiveness software prefetching average lu luopt luopt considered since provides better performance lu prefetching ilp factor lu lu fft mp3d water radix opt opt late prefetches resource contention clustered load overlapped early prefetches speculative prefetches fig 10 factors affecting performance prefetching ilp c impact software prefetching execution time despite reduced effectiveness addressing memory stall time software prefetching achieves significant execution time reductions ilp three cases lu mp3d water two main reasons first memory stall time contributes larger portion total execution time ilp thus even reduction small fraction memory stall time imply reduction overall execution time similar greater seen simple second ilp systems see less instruction overhead prefetching compared simple systems ilp techniques allow overlap instructions computation alleviating late prefetches contention results show late prefetches resource contention two key limitations effectiveness prefetching ilp tried several straightforward modifications prefetching algorithm system address limitations 12 specifically doubled quadrupled prefetch distance ie distance prefetch corresponding demand access increased number mshrs however modifications traded benefits among late prefetches early prefetches contention without improving combination factors enough improve overall per formance also tried varying prefetch distance access according expected latency access versus common distance accesses prefetching l2 cache modifications achieved purpose provide significant performance benefit applications 12 v discussion results show sharedmemory systems limited effectiveness exploiting ilp processors due limited benefits ilp techniques memory sys tem analysis section iii implies key reasons limited benefits lack opportunity overlapping load misses andor increased contention system compiler optimizations akin loop interchanges used generate luopt fftopt may able expose potential load miss overlap applica tion simple loop interchange used luopt provides 13 reduction execution time compared lu ilp multiprocessor hardware enhancements also increase load miss overlap eg larger instruction window targeting contention requires increased hardware resources latency reduction techniques results section iv show software prefetching improves memory system performance ilp processors change memorybound nature systems applications latencies long hide prefetching andor increased contention results motivate prefetching algorithms sensitive increases resource usage also motivate latencyreducing rather tolerating techniques producer initiated communication improve effectiveness prefetching 1 vi conclusions paper evaluates impact ilp techniques supported stateoftheart processors performance sharedmemory multiprocessors applications see performance improvements current ilp techniques however ilp techniques effectively address cpu component execution time less successful improving data memory stall time applications see full benefit latencytolerating features ilp processors insufficient opportunities overlap multiple load misses increased contention system resources frequent memory accesses thus ilpbased multiprocessors see larger bottleneck memory system performance generally poorer parallel efficiencies previousgeneration multiprocessors softwarecontrolled nonbinding prefetching latency hiding technique widely recommended previousgeneration sharedmemory multiprocessors find software prefetching results substantial reductions execution time cases ilp system increased late prefetches increased contention resources cause software prefetching less effective reducing memory stall time ilpbased systems even addition software prefetching applications remain largely memory bound thus despite latencytolerating techniques integrated within ilp processors multiprocessors built ilp processors greater need additional techniques hide reduce memory latency previousgeneration multiprocessors one ilpspecific technique discussed paper software clustering load misses additionally latencyreducing techniques producerinitiated communication improve effectiveness prefetching appear promising r evaluation finegrain producer initiated communication cachecoherent multiprocessors adaptive integrated data cache prefetching sharedmemory multiprocessors sgi origin tolerating latency softwarecontrolled data prefetching evaluation design alternatives multiprocessor microprocessor case singlechip multiprocessor evaluation memory consistency models sharedmemory systems ilp processors rsim reference manual impact instruction level parallelism multiprocessor performance simulation methodology interaction software prefetching ilp processors sharedmemory systems splash stanford parallel applications sharedmemory splash2 programs characterization methodological considerations tr ctr vijay pai sarita adve code transformations improve memory parallelism proceedings 32nd annual acmieee international symposium microarchitecture p147155 november 1618 1999 haifa israel manuel e acacio jos gonzlez jos garca jos duato owner prediction accelerating cachetocache transfer misses ccnuma architecture proceedings 2002 acmieee conference supercomputing p112 november 16 2002 baltimore maryland christopher j hughes praful kaul sarita v adve rohit jain chanik park jayanth srinivasan variability execution multimedia applications implications architecture acm sigarch computer architecture news v29 n2 p254265 may 2001 christopher j hughes sarita v adve memoryside prefetching linked data structures processorinmemory systems journal parallel distributed computing v65 n4 p448463 april 2005 xianhe sun surendra byna yong chen serverbased data push architecture multiprocessor environments journal computer science technology v22 n5 p641652 september 2007