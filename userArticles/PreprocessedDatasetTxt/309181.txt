variable metric proximal point algorithm monotone operators proximal point algorithm ppa method solving inclusions form 0in tz monotone operator hilbert space algorithm one powerful versatile solution techniques solving variational inequalities convex programs convexconcave minimax problems possesses robust convergence theory general problem classes basis wide variety decomposition methods called splitting methods yet classical ppa typically exhibits slow convergence many applications reason acceleration methods ppa algorithm great practical importance paper propose variable metric implementation proximal point algorithm essence method newtonlike scheme applied moreauyosida resolvent operator article establish global linear convergence proposed method addition characterize superlinear convergence method companion work establish superlinear convergence method implemented broyden updating nonsymmetric case bfgs updating symmetric case b introduction proximal point algorithm ppa one powerful versatile solution techniques problems convex programming minimax convexconcave program ming possesses robust convergence theory general problem classes finite infinitedimensions eg see 11 16 21 22 23 28 32 41 40 basis wide variety decomposition methods called splitting methods eg see 4 9 12 43 44 yet classical ppa typically exhibits slow convergence many applications reason acceleration methods ppa great practical importance paper propose variable metric implementation proximal point algorithm approach extends refines results originally appeared 38 spirit several recent articles 3 7 10 18 20 24 25 36 however fundamental difference method presented studied 3 7 10 18 20 24 25 36 difference profound impact methodology applied article previous work topic except 38 applies exclusively monotone operators arise subdifferential finitevalued finitedimensional convex function results article apply general monotone operators hilbert space resulting difference methodology roughly corresponding difference methods function minimization methods solving systems equations advantages disadvantages general approach advantages method applies much broader class problems theory developed hilbert space setting impor tantly many monotone operators cannot represented subdifferential finitevalued finitedimensional convex function general monotone operators possess many rich structural properties associated subdifferential convex function eg subdifferentials convex functions maximal cyclically monotone operators 33 addition case operator subdifferential convex function require usual assumption underlying function finitevalued disadvantages general approach arise fact method cannot make use additional structure present operator subdifferential convex function complicates structure method analysis particular note regard complexity global convergence result operator subdifferential convex function solving inclusion equivalent minimizing underlying convex function global convergence method typically driven linesearch routine eg see 3 7 10 18 20 24 25 36 general setting direct recourse strategy complicates structure algorithm convergence theory nonetheless proof technique developed paper refined convex programming setting thereby significantly simplifying global local convergence results 5 6 notwithstanding differences methodology approach still nicely motivated recalling behavior ppa context convex programming min h hilbert space f h 7 ir f1g lower semicontinuous convex function identically 1 define moreauyosida regularization f function f h 7 ir given f z min set solutions 1 corresponds precisely set points f attains minimum value function f continuously frechet differentiable 28 proposition 7d ppa applied 1 approximately steepest decent algorithm applied f 11 analogy immediately suggests variable metric approach could applied function f accelerate method idea first studied 38 basis acceleration techniques described 3 7 10 18 20 24 25 36 3 bonnans gilbert lemarechal sagastizabal develop methods along algorithmic pattern originally suggested qian 38 pattern circumvents many difficulties associated variable metric approach applied directly function f key employ matrix secant update based function f instead f local convergence results 3 section 3 require smoothness assumptions partic ular linear convergence established function f differentiable lipschitz continuous derivative superlinear convergence established f twice strictly frechet differentiable unique solution z second derivative positive definite speak quotient qrate convergence 18 20 24 25 authors apply bundle concept nonsmooth convex minimization 17 approximate moreauyosida regularization f derivative variable metric updates particular quasinewton updates applied using approximate values superlinear convergence results papers 18 20 24 either require strong smoothness assumptions function f lipschitz continuity rf regularization parameter diverges 1 20 lemarechal sagastizabal propose clever reversal quasinewton formula uses value gradient f variety points strictly obtained iterates promising idea deserves theoretical numerical study 10 36 authors develop approach based newtons method semi smooth functions developed 30 31 37 34 properly speaking methods neither adaptation ppa algorithm variable metric method nonetheless flavor methodologies present order obtain superlinear convergence smoothness hypotheses required however hypotheses somewhat technical nature specifically required function f semismooth unique solution 1 37 b every element setvalued mapping exists nonsingular unique solution z c sequence hessian approximates fv k g used generate iterates fz k g must lim dist one show semismoothness hypotheses satisfied many cases interest f finitevalued moreover rademachers theorem differentiability lipschitz continuous functions follows setvalued mapping 2 b fz always welldefined compactvalued finite dimensional finitevalued case nonsingularity property closely tied usual hypothesis strong convexity although limiting hypotheses v k bit strong entirely unreasonable absence differentiability 36 chen qi propose nice preconditioning technique wherein exact value gradient shifted moreauyosida regularization computed inexact values gradient f technique similar spirit reversal quasinewton formula found 20 techniques prove useful numerical implementations algorithm presented paper closely related methods proposed chen fukushima 7 mifflin sun qi 25 however several fundamental distinctions foremost methods 7 25 restricted finite dimensional finitevalued convex programming problems within framework authors use bundle strategies approximate f gradient establish global convergence methods aid line search routine chen fukushima establish global linear convergence results along generalization dennismore characterization theorem superlinear convergence 14 one important features chenfukushima algorithm line search based function f rather approximations function f important practise since obtaining sufficiently accurate approximations function f usually quite time consuming linear superlinear convergence results blend bundle techniques theory nonsmooth equations consequently convergence hypotheses reminiscent employed 10 36 particular require semismoothness cdregularity strong approximation property 2 6 methods paper applied chenfukushima algorithm obtain superlinear convergence method bfgs matrix secant updating employed 25 mifflin sun qi obtain first superlinear convergence result variable metric proximal point algorithm using bfgs matrix secant update setting finite dimensional finitevalued convex programming proposed algorithm uses line search based approximations function f requires function f strongly convex rf frechet differentiable unique global solution convex program addition assumed iterates satisfy certain approximation property involving gradient rf section 4 paper discuss hypotheses related also required convergence analysis paper provide general theory variable metric proximal point algorithm applied maximal monotone operators hilbert space important special case convex programming taken subdifferential function f assume f finitevalued differentiable whole space however obtain superlinear convergence require certain smoothness hypotheses unique global solution z smoothness hypotheses differ assumed 3 18 20 24 since imposed operator gamma1 rather regard reminiscent hypotheses employed 25 choice smoothness hypotheses deep significance context convex programming differentiability hypotheses imply secondorder differentiability f whereas differentiability hypotheses related standard strong secondorder sufficiency conditions convex programming 40 proposition 2 thus reduce standard hypotheses used local analysis convergence particular differentiability f gamma1 imply f singlevalued differentiable imply f finitevalued smoothness hypotheses also differ appear 7 10 36 methods rely theory nonsmooth equations require hypotheses semi smoothness nonsingularity elements 2 addition proof theory methods specifically requires underlying convex function finitevalued neighborhood unique solution 1 methods assume function finitevalued ir n limits direct application constrained problems since constrained case solutions typically lie boundary constraint region ie boundary domain essential objective function throughout paper illustrate many ideas results applying case convex programming purpose show results applied also ground familiar surroundings concrete application details application results case convex programming found 5 paper structured follows begin review classic proximal point algorithm x2 vmppa introduced x3 section contains approximation criteria must satisfied iteration two criteria presented first required obtain global convergence second required accelerate local convergence method division global local criteria one recurring themes paper global level method behaves like steepest descent method local level becomes newton like feature common general purpose methods nonlinear programming nonmonotone descent methods dogleg method trustregion methods x4 discuss smoothness hypotheses required local analysis also extend differentiability results appearing 19 35 maximal monotone operators x5 study operators n k associated newtonlike iteration proposed x3 focus section provide conditions operators n k nonexpansive solution inclusion global convergence result paralleling rockafellars 1976 result 41 given x6 x7 study local convergence rates linear convergence established lipschitz continuity assumption gamma1 characterization superlinear convergence vmppa also given characterization modeled landmark characterization superlinear convergence variable metric methods nonlinear programming due dennis 14 6 use characterization result establish superlinear convergence method derivatives approximated using bfgs broyden updating strategies word notation order denote closed unit ball hilbert space h ib ball center radius r denoted rib given set element z 2 h distance z z dist z two hilbert spaces given multifunction also referred mapping operator depending context graph gph subset product space h 1 theta h 2 defined gph zg domain set domt fz g identity mapping denoted inverse operator defined gph tg given lower semicontinuous convex function f f1g conjugate f defined f z fzg monotone operators classic algorithm given real hilbert space h inner product hdelta deltai say multifunction every z z 0 domt w 2 z w said strongly monotone modulus monotone operator said maximal graph properly contained graph monotone operator important example monotone operator subgradient convex function see minty 27 28 concerned solving inclusions form maximal monotone operator case convex programming problem 1 operator subdifferential convex function f inclusion 3 characterizes points z f attains minimum value wide variety problems cast framework eg variational inequalities complementary problems minimax problems existence results inclusion 3 found 41 1962 minty 27 showed operator maximal monotone moreau yosida resolvent singlevalued nonexpansive h result suggests solution inclusion iteratively approximated recursion z modify scheme varying scalar choosing iterates z k1 approximate solution equation proximal point algorithm applies precisely ideas algorithm starting point z 0 generates sequence h approximation rule z principle difficulty applying proximal point algorithm lies executing operators case convex programming iteration 4 reduces iteration z notice executing algorithm exactly ie instead algorithm difficult solving original problem directly hence critical convergence results obtained assumption approximation 22 23 martinet proved convergence exact proximal point algorithm certain cases operator fixed c k j c first theorem convergence general proximal point algorithm proved rockafellar 41 1976 theorem insures global convergence approximating rule also describes global behavior inclusion 0 2 z solution convergence rate proximal point algorithm depends properties operator choice sequence fc k g accuracy approximation 4 first rate convergence results also obtained rockafellar 41 1976 assumption solution set singleton fzg proved sequence bounded away 0 gamma1 w bounded linear function kwk w near 0 rate convergence least linear luque 21 extended rockafellars theorem case gamma1 0 required singleton showed estimate convergence rate tight 3 algorithm approximation criteria algorithm proposed section newtonlike iteration solving resolvent equation context convex programming problem iteration takes form z operator h k used approximate secondorder properties function f f twice differentiable r method one sets however general f known differentiable lipschitz continuous gradient 28 thus finite dimensional case hessian r 2 f x guaranteed exist dense subset rademachers theorem results secondorder properties f found 19 35 42 well known negative gradient gammarf z k unique element w k solving problem min equivalently satisfying inclusion proximal point algorithm general maximal monotone operator formally derived equation 5 replacing z k f c k z k respectively obtain equivalently equality follows fact w k unique motivates us define operator operator provides analog direction steepest descent operator setting algorithm propose solving inclusion 0 2 z succinctly stated follows variable metric proximal point algorithm z k set z choose c k1 1 mentioned previous section critical convergence results obtained assumption k z k approximated use following approximation criteria approximation criteria g used establish global convergence properties criteria l used obtain local rates convergence although criteria used proof convergence impractical perspective implementation stead provide criteria imple mentable obtain criteria recall following result rockafellar 41 proposition 1 41 proposition 3 let k w z k bound dist 0 k proposition 1 yields following alternative approximation criteria w k since result immediate consequence proposition 1 proof omitted proposition 2 consider following acceptance criteria w k dist 0 k w k dist 0 k w k remark note satisfy either g 0 l 0 necessary find element least norm leaving section recall 41 properties operators k essential analysis follow proposition 3 41 proposition 1 operator k expressed z b z z c z z remark important consequence part c operators p k k lipschitz continuous lipschitz constant 1 nonexpansive henceforth use fact 4 differentiability gamma1 k newtons method minimization locates roots gradient one view variable metric proximal point algorithm newtonlike method locating roots operator k perspective motivates approach local convergence analysis analysis require operator gamma1 possesses certain smoothness properties properties turn imply smoothness operators k smoothness hypotheses used convergence analysis much way used convergence analysis newtons method example recall ensure quadratic convergence newtons method one requires derivative solution locally lipschitz nonsingular nonsingularity insures iterates well defined bounded lipschitzian hypothesis guarantees error linearization quadratically bounded see 29 3212 1022 make use similar properties analysis order discuss smoothness gamma1 k recall various notions differentiability multivalued functions literature thorough treatment ideas context monotone operators refer reader 1 19 26 35 42 definition 4 say operator continuous point modulus ff 0 set psi w nonempty 0 say psi differentiable point w consists single element z continuous linear transformation j h ffi 0 write w remarks 1 definitions lipschitz continuity differentiability multifunction taken 41 pages 885 887 also see 2 page 41 note notions lipschitz continuity differentiability correspond usual notions psi singlevalued rockafellar 41 theorem 2 first use lipschitz continuity establish rates convergence proximal point algorithm set psi w restricted singleton fzg differentiability psi implies lipschitz continuity psi moreover one take ff observation verified 41 proposition 4 follows definition monotonicity maximal monotone operator operator rt x positive semidefinite whenever exists give result relates differentiability multivalued function differentiability inverse proof omitted since parallels proof similar result singlevalued functions lemma 5 assume psi h gamma gamma h differentiable z wg j gamma1 bounded also assume psi gamma1 lipschitz continuous w psi fzg differentiable w rpsi two examples follow examine concepts introduced definition 4 operator question subdifferential convex function first example illustrates f gamma1 lipschitz continuous differentiable origin second example f gamma1 differentiable origin f differentiable example 6 let z z 0 z gamma1 lipschitz continuous 0 differentiable 0 example 7 let gammaz z 0 z 53 z 0 z z 23 z 0 32 0 gamma1 differentiable 0 differentiable gamma1 0 superlinear convergence result x7 requires assumption operator gamma1 differentiable origin although severe restriction applicability results turns case convex programming consequence standard secondorder sufficiency conditions constrained mathematical programs related results established rockafellar 40 proposition 2 context important note secondorder sufficiency condition standard hypothesis used mathematical programming literature insure rapid local convergence numerical methods least context constrained convex programming differentiability hypothesis severe assumption one might first suspect contrary bit weaker standard hypothesis employed results sake completeness recall portion rockafellars result theorem 8 consider convex programming problem 1 given otherwise suppose following conditions functions f k 2 times continuously differentiable neighborhood point z 2 ir n ii kuhntucker vector z iii gradients frf linearly independent iv matrix operator f gamma1 continuously differentiable neighborhood origin remark theorem 8 follows applying implicit function theorem kuhntucker conditions parameterized problems minffz gamma hw zig neighborhood relationship f gamma1 comes fact f zig rockafellar establishes result 2 extension k 2 follows trivially implicit function theorem examine differentiability properties mapping k two results direction given first uses equation 8 relate differentiability operators gamma1 k second uses definition k given 6 relate differentiability operators k proposition 9 let w operator gamma1 differentiable bounded operator differentiable z either case proof first assume gamma1 differentiable rt gamma1 bounded differentiability gamma1 clearly implies gamma1 w since lipschitzian implies differentiable z derivative given 10 since rd conclude latter bounded conversely assume differentiable z rdz gamma1 bounded show gamma1 singlevalued lipschitzian w result follow lemma 5 definition 4 rdz since singlevalued rdz surjective invertible may apply standard open mapping result functional analysis eg 8 theorem 155 obtain existence ae 0 hence w 2 bounded 0 hence reducing ae necessary may assume wk aeib second inequality follows since nonexpansive therefore assume okz gamma waeib z 2 ffiib substituting 12 rearranging obtain w aeib z 2 show 13 implies existence ffl 0 ffiib indeed case would exist sequences fw g fz g z images convex hence 11 exists sequence fz g implies contradicts fact w w kz ffl 0 must exist fact combined 13 implies gamma1 lipschitzian w applies yield result defined 9 let dz operator differentiable bounded operator differentiable z rdz gamma1 bounded either case formula proof replace p observe differentiable z rdz gamma1 bounded p differentiable z rp z gamma1 bounded proof follows argument proof proposition 9 replaced replaced w replaced propositions 9 10 say quite different things differentiability k illustrate difference observe example 7 operator differentiable 0 gamma1 differentiable 0 hand take differentiable 0 differentiable also important note even neither gamma1 differentiable may differentiable case know propositions 9 10 differentiable neither gamma1 differentiable rdz rp z singular unbounded inverses discussion issues context finite dimensional convex programming see 35 assumed subdifferential convex function f propositions 9 refined making use relation f convex conjugate f 39 corollary 12a allows us extend 35 theorem 1 35 theorem 2 hilbert space setting also see 19 theorem 31 however caution terminology required since f necessarily twice differentiable classical sense points f differentiable sense definition 4 indeed f may multivalued arbitrarily close point differentiability best way interpret result alexandrovs theorem 1 states almost every point z interior domain convex function f ir n 7 ir f1g quadratic function q z 19 35 matrix r 2 q z called generalized hessian denoted hfx note existence generalized hessian point z guarantees f strictly differentiable z moreover fx singlevalued neighborhood point z hfz exists r 2 fz exists equals hfz extend terminology hilbert space setting following definition definition 11 let oe h 7 ir f1g function hilbert space h say oe twice differentiable generalized sense point continuous quadratic functional q z z called generalized hessian oe z denoted hoez terminology hand apply propositions 9 10 case convex programming proofs results required since direct translation propositions 9 10 terminology convex programming corollary f1g lower semicontinuous convex let set differentiable z bounded f generalized hessian bounded either case f1g lower semicontinuous convex let set differentiable z r 2 f z gamma1 bounded f twice differentiable generalized sense either case remark observed earlier generalized hessian necessarily positive semidefinite observation used refine statement corollaries 12 13 5 newton operators section study operators associated variable metric proximal point iteration notation emphasizes fact operators produce newtonlike iterates case classical newtons method equation solving 29 x126 one keys convergence analysis show operators contractive respect solution set gamma1 0 clearly operators n k singlevalued moreover fixed points operators n k solutions inclusion 0 2 z since thus conditions ensure operators n k nonexpansive respect important global analysis variable metric proximal point iteration obtain property impose following conditions linear transformations g h k continuous linear transformation continuous inverse h2 nonempty closed bounded subset gamma gamma1 0 remark set gamma h2 used guarantee boundedness sequence fz k g taking one show every weak cluster point sequence fz k g element gamma1 0 observed iusem 13 gamma1 0 bounded one takes sequence fz k g weak limit z theorem 17 41 theorem 1 hypothesis h1 standard automatically satisfied finite dimensional case hand hypothesis h2 quite technical requires careful examination hypothesis problematic since specifies matrices h k satisfy condition depends unknown values oe k kd k z k k show certain cases possible satisfy h2 without direct knowledge unknown values done two steps first shown lemma 14 gamma1 lipschitz continuous differentiable origin fl k bounded positive constant taken 16 kd k z k k approaches zero lemma 15 shown h2 satisfied related condition terms h k w k satisfied taken together results imply least locally h2 satisfied checking condition based known quantities insight hypothesis h2 gained considering case gamma1 differentiable origin case h k intended approximate gammard k 0 k j therefore one guarantee h2 satisfied choosing c k sufficiently large h k fact used 6 establish superlinear convergence method h k obtained via matrix secant updating techniques purpose hypothesis h2 globalize essentially local algorithm newtons method context convex programming one commonly obtains global convergence properties aid line search routine applied objective function f regularization f however operator setting natural underlying objective function line search applied key difference approach taken paper 3 7 10 18 20 24 36 convex programming setting global convergence vmppa driven line search routine applied objective function f regularization f operator setting hypothesis h2 replaces line search associated hypotheses needed make line search strategy effective finitevaluedness objective function f boundedness sequence fh k g hand known operator subdifferential finitevalued finite dimensional convex function algorithm paper modified include line search routine chen fukushima 7 thereby avoiding need hypothesis h2 6 show three cases fl k bounded away zero lemma 14 suppose gamma1 0 nonempty operator strongly monotone modulus 52 k ii operator gamma1 lipschitz continuous origin modulus ff dist k kd k z k k given definition 4 moreover 52ff k kd k z k k differentiable origin derivative j k kd k z k k k oe proof strongly monotone modulus kz gamma z z z singlevalued continuous let z 3 hence since definition fl k establishes result since c k 1 k ii kd k z k k definition 4 implies hence 15 holds lower bound fl k follows part iii result follows part ii using second remark definition 4 w k k z k one establish inequality hypothesis h2 related condition vectors w k specific technique accomplishing given following lemma lemma let h k continuous linear transformation h z therefore h1 criterion l satisfied sequence ffl defined h2 hypothesis h2 satisfied proof 16 17 hence 17 since inequality fl k 13 implies 6 1 conclude section showing operators n k nonexpansive respect set gamma1 0 proposition nonempty linear transformations fh k g satisfies hypotheses h1 h2 k kh k k z k k 3kd k z k k proof let z 2 gamma definitions p k n k hence hypothesis h2 hence hypothesis h2 thus 20 21 letting z proposition 3 part c yields 22 23 consider ff ff k fl k 18 holds 24 suppose 19 therefore 23 using inequality 2a b 0 zk kd k z k k 25 obtain 18 6 global convergence statement proof global convergence result given parallels development given rockafellar 41 theorem 1 classical proximal point algorithm theorem 17 let fz k g sequence generated variable metric proximal point algorithm criterion g g 0 suppose solution set gamma1 0 nonempty sequence linear transformations fh k g satisfies hypotheses h1 h2 sequence fz k g bounded weak cluster point sequence element 0 also assumed gamma1 0 bounded h2 z converges weakly z order establish result require following technical lemma whose proof straightforward omitted lemma suppose nonnegative sequences fffl k g satisfies nonnegative sequence satisfying u cauchy sequence proof theorem 17 begin showing limit lim k kz exists every z 2 gamma end let observe definition n k proposition imply therefore lemma implies sequence fkz zkg cauchy z exists every z 2 gamma immediate consequence existence limits boundedness sequences fz k g oe k show sequence fd k z k g converges strongly origin indeed case subsequence j ae turn implies inf j due boundedness sequence foe k g let z 2 gamma proposition 16 z n k z k fc k g bounded final inequality follows criterion g hence whereby obtain contradiction therefore lim k kd k z k next let j ae subsequence fz k g j converges weakly z 1 ie z 1 weak cluster point sequence fz k g show z 1 must element gamma1 0 proposition 3 gamma 1 equivalently hz gamma z k z w w 2 z taking limit j yields inequality z w w 2 z since maximal monotone get assumption 0 argument showing one weak cluster point fz k g identical one given rockafellar 41 theorem 1 remark ensure strong convergence sequence fz k g one requires growth condition inverse mapping gamma1 neighborhood origin rockafellar shown lipschitz continuity gamma1 origin suffices purpose 41 theorem 2 conditions found work luque 21 proposition 12 results rockafellar luque easily extended variable metric proximal point algorithm 7 convergence rates 71 linear convergence rockafellar 41 theorem 2 require operator gamma1 lipschitz continuous origin order establish convergence rate least linear theorem 19 let fz k g sequence generated variable metric proximal point algorithm satisfying criterion g l k assume gamma1 lipschitz continuous origin modulus ff solution set gamma1 0 singleton fzg sequence fh k g satisfies hypotheses h1 h2 sequence fz k g strongly converges solution index k oe k satisfies limsup k1 oe k 1 convergence rate linear proof theorem 17 kd k z k k 0 hence part ii lemma 14 implies converges strongly z establish linear rate definition 4 let k k 1 proposition 3 lipschitz continuity gamma1 0 hence relation 14 hypothesis h2 yield zk let k ff using 26 27 let proposition 16 lemma 14 k k 28 29 k k 2 30 31 criterion l l 0 proposition 3 c zk since k sufficiently large moreover limsup k1 oe limsup ffi 72 superlinear convergence give analog dennis 14 characterization theorem superlinear convergence variable metric methods nonlinear programming applies vmppa result used 6 establish superlinear convergence variable metric proximal point algorithm broyden nonsymmetric case bfgs symmetric case updating formulas used generate matrices h k theorem 20 let fz k g sequence generated variable metric proximal point algorithm satisfying criterion l k suppose operator gamma1 differentiable origin converges solution z superlinearly remark proposition 9 consequently condition 32 recast familiar form given 15 theorem 824 note assumption 32 sequence fh k g much weaker assuming sequence converges specific choices linear transformations h k satisfying 32 discussed 6 proof theorem 20 requires following lemma lemma 21 conditions theorem 20 zkib k sufficiently large proof part let ffi 0 z ae okwkib 33 whenever k k 1 kd k z k k ffi 33 proposition 3 c k prove b note n k z k hence criterion l therefore 34 proposition 3 c proof theorem 20 let z k1 n k z k z hence z z equivalently z z z z k1 lemma 21 first third three terms appearing right hand side inclusion bounded expression form okz zkib 32 holds therefore positive sequences fff 1k g fff 2k g converging zero k zk ff 1k 1for k k 2 denoting ff 1k ff 2k k zk converges z superlinearly conversely suppose lim zk zk divide 35 kz 36 lemma 21 obtain however 36 zk zk zk k 1 hence 32 holds concluding remarks paper introduced new proximal point algorithm solving inclusion arbitrary maximal monotone operator global convergence algorithm demonstrated inexact solution step important practice since solving exact solution step impractical may fact almost difficult solving original problem assumed gamma1 lipschitz continuous origin method shown linearly convergent assume gamma1 differentiable origin classical characterization superlinear convergence due dennis also holds vmppa 6 characterization superlinear convergence applied establish superlinear convergence method certain matrix secant updating strategies employed generate matrices h k 5 give implementation details case convex programming show apply method solve associated primal dual lagrangian saddle point problems particular shown bundle technique 17 applied satisfy approximation criteria l g primal saddle point solution techniques preliminary numerical results comparing three approaches also presented acknowledgments authors would like thank reviewers thorough work comments suggestions greatly contributed exposition particular would like thank professor alfredo iusem observing error earlier version theorem 17 suggested revision result set gamma1 0 assumed bounded r existence almost everywhere second differential convex function associated properties convex surfaces family variable metric proximal point methods method successive projection finding common point convex sets application variable metric proximal point algorithm convex programming superlinear convergence variable metric proximal point algorithm using broyden bfgs matrix secant updating proximal quasinewton methods nondifferentiable convex optimization nonlinear functional analysis splitting methods monotone operators application tp parallel optimization globally superlinearly convergent algorithm nonsmooth convex minimization new proximal point algorithms convex minimization decomposition method application convex programming personal communication characterization superlinear convergence application quasinewton methods numerical methods unconstrained optimization nonlinear equations proximal points algorithm reflexive banach spaces bundle methods nonsmooth optimization approach variable metric bundle meth ods practical aspects moreauyosida regularization theoretical preliminaries variable metric bundle methods conceptual implementable forms asymptotic convergence analysis proximal point algorithm regularisation dinequations variationelles par approximations successive determination approachee dun point fixe dune application pseudo quasisecondorder proximal bundle algorithm control dan les inequations variationelles elliptiques proximite et dualite dans un espace hilbertien iterative solution nonlinear equations several variables nonsmooth equations motivation algorithms globally convergent newton method sc 1 problems weak convergence theorems nonexpansive mappings banach spaces convex functions convergence analysis algorithms solving nonsmooth equations preconditioning proximal newton method nondifferentiable convex optimization nonsmooth version newtons method variable metric proximal point algorithm theory application conjugate duality optimization augmented lagrangians applications proximal point algorithm convex programming monotone operators proximal point algorithm maximal monotone relations second derivatives nonsmooth functions partial inverse monotone operator applications methods partial inverses convex programming decomposition tr