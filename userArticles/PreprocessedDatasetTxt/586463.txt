parallelizing data cube paper presents general methodology efficient parallelization existing data cube construction algorithms describe two different partitioning strategies one topdown one bottomup cube algorithms partitioning strategies assign subcubes individual processors way loads assigned processors balanced methods reduce inter processor communication overhead partitioning load advance instead computing individual groupby parallel partitioning strategies create small number coarse tasks allows sharing prefixes sort orders different groupby computations methods enable code reuse permitting use existing sequential external memory data cube algorithms subcube computations processor supports transfer optimized sequential data cube code parallel settingthe bottomup partitioning strategy balances number single attribute external memory sorts made processor topdown strategy partitions weighted tree weights reflect algorithm specific cost measures like estimated groupby sizes partitioning approaches implemented shared disk type parallel machine composed p processors connected via interconnection fabric access shared parallel disk arraywe implemented parallel topdown data cube construction method c mpi message passing library communication leda library required graph algorithms tested code eight processor cluster using variety different data sets range sizes dimensions density skew comparison tests performed sunfire 6800 tests show partitioning strategies generate close optimal load balance processors actual run times observed show optimal speedup p b figure 1 4dimensional lattice aggregated distinct combinations ab groupby child parent groupby child computed parent aggregating attributes parentchild relationships allow algorithms share partitions sorts partial sorts different groupbuys example data sorted respect ab cuboid groupby generated ab without sorting generating abc requires sorting blocks entries cube algorithms differ make use commonalities bottomup approaches reuse previously computed sort orders generate detailed groupbuys less detailed ones less detailed groupby contains subset attributes topdown approaches use detailed groupbys compute less detailed ones bottomup approaches better suited sparse rela tions relation r sparse n much smaller number possible values given ddimensional space present different partitioning load balancing approaches depending whether topdown bottomup sequential cube algorithm used conclude section brief discussion underlying parallel model standard shared disk parallel machine model assume p processors connected via interconnection fabric processors typical workstation size local memories concurrent access shared disk array purpose parallel algorithm design use coarse grained multicomputer cgm model 5 8 15 18 27 precisely use emcgm model 6 7 9 multiprocessor version vitters parallel model 2830 parallel data cube construction methods assume ddimensional input data set r size n stored shared disk array output ie groupbys comprising data cube written shared disk array subsequent applications may impose requirements output example visualization application may require storing groupby striped format entire disk array support fast access individual groupbys 3 parallel bottomup data cube construction bottomup data cube construction methods calculate groupbys order emphasizes reuse previously computed sorts generate detailed groupbuys less detailed ones bottomup methods well suited sparse relations support selective computation blocks groupby eg generate blocks specify userdened aggregate condition 4 previous bottomup methods include buc 4 partitioncube part 24 main idea underlying bottomup methods captured follows data previously sorted attribute creating ab sort order require complete resorting local resorting ablocks blocks consecutive elements attribute used instead sorting ablocks often performed local memory hence instead another external memory sort ab order created one single scan disk bottomup methods 4 24 attempt break problem sequence single attribute sorts share prexes attributes performed local memory single disk scan outlined 4 24 total computation time methods dominated number single attribute sorts section describe partitioning groupby computations p independent subproblems partitioning generates subproblems processed efciently bottomup sequential cube methods goal partitioning balance number single attribute sorts required subproblem ensure subproblem overlapping sort sequences way sequential methods thereby avoiding additional work let a1ad attributes relation r assume a1a2 ad ai number different possible values attribute ai observed 24 set groupsbys data cube partitioned contain a1 contain a1 partitioning approach groupsbys containing a1 sorted a1 indicate saying contain a1 prex groupbys containing a1 ie a1 projected contain a1 postx recurse scheme remaining attributes shall utilize property partition computation groupbys independent subproblems load subproblems balanced overlapping sort sequences way sequential methods following give details partitioning method let x z sequences attributes representing sort orders let arbitrary single attribute introduce following denition sets attribute sequences representing sort orders respective groupbys entire data cube construction corresponds set sd a1 ad sort orders respective groupbys dimension data cube refer rank si set sd a1 ad union two subsets turn figure 2 partitioning 4dimensional data cube attributes b c 8 s1sets correspond groupbuys determined four attributes union four subsets rank 2 sd2a1 a2 a3 ad sd2a1 a3 ad a2 sd2a2 a3 ad a1 sd2 a3 ad a2 a1 complete example 4dimensional data cube attributes b c shown gure 2 sake simplifying discussion assume p power 2 consider 2pssets rank 2p sets order dened eq 2 dene partitioning assigns set summarized algorithm 1 algorithm 1 parallel bottomup cube construction processor pi 1 p performs following steps independently parallel 1 determine two sets forming described 2 compute groupbys using sequential externalmemory bottomup cube construction method end algorithm illustrate partitioning using example 8 values generate 16 ssets rank 6 giving indices attributes a1 a2 a3 figure 3 sets assigned 8 processors represents projectedout attribute represents existing attribute a4wehave processor assigned computation 27 groupbys shown gure 3 every processor access copy relation r processor performs attribute sorts generate data ordering needed groupbys one copy r readconicts avoided sorting sequences using binomial heap broadcast pattern 19 results every processor pi receiving two sorted sequences forming time needed single attribute sorts figure 4 shows sequence sorts 8processor example index inside circles indicates processor assignment ie processor 1 performs total four single attribute sorts original relation r starting sort attribute a1 using binomial heap properties follows processor k 1 single attribute sorts 2p sorted sequences available time needed figure 4 binomial heap structure generating 2p gammasets without read conicts algorithm 1 easily generalized values p powers 2 also note algorithm 1 requires p 2d1 usually case practice however parallel algorithm needed larger values p partitioning strategy needs augmented augmentation could example partitioning strategy based number data items particular attribute would applied partitioning based number attributes done since range p 20 2d1 covers current needs respect machine dimension sizes discuss augmentations paper following four properties summarize main features algorithm 1 make load balanced communication efcient computation groupby assigned unique processor calculation groupbys assigned processor pi requires number single attribute sorts 1 sorts performed processor pi share prexes attributes way 4 24 performed disk scans manner 4 24 algorithm requires interprocessor communication 4 parallel topdown data cube construction topdown approaches computing data cube like sequential pipesort pipe hash overlap methods 1 10 25 use detailed groupbys compute less detailed ones contain subset attributes former apply data sets number data items groupby shrink considerably number attributes decreases data reduction pipesort pipehash overlap methods select spanning tree lattice rooted groupby containing attributes pipesort considers two cases parentchild relationships ordered attributes child prex ordered attributes parent eg abcd abc simple scan sufcient create child parent otherwise sort required create child pipesort seeks minimize total computation cost computing minimum cost matchings successive layers lattice pipehash uses hash tables instead sorting overlap attempts reduce sort time utilizing fact overlapping sort orders always require complete new sort example abc groupby partitions sorted independently c produce ac sort order may permit independent sorts memory rather always using external memory sort next outline partitioning approach generates p independent subproblems solved one processor using existing externalmemory topdown cube algorithm rst step algorithm determines spanning tree lattice using one existing approaches like pipesort pipehash overlap respectively balance load different processors next perform storage estimation determine approximate sizes groupbys done example using methods described 11 26 work weighted tree crucial part solution partitioning tree partitioning subtrees induces partitioning data cube problem p subproblems subsets groupbys determining optimal partitioning weighted tree easily shown npcomplete problem making example reduction processor scheduling since weights tree represent estimates heuristic approach generates p subproblems control sizes subproblems holds promise want sizes p subproblems balanced also want minimize number subtrees assigned processor every subtree may require scanning entire data set r thus many subtrees result poor io performance solution develop balances two considerations heuristics makes use related partitioning problem trees efcient algorithms exist minmax tree kpartitioning problem 3 dened follows given tree n vertices positive weight assigned vertex delete k edges tree largest total weight resulting subtree minimized minmax tree kpartitioning problem studied 3 12 23 methods assume weights xed note partitioning problem different cut subtree additional cost introduced groupby associated root must computed scratch separate sort hence cutting weight root increased accordingly adapted algorithm 3 account changes weights required algorithm based pebble shifting scheme k pebbles shifted tree root towards leaves determining cuts made adapted version cuts made cost parent new partition adjusted reect cost additional sort original cost saved hash table possible future use since cuts moved many times reaching nal position remainder shall refer method modied minmax tree kpartitioning however even perfect minmax kpartitioning necessarily result partitioning subtrees equal size address tradeoffs arising number subtrees assigned processor use treepartitioning initial step partitioning achieve better distribution load apply partitioning strategy instead partitioning tree p subtrees partition p subtrees integer 1 use packing heuristic determine subtrees belong processors assigning subtrees every processor packing heuristic considers weights subtrees pairs subtrees weights control number subtrees consists matching phases p largest subtrees groups subtrees p smallest subtrees groups subtrees matched details described step 2b algorithm 2 algorithm 2 sequential treepartitiont p inputaspanningtree ofthelatticewithpositiveweightsassignedtothenodesrepresent ing cost build node ancestor integer parameters oversampling ratio p number processors output partitioning p subsets 1p subtrees 1 compute modied minmax tree ppartitioning p subtrees t1 tsp 190 dehne et al 2 distributesubtrees t1tsp amongthe p subsets1ps subtreespersubset follows 2a create p sets trees named 1 sp initially ti weight dened total weight trees 2b sort sets weight increasing order wlog let 1 spj1p resulting sequence end algorithm tree partition algorithm embedded parallel topdown data cube construction algorithm method provides framework parallelizing sequential topdown data cube algorithm outline approach given following algorithm 3 algorithm 3 parallel topdown cube construction processor pi 1i p performs following steps independently parallel 1 apply storage estimation method 11 26 determine approximate sizes groupbys 2 select sequential topdown cube construction method eg pipesort pipe hashoroverlap compute spanning tree lattice used method compute weight node estimated cost build node ancestor 3 execute algorithm treepartitiont p shown creating p sets 1p set contains subtrees 4 compute groupbys subset using sequential topdown cube construction method chosen step 1 end algorithm performance results described section 6 show partitioning 3 achieves good results respect balancing loads assigned processors important result since small value crucial optimizing performance 5 parallel arraybased data cube construction method section 4 easily modied obtain efcient parallelization arraycube method presented 32 arraycube method aimed dense data cubes structures raw data set ddimensional array stored disk sequence chunks chunking way divide ddimensional array small size ddimensional chunks chunk portion containing data set ts disk block xed sequence chunks stored disk calculation groupby requires certain amount buffer space 32 arraycube method calculates minimum memory spanning tree groupbys mmst spanning tree lattice total amount buffer space required minimized total number disk scans required computation groupbys total amount buffer space required divided memory space available arraycube method therefore parallelized simply applying algorithm 3 mmst 6 experimental performance analysis implemented tested parallel topdown data cube construction method presented section 4 implemented sequential pipesort 1 c parallel topdown data cube construction method section 4 c mpi 2 required graph algorithms well data structures like hash tables graph representa tions drawn leda library 21 still implementation took one person year full time work chose implement parallel topdown data cube construction method rather parallel bottomup data cube construction method former tunable parameters wish explore primary parallel hardware platform use pc cluster consisting frontend machine eight processors frontend machine used partition lattice distribute work among 8 processors frontend machine ibm netnity server two 9 gb scsi disks 512 mb ram 550mhz pentium processor processors 166 mhz pentiums 2g ide hard drives 32 mb ram except one processor 133 mhz pentium processors run linux connected via 100 mbit fast ethernet switch full wire speed ports clearly low end older hardware platform experiments reported remainder section represent several weeks 24 hrday testing pc cluster platform described advantage available exclusively experiments without user disturbing measurements main goal studying speedup obtained parallel method rather absolute times platform proved sufcient verify results also hold newer machines faster processors memory per processor higher bandwidth ported code sunfire 6800 performed comparison tests data sets sunfire 6800 used recent sun multiprocessor sun ultrasparc iii 750 mhz processors running solaris 8 24 gb ram sun t3 shared disk figure 5 shows pc cluster running time observed function number processors used data set measured sequential time sequential pipesort 1 parallel time obtained parallel topdown data cube construction method section 4 using oversampling ratio 2 data set consisted 1000000 records dimension 7 test data values uniformly distributed 10 values dimension figure 5 shows running times algorithm increase number processors three curves shown runtime curve shows time taken slowest processor ie processor received largest workload second curve shows average time taken processors time taken frontend machine partition lattice distribute work among compute nodes figure 5 pc cluster running time seconds function number processors fixed parameters data 7 experiments per data point 5 insignicant theoretical optimum curve shown gure 5 sequential pipesort time divided number processors used observe runtime obtained code theoretical optimum essentially identical oversampling ratio 2 optimal speedup p observed anomaly runtime curve due slower 133 mhz pentium processor interestingly average time curve always theoretical optimum curve even runtime curve sometimes theoretical optimum curve one would expected runtime curve would always theoretical optimum curve believe superlinear speedup caused another effect benets parallel method improved io sequential pipesort applied 10 dimensional data set lattice partitioned pipes length 10 order process pipe length 10 pipesort needs write 10 open les time appears linux number open les considerable impact performance 100000 records writing 4 les took 8 seconds system writing 6 les took 23 seconds 12 writing 8 les took 48 seconds 16 benets parallel method since partition lattice rst apply pipesort part therefore pipes generated parallel method considerably shorter order verify results also hold newer machines faster processors memory per processor higher bandwidth ported code sunfire 6800 performed comparison tests data sets figure 6 shows running times observed sunfire 6800 absolute running times observed considerably faster expected sunfire approximately 4 times faster pc cluster figure 6 sunfire 6800 running time seconds function number processors data set gure 5 importantly shapes curves essentially pc cluster runtime slowest proc average time curves similar close theoretical optimum curve oversampling ratio 2 optimal speedup p also observed sunfire 6800 larger sunfire installation also allowed us test code larger number processors shown gure 6 still obtain optimal speedup p using 16 processors dataset figure 7 shows pc cluster running times topdown data cube parallelization increase data size 100000 1000000 rows main observation parallel runtime increases slightly linear respect data size consistent fact sorting requires time log n figure 7 shows parallel topdown data cube construction method scales gracefully respect data size figure 8 shows pc cluster running time function oversampling ratio observe test case parallel runtime ie time taken slowest processor best 3 due following tradeoff clearly workload balance improves increases however total number subtrees p generated tree partitioning algorithm increases need perform sorts root nodes subtrees optimal tradeoff point test case 3 important note oversampling ratio tunable parameter best value depends number factors experiments show 3issufcient load balancing however data set grows size time sorts root nodes subtrees increases linear whereas effect imbalance linear substantially larger data sets eg 1g rows expect optimal value 2 figure 9 shows pc cluster running time topdown data cube parallelization increase dimension data set 2 10 note number groupbys must computed grows exponentially respect dimension data set gure 9 observe parallel running time grows essentially linear respect 194 dehne et al figure 7 pc cluster running time seconds function data size fixed parameters number 7 experiments per data point 5 figure 8 pc cluster running time seconds function oversampling ratio fixed parameters data rows number processors 8 dimensions 7 experiments per data point 5 figure 9 pc cluster running time seconds function number dimensions fixed parameters data 200000 rows number processors 8 experiments per data point 5 note work grows exponentially respect number dimensions output size also tried code high dimensional data size output becomes extremely large example executed parallel algorithm 15 dimensional data set 10000 rows resulting data cube size 1g figure shows pc cluster running time topdown data cube parallelization increase cardinality dimension number different possible data figure 10 pc cluster running time seconds function cardinality ie number different possible data values dimension fixed parameters data size 200000 rows number processors 8 dimensions 196 dehne et al figure 11 pc cluster running time seconds function skew data values dimension based zipf fixed parameters data size 200000 rows number processors 8 dimensions 7 experiments per data point 5 values dimension recall topdown pipesort 1 aimed dense data cubes experiments performed 3 cardinality levels 5 10 100 possible values per dimension results shown gure 6 conrm expectation method performs better denser data figure 11 shows pc cluster running time topdown data cube parallelization data sets skewed distribution used standard zipf distribution dimension data reduction topdown pipesort 1 increases skew total time observed expected decrease skew exactly observe gure 11 main concern regarding parallelization method balanced partitioning tree would presence skew main observation gure 11 relative difference runtime slowest processor average time increase increase skew appears indicate partitioning method robust presence skew 7 comparison previous results section summarize previous results parallel data cube computation compare results presented paper 13 14 authors observe groupby computation essentially parallel prex implementation method mentioned experimental performance evaluation presented method creates large communication overhead likely show unsatisfactory speedup methods 20 22 well methods presented paper reduce communication overhead partitioning load assigning sets groupby computations individual processors discussed 20 22 balancing load assigned different processors hard problem approach 20 uses simple greedy heuristic parallelize hashbased data cube computation observed 20 simple method scalable load balance speedup satisfactory 4 processors subsequent paper group 31 focuses overlap multiple data cube computations sequential setting approach 22 considers parallelization sortbased data cube construction studies parallel bottomup icebergcube computation four different methods presented rp rpp asl pt experimental results presented indicate asl pt better performance among four main reason rp rpp show weak load balancing pt somewhat similar parallel bottomup data cube construction method presented section 3 since pt also partitions bottomup tree however pt partitions bottomup tree simply subtrees equal numbers nodes requires considerably tasks processors obtain good load balance observed 22 larger number tasks required performance problems arise approach reduces possibility sharing prexes sort orders different groupby computations contrast parallel bottomup method section 3 assigns two tasks processor tasks coarse grained greatly improves sharing prexes sort orders different groupby computations therefore expect method decrease performance larger number processors observed 22 asl method uses parallel topdown approach using skiplist maintain cells groupby asl parallelized making construction groupby separate task hoping large number tasks create good overall load balancing uses simple greedy approach assigning tasks processors similar 20 observed 22 large number tasks brings performance problems reduces possibility sharing prexes sort orders different groupby computations contrast parallel topdown method section 4 creates coarse tasks precisely algorithm assigns tasks subtrees processor oversampling ratio shown section 6 oversampling ratio 3issufcient obtain good load balancing sense method answers open question 22 obtain good load balancing without creating many tasks also clearly reected experimental performance methods comparison experiments reported 22 observed 22 experiments gure 10 22 indicate asl obtains essentially zero speedup number processors increased 8 16 contrast experiments gure 6 section show parallel topdown method section 4 still doubles speed number processors increased 8 16 obtains optimal speedup p using processors 8 conclusion future work presented two different partitioning based data cube parallelizations standard shared disk type parallel machines partitioning strategies bottomup topdown data 198 dehne et al cube parallelization balance loads assigned individual processors loads measured dened original proponents respective sequential methods subcube computations carried using existing sequential data cube algorithms topdown partitioning strategy also easily extended parallelize arraycube method experimental results indicate partitioning methods efcient practice compared existing parallel data cube methods parallelization approach brings signicant reduction interprocessor communication important practical benet enabling reuse existing sequential data cube code possible extension data cube parallelization methods consider shared nothing parallel machine model possible store duplicate input data set r oneachprocessorsdiskthenourmethodcanbeeasilyadaptedforsuchanarchitecturethis clearly always possible solve cases total output size considerably larger input data set example sparse data cube computations data cube several hundred times large r sufcient total disk space necessary store output one single copy distributed different disks p times duplication r may smaller output data cube parallelization method would partition problem way described sections 3 4 subcube computations would assigned processors way well computing subcube processor would read r local disk output two alternatives processor could simply write subcubes generated local disk could however create bottleneck example visualization application following data cube construction needs read single groupby case groupby distributed disks example striped format obtain data distribution processors would write subcubes directly local disks buffer output whenever buffers full would permuted network summary observe approach aimed shared disk parallel machines applicability shared nothing parallel machines depends mainly distribution availability input data set r interesting open problem identify ideal distribution input r among p processors xed amount replication input data allowed ie r copied r times 1 r p another interesting question future work relationship topdown bottomup data cube computation parallel setting two conceptually different methods existing literature suggests bottomup methods better suited high dimensional data far implemented parallel topdown data cube method took one person year full time work chose implement topdown method tunable parameters discovered experimentation possible future project could implement parallel bottomup data cube method similar environment compiler message passing library data structure libraries disk access methods etc measure various tradeoff points two methods indicated 22 critical parameters parallel bottomup data cube computation similar good load balance small number coarse tasks leads us believe parallel bottomup method perform well compared parallel topdown method parallel bottomup method fewer parameters available netuning code therefore tradeoff points parallel setting topdown bottomup methods may different sequential setting relatively little work done difcult problem generating partial data cubes entire data cube given subset groupbys given lattice set selected groupbys generated challenge deciding groupbys computed order minimize total cost computing partial data cube many cases computing intermediate groupbys selected set several views selected set computed cheaply reduce overall computation time sarawagi et al 25 suggest approach based augmenting lattice additional vertices represent possible orderings views attributes additional edges represent relationships views minimum steiner tree approximation algorithm run identify number inter mediate nodes socalled steiner points added selected subset best reduce overall cost approximation algorithm used optimal minimum steiner tree problem npcomplete intermediate nodes introduced method areofcoursetobedrawnfromthenonselectednodesintheoriginallatticebyaddingthese additional nodes cost computing selected nodes reduced although theoretically neat approach effective practice problem augmented lattice far many vertices edges processed efciently example 6 dimensional partial data cube number vertices edges augmented lattice increase factors 30 8684 respectively 8 dimensional partial data cube number vertices edges increase factors 428 701346 respectively augmented lattice 9 dimensional partial data cube 2000000000 edges another approach clearly necessary authors currently implementing new algorithms generating partial data cubes consider important area future research acknowledgments theauthorswouldliketothankstevenblimkiezimminchenkhoimanhnguyenthomas pehle suganthan sivagnanasundaram contributions towards implementation described section 6 rst second fourth authors research partially supported natural sciences engineering research council canada third authors research partially supported national science foundation grant 9988339ccr r introduction parallel computing max planck institute tr probabilistic counting algorithms data base applications optimal algorithms tree partitioning introduction parallel computing scalable parallel geometric algorithms coarse grained multicomputers implementing data cubes efficiently towards efficiency portability arraybased algorithm simultaneous multidimensional aggregates efficient external memory algorithms simulating coarsegrained parallel algorithms external memory algorithms bottomup computation sparse iceberg cube parallel virtual memory shifting algorithm minmax tree partitioning icebergcube computation pc clusters data cube high performance olap data mining parallel computers reducing io complexity simulating coarse grained parallel algorithms fast computation sparse datacubes storage estimation multidimensional aggregates presence hierarchies computation multidimensional aggregates multicube computation bsplike externalmemory computation bulk synchronous parallel computinga paradigm transportable software parallel scalable infrastructure olap data mining ctr ying chen frank dehne todd eavis andrew rauchaplin parallel rolap data cube construction sharednothing multiprocessors distributed parallel databases v15 n3 p219236 may 2004 frank dehne todd eavis andrew rauchaplin cgmcube project optimizing parallel data cube generation rolap distributed parallel databases v19 n1 p2962 january 2006 ge yang ruoming jin gagan agrawal implementing data cube construction using cluster middleware algorithms implementation experience performance evaluation future generation computer systems v19 n4 p533550 may ying chen frank dehne todd eavis andrew rauchaplin pnp sequential external memory parallel iceberg cube computation distributed parallel databases v23 n2 p99126 april 2008