towards modeling performance fast connected components algorithm parallel machines present analyze portable highperformance algorithm finding connected components modern distributed memory multiprocessors algorithm hybrid classic dfs subgraph local processor variant shiloachvishkin pram algorithm global collection subgraphs implement algorithm splitc measure performance cray t3d meiko cs2 thinking machines cm5 using class graphs derived cluster dynamics methods computational physics 256 processor cray t3d implementation outperforms previous solutions order magnitude characterization graph parameters allows us select graphs highlight key performance features study effects parameters machine characteristics balance time local global phases algorithm find edge density surfacetovolume ratio relative communication cost dominate performance understanding effect machine characteristics performance study sheds light impact improvements computational andor communication performance challenging problem b introduction problem finding connected components graph broad importance computer science computational science computer vision example makes extensive use connected components algorithms problems edge detection object recognition use connected components algorithms also advanced study various physical properties magnetic materials near critical temperatures among physical phenomena connected components also appears offer unique challenge parallel computing sequential solutions well understood commonly used introductory computer science theory courses application depthfirst breadthfirst search parallel solutions received great deal attention theorists practical computer scientists proven difficult theoretical work shows good results crcw pram model3 10 11 24 assumes uniform memory access time arbitrary bandwidth memory location material based upon work supported national science foundation presidential faculty fellowship award graduate research fellowship infrastructure grant number cda8722788 well lawrence livermore national laboratories inst scientific research grants ucberl9269 ucberl92172 opinions findings conclusions recommendations expressed publication authors necessarily reflect views either organization inherent contention algorithm made even erew solutions much challenging 5 14 15 17 practical application theoretical work parallel machines largely restricted small sharedmemory machines simd machines slow processors 11 many practical solutions developed modern mimd massively parallel platforms mpps 6 9 13 22 18 vector machines 8 22 practical solutions typically emphasize performance issues portability scalability generality still rarely obtain good performance paper present portable highperformance algorithm finding connected components general graph distributed memory machines 256node t3d algorithm demonstrates fastest solution date graphs practical importance algorithm hybrid classic dfs subgraph local processor variant shiloachvishkin pram algorithm collection subgraphs analyze performance characteristics algorithm across range graph types modern parallel machines study sheds light performance impact improvements computational andor communication performance challenging problem tradeoffs particularly important quantify given microprocessor performance improving rapidly time greater computational performance easily obtained expense communication performance building less tightly integrated systems1 study use class graphs motivated cluster dynamics problems computational physics traditionally numerical simulations materials near critical temperatures utilize monte carlo algorithms local methods information propagation quality statistics returned simulations depends heavily size sample l although larger sample size leads better statistics number steps needed propagate information grows generally worse using traditional methods 4 16 21 27 28 recent methods swendsenwang algorithm reduce correlation time simulations twodimensional ising model correlation time using sw grows ol 035 25 26 allowing much larger samples studied heart sw connected components algorithm algorithm generates random graph finds connected components graph applies simple transformation component starts 26 years since introduction sw algorithm cluster dynamics hence use connected components also found widespread use areas including computational field theory experimental high energy particle physics statistical mechanics class graphs arising cluster dynamics random graph built underlying lattice structure ie probabilistic mesh instance graph produced using single probability p decide whether edge underlying lattice present graph instance decision edge made independently decision edge figure 1 illustrates possible instances probabilistic meshes 2dp built 2dimensional lattice 3dp built 3dimensional lattice remainder paper structured follows section 2 describes parallel algorithm figure 1 probabilistic meshes used study physical phenomena edge underlying mesh present given graph instance probability p independent edges section 3 provides details mpps used study section 4 explains application scaling rule used study justifies probabilistic analysis identifies key performance issues inherent scaling problem across edge presence probabilities section 5 demonstrates develops model performance characteristics hybrid algorithm across range machines graph types section 6 draws conclusions algorithm section outlines connected components algorithm discusses portable parallel language used implementation obtaining good performance distributed memory machine demands pay attention locality access order minimize communication costs many algorithms results hybrid approach combination local global phases local phases algorithm deals data reside processors local memory global phases algorithm addresses issues efficient remote data access synchronization processors phases employ different algorithms optimized using different cost models performance generally depends balance time spent two phases usually good local phase dominates total time algorithms balance imply extra local work reduce complexity global phase global algorithm grows complexity understanding avoiding load imbalance network contention becomes much difficult followed strategy hybridization create algorithm merging simple fast local algorithm depthfirst search dfs powerful efficient global algorithm shiloach vishkin 24 sv neither local global algorithm sufficient alone might try extend dfs task queue assigning new tree depthfirst forest whenever processor needs work complete ignorance data locality inherent method proves fatal performance sv designed crcw pram mind suffers nearly problemthe pram model fails recognize nonuniform access times distributed memory machines resulting poor performance demonstrate later merging local dfs global sv optimizing result create algorithm performs scales well interesting set graphs compared purely global approaches algorithm performs order magnitude better optimized algorithm follows detail data structures process optimization see 19 1 local phase perform local depthfirst search dfs processors portion graph collapsing local connected component representative node local phase results much smaller graph global phase component global graph marked unique value global phase 2 global initialization finish preparation global graph pointing remote edge one local representative nodes selected local phase 3 global iterations beginning reduced global graph representative nodes remote edges iterate following steps termination check components remain processed processor quit b hooking step merge components larger components attempting link component across shared edge conditions attachment guarantee cycles formed c star formation step collapse newly formed components trees height one stars ensure representative nodes component single consistent value e selfloop removal step component remove edges point nodes within component ie nodes value 4 local cleanup node selected representative node global graph update value node representative implementation difficulty distributed memory algorithm occurs almost completely global phases must devise methods access remote data efficiently possible maintaining correctness minimizing synchronization costs optimization global phases essential good performance requires language provide easy understand cost model parallel operations also model must stable across variety platforms make algorithm truly portableif model changes dramatically new machine must optimize independently machine splitc language 7 provides cost model giving us set simple yet effective abstractions programming parallel machine allowing us implement algorithm straightforward fashion optimize global phase platforms particular figure 2 natural partitioning probabilistic meshes underlying lattice cut equalsize blocks processors provides global address space processor may access location global address space using global pointers processor owns specific region global space local region global pointer used like normal pointer reference entire global address space standard pointers reference portion local processor pointers help build data abstractions c distributed data structures splitc realized laying data across global address space using global pointers hook together edges distributed graph example merely global pointers one distributed vertex another global pointers also allow programmer determine owner referenced data without actually dereferencing pointer use ability local phase differentiate local edges must collapsed single nodes graph global phase remote edges must retained edges global graph clear distinction local global objects helps us apply cost model optimization using splitc also gives implementation portability versions splitc exist cray t3d ibm sp1 sp2 intel paragon thinking machines corp cm5 meiko cs2 networks workstations 2 20 23 29 although algorithm accepts arbitrary graphs input obtaining optimal performance requires reasonable partitioning graph across processors enhance locality load bal ancing partitioning techniques rely ability determine properties graph structure fortunately probabilistic meshes offer natural geometric partitioning via underlying lattice structures exploit partitioning high performance shown figure 2 simply cut underlying lattice equalsize sections one processor effectiveness simple partitioning discussed length later paper summary implemented portable hybrid algorithm finding connected components general graph implementation scales reasonably performs efficiently class graphs interest scientific community 3 machines study consider three largescale parallel machines cray t3d meiko cs2 thinking machines corp cm5 1 machines offer range computational communication performance evaluate algorithm implementation case splitc compiler built extension gcc version 245 provides local code generation code generated global access fully optimized exploit specific communication hardware capabilities t3d based dec alpha 21064 64bit dual issue second generation risc processor clocked 150 mhz 8 kb split instruction data caches splitc global read involves short instruction sequence gain addressability remote node load remote memory taking approximately 1 s2 cs2 based 90 mhz dualissue sparc microprocessor large cache communication supported dedicated elan processor within network interface access remote memory via wordbyword dma network transactions splitc global read issues command communications coprocessor via exchange instruction causes waiting elan thread either access remote memory begin dma transfer depending length remote read requires roughly 20 s23 29 cm5 based cypress sparc microprocessor clocked 33 mhz 64 kb unified instruction data cache splitc global read involves issuing cmaml active message access remote location reply value taking approximately 12 traditional measures computational performance node mflops specmarks offer little indication performance graph algorithm stresses storage hierachy calibrate local node performance empirically section 5 application scaling bottomline performance algorithm shown figure 1 four important graph types 2d40200 2d60200 graphs rectangular twodimensional probabilistic meshes 40 60 edge probability respectively 200x200 subgraph per processor similarly 3d4030 threedimensional probabilistic meshes 20 40 edge prob ability respectively 30x30x30 subgraph per processor importance particular graphs discussed fixed problem size per node scaling used t3d solving larger problem cm5 cs2 however problem size extremely large problems reasonably run single node use larger graphs would make obtaining high performance easier also shown table best published uniprocessor performance obtained cray c90 comparable graphs 2 c90 performance rate essentially independent problem size long fits memory previously flanigan 1 final version paper intend include ibm sp12 intel paragon active messages highperformance network workstations splitc language currently available platforms connected components algorithm running time writing draft data collection yet complete 2 use lower density graph 2d30 case makes comparison somewhat optimistic favor c90 graph greiner type mns mns mns mns 43 table 1 raw performance parallel connected components algorithm values table millions nodes processed per second using hybrid algorithm described text rightmost column shows millions nodes processed per second single node cray c90 using algorithm developed greiner tamayo achieved 125 million nodes per second much larger 2d graphs using 256node cm5 9 also hackl et al achieved nearly 7 mns 32 processors intel ipsc860 large 2d graph 12 neither results compared directly since applied particular problem specifically state edge presence probability graphs bottomline performance results pleasant goal understand depth performance characteristics hybrid algorithm fundamental bottlenecks presents performance affected machine characteristics end need examine range problem sizes graph structures across machines addition working class random graphs specific problem instance section sketch key aspects connected components problem influence algorithm performance validate probabilistic analysis sample graphs wellbehaved 41 equal nodesperprocessor scaling one expects total work solving connected components function number nodes v number edges e number components c graph class graphs considering expected number edges proportional number nodes p edge presence probability sampling random graphs figure 3 indicates given edge probability number components graph also proportional graph size scaling number nodes linearly number processors scale aspects problem uniformly figure 3 also suggests ratio components graph nodes varies significantly edge probability since slopes lines differ fortunately since number components per node largely independent graph size examine varies edge probability figure 4 shows expected number components per node 2d 3d graphs range edge presence probability curves normalized graphs 128 0864 million nodes 12e060 200000400000600000800000 1e0612e0614e06 number connected components number nodes connected components vs graph size varying edge presence 0 20 40 80 100 figure 3 expected number connected components 2d graphs given edge presence probability underlying lattice expected number components linear number connected components per node edge presence percentage 2d mesh 3d mesh figure 4 expected number connected components per node graph size increases fractional variance decreases making functions accurate large graphs fraction nodes largest component edge presence percentage 2d mesh 3d mesh figure 5 size largest connected component graph note rapid transition 2 3 dimensions 42 graph phase transition observe components per node shown figure 4 highly nonlinear behavior result interesting phenomenon critical understanding performance exhibited connected component algorithms edge presence probability crosses boundary nodes average degree two 50 2d graphs 33 3d graphs phase transition occurs mostly unconnected mostly connected graphs boundary graph consists large number fairly regular sized components boundary graph consists single large component number tiny components figure 5 shows fraction nodes largest component 2d 3d probabilistic meshes function edge presence probability vertical bars mark theoretical boundaries two graphs point node expected connect two nodes illustrate phase transition graphically figure 6 shows largest three components 2d graphs edge percentages boundary following table gives number nodes components shown rank 40 50 60 largest 281 38884 62174 2nd largest 219 2064 19 3rd largest 119 177 6 mostly connected graphs must expect performance algorithm strongly influenced presence single large component although phase transition makes difficult compare across graph types still good scaling rule particular graph type interest next section use nodespersecond metric help understand figure largest connected components several 256x256 2d graphs left right graphs edge presences percentages 40 50 60 white component largest graph followed red green blue sections made smaller count number connected components connected components distribution 2d40 measured gaussian figure 7 distribution number connected components note closely distribution 1000 samples 256x256 2d40 graph matches gaussian distribution mean standard deviation vertical lines mark mean one standard deviation either direction performance local global phases algorithm graph type naturally might question whether expected number components statistically well behavedit certainly obvious complex discrete operation like forming connected components yield nice distribution even random graphs find however expected number connected components per node indeed treated normal random variable consider figure 7 shows distribution number connected components found 1000 2d40 graphs 65536 nodes average number components per node saw figure 3 standard deviation 000220 samples binned tenths standard deviation figure also shows gaussian distribution mean standard deviation allow easy comparison two second implication figure 7 mostly unconnected graph partitioned large number subgraphs similar structure subgraphs likely number components much larger average causing fundamental load balancing problem 43 graph selection order study performance algorithm must select specific graphs study discussion suggests two important characteristics require attention process phase transition graph structure surfacetovolume ratio first algorithm act differently mostly connected mostly unconnected graphs second global phase algorithm fundamentally influenced cost remote access execution time local phase algorithm expected proportional number nodes local portion graph assume cost global phase monotonically increasing number nodes boundary local portionas size boundary increases number remote edges increases processing time rise cost model distributed memory machines tells us work done locally done much rapidly work done remotely putting facts together recognize surfacetovolume ratio increases performance decreases realization equivalent saying cannot achieve perfect speedup problem interesting exploration probabilistic meshes requires consider least two distinct surfacetovolume ratios demonstrate performance concepts interesting values surfacetovolume ratio characteristic obtained simply examining 2d 3d graphsa 3d cube number nodes 2d square much larger surfacetovolume ratio choose examine scaled speedup since allows us fix surfacetovolume ratio graph thereby simplifies analysis exploration phase transition requires examine graphs phases underlying lattice giving us total four graphs measure argument given obtain best results using largest graphs fit memory choose instead use modest amount memory shown table describing four graphs figure 8 possible local portion 2d50 graph white areas parts compo nent connect great distance processor underlying edge nodes per surfacetovolume memory per name lattice presence processor ratio processor 43 mb turns many expensive graphs compute connected components near edge presence phase transition boundary reason illustrated figure shows local section 2d50 graph one processor might several local components connected way remote parts graph however behavior graphs fairly chaotic near boundary large variations size largest component include graph types study 5 performance models section present results analyze algorithm building notions scaling begin discussion local phase illustrating phase transition affects even depthfirst search briefly discuss alternatives hybrid approach demonstrate deficiencies finally examine balance local global phases algorithm graph types selected previous section explain balance used understand speedup results 51 modeling local phase first step compare computational capabilities machines table 2 shows processing power single node platforms second column table gives graph c90 greiner t3d cs2 cm5 type mns mns mns mns 43 42 0885 0375 0239 table 2 single node connected components performance values table millions nodes processed per second using optimized depthfirst search algorithm comparison second column shows millions nodes processed per second cray c90 reported time number nodes dfs time required graphs varying size figure 9 time required dfs graph cm5 sparc node execution time linear size graph processing speeds single node one todays powerful sequential supercomputers cray c90 reported greiner 11 consider relative performance t3d cm5 cycle time alpha node t3d 45 times faster sparc cm5 hand memory access latency 3 times fast see table 2 performance t3d node ranges 35 41 times better performance cm5 node depending type graph begin understand characteristics machine dominate behavior connected components algorithms saw earlier connected components performance given graph expressed function number nodes since expected number edges connected components probabilistic mesh also functions number nodes fact local phase function linear size graph shown figure 9 figure see execution time edge presence percentage dfs time required various edge presences 200x200 128x128 100x100 figure 10 time required dfs graph cm5 sparc node expect time rise linearly edge presence probability p see strange behavior around 50 mark time local phase two graph types various size linear behavior makes nodes persecond metric meaningful local phase regardless problem size clearly true parallel algorithm graph size direct impact performance surfacetovolume ratio although information singlenode performance numbers cannot use make accurate predictions graph types machines 3 might consider measuring independent processing rates nodes edges allow us characterize probabilistic meshes phase transition graph structure breaks model shown figure 10 figure shows execution time 2d graphs rises rapidly around phase transition boundary 50 graphs figure large enough cache behavior problem unless graph structure changes significantly phase transition exactly however causing algorithm behave much differently since single dfs covers small portion graph less connected phase might think liquid phase covers nearly entire graph connected solid phase algorithmic differences lead differing cache behavior turn explains rise execution time around phase transition boundary nevertheless nodespersecond metric provide scalable information local phase graph type use information understand local global phases interact predict speedup 3 cm5 cs2 performance ratings table fairly similar numbers cs2 one number cm5 predict cm5 numbers within 4 actual values remember machines based sparc architecture however expect numbers similar predictions cm5 cs2 t3d result errors 16 either machine scaled number processors execution time number processors figure 11 cm5 execution time purely global algorithm 3d2030 processor 27000 nodes time 1 processor based local phase algorithm illustrates large overheads incurred skipping local phase parallel executions 52 comparing solutions continuing modeling process stop moment ask use hybrid algorithm algorithm assumes uniform access time entire global address space certainly easier write arguably easier understand well algorithm achieve good performance according cost model supported splitc distributed memory machines answer cannot ignoring data locality proves fatal performance skipping local phase algorithm using sv implementation find connected components able illustrate concept figure 11 figure shows cm5 execution time purely global algorithm 3d2030 graph although cost global phase asymptotically approaches constant value resulting constant fraction ideal speedup fact remains parallel algorithm requires 24 processors find components graph single processor using sequential algorithm results 2d40200 slightly better requiring 20 processors achieve uniprocessor figure merit highly connected graphs suffer phenomenon associated phase transition discuss section 53 never achieve speed single processor second alternative programmer use hybrid algorithm discard sequential programming abstractions writing global phase algorithm typically approach often taken overhead associated communication layers used high 912 global phases algorithms build new programming abstractions allow bulk communication processors added complexity design often results worse performance forces programmers rewrite implementation algorithm accept anything specific type graph 9 12 example work 2d graphs conversely hybrid implementation uses many sequential programming abstractions accept arbitrary graphs algorithm performs well provided graph partitioned well processors graphs interest many areas science execution time number processors total local global scaled number processors figure 12 execution time 3d2030 t3d local cost remains nearly constant number processors increases global cost graph disconnected phase quickly reaches plateau indicating speedup close constant fraction ideal speedup 53 modeling global phase consider work done global phase algorithm finding connected components local portion processor must examine remote edges process assume remote interactions neighboring processors fairly well loadbalanced global phase requires constant amount time independent number processors balance local global phases constant well giving us constant fraction ideal speedup fractional value depending relative cost two phases graphs disconnected phase approximation reasonable number remote edges wellbehaved random variable distributed components split across two processors handful might spread four even eight 3d graphs rarely component cross breadth processors local portion link together multiple processors global cost case consists primarily trading information neighbors cost rise number processors increases hypothesis verified data figure 12 shows local global execution times 3d2030 t3d expect nearly constant execution time global phase results nearly constant fraction ideal speedup increase number processors exact fraction obtain depends relative cost local global phases expect turn depend surfacetovolume ratio since relatively smaller number nodes boundary local portion result smaller global execution time relative local time fact case 2d40200 surfacetovolume ratio 199 achieves speedup 0508 per processor 3d2030 surfacetovolume ratio 187 achieves somewhat less 0395 per processor although global execution time nearly constant large numbers processors small numbers processors exhibit clear upward trend trend explained straight 0scaled number processors execution time number processors total local global figure 13 execution time 3d4030 t3d global cost graph connected phase rises almost linearly number processors indicating nearly flat speedup large numbers processors forward statistical analysis consider normal distribution random variable shown figure 7 distribution number connected components graph value single variable distribution likely end within standard deviation average almost surely within two standard deviations hybrid algorithm however works bulk synchronous phases processor work synchronize barrier synchronization effectively applies max function cost step naturally select increasing number independent variables normal distribution expect increasing maximum value set variables fact expect maximum rise fairly quickly first taper global execution cost figure 12 established foundations model disconnected phase consider performance graphs connected phase figure 13 breaks local global execution times 3d4030 t3d unlike graphs disconnected phase global cost almost linearly number processors clearly phase transition connected phase broken intuitive model disconnected phase connected phase graph dominated single large component single processor big component processor attaches single large component main component sends unresolved edges owner number remote edge structures explored owner large component thus grows nearly linearly number processors since algorithm operates bulk synchronous fashion processors must sit idle single processor handles bulk work shown figure 14 addition load imbalance caused single component phenomenon connected phase also note high degree contention around end algorithm large component completely formed another global iteration required remove connected representatives processing list iteration processors must access large component check remaining edges cm5 observe average time 100 remote access termination check recall uncongested remote access time number edge structures read edge presence percentage 2d maximum 2d average figure 14 load imbalance shown 2d graphs past phase transition boundary maximum number edge structures read processor grows rapidly though chaotically average number remains much smaller phenomenon occurs even dramatically 3d graphs 12 indicating eightfold increase policy caching could greatly improve algorithm regard effort required solve load balancing problem saw graphs disconnected phase graphs connected phase obey relationship surfacetovolume ratio performance 3d4030 speedup t3d nearly flattened 303 256 processors 2d60200 however speedup still increasing fairly quickly reaches 115 256 processors observe actual plateaus cm5 cs2 results machines relatively slow communication compared t3d figure shows scaled speedup four graphs 256 processors t3d interest ingly results indicate better performance graphs connected phase number processors small speedup lines surfacetovolume ratio cross number processors increase disconnected phase speedup continues rise linearly connected phase falls small numbers processors perform better graphs connected phase answer lies relative cost local global phases cost local phase significantly greater connected phase disconnected phase greater number edges also different algorithmic behavior discussed modeling local phase small numbers processors extra cost reflected global phase ten remote edges link component might explored cheaply five link distinct components since algorithm discards redundant remote edges soon recognizes cost global phase thus relatively less scaled number processors scaled speedup t3d figure 15 scaled speedup cray t3d note crossovers mostly unconnected mostly connected graphs 2 3 dimensions graphs connected phase tend many redundant edges graphs disconnected phase tend redundant edges data figures 12 13 illustrate difference execution time global phase 3d4030 catches time 3d2030 around 8 processors two graphs one disconnected phase one connected phase given us good intuitive grasp algorithms performance expect speedup disconnected graphs linear increase surfacetovolume ratio decreases connected graphs expect find speedup asymptotically approaching maximum value defined surfacetovolume ratio also gained intuition might create better algorithm solving connected graphs identifying nature problems reduce speedup yet investigate one major factor however relative costs communication computation given machine cray t3d best relative communication performance expect give best speedup results next section attempt use measurements communication computational performance predict speedup another platform cm5 54 speedup prediction recall moment simple model algorithm amount computation amount communication local phase algorithm pure computation measured terms number nodes processed per second saw modeling phase let us assume global phase dominated computation number processors scaled speedup cm5 speedup vs prediction t3d relative costs actual 2d40200 predicted 2d40200 actual predicted figure performance scaled speedup 2d40200 3d4030 predicted using scaled speedup t3d relative costs communication computation two machines rudimentary cost model specifically dominated remote reads small pieces data calling cost local phase p processors c p local cost global phase c p global express scaled speedup follows local c p local global local 1 inverting equation express ratio global phase cost local phase cost terms number processors speedup c p global c p local using assumptions content phases use 1 2 predict speedup one machine using speedup another machine relative costs communication computation example 2d40 graph local phase measurements indicate cm5 takes 35 times longer t3d similarly measurements discussed section 3 time read remote value machines indicate cm5 takes 12 times long t3d multiply 2 12 back 1 predict speedup cm5 figure shows results attempt 2d40 3d40 predictions terribly good expect simple model within 25 values 2d40 18 values 3d40 scaled number processors scaled speedup cm5 figure 17 scaled speedup thinking machines cm5 2d60 speedup yet drop 2d40 graph processors 55 results cm5 cs2 figure 17 gives speedup results cm5 figure 18 gives results cs2 expect scalability algorithm decreases relative communication performance machine goes 6 conclusion connected components important problem underlying emerging areas computational physics well many areas computer science although simple elegant sequential solution parallel solution proved challenging practice order obtain high performance modern distributed memory machines critical exploit local node performance approach utilizes hybrid algorithm combines simple sequential solution subgraph local node variant classic pram solutions resulting global graph subgraphs implemented alogithm splitc used find connected components class graphs called probabilistic meshes arising cluster dynamics methods graphs built underlying lattice structure instance graph produced using single probability decide whether edge underlying lattice present graph instance hybrid approach proved effective local phases fast reducing global graph reduced amount work global phases resulting solution yields best performance reported date problem however solution still demands high communication performance obtain reasonable speedups thorough analysis inherent properties algorithm provides natural scaling rule exposes critical phase transition scaled number processors scaled speedup cs2 figure scaled speedup meiko cs2 edge probability increases point graphs mostly unconnected collections small components scaled speedup nearly linear slope determined essential ratio communication computational performance point graph mostly single large component speedup limited due load imbalance remote memory contention effects quantified cray t3d meiko cs2 thinking machines cm5 r case empirical evaluation cray t3d compiler perspective new connectivity msf algorithms ultracomputer pram journal physics 17 finding connected components olog n log log n time erew pram connected component labeling coarse grain parallel com puters experimental study parallel programming splitc vectorized parallel cluster labeling method monte carlo dynamics optimal randomized parallel algorithm finding connected components graph comparison parallel algorithms connected components parallelization 2d swendsenwang algorithm study connected component labeling algorithms mpp computing connected components parallel computers connected components olog 3 2n parallel time crew pram journal physics 17 fast connected components algorithm erew pram connected components distributed memory machines implementing efficient portable global memory layer distributed memory multiprocessors physical review letters 41 vectorized algorithm cluster formation swendsenwang dynamics experience active messages meiko cs2 olog n parallel connectivity algorithm nonuniversal critical dynamics monte carlo simulations cluster monte carlo algorithms journal physics journal physics society japan 27 splitc meiko cs2 tr robot vision italicoitalicnsupscrpt2 log italicn parallel maxflow algorithm optimal randomized parallel algorithm finding connected components graph connected components italicoitaliclgsupscrpt32supscrptitalicvitalic parallel time crew pram extended abstract fast connected components algorithms erew pram parallel programming splitc connected component labeling coarse grain parallel computers comparison parallel algorithms connected components finding connected components italicoitaliclog italicnitalic loglog italicnitalic time erew pram computing connected components parallel computers scaling parallel programs multiprocessors case networks workstations experience active messages meiko cs2 implementing efficient portable global memory layer ctr michael mitzenmacher designing stimulating programming assignments algorithms course collection exercises based random graphs acm sigcse bulletin v28 n3 p2936 sept 1996 richard p martin amin vahdat david e culler thomas e anderson effects communication latency overhead bandwidth cluster architecture acm sigarch computer architecture news v25 n2 p8597 may 1997 mark w goudreau kevin lang satish b rao torsten suel thanasis tsantilas portable efficient parallel computing using bsp model ieee transactions computers v48 n7 p670689 july 1999