hash based parallel algorithms mining association rules paper propose four parallel algorithms npa spa hpa hpaeld mining association rules sharednothing parallel machines improve performance npa candidate itemsets copied amongst processors lead memory overflow large transaction databases remaining three algorithms partition candidate itemsets processors partitioned simply spa transaction data braodcast processors hpa partitions candidate itemsets using hash function eliminate broadcasting also reduces comparison workload significantly hpaeld fully utilizes available memory space detecting extremely large itemsets copying also effective flattering load processors implemented algorithms sharednothing environment performance evaluations show best algorithm hpaeld attains good linearity speedup ratio effective handling skew b introduction recently database mining begun attract strong attention progress barcode technology pointofsales systems retail company become generate large amount transaction data data archived used effi ciently advance microprocessor secondary storage technologies allows us analyze vast amount transaction log data extract interesting customer behaviors database mining method efficient discovery useful information rules previously unknown patterns existing data items embedded large databases allows effective utilization existing data one important problems database mining mining association rules within database 1 called basket data analysis problem basket data type typically consist transaction identifier bought items partransaction analyzing transaction data extract association rule 90 customers buy b also buy c several algorithms proposed solve problem1234567 however sequential algorithms finding association rules requires scanning transaction database repeatedly order improve quality rule handle large amounts transaction data requires incredibly long computation time general difficult single processor provide reasonable response time 7 examined feasibility parallelization association rule mining 1 6 parallel algorithm called pdm mining association rules proposed pdm copies candidate itemsets among processors explain later second pass apriori algorithm introduced ragrawal rsrikant2 candidate itemset becomes large fit local memory single processor thus requires reading transaction dataset repeatedly disk results significant performance degradation paper propose four different parallel algorithms npa spa hpa hpaeld mining association rules based apriori algorithm npa non partitioned apriori candidate itemsets copied among processors pdm mentioned corresponds npa remaining three algorithms partition candidate itemsets processors thus exploiting aggregate memory system effectively partitioned simply simply partitioned apriori transaction data broadcast processors hpa hash partitioned apriori partitions candidate itemsets using hash function hash join eliminates transaction data broadcasting reduce comparison workload significantly case size candidate itemset smaller 1 paper presented local workshop japan available system memory hpa use remaining free space however hpaeld hpa extremely large itemset duplication utilize memory copying itemsets itemsets sorted based frequency appearance hpaeld chooses frequently occurring itemsets copies processors memory space used contributes reduce communication among processor hpaeld extension hpa treats frequently occurring itemsets special way reduce influence transaction data skew implementation sharednothing 64node parallel computer fujitsu ap1000ddv shows best algorithm hpaeld attains satisfactory linearity speedup also effective skew handling paper organized follows next section describe problem mining association rules section 3 propose four parallel algorithms performance evaluations detail cost analysis given section 4 section 5 concludes paper mining association rules first introduce basic concepts association rules using formalism presented 1 let set literals called items set transactions transaction sets items transaction associated unique identifier called id say transaction contains set items itemset x support transaction set transactions contain x denote association rule implication form x rule two measures value support confidence support rule x supportx confidence c rule x transaction set means c transactions contain x also contain written ratio supportx problem mining association rules find rules satisfy userspecified minimum support minimum confi dence decomposed two subproblems 1 find itemsets support userspecified minimum support itemset called large itemsets 2 large itemset derive rules userspecified minimum confidence follows large itemset x minimum confidence rule x0y derived example let 5g transaction database let minimum support minimum confidence 60 70 respectively first step generates large itemsets 3g second step association derived finding large itemsets association rules derived straightforward manner second subproblem big issue however large scale transaction data sets used database mining first subproblem nontrivial problem much research date focused first subproblem briefly explain apriori algorithm finding large itemsets proposed 2 since parallel algorithms proposed us section 3 based algorithm figure 1 gives overview algorithm using notation given table 1 kitemset itemset k items large kitemsets whose support larger userspecified minimum support candidate kitemsets potentially large itemset table 1 notation first pass pass 1 support count item counted scanning transaction database hereafter prepare field named support count itemset used measure many times itemset appeared transactions since itemset contains single item item support count field items satisfy minimum support picked items called large 1itemset l 1 kitemset defines set k items second pass pass 2 2 itemsets generated using large 1itemset called candidate 2itemsets c 2 support count candidate 2itemsets counted scanning transaction database support count itemset means number transactions contain itemset end scan large 1itemsets l k01 6 candidates size k generated l k01 transactions 2 increment support count candidates c k contained candidates c k satisfy minimum support answer figure 1 apriori algorithm ning transaction data large 2itemsets l 2 satisfy minimum support determined following denotes kth iteration pass k 1 generate candidate itemset candidate kitemsets c k generated using large k 0 1itemsets l k01 determined previous pass see section 21 2 count support support count candidate kitemsets counted scanning transaction database 3 determine large itemset candidate kitemsets checked whether satisfy minimum support large kitemsets l k satisfy minimum support determined 4 procedure terminates large itemset becomes empty otherwise k 1 21 apriori candidate generation procedure generating candidate kitemsets using 1itemsets follows given large k 0 1itemset want generate superset set large kitemsets candidate generation occurs two steps first join step join large k 0 1 itemset k 0 1itemset next prune step delete itemsets candidate kitemset k 01subset candidate itemsets large k 0 1itemset 3 parallel algorithms section describe four parallel algorithms npa spa hpa hpaeld first sub problem call count support processing finding large itemsets sharednothing parallel machines 31 algorithm design sequential algorithm count support processing requires largest computation time transaction database scanned repeatedly large number candidate itemsets examined designed parallel algorithm count support processing processor hold candidate item sets parallelization straightforward 2 however large scale transaction data sets assumption hold figure 2 shows number candidate itemsets large itemsets pass statistics taken real pointofsales data figure 2 vertical axis log scale candi100100001e061e08 memory usage itemsets bytes pass number candidate itemsets large itemsets figure 2 real pointofsales data date itemset pass 2 large fit within local memory single processor npa candidate itemsets copied amongst processors case candidate itemsets fit within local memory single processor candidate itemsets partitioned fragments fits memory size processor support count processing requires repetitive scanning transaction database remaining three algorithms spa hpa hpaeld partition candidate itemsets memory space processors thus spa hpa hpaeld exploit total sys tems memory effectively number processors increases simplicity assume size candidate itemsets larger size local memory single processor smaller sum memory processors easy later introduce algorithm named npa reason parallelization easy clarified extend algorithm handle candidate itemsets whose size exceeds sum processors memories 32 non partitioned apriori npa npa candidate itemsets copied processors processor work independently final statistics gathered coordinator processor minimum support conditions examined figure 3 gives behavior pass k pth processor npa using notation given table 2 items fc fragments fits processors local memory increment support count candidates 1 contained send support count c 1 coordinator 3 coordinator determine l satisfy userspecified minimum support c 1 broadcast l 1 processors 3 receive l 1 coordinator l dk 2 l k01 6 candidates size k generated l k01 fc partition c k fragments fits processors local memory increment support count candidates k contained send support count c k coordinator 3 coordinator determine l k satisfy userspecified minimum support c k broadcast l 1 processors 3 receive l k coordinator l figure 3 npa algorithm processor works follows 1 generate candidate itemsets large kitemsets candidate kitemsets size c k bytes size main memory bytes transactions stored local disk pth processor sets fragment candidate kitemsets fragment fits local memory processor j size c k bytes k sets large kitemsets derived c k table 2 notation processor generates candidate kitemsets using large k 0 1itemsets insert hash table 2 scan transaction database count support count value processor reads transaction database local disk generates kitemsets transaction searches hash table hit occurs increment support count value 3 determine large itemsets reading transaction data pro cessors support count gathered coordinator checked determine whether minimum support condition satisfied 4 large kitemset empty algorithm termi nates otherwise k coordinator broadcasts large kitemsets processors goto 1 size candidate itemsets exceeds local memory single processor candidate itemsets partitioned fragments fits within processors local memory process repeated fragment figure 3 beginning loop shows method candidate itemsets divided fragments fragment processed sequentially although algorithm simple transaction data exchanged among processors second phase disk io cost becomes large since algorithm reads transaction database repeatedly candidate itemsets large fit within processors local memory 33 simply partitioned apriori spa npa candidate itemsets partitioned copied among processors however candidate itemsets usually becomes large fit within local memory single processor generally occurs second pass spa partitions candidate itemsets equally memory space processors thus exploit aggregate memory systems memory efficiency low copy based npa since candidate itemsets partitioned among processors processor broadcast transaction data processors second phase broadcast required npa figure 4 gives behavior pass k pth processor spa using notation table 3 assume size candidate itemset smaller size sum processors memory extension algorithm handle much larger candidate itemset easy divide candidate itemsets fragments like npa large kitemsets candidate kitemsets transactions stored local disk pth processor sets candidate kitemsets assigned k pth processor means number processors k sets large kitemsets derived table 3 notation processor works follows 1 generate candidate itemsets processor generates candidate kitemsets using large k 0 1itemsets inserts part candidate itemsets hash table candidate kitemsets assigned processors roundrobin manner 3 2 scan transaction database count support count value 3 kitemsets assigned equally processors roundrobin manner roundrobin mean candidates assigned processors cyclical manner ith candidate assigned processor mod n number processors system items assigned pth processor broadcast processors receive transaction sent processors increment support count candidates contained received trans action candidates c p satisfy userspecified minimum support 3 processor determine individually whether assigned candidate kitemset userspecified minimum support 1 coordinator 3 coordinator make l 1 1 broadcast processors 3 receive l 1 coordinator l k01 6 k g candidates size k assigned pth processor generated broadcast processors receive transaction sent processors increment support count candidates contained received transaction candidates c p satisfy userspecified minimum support k coordinator 3 coordinator make l k k broadcast processors 3 receive l k coordinator figure 4 spa algorithm processor reads transaction database local disk also broadcasts processors transaction en try read disk received another processors support count incremented way npa 3 determine large itemsets reading transaction data processor determine individually whether candidate kitemset satisfy userspecified minimum support processor send l p k coordinator l k derived 4 large kitemset empty algorithm termi nates otherwise k coordinator broadcasts large kitemsets processors goto 1 although algorithm simple easy im plement communication cost becomes large since algorithm broadcasts transaction data second phase 34 hash partitioned apriori hpa hpa partitions candidate itemsets among processors using hash function like hash join eliminates broadcasting transaction data reduce comparison workload significantly figure 5 gives behavior pass k pth processor hpa using notation table 3 processor works follows 1 generate candidate itemsets processor generates candidate kitemset using large k 0 1itemsets applies hash function determines destination processor id id insert hash table discarded 2 scan transaction database count support count processor reads transaction database local disk generates kitemsets transaction applies hash function used phase 1 derives destination processor id sends kitemset itemsets received processors locally generated whose id equals pro cessors id search hash table hit increment support count value 3 determine large itemset spa items assigned pth processor based hashed value forall items x 2 determine destination processor id applying hash function used item partitioning send item id increment support count item receive item processors increment support count item candidates c p 1 minimum sup port 3 processor determine individually whether assigned candidate kitemset userspecified minimum support 3 1 coordinator 3 coordinator make l 1 1 broadcast processors 3 receive l 1 coordinator l k01 6 candidate kitemsets whose hashed value corresponding pth processor determine destination processor id applying hash function used item partitioning send kitemset id increment support count itemset receive kitemset processors increment support count itemset candidates c p k minimum support k coordinator 3 coordinator make l k k broadcast processors 3 receive l k coordinator figure 5 hpa algorithm 4 large kitemset empty algorithm termi nates otherwise k coordinator broadcasts large kitemsets processors goto 1 35 hpa extremely large itemset duplication case size candidate itemset smaller available system memory hpa use remaining free space however hpaeld utilize memory copying itemsets itemsets sorted based frequency ap pearance hpaeld chooses frequently occurring itemsets copies processors memory space used contributes reduce communication among pro cessor hpa generally difficult achieve flat workload distribution transaction data highly skewed itemsets appear frequently transaction data processor itemsets receive much larger amount data others might become system bottleneck real situations skew items easily discovered retail applications certain items milk eggs appear frequently others hpaeld handle problem effectively since treats frequently occurring itemset entries special way hpaeld copies frequently occurring itemsets among processors counts support count locally like npa first phase processors generate candidate kitemset using large k01itemsets sum support values large itemset exceeds given threshold inserted processors hash table remaining candidate itemsets partitioned hpa threshold determined available memory fully utilized using sort reading transaction data processors support count gathered checked whether satisfies minimum support condition since algorithm steps equal hpa omit detailed description hpaeld performance evaluation figure 6 shows architecture fujitsu ap1000ddv system measured performance proposed parallel algorithms mining association rules npa spa hpa hpa eld ap1000ddv employs sharednothing archi tecture 64 processor system used processor called cell 25mhz sparc 16mb local memory 1gb local disk drive pro host snet bnet tnet cell disk figure organization ap1000ddv system jdj number transactions jtj average number items par transactions jij average number items maximal potentially large itemsets name jtj jij jdj size table 4 parameters data sets cessor connected three independent networks net bnet snet communication processors done via torus mesh network called tnet broadcast communication done via bnet addition special network barrier syn chronization called snet provided evaluate performance four algo rithms synthetic data emulating retail transactions used generation procedure based method described 2 table 4 shows meaning various parameters characteristics data set used experiments 41 measurement execution time figure 7 shows execution time four proposed algorithms using three different data sets varying minimum support values 164 2 processors used experiments transaction data evenly spread processors local disks experiments parallel algorithm adopted pass 2 remaining passes performed using npa since single processors memory cannot hold entire candidate itemsets pass 2 fits npa efficient hpa hpaeld significantly outperforms spa elapsed time sec hpa hpaeld elapsed time sec hpa hpaeld elapsed time sec hpa hpaeld figure 7 execution time varying minimum support value since transaction data broadcast processors spa communication costs much larger spa hpa hpaeld data broadcasted transfered one processor determined hash function addition spa transmits transaction data hpa hpa eld transmit itemsets reduces communication costs npa execution time increases sharply minimum support becomes small since candidate itemsets becomes large small minimum sup port single processors memory cannot hold entire candidate itemsets npa divide candidate itemsets fragments processors scan transaction data repetitively frag ment significantly increases execution time 42 communication cost analysis analyze communication costs algorithm since size transaction data usually much larger candidate item set focus transaction data transfer npa candidate itemsets initially copied processors incurs processor communication addition last phase processing processor sends support count statistics coordinator minimum support condition ex amined also incurs communications overhead ignore overhead concentrate transaction data transfer spa hpa second phase spa processor broadcasts transaction data processors total amount communication data spa pass k expressed follows n number processors ip number items ith transaction pth processor p number pth processors transactions jdj number transactions hpa itemsets transaction transmitted limited number processors instead broadcasting number candidates dependent data synthesized generator total amount communication hpa pass k expressed follows ip one transaction potentially generate ip c k candi dates however practice filtered denoted parameter ff k ip since ff usually small 4 spa k since difficult derive ff measured amount data received processor figure 8 shows total amounts received messages spa hpa hpaeld t15i4 transaction data used 04 minimum support see figure 8 amount messages received hpa much smaller spa hpaeld amount messages received reduced since part candidate itemset handled separately itemsets correspond transmitted locally processed5001500amount receive message size spa hpa hpaeld figure 8 amount messages received pass 2 43 search cost analysis second phase hash table consists candidate itemsets probed transaction itemset 4 number processors small number items transaction large hpa k could larger spa reasonable number processors happen see figure 8 currently experiments mining association rules items classification hierarchy combination items becomes much larger ordinary mining association rules ff k increases hpa k tends increase well report case future paper npa number probes pass k expressed follows ip amount candidate itemset bytes size main memory single processor bytes npa candidate itemsets large fit single processors memory candidate itemsets divided supports counted scanning transaction database repeatedly spa every processor must process transaction data number searches pass k expressed follows ip hpa hpaeld number searches pass k expressed follows ip search cost hpa hpaeld always smaller spa apparent hpa k communication cost also search cost also reduced significantly employing hash based algorithms quite similar way hash join algorithm works much better nested loop algorithms npa search cost depends size candidate itemsets candidate itemset becomes large npa k could larger spa k fits npa k search cost much smaller spa almost equal hpa figure 9 shows search cost three algorithms pass t15i4 data set used 16 processors minimum support 04 experimental results far shown passes except pass 2 adopts npa algorithm applied different algorithms pass 2 computationally heaviest part total processing however order focus search cost individual algorithm clearly algorithm applied passes cost of101000 number probe millions pass number hpa figure 9 search cost spa npa hpa npa changes drastically pass 2 search cost npa highly dependent size available main memory memory insufficient npas performance deteriorates significantly due cost increase pass 2 figure 9 search cost npa less spa however explained incurred lot additional io cost therefore total execution time npa much longer spa 44 comparison hpa hpaeld section performance comparison hpa hpaeld described hpaeld treat frequently appearing itemsets sepa rately order determine itemset pick use statistics accumulated pass 1 number pass increases size candidate itemsets decreases thus focused pass 2 number candidate itemsets separated adjusted sum nonduplicated itemsets duplicated itemsets would fit available memory figure shows execution time hpa hpaeld t15i4 varying minimum support value 16 processors system hpaeld always faster hpa smaller minimum support larger ratio difference execution times two algorithms becomes minimum support value decreases number candidate itemsets count support increases candidate itemsets frequently found cause large amounts communication performance hpa degraded high communications traffic100300500 elapsed time sec hpa hpaeld figure 10 execution time hpa hpaeld pass 2 figure 11 shows number probes processor hpa hpaeld t15i4 using processor system pass 2 picked example highly skewed horizontal axis denotes processor id hpa distribution number processor hpa hpaeld number probe millions figure 11 number search hpa hpa eld pass 2 probes flat since candidate itemset allocated one processor large amount messages concentrate certain processor many candidate itemsets occurring frequently hpaeld number probes comparatively flat hpaeld handle certain candidate itemsets separately thus reducing influence data skew however see figure 11 still remain deviation load amongst processors parallelize mining 64 proces sors introduces sophisticated load balancing mechanism requires investigation 45 speedup figure 12 shows speedup ratio pass 2 varying number processors used 16 32 48 64 curve normalized 16 processor execution time minimum support value set 040515253545 speedup ratio number processors hpa hpaeld ideal figure 12 speedup curve npa hpa hpaeld attain much higher linearity spa hpaeld extension hpa extremely large itemset decomposition increases linearity hpaeld attains satisfactory speed ratio algorithm focuses item distribution transaction file picks extremely frequently occurring items transferring items could result network hot spots hpaeld tries send items process locally small modification original hpa algorithm could improve linearity substantially 46 effect increasing transaction database size sizeup figure 13 shows effect increasing transaction database size number transactions increased 256000 2 million transactions used data set t15i4 behavior results change increased database size minimum support value set 04 number processors kept 16 shown parallel algorithms attains linearity 5 summary related work paper proposed four parallel algorithms mining association rules summary four200600100014001800 elapsed time sec amount transaction thousands hpa hpaeld figure 13 sizeup curve algorithms shown table 5 npa candidate itemsets copied amongst proces sors processor works entire candidate itemsets npa requires data transfer supports counted however case entire candidate itemsets fit within memory single processor candidate itemsets divided supports counted scanning transaction database repeatedly thus disk io cost npa high pdm proposed 6 copies candidate itemsets among processors disk io pdm also high remaining three algorithms spa hpa hpaeld partition candidate itemsets memory space processors better exploits total systems memory disk io cost low spa arbitrarily partitions candidate itemsets equally among processors since processor broadcasts local transaction data pro cessors communication cost high hpa hpaeld partition candidate itemsets using hash function eliminates need transaction data broadcasting reduce comparison workload significantly hpaeld detects frequently occurring itemsets handles separately reduce influence workload skew 6 conclusions since mining association rules requires several scans transaction file computational requirements large single processor reasonable response time motivates research paper proposed four different parallel algorithms mining association rules sharednothing parallel machine examined viabil npa spa hpa hpaeld candidate copy partition partition itemset partially copy io cost high low communica tion cost high low skew handling table 5 characteristics algorithms ity implementation 64 node parallel chine fujitsu ap1000ddv single processor hold candidate item sets parallelization straightforward sufficient partition transaction processors processor process allocated transaction data parallel named algorithm npa however try large scale data mining large transaction file candidate itemsets become large fit within main memory single processor addition size transaction file small minimum support also increases size candidate itemsets decrease minimum support computation time grows rapidly many cases discover interesting association rules spa hpa hpaeld partition transaction file partition candidate itemsets among processors implemented algorithms shardnothing parallel machine performance evaluations show best algorithm hpaeld attains good linearity speedup fully utilizing available memory space also effective skew handling present parallelization mining generalized association rules described 9 includes taxonomy isa hierarchy item belongs class hierarchy mining associations higher class lower class also examined thus candidate itemset space becomes much larger computation time also takes even longer naive single level association mining parallel processing essential heavy mining processing acknowledgments research partially supported priority research program ministry education would like thank fujitsu parallel computing research center allowing us use ap1000ddv systems r min ing association rules sets items large databases fast algorithms mining association rules effective hashbased algorithm mining association rules ef ficient algorithms discovering association rules effective algorithm mining association rules large databases efficient parallel data mining association rules considera tion parallelization database mining perfor mance evaluation ap1000 effects message handling broadcast barrier synchronization benchmark performance mining generalized association rules tr ctr ferenc kovcs sndor juhsz performance evaluation distributed association rule mining algorithms proceedings 4th wseas international conference software engineering parallel distributed systems p16 february 1315 2005 salzburg austria mohammed j zaki parallel distributed association mining survey ieee concurrency v7 n4 p1425 october 1999 takahiko shintani masaru kitsuregawa parallel mining algorithms generalized association rules classification hierarchy acm sigmod record v27 n2 p2536 june 1998 david w cheung kan hu shaowei xia asynchronous parallel algorithm mining association rules sharedmemory multiprocessors proceedings tenth annual acm symposium parallel algorithms architectures p279288 june 28july 02 1998 puerto vallarta mexico masahisa tamura masaru kitsuregawa dynamic load balancing parallel association rule mining heterogenous pc cluster systems proceedings 25th international conference large data bases p162173 september 0710 1999 masaru kitsuregawa masashi toyoda iko pramudiono web community mining web log mining commodity cluster based execution australian computer science communications v24 n2 p310 januaryfebruary 2002 euihong sam han george karypis vipin kumar scalable parallel data mining association rules ieee transactions knowledge data engineering v12 n3 p337352 may 2000 takayuki tamura masato oguchi masaru kitsuregawa parallel database processing 100 node pc cluster cases decision support query processing data mining proceedings 1997 acmieee conference supercomputing cdrom p116 november 1521 1997 san jose ca masato oguchi masaru kitsuregawa optimizing transport protocol parameters large scale pc cluster evaluation parallel data mining cluster computing v3 n1 p1523 2000 david w cheung kan hu shaowei xia adaptive algorithm mining association rules sharedmemory parallel machines distributed parallel databases v9 n2 p99132 march 2001 dejiang jin sotirios g ziavras superprogramming approach mining association rules parallel pc clusters ieee transactions parallel distributed systems v15 n9 p783794 september 2004 valerie guralnik george karypis parallel treeprojectionbased sequence mining algorithms parallel computing v30 n4 p443472 april 2004 david w cheung yongqiao xiao effect data distribution parallel mining associations data mining knowledge discovery v3 n3 p291314 september 1999 masato oguchi masaru kitsuregawa dynamic remote memory acquisition parallel data mining atmconnected pc cluster proceedings 13th international conference supercomputing p246252 june 2025 1999 rhodes greece lilian harada naoki akaboshi kazutaka ogihara riichiro take dynamic skew handling parallel mining association rules proceedings seventh international conference information knowledge management p7685 november 0207 1998 bethesda maryland united states claudio silvestri salvatore orlando distributed approximate mining frequent patterns proceedings 2005 acm symposium applied computing march 1317 2005 santa fe new mexico w cheung lee v xiao effect data skewness workload balance parallel data mining ieee transactions knowledge data engineering v14 n3 p498514 may 2002 frans coenen paul leng partitioning strategies distributed association rule mining knowledge engineering review v21 n1 p2547 march 2006 sung zhao li chew l tan peter ng forecasting association rules using existing data sets ieee transactions knowledge data engineering v15 n6 p14481459 november john holt soon chung parallel mining association rules text databases journal supercomputing v39 n3 p273299 march 2007 vipin kumar mohammed zaki high performance data mining tutorial pm3 tutorial notes sixth acm sigkdd international conference knowledge discovery data mining p309425 august 2023 2000 boston massachusetts united states