interpreting stale load information abstractin paper examine problem balancing load largescale distributed system information server loads may stale wellknown sending request machine apparent lowest load behave badly systems yet technique common practice systems use roundrobin random selection algorithms entirely ignore load information use small subset load information rather risk extremely bad performance one hand ignore chance use load information improve performance develop strategies interpret load information based age simulation examine several simple algorithms use load interpretation strategies range workloads experiments suggest properly interpreting load information systems 1 match performance aggressive algorithms load information fresh relative job arrival rate 2 outperform best algorithms examine much percent information moderately old significantly outperform random load distribution information older still 4 avoid pathological behavior even information extremely old b introduction balancing load distributed system well known strategy sending request leastloaded machine behave badly load information old 11 18 21 systems herd effect often develops machines appear underutilized quickly become overloaded everyone sends requests machines new load information propagated combat problem systems work supported part nsf cise grant cda9624082 grants novell sun dahlin also supported nsf career grant 9733842 adopt randomized strategies ignore load information use small subset load information systems may give opportunity avoid heavily loaded machines load balancing stale information becoming increasingly important problem distributed operating systems many recent experimental operating systems included process migration facilities 2 6 9 16 17 23 24 25 26 30 common workstation clusters include production load sharing programs lsf 31 dqs 10 addition many network dns servers routers switches include ability multiplex incoming requests among equivalent servers 1 5 8 several runtime systems distributed parallel computing clusters metacomputers include modules balance requests among nodes 12 14 server load may also combined locality information wide area network wan information systems selecting http server cache 13 22 28 systems include larger numbers nodes distance nodes increases becomes expensive distribute uptodate load information thus important systems make best use old information paper attempts systematically develop algorithms using old information core idea use servers last reported load information l also use age information estimate rate new jobs arrive change information example periodic update model load information 21 updates server load information every seconds clients using algorithm calculate fraction requests send server order equalize load across servers end epoch new request epoch clients randomly choose server according probabilities paper devise load interpretation li algorithms analyzing relevant queuing systems evaluate algorithms via simulation range load information models workloads li algorithms load information fresh eg small algorithms tend send requests machines recently reported low load algorithms match performance aggressive algorithms exceeding performance algorithms use random subsets load information pure random algorithms use load information conversely load information stale li algorithms tend distribute jobs uniformly across servers thus perform well randomized algorithms dramatically better algorithms naively use load information finally load information modest age li algorithms outperform current alternatives much 60 algorithms attempt cope stale load information proposed mitzenmacher 21 added benefit restricting amount load information clients may consider dispatching jobs may reduce amount load information must sent across network examine variations li algorithms base decisions similarly reduced information conclude even severely restricted information algorithms use li outperform furthermore modest amounts load information allow li algorithms achieve nearly full performance thus li decouples question much load information used question interpret information primary disadvantage approach requires clients estimate told job arrival rate age load information information avail able clients misestimate values algorithms poor performance note however although algorithms make use stale load information explicitly track factors algorithm implicitly assume parameters fall within range values load information considered fresh parameters fall outside range algorithms perform quite badly con versely algorithms explicitly include parameters gracefully degrade information becomes relatively stale rest paper proceeds follows section 2 describes related work particular emphasis mitzenmachers recent study 21 base much methodology several system models section 3 introduces models old information section 4 describes load interpretation algorithms use section 5 contains experimental evaluation algorithms section 6 summarizes conclusions related work awerbuch et al 3 examined load balancing limited information however model differs considerably particular focus task selecting good server job jobs placed adversary model jobs placed entities act best interest seek interfere one another difference allows us aggressively use past information predict future number theoretical studies 4 7 15 20 27 suggested load balancing algorithms often quite effective even amount information examined severely restricted explore combine idea li section 56 several studies examined behavior load balancing old limited information queuing studies eager et al 11 found simple strategies using limited information worked well mirchandaney et al 18 19 found delay increases random assignment performs well strategies use load information several system used heuristic weighing recent information heavily old information example smart clients prototype 29 distributed network requests across group servers using heuristic additionally common technique process migration facilities use exponentially decaying average estimate load machine eg load new load old k current 1 unfortunately algorithms used systems somewhat ad hoc clear circumstances use algorithms set constants goal study construct systematic framework using old load information study closely resembles mitzenmachers work 21 mitzenmacher examined system arriving jobs sent one several servers based stale information servers loads goal system minimize average response time examined family algorithms make server choice small random subsets servers avoid herd effect cause systems exhibit poor behavior clients chase apparently least loaded server mitzenmachers algorithm n servers instead sending request least loaded n servers client randomly selects subset size k servers sends request least loaded server subset note algorithm equivalent uniform random selection without load information equivalent sending request apparently least loaded server addition formulating ksubset algorithms solution problem mitzenmacher uses fluid limit approach develop analytic models systems case n 1 however primary results study come simulating queuing systems follow similar simulation methodology mitzenmacher concludes version algorithm good choice situations finds seldom performs significantly worse generally performs significantly better aggressive algorithms eg even modestly aggressive outperforms uniform random algorithm wide range update frequencies believe however approach still drawbacks particular note update frequency load informationchanges optimal value k also changes example mitzenmachers periodic update model one sample workload examines quickly becomes much better larger values similarly although outperforms workload reverse true larger values example algorithm factor 2 better variation also note mitzenmachers algorithms resulting arrival rate server depends servers rank sorted list server loads magnitude difference queue lengths servers furthermore least loaded servers receive requests phase generally servers ordered load lowest load ngamma1 highest given request sent server 1 servers 0 igamma1 chosen part random subset k servers 2 server chosen part subset probability server j chosen part kserver subset k probability conditions 1 2 hold fraction requests server rank figure 1 distribution requests servers ksubset algorithm numerator number ways choose place slots slot denominator number ways choose k elements n elements assuming element always chosen figure 1 illustrates resulting distributions range ks distributions something right flavormore heavily loaded nodes get fewer requests lightly loaded nodes however obvious slope one lines general right figure also illustrates large values k inappropriate large large fraction requests concentrated small number servers long period time 3 models old information several reasonable ways model delay load information sampled decision made job consideration arrives server different models appropriate different practical systems mitzenmacher found significant differences system behavior among models 21 therefore examine performance three models understand results wide range situations compare results directly literature take first two models periodic update continuous update mitzenmachers study 1 third model updateonaccess abstracts additional systems practical interest describe models detail 31 periodic continuous update mitzenmachers periodic update continuous update models visualized variations bulletin board model periodic update model imagine every seconds bulletin board visible arriving jobs updated reflect current load servers period bulletin board updates termed phase load information thus accurate beginning phase may grow increasingly inaccurate phase progresses continuous update model bulletin board constantly updated load information average board state seconds behind true system state request thus bases decisions state system average seconds earlier mitzenmacher finds probability distribution significant impact effectiveness different algorithms given average delay distributions high variance requests see newer information others see older information outperform distributions less variance jobs see data seconds old note real systems abstracted models would typically include centralized bulletin board periodic model could represent instance system periodically gathers load information servers multicasts clients continuous update model could represent system arriving job probes servers load information chooses server delay due network latency transfer time servers send load information clients job arrives destination server 32 updateonaccess final model examine examined mitzenmacher updateonaccess model explicitly model separate clients sending requests servers different clients may different views system load particular client sends request server assume server replies message contains systems current load snapshot system load may used clients next request system average load update delay equal clients interrequest time thus updateonaccess model assumes jobs sent active clients fresher picture load jobs sent inactive clients mitzenmacher found third model individual updates similar behavior periodic update model omit analysis model compactness consider model may applicable problems server selection problem internet 13 22 may prohibitively expensive maintain load information clients actively using service may possible clients maintain good pictures server load actively using service furthermore hypothesize system exhibits bursty access patterns may able perform good load balancing even though average nodes load information average quite stale disadvantage using updateonaccess model complex previous models example model results depend aggregate request rate also number clients generating given number requests many clients generating certain number requests load information average older one client generating number requests 4 algorithms interpreting old information section describe algorithms load balancing work interpreting load information context age first describe basic algorithm periodic update model describe aggressive algorithm model finally describe minor variations algorithms adapt continuous update updateonaccess models general algorithms interpreting load information follow two principles distinguish previous algorithms first consider magnitude imbalance nodes nodes ranks second modify interpretation information based age arrival rate requests system account expected changes system state time descriptions use following notation average age load information specific meaning depends update model n number servers average perserver arrival rate reported load queue length server arrive number requests expected arrive phase probability arriving request sent server 41 algorithms periodic update model phase length arrive arrive system goal basic load interpretation basic li algorithm determine fraction requests sent server order balance load represented server queue length sum jobs servers start phase plus jobs arrive phase equal across servers 2 begin l tot jobs servers l jobs server probability p send arriving job server arrive 8i l tot arrive see otherwise 1 first term numerator number jobs end server evenly divide incoming jobs plus current jobs second term numerator jobs already server numerator number jobs sent server phase denominator total number jobs expected arrive phase thus basic li algorithm send arriving request server probability p calculated current phase note l tot arrive phase short completely equalize load case want place arrive requests least loaded buckets even things well use following simple procedure case start phase pretend place arrive requests greedily sequentially least loaded buckets keep track number requests placed bucket tmp phase send arriving request server probability arrive 411 aggressive algorithm algorithm seems suboptimal following sense tries equalize load across servers end phase thus phase long system may spend long time significantly unbalanced server load aggressive algorithm might attempt subdivide phase use first part phase bring machines even state distribute requests uniformly across servers rest phase aggressive algorithm works follows without loss generality assume servers sorted l machine ith least loaded server set l sentinal value subdivide phase n intervals evenly distribute arrivals across machines bring 2 notice make simplifying assumption departure rate servers ignore effect departing jobs relative server queue lengths assumption correct servers always busy incorrect servers idle time phase assumption justified primarily concerned algorithms work well system heavily loaded case queues seldom empty impact simplification lightlyloaded systems overestimate queue length lightlyloaded machines send requests case probability distribution somewhat uniform across servers would ideal algorithms aggressive could experiments suggest simplification little performance impact loads l j1 thus subinterval j length n subinterval j probability arriving request sent machine 2 42 algorithms update models adapting basic li algorithm continuous update updateonaccess model simple use equation 1 calculate probabilities p sending incoming requests server difference periodic update model calculation based l estimates hold entire phase new models may change request p thought current estimate instantaneous rate requests sent server adapting aggressive li algorithm problematic use equation 2 calculate values based current l array however continuous update model effectively always end phase information seconds old although aggressive algorithm aggressive basic algorithm early subintervals phase eg j near 0 less aggressive later subintervals eg j near n thus aggressive algorithm may actually less aggressive basic algorithm update models large 5 evaluation section evaluate algorithms range update models workloads primary methodology simulate queuing systems model task arrivals poisson stream rate n collection n servers task arrives send one server queues based algorithm study server queues follow firstinfirstout discipline select default system parameters match used mitzenmachers study 21 facilitate direct comparison algorithms particular unless otherwise noted assume server service rate 1 service time task exponentially distributed mean time 1 initially examine basic li aggressive li algorithms periodic update continuous update updateonaccess models compare performance ksubset algorithms examined mitzenmacher explore three key questions li algorithms 1 impact bursty arrival patterns 2 impact misestimating system arrival rate 3 impact limiting amount load information available algorithms basic li aggressive li figure 2 service time v update delay periodic update model 51 periodic update model figure shows system performance periodic update model default param eters performance li algorithms good wide range update intervals large li algorithms suffer poor performance ksubset algorithms encounter k 1 fact li algorithms maintain measurable advantage oblivious random algorithm even large values example outperforms oblivious algorithm 9 aggressive li outperforms oblivious algorithm 22 modest values advantages larger example aggressive li 60 faster ksubset algorithms basic li 41 faster ksubset algorithms figure 3 details performance algorithms small values aggressive li outperforms algorithms least percent smallest value examined basic li generally better always least good ksubset algorithm range figure 4 shows performance system workload lighter load default load lighter need load balancing less pronounced gains algorithm random modest information fresh algorithms perform factor two better oblivious algorithm information stale performance ksubset algorithms nearly bad heavier load although exhibit poor behavior compared oblivious algorithm large entire range staleness examined 01 200 basic li aggressive li algorithms perform well better best ksubset oblivious algorithm basic li aggressive li figure 3 detail service time v update delay periodic update model average response time update interval basic li aggressive li figure 4 service time v update delay periodic update model basic li aggressive li figure 5 service time v update delay periodic update model figure 5 shows performance system servers rather standard 100 results qualitatively similar results standard 52 continuous update model figure 6 shows performance algorithms continuous update model system behavior depends distribution delay parameter show results four distributions delay average value order increasing variation constantt uniform tto 3t uniform0 2t exponentialt earlier discussion suggests aggressive li algorithm actually less aggressive basic li algorithm basic generally outperforms aggressive model therefore focus basic li algorithm mitzenmacher notes given ksubset algorithms performance improves distributions contain mix recent older information relationship seems present less pronounced li algorithms result distributions variability increases advantage li ksubset algorithms shrinks thus basic li seems clear choice constant uniform distributions value performance good ksubset algorithms given ksubset algorithm range basic lis performance significantly better exponential distribution however ksubset algorithms enjoy advantage 16 basic li figure 7 tests hypothesis relatively poor performance basic li situation algorithm calculates p using expected value whereas individual request may see significantly different values figure still varies according specified distribution rather knowing average average response time update interval basic li aggressive li constant b uniform tto 3t01020 average response time update interval basic li aggressive li26100 50 100 150 200 average response time update interval basic li aggressive li c uniform 0 2t exponential mean figure update delay continuous update model clients know average delay show result different distributions delay around uniform tto 3t01020 average response time update interval basic li aggressive li26100 50 100 150 200 average response time update interval basic li aggressive li b uniform 0 2t c exponential mean figure 7 service time v update delay continuous update model clients know age information actually encountered request c show result different figure 8 service time v update delay updateonaccess model value request knows value holds request algorithm calculates p vector using certain information compared performance figure 6 extra information improves performance distribution improvement becomes pronounced distributions variation conclude good estimates delay load information gathered request arrive server important getting best performance li algorithms 53 updateonaccess model figure 8 shows performance updateonaccess model model simulate number clients client uses load information gathered sending one request decide send next request thus equals perclient interrequest time vary fixed total arrival rate simply vary number clients requests issued model algorithms perform reasonably well appears perclient updates desynchronize clients enough reduce herd effect basic li algorithm outperforms others provides modest speedup wide range update intervals figure 9 service time v update delay updateonaccess model bursty workload 54 bursty arrivals figure 9 shows performance burstyarrival version updateonaccess model standard updateonaccess model client uses server loads discovered one request route next one generate burstyarrivals workload rather assume client produces exponentiallydistributed arrivals assume client whose average interrequest time produces burst b requests separated seconds bursts separated exponentialt b seconds figure 9 bursty workload significantly increases performance algorithms use server load compared oblivious algorithm although time clients picture server load average seconds old average request sees much fresher picture l vector suggests may often possible significantly outperform oblivious strategy even challenging workloads internet server selection 13 22 information likely old average clients requests service bursty basic li algorithm best tied best entire range examined 01 200 55 impact imprecise information primary drawback li algorithms require good estimates subsection 52 examined impact uncertainty subsection examine happens estimate incorrect believe servers supporting average response time update interval basic li 8load basic li 4load basic li 2load basic li 1load basic li 05load basic li 025load basic li 0125load figure 10 service time v update delay periodic update model clients misestimate arrival rate li algorithms would equipped inform clients current load arrival rate requests anticipate example server might report arrival rate seen recent period time might report maximum request rate anticipates encountering however may difficult systems accurately predict future request patterns based history figure shows performance periodic update model li algorithm uses incorrect estimate line shows performance used calculating p multiplied error factor e 1and 8 overestimate load algorithm conservative performance suffers bit underestimate load algorithm sends many requests apparentlyleastloaded servers performance poor conclude systems err side caution estimating figure note estimated 2 actual performance marginally worse actual also note experiments actual 09 system would unstable actual 10 words overestimate factor two one would predict service rate 18 times larger could ever sustained system suggest following strategy estimating systems maximum achievable throughput known use throughput estimate purposes li algo rithms system heavily loaded estimate little bit higher actual arrival rate lightly loaded estimate far high seen algorithm relatively insensitive overestimates arrival rate overestimating arrival rate little harm system lightly loaded average response time arrival rate lambda basic li actual lambda basic li assume lambda 10 figure 11 service time v arrival rate periodic update model 20 graph compares standard algorithms well variation basic li algorithm overestimates maximum achievable system throughput case conservative estimate tends make li algorithm distribute requests uniformly across servers acceptable strategy load low figure 11 illustrates effect assuming estimated 10 vary actual system two basic li linesone exact conservative estimates almost indistinguishable points difference two results less 45 07 difference always less 15 56 impact reduced information ksubset algorithms additional purpose beyond attempting cope stale load information restricting amount load information clients may consider dispatching jobs may reduce amount load information sent across network number theoretical 4 7 15 20 27 empirical 11 18 studies suggested load balancing algorithms often quite effective even amount information severely restricted basic li algorithm also adapted use subset server load information rather requiring vector servers loads ksubset version basic li algorithm basic lik select random subset k servers use algorithm determine bias requests among k nodes particular modify equation 1 use p 0 arrays size k rather n compute l 0 tot smaller l 0 array replace n k calculate arrive 0 k note standard ksubset algorithms select different subset incoming request average response time update interval basic li k2 basic li k3 basic li k100 a26100 50 100 150 200 average response time update interval basic li k2 basic li k3 basic li k10 basic li k100 b figure 12 service time v update delay ksubset version basic li algorithm updateonaccess model b continuous update fixed delay model figure 12 examines impact restricting information available basic li al gorithm experiment suggests basic lik algorithm achieve good perfor mance updateonaccess model original ksubset algorithms perform well li2 algorithms performance similar standard algorithms unlike standard ksubset algorithms lik algorithm given information performance becomes better li3 algorithm outperforms standard ksubset algorithms noticeable amount full basic li algorithm widens margin continuous update fixed delay model figure 12b performance lik algorithms also good case original ksubset algorithms behave badly lik versions behave nearly identically basic li system experiment versions li algorithm slightly better version smaller k consistently giving slightly better performance explanation improving behavior reduced information experiment experiments conclude li effective technique environments wish restrict much load information distributed system modest amounts load information allow li algorithms achieve nearly full performance thus li decouples question much load information used question interpret information 6 conclusions primary contribution paper present simple strategy interpreting stale load information approach resolves paradox algorithms using additional information often results worse performance using less information none load interpretation li strategy propose interprets load information based age system essentially always better uses information information fresh algorithm aggressively addresses imbalances information stale algorithm conservative believe approach may open door safely using load information attempt outperform random request distribution environments difficult maintain fresh information system designer know age information priori experiments suggest interpreting load information systems 1 match performance aggressive algorithms load information fresh 2 outperform current algorithms much 60 information moderately old 3 significantly outperform random load distribution information older still 4 avoid pathological behavior even information extremely old r next step server load balancing designing process migration facility charlotte experience making commitments face uncertainty pick winner almost every time balanced allocations dns support load balancing network tasking locus distributed unix system towards developing universal dynamic mapping algorithms cisco distributed director transparent process migration design alternatives sprite implementation research toward heterogeneous networked computing cluster distributed queuing system version 30 adaptive load sharing homogeneous distributed systems locating nearby copies replicated internet servers load distribution implementation mach microkernel analysis effects delays load sharing adaptive load sharing heterogeneous distributed systems power two choices randomized load balancing useful old information performance characteristics mirror servers internet using idle workstations shared computing environment process migration demosmp experiences amoeba distributed operating system queuing systems selection shortest two queues asymptotic approach squid internet object cache using smart clients build scalable services attacking process migration bottleneck load sharing facility large tr ctr michael rabinovich zhen xiao amit aggarwal computing edge platform replicating internet applications web content caching distribution proceedings 8th international workshop kluwer academic publishers norwell 2004 mauro andreolini michele colajanni riccardo lancellotti francesca mazzoni fine grain performance evaluation ecommerce sites acm sigmetrics performance evaluation review v32 n3 december 2004 simon fischer berthold vcking adaptive routing stale information proceedings twentyfourth annual acm symposium principles distributed computing july 1720 2005 las vegas nv usa request redirection algorithms distributed web systems ieee transactions parallel distributed systems v14 n4 p355368 april giovanni aloisio massimo cafaro euro blasi italo epicoco grid resource broker ubiquitous grid computing framework scientific programming v10 n2 p113119 april 2002 mauro andreolini michele colajanni ruggero morselli performance study dispatching algorithms multitier web architectures acm sigmetrics performance evaluation review v30 n2 september 2002 suman nath phillip b gibbons srinivasan seshan adaptive data placement widearea sensing services proceedings 4th conference usenix conference file storage technologies p44 december 1316 2005 san francisco ca bogumil zieba marten van sinderen maarten wegdam qualityconstrained routing publishsubscribe systems proceedings 3rd international workshop middleware pervasive adhoc computing p18 november 28december 02 2005 grenoble france mauro andreolini sara casolari load prediction models webbased systems proceedings 1st international conference performance evaluation methodolgies tools october 1113 2006 pisa italy yu state art locally distributed webserver systems acm computing surveys csur v34 n2 p263311 june 2002