analysis cacherelated preemption delay fixedpriority preemptive scheduling abstractwe propose technique analyzing cacherelated preemption delays tasks cause unpredictable variation task execution time context fixedpriority preemptive scheduling proposed technique consists two steps first step performs pertask analysis estimate cacherelated preemption cost execution point given task second step computes worst case response time task includes cacherelated preemption delay using response time equation linear programming technique step takes input preemption cost information tasks obtained first step paper also compares proposed approach previous approaches results show proposed approach gives prediction worst case cacherelated preemption delay 60 percent tighter obtained previous approaches b introduction realtime systems tasks timing constraints must satisfied correct op eration guarantee timing constraints extensive studies performed schedulability analysis 1 2 3 4 5 6 many cases make number assumptions simplify analysis one simplifying assumption cost task preemption zero real systems however task preemption incurs additional costs process interrupts 7 8 9 10 manipulate task queues 7 8 10 actually perform context switches 8 10 many direct costs addressed number recent studies focus practical issues related task scheduling 7 8 9 10 however addition direct costs task preemption introduces indirect costs due cache memory used almost computing systems today computing systems cache memory task preempted large number memory blocks 1 belonging task displaced cache memory time task preempted time task resumes execution preempted task resumes execution spends substantial amount time reload cache previously displaced memory blocks cache reloading greatly increases task execution time may invalidate result schedulability analysis overlooks cacherelated preemption costs rectify problem recent studies addressed issue incorporating cacherelated preemption costs schedulability analysis 12 13 studies assume cache block used preempting task replaces cache memory block needed preempted task pessimistic assumption leads loose estimation cacherelated preemption delay since replaced memory block may useful preempted task example possible replaced memory block one longer 1 block minimum unit information either present present cachemain memory hierarchy 11 assume without loss generality memory references made unit blocks needed one replaced without rereferenced even preemptions paper propose schedulability analysis technique considers usefulness cache blocks computing cacherelated preemption delay goal reduce prediction inaccuracy resulting pessimistic assumption proposed technique consists two steps first step perform pertask analysis compute number useful cache blocks execution point given task useful cache block execution point defined cache block contains memory block may rereferenced replaced another memory block number useful cache blocks execution point gives upper bound cacherelated preemption cost incurred task preempted point results pertask analysis given table specifies worst case preemption cost given number preemptions task table second step derives worst case response times tasks using linear programming technique 14 worst case response time equation 2 6 paper organized follows section ii survey related work section iii describes overall approach schedulability analysis considers cacherelated preemption cost sections iv v detail two steps proposed schedulability analysis technique focusing directmapped instruction cache memory section vi presents results experiments assess effectiveness proposed approach section vii describes extensions proposed technique setassociative cache memory also data cache memory finally conclude paper section viii ii related work schedulability analysis fixedpriority scheduling large number schedulability analysis techniques proposed within fixedpriority scheduling framework 2 3 4 6 liu layland 4 show rate monotonic priority assignment task shorter period given higher priority optimal task deadlines equal periods also give following sufficient condition schedulability task set consisting n periodic tasks c worst case execution time wcet estimate period 2 condition states total utilization task set ie lower given utilization bound ie n 2 1n gamma 1 task set guaranteed schedulable rate monotonic priority assignment later lehoczky et al develop necessary sufficient condition schedulability based utilization bounds 3 another approach schedulability analysis worst case response time approach 2 6 approach uses following recurrence equation compute worst case response hpi set tasks whose priorities higher equation term j2hpi r ec j total interferences higher priority tasks r c execution time equation solved iteratively iteration terminates r converges value r value compared deadline notations used throughout paper along denotes deadline assume without loss generality higher priority determine schedulability recently katcher et al 10 burns et al 7 8 provided methodology incorporating cost preemption schedulability analysis approaches preemption costs arising interrupt handling task queue manipulation contextswitching taken account schedulability analysis paper also interested incorporating cost preemption schedulability analysis however unlike studies main focus indirect preemption costs due cache memory increasingly used realtime computing systems b caches realtime systems cache memory used almost computing systems today bridge ever increasing speed gap processor main memory however due unpredictable per formance cache memory widely used realtime computing systems guaranteed worst case performance great importance unpredictable performance comes two sources intratask interference intertask interference interference occurs memory block task conflicts cache another block task recently considerable progress analysis intratask interference due cache memory interested readers referred 15 16 17 18 19 intertask interference main focus paper occurs memory blocks different tasks conflict one another cache two ways address unpredictability resulting intertask interference first way use cache partitioning cache memory divided disjoint partitions one partitions dedicated realtime task 20 21 22 23 techniques task allowed access partitions thus need consider intertask terference two different approaches cache partitioning hardwarebased softwarebased hardwarebased approaches extra addressmapping hardware placed processor cache memory limit cache access task partitions 20 21 22 hand softwarebased approaches specialized compiler linker used map tasks code data assigned cache partitions 23 cache partitioning improves predictability system removing cacherelated intertask interference number drawbacks one common drawback hardware softwarebased approaches require modification existing hardware software another common drawback limit amount cache memory available individual tasks finally case hardwarebased approach extra addressmapping hardware may stretch processor cycle time affects execution time every instruction way address unpredictability resulting intertask interference devise efficient method analyzing timing effects 12 basumallick nilsen extend rate monotonic analysis explained previous subsection take account intertask interference approach wcet estimate task modified c 0 original wcet estimate computed assuming task executes without preemption fl worst case preemption cost task might impose preempted tasks modification based pessimistic assumption cache block used preempting task replaces cache memory block needed preempted task approach total utilization given task set computed modified wcet estimates compared utilization bound given equation 1 determine schedulability task set one drawback approach suffers pessimistic utilization bound given equation 1 approaches 0693 large n 4 many task sets total utilization higher bound successfully scheduled rate monotonic priority assignment 3 rectify problem busquetsmataix et al 13 propose technique based response time approach technique makes pessimistic assumption cache block used preempting task replaces cache memory block needed preempted task assumption leads following equation computing worst case response time task e theta c j cacherelated preemption cost task j might impose lower priority tasks term fl j computed multiplying number cache blocks used task j time needed refill cache block utilization bound based response time based approaches assume cache block used preempting task replaces cache memory block needed preempted task pessimistic assumption leads loose estimation cacherelated preemption delay since possible replaced memory block one longer needed one replaced without rereferenced even lower priority task executed without preemption iii overall approach section overviews proposed schedulability analysis technique aims minimize overestimation cacherelated preemption delay due pessimistic assumption explained previous section purpose response time equation augmented follows proposed pc r total cacherelated preemption delay task r ie total cache reloading times 1 r 22 2231 fig 1 example pc r meaning pc r best explained example one given figure 1 example three tasks arrow figure denotes point task preempted shaded rectangle denotes cache reloading corresponding task resumes execution settings pc 3 r 3 total cacherelated preemption delay task 3 r 3 total sum cache reloading times 1 2 3 r 3 corresponds sum shaded rectangles figure augmented response time equation solved iteratively follows r 0 4 r k1 r k iterative procedure terminates r converged r value compared deadline determine schedulability compute pc r k iteration take following twostep approach 1 pertask analysis statically analyze task determine cacherelated preemption cost execution point cost task pays preempted execution point upperbounded number useful cache blocks execution point based information information worst case visit counts execution points construct following preemption cost table task preemptions 1 cost table f k kth marginal preemption cost cost task pays worst case kth preemption preemption 2 preemption delay analysis use linear programming technique compute r k preemption cost tables tasks set constraints number preemptions task higher priority tasks following two sections detail two steps iv pertask analysis useful cache blocks section describe pertask analysis technique obtain preemption cost table task initially focus case directmapped 3 instruction cache memory section section vii discuss extensions setassociative cache memory also data cache memory example cacherelated preemption cost consider directmapped cache four cache blocks assume cache 3 time cache blocks c 0 assume following memory block references made 3 directmapped cache memory block placed exactly one cache block whose index given memory block number modulo number blocks cache example useful cache blocks time cache blocks c 1 c 2 since contain memory blocks 5 6 respectively rereferenced replaced hand cache blocks c 0 c 3 useful time since 0 3 replaced 4 7 without rereferenced preemption occurs time memory blocks 5 6 contained cache blocks c 1 c 2 may replaced memory blocks intervening tasks thus need reloaded cache resumption additional time reload useful cache blocks cacherelated preemption cost time note additional cache reload time needed task preempted following explain technique estimating number useful cache blocks point program estimation number useful cache blocks technique estimating number useful cache blocks based data flow analysis 24 tasks program expressed control flow graph 4 cfg give 4 cfg node represents basic block edges represent potential flow control basic blocks 25 x b cache state p useful useful cache block j cache block control flow p fig 2 analysis usefulness cache blocks intuitive idea data flow analysis consider cfg given figure 2 figure pair c denotes reference memory block mapped cache block c cfg two incoming paths execution point p ie 1 2 two outgoing paths p ie 1 2 control flow came incoming path 1 cache block c would contain memory block point p since last reference cache block c reaching p similarly cache block c would memory block b point p control come incoming path 2 thus either b may reside cache block c p depending incoming path either first reference cache block c outgoing path p cache block may reused thus defined useful point p outgoing path 2 path thus cache block c defined useful p formal description define reaching memory blocks rmbs live memory blocks lmbs cache block similar reaching definitions live variables used traditional data flow analysis 24 set reaching memory blocks cache block c point p denoted rmb c contains possible states cache block c point p possible state corresponds memory block may reside cache block point memory block reside cache block c first mapped cache block c furthermore last reference cache block execution path reaching p set live memory blocks cache block c point p denoted p defined similarly set memory blocks may first reference cache block c p definitions useful cache block point p defined cache block whose rmbs lmbs least one common memory block figure 2 rmb c fm b g lmb c p fm g thus cache block c defined useful point p following explain compute rmbs cache blocks various execution points given program initially focus rmbs beginning end points basic blocks 5 rmbs points easily derived rmbs basic block boundaries see later formulate problem computing rmbs data flow problem define set gen c b set either null contains single memory block null basic block reference memory blocks mapped cache block c hand basic block b least one reference memory block mapped c gen c b contains unique element memory block last reference cache block c basic block note latter case memory block gen c b one reside cache block c end basic block b also note gen c b defined manner computed locally basic block example consider cfg given figure 3 cfg shows instruction memory block references made basic block assuming instruction cache direct mapped two blocks gen c 0 b 1 fm 2 g since 2 memory block whose reference last reference c 0 b 1 gen c b sets basic blocks cache blocks computed similarly given figure 3 gen c b defined manner rmbs c beginning b end b denoted rmb c b rmb c b respectively computed following two equations p predecessor b b 5 gen c b gen c b null b otherwise first equation states memory blocks reach beginning basic block derived reach ends predecessors b second equa 5 basic block sequence consecutive instructions flow control enters beginning leaves end without halt possibility branching except end 24 3 c 4 c 1 3 10 c fig 3 example gen c b tion states rmb c b equal gen c b gen c b null rmb c b otherwise 6 data flow equations solved using wellknown iterative approach 24 starts rmb c iteratively converges desired values rmb c rmb c iterative process described procedurally follows algorithm 1 find rmbs cache blocks beginning end basic block assuming gen c b computed basic block b cache block c initialize rmb c b rmb c bs cs basic block b cache block c begin 6 equation rewritten set kill c b set reaching memory blocks cache block c killed basic block b set kill c b obtained follows 1 null gen c b null 2 mc gamma gen c b gen c b null mc set memory blocks mapped c program rewritten form commonly used traditional data flow analysis b b gen c b change true change begin change false basic block b cache block c begin b p predecessor b p oldout rmb c b b gen c b else rmb c b rmb c b rmb c b 6 oldout change true indicated earlier rmbs points within basic block computed rmbs beginning basic block assume basic block following sequence instruction memory block references references processed sequentially starting c clear 1 cache block c 1 point following reference conflicting memory blocks c 1 point therefore rmbs c 1 reference simply fm 1 g however rmbs cache blocks b general rmbs c cache blocks c problem computing lmbs formulated similarly case rmbs difference lmb problem backward data flow problem 24 sets ie lmb c b computed sets ie lmb c b whereas rmb problem forward data flow problem 24 sets ie rmb c b computed sets ie rmb c b lmb problem set gen c b either set one element corresponding memory block whose reference first reference cache block c basic block b null none references b memory blocks mapped c using gen c b defined manner following two equations relate lmb c b lmb c b successor b b 6 gen c b gen c b null b otherwise iterative algorithm similar one computing rmbs used solve backward data flow problem difference algorithm starts lmb c bs cs uses two equations instead given equation 5 compute lmbs beginning end basic block lmbs points computed analogously case rmbs difference processing references backward starting end basic block rather forward starting beginning lmb problem lmbs c reference cache blocks usefulness cache block determined point computing intersection cache blocks rmbs lmbs point trivial calculate total number useful cache blocks point simply count useful cache blocks point multiplying total number useful cache blocks time refill cache block worst case cacherelated preemption cost point computed b derivation preemption cost table subsection explains construct preemption cost table task whose kth entry cost task pays worst case kth preemption th preemption preemption cost table constructed two types information 1 preemption cost point 2 worst case visit count point directly derived cfg given program loop bound loop program construction assumes worst case preemption scenario since cannot predict advance preemptions actually occur worst case preemption scenario occurs first preemption point largest preemption cost ie point largest number useful cache blocks second preemption point next largest preemption cost worst case preemption scenario assumed analysis safe worst case preemption scenario entries preemption cost table filled follows first pick point p 1 largest preemption cost fill first entry v p 1 th entry preemption cost v p 1 worst case visit count p 1 pick point second largest preemption cost perform steps starting v p 1 1th entry process repeated number entries preemption cost table exhausted assuming number entries table k k 0 th marginal preemption cost k 0 k conservatively estimated kth marginal preemption cost since marginal preemption cost nonincreasing applying pertask analysis explained section tasks task set obtain following set preemption cost tables one task f ij jth marginal preemption cost preemptions 1 cost f 11 f 12 f 13 preemptions 1 cost f 21 f 22 f 23 preemptions 1 cost f 31 f 32 f 33 preemptions 1 cost f n1 f n2 f n3 v calculation worst case preemption delays tasks section explain compute safe upper bound pc r k used equation 4 section iii preemption cost table formulate problem integer linear programming problem set constraints first define g jl number invocations j preempted least l times given response time r k example consider figure 4 task j invoked three times given r k first invocation task j ie j1 preempted three times second third invocations j ie j2 j3 preempted definition g jl g j1 3 g j2 1 g j3 1 note since highest priority task 1 cannot preempted fig 4 definition g jl assume know g jl values give worst case preemption scenario among tasks calculate worst case cacherelated preemption delay r k r k f jl lth marginal preemption cost j note total cacherelated preemption delay includes delay due preemptions higher priority tasks r k general however cannot determine exactly g jl combination give worst case preemption delay task analysis safe conservatively assume scenario guaranteed worse actual preemption scenario conservative scenario derived constraints valid g jl combination satisfy give number constraints g jl following first g jl given interval r k cannot larger number invocations j interval thus formulation n j maximum number preemptions single invocation experience r k upper bound n j value calculated a1 r j a1 r k worst case response times higher priority tasks available worst case response time computed index l g jl bounded n j formulation second number preemptions task j given interval r k cannot larger total number invocations interval since arrivals tasks priorities higher j preempt j thus generally total number preemptions given interval r k cannot larger total number invocations interval thus na note constraint subsumes previous constraint maximum value pc r k satisfying constraints safe upper bound total cacherelated preemption delay task r k problem formulated integer linear programming problem follows maximize r k subject constraint 1 constraint 2 na iteration iterative procedure explained section iii integer linear programming problem solved compute pc r k application iterative procedure given appendix vi experimental results assess effectiveness proposed approach predicted worst case response times tasks sample task sets using proposed technique compared predicted using previous approaches validation purposes predicted worst case response times also compared measured response times target machine idt7rs383 board 20 mhz r3000 risc cpu r3010 fpa floating point accelerator instruction cache data cache 16 kbytes caches direct mapped block sizes 4 bytes sram static ram used target machines main memory cache refill time 4 cycles although target machine timer chip provides userprogrammable timers resolution low measurement purposes accurately measure execution response times tasks built daughter board implements timer resolution one machine cycle experiments also implemented simple fixedpriority scheduler based tick scheduling explained 7 scheduler manages two queues run queue delay queue run queue maintains tasks ready run tasks ordered priorities delay queue maintains tasks waiting next periods tasks ordered release times scheduler invoked timer interrupts occur every 160000 machine cycles invoked scheduler scans delay queue tasks delay queue release times invocation time scheduler moved run queue one newly moved tasks higher priority currently running task scheduler performs context switch currently running task highest priority task task completes execution placed delay queue next highest priority task dispatched run queue take account overheads associated scheduler used analysis technique explained 7 technique scheduler overhead response time r given number scheduler invocations r number times scheduler moves task delay queue run queue r ffl c int time needed service timer interrupt 413 machine cycles exper iments set task period wcet instruction useful memory blocks cache blocks unit machine cycles unit blocks task set specifications ffl c ql time needed move first task delay queue run queue 142 machine cycles experiments ffl c qs time needed move additional task delay queue run queue 132 machine cycles experiments detailed explanation equation beyond scope paper interested readers referred 7 used three sample task sets experiments specifications given table first column table task set name second column lists tasks task set four different tasks used fft lud lms fir task fft performs fft inverse fft operations array 8 floating point numbers using cooleytukey algorithm 26 lud solves 10 simultaneous linear equations doolittles method lu decomposition 27 fir implements 35 point finite impulse response fir filter 28 generated signal finally lms 21 point adaptive fir filter filter coefficients updated input signal 28 firdata section mapped noncacheable area lms fft lud instruction cache memory scheduler scheduler fig 5 code placement task set 3 table also gives third fourth columns period wcet task task set respectively used measured execution times tasks wcets since tight prediction tasks wcets accurate estimation cacherelated preemption delay two orthogonal issues measured execution time task obtained executing task without preemption execution time includes time initializing task also time two context switches one context switch task task another task upon completion table also gives total number instruction memory blocks maximum number useful cache blocks task fifth sixth columns respectively experiments intentionally placed code tasks way caused conflicts among memory blocks different tasks although instruction cache target machine large enough hold code used tasks expect case typical largescale realtime systems figure 5 shows code placement task set 3 furthermore since consider preemption delay related instruction caching cf section vii disabled data caching mapping data 4449284 1365026 3113858 29600 3113778 29520 3104178 19920 3073229 unit machine cycles ii worst case response time predictions measured response times stack segments tasks noncacheable area table ii shows predicted worst case response time lowest priority task task set four different methods used predict worst case response time task method worst case preemption cost assumed cost completely refill cache c method explained 13 u method worst case preemption cost assumed cost completely reload code used preempted task finally p method proposed paper worst case preemption cost assumed cost reload maximum number useful cache blocks table worst case response time predictions four methods denoted ra rc ru r p respectively also denoted delta predicted worst case cacherelated preemption delay method difference worst case response time predictions method without cacherelated preemption costs results show proposed technique gives significantly tighter predictions cacherelated preemption delay previous approaches results fact unlike approaches proposed approach considers useful cache blocks computing cacherelated preemption costs one case task set 1 proposed technique gives prediction 60 tighter best previous approaches 1304 cycles vs 3392 cycles however still nontrivial difference r p measured response time difference results number sources first contrary pessimistic assumption useful cache blocks task replaced cache time task preempted time task resumes execution replaced preemption actual execution second many actual preemptions occurred execution points execution point maximum number useful cache blocks finally worst case preemption scenario assumed deriving upper bound cacherelated preemption delay linear programming technique occur actual execution another point note results cacherelated preemption delay ie delta occupies small portion worst case response time less 1 cases results following two reasons first wcets tasks unrealistically large experiments since disabled data caching diminished relative impact cacherelated preemption delay worst case response time second since target machine uses sram main memory cache refill time much smaller current computing systems ranges 8 cycles 100 cycles dram used main memory 11 dram used instead worst case cacherelated preemption delay would occupied much greater portion worst case response time furthermore since speed improvement processors much faster drams 11 expect worst case cacherelated preemption delay occupy increasingly large portion worst case response time future assess impact cacherelated preemption delay worst case response time typical setting predicted worst case response time task set 1 increase cache refill time enabling data caching figures 6a b show delta deltaw orst case response time respectively new setting results show cache refill time increases delta increases linearly four methods results wider gap cacherelated preemption delay predicted method p cache refill time200000a u cache refill time05 worst case response u b fig 6 cache refill time vs delta deltaw orst case response time methods cache refill time increases result task set deemed unschedulable methods c u cache refill time 40 190 210 cycles respectively hand task set schedulable p even cache refill time 300 cycles methods c u sudden jumps delta cache refill time 120 cycles jumps occur increase worst case response time due increased cache refill time causes additional invocations higher priority tasks results also show cache refill time increases cacherelated preemption delay takes proportionally larger percentage worst case response time result even method p cacherelated preemption delay takes 10 worst case response time cache refill time 100 cycles methods cache preemption delay takes much higher percentage worst case response time vii extensions set associative caches computing number useful cache blocks section iv considered simplest cache organization called directmapped cache organization memory block placed one cache block general cache organization called nway setassociative cache organization memory block placed one n blocks mapped set whose index given memory block number modulo number sets cache setassociative cache organization requires policy called replacement policy decides block replace make room new block among blocks mapped set least recently used lru policy replaces block referenced longest time typically used purpose following explain compute maximum number useful cache blocks setassociative caches assuming lru replacement policy according definition section iv set rmb c contains possible states cache block c execution point p case directmapped caches possible state corresponds memory block cache block c may execution point p interpretation state needs extended setassociative caches since indexed unit cache sets rather unit cache blocks state cache set nway setassociative cache defined vector 1 2 1 least recently referenced block n recently referenced block following formulate problem computing rmbs setassociative caches data flow analysis terms directmapped caches initially focus rmbs beginnings ends basic blocks define sets rmb c b rmb c b sets possible states cache set c beginning end basic block b respectively set gen c b contains state cache set c generated basic block b element n distinct memory blocks referenced basic block b mapped cache set c specifically either empty none memory blocks mapped cache set c referenced basic block b singleton set whose element vector gen c vector component gen c n b memory block whose last reference basic block b last reference cache set c b similarly component gen c memory block whose last reference b last reference c b excepting references memory block gen c n b general gen c memory block whose last reference b last reference c b excepting references memory blocks gen c example consider cache two sets assume following memory block references made cache set 0 basic block b according definition gen c b cache set four blocks ie 4way setassociative cache set gen c 0 b fm similarly cache set eight blocks ie 8way setassociative cache set gen c0 b fnull null null definition gen c b sets rmb c b rmb c b whose elements vectors n memory blocks related follows p predecessor b b 12 fgen c gen c gen c gen c b gen c b empty case directmapped caches rmbs cache set c points beginning end basic block b derived rmb c b memory block references within basic block assume basic block following sequence memory block references reference memory block mapped cache set c references processed sequentially starting c processings needed follows element rmb rmb c updated rmb since rmb j recently referenced memory block cache set updated rmb note rmb c needs updated reference reference affect states cache sets set lmb c setassociative caches contains possible reference sequences cache set c p reference sequence sufficient information determine block cache set c whether rereferenced replaced nway setassociative cache lru replacement policy information corresponds n distinct memory blocks referenced p reason set gen c b lmb problem defined either empty singleton set fgen c whose components first n distinct memory blocks referenced basic block b mapped cache set c specifically gen c 1 b memory block whose first reference b first reference c b gen c 2 b memory block whose first reference b first reference c b excepting references memory block gen c 1 b sets lmb c b lmb c b correspond sets possible reference sequences cache set c beginning end basic block b respectively related follows successor b fgen c gen c b fgen c gen c b fgen c gen c b gen c b empty lmbs beginnings ends basic blocks computed lmbs points within basic blocks computed analogous manner case rmbs sets rmb c p lmb c computed execution point p calculation maximum number useful blocks cache set c p straightforward element rmb lmb rmb c compute number cache hits would result references memory blocks lmb applied cache set state defined rmb pick element rmb yields largest number cache hits gives maximum number useful cache blocks cache set c p total number useful cache blocks p computed summing maximum numbers useful cache blocks cache sets cache information preemption cost table constructed case directmapped cache b data cache memory focused preemption costs resulting use instruction cache memory subsection explain extension proposed technique needed data cache memory unlike instruction references data references addresses fixed compiletime example references loadstore instruction used implement array access different addresses data references complicate direct application proposed technique data cache memory since technique requires addresses references basic block fixed references also complicate wcet analysis tasks wcet analysis techniques take conservative approach fortunately conservative approach greatly simplifies adaptation proposed technique data cache memory take extended timing schema approach 19 example following discussion wcet analysis based extended timing schema approach loadstore instruction references one memory block called dynamic loadstore instruction 29 two cache miss penalties assumed reference one cache miss penalty reference may miss cache may replace useful cache block analysis preemption costs resulting use data cache memory loadstore instruction dynamic references handled exactly way case instruction references since addresses cfg fixed also extended timing schema approach assumes references dynamic loadstore instruction miss cache cannot contribute useful cache blocks furthermore since approach conservatively assumes every one replaces useful cache block deriving wcet estimate completely ignore computing rmbs lmbs viii conclusion cache memory introduces unpredictable variation task execution time used realtime systems preemptions allowed among tasks proposed new schedulability analysis technique takes execution time variation account proposed technique proceeds two steps first step pertask analysis technique constructs task table called preemption cost table table gives given number preemptions upper bound cacherelated delay caused second step computes worst case response time task using linear programming technique takes input preemption cost table obtained first step experimental results showed proposed technique gives prediction worst case cacherelated preemption delay 60 tighter obtained previous approaches improved prediction accuracy results fact proposed technique considers useful cache blocks deriving worst case cacherelated preemption delay number extensions possible analysis technique explained paper example pertask preemption cost information made accurate pertask analysis section iv cache block considered useful useful least one path many paths however cannot taken simultaneously example figure shows example cache block c useful flow control 1 2 hand cache block c j useful flow control 2 1 two flows control compatible one two cache blocks useful one time nevertheless cache blocks considered useful data flow analysis order rectify problem preemption cost computed path basis initial attempt based idea described 30 another interesting extension proposed analysis technique consider intersection cache blocks used preempted task used higher priority tasks released former task preempted 12 13 purpose proposed technique augmented follows 1 perform data flow analysis explained section iv preempted task 2 count useful cache blocks mapped intersection cache blocks used preempted task used higher priority tasks released preemption although approach accurate approach explained paper requires large number analyses ie one analysis preemption instance currently working approximate technique similar approach trades accuracy low analysis complexity appendix consider task set consisting three tasks preemption cost tables given preemptions 1 cost preemptions 1 cost 6 5 4 4 3 3 2 note need preemption cost table highest priority task 1 since cannot preempted worst case response time lowest priority task 3 computed follows r 0 r 0 r 0 pc 3 r 0 computed solving following integer linear programming problem maximize r 0 subject constraint 1 e r 0t 3 e constraint 2 e e problem formulation use fact since task 1 highest priority task cannot preempted thus g also gives n maximum number preemptions single invocation task 2 experience computed dividing worst case response time 2 period task 1 worst case response time 2 equal 49 must computed beforehand thus available compute worst case response time task 3 gives 2 n 3 maximum number preemptions task 3 computed dividing r 0 3 periods tasks 1 2 cf equation 9 solving integer linear programming problem pc 3 r 0 gives r 1 3 value used next iteration compute r 2 3 obtained solving following integer linear programming problem maximize 34 theta f 34 subject constraint 1 e e constraint 2 e e solution integer linear programming problem gives pc 3 r 1 turn gives r 2 repeat procedure r 2 r 3 thus procedure converges r 2 safe upper bound worst case response time task 3 since worst case response time smaller task 3 deadline 400 task 3 schedulable even cacherelated preemption delay considered acknowledgments authors grateful jose v busquetsmataix helpful suggestions comments earlier version paper r results earliest deadline scheduling al gorithm finding response times realtime system rate monotonic scheduling algorithm exact characterization average case behavior scheduling algorithms multiprogramming hard realtime environment dynamic scheduling hard realtime tasks realtime threads extendible approach analyzing fixed priority hard realtime tasks effective analysis engineering realtime fixed priority schedulers impact ada runtime systems performance characteristics scheduling models accounting interrupt handling costs dynamic priority task systems engineering analysis fixed priority schedulers computer architecture quantitative approach cache issues realtime systems adding instruction cache effect schedulability analysis preemptive realtime systems linear nonlinear programming bounding worstcase instruction cache performance integrating timing analysis pipelining instruction caching worst case timing analysis risc processors r3000r3010 case study efficient microarchitecture modeling path analysis realtime software accurate worst case timing analysis technique risc pro cessors smart strategic memory allocation realtime cache design smart strategic memory allocation real time cache design using mips r3000 allocating smart cache segments schedulability softwarebased cache partitioning realtime applications high performance compilers parallel computing dftfft convolution algorithm theory elementary numerical analysis c algorithms realtime dsp efficient worst case timing analysis data caching calculating worst case preemption costs instruction cache tr ctr anupam datta sidharth choudhury anupam basu using randomized rounding satisfy timing constraints realtime preemptive tasks proceedings 2002 conference asia south pacific design automationvlsi design p705 january 0711 2002 yudong tan vincent j mooney iii wcrt analysis uniprocessor unified prioritized cache acm sigplan notices v40 n7 july 2005 kandemir g chen w zhang kolcu data space oriented scheduling embedded systems proceedings conference design automation test europe p10416 march 0307 hiroshi nakashima masahiro konishi takashi nakada accurate efficient simulationbased analysis worst case interruption delay proceedings 2006 international conference compilers architecture synthesis embedded systems october 2225 2006 seoul korea mahmut kandemir guilin chen localityaware process scheduling embedded mpsocs proceedings conference design automation test europe p870875 march 0711 2005 accounting cacherelated preemption delay dynamic priority schedulability analysis proceedings conference design automation test europe april 1620 2007 nice france hemendra singh negi tulika mitra abhik roychoudhury accurate estimation cacherelated preemption delay proceedings 1st ieeeacmifip international conference hardwaresoftware codesign system synthesis october 0103 2003 newport beach ca usa hiroyuki tomiyama nikil dutt program path analysis bound cacherelated preemption delay preemptive realtime systems proceedings eighth international workshop hardwaresoftware codesign p6771 may 2000 san diego california united states kadayif kandemir kolcu g chen localityconscious process scheduling embedded systems proceedings tenth international symposium hardwaresoftware codesign may 0608 2002 estes park colorado sheayun lee sang lyul min chong sang kim changgun lee minsuk lee cacheconscious limited preemptive scheduling realtime systems v17 n23 p257282 nov 1999 yudong tan vincent j mooney iii timing analysis preemptive multitasking realtime systems caches proceedings conference design automation test europe p21034 february 1620 2004 yudong tan vincent mooney timing analysis preemptive multitasking realtime systems caches acm transactions embedded computing systems tecs v6 n1 february 2007 jan staschulat rolf ernst scalable precision cache analysis preemptive scheduling acm sigplan notices v40 n7 july 2005 zhang chandra krintz adaptive code unloading resourceconstrained jvms acm sigplan notices v39 n7 july 2004 johan strner lars asplund measuring cache interference cost preemptive realtime systems acm sigplan notices v39 n7 july 2004 jan staschulat rolf ernst multiple process execution cache related preemption delay analysis proceedings 4th acm international conference embedded software september 2729 2004 pisa italy sungpack hong sungjoo yoo hoonsang jin kyumyung choi jeongtaek kong sookwan eo runtime distributionaware dynamic voltage scaling proceedings 2006 ieeeacm international conference computeraided design november 0509 2006 san jose california changgun lee kwangpo lee joosun hahn yangmin seo sang lyul min rhan ha seongsoo hong chang yun park minsuk lee chong sang kim bounding cacherelated preemption delay realtime systems ieee transactions software engineering v27 n9 p805826 september 2001 nikil dutt alex nicolau hiroyuki tomiyama ashok halambi new directions compiler technology embedded systems embedded tutorial proceedings 2001 conference asia south pacific design automation p409414 january 2001 yokohama japan