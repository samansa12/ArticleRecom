parallel variable distribution constrained optimization parallel variable distribution framework solving optimization problems pvd variables distributed among parallel processors processor primary responsibility updating block variables allowing remaining secondary variables change restricted fashion along easily computable directions constrained nonlinear programs convergence theory pvd algorithms previously available case convex feasible set additionally one either assume constraints blockseparable use exact projected gradient directions change secondary variables paper propose two new variants pvd constrained case without assuming convexity constraints assuming blockseparable structure show pvd subproblems solved inexactly solving quadratic programming approximations extends pvd nonconvex separable feasible sets provides constructive practical way solving parallel subproblems inseparable constraints assuming convexity develop pvd method based suitable approximate projected gradient directions approximation criterion based certain error bound result readily implementable using approximate directions may especially useful projection operation computationally expensive b ams subject classications 90c30 49d27 1 introduction motivation consider parallel algorithms solving constrained optimization problems min research rst author supported cnpq faperj second author supported part cnpq grant 300734956 pronexoptimization faperj c 2001 kluwer academic publishers printed netherlands sagastizabal solodov c nonempty closed set n f continuously dierentiable function approach consists partitioning problem variables x 2 n p blocks x distributing among p parallel processors note notation shall explicitly account possible rearranging variables parallel algorithms iteration usually consists two steps parallelization synchronization 2 consider moment unconstrained case corresponding typically synchronization step aims guaranteeing sucient decrease objective function parallel step produces candidate points candidate directions simultaneously solving certain subproblems p l dened subspace dimension smaller n way p subspaces span whole variable space n methods block jacobi 2 updated conjugate subspaces 10 coordinate descent 21 parallel gradient distribution 14 dene p l minimization problem lth block variables ie n l recently parallel variable distribution pvd algorithms introduced 6 studied extended 19 20 7 advocated subproblems l slightly higher dimensions n l lth subproblem addition associated primary optimization variables x l representing condensed form remaining n n l problem variables see algorithm 1 since remaining n n l variables allowed change restricted fashion subspace dimension p 1 additional computational burden solving enlarged subproblems big idea forgetmenot terms add extra degree freedom yielding algorithms better robustness faster convergence refer reader 6 22 12 numerical validation pvdtype methods note variables parallel subproblems completely xed especially important constrained case ie method simply fail shall return issue later 11 general pvd framework formalize notion primary secondary variables need introduce notation let l denote complement l index set number parallel processors given parallel variable distribution 3 direction 2 n shall call pvddirection denote l n l p 1 blockdiagonal matrix formed placing blocks chosen direction along block diagonal follows id id l 1 note denotes identity matrix appropriate dimension linear transformation l n l n l l l maps l l unconstrained case general transformations used give rise fairly broad parallel variable transformation framework discussed 7 however theory 7 appear extend constrained case focus present paper describe next basic pvd algorithm 6 19 20 algorithm 1 pvd start x 0 2 c choose pvd direction x check stopping criterion satised compute x i1 follows parallelization processor l 2 possibly solution l l l l l l l synchronization compute l2f1pg l choose new pvddirection repeat 4 sagastizabal solodov idea pvd algorithm balance reduction number variables p l allowing enough freedom change secondary variables due parallel subproblems better approximate original problem resolved note secondary variables change along chosen xed directions inclusion signicantly increase dimensionality parallel subproblems p l indeed number variables p l secondary variables excluded course context parallel computing reasonable assume p small relative n l synchronization step algorithm 1 may consist minimizing objective function ane hull subject feasibility points computed parallel p processors would require solving pdimensional problem small compared original one 11 convex program one alternatively dene x i1 convex combination candidate points prin ciple convergence purposes point objective function value least good smallest computed processors acceptable pvddirections typically easily computable feasible descent directions objective function f current iterate x eg quasinewton steepest descent directions unconstrained case algorithm 1 rather general framework rened specialized obtain implementablepractical versions respect two principal questions set parallel subproblems eg choose pvd directions solve parallel subproblem including criteria inexact resolution 11 unconstrained issues addressed 19 contains improved convergence results compared 6 including linear rate convergence results well useful generalizations algorithms inexact subproblem solution certain degree asynchronization obtained imposing natural restrictions sucient descent type pvd directions even general framework unconstrained case developed later 7 subproblems obtained via certain nondegenerate transformations original variable space transformations general need even distinction primary secondary variables however approach seem extend constrained case seems parallel variable distribution 5 also intuitively justiable transformations kind primarysecondary variable structure reasons present paper shall restrict consideration specic transformations form 12 11 constrained optimization problem many questions still open especially nonconvex feasible set c c convex blockseparable structure ie c cartesian product closed convex sets shown 6 every accumulation point pvd iterates satises rstorder necessary optimality conditions problem 11 stated case inseparable convex constraints pvd approach may fail conclusion supported counterexample reproduce strongly convex quadratic program unique global solution 1 consider point x 6 x observe therefore apply algorithm 1 using x starting point xing secondary variables pvd variant stay nonoptimal point thus failing solve original problem shows constrained case special care taken setting parallel subproblems general convex feasible set shown 20 using projected gradient direction dx x p c x rfx secondary variables job p c stands orthogonal projection map onto closed convex set c specically established setting dx would ensure convergence pvd methods problems general inseparable convex constraints criteria inexact subproblem solution also given 20 c polyhedral set computing projected gradient direction requires solving every synchronization step algorithm 1 single quadratic programming problem wealth fast reliable algorithms available 11 5 noted case nonlinear constraints task computing projected gradient directions considerably computationally expensive actually even ane case computing directions exactly accurately may turn rather wasteful especially far solution original problem therefore improvements necessary 6 sagastizabal solodov paper propose two new versions pvd nonlinearly constrained case rst one applies problems block separable nonconvex feasible sets based use sequential quadratic programming techniques sqp second proposal inseparable convex feasible sets introduce computable approximation criterion allows employ inexact projected gradient directions criterion based error bound result independent interest emphasize readily implementable preserves global convergence pvd methods based exact directions notation fairly standard usual inner product two vectors denoted hx yi associated norm given using norms shall specify explicitly analogous notation used subspaces dimensions example reduced subspaces n l sake simplicity sometimes use compact transposed notation referring composite vectors instance stands column vector l dierentiable function denote ndimensional column vector partial derivatives f point dierentiable vectorfunction c denote n jacobian matrix whose rows transposed gradients components c function h partial derivatives lipschitzcontinuous set x modulus l 0 2 nonconvex separable constraints suppose feasible set c 11 described system inequality constraints section assume c block separable structure specically proposal solve parallel subproblems p l algorithm 1 exactly making one step sequential quadratic programming method sqp 3 ch 13 since sqp methods local nature parallel variable distribution 7 modify synchronization step algorithm 1 introducing suitable linesearch based following exact penalty function 9 l positive parameter taken componentwise given x lth block rfx denoted l n l n l l algorithm following algorithm 2 start x 0 2 c choose parameters 0 2 0 1 positive denite n l n l matrices 0 x check stopping criterion satised compute x i1 follows parallelization processor l 2 l l kkt point h l l l c l l l l synchronization dene linesearch choose l l using merit function 25 nd smallest nonnegative integer l repeat proposal compared original pvd algorithm 6 two remarks order first algorithm 2 viewed pvd method inexact solution parallel subproblems indeed hard nonlinear pvd subproblems p l algorithm 1 approximated easy quadratic programming subproblems qp l secondly pvd approach extended case nonconvex 8 sagastizabal solodov constraints note inclusion forgetmenot terms seem crucial blockseparable case example terms specied analysis 6 thus drop secondary variables p l taking null pvddirections however use secondary variables deserves study feasible sets inseparable constraints also note algorithm 2 thought distributed parallel implementation sqp blockdiagonal matrix chosen generate quadratic approximations objective function remark contribution algorithm 2 meant primarily pvd framework rather general sqp methods algorithm 2 assume subproblems qp l nonempty feasible sets every iteration guaranteed example l jacobian c 0 l maps n l onto l c l convex alter natively known feasibility subproblems forced introducing extra slack variable 1 p 377 sometimes argued sqp methods convenient solving largescale n large nonlinear programs inequality constraints present precisely argued combinatorial aspect introduced complementarity condition kkt system associated qp makes subproblems resolution relatively costly hand sqp techniques known ecient small medium size problems often method choice setting relation algorithm 2 important note subproblem qp l quadratic programming problem relatively small dimension n l l presumed small compared n exist number fast reliable algorithms solving problems see eg 5 section 4 report numerical results algorithm 2 obtained via simulation serial computer reference state optimality conditions qp l pair l solves kkt system l l l l c l l l l l 0 h l l denoting 0 x usual directional derivative merit function x 2 n direction 2 n next state descent direction function point x turn imply linesearch procedure algorithm 2 welldened parallel variable distribution 9 lemma 1 f c 2 c 11 h l l l l dened algorithm 2 proof dening indexsets eg see 9 p 301 using 27b componentwise implying hence l l obtain rightmost inequality 28 proceed follows l l l l l l l l l l sagastizabal solodov second equality 27a fourth 27c inequality follows 27c fact c l l l combining latter relation 29 l l l l j 1 j l l j 1 l l l second inequality cauchyschwarz inequality last choice completes proof algorithm 2 globally convergent assume penalization parameter kept bounded essentially means multipliers l stay bounded practical point view latter assumption natural follows show convergence algorithm 2 karushkuhntucker kkt points 11 ie pairs x theorem 1 suppose f c 2 c 11 let feasible set c blockseparable structure given 24 let fx sequence generated algorithm 2 assume exists iteration matrices l uniformly positive denite bounded iteration indices either every accumulation point x sequence fx g kkt point problem ie satises 210 proof iteration index happens l l 27a27c reduce l l l c l l l 0 h parallel variable distribution 11 taking account separability constraints follows kkt system 210 suppose 211 hold 28 lemma direction descent merit function x standard argument linesearch procedure welldened terminates nitely stepsize 0 entire method welldened generates innite sequence iterates prove rst sequence stepsizes ft g bounded away 0 take 2 0 1 since f 2 c 11 similarly since c 2 c 11 using equivalence norms nitedimensional setting r 1 0 l l l l l l l l l l l j 1 c l l l l l l l l l l l l l 1 tc l l equality obtained adding subtracting tc l l second inequality follows jaj 1 ja used last inequality rewriting relation obtain l l l j 1 j l l l tj c l l l l l l l l second inequality convexity j equality 27b together 212 last inequality yields l l l j 1 l l 12 sagastizabal solodov xed constant depending r 1 direct comparison latter relation 26 conclude 26 guaranteed satised large enough within set satisfying particular since linesearch procedure accept stepsize follows either 28 lemma 1 uniform positive deniteness matrices l exists r 3 0 l using 213 conclude assumption 0 26 follows sequence f nonincreasing hence either unbounded converges latter case 26 implies since 0 holds denition uniform positive deniteness matrices l conclude l l passing onto limit 27a27c 1 taking account boundedness matrices l obtain assertions theorem 3 convex inseparable constraints suppose feasible set c dened system convex inequalities 23 convex components emphasize section assume separability constraints setting appears currently known way ensure convergence pvd algorithm use parallel variable distribution 13 projected gradient directions change secondary variables 20 omitting iteration indices x current iterate compute directions one solve subproblem following structure min done iterative algorithm already discussed section 1 solving problem general nonlinear case quite costly moreover far solution original problem exact even accurate projection directions perhaps unnec essary suggests developing stopping rule solving 314 equivalently approximation criterion projection directions used pvd scheme algorithmic purposes important make criterion constructive implementable assuming constraint qualication condition 13 z solves 314 ie pair z kkt system r z lz u 0 hu standard lagrangian problem 314 suppose z 2 c u current approximation primaldual optimal solution 314 generated iterative algorithm applied solve problem lemma 2 establishes error bound distance z p c x rfx terms violations kkt conditions 315 pair z u nice feature estimate unlike perhaps error bounds results literature see 18 survey involve expressions readily computable quantities observable furthermore error bound holds globally ie neighbourhood solution point thus easily employed algorithmic purposes lemma 2 related error bounds strongly convex programs obtained 15 however theorem 22 15 involves certain constants general computable corollary 24 15 assumes z primal feasible also z u dual feasible means u lemma 2 14 sagastizabal solodov assume z primal feasible approximate multiplier u nonnegative assumptions weaker also appear suitable algorithmic framework satised iteration many standard optimization methods dual feasibility contrast unlikely satised along iterations typical algorithms except limit lemma 2 let c convex dierentiable suppose set c given 23 satises constraint qualication z 2 c u holds z u 2 jr z lz uj 2 jr z lz uj 2 4hu czi 317 proof let u associated multiplier pair z u satises 315 denote l dened 316 take z 2 c u denoting ui jz inequality follows u 0 facts convexity c cz cz c 0 zz z 0 cz cz hand zi jr z lz ujjz equality follows kkt conditions 315 inequality follows facts cz 0 cauchyschwarz inequality denoting jz zj jr z lz uj 0 hu czi 0 combining 318 319 obtain following quadratic inequality parallel variable distribution 15 resolving inequality obtain recalling denitions quantities involved conclude jz zj 2 jr z lz uj 2 jr z lz uj 2 4hu propose following pvd algorithm based approximations projected gradient directions algorithm 3 choose parameters 1 2 0 1 2 2 0 1 start x 0 2 c set 0 x check stopping criterion satised proceed follows pvddirection choice compute z associated approximate lagrange multiplier u problem 314 satisfy compute parallelization processor l 2 compute solution l dened algorithm 1 synchronization compute l2f1pg l l repeat note algorithm 31 generalpurpose method problems special structure example presented introduction shows problems computing meaningful direction secondary variables indispensable compared alterna tive computing approximate projected gradient direction seems quite favorable terms cost perhaps even best one sagastizabal solodov regarding tolerance criterion 320 note z exact projection point z rstorder necessary optimality condition easy see projection problem 314 always inexact solutions satisfying 320 hence method welldened convergence properties algorithm 3 result following theorem 2 let c convex dierentiable l c suppose fx g sequence generated algorithm 3 either f unbounded c sequence ffx g converges every accumulation point sequence fx g satises rstorder necessary optimality condition proof consider iteration processor l 2 point l rst show point feasible corresponding subproblem p l minimizing function l l l l indeed using notation 12 l l l e l l l rst equality follows structure l inclusion facts x 1 convexity set c result l l l l l l l l l l e l last inequality follows f 2 c 11 a24 parallel variable distribution 17 construction algorithm lemma 2 hence exists 2 n dene continuous residual function notation properties projection operator eg see 1 proposition b11 since x 2 c hence combining latter relation 321 322 taking account synchronization step algorithm 3 obtain observe rst relation cauchyschwarz inequality second follows 322 last 320 hence using 323 latter inequality 320 choice assumptions parameters 1 2 sequence ffx g nonincreasing f bounded c bounded hence converges latter case sagastizabal solodov 324 hand conclude frx g also tends zero continuity r follows every accumulation point x sequence well known latter equivalent minimum principle necessary optimality condition xi 0 x 2 c 4 preliminary numerical experience get insight computational properties approach x 2 considered test problems taken study sphere packing problems 4 particular problems chosen ones used 16 access parallel computer carried simulation ie subproblems solved serially serial machine even though admittedly rather crude experiment nevertheless gives idea one might expect actual parallel implementation given p spheres x 2 p problems follows problem 1 min x 1t x j 1t st 1t problem 2 given integer 1 min st parallel variable distribution 19 problem 3 min st implemented matlab algorithm 2 serial sqp method general pvd algorithm 1 codes run matlab version 60088 release 12 sun ultrasparcstation details implementation follows algorithm 1 implemented using function constrm matlab optimization toolbox solving subproblems p l algorithm 2 serial sqp method quite sophisticated quasinewton implementations linesearch based merit function 25 qp l solved nullspace method 8 penalty parameter updated using modication mayne polak rule 17 allows decrease warranted stepsize computed armijo rule uses safeguarded quadratic interpolation order prevent maratos eect ie ensure unit stepsize asymptotically accepted possible secondorder correction added search direction necessary 3 ch 134 finally quasinewton matrices updated bfgs formula using also powell correction orenluenberger scaling methods stop relative error measured sum norm reduced gradient norm constraints less 10 5 rst compare algorithm 2 general pvd algorithm 1 since comparison turned rather obvious onesided certainly surprising report exhaustive testing part table ii report number iterations running times algorithms 1 2 problem 1 results problems follow trend yield insight note running times reported serial without regard parallel nature algorithms iterative structure two algorithms seems meaningful fair comparison already clear intuition solving exactly general nonlinear subproblems p l algorithm 1 costly unlikely yield competitive algorithm easily conrmed experience course one could use heuristic considerations somehow adjust dynamically tolerance solving subproblems experiment subproblems solved within 20 sagastizabal solodov table results algorithm 2 sqp method sphere packing prob lems starting points generated randomly coordinates 10 10 times solving qps measured seconds using intrinsic matlab function cputime problem p qp iter calls qp speedup iter calls 26 28 28 59 prob3 parallel variable distribution 21 table ii results algorithms 1 2 problem 1 starting points feasible generated randomly times measured seconds using intrinsic matlab function cputime algorithm 1 algorithm 2 name p iter time iter time tolerance 10 5 original problem however convergence analysis support strategy within algorithm 1 constrained case strictly speaking even convergence proof algorithm 1 case consideration since feasible set nonconvex discussed one motivations algorithm 2 precisely provide constructive implementable way solving subproblems p l inexactly solving quadratic approx imations results table ii conrm proposed approach certainly makes sense next set experiments concerns comparison algorithm 2 serial sqp easy come meaningful comparison serial method parallel method serial machine example overall running time parallel method obtained serial machine considered notoriously unreliable predict time would required parallel implementation ie dividing time number processors good measure therefore focus indicators feel still meaningful actual parallel implementation discussed evaluate gain computing search directions report total time spent solving quadratic programming subproblems within sqp within serial implementation algorithm 2 since communication processors would needed within phase expected time solving qps parallel implementation algorithm 2 indeed obtained dividing number processors somewhat surprising even smaller problems see table serial time solving qps algorithm 2 already considerably smaller standard sqp dierence 22 sagastizabal solodov becoming drastic larger problems approximate speedup eciency computing directions parallel time spent solving qps sqp time spent solving qps serial algorithm 2 100 eciencies vary acceptable around 80 high 10000 grow fast size problem conrms motivation discussed x 2 ie smaller qps signi cantly easier solve obviously possible price pay algorithm 2 deterioration directions compared good implementation full sqp algorithm issue addressed reporting numbers iterations calls oracle evaluating function derivatives values see table note numbers iterations functionderivatives evaluations usually slightly higher algorithm 2 always overall numbers two methods quite similar given impressive gain algorithm exhibits solving qp subproblems indicates parallel implementation algorithm indeed ecient least computing function derivatives cheap relative solving qps particular case largescale problems functions derivatives given explicitly 5 concluding remarks two new parallel constrained optimization algorithms based variables distribution pvd presented rst one consists parallel sequential quadratic programming approach case blockseparable constraints rst pvdtype method whose convergence established nonconvex feasible sets second proposed algorithm employs approximate projected gradient directions case general inseparable convex constraints use inexact directions particular relevance projection operation computationally costly r nonlinear programming parallel distributed computation sphere packings linear complementarity problem nonlinear programming tr iterative methods large convex quadratic programs survey spherepackings lattices groups parallel distributed computation numerical methods dual coordinate ascent methods nonstrictly convex minimization parallel gradient distribution unconstrained optimization new inexact parallel variable distribution algorithms bounds mathematical programming twophase model algorithm global convergence nonlinear programming testing parallel variable transformation parallel synchronous asynchronous spacedecomposition algorithms largescale minimization problems convergence constrained parallel variable distribution algorithms parallel variable transformation unconstrained optimization