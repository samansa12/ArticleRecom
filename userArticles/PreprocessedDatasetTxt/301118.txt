performance prediction large parallel applications using parallel simulations accurate simulation large parallel applications facilitated use direct execution parallel discrete event simulation paper describes use compass direct executiondriven parallel simulator performance prediction programs include communication io intensive applications simulator used predict performance applications distributed memory machines like ibm sp sharedmemory machines like sgi origin 2000 paper illustrates usefulness compass versatile performance prediction tool use realworld applications synthetic benchmarks study application scalability sensitivity communication latency interplay factors like communication pattern parallel file system caching application performance also show simulator accurate predictions also efficient ability use parallel simulation reduce execution time cases yielded nearlinear speedup b introduction accurate efficient performance prediction existing parallel applications multiple target architectures challenging problem analytical simulation approaches used successfully purpose whereas analytical solutions advantage efficiency also suffer limitation many complex systems analytically intractable although simulation widely applicable tool major limitation extremely long execution time largescale systems number simulators including parallel proteus 22 lapse 13 simos 30 wisconsin wind tunnel 28 mpisim 26 developed control execution time simulation models parallel programs simulators typically use direct execution reduce cost simulating sequential instructions use parallel discrete event simulation exploit parallelism within simulator reduce impact scaling target configuration simulated existing program simulators designed study cpuintensive parallel programs however inadequate parallel io performance become significant deterrent overall performance many applications number solutions proposed improve parallel io performance 10 29 ability include io cpuintensive applications unified performance prediction environment thus appears significant value developed compass componentbased parallel system simulator portable execution driven asynchronous parallel discrete event simulator used predict performance largescale parallel programs including computation io intensive applications targeted execution sharednothing shared memory architectures well smp clusters particular simulation modules developed predict performance applications function communication latency number available processors machine interest different caching strategies parallel i0 parallel file system characteristics alternative implementations collective communication i0 commands simulator used detailed program simulations within poems project 12 poems performance oriented endtoend modeling system collaborative multiinstitute project whose goal create experimentally evaluate problem solving environment endtoend performance modeling complex paralleldistributed systems paper describes simulator use evaluating performance large scale complex applications function various system characteristics demonstrate simulator accurate also fast due ability run parallel use real world applications however cases used synthetic benchmarks highlight particular feature simulator show simulators portability accuracy validating tool two platforms distributed memory ibm sp shared memory sgi origin 2000 range synthetic real world applications instance show predicted execution time asci kernel called sweep3d 32 within 5 measured execution time architectures second demonstrate scalability tool major impediment widespread use program simulators execution inefficiency show compass effectively exploit parallel model execution dramatically reduce execution time simulation model without sacrificing accuracy particular show configuration application kernel called sweep3d target machine 64 processors simulator reduces slowdown factor 35 using sequential simulation low 25 using parallel simulator running 64 processors larger amounts memory available parallel platform allowed us conduct scalability studies target configurations least two orders magnitude larger obtained sequential machine instance sweep3d application memory constraints sequential simulator would limited us simulating target architecture 13 processors 150 3 problem size using memory available us 128node sp able predict performance sweep3d 1600 processors established simulators accuracy scalability demonstrate capabilities 1 use simulator predict scalability properties application using standard measures scalability include isoefficiency scaleup function number processors 2 analyze behavior application function communication latency target architecture demonstrate applications sweep3d sensitive latency variations implying executing applications network workstations rather massively parallel system reasonable alternative 3 show compass model new architectures consisting clusters smps newest ibm sp even though hardware smp cluster exists mpi software yet available exploit faster communication available among processors smp node using compass show application would perform new architecture fast intranode mpi communications made available particular using synthetic benchmarks identify type application run 20 faster using four 4way smps rather sixteen processors 4 using synthetic benchmark demonstrate sensitivity different communication patterns variations communication latencies target architecture 5 parallel file systems becoming complex allowing compute ionode caching demonstrate various caching policies affect performance benchmark particular io intensive benchmark see network latency degrades gains cooperative caching 10 7 become negligible next section gives brief description simulator section 3 describes benchmarks target host architectures used performance study section 4 presents results validation scalability simulator section showcases features simulator described point 1 5above section 6 discusses related work concludes discussion future research directions 2 compass goal simulator enable simulation largescale parallel applications written using mpi mpiio variety high performance architectures application program simulated referred target program architecture performance predicted referred target architecture machine simulator executed referred host machine may sequential parallel simulation environment composed several distinct yet tightly coupled componentsthe simulation kernel mpi communication library simulator mpisim parallel io simulator piosim parallel file system simulator pfs sim successive component builds upon extends capabilities previous components expanding breadth depth performance issues may investigated simulator simulation kernel provides framework implements simulation protocols provides support scheduling execution threads mpisim provides capability simulate individual collective mpi communication routines piosim extends mpisims capabilities include io routines well providing several implementations collective io ability handle user defined data types needed support complex io operations simple io service time model pfssim completes simulation environment providing detailed simulation parallel file system multiple caching algorithms simulator portable runs variety parallel platformsthe ibm sp origin 2000 intel paragon simulation kernel heart simulation environment general number processors host machine less number processors target architecture simulated simulator must support multithreading kernel processor schedules threads ensures events processors executed correct timestamp order target thread simulated follows local code simulated direct execution 8 communication io commands trapped simulator uses appropriate model predict execution time corresponding activity target architecture corresponding communication io commands also executed consistency target program physical time taken executing operation ignored use direct execution simulation local code requires processors host target machines similar however interconnection network parallel io system file systems two architectures may different compass supports commonly used mpi communication routines pointtopoint collective communications simulator collective communication functions implemented terms pointtopoint communication functions pointtopoint communication functions implemented using set core nonblocking mpi functions 27 interconnection network model currently ignores contention network detailed models developed given excellent validation obtained simpler model variety benchmarks previous work 26 considered serious limitation parallel io component compass simulates individual collective io constructs provided mpiio constructs include creating opening closing deleting file data access readwrite operations local datatype constructor introduced part mpiio specification file system component compass simulates parallel file system used service io requests generated mpiio programs component selfcontained may replaced simple disk access model order speed simulation whenever detailed system model required however using detailed model allows study wide variety parallel file system configurations basic structure functionality file system component taken vesta parallel file system highly scalable experimental file system developed ibm 6 behavior physical disks simulated set disk models included simple models based seek time rotational latency data transfer rate well highly detailed model developed dartmouth 24 detailed system simulations slow parallel simulators potentially reduce execution time model provide greater amounts memory another necessity large detailed simulations simulation kernel provides support sequential parallel execution simulator parallel execution supported via set conservative parallel simulation protocols 26 combined kernels builtin multithreading capabilities allows simulator effectively use however many host processors available without limiting size type experiments may run simulator also supports number optimizations based analysis behavior parallel application among optimizations made available program behavior analysis technique allows simulation protocols described actually turned eliminating costly overhead global synchronization submission 3 benchmarks systems 31 real world application benchmarks 311 sweep3d sweep3d solver threedimensional time independent neutron particle transport computation calculates flux particles given region space flux region dependent flux neighboring cells threedimensional space xyz discretized threedimensional cells ijk computation progresses wavefront manner eight octants space octant containing six independent angles angles correspond six independent directions flux one face cubecell sweep3d uses 2d domain decomposition onto 2d array processors j directions configuration sweep progresses processor computes flux column cells sends outgoing flux information two neighboring processors order improve performance k dimension angles divided blocks allowing processor calculate part values dimension angles sending values neighboring processors 312 nas benchmarks nas parallel benchmarks npb suite parallel scientific benchmarks made available numerical aerodynamic simulation nas project nasa ames research center 2 nas suite contributes strong core experimental set represents number wellknown different realworld nonvendorspecific codes easily tailored utilize compass system used npb 22 release software included variety applications four bt lu mg sp deemed stable authors bt sp lu compute solutions systems discretized navierstokes equations mg solves threedimensional scalar poisson equation npb distribution provides preconfigured set problem sizes f77 constraint dynamic memory programs operate application problem sizes increasing order b c furthermore programs run parallel specific number processors bt sp run 4 9 16 processors lu mg run 4 8 16 npb suite sweep3d originally programmed fortran mpisim currently supports c programs first translated using f2c 14 subsequently translated code automatically localized allow simulator simulate multiple simultaneous threads target program single processor host machine localizer also converts mpi mpii0 calls equivalent calls defined within compass library localizer fully automated used successfully large applications 32 synthetic benchmarkssample although real world applications kernels like sweep3d nas useful benchmarks simulators compass major disadvantage core algorithms difficult understand impossible modify evaluate impact alternative types program structures including computation granularity communication patterns programs provided means parameter adjustment large granularity changes could made serve need measure performance function specific runtime behavior thus addition using real world benchmarks sought write synthetic application allows explicit tuning communication computation parameters effort resulted sample synthetic application messagepassing library environments c program performs precisely changeable amounts calculation messagepassing interprocess communications suitable experimental analysis sample executes message passing via calls targeted either compass actual mpi library facilitate validation sample simple loop contains two inner loops first pure computation loop whose duration varied adjusting number floating point divisions executed second communication loop implement multiple communication patterns changing frequency size destination messages sent received process message distribution take wide variety patterns described 17 using mpis pointtopoint capability implemented number methods wavefront nearest neighbor ring onetoall alltoall communications using predefined metrics user easily change communication computation ratio program 33 io benchmark since implementations mpii0 standard yet widely available hard find real world applications stress parallel io simulation capabilities simulator hence set synthetic benchmarks developed purpose benchmark uses n processes mapped unique compute node process generates read write requests blocks data given size interarrival times io requests sampled normal random distribution given mean blocks file distributed across io nodes disks total md disks process issues r requests given c first rc requests used warm caches parameters easily modified 34 host target architectures sgi origin 2000 19 multiprocessor system silicon graphics inc origin provides cachecoherent numa distributed shared memory layout two mips r10000 processors comprising processing node multiplexed hub chip reduce memory latency increase memory bandwidth origin testbed small ten 180 mhz r10000 processors sharing 320 mb memory due limited number processors memory could completely perform number size experiments ibm sp ibm scalable parallel sp system scalable multiprocessor condenses several complete rs6000 workstations one system 9 forming sharednothing collection processing nodes connected typically bidirectional 4ary 2fly multistage interconnection network achieve simultaneous anytoany connections 21 packetswitched network use alternative ip protocol named user space communication subsystem us css provide nearconstant latency bandwidth used us css baseline protocol experiments sp2 new generation ibm sp showcases cluster architecture nodes machine 4way smp example machine new ibm sp lawrence livermore national laboratory currently machine includes 158 compute nodes four 332 mhz 604e processors sharing 512 mb memory attached 1gb disks internode communications sp give bandwidth 100mbsecond latency 35 microseconds use sp high performance switch tb3 currently performance possible application running one four processors node simulated behavior system running mpi applications internode communications handled way sharednothing architecture modeling communications processors using highperformance switch however intranode communications modeled using shared memory information implementation mpi constructs designed exploit shared memory yet available ibm sp fact current implementation processors node communicate using much slower ip compass model based mpi implementation sgi origin 2000 22 certainly performance application depend exact implementation allows us demonstrate capability tool enabling studies 4 validation performance compass 41 validation first set experiments aimed validating predictions compass ibm sp sgi origin 2000 figure 1 graph execution time measured sweep3d program compared execution time predicted compass curves function number processors used sweep3d number target processors simulated compass validation thus limited number physical processors origin 10 compass data taken average running times multithreaded combinations target processor number instance eight target processors average running time taken executions 1 2 4 8 host processors graph seen compass indeed accurate correctly predicting execution time benchmark within 5 ibm sp 3 o2k even multithreaded operation conducting scalability studies often case number available host processors significantly less number target processors results several simulation threads running processor since multithreading might affect results simulation threads might affect others runtime important study whether effects exists quantify effect multithreading ability simulator correctly predict runtime application simulated sweep3d using wide range host target processors seen figure 2 even relatively high degree multithreading 8 target mpi processes single host processor variation predicted runtime small 2 runtime sweep3don ibmsp 503problemsize mk10 mmi320601001 2 4 8 processors runtime measuredruntime figure validation compass sweep3d ibm sp number processors time seconds measured predicted figure validation compass sweep3d o2k hostprocessors 2target 4target 8target 16target 64target 128target figure 2 effect compass multithreading predicted performance ibm sp compass also validated suite nas benchmarks present results sp bt benchmarks origin 2000 mentioned earlier nas programs come configured run parallel predetermined number processors predetermined set problem sizes processor memory constraints relatively small o2k restricted us size benchmarks figure 3 shows results validation experiments bt sp class show good validation accuracy within 85 21 respectively points 16 processors graph shows predicted performance since 8 host processors available machine since sweep3d nas benchmarks computationally intensive also used communicationintensive synthetic benchmark sample validate communication models measured predicted execution times sample benchmark also showed excellent validation compass variety configurations figure 4 shows sample run using wavefront communication pattern computationtocommunication ratio 11 1 seen figure compass accurately predicts running time within 3 percent results similar patterns omitted brevity validation nasspandbt o2k05154 9 number processors btmeas btpred spmeas sppred figure 3 validation nas benchmarks o2k validation sampleon o2k35452 4 8 number processors measured predicted figure 4 validation sample o2k 42 scalability simulator present number results demonstrate relative improvement performance simulator obtained parallel execution figure 5a shows performance compass simulating execution sweep3d problem sizes 50 3 100 3 cells 64 target processors ibm sp seen figure simulator effectively use additional processors parallel simulation 64 processors achieves speedup almost 35 100 3 problem size compared sequential execution time simulator speedup 50 3problem size smaller ultimately performance simulator bound performance application another metric commonly used evaluate performance simulator slowdown simulator relative target architecture define slowdownst time simulate application using host processors time execute application processors figure 5b shows slowdown compass simulating target problem size 50 3 number host processors equal number target processors simulator slowdown factor less 3 host architecture fewer available processors target machine slowdown get worse overall performance reasonable thus iratio number target processors number host processors 16 64 target processors 4 host processors slowdown factor 10 speedup compass running 64 target processor sweep3d mk10 mmi3 ibmsp5152535 processors 503 1003 figure 5 speedup compass ibm sp sweep3d slowdownof compass 503sweep3d mk10 host processors available figure slowdown compass ibm spsweep3d largest configuration studied 1600 target processors using 64 host processors iratio 25 yielded slowdown 18 considerably better slowdown factors reported program simulators like wwt 23 lapse 13 slowdown factors reported high 100 computationally intensive applications figure 6a show speedup attained compass origin 2000 simulates processors two problem sizes sweep3d origin 2000 compass achieves nearlinear speedup number host processors increased reaching speedup 7 8 host processors used slowdown graph 8target processor configuration shown figure 6b shows ratio 1 simulator slowdown 2 slowdown 4 host processors slightly 2 shows even half desired number processors available simulator runs twice slower application target processors would speedup compassrunningsweep3dwith32 target processors number host processors 503 1003 figure compass sgi origin 2000 sweep3d speedup slowdown compass simulating nas benchmarks also show improvements parallel execution albeit lesser degree figures 7a 8a show speedup bt sp applications respectively see speedup simulator increases progressively number host processors increased rate increase well final speedup attained 8 hosts lower seen previous benchmark simulator produces speedup high 545 bt benchmark 438 sp benchmark similarly slowdown curves reach low 142 167 respectively application see figures 7b 8b investigation indicated applications scale well sweep3d hence differences performance compass directly related performance target program simulated speedup slowdown experiments show compass exploit parallelism available application without adding considerable overhead number host processors 8target figure performance compass sgi origin 2000 sweep3d number host processors 4target 9target figure 7a speedup compass sgi origin 2000 bt number host processors 9 target figure 7b slowdown compass sgi origin 2k bt 5 results features compass scalability sweep3dthe performance study first evaluated scalability sweep3d function various parameters including size problem number processors function network latency performed study ibm sp using 64 host processors figure 9a demonstrates scalability sweep3d three problem sizes 50 3 100 3 150 3 large problems study showed performance scales well number processors increased almost 1600 although relative improvement performance drops beyond 256 processors largest problem size runtime application shown 125 times smaller running 1600 processors compared running application 4 processors smaller problem size elements performance appears peak 1024 processors subsequently gets worse observation strengthened isoefficiency analysis efficiency defined speedup sp number processors isoefficiency function determines rate problem size needs increased respect number processors maintain fixed efficiency 16 system highly scalable problem size needs increased linearly function number processors total work w time run algorithm single processor p time run algorithm p processors sum overhead processors giving efficiency needs grow fep maintain efficiency e fep defined isoefficiency function speedupof compassrunningsp135 number host processors 4target 9target figure compass origin 2k sp slowdown compassrunning sp261014 number processors 9 target figure slowdown compass origin 2k sp performanceof sweep3d mk10 mmi3 ibmsp101000 processors predicted runtime 503 1003 figure 9a scalability sweep3d ibm sp figure 9b shows isoefficiency function sweep3d various numbers efficiencies graph shows problem size needed maintain given efficiency 204060 90 given number processors first observe maintaining 90 even 60 efficiency hard however 40 manageable second using large number processors given problem size efficient example 500000 22221000 problem size using less 16 processors gives best efficiency 90 using 100 processors results 20 efficiency since running problem processors might result slow runtime tradeoff time efficiency made 36 processors used resulting 60 efficiency figure 9b also demonstrates isoefficiency hard capture simple extrapolation example 40 isoefficiency curves flattens 16 million problem size implying giving processors application improve efficiency isoeffiency functionmk1mmi650015000 100 200 300 400 500 processors problem size 20 40 90 figure isoefficiency sweep3d ibm sp 51 impact latency variation performance also studied effect communication latency performance figure 10 shows performance sweep3d latency network varied problem sizes 50 3 seen figure faster communication switch significant impact applicationthe performance changes 5 variations latency 0 10x current switch latency processors 128 larger problem difference negligible however performance appear suffer significantly latency increased factor 50 might case application ported network workstations latency impacts much significant small number processors processor contains larger portion computational region causing messages become large sensitive latency 52 modeling smp cluster architectures preceding experiments evaluated application performance distributed memory architecture new architectures ibm sp cluster architecture use 4way smp nodes described section 34 exploit fast memory access shared memory systems scalability distributed memory machines next set experiments projects improvements execution time benchmarks obtained migrating architecture since previous experiments showed nas sweep3d benchmarks relatively insensitive communication latency hardly surprising appear benefit noticeably fast intranode communication brevity omit results however demonstrated sample benchmark applications higher percentage communication new architecture appears offer benefits 503problemsize mk10 processors runtime seconds 0xsp sp 50xp figure 10a sensitivity sweep3d latency small problem size 1503problemsize mk10 mmi3 ibmsp2006001000 processors runtime seconds 0xsp sp 50xsp 100xsp figure 10b sensitivity sweep3d latency large problem size figure 11a shows performance sample fixed problem size per processor see simulator validates well one processor per node case meas non smp compass also notice predict slightly better performance running smp node would support fast intranode communications compass smp even though current implementation mpi communications smp nodes poor performance meas smp similarly figure 11b shows performance sp running sample function number computational iterations time communications 37 total runtime number iterations increases ratio computation communication constant see predicted smp performance improves average 20 compared single processor per node performance see clear drawbacks using intranode communications supported currently meas current smp even though mpi sp support fast intranode communications processors smp share main memory might tempt application developers redesign existing mpi application use main memory processors node mpi nodes simulator like compass help make decision investment time effort would result better performance numberofprocessors meas nonsmp meas current compass compassfor figure smp performance ibm sp sample constant computation per computational loops runtime seconds meas nonsmp meas current compass compass figure smp performance ibm sp sample increasing computation per processor 53 simulating common communication patterns another set experiments involved investigating impact different communication patterns program performance use synthetic benchmark sample scientific programs produce wide variety traffic patterns depending algorithm used sought understand different types message dispersal affected application performance sample benchmark used generate number messagepassing schemes study wavefront pattern involves 2dimensional mesh 0th processor residing upperlefthand corner initializing communication wave towards lowerrighthand corner using mesh layout nearestneighbor dispersal processor sending receiving message four logically adjacent processors ring pattern forms cycle single message token sent around logical ring processors finally onetoall pattern processor broadcasts message routed using broadcast tree others performance various communication patterns evaluated function communication latency number processors host machine selected experiments origin 2000 8 processors figure 12a shows performance sample function latency target o2k architecture processors figure 12b shows performance function number processors target architecture expected ring pattern sensitive latency processor count message traverses sequentially ring somewhat surprising result relative insensitivity wavefront ontoall communications however note patterns block initiator processor immediately initiating communication corresponding process executes next iteration hence reasonably well overlapped communication producing observed insensitivity slight jump predicted execution time increasing processors attributed change depth broadcast tree figure 12b sensitivity communication patterns latency100300500 latency xorigin nearest neighbor onetoall ring wavefront figure performance communication patterns function latency scalabilityof communicationpatterns901000 20 40 processors nearestneighbor onetoall ring wavefront figure performance communication patterns function number processors o2k 54 effect latency parallel file system caching strategies last experiment demonstrates use simulator evaluating impact architectural features io intensive programs cooperative caching techniques proposed improve performance applications large io requirements 710 suggesting caches least partially managed globally rather entirely local manner cases compute node cnodes io nodes ionodes caches base caching simply allows node manage cache greedy forwarding allows ionode cache miss check node caching required data going fetch disk centrally coordinated caching portions cnode caches collectively managed ionodes remaining portion cnode cache managed locally cnode percentage coordinately managed cache varied experiment globally managed caching similar 100 coordinate caching except strategy block placement caches modified allow ionode caches hold data evicted cnode caches caching techniques depend efficient access remote memory order improve cache hits rates application performance performance dependent communication latency network figure 13 shows results set experiments designed measure impact changing network latencies ibm sp cooperative caching techniques supported compass benchmark 16 processes separate compute nodes randomly read write 512 byte blocks data blocks file distributed across 2 io nodes 2 disks total 4 disks process issues 10000 requests first 5000 requests used warm caches 80 requests read requests graph plots predicted execution time benchmark network latency increased caching performance base caching cooperation greedy forwarding centrally coordinated 40 80 100 percent coordination globally managed caching shown network latencies 0 1 10 100 times latency interconnect absoluteperformanceof cachingtechniques2006000 50 100 networklatency xsp latency base greedy global figure comparison caching techniques ibm sp understandably network latency increased predicted execution time benchmark also increases however experiment also hints extreme sensitivity cooperative caching techniques increased network latency may appear caching techniques even base caching equally affected increasing network latency found case absolute difference predicted execution time diminishes slightly latency increased relative difference different caching techniques decreases markedly shown figure 13b effect network becomes slower benefit using cooperative caching lost performance degrades slightly better base caching result important implications use technique large networks workstations design hybrid strategies caches managed cooperatively small regions network rather entire network performance cooperative cachingrelative tobase network latency x sp latency base greedy global figure performance caching techniques relative base caching ibm sp 6 related work accurate efficient performance prediction existing parallel applications target machines thousands processors challenging problem first generation simulators like proteus 4 used sequential simulation slow slowdown factors ranging 2 35 process target program led many efforts improving execution time program simulators dpsim 25 lapse 13 parallel proteus 20 simos 30 wisconsin wind tunnel 28 tango 11 mpisim 2627 designed purpose simulators typically use direct execution portions code reduce cost simulating sequential instructions typically use variation conservative parallel discreteevent simulation 5 algorithm exploit parallelism within simulator reduce impact scaling target machine size many parallel simulators use synchronous approach simulation simulation processes synchronize globally fixed time intervals order maintain program correctness interval quantum taken larger communication latency network simulated guarantees message sent one quantum cannot received next interval also implies messages processed correct order synchronous simulators proteus parallel architecture simulation engine tango shared memory architecture simulation engine tunnel wwt shared memory architecture simulation engine simos complete system simulator multiple programs plus operating system terms simulation communications two simulation engines use approaches similar parallel proteus lapse distinguishing feature compass portable part due implemented use mpi since mpi readily available parallel distributed system simulator able use data movement synchronization hand lapse designed specifically run intel paragon using paragons native communication primitives made lapse broad usefulness limited compass also fast slowdowns around 2 proteus typical slowdowns range number simulators also designed simulate io operations although tended use sequential simulators set collective io implementations compared using starfish 18 simulator based proteus 3 hybrid methodology evaluating performance parallel io subsystems described pios tracedriven io simulator used calculate performance io system subset problem evaluated analytical model used remainder scalability distributed memory machines examined 31 used application kernels investigate network performance contention libraries also developed ppfs 15 portable parallel file system library designed sit top multiple ufs instances provide wide variety parallel file system capabilities caching prefetching data distribution compass environment described paper used parallel io system simulator detailed 1 perhaps simulator combines ability integrated interconnection network io file system scalability studies also used simulation data parallel programs compiled messagepassing codes 25 additionally simulator highly scalable slowdown factors single digits large target applications architectures 7 conclusions future research demonstrated compass used study wide range applications function variety architectural characteristics ranging standard scalability studies network stress test parallel io properties shown compass accurate validated multiple applications architectures within percent physical measurements also fast achieving excellent performance ibm sp well sgi origin 2000 achieves nearlinear speedups highly parallel applications suffers moderate slowdowns shown useful wide range architectural performance studies combine separate areas io parallel file system performance interconnection network communication library simulators compass used detailed program simulations within poems project collaboration poets working developing hybrid performance models combine analytical simulation modeling techniques also part project compass integrated detailed memory processor model allow us break away dependency requiring host processor architecture similar target processor architecture direct execution simulation also provide opportunity extend use parallel simulation techniques processor memory simulations 8 acknowledgments work supported advanced research projects agency darpacsto contract f3060294c0273 scalable systems software measurement evaluation darpaito contract n6600197c8533 endtoend performance modeling large heterogeneous adaptive paralleldistributed computercommunication systems thanks office academic computing ucla paul hoffman help ibm sp2 well lawrence livermore national laboratory use ibm sp many experiments executed 9 r parallel simulation parallel file systems io programs nas parallel benchmarks 20 methodology evaluating parallel io performance massively parallel processors proteus highperformance parallel architecture simulator distributed simulation case study design verification distributed programs vesta parallel file system avoiding cachecoherence problem paralleldistributed file system rice parallel processing testbed parallel computer architecture hardwaresoftware approach remote client memory improve file system performance multiprocessor simulation tracing using tango poems endtoend performance design large parallel adaptive computational systems parallel direct execution simulation messagepassing parallel programs f2c fortran c converter ppfs high performance portable parallel file system analysis scalability parallel algorithms architectures survey introduction parallel computing design analysis algorithms tuning starfish sgi origin ccnuma highly scalable server asci bluepacific ibm rs6000 tr system lawrence livermore national laboratory reducing synchronization overhead parallel simulation mpi performance study sgi origin 2000 wisconsin wind tunnel ii fast portable parallel architecture simulator galley parallel file system parallel simulation data parallel programs performance prediction parallel programs using parallel simulation evaluate mpi programs wisconsin wind tunnel virtual prototyping parallel computers improved parallel io via twophase runtime access strategy using simos machine simulator study complex computer systems simulation based scalability study parallel systems asci sweep3d benchmark code tr rice parallel processing testbed analysis scalability parallel algorithms architectures introduction parallel computing wisconsin wind tunnel simulationbased scalability study parallel systems ppfs vesta parallel file system galley parallel file system reducing synchronization overhead parallel simulation parallelized direct execution simulation messagepassing parallel programs using simos machine simulator study complex computer systems sgi origin poems mpisim parallel simulation parallel file systems io programs parallel computer architecture avoiding cachecoherence problem paralleldistributed file system parallel simulation data parallel programs tuning starfish proteus highperformance parallelarchitecture simulator performance prediction parallel programs ctr sundeep prakash ewa deelman rajive bagrodia asynchronous parallel simulation parallel programs ieee transactions software engineering v26 n5 p385400 may 2000 clia l kawabata regina h c santana marcos j santana sarita bruschi kalinka r l j castelo branco performance evaluation cmb protocol proceedings 37th conference winter simulation december 0306 2006 monterey california rajive bagrodia ewa deelman thomas phan parallel simulation largescale parallel applications international journal high performance computing applications v15 n1 p312 february 2001 leo yang xiaosong frank mueller crossplatform performance prediction parallel applications using partial execution proceedings 2005 acmieee conference supercomputing p40 november 1218 2005 thomas phan rajive bagrodia optimistic simulation parallel messagepassing applications proceedings fifteenth workshop parallel distributed simulation p173181 may 1518 2001 lake arrowhead california united states vikram adve rajive bagrodia ewa deelman thomas phan rizos sakellariou compilersupported simulation highly scalable parallel applications proceedings 1999 acmieee conference supercomputing cdrom p1es november 1419 1999 portland oregon united states vikram adve rizos sakellariou application representations multiparadigm performance modeling largescale parallel scientific codes international journal high performance computing applications v14 n4 p304316 november 2000 ewa deelman rajive bagrodia rizos sakellariou vikram adve improving lookahead parallel discrete event simulations largescale applications using compiler analysis proceedings fifteenth workshop parallel distributed simulation p513 may 1518 2001 lake arrowhead california united states vikram adve rajive bagrodia ewa deelman rizos sakellariou compileroptimized simulation largescale applications high performance architectures journal parallel distributed computing v62 n3 p393426 march 2002 ihsin chung jeffrey k hollingsworth using information prior runs improve automated tuning systems proceedings 2004 acmieee conference supercomputing p30 november 0612 2004 vikram adve rajive bagrodia james c browne ewa deelman aditya dube elias n houstis john r rice rizos sakellariou david j sundaramstukel patricia j teller mary k vernon poems endtoend performance design large parallel adaptive computational systems ieee transactions software engineering v26 n11 p10271048 november 2000 murali k nethi james h aylor mixed level modelling simulation large scale hwsw systems high performance scientific engineering computing hardwaresoftware support kluwer academic publishers norwell 2004