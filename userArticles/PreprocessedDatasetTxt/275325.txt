compiler blockability dense matrix factorizations goal lapack project provide efficient portable software dense numerical linear algebra computations recasting many fundamental dense matrix computations terms calls efficient implementation blas basic linear algebra subprograms lapack project large part achieved goal unfortunately efficient implementation blas results often machinespecific code portable across multiple architectures without significant loss performance significant effort reoptimize article examines wheter hand optimizations performed matrix factorization codes unnecessary performed compiler believe better programmer express algorithms machineindependent form allow compiler handle machinedependent details gives algorithms portability across architectures removes errorprone expensive tedious process hand optimization although currently exist production compilers perform loop transformations discussed article description current research compiler technology provided prove beneficial numerical linear algebra community show cholesky optimized automaticlaly compiler efficient handoptimized version found lapack also show qr factorization may optimized compiler perform comparably handoptimized lapack version modest matrix sizes approach allows us conclude advent compiler optimizations dicussed article matrix factorizations may efficiently implemented blasless form b introduction processing power microprocessors supercomputers increased dramatically continues time demand memory system computer increase dramatically size due cost restrictions typical workstations cannot use memory chips latency bandwidth required todays processors instead main memory constructed cheaper slower technology resulting delays may hundreds cycles single memory access alleviate memory speed problem machine architects construct hierarchy memory highest level registers smallest fastest lower level larger research supported nsf grant ccr9120008 nsf grant ccr9409341 second author also supported us department energy contracts defg0f91er25103 w31109eng38 department computer science michigan technological university houghton mi 49931 carrcsmtuedu z mathematics computer science division argonne national laboratory argonne il 60439 lehoucqmcsanlgov httpwwwmcsanlgovhomelehoucqindexhtml slower bottom hierarchy purposes main memory typically one two levels cache memory fall registers main memory cache memory faster main memory often fraction size cache memory serves buffer recently accessed data program working set cache becomes ineffective working set program larger size three factorizations considered paper lu cholesky qr among frequently used numerical linear algebra applications first two used solving linear systems equations last typically used linear least squares problems square matrices order n three factorizations involve order n 3 floating point operations data needs n 2 memory locations advent vector parallel supercomputers efficiency factorizations seen depend dramatically upon algorithmic form chosen implementation 16 18 32 studies concluded managing memory hierarchy single important factor governing efficiency software implementation computing factorization motivation lapack 2 project recast algorithms eispack 35 linpack 14 software libraries block ones block form algorithm restructures algorithm terms matrix operations attempt minimize amount data moved within memory hierarchy keeping arithmetic units machine occupied lapack blocks many dense matrix algorithms restructuring use level 2 3 blas 11 12 motivation basic linear algebra subprograms blas 29 provide set commonly used vector operations programmer could invoke subprograms instead writing code directly level 2 3 blas followed matrixvector matrixmatrix operations respectively often necessary high efficiency across broad range high performance computers higher level blas better utilize underlying memory hierarchy level 1 blas responsibility optimizing higher level blas left machine vendor another interested party study investigates whether compiler ability block matrix factorizations although compiler transformation techniques may applied directly blas interesting draw comparison applying directly factorizations benefit possibility blasless linear algebra package nearly efficient lapack example 30 demonstrated computers best lu factorization inlined approach even highly optimized set blas available deem algorithm blockable compiler automatically derive efficient block algorithm study one found lapack corresponding machineindependent point algorithm particular show lu cholesky factorizations blockable algo rithms unfortunately qr factorization householder transformations blockable ever show alternative block algorithm qr derived using compiler methods used lu cholesky factorizations study yielded two major results first detailed another paper 9 reveals hand loop unrolling performed optimizing level 2 3 blas 11 12 often unnecessary blas useful hand optimization required obtain good performance particular architecture may left compiler experiments show cases compiler automatically unroll loops effectively hand optimization second result discuss paper reveals possible block matrix factorizations automatically results show block algorithms derived compiler competitive lapack 2 modest sized matrices order 200 less compilerderived variants often superior begin presentation review background material related compiler optimiza tion describe study application compiler analysis derive three block algorithms lapack considered corresponding point algorithms present experiment comparing performance handoptimized lapack algorithms compiler derived algorithms attained using techniques also briefly discuss related approaches finally summarize results provide draw general conclusions background transformations use create block versions matrix factorizations corresponding point versions well known mathematical software community 15 section introduces fundamental tools compiler needs perform transformations automatically compiler optimizes point versions matrix factorizations analysis array access patterns rather linear algebra 21 dependence vectorizing parallelizing compilers dependence critical compiler tool performing transformations improve memory performance loops dependence necessary determining legality compiler transformations create blocked versions matrix factorizations giving partial order statements within loop nest dependence exists two statements exists control flow path first statement second statements reference memory location 26 ffl first statement writes location second reads true dependence also called flow dependence ffl first statement reads location second writes antide pendence ffl statements write location output dependence ffl statements read location input dependence dependence carried loop references source sink beginning end dependence different iterations loop dependence carried outer loop 1 loop true dependence aij ai1j carried iloop true dependence aij aij1 carried jloop input dependence aij1 ai1j carried iloop enhance dependence information section analysis used describe portion array accessed particular reference set references 5 21 sections describe common substructures arrays elements rows columns diagonals example section analysis consider following loop declared 100 theta 100 section accessed loop would shown shaded portion figure 1 matrix factorization codes require us enhance basic dependence information portion matrix involved block update compiler uses section analysis reveal portion matrix block updated section 311 discusses detail10100 figure 1 section 3 automatic blocking dense matrix factorizations section show derive block algorithms lu cholesky factorizations using current compiler technology section analysis enhance dependence information also show qr factorization householder transformations blockable ever present performancecompetitive version qr factorization derivable compiler 31 lu factorization lu decomposition factors nonsingular matrix product two matrices l u lu 20 l unit lower triangular matrix u upper triangular matrix factorization obtained multiplying matrix series elementary lower pivot matrices used make lu factorization numerically stable process first examine blockability lu factorization since pivoting creates difficulties first show block lu factorization without pivoting show handle pivoting 311 pivoting consider following algorithm lu factorization point algorithm referred unblocked rightlooking 13 algorithm exhibits poor cache performance large matrices transform point algorithm block algorithm compiler must perform stripmineandinterchange kloop 38 36 transformation used create block update apply transformation first strip kloop fixed size sections size dependent upon target architectures cache characteristics beyond scope paper 28 10 shown ks machinedependent strip size related cache size complete transformation kkloop must distributed around loop surrounds statement 20 around loop nest surrounds statement 10 interchanged innermost position loop surrounding statement 10 37 distribution yields unfortunately loop longer correct loop scales number values updates dependence analysis allows compiler detect avoid change semantics recognizing dependence cycle aikk statement 20 aij statement 10 carried kkloop using basic dependence analysis appears compiler would prevented blocking lu factorization due cycle however enhancing dependence analysis section information reveals cycle exists portion data accessed statements figure 2 shows sections array accessed entire execution kkloop section accessed aikk statement 20 subset section accessed aij statement 10 since recurrence exists portion iteration space loop surrounding statement 10 split jloop two loops one loop iterating portion dependence cycle exists one loop iterating portion cycle exist using transformation called indexset splitting 38 j split point create two loops shown figure 2 sections lu factorization dependence cycle exists statements 20 30 statement 10 longer cycle stripmineandinterchange continued distributing kkloop around two new loops shown finish stripmineandinterchange need move kkloop innermost position nest surrounding statement 10 however lower bound iloop contains reference kk creates triangular iteration space shown figure 3 interchange kk loops intersection line ikk1 iteration space point kk1 must handled therefore interchanging loops requires kkloop iterate trapezoidal region upper bound i1 i1 kks1 see wolfe carr kennedy details transforming nonrectangular loop nests 38 8 gives following loop nest kk figure iterations space lu factorization point rightlooking 13 block algorithm obtained therefore lu factorization blockable loop nest surrounding statement 10 matrixmatrix multiply optimized depending upon architecture superscalar architectures whose performance bound cache outer loop unrolling nonrectangular loops applied j iloops improve performance 8 9 vector architectures different loop optimization strategy may beneficial 1 many transformations used obtain block version lu factorization well known compiler community exist many commercial compilers eg hp dec sgi one contributions study compiler research show addition section analysis allows compiler block matrix factorizations note none aforementioned compilers uses section analysis purpose 312 adding partial pivoting although compiler discover potential blocking lu decomposition without pivoting using indexset splitting section analysis cannot said partial pivoting added see figure 4 lu decomposition partial pivoting partial pivoting algorithm new recurrence exists fit form handled indexset splitting consider following sections code applying indexset splitting algorithm figure 4 reference aimaxj statement 25 reference aij statement 10 access sections distributing kkloop around jloops would convert true dependence aij aimaxj antidependence reverse direction rules preservation data dependence prohibit reversing dependence direction would seem preclude existence block analogue similar nonpivoting case however block algorithm ignores preventing recurrence distributes kkloop still mathematically derived 15 consider following gammam 1 result shows postpone application eliminator 1 application permutation matrix p 2 also permute rows eliminator extending equation 1 entire formulation implementation block algorithm p cannot computed step point algorithm p depends upon first columns allowing computation k p blocking factor block application c pick pivot imax figure 4 lu decomposition partial pivoting install result compiler examine implications data dependence viewpoint point version row interchange followed wholecolumn update row element updated independently block version multiple row interchanges may occur particular column updated computations column updates performed point block versions computations may occur different locations rows array key concept compiler understand row interchanges wholecolumn updates commutative operations data dependence alone sufficient understand data dependence relation maps values memory locations reveals sequence values pass particular location block version lu decomposition sequence values pass location different point version although final values identical unless compiler understands row interchanges column updates commute lu decomposition partial pivoting blockable fortunately compiler equipped understand operations whole columns commutable row permutations upgrade compiler one would install pattern matching recognize row permutations wholecolumn updates prove recurrence involving statements 10 25 indexset split code could ignored forms pattern matching already done commercially available compilers vectorizing compilers pattern match specialized computations searching vectors particular conditions 31 preprocessors pattern match recognize matrix multiplication turn output predetermined solution optimal particular machine reasonable believe pivoting recognized implemented commercial compilers importance emphasized 32 cholesky factorization matrix symmetric positive definite lu factorization may written diagonal matrix consisting main diagonal u decomposition product triangular matrix transpose called cholesky factorization thus need work lower triangular half essentially dependence analysis applies lu factorization without pivoting may used note respect floating point computation cholesky factorization differs lu two regards first n square roots cholesky second lower half matrix needs updated strip mined version cholesky factorization shown case lu factorization recurrence aij statement 10 aikk statement 20 carried kkloop data access patterns cholesky factorization identical lu factorization see figure 2 indexset splitting applied jloop kks1 allow kkloop distributed achieving lapack block algorithm 33 qr factorization section examine blockability qr factorization first show algorithm lapack blockable give alternate algorithm blockable 331 lapack version lapack point algorithm computing qr factorization consists forming sequence 1 initial matrix rows n columns study assume n elementary reflectors update k order first k columns k1 form upper triangular matrix update accomplished performing matrix vector multiplication w followed rank one update efficiency implementation level 2 blas subroutines determines rate factorization computed detailed discussion qr factorization see book golub van loan 20 lapack block qr factorization attempt recast algorithm terms calls level 3 blas 15 level 3 blas handtuned particular architecture block qr algorithm may perform significantly better point version large matrix sizes cause working set much larger cache size unfortunately block qr algorithm lapack automatically derivable compiler block application number elementary reflectors involves computation storage exist original point algorithm 15 block number eliminators together following computed compiler cannot derive gamma v tv original point algorithm using dependence infor mation illustrate consider block two elementary reflectors computation matrix part original algorithm hence lapack version block qr factorization different algorithm point version rather reshaping point algorithm better performance compiler reshape algorithms cannot derive new algorithms data dependence information case compiler would need understand linear algebra derive block algorithm next section compilerderivable block algorithm qr factorization presented algorithm gives comparable performance lapack version small matrices retaining machine independence 332 compilerderivable qr factorization consider application j matrices v k k compiler derivable algorithm henceforth called cdqr forms columns k k kj updates remainder matrix j elementary reflectors final update trailing columns rich floating point operations compiler organizes best suit underlying hardware code optimization techniques stripmine andinterchange unrollandjam left compiler derived algorithm depends upon compiler efficiency contrast lapack algorithm depends hand optimization blas cdqr obtained point algorithm qr decomposition using array section analysis reference segments code point algorithm strip mining outer loop shown figure 5 complete transformation code figure 5 obtain cdqr iloop must distributed around loop surrounds computation v around update interchanged jloop however recurrence definition use akj within update section definition use aji computation recurrence carried iloop appears prevent distribution generate elementary reflector vi enddo update aimi1n vi enddo enddo enddo enddo enddo figure 5 stripmined point qr decomposition ii ii figure 6 regions accessed qr decomposition figure 6 shows sections array accessed entire execution iloop sections accessed aji akj examined legal partial distribution iloop revealed note similarity lu cholesky factorization section accessed aji black region subset section accessed akj black gray regions indexset j split point create new loop executes iteration space memory locations accessed akj disjoint accessed aji new loop iterates disjoint region optimized compiler depending upon target architecture 333 comparison two qr factorizations algorithm cdqr exhibit much cache reuse lapack version large matrices reason lapack algorithm able take advantage level 3 blas routine dgemm highly optimized cdqr uses operations closer level 2 blas worse cache reuse characteristics therefore would expect lapack algorithm perform better larger matrices could possibly take advantage highly tuned matrixmatrix multiply kernel 34 summary transformations summary table 1 lists analyses transformations must used compiler block matrix factorizations items 1 2 discussed section 2 items 3 7 discussed section 31 item 8 discussed compiler literature 28 10 item 9 discussed section 312 many commercial compilers eg ibm34 hp dec sgi contain items 1 3 4 5 6 7 8 however noted items 2 9 likely found todays commercial compilers table 1 summary compiler transformations necessary block matrix factorizations dependence analysis section 21 26 19 array section analysis section 21 5 21 3 stripmineandinterchange section 31 38 36 4 unrollandjam section 31 9 measured performance block factorization algorithm four different architectures ibm power2 model 590 hp model 71280 dec alpha 21164 sgi model indigo2 mips r4400 table 2 summarizes characteristics machine architectures chosen representative typical highperformance workstation table machine characteristics machine clock speed peak mflops cache size associativity line size dec alpha 250mhz 500 8kb 1 machines used vendors optimized blas example ibm power2 sgi indigo2 linked libraries lessl engineering scientific subroutine library 22 lblas respectively compileroptimized versions obtained hand using algorithms literature reason process could fully automated current deficiency dependence analyzer tool 4 6 table 3 lists fortran compiler flags used compile factorizations table 3 fortran compiler switches machine compiler flags hp 712 f77 v916 dec alpha f77 v38 o5 sgi indigo2 f77 v53 o3 mips2 tables 46 performance reported double precision megaflops number floating point operations lu qr cholesky factorizations 23n 3 respectively n number rows columns respectively used lapack subroutines dgetrfdgeqrf dpotrf lu qr cholesky factorizations respectively factorization routine run block sizes 1 2 4 8 16 24 32 48 64 1 table columns interpreted follows lablk best blocking factor lapack algorithm lamf best megaflop rate lapack algorithm corresponding lablk cblk best blocking factor compilerderived algorithm cmf best megaflop rate compilerderived algorithm corresponding cblk order explicitly set block size lapack factorizations modified lapack integer function ilaenv include common block benchmarks run computer systems free computationally intensive jobs benchmarks typically run two times differences time within 5 41 lu factorization table 4 shows performance compilerderived version lu factorization pivoting versus lapack version table 4 lu performance ibm hp dec sgi size lablk lamf cblk cmf speedup labk lamf cblk cmf speedup 100x100 200x200 300x300 dec alpha sgi indigo2 size lablk lamf cblk cmf speedup labk lamf cblk cmf speedup 100x100 200x200 300x300 500x500 1 although compiler effectively choose blocking factors automatically implementation available algorithms 28 10 ibm power2 results show size matrix increases 100 compiler derived algorithms edge lapack diminishes remaining matrix sizes compiler derived algorithm stays within 7 lapack one clearly fortran compiler ibm power2 able nearly achieve performance hand optimized blas available essl library block matrix factorizations hp 712 table 4 indicates unexpected trend compilerderived version performs better matrix sizes except 50x50 dramatic improvements matrix size increases indicates handoptimized dgemm efficiently use cache optimized cache performance compiler derived algorithm evident size matrices exceeds size cache significant performance degradation 50x50 case interesting matrix small cache performance factor believe performance difference comes way code generated superscalar architectures like hp code generation scheme called software pipelining used generate highly parallel code 27 33 however software pipelining requires lot registers successful code performed unrollandjam improve cache performance however unrollandjam significantly increase register pressure cause software pipelining fail 7 version lu decomposition hp compiler diagnostics reveal software pipelining failed main computational loop due high register pressure given handoptimized version highly software pipelined result would highly parallel handoptimized loop notasparallel compilerderived loop matrix size 25x25 enough loop iterations expose difference matrix size 50x50 difference significant matrix sizes 75x75 greater cache performance becomes factor time known compiler algorithms deal tradeoffs unrollandjam software pipelining important area future research dec alpha table 4 shows algorithm performs well better lapack version matrices order 100 less size 100x100 secondlevel cache alpha 96k begins overflow compilerderived version blocked multiple levels cache lapack version blocked 2 levels cache 25 thus compiler derived algorithm suffers many cache misses level2 cache lapack version possible compiler perform extra blocking multiple levels cache know compiler currently additionally blas algorithm utilized following architectural features 25 ffl use temporary arrays eliminate conflicts level1 directmapped cache translation lookaside buffer 28 10 ffl use memoryprefetch feature alpha hide latency cache memory although optimizations could done dec product compiler optimization would give additional performance algorithm using temporary buffer may provide small improvement prefetching provide significant performance improvement latency main memory order 50 cycles prefetches cannot issued source code unable try optimization results sgi roughly similar dec alpha difficult us determine exactly performance lower smaller matrices diagnostic tools could software pipelining architectural feature aware note code generated sgi compiler worse expected additionally 2level cache comes play larger matrices comparing results ibm power2 multilevel cache hierarchy systems dec sgi shows compilerderived versions effective singlelevel cache evident work needs done optimizing update portion factorizations obtain relative performance singlelevel cache system multilevel cache system 42 cholesky factorization table 5 shows performance compilerderived version cholesky factorization versus version ibm power2 results show size matrix increases 200 compiler derived algorithms edge lapack diminishes remaining matrix sizes compiler derived algorithm stays within 8 lapack one case lu factorization compiler version performs well large matrix sizes highly tuned blas used lapack factorization cause lapack faster table 5 shows slightly irregular pattern block size used compiler derived algorithm remark matrix sizes 50 200 mflop rate two block sizes 8 16 nearly equivalent hp observe pattern lu factorization cache performance critical outperform lapack version cache performance critical lapack version gives better results except matrix small algorithm performed much better 25x25 matrix likely due high overhead associated software pipelining short loops since cholesky factorization fewer operations lu factorization update portion code would expect high overhead associated small matrices also effects cache seen larger matrix sizes compared lu factorization due smaller update portion factorization table 5 cholesky performance ibm hp dec sgi size lablk lamf cblk cmf speedup lablk lamf cblk cmf speedup 50x50 100x100 200x200 300x300 dec outperform lapack version 500x500 matrix pattern seen lu factorization except takes longer appear due smaller size update portion factorization results sgi show compiler derived version performs better lapack matrix sizes 100 matrix size increases 500 150 compiler derived algorithms performance decreases 15 compared lapack factorization believe 2level cache hierarchy finally remark although table 5 shows similar pattern table 4 differences recall explained x 32 cholesky factorization approximately half floating point operations lu since neglects strict diagonal upper triangular portion matrix update phase moreover computation square root diagonal element n iterations 43 qr factorization table 6 shows performance compilerderived version qr factorization versus lapack version since compilerderived algorithm block qr factorization worse cache performance lapack algorithm 2 less computation expect worse performance cache performance becomes critical plain words lapack algorithm uses level 3 blas matrix multiply kernel dgemm compiler derived algorithm utilize operations similar level 2 blas hp see pattern however since cache performance algorithm good lapack version see much smaller improvement algorithm superior performance also see matrix sizes stay within limits cache lapack outperforms algorithm table 6 qr performance ibm hp size lablk lamf cblk cmf speedup lablk lamf cblk cmf speedup 28 075 300x300 three machines see pattern previous factorizations except degradations much larger large matrices due inferior cache performance cdqr interesting trend revealed table 6 ibm power2 slightly irregular block size pattern matrix size increases remark matrix sizes less equal 75 interesting behavior first two matrix sizes optimal block size larger dimension matrix implies blocking performed level 3 blas used lapack algorithm matrix size 75 rate achieved lapack algorithm block size 8 within 46 unblocked factorization 5 related work briefly review summarize investigations parallel evident active amount work remove substantial hand coding associated efficient dense linear algebra computations 51 blocking gemm based approach since lapack depends upon set highly tuned blas efficiency remains practical question optimized discussed introduction efficient set blas requires nontrivial effort software engineering see 23 discussion software efforts provide optimal implementations level 3 blas approach efficient practical gemmbased one proposed kagstrom ling van loan 23 recent study approach advocates optimizing general matrix multiply add kernel gemm rewriting remainder level 3 blas terms calls kernel benefit approach kernel needs optimized whether hand compiler thorough analysis highlights many issues must considered attempting construct set highly tuned blas moreover provide high quality implementations blas general use well performance evaluation benchmark 24 emphasize study examines whether necessary optimizations may left compiler also whether applied directly matrix factorizations beyond ability compiler recasting level 3 blas terms calls gemm 52 phipac another recent approach methodology expressed developing portable highperformance matrixvector libaries ansi c phipac 3 project motivated many reasons outlined introduction major difference approaches make parallel study gemm based approach seek support blas aim efficient vendor supplied blas however unlike study gemm one phipac assumes ansi c programming language various c semantics phipac instead seeks provide parameterized generators produce optimized code see report 3 discussion inhibitors c prevent optimizing compiler generating efficient code 53 autoblocking matrix multiplication frens wise present alternative algorithm matrixmatrix multiply based upon quadtree representation matrices 17 solution recursive suffers lack interprocedural optimization commercial compilers results show paging becomes problem sgi multiprocessor systems quadtree algorithm superior performance blas 3 smaller problems quadtree algorithm inferior performance relation work could expect compiler replace blas 3 quadtree approach appropriate change algorithm rather reshaping addition specialized storage layout used frens wise calls question effect entire program 6 summary set determine whether compiler automatically restructure matrix factorizations well enough avoid need hand optimization end examined collection implementations lapack programs determined whether plausible compiler technology could succeed obtaining block version point algorithm results study encouraging demonstrated exist implementable compiler methods automatically block matrix factorization codes achieve algorithms competitive lapack results show modestsized matrices advanced microprocessors compilerderived variants often superior matrix sizes typical workstations given future machine designs certain increasingly complex memory hierarchies compilers need adopt increasingly sophisticated memorymanagement strategies programmers remain free concentrate program logic given potential performance attainable automatic techniques believe possible user express machineindependent point matrix factorization algorithms without blas still get good performance compilers adopt enhancements already existing methods acknowledgments ken kennedy richard hanson provided original motivation work ken kennedy keith cooper danny sorensen provided financial support research begun rice university also wish thank tomas lofgren john pieper dec help obtaining dxml libraries diagnosing compilers performance respectively also thank per ling university umea ken stanley university california berkeley help benchmarks discussions r automatic translation fortran programs vector form portable parallel programming environment analysis interprocedural side effects parallel programming environment improving software pipelining unrollandjam compiler blockability numerical algorithms improving ratio memory operations floatingpoint operations loops tile size selection using cache organization set level 3 basic linear algebra subprograms extended set fortran basic linear algebra subprograms solving linear systems vector shared memory computers solving linear systems vector sharedmemory computers implementing linear algebra algorithms dense matrices vector pipeline machine parallel algorithms dense linear algebra computations practical dependence testing matrix computations implementation interprocedural bounded regular section analysis structure computers computations volume software pipelining effective scheduling technique vliw machines cache performance optimizations blocked algorithms basic linear algebra subprograms fortran usage implementing efficient portable dense matrix factorizations comparative study automatic vectorizing compilers introduction parallel vector solutions linear systems register allocation software pipelined loops automatic selection high order transformations ibm xl fortran compilers data locality optimizing algorithm advanced loop interchange iteration space tiling memory hierarchies tr automatic translation fortran programs vector form extended set fortran basic linear algebra subprograms software pipelining effective scheduling technique vliw machines introduction parallel myampersandamp vector solution linear systems analysis interprocedural side effects parallel programming environment parallel algorithms dense linear algebra computations set level 3 basic linear algebra subprograms cache performance optimizations blocked algorithms practical dependence testing data locality optimizing algorithm register allocation software pipelined loops compiler blockability numerical algorithms memoryhierarchy management improving ratio memory operations floatingpoint operations loops tile size selection using cache organization data layout matrix computations 3rd ed autoblocking matrixmultiplication tracking blas3 performance source code automatic selection highorder transformations ibm xl fortran compilers basic linear algebra subprograms fortran usage solving linear systems vector shared memory computers structure computers computations implementation interprocedural bounded regular section analysis iteration space tiling memory hierarchies implementing efficient portable dense matrix factorizations improving software pipelining unrollandjam ctr mahmut kandemir j ramanujam alok choudhary improving cache locality combination loop data transformations ieee transactions computers v48 n2 p159167 february 1999 nikolay mateev vijay menon keshav pingali fractal symbolic analysis proceedings 15th international conference supercomputing p3849 june 2001 sorrento italy kgstrm per ling charles van loan gemmbased level 3 blas highperformance model implementations performance evaluation benchmark acm transactions mathematical software toms v24 n3 p268302 sept 1998 steve carr soner nder case workingsetbased memory hierarchy proceedings 2nd conference computing frontiers may 0406 2005 ischia italy jeremy frens david wise autoblocking matrixmultiplication tracking blas3 performance source code acm sigplan notices v32 n7 p206216 july 1997 qing yi vikram adve ken kennedy transforming loops recursion multilevel memory hierarchies acm sigplan notices v35 n5 p169181 may 2000 qing yi ken kennedy haihang keith seymour jack dongarra automatic blocking qr lu factorizations locality proceedings 2004 workshop memory system performance june 0808 2004 washington dc vijay menon keshav pingali nikolay mateev fractal symbolic analysis acm transactions programming languages systems toplas v25 n6 p776813 november qing yi ken kennedy vikram adve transforming complex loop nests locality journal supercomputing v27 n3 p219264 march 2004