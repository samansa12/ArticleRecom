solving trustregion subproblem using lanczos method approximate minimization quadratic function within ellipsoidal trust region important subproblem many nonlinear programming methods number variables large widely used strategy trace path conjugate gradient iterates either convergence reaches trustregion boundary paper investigate ways continuing process boundary encountered key observe trustregion problem within currently generated krylov subspace special structure enables solved efficiently compare new strategy existing methods resulting software package available hslvf05 within harwell subroutine library b introduction trustregion methods unconstrained minimization blessed strong theoretical convergence properties good reputation practice main computational step methods find approximate minimizer model true objective function within trust region suitable norm correction lies within given bound restriction known trustregion constraint bound norm radius radius adjusted successive model problems mimic true objective within trust region widelyused models quadratic approximations objective function simple manipulate may lead rapid convergence underlying method theoretical point view norm defines trust region irrelevant long uniformly related 2 norm practical perspective choice certainly affects subproblem thus methods one consider solving popular practical choices 2 1 norms weighted variants thereof opinion important choice norm reflects underlying geometry problem simply picking may adequate problem large eigenvalues hessian model widely spread believe weighting norm essential many largescale problems paper consider solution quadraticmodel trustregion subproblem weighted 2 norm interested solving large problems thus cannot rely solely factorizations matrices involved thus concentrate iterative methods model hessian known positive definite trustregion radius sufficiently large trustregion constraint inactive unconstrained minimizer model obvious way solve problem use preconditioned conjugategradient method note role preconditioner role norm used trustregion namely change underlying geometry hessian rescaled space better conditioned thus come surprise two intimately connected formally shall require weighting 2 norm preconditioning performed matrix radius smaller critical value unconstrained minimizer model longer lie within trustregion thus required solution lie trustregion boundary simplest strategy case consider piecewise linear path connecting conjugategradient iterates stop point path leaves trust region strategy first proposed independently steihaug 1983 toint 1981 shall refer terminating point steihaugtoint point remarkably easy establish global convergence trustregion method based simple strategy key global convergence may proved provided accepted estimate solution model value larger cauchy point see powell 1975 cauchy point simply minimizer model within trustregion along preconditioned steepestdescent direction first segment piecewiselinear conjugategradient path gives n gould lucidi roma ph l toint precisely point model value monotonically decreasing along entire path steihaugtoint strategy ensures convergence model hessian indefinite solution must also lie trustregion boundary case may also simply handled using preconditioned conjugate gradients piecewise linear path followed either leaves trustregion segment negative curvature found vector p direction negative curvature inner product model hessian latter case path continued downhill along direction negative curvature far constraint boundary variant proposed steihaug 1983 toint 1981 suggests simply returning cauchy point global convergence ensured either terminating points objective function values larger cauchy point consistency previous paragraph shall continue refer terminating point steihaugs algorithm steihaugtoint point although strictly toints point case may different steihaugtoint method basically unconcerned trust region blunders boundary stops rather unfortunate particularly considerable experience shown frequently happens first often first iterations negative curvature present resulting step barely better cauchy direction may lead slow globally convergent algorithm theory barely convergent method practice paper consider alternative aims avoid drawback trying harder solve subproblem boundary encountered maintaining efficiencies conjugate gradient method long iterates lie interior mechanism use lanczos method paper organized follows section 2 formally define problem notation use basis new method given section 3 section 4 review basic properties preconditioned conjugategradient lanczos methods new method given detail section 5 numerical experiments demonstrating effectiveness approach given section 6 number conclusions perspectives drawn final section solving trustregion subproblem using lanczos method 3 2 trustregion subproblem solution let symmetric positivedefinite easilyinvertible approximation symmetric matrix h furthermore define mnorm vector hdelta deltai usual euclidean inner product paper consider mnorm trustregion problem minimize subject ksk delta 21 vector g radius delta 0 global solution problem characterized following result theorem 21 gay 1981 sorensen 1982 global minimizer qs subject satisfies equation positive semidefinite 0 ks positive definite unique result basis series related methods solving problem appropriate forming factorizations h j h number different values realistic either solution lies interior hence g solution lies boundary satisfies nonlinear equation denotes pseudoinverse h equation 23 straightforward solve using safeguarded newton iteration except socalled hard case g lies nullspace h case additional vector rangespace h may required solution trustregion boundary sought goldfeldt quandt trotter 1966 hebden 1973 gay 1981 proposed algorithms form sophisticated algorithm date sorensen 1983 available subroutine gqtpar minpack2 package guarantees nearly optimal solution obtained finite number factorizations algorithms appropriate large problems special hessian structure band matrices demands factorization iteration limits applicability general large problems reason methods require factorizations interest throughout paper shall denote k k identity matrix k jth column e j set vectors fq g said morthonormal hq kronecker delta 4 n gould lucidi roma ph l toint matrix q formed vectors morthonormal matrix set vectors fp g hconjugate horthogonal hp 3 new algorithm largescale trustregion subproblems set scene paper recall cauchy point may defined solution problem minimize subject ksk delta 31 minimizer q within trust region restricted 1dimensional subspace span phi dogleg methods see powell 1970 dennis mei 1979 aim solve problem onedimensional arc byrd schnabel schultz 1985 twodimensional subspace cases solution easy find search space small difficulty general problem 21 search space r n large leads immediately possibility solving compromise problem minimize subject ksk delta 32 specially chosen subspace r n consider steihaugtoint algorithm iteration k trustregion boundary encountered case point k1 solution 32 set span krylov space generated starting vector gamma1 g matrix gamma1 h steihaugtoint algorithm gradually widens search space using efficient preconditioned conjugate gradient method however soon steihaugtoint algorithm moves across trustregion boundary terminating point k1 longer necessarily solves problem 33 indeed unlikely k 0 iterates generated method increase mnorm iterate leaves trust region solution 33 thus 21 must lie boundary see steihaug 1983 theorem 21 details better yes recalling preconditioned conjugate gradient lanczos methods generate different bases krylov space solving trustregion subproblem using lanczos method 5 4 preconditioned conjugategradient lanczos methods preconditioned conjugategradient lanczos methods may viewed efficient techniques constructing different bases krylov space k k conjugate gradient method aims hconjugate basis lanczos method obtains morthonormal basis algorithm 41 preconditioned conjugate gradient method perform iteration algorithm 42 preconditioned lanczos method perform iteration conjugate gradient method generates basis algorithm 41 lanczos method generates basis 6 n gould lucidi roma ph l toint algorithm 42 lanczos iteration often written compact form k1 414 matrix q matrix tridiagonal follows directly two methods intimately related particular long conjugategradient iteration break lanczos vectors may recovered conjugategradient iterates lanczos tridiagonal may expressed ff ff kgamma2 ff conjugate gradient iteration may breakdown hp occur h positive definite stop hg hand lanczos iteration fail k j invariant subspace gamma1 h qs convex manifold k j1 minimizer j1 q manifold satisfies long initial value chosen thus estimate easy recur conjugategradient iteration minimizers successive manifolds may also easily obtained using lanczos process although conjugategradient iteration slightly less expensive thus preferred solving trustregion subproblem using lanczos method 7 vector g j1 conjugate gradient method gives gradient qs j1 quite common stop method soon gradient sufficiently small method naturally records gamma1 norm gradient kg norm also available lanczos method solves tridiagonal linear system k last component k1 h k available byproduct 5 truncated lanczos approach rather use preconditioned conjugate gradient basis fp shall use equivalent lanczos morthonormal basis fq g lanczos basis previously used nash 1984 convexify quadratic model lucidi roma 1997 compute good directions negative curvature within linesearch based method unconstrained minimization shall consider vectors form seek solves problem minimize 2s subject ksk delta 52 follows directly 415 417 418 h k solves problem minimize subject khk 2 delta 53 number crucial observations made firstly important note resulting trustregion problem involves twonorm rather mnorm secondly k tridiagonal feasible use moresorensen algorithm compute model minimizer even n large thirdly found h k matrix q k needed recover thus lanczos vectors either need saved backing store regenerated shall see need q satisfied continuing lanczos process give little extra benefit fourthly one would hope sequence problems may solved k changes addition extra diagonal superdiagonal entry solution data one subproblem may useful starting next consider issue section 52 basic trustregion solution classification theorem theorem 21 shows 8 n gould lucidi roma ph l toint k1 positive semidefinite tell us k firstly using 417 418 54 additionally comparing trustregion classification theorem see k galerkin approximation space spanned q k may ask good approximation particular error h g simplest way measuring error would calculate h k k solving 53 recover k q k h k finally substitute k k h ms g however inconvenient requires easy access q k fortunately far better way theorem 51 proof iw k1 414 iw k1 54 directly gives 56 57 follows gamma1 orthonormality w k1 2 therefore indirectly measure error gamma1 norm knowing simply fl k1 last component h k need k q k observant readers notice strong similarity error estimate estimate 422 gradient model lanczos method surprising two methods aiming point trustregion radius large enough interpretation 57 also identical 422 error small either fl k1 last component h k small consider problem 53 detail say symmetric tridiagonal matrix degenerate one offdiagonal entries zero otherwise nondegenerate following preliminary result solving trustregion subproblem using lanczos method 9 lemma 52 see also parlett 1980 theorem 795 suppose tridiagonal matrix nondegenerate v eigenvector first component v nonzero proof definition eigenvector suppose first component v zero considering first component 58 second component v zero tridiagonal nondegenerate repeating argument ith component 58 deduce 1st component v zero hence contradicts assumption v eigenvector first component v cannot zero 2 immediately yields following useful result theorem 53 suppose k nondegenerate hard case cannot occur subproblem 53 proof suppose hard case occurs definition fl 0 e 1 orthogonal v k eigenvector corresponding leftmost eigenvalue gamma k k thus first component v k zero following lemma 52 contradicts assumption v k eigenvector thus hard case cannot occur 2 result important suggests full power sorensen 1983 algorithm needed solve 53 shall return section 52 also immediate corollary corollary 54 suppose ngamma1 nondegenerate hard case cannot occur original problem 21 proof columns q forms basis r n thus problems 21 52 identical 52 53 related nonsingular transformation result follows directly theorem 53 case thus hard case occurs 21 lanczos tridiagonal must degenerate stage n gould lucidi roma ph l toint theorem 55 suppose k nondegenerate h k k satisfy 54 k1 positive semidefinite k1 positive definite proof suppose k k1 singular nonzero eigenvector v k hence combining 54 reveals hence first component v k zero contradicts lemma 52 hence k1 positive semidefinite nonsingular thus positive definite 2 result implies 54 unique solution consider solution theorem 56 suppose k1 proof suppose k degenerate k 1st component h k zero nondegeneracy k k 1st equation 54 deduce kth component h k zero repeating argument 1st equation 54 deduce ith component h k zero 1 k hence h contradicts first equation 54 thus k must degenerate 2 thus see two possibilities suggested theorem 51 obtaining k possibility fl occurs k1 theorem 57 suppose hard case occur 21 fl proof krylov space k k invariant subspace gamma1 h construction first basis element space gamma1 g hard case occur 21 space must also contain least one eigenvector corresponding leftmost eigenvalue gamma gamma1 h thus one eigenvalues k must gamma k k k k1 positive semidefinite implies h positive semidefinite combines 51 55 theorem 51 show k satisfies optimality conditions shown theorem 21 2 thus see easy case required solution obtained first nondegenerate block lanczos tridiagonal remains us consider hard case solving trustregion subproblem using lanczos method 11 view corollary 54 case occur k degenerate suppose therefore k degenerates blocks form k defines invariant subspace gamma1 h last block k first yield leftmost eigenvalue gamma gamma1 h two cases consider theorem 58 suppose hard case occurs 21 k described 59 last block k first yield leftmost eigenvalue gamma gamma1 h 1 k1 solution 21 given k1 solves positivedefinite system 2 k1 solution 21 given h solution nonsingular tridiagonal system u eigenvector k corresponding gamma ff chosen proof case 1 h positive semidefinite k 1 remaining optimality conditions satisfied k1 1 positive definite follows theorem 55 case 2 hm positive semidefinite furthermore easy show khk 2 kh k1 k 2 delta hence root ff ks delta finally q k defines invariant subspace hq k writing u therefore n gould lucidi roma ph l toint optimality conditions 52 2 notice obtain k described theorem require lanczos vectors corresponding blocks one perhaps k claim solve problem outlined theorem 58 realistic relies sure located leftmost eigenvalue gamma1 h lanczostype methods one cannot normally guarantee eigenvalues including leftmost found unless one ensures invariant subspaces investigated may prove expensive large problems particular lanczos algorithm algorithm 42 terminates time invariant subspace determined must restarted using vector q morthonormal previous lanczos directions vector may obtained gramschmidt process reorthonormalizing suitable vector vector component morthogonal existing invariant subspaces perhaps random vector respect previous lanczos directions means directions regenerated reread backing store thus form solution theoretical interest unlikely practical interest cheap approximation solution required 51 algorithm may outline algorithm algorithm 51 generalized lanczos trust region gltr method stress goal merely improve upon value delivered steihaugtoint method use full power theorem 58 content investigate first invariant subspace produced lanczos algorithm almost cases subspace contains global solution problem complications costs required implement method based theorem 58 believe prohibitive context solving trustregion subproblem using lanczos method 13 algorithm 51 generalized lanczos trust region method set flag interior true convergence perform iteration using 420 interior true ff k 0 ks k reset interior false interior true else solve tridiagonal trustregion subproblem 53 obtain h k interior true test convergence using residual kg else test convergence using value fl k1 jhe interior false recover rerunning recurrences obtaining q k backing store recovering rerunning recurrences economies made saving first pass reusing second potentially bigger saving may made one prepared accept slightly inferior value objective function idea simply save value q iteration convergence one looks back list find iteration say required percentage best value obtained recompute h accept required estimate solution required percentage occurs iteration boundary encountered final point boundary steihaugtoint point suitable available without need second pass note used conjugategradient method algorithm 41 generate lanczos vectors innerproduct hp k hp k proves tiny easy continue using lanczos method algorithm 42 vectors 14 n gould lucidi roma ph l toint required continue lanczos recurrence 411 directly calculable conjugategradient method stage steihaugtoint algorithm gltr method algorithm 51 need calculate ks k issue discussed steihaug implicitly assumed available however may case actually available procedure returns gamma1 v given input v thus unavailable fortunately significant drawback possible calculate ks k ffp k k available information see observe ks ks thus find ks k1 k 2 ks k k 2 long already know hs k mp k straightforward show quantities may calculated pair recurrences 512 course hg k v k already calculated part preconditioned conjugategradient method 52 solving nondegenerate tridiagonal trustregion subproblem view theorem 53 nondegenerate tridiagonal trustregion subproblem 53 theory easier solve general problem hessian tridiagonal thus inexpensive factorize hard case cannot occur cautious socalled almost hard case occurs g tiny component rangespace h may still happen trustregion problem case naturally ill conditioned thus likely difficult solve sorensen 1983 algorithm based able form factorizations model hessian certainly case tridiagonal try calculate leftmost eigenvalue pencil h tridiagonal case computing extreme eigenvalues straightforward particularly sequence related problems solved thus rather using sorensen algorithm prefer following method restrict case solution lies trustregion boundary switch approach conjugate gradient iteration leaves trust region basic iteration identical proposed sorensen 1983 namely apply newtons method solving trustregion subproblem using lanczos method 15 recalling denote leftmost eigenvalue k gamma k main difference approach sorensens always start point interval interval characterized positive definite resulting newton iteration globally linearly asymptotically quadratically convergent without safeguards newton iteration performed using algorithm 52 algorithm 52 newtons method solve 1 factorize unit bidiagonal diagonal matrices respectively 2 solve bdb 3 solve 4 replace newton correction step 4 algorithm given exact form given obtained using identity w computed step 3 slightly efficient pick b unit upper bidiagonal rather unit lowerbidiagonal step 2 simplifies b structure righthand side obtain suitable starting value two possibilities considered firstly attempt use solution value kgamma1 previous subproblem recall k merely appended row column already factorization trivial obtain thus determine latter positive definite turns positive definite h k kgamma1 computed 515 used start newton iteration secondly kgamma1 unsuitable monitor k see indefinite trivial instance matrix positive definite long ff generated conjugategradient method positive k positive definite start newton iteration value assumption gives kh k 0k 2 delta unconstrained solution lies outside trust region otherwise determine leftmost eigenvalue gamma k k start n gould lucidi roma ph l toint positive number chosen make k numerically positive definite mean bdb factorization exist ffl small possible found value 1 unit roundoff almost always suitable added precaution multiplying value increasing powers 2 long factorization fails need compute leftmost eigenvalue k use iteration based upon lastpivot function proposed parlett reid 1981 lastpivot function ffi k simply value last diagonal entry bdb factor k k gamma k1 value zero diagonal entries positive uncertainty l u placed around required root initial interval given gersgorin bounds leftmost eigenvalue known leftmost eigenvalue gamma kgamma1 may used improve lower bound cauchy interlacing property eigenvalues kgamma1 k see instance parlett 1980 theorem 1012 given initial estimate k improvement may sought applying newtons method ffi k derivative ffi k easy obtain recurrence however parlett reid point thus pole hence better choose new point fitting model function derivative value current pick new iterate larger root ffi new iterate lies outside interval uncertainty replaced midpoint interval interval contracted computing ffi k new iterate replacing appropriate endpoint iterate iteration stopped length interval value kgamma1 known initial iterate chosen positive ffl successive iterates generated 516 iterates convergence globally asymptotically superlinearly left newton iteration used required root frequently obscured scheme resorts interval bisection thus parlett reid scheme preferred means locating required eigenvalue based using determinant dett tried proved less reliable huge numerical range thus potential overflow determinant solving trustregion subproblem using lanczos method 17 6 numerical experiments algorithm sketched sections 51 52 implemented fortran 90 module hsl vf05 within harwell subroutine library 1998 main interest using methods described paper within trustregion algorithm particularly concerned two issues firstly obtain significantly better values model finding better approximations solution steihaug toint method secondly better approximations minimizer model necessarily translate fewer iterations trustregion method section address outstanding questions throughout consider basic problem minimizing objective fx n real variables x shall use following standard trustregion method algorithm 61 standard trustregion algorithm initial point x 0 initial trustregion radius delta 0 given constants ffl g required satisfy conditions 1 stop kr x fx k k 2 ffl g 2 define secondorder taylor series model q k positivedefinite preconditioner compute step k sufficiently reduce model q k within trustregion 3 compute ratio 4 set increment k one go step 1 choose specific values ffl set upper limit n iterations step k step 2 computed using either algorithm 51 steihaugtoint algorithm convergence algorithms subproblem occurs soon n gould lucidi roma ph l toint n iterations performed addition course steihaugtoint algorithm terminates soon boundary crossed tests performed ibm risc system6000 3bt workstation 64 megabytes ram codes double precision fortran 90 compiled xlf90 optimization ibm library blas used test examples consider larger examples cute test set see bongartz conn gould toint 1995 negative curvature frequently encountered tests terminated thirty cpu minutes elapsed 61 get much better model values steihaugtoint first consider problems form 21 test examples generated running algorithm 61 cute set 10 iterations taking trustregion subproblem iteration example idea simulate kind subproblems occur practice result starting point algorithm points frequently special favourable properties aim see whether significant advantage continuing minimization trustregion subproblem boundary trust region encountered ran hsl vf05 convergence stopping kg iterations performed experiments reported best value found fact optimum value factorization h used confirm matrix positive semidefinite algorithm ensured remaining optimality conditions hold although course guarantee always case measured iteration st percentage ratio optimal value obtained point steihaugtoint method left trust region well number iterations taken achieve 10 90 99 optimal reduction 10 90 99 respectively results experiments summarized table 61 table give name example used along dimension n statistics ratioexpressed form xy shorthand x theta 10 st 10 90 99 described problems interior solutions case ratio st statistics absent indicated dash considered unpreconditioned method variety standard preconditioners band preconditioner semibandwidth 5 modified incomplete sparse cholesky factorizations modifications proposed schnabel eskow 1991 used lancelot package see conn gould toint 1992 chapter 3 cholesky factorization methods failed problem msqrtals hessian matrix required much storage make number observations 1 problems steihaugtoint point gives model value good approximation optimal value solving trustregion subproblem using lanczos method 19 preconditioner 5band example brybnd 1000 35 23 24 28 chainwoo 1000 45 15 cosine 1000 genrose 1000 msqrtals 1024 11 12 11 23 incomplete cholesky modified cholesky example cosine 1000 genrose 1000 msqrtals 1024 factorization failure factorization failure table 61 comparison number iterations required achieve given percentage optimal model value variety preconditioners see text key data n gould lucidi roma ph l toint 2 problems extra iterations beyond steihaugtoint point pay handsome dividends 3 getting within 90 even 99 best value rarely requires many iterations find steihaugtoint point conclusion based numbers suggest good strategy would perform say 5 iterations beyond steihaugtoint point accept improved point model value significantly better cost second pass compute lanczos vectors shall consider next section 62 better values steihaugtoint imply better trustregion method consider methods described approximately solving trustregion subproblem perform within trustregion algorithm particular interest question whether solving subproblem accurately reduces number trustregion iterations particularly cost solving problem number iterations concern evaluation objective function derivatives dominant cost direct correlation number iterations overall cost solving problem tables 62 63 compare steihaugtoint scheme gltr algorithm algorithm 51 run high accuracy exclude problem hydc20ls reported results method succeeded solving problem fewer limit n iterations problems broydn7d spmsrtls number different local minima found tables addition name dimension example give number objective function f derivative g values computed total number matrixvector products prod required solve subproblems total cpu time required seconds compare preconditioners used previous section indicate cases one method performs least 10 better competitor highlighting relevant figure bold observe following 1 use different leads radically different behaviour different preconditioners appear particularly suited different problems surprisingly perhaps unpreconditioned algorithm often performs best overall 2 unpreconditioned case modeloptimum variant frequently requires significantly fewer function evaluations steihaugtoint method however extra algebraic costs per iteration often outweigh reduction numbers iterations advantage function calls preconditioners less pronounced ideally one would like retain advantage numbers function calls reducing cost per iteration noted section 61 one normally gets good approximation optimal model value modest number iterations moreover steihaugtoint point often gives significantly suboptimal value extra iterations usually suffices give solving trustregion subproblem using lanczos method 21 preconditioner steihaugtoint model optimum example iterations 865 577 34419 14502 dqrtic 1000 43 43 83 03 43 43 91 03 freuroth 1000 17 17 34 04 17 17 34 04 genrose 1000 859 777 6092 288 773 642 24466 822 msqrtals 1024 44 34 7795 4860 noncvxun 1000 492 466 177942 10179 1800 seconds sensors 100 20 19 37 64 20 19 140 88 sinquad 5000 182 114 363 243 161 106 382 246 5band steihaugtoint model optimum example chainwoo 1000 146 99 145 48 191 123 196 63 cosine 1000 21 15 20 04 21 15 cragglvy 1000 22 22 21 11 22 22 21 11 dqrtic 1000 54 54 53 09 54 54 53 10 eigenals 930 56 43 171 752 53 42 222 758 freuroth 1000 20 iterations n iterations mancino 100 91 72 90 872 52 43 90 522 msqrtals 1024 88 62 9793 7002 73 52 19416 12922 22 n gould lucidi roma ph l toint incomplete cholesky steihaugtoint model optimum example chainwoo 1000 174 115 173 81 183 121 309 103 cosine 1000 22 17 26 08 22 19 49 12 cragglvy 1000 22 22 21 15 22 22 21 15 dqrtic 1000 54 54 53 09 54 54 53 11 iterations n iterations genrose 1000 948 629 951 355 496 322 847 235 28 msqrtals 1024 factorization failure factorization failure 28 150 412 iterations n iterations iterations n iterations sinquad 5000 77 52 89 5426 78 50 121 5267 modified cholesky steihaugtoint model optimum example brybnd 1000 15 15 14 22 59 37 61 77 chainwoo 1000 178 119 177 76 183 121 309 103 cosine 1000 cragglvy 1000 23 23 33 14 22 22 21 16 dqrtic 1000 54 54 53 12 54 54 53 11 iterations n iterations genrose 1000 462 332 463 165 496 322 847 234 mancino 100 31 28 28 msqrtals 1024 factorization failure factorization failure solving trustregion subproblem using lanczos method 23 large percentage optimum thus next investigate issues context overall trustregion method tables 64 65 compare number function evaluations f cpu time taken solve problem steihaugtoint st method number variations basic gltr method algorithm 51 basic requirement compute model value least 90 best value found first pass gltr method value obtained iterate gives steihaugtoint point steihaug toint point accepted otherwise second pass performed recover first point 90 best value observed ingredient choice stopping rule first pass one possibility stop pass soon test 64 satisfied denote strategy 90best possibility stop either 64 satisfied fixed number iterations beyond steihaugtoint point occurred refer 90stk k gives number additional iterations allowed investigate cases compare preconditioners used previous section highlight bold entries least 10 better competition conclusions broadly method successes failures clear overall best method preconditioner although unpreconditioned version performs surprisingly well restricting number iteration allowed steihaugtoint point found appears curb worst behaviour unrestricted method n gould lucidi roma ph l toint preconditioner st 90st1 90st5 90st10 90best example cragglvy 1000 19 10 19 09 19 09 19 09 19 10 dqrtic 1000 43 03 43 03 43 03 43 03 43 03 genrose 1000 859 288 748 389 721 481 738 573 728 600 msqrtals 1024 44 4860 noncvxun 1000 492 10179 368 8613 1800 secs 1800 secs 433 11986 sensors 100 20 64 23 73 21 81 21 80 21 81 sinquad 5000 182 243 152 208 152 217 152 214 152 215 5band st 90st1 90st5 90st10 90best example chainwoo 1000 146 48 159 51 159 51 159 52 159 51 cosine 1000 21 04 21 05 21 04 21 04 21 05 cragglvy 1000 22 11 22 10 22 11 22 11 22 11 dqrtic 1000 54 09 54 09 54 10 54 10 54 10 mancino 100 91 872 52 518 52 518 52 520 52 518 msqrtals 1024 88 7002 97 7567 73 7049 74 8447 79 9815 solving trustregion subproblem using lanczos method 25 incomplete cholesky st 90st1 90st5 90st10 90best example brybnd 1000 55 39 56 42 56 43 56 43 56 50 chainwoo 1000 174 81 199 97 199 101 199 102 199 101 cragglvy 1000 22 15 22 16 22 16 22 15 22 16 dqrtic 1000 54 09 54 10 54 11 54 11 54 11 eigenals 930 76 946 77 972 74 972 74 973 74 968 genrose 1000 948 355 500 224 499 230 499 230 499 230 msqrtals 1024 fact failure fact failure fact failure fact failure fact failure sinquad 5000 77 5426 68 4842 68 4841 68 4854 68 4890 modified cholesky st 90st1 90st5 90st10 90best example brybnd 1000 15 22 15 22 15 23 15 22 15 22 chainwoo 1000 178 76 176 79 176 79 176 78 176 80 cosine 1000 41 11 41 13 41 13 41 13 41 13 dqrtic 1000 54 12 54 12 54 13 54 13 54 13 genrose 1000 462 165 434 188 434 193 434 191 434 191 mancino 100 31 1293 64 2323 77 2759 77 2755 77 2756 msqrtals 1024 fact failure fact failure fact failure fact failure fact failure 26 n gould lucidi roma ph l toint 7 perspectives conclusions considered number methods aim find better approximation solution trustregion subproblem delivered steihaugtoint scheme methods based solving subproblem within subspace defined krylov space generated conjugategradient lanczos methods krylov subproblem number useful properties lead efficient solution resulting algorithm available fortran 90 module hsl vf05 within harwell subroutine library 1998 must admit slightly disappointed new method perform uniformly better steihaugtoint scheme genuinely surprised accurate approximation appear significantly reduce number function evaluations within standard trustregion method may limit use methods developed also calls question number recent eigensolutionbased proposals solving trustregion subproblem see rendl vanderbei wolkowicz 1995 rendl wolkowicz 1997 sorensen 1997 santos sorensen 1995 authors demonstrate methods provide effective means solving subproblem make effort evaluate whether actually useful within trustregion method results given paper suggest may fact case also leads interesting question whether possible obtain useful lowaccuracy solutions methods pretend formulae given paper exact even accurate floatingpoint arithmetic indeed wellknown floatingpoint matrices q k lanczos method quickly loose morthonormality see instance parlett 1980 section 133 despite method given appears capable producing usable approximate solutions trustregion subproblem currently investigating one possibility considered far find estimate using first pass algorithm 51 compute required minimizing unconstrained model using preconditioned conjugate gradient method advantage instability first pass necessarily reappear auxiliary calculation disadvantages may require work simply using 51 must computed sufficiently large ensure h positive semidefinite acknowledgement would like thank john reid helpful advice computing eigenvalues tridiagonal matrices jorge useful comments sorensen 1983 method grateful british councilmurst travel grant rom8899553 made research possible solving trustregion subproblem using lanczos method 27 r cute constrained unconstrained testing environment family trustregionbased algorithms unconstrained minimization strong global convergence properties lancelot fortran package largescale nonlinear optimization release two new unconstrained optimization algorithms use function gradient values computing optimal locally constrained steps maximization quadratic hillclimbing catalogue subroutines release 13 algorithm minimization using exact second derivatives numerical experience new truncated newton methods large scale unconstrained optimization computing trust region step symmetric eigenvalue problem tracking progress lanczos algorithm large symmetric eigenproblems convergence properties class minimization algorithms semidefinite framework trust region subproblems applications large scale minimization new matrixfree algorithm largescale trustregion subproblem new modified cholesky factorization newtons method model trust modification minimization largescale quadratic function subject spherical constraint siam journal optimization conjugate gradient method trust regions large scale optimization towards efficient sparsity exploiting newton method minimization tr ctr nicholas gould philippe l toint filtrane fortran 95 filtertrustregion package solving nonlinear leastsquares nonlinear feasibility problems acm transactions mathematical software toms v33 n1 p3es march 2007 giovanni fasano massimo roma iterative computation negative curvature directions large scale optimization computational optimization applications v38 n1 p81104 september 2007 nicholas gould dominique orban philippe l toint galahad library threadsafe fortran 90 packages largescale nonlinear optimization acm transactions mathematical software toms v29 n4 p353372 december nicholas gould dominique orban philippe l toint cuter sifdec constrained unconstrained testing environment revisited acm transactions mathematical software toms v29 n4 p373394 december nicholas gould philippe l toint iterative workingset method largescale nonconvex quadratic programming applied numerical mathematics v43 n12 p109128 october 2002