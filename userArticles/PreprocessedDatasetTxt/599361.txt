bayesian parameter estimation via variational methods consider logistic regression model gaussian prior distribution parameters show accurate variational transformation used obtain closed form approximation posterior distribution parameters thereby yielding approximate posterior predictive model approach readily extended binary graphical model complete observations graphical models incomplete observations utilize additional variational transformation obtain closed form approximation posterior finally show dual regression problem gives latent variable density model variational formulation leads exactly solvable em updates b introduction bayesian methods number virtues particularly uniform treatment uncertainty levels modeling process formalism also allows ready incorporation prior knowledge seamless combination knowledge observed data bernardo smith 1994 gelman 1995 heckerman et al 1995 elegant semantics however often comes sizable computational costposterior distributions resulting incorporation observed data must represented updated generally involves highdimensional integra tion computational cost involved carrying operations call question viability bayesian methods even relatively simple settings generalized linear models mccullagh nelder 1983 concern paper particular generalized linear modellogistic regressionand focus bayesian calculations computationally tractable particular describe exible deterministic approximation procedure allows posterior distribution logistic regression represented updated eciently also show methods permit bayesian treatment complex modela directed graphical model belief network node logistic regression model deterministic approximation methods develop paper known generically variational methods variational techniques used extensively physics literature see eg parisi 1988 sakurai 1985 also found applications statistics rustagi 1976 roughly speaking objective methods transform problem interest optimization problem via introduction extra degrees freedom known variational parameters xed values variational parameters transformed problem often closed form solution providing approximate solution original problem variational parameters adjusted via optimization algorithm yield improving sequence approximations introduction variational methods context graphical models see jordan et al 1999 let us brie sketch variational method develop paper study logistic regression model gaussian prior parameter vector variational transformation replaces logistic function adjustable lower bound gaussian form exponential quadratic function parameters product prior variationally transformed likelihood thus yields gaussian expression posterior conjugacy optimize variationally procedure iterated successive data point methods compared laplace approximation logistic regression cf spiegelhalter lauritzen 1990 closely related method also utilizes gaussian approximation posterior anticipate discussion following sections see variational approach advantage laplace approximation particular use variational parameters gives variational approach greater exibility show exibility translates improved accuracy approximation variational methods also contrasted sampling techniques become method choice bayesian statistics thomas et al 1992 neal 1993 gilks et al 1996 sampling techniques enjoy wide applicability powerful evaluating multidimensional integrals representing posterior distributions however yield closed form solutions guarantee monotonically improving approximations precisely features characterize variational methods paper organized follows first describe detail variational approximation method bayesian logistic regression followed evaluation accuracy method comparison laplace approximation extend framework belief networks considering complete data incomplete data finally consider dual regression problem show techniques lead exactly solvable em updates bayesian logistic regression begin logistic regression model given logistic function binary response variable set explanatory variables represent uncertainty parameter values via prior distribution p assume gaussian possibly full covariance structure predictive distribution therefore z order utilize distribution need able compute posterior parameter distribution assume r g complete observation calculation intractable large n thus consider variational approximation approach involves nding variational transformation logistic function using transformed function approximate likelihood particular wish consider transformations combine readily gaussian prior sense gaussian prior becomes conjugate prior transformed likelihood begin introducing type variational transformations use purpose 21 brief introduction variational methods consider continuously dierentiable convex function fz figure 1 provides example convex function make use later convexity function guarantees denition tangent line always remains function may thus interpret collection tangent lines parameterized family lower bounds convex function cf convex duality rockafellar 1976 tangents family naturally parameterized locations point view approximating convex nonlinear function f seems natural use one simpler tangent lines lower bound formulate little precisely let lz z 0 tangent line z follows fz lz z 0 z z 0 fz 0 terminology variational methods lz z 0 variational lower bound fz parameter z 0 known variational parameter since lower bound lz z 0 considerably simpler linear case nonlinear function fz may attractive substitute lower bound f note free adjust variational parameter z 0 location tangent make lz z 0 accurate approximation fz possible around point interest ie z z 0 quality approximation degrades z receeds z 0 rate happens depends curvature fz whenever function f relatively low curvature case figure 1 adjustable linear approximation seems quite attractive 5 figure 1 convex function f two tangent lines locations tangents indicated short vertical line segments 22 variational methods bayesian logistic regression illustrate variational methods type described used transform logistic likelihood function form readily combines gaussian prior conjugacy precisely transformed logistic function depend parameters quadratically exponent begin symmetrizing log logistic function log noting function variable x 2 readily veried taking second derivatives behavior fx function x 2 shown figure 1 discussed tangent surface convex function global lower bound function thus bound fx globally rst order taylor expansion variable x note lower bound exact whenever combining result eq 4 exponentiating yields desired variational transformation logistic tanh24 also introduce following p denotes variational lower bound logistic function lower bound longer normalized refer eq 8 transformation conditional probability xed value h fact recover exact value logistic function via particular choice variational parameter indeed maximizing lower bound respect yields substituting value back lower bound recovers original conditional probability values obtain lower bound true posterior p jd computed normalizing p sjx p given calculation feasible general instead form bound normalize variational approximation p sjx p given p gaussian given choice gaussian variational form p sjx normalized variational distribution gaussian note although p lower bound true conditional probability variational posterior approximation proper density thus longer bound approximate bayesian update amounts updating prior mean prior covariance matrix posterior mean posterior covariance matrix omitting algebra nd updates take following form single observation x successive observations incorporated posterior applying updates recursively work nished however posterior covariance matrix depends variational parameter yet specify choose via optimization procedure particular nd value yields tight lower bound eq 9 fact variational expression eq 9 lower bound importantit allows us use em algorithm perform optimization derive em algorithm appendix result following closed form update equation post expectation taken respect p jd old variational posterior distribution based previous value owing em formulation update corresponds monotone improvement posterior approximation empirically nd procedure converges rapidly iterations needed accuracy approximation considered following two sections summarize variational approach allows us obtain closed form expression posterior predictive distribution logistic regression z posterior distribution p jd comes making single pass data set applying updates eq 10 eq 11 optimizing associated variational parameters step predictive lower bound p jx takes form log complete observation signify parameters p jd subscript refers posterior p jd found augmenting data set include point note nally variational bayesian calculations presented need carried sequentially could compute variational approximation posterior probability p jd introducing separate transformations logistic functions resulting variational parameters would optimized jointly rather one time believe sequential approach provides cleaner solution 3 accuracy variational method logistic function shown figure 2a along variational approximation 2 noted value variational parameter particular point x approximation exact remaining values x approximation lower bound 4 2 0 2 402061 figure 2 logistic function solid line variational form dashed line 2 b dierence predictive likelihood variational approximation function g 0 described text integrating eq 9 parameters obtain lower bound predictive probability observation tightness lower bound measure accuracy approximation assess variational approximation according measure compared lower bound true predictive likelihood evaluated numerically note single observation evaluation predictive likelihood reduced onedimensional integration problem z z eective prior p gaussian mean actual prior distribution p mean covariance reduction eect accuracy variational transformation thus used evaluating overall accuracy figure 2b shows dierence true predictive probability variational lower bound various settings eective mean 0 variance 2 optimized separately dierent values 0 2 fact variational approximation lower bound means dierence predictive likelihood always positive emphasize tightness lower bound relevant measure accuracy indeed tight lower bound predictive probability assures us associated posterior distribution highly accurate converse true general words poor lower bound necessarily imply poor approximation posterior distribution point interest longer guarantees good accuracy practice expect accuracy posterior important predictive probability since errors posterior run risk accumulating course sequential estimation procedure defer evaluation posterior accuracy following section comparisons made related methods 4 comparison methods sequential approximation methods yield closed form posterior parameter distributions logistic regression models method closely related spiegelhalter lauritzen 1990 refer sl approximation paper method based laplace ap proximation utilize local quadratic approximation complete loglikelihood centered prior mean parameter updates implement approximation similar spirit variational updates eq 10 eq post post x 18 x since additional adjustable parameters approximation simpler variational method however would expect lack exibility translate less accurate posterior estimates compared accuracy posterior estimates two methods context single observation simplify comparison utilized reduction described previous section since accuracy neither method aected reduction suces purposes carry comparison simpler setting 1 posterior probability interest therefore p 0 jd computed various choices values prior mean 0 prior standard deviation correct posterior mean standard deviations obtained numerically figures 3 4 present results plot signed dierences comparing obtained posterior means correct ones relative errors used posterior standard deviations error measures left signed reveal systematic biases note posterior mean variational method guaranteed lower bound true mean guarantees given predictive likelihood seen figures 3a 4a variational method yields signicantly accurate estimates posterior means values prior variance posterior variance sl estimate variational estimate appear yield roughly comparable accuracy small value prior variance figure 3b however larger prior variance variational approximation superior figure 4b note variational method consistently underestimates true posterior variance fact could used rene approximation finally terms kldivergences approximate true posteriors variational method sl approximation roughly equivalent small prior variance 1 note true posterior distribution always recovered posterior computed onedimensional reduced parameter variational method superior larger value prior variance shown figure 5 error mean sl approximation variational 002002relative error stdv sl approximation variational figure 3 errors posterior means function g 0 0 prior mean prior b relative errors posterior standard deviations function g 0 prior distribution 04 error mean sl approximation variational 02 0101relative error stdv sl approximation variational figure 4 plots figure 3 prior distribution 5 extension belief networks belief network probabilistic model set variables fs g identied nodes acyclic directed graph letting denote set parents node graph dene joint distribution associated belief network following product kldivergence sl approximation variational kldivergence sl approximation variational figure 5 kldivergences approximate true posterior distribution function g 0 prior b 3 two approximation methods visually identical curves refer conditional probabilities p js local probabilities associated belief network section extend earlier work paper consider belief networks logistic regression used dene local probabilities models studied nonbayesian setting neal 1992 saul jaakkola jordan 1994 thus introduce parameter vectors one binary variable consider models local probability p js logistic regression node parents simplify arguments following sections consider augmented belief networks parameters treated nodes belief network see figure 6 standard device belief network literature course natural within bayesian formalism 51 complete cases complete case refers data point variables fs g observed data points complete cases methods developed previous section apply immediately belief networks seen follows consider markov blankets associated parameters figure 6a complete cases nodes within markov blanket parameters observed shaded diagram independence semantics belief networks implies posterior distributions parameters independent one another conditioned observed data thus problem estimating posterior distributions parameters reduces set n independent subproblems bayesian logistic regression problem apply methods developed previous sections directly ss ss figure complete observation shaded variables markov blanket dashed line associated parameters 4 b observation value 4 missing unshaded gure 52 incomplete cases situation substantially complex incomplete cases data set incomplete cases imply longer markov blankets parameters network thus dependencies arise parameter distributions dierent conditional models let us consider situation detail missing value implies observations arise marginal distribution obtained summing missing values unobserved variables marginal distribution thus mixture distribution mixture component corresponds particular conguration missing variables weight assigned component essentially posterior probability associated conguration spiegelhalter lauritzen 1990 note dependencies arising missing values observations make network quite densely connected missing value node eectively connects neighboring nodes graph dense connectivity leaves little structure exploited exact probabilistic computations networks tends make exact probabilistic calculations intractable approach developing bayesian methods belief networks missing variables combines two variational techniques particular augment transformation introduced earlier second variational transformation refer qtransformation purpose transformation convert local conditional probability form integrated analytically purpose qtransformation approximate eect marginalizing across missing values associated one parents 2 intuitively qtransformation 2 treating parameter parent node helps emphasize similarity two variational transformations principal dierence parameter node single lls missing values allowing variational transformation complete data invoked overall result closedform approximation marginal posterior correct marginalization across missing variables global operation aects conditional models depend variables marginalized variational approximation describe marginalization local operation acts individually relevant conditional models 521 approximate marginalization consider problem marginalizing set variables 0 joint distribution performed marginalization exactly resulting distribution would retain factorization original joint assuming 0 involved one conditionals seen partitioned product set factors depend 0 indexed 0 indexed 00 marginalization generally local operation individual node probabilities p locality desirable goal computational reasons achieved forgo exact marginalization instead consider approximations particular describe variational approximation preserves locality expense providing lower bound marginal probability instead exact result obtain desired variational transformation exploit convexity property particular given sequence consider geometric average probability distribution well known geometric average less equal arithmetic average easily established via invocation jensens inequality exploit fact follows consider arbitrary distribution qs 0 rewrite marginalization operation following way child general parents multiple children inequality comes transforming average bracketed term respect distribution q geometric average third line follows plugging form joint distribution exchanging order products logarithm multiplicative constant cq entropy variational distribution q therefore log let us make observations result eq 24 first note lower bound equation factored form original joint probability particular dene qtransformation ith local conditional probability follows lower bound eq 24 product qtransformations second note conditionals transformed distribution q change q thus aect transformed conditionals means dependencies variables would resulted exact marginalization 0 replaced eective dependencies shared variational distribution q bound eq 24 holds arbitrary variational distribution qs 0 obtain tight bound need optimize across qs 0 practice involves choosing constrained class distributions optimizing across class simplest form variational distribution completely factorized distribution yields variational bound traditionally referred mean eld approximation simplied approximation appropriate dense models relatively large number missing values generally one consider structured variational distributions involving partial factorizations correspond tractable substructures graphical model cf saul jordan 1996 consider topic following two sections although main constraint choice qs 0 computational one associated evaluation optimization one additional constraint must borne mind particular qtransformed conditional probabilities must form subsequent transformation invoked yielding result tractable bayesian integral simple way meet constraint require variational distribution qs 0 depend parameters discuss following section case qtransformations simply involve products logistic functions behave well transformation 522 bayesian parameter updates derivation presented previous section shows approximate variational marginalization across set variables 0 viewed geometric average local conditional probabilities qs 0 variational distribution missing values note transformations carried separately relevant conditional model variational distribution q associated missing values across qtransformations given transformation eq 28 approximate bayesian updates obtained readily particular conditioning data point missing components rst apply qtransformation eectively lls missing values resulting transformed joint distribution factorizes case complete observations posterior parameter distributions therefore obtained independently parameters associated transformed local probabilities two issues need considered first transformed conditional probabilities cf eq 28 products logistic functions therefore complicated transformation method however transforms logistic function exponential quadratic dependence parameters products transforms also exponential quadratic dependence parameters thus approximate likelihood gaussian prior multivariate gaussian approximate posterior also gaussian second issue dependence posterior parameter distributions variational distribution q optimize variational parameters distribution case make bounds tight possible particular set q distribution maximizes lower bound optimization carried conjunction optimization parameters transformations logistic functions also lower bounds show appendix b1 fact approximations lower bounds implies devise em algorithm perform maximization updates derived appendix follows pos vector parents expectations taken respect variational distribution q 523 numerical evaluation section provide numerical evaluation proposed combination qtransformation transformation study simple graph consists single node parents contrast simple logistic regression case analyzed earlier parents observed instead distributed according distribution p distribution manipulate directly experiments essentially provides surrogate eects pattern evidence ancestral graph associated node cf spiegelhalter lauritzen 1990 interest posterior probability parameters associated conditional probability p sjs suppose observe 1 exact posterior probability parameters case given variational method focuses lower bounding evidence term brackets natural evaluate overall accuracy approximation evaluating accuracy marginal data likelihood consider two dierent variational approximations rst approximation variational distribution q left unconstrained second use approximation factorizes across parents mean eld approximation emphasize cases variational posterior approximation parameters single gaussian results experiment shown figures 7 8 gure displays three curves corresponding exact evaluation data likelihood p two variational lower bounds number parents 5 prior distribution p taken zero mean gaussian variable covariance matrix symmetry gaussian distribution sigmoid function exact value p 05 cases considered several choices p p rst case p assumed factorize across parents leaving single parameter p species stochasticity p similar setting would arise applying mean eld approximation context general graph figure 7 shows accuracy variational lower bounds function p figure 7a p ie covariance matrix diagonal diagonal components set 15 figure sample covariance matrix 5 gaussian random vectors distributed according n0 i5 results figure 7b averaged 5 independent runs choice scaling n0 i5 made insure j gures indicate variational approximations reasonably accurate little dierence two methods figure 8 see mean eld approximation unimodal deteriorates distribution p changes factorized distribution toward mixture distribution specically let p f jp uniform factorized distribution discussed parameter p let pm pure mixture distribution assigns probability mass 13 three dierent randomly chosen congurations parents let p parameter p controls extent p resembles pure mixture distribution figure 8 illustrates accuracy two variational methods function p figure 8a expected mean eld approximation deteriorates increasing p whereas rst variational approximation remains accurate 6 dual problem logistic regression formulation eq 1 parameters explanatory variables x play dual symmetric role cf nadal parga 1994 bayesian logistic regression setting symmetry broken associating parameter vector multiple occurences explanatory variables x shown figure 9 alternatively may break symmetry associating single instance explanatory variable x multiple realizations sense explanatory variables x play role parameters functions continuous latent variable dual bayesian regression model thus latent likelihood likelihood figure 7 exact data likelihood solid line variational lower bound 1 dashed line variational lower 2 dotted line function stochasticity parameter p p p b p sample covariance 5 random vectors distributed according n0 i5 likelihood likelihood figure 8 exact data likelihood solid line two variational lower bounds dashed dotted lines respectively function mixture parameter p 01 b variable density model binary response variable graphically dual interpretation single parameter node x whereas separate nodes required dierent realizations illustrated gure explain successive observations latent variable density model single binary variable particularly interesting generalize response variable vector binary variables component distinct set parameters associated latent variables however remain dual interpretation note strictly speaking dual interpretation would require us assign prior distribution new parameters vectors x simplicity however omit consideration treat x simply adjustable parameters resulting latent variable 1 2 3 1 2 3 1 2 3 x 1 2 3 b figure 9 bayesian regression problem b dual problem density model binary vectors akin standard factor analysis model see eg everitt 1984 model already used facilitate visualization high dimensional binary vectors tipping 1999 turn technical treatment latent variable model joint distribution given conditional probabilities binary observables logistic regression models would like use em algorithm parameter estimation achieve exploit variational transformations transformations introduced conditional probability joint distribution optimized separately every observation n g database consisting values binary output variables logistic regression case transformations change unwieldy conditional models simpler ones depend parameters quadratically exponent variational ev idence product transformed conditional probabilities retains property consequently variational approximation compute posterior distribution latent variables closed form mean covariance posterior obtained analogously regression case giving variational parameters associated observation conditional model updated using eq 12 x replaced x vector parameters associated th conditional model solve mstep emalgorithm accumulating sucient statistics parameters x based closed form posterior distributions corresponding observations data set omitting algebra obtain following explicit updates parameters subscript denotes quantities pertaining observation note since variational transformations expoited arrive updates lower bounds mstep necessarily results monotonically increasing lower bound logprobability observations desirable monotonicity property unlikely arise types approximation methods laplace approximation exemplied use variational techniques setting bayesian parameter estimation found variational methods exploited yield closed form expressions approximate posterior distributions parameters logistic regression methods apply immediately bayesian treatment logistic belief networks complete data also showed combine mean eld theory variational transformation thereby treat belief networks missing data finally variational techniques lead exactly solvable em algorithm latent variable density modelthe dual logistic regression problem also interest note variational method provides alternative standard iterative newtonraphson method maximum likelihood estimation logistic regression algorithm known iterative reweighted least squares irls advantage variational approach guarantees monotone improvement likelihood present derivation algorithm appendix c finally alternative perspective application variational methods bayesian inference see hinton van camp 1993 mackay 1997 authors developed variational method known ensemble learning viewed mean eld approximation marginal likelihood r bayesian theory learning bayesian networks combination knowledge statistical data introduction latent variable models bayesian data analysis markov chain monte carlo practice keeping neural networks simple minimizing description length weights introduction variational methods graphical models generalized linear models ensemble learning hidden markov models duality learning machines bridge supervised unsupervised learning technical report crgtr931 convex analysis variational methods statistics exploiting tractable substructures intractable net works sequential updating conditional probabilities directed graphical structures probabilistic visualisation highdimensional binary data bugs program perform bayesian inference using gibbs sampling optimization variational parameters optimize variational approximation eq form posterior however remains least unwieldy bayesian logistic regression problem considered earlier paper proceeding analogously transform logistic functions eq 7 corresponding conditional probabilities product obtain p optimization parameters shown appendix b metric optimizing parameters comes fact transformations associated parameters introduce lower bound probability observations similarly case simple bayesian logistic regression considered previously see tr ctr edward snelson zoubin ghahramani compact approximations bayesian predictive distributions proceedings 22nd international conference machine learning p840847 august 0711 2005 bonn germany r ksantini ziou b colin f dubeau weighted pseudometric fast cbir method machine graphics vision international journal v15 n3 p471480 january 2006 aaron dsouza sethu vijayakumar stefan schaal bayesian backfitting relevance vector machine proceedings twentyfirst international conference machine learning p31 july 0408 2004 banff alberta canada dmitry kropotov dmitry vetrov one method nondiagonal regularization sparse bayesian learning proceedings 24th international conference machine learning p457464 june 2024 2007 corvalis oregon wang titterington lack consistency mean field variational break bayes approximations state space models neural processing letters v20 n3 p151170 november 2004 stephen roberts evangelos roussos rizwan choudrey hierarchy priors wavelets structure signal modelling using ica signal processing v84 n2 p283297 february 2004 shinichi nakajima sumio watanabe variational bayes solution linear neural networks generalization performance neural computation v19 n4 p11121153 april 2007 r choudrey j roberts variational mixture bayesian independent component analyzers neural computation v15 n1 p213252 january perlich foster provost jeffrey simonoff tree induction vs logistic regression learningcurve analysis journal machine learning research 4 p211255 1212003