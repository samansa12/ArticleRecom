robust competitive clustering algorithm applications computer vision abstractthis paper addresses three major issues associated conventional partitional clustering namely sensitivity initialization difficulty determining number clusters sensitivity noise outliers proposed robust competitive agglomeration rca algorithm starts large number clusters reduce sensitivity initialization determines actual number clusters process competitive agglomeration noise immunity achieved incorporating concepts robust statistics algorithm rca assigns two different sets weights data point first set constrained weights represents degrees sharing used create competitive environment generate fuzzy partition data set second set corresponds robust weights used obtain robust estimates cluster prototypes choosing appropriate distance measure objective function rca used find unknown number clusters various shapes noisy data sets well fit unknown number parametric models simultaneously several examples clusteringmixture decomposition lineplane fitting segmentation range images estimation motion parameters multiple objects shown b introduction traditional clustering algorithms classified two main categories 1 hierarchical partitional hierarchical clustering number clusters need specified priori problems due initialization local minima arise however since hierarchical methods consider local neighbors step cannot incorporate priori knowledge global shape size clusters result cannot always separate overlapping clusters moreover hierarchical clustering static points committed given cluster early stages cannot move different cluster prototypebased partitional clustering algorithms divided two classes crisp hard clustering data point belongs one cluster fuzzy clustering every data point belongs every cluster certain degree fuzzy clustering algorithms deal overlapping cluster boundaries partitional algorithms dynamic points move one cluster another incorporate knowledge shape size clusters using appropriate prototypes distance measures algorithms extended detect lines planes circles ellipses curves surfaces 2 3 4 5 partitional approaches use alternating optimization technique whose iterative nature makes sensitive initialization susceptible local minima two major drawbacks partitional approach difficulty determining number clusters sensitivity noise outliers paper describe new approach called robust competitive agglomeration rca combines advantages hierarchical partitional clustering techniques 6 rca determines optimum number clusters via process competitive agglomeration knowledge global shape clusters incorporated via use prototypes overcome sensitivity outliers incorporate concepts robust statistics overlapping clusters handled use fuzzy memberships algorithm starts partitioning data set large number small clusters reduces sensitivity initialization algorithm progresses adjacent clusters compete points clusters lose competition gradually vanish however unlike traditional hierarchical clustering points move one cluster another rca uses two different sets weights memberships data point first one set probabilistically constrained memberships represent degrees sharing among clus ters constraint generates good partition introduces competition among clusters second set memberships unconstrained possibilistic 8 9 10 represents degrees typicality points respect clusters memberships used obtain robust estimates cluster prototypes organization rest paper follows section 2 briefly review related approaches section 3 present rca algorithm section 4 illustrate power flexibility rca incorporate various distance measures section 5 describe application rca segmentation range images section 6 formulate multiple model general linear regression algorithm based rca apply simultaneous estimation motion parameters multiple objects finally section 7 contains conclusions related work prototypebased partitional clustering algorithms kmeans fuzzy cmeans assume number clusters c known moreoever since use least squares criterion break easily e prototype parameter estimates arbitrarily wrong 11 presence noise goal clustering identify clusters data set implicitly assumes definition valid cluster thus idea break 11 extended clustering domain via use validity 12 number clusters c known ideal cluster breaks outliers form valid cluster cardinality higher cardinality min smallest good cluster gives us theoretical breakdown point n min n n number points data set recent solutions robust clustering c known divided two categories first category algorithms derived modifying objective function fcm 13 14 10 algorithms still sensitive initialization parameters 12 algorithms second category incorporate techniques robust statistics explicitly objective functions notable nonfuzzy clustering algorithms category kmedoids algorithm 15 bobrowski bezdek 16 proposed l 1 normbased fuzzy clustering algorithm also falls category however mention robustness paper variation algorithm motivated robustness found 17 another early fuzzy clustering algorithm rca based robust cprototypes rcp algorithm 18 uses mestimator 19 fuzzy trimmed c prototypes algorithm 20 uses least trimmed squares estimator 21 robust fuzzy c means rfcm algorithm 22 uses mestimator different way fuzzy c least median squares fclms algorithm 23 uses least median squares estimator 21 ftcp fclms achieve theoretical breakdown point n min n trivial modification objective functions however theory require exhaustive search reduce computational complexty heuristic search used 20and genetic search used 23 c unknown one way state clustering problem find valid clusters data set see 12 precise definition case ideal algorithm break identify good clusters correctly say exhaustive search addtion spurious ones alternative way state problem identify valid clusters formed good data case ideal algorithm break outliers form valid cluster giving us breakdown point n minval n n minval minimum number points required form valid cluster note given clustering algorithm may achieve theoretical breakdown points traditional approach determining c evaluate certain global validity measure cpartition range c values pick value c optimizes validity measure 25 1 26 27 alternative perform progressive clustering 28 27 5 clustering initially performed overspecified number clusters convergence spurious clusters eliminated compatible clusters merged good clusters identified another variation progressive clustering extracts one cluster time 29 30 approaches either computationally expensive rely validity measures global individual difficult devise robust approaches clustering c unknown treat data mixture components use robust estimator estimate parameters component generalized mve gmve 29 based minimum volume ellipsoid estimator 21 model fitting mf algorithm 31 possibilistic gaussian mixture decomposition pgmd algorithm 30 examples approaches data set classified set inliers ie points belonging cluster set outliers since set outliers includes points clusters proportion outliers high therefore even use robust estimaor theoreticalbest breakdown point 50 sufficient make algorithms highly robust overcome problem algorithms consider validity cluster formed inliers try extract every valid cluster data set order guarantee good solution gmve pgmd use many random initializations cooperative robust estimation cre 32 minpran 33 two robust modelfitting approaches fall category cre algorithm attempts overcome low breakdown point mestimators initializing large number hypotheses selecting subset initial hypotheses based minimum description length mdl criterion cre technique assumes scale 32 known minpran assumes outliers randomly distributed within dynamic range sensor noise outlier distribution known assumptions cre minpran easily extend clustering domain data expected multiple curves minpran seeks one curvesurface time 12 relation progressive approaches robust clustering algorithms explored clusters overlap idea extracting serial fashion work removing one cluster may partially destroy structure clusters might get bridging fits 33 fig 2a shows one noisy data set two crossing clusters algorithm propose designed overcome drawback moreover current algorithms use hard finite rejection 34 ie points within inlier bound given weight 1 points outside bound given weight zero means algorithms handle region doubt 21 well overcome problem use smooth 34 21 fuzzy rejection weight function drops zero gradually 3 robust competitive agglomeration rca algorith 31 algorithm development set n vectors ndimensional feature space coordinate axis labels ctuple prototypes characterizes one c clusters fi consists set parameters fuzzy cmeans algorithm 2 minimizes subject 1 2 ij represents distance feature vector x j prototype fi represents degree x j belongs cluster c thetan matrix called constrained fuzzy cpartition matrix 2 0 1 known fuzzifier jm essentially sum fuzzy intracluster distances monotonic tendency minimum value zero cn therefore useful automatic determination c overcome drawback add second regularization term prevent overfitting data set many prototypes resulting objective function ja minimized subject constraint 2 3 second term negative sum squares cardinalities clusters minimized cardinality one clusters n rest clusters empty proper choice ff balance two terms find solution c ja still robust since first term least squares objective function therefore robustify ja yield objective function proposed rca algorithm follows 4 ae robust loss function associated cluster w represents typicality point x j respect cluster function ae corresponds loss function used mestimators robust statistics w represents weight function equivalent westimator see 11 example particular choice robustification motivated need keep computational complexity low loss function reduces effect outliers first term weight function discounts outliers computing cardinalities selecting ij ff prudently jr used find compact clusters various types partitioning data set minimal number clusters minimize jr respect prototype parameters fix u set derivative jr respect fi zero ie 0 5 simplification 5 depends ae ij since distance measure application dependent return issue section 4 minimize 4 respect u subject 2 apply lagrange multipliers obtain fix b solve st st ae 2 st equations 7 2 represent set n thetac n linear equations n thetac n unknowns st computationally simple solution obtained computing using memberships previous iteration yields st 2ff theta st solving using 8 2 substituting 8 obtain following update equation membership u st feature point x cluster st st ff ae 2 st st st u rr st degree cluster shares x computed using robust distances st signed bias term depends difference robust cardinality cluster interest weighted average cardinalities kt c kt bias term u bias st positivenegative clusters cardinality higherlower average hence membership x clusters appreciatedepreciate feature point x j close one cluster say cluster far clusters n n j u bias implying competition hand point roughly equidistant several clusters clusters compete point based cardinality cardinality cluster drops threshold discard cluster update number clusters possible u ij become negative n small point x j close dense clusters case safe set u ij zero also possible u ij become larger 1 n large feature point x j close low cardinality clusters case clipped 1 practice customary optimization theory process agglomeration controlled ff slow beginning encourage formation small clusters increased gradually promote agglomeration iterations number clusters becomes close optimum value ff decay slowly allow algorithm converge therefore appropriate choice ff iteration k ij 10 ff j functions iteration number k superscript used ij w ij denote values iteration k gamma 1 good choice j initial value time constant k 0 iteration number j starts decrease examples presented paper except section 5 parameters finetuned best performance choose proper initialization values reasonable regardless application initialization issues discussed section 7 32 choice weight function curvesurface fitting linear regression reasonable assume residuals symmetric distribution zero therefore choose tukeys biweight function 11 given jr r ij stands normalized residual defined r c theta mad 1214 r ij residual j th point respect th cluster med median residuals th cluster mad median absolute deviations 11 th cluster words iteration data set x crisply partitioned med mad estimated cluster distances rather residuals used symmetric distribution assumption hold suggest monotonically nonincreasing weight function w 0 1 w 2 constant given choosing w results following weight function corresponding loss function shown 17 k integration constant used make ae reach maximum value choice ensures noise points membership value clusters fig 1 shows plot weight function corresponding loss function 14 16 17 c tuning constant 11 normally chosen 4 12 c large many outliers small nonzero weights thus affecting parameter estimates hand c small subset data points visible estimation process making convergence local minimum likely compromise start estimation process large value c decrease gradually function iteration number k ie c 0 12 c min 4 deltac1 rca algorithm summarized fix maximum number clusters initialize prototype parameters set repeat compute estimate using 15 update weights w ij using 13 16 update ffk using 10 update partition matrix u k using 9 compute robust cardinality n update number clusters c update tuning factor c using 18 update prototype parameters prototype parameters stabilize 4 examples distance measures mentioned section 31 rca used variety distance measures depending nature application section discuss distance measures suitable ellipsoidal clusters hyperplanes 41 detection ellipsoidal clusters detect ellipsoidal clusters data set use following distance measure 35 36 19 c center cluster fi c covariance matrix see 37 interpretation 2 cij using 5 shown update equations centers c covariance matrices c assume c reduces euclidean distance simplified version used clusters expected spherical fig 3 illustrates rca using 2 cij fig 3a shows synthetic gaussian mixture consisting 4 clusters various sizes orientations uniformly distributed noise constituting 40 total points added data set fig 3b shows initial 20 prototypes superimposed data set signs indicate cluster centers ellipses enclose points mahalanobis distance less 9 prototypes obtained running gk algorithm 36 5 iterations 2 iterations rca 9 empty clusters discarded see fig 3c number clusters reduced 6 3 iterations 4 4 iterations final result total 10 iterations shown fig 3d illustrate ability rca handle nonuniform noise fig 4 shows result rca data set containing gaussian clusters roughly 25 noise illustrate ability rca algorithm detect overlapping clusters fig 2b show result rca data set fig 2a algorithm converged 10 iterations 42 detection linear clusters detect clusters resemble lines planes use generalization distance measure proposed 3 2 distance given e ik k th unit eigenvector covariance matrix c eigenvectors assumed arranged ascending order corresponding eigenvalues value ik 22 chosen dynamically every iteration k th eigenvalue c shown distance measure 22 update equations c c given 20 21 respectively fig 5a shows image consisting 10 line segments noisy background fig 5b shows 20 initial prototypes obtained running afc algorithm 3 5 iterations 2 iterations rca number clusters drops 15 shown fig 5c 9 iterations number clusters reduces optimal number algorithm converges total 12 iterations final result shown fig 5d 5 application range image segmentation 51 planar range image segmentation since planar surface patches modeled flat ellipsoids distance measure 2 19 also used find optimal number planar patches avoid missing tiny surfaces start dividing image nonoverlapping windows sizes w thetaw apply rca window estimate optimal number planar patches within window finally pool resulting say prototypes initialize rca algorithm cm nature 2 cij planar surfaces nonconvex shapes may approximated several planar patches several spatially disconnected planar patches may approximated single cluster therefore rca converges merge compatible clusters 27 adjacent perform connected component labeling cluster assign different labels disjoint regions rcabased algorithm tested two standard data sets abw data set perceptron data set created benchmarking range image segmentation algorithms 38 set contains 40 images size 512theta512 randomly divided 10image training set 30image testing set use performance measures developed hoover et al 38 evaluate performance rca measures rely comparing machine segmented ms image ground truth gt image classify regions one 5 categories correct detection oversegmentation segmentation missed noise accuracy segmentation quatified computing average standard deviation differences angles made pairs adjacent regions instances correct detection ms gt images data sets performance measures used 38 compare university south florida usf university edinburgh ue washington state university wsu university bern ub segmentation algorithms reproduce set experiments include rca algorithm comparison training phase finetuned parameters rca follows window size used initialization w initial number prototypes window c j see 10 parameters optimal abw perceptron data sets since perceptron data noisy use c 4 abw data c 8 also reduce computations images subsampled x directions factor 3 parameters fixed testing phase fig 6a shows intensity image one abw test images segmented range image shown fig 6b shaded gray regions correspond background points ignored segmentation fig 7 shows example perceptron data set 38 compute performance metrics five segmentation algorithms varying compare tool tolerance 51 95 due space limitation show plots correct detection measure fig 8 performance measures using 80 compare tolerance five segmenters listed table 1 abw data table 2 perceptron data rca compares well best segmenters among 5 planar surface segmenters comparison ue wsu rca capability segment curved surfaces rca additional advantage handle irregularly spaced sparse data well eg range data computed stereo methods 52 quadric range image segmentation let th prototype fi represented parameter vector p define equation quadric surface p 3d point since exact distance point x j quadric surface fi closedform expression use approximate distance 39 40 given jacobian q evaluated x j avoid allzero trivial solution following constraint may chosen 39 starting 5 shown use aij leads solution p based following generalized eigenvector problem f g obtain reliable initialization divide image small nonoverlapping win dows apply rca window c1 finally pool resulting prototype parameters initialize rca algorithm initially might several initial prototypes corresponding surface however due competition one surfaces survive examples used section consist 240theta240 real synthetic range images 1 sampling rate 3 x directions used reduce computations used estimate initial prototypes fig 9a shows synthetic range image plastic pipe fig 9b shows initial 36 surface patches patches generated assigning point nearest prototype fig 9c shows final results surface displayed different gray value boundaries shown black fig 10a shows real range image three plastic pipes different sizes orientations final results rca algorithm consisting correctly identified surfaces shown fig 10b test robustness rca gaussian noise oe4 added image fig 9a 10 data points randomly altered become outliers images obtained michigan state university washington state university via anonymous ftp results shown fig 11 noise points ie points zero weight w ij clusters shown black 6 estimation multiple motion groups segmen tation section show rca used perform multiple model linear regression apply estimation motion parameters multiple motion groups 61 general linear regression general linear regression glr 41 solving set homogeneous equations motion parameters written design matrix x parameter vector residual vector since system homogeneous fix fi reformulate glr model ndimensional vector every component equal 1 solved least squares minimization min fi solution least squares sensitive noise alternative weighted least squares min fi solution fi data set contains multiple models glr model must applied repetetively extract one model time approach computationally expensive requires models well separated needs high breakdown estimator since extracting th model models considered outliers sensitive initialization deal problems propose multiplemodel general linear regression mmglr method allows simultaneous estimation unknown number models 62 multiplemodel general linear regression let th model parameter vector fi represented residual corresponding j th data vector th model mmgrl minimizes 4 2 ij replaced r 2 subject constraint 2 solving 5 corresponding situation leads resulting update equation parameters linear regression customary use studentized residuals r h jj j th diagonal element hat matrix huang et al 41 showed corresponding hat matrix glr h extend principle mmglr compute c hat matrices e one per model residuals normalized r jj however normalization introduces bias towards noise points w ij 0 points belonging models case h jj 0 hence normalization takes place also residuals inflated points typical th model since divided factor smaller one therefore modify normalization process follows r otherwise h words points known atypical th model forced receive maximum possible inflation factor mmglr used estimate motion parameters multiple objects scene instantaneous velocity pt point located surface translating object rotating instantaneous angular velocity characterized vector involving translation let xt 2d prespective projection pt onto image plane z1 let ut vt denote projective instantaneous velocity motion estimation consists solving k using set n observations corresponding done solving n h determined motion parameters k easily obtained 42 since h 9dimensional represents set homogeneous equations need 8 observations solve optical flow 42 scene consists c independently moving objects motion object characterized different vector h situation need solve ah solves set equations x fi correspond h respectively finds c automatically mmglr requires overspecified number c initial parameter estimates obtain one estimates solving randomly selected subset 8 observa tions c estimates pooled together initialize mmglr algorithm ensure reliable result initial number models c needs high however since c decreases drastically subsequent iterations method still efficient since mmglr allows points move one model another since fuzzy rejection allows points change inliers outliers vice versa smoothly afford use smaller number initializations algorithms based hard rejection experiments described subsection use c50 fig 12a shows synthetic 3d scene consisting 4 touching rigid objects undergoing motion different rotational translational velocities fig 12a displays subsampled scaled true optic flow field contaminated optic flow field gaussian noise snr70 additionally altered 20 observations randomly make outliers resulting optic flow field shown fig 12b mmglr succeeds determining correct number motion groups scene also estimates motion parameters accurately shown table 3 fig 12c shows segmented optic flow field motion group represented different symbol correctly identified outliers points zero weight w ij models shown black dots fig 12c recovered optic flow field shown fig 12d figs 13a 13b show two 512theta512 subimages 13 th 14 th frames motion sequence 43 containing moving truck experiment background motion due camera panning treated another motion group create multiple motion scenario selected target points vehicle another 30 points background matches 60 points computed using robust matching algorithm 44 verified manually illustrate robustness mmglr added another 10 target points erroneous matches 70 points marked figs 13a 13b target points matches first converted pixel coordinates image coordinates calibrated 43 finally target points integrated form mixture data set fx image coordinates th target point 13 th frame displacement vector ground truth vehicle motion unknown also since rotation angle truck small 5 could estimated reliably using twoview point correspondence threeview line correspondence algorithms 45 since testing robustness mmglr ability detect multiple models performance linear optic flow algorithm compare results obtained linear optic flow algorithm supplied correct data subset motion see table 4 mmglr first run 60 good target points added outliers cases algorithm able detect correct number motion groups 2 estimate parameters correctly fig 14 shows partition optic flow field two motion groups detected outliers denoted different symbols 7 discussion conclusions 71 general comments rca attempt addressing three main issues partitional clustering algorithms difficulty determining number clusters sensitivity initialization sensitivity outliers without sacrificing computational efficiency rca minimizes fuzzy objective function order handle overlapping clusters constrained fuzzy memberships used create competitive environment promotes growth good clusters possibilistic memberships 10are used obtain robust estimates prototype parame ters concepts robust statistics incorporated rca make insensitive outliers handle region doubt reduce sensitivity initialization rca uses soft finite rejection agglomerative property makes relatively insensitive initialization local minima effects using suitable distance measures apply algorithm solve many computer vision problems choice ff 10 quite critical algorithm however ff chosen trial error produce stable results given application variety examples presented paper show possible rca provide robust estimates prototype parameters even clusters vary significantly size shape data set contaminated 72 computational complexity rca algorithm computational complexity similar fcm 2 onc iteration n number data points c number clusters however additional time required estimate weight function wd 2 requires us compute median squared distances twice first compute median compute mad median data set computed iteratively using procedure converges olog n passes data set since distribution squared distances change significantly one iteration procedure converges even faster median previous iteration used initialize computation median current iteration thus overall complexity estimated log n nc per iteration onklogn c k number iterations noted value c varies c max c final except application motion analysis cases use standard algorithm fcm initialize rca therefore initialization overhead onkc max k small 5 integer 73 breakdown issues discussed section 2 c known breakdown point n min n c unknown breakdown either undefined n minv al n results derived use validity ideal clustering algorithm would use validity measure expensive exhaustive search achieve level robustness 12 however validity measures hard define practice unless distribution good points known moreover deviations assumed distribution occur widely varying degrees real applications hard choose thresholds optimal values vary widely among different data sets even among clusters data set rca general purpose algorithm attempts achieve robustness reasonable computational complexity rationale behind choice mestimator robustify rca choice limits breakdown point rca 1 p dimensionality parameter vector estimated however since rca starts large mber initial prototypes possible increase robustness certain conditions rca uses initial prototypes generate partition algorithm consists updating weight function component partition updating memberships finally updating prototypes process repeated convergence since weight function uses median mad tolerate 50 noise points within component provided starts good initialization let c actual clusters let good points k th actual cluster given label k let noise points labeled 0 let hard partition corresponding c max initial prototypes labeled follows given component noise points labeled 0 otherwise labeled label majority good points component let p max denote largest component label rca algorithm give robust results require initialization satisfies following conditions exists least one component label prototype corresponding p max good point th actual cluster iii largest component labeled 0 smaller p contains 50 points labeled since cluster region definition denser noise region using sufficiently large number prototypes usually possible achieve initialization meet conditions practice initial prototypes placed cluster region naturally larger cardinalities noise region smaller ones conditions iiv need satisfied following iterations well guarantee algorithm converge correct result however since cardinalities replaced robust cardinalities subsequent iterations becomes easier satisfy conditions components coalesce form final result noise point crisply assigned one components computing weight function worst case noise points assigned smallest cluster therefore conditions iii iv translate requirement number noise points smaller cardinality smallest cluster thus iiv satisfied rca achieve theoretical breakdown point similar discussion applies nonpoint prototypes well minor modifications case initial prototype generated n data points n number parameters prototype 74 initialization issues discussion clear initialization plays important role rca algorithm initialization procedure necessarily varies type prototypes used distance measure used type data finally application outline guidelines initialization compute theoretical value initial number clusters c max follows let c exp number actual clusters expected data set let n denote cardinality cluster let n number points required generate prototype randomly pick n points generate prototype probability p pick c exp good prototypes one cluster given qcexp selection repeated k times probability one selections generates good prototypes c exp clusters given p given value p g compute value k log 1gammap e c max estimated c value c max grows exponentially c exp n therefore unrealistic practice existing clustering algorithm fcm 2 gk 36 afc 3 used initialization end initialization although c max prototypes expected good assume c exp clusters fairly high probability p init represented one c max initial prototypes example consider case finding lines 2d data set ie 2 n total points nn possible ways pick pair points hence nn possible random initializations line however initializations involve points far away constitute poor initializations hand algorithm afc use nearby points probability two nearby points belong line high data set image dividing image small windows applying conventional clustering algorithm suitable number clusters window dramatically increase value p init probability c exp clusters represented initialization given case much smaller number initial clusters suffice based discussion suggest following rules thumb general clus choose c max n 10lambdan use simple clustering algorithm fcm generate initial prototypes since good points definition dense regions initialization expected meet conditions discussed previous subsection case plane surface fitting handled dividing image small windows applying suitable clustering algorithm window case regression initialization techniques longer applicable hence use random sampling procedure generate prototypes randomness require larger value c max applications set c max n acknowledgments authors would like thank anonymous reviewers valuable comments work partially supported grant office naval research n00014 9610439 r algorithms clustering data pattern recognition fuzzy objective function algorithms use adaptive fuzzy clustering algorithm detect lines digital images adaptive fuzzy cshells clustering detection ellipses fuzzy possibilistic shell clustering algorithms application boundary detection surface approximation robust clustering algorithm based competitive agglomeration soft rejection outliers clustering competitive agglomeration fuzzy sets basis theory possibility possibility theory approach computerized processing uncertainty possibilistic approach clustering robust statistics approach based influence functions fuzzy clustering robust estimation finding groups data introduction cluster analysis cmeans clustering l 1 l 1 norms fuzzy median fuzzy mad robust clustering algorithm based estimator robust statistics john wiley fuzzy robust formulations maximum likelihoodbased gaussian mixture decomposition genetic algorithm robust clustering based fuzzy least median squares criterion new indices cluster validity unsupervised optimal fuzzy clustering fitting unknown number lines planes image data compatible cluster merging progressive fuzzy clustering algorithms characteristic shape recognition robust clustering applications computer vision gaussian mixture density modeling decomposition applications highly robust estimator partially likelihood function modeling application computer vision cooperative robust estimation using layers support minpran new robust estimator computer vision fuzzy clustering fuzzy covariance matrix fuzzy possibilistic clustering methods computer vision experimental comparison range image segmentation algorithms estimation planar curves surfaces nonplanar space curves defined implicit equations application edge range image segmentation comparison fuzzy shellclustering methods detection ellipses optic flow field segmentation motion estimation using robust genetic partitioning algorithm simplified linear optic flowmotion algorithm sequence stereo image data moving vehicle outdoor scene robust technique matching two uncalibrated images recovery unknown epipolar geometry vehicletype motion estimation multiframe images tr ctr ujjwal maulik sanghamitra bandyopadhyay performance evaluation clustering algorithms validity indices ieee transactions pattern analysis machine intelligence v24 n12 p16501654 december 2002 raffaele cappelli dario maio davide maltoni multispace kl pattern representation classification ieee transactions pattern analysis machine intelligence v23 n9 p977996 september 2001 sanghamitra bandyopadhyay ujjwal maulik evolutionary technique based kmeans algorithm optimal clustering rn information sciencesapplications international journal v146 n14 p221237 october 2002 ana l n fred anil k jain combining multiple clusterings using evidence accumulation ieee transactions pattern analysis machine intelligence v27 n6 p835850 june 2005 scionti j p lanslots stabilisation diagrams pole identification using fuzzy clustering techniques advances engineering software v36 n1112 p768779 november 2005 miinshen yang kuolung wu similaritybased robust clustering method ieee transactions pattern analysis machine intelligence v26 n4 p434448 april 2004 chengfa tsai chunwei tsai hanchang wu tzer yang acodf novel data clustering approach data mining large databases journal systems software v73 n1 p133145 september 2004 bogdan gabrys agglomerative learning algorithms general fuzzy minmax neural network journal vlsi signal processing systems v32 n12 p6782 augustseptember 2002 k qin p n suganthan robust growing neural gas algorithm application cluster analysis neural networks v17 n89 p11351148 octobernovember 2004 ana l n fred jos n leito new cluster isolation criterion based dissimilarity increments ieee transactions pattern analysis machine intelligence v25 n8 p944958 august gianluca pignalberi rita cucchiara luigi cinque stefano levialdi tuning range image segmentation genetic algorithm eurasip journal applied signal processing v2003 n1 p780790 january geovany araujo borges mariejos aldon line extraction 2d range images mobile robotics journal intelligent robotic systems v40 n3 p267297 july 2004 jun liu jim p lee lingjie li zhiquan luo k max wong online clustering algorithms radar emitter classification ieee transactions pattern analysis machine intelligence v27 n8 p11851196 august 2005 martin h c law mario figueiredo anil k jain simultaneous feature selection clustering using mixture models ieee transactions pattern analysis machine intelligence v26 n9 p11541166 september 2004 kuolung wu miinshen yang alternative learning vector quantization pattern recognition v39 n3 p351362 march 2006 kuolung wu miinshen yang mean shiftbased clustering pattern recognition v40 n11 p30353052 november 2007 ozy sjahputera james keller j wade davis kristen h taylor farahnaz rahmatpanah huidong shi derek anderson samuel n blisard robert h luke mihail popescu gerald c arthur charles w caldwell relational analysis cpg islands methylation gene expression human lymphomas using possibilistic cmeans clustering modified cluster fuzzy density ieeeacm transactions computational biology bioinformatics tcbb v4 n2 p176189 april 2007 mohamed ben hadj rhouma hichem frigui selforganization pulsecoupled oscillators application clustering ieee transactions pattern analysis machine intelligence v23 n2 p180195 february 2001 jian yu general cmeans clustering model ieee transactions pattern analysis machine intelligence v27 n8 p11971211 august 2005 anil k jain robert p w duin jianchang mao statistical pattern recognition review ieee transactions pattern analysis machine intelligence v22 n1 p437 january 2000 jasit suri sameer singh k setarehdan rakesh sharma keir bovis dorin comaniciu laura reden note future research segmentation techniques applied neurology cardiology mammography pathology advanced algorithmic approaches medical image segmentation stateoftheart application cardiology neurology mammography pathology springerverlag new york inc new york ny 2001