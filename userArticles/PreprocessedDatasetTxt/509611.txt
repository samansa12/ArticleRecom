portable performance data parallel languages portable program executes different platforms yields consistent performance focus portability paper presents indepth study performance three nas benchmarks ep mg ft compiled three commercial hpf compilers apr pgi ibm ibm sp2 benchmark evaluated two versions using loops using f90 constructs andor hpfs forall statement baseline comparison provided versions benchmarks written fortranmpi zpl data parallel language developed university washingtonwhile f90forall programs achieve scalable performance compilers results indicate considerable portability problem hpf programs two sources problem identified first fortrans semantics require extensive analysis optimization arrive parallel program therefore relying compilers capability alone leads unpredictable performance second wide differences parallelization strategies used compiler may require hpf program customized particular compiler improving compiler optimizations may help reduce performance variations results suggest foremost criteria portability concise performance model compiler must adhere users rely b introduction portability defined ability use program different platforms achieve consistent performance developing parallel program portable scalable well recognized challenging endeavor however difficulty necessarily intrinsic property parallel computing assertion especially clear case data parallel algorithms provide abundant parallelism tend involve computation regular data parallel model adequate general parallel programming however simplicity coupled prevalence data parallel problems scientific applications research supported ibm resident study program darpa grant n0001492j4041 motivated development many data parallel languages goal simplifying programming achieving scalable portable performance languages high performance fortran 10 constitutes widespread effort involving large consortium companies universities one hpfs distinctions first parallel language recognized standard indeed hpf regarded integration several similar data parallel languages including fortran vienna fortran cm fortran 6 14 22 attractions hpf manifold first use fortran base language promises quick user acceptance since language well established target community second use directives parallelize sequential programs implies ease programming since directives added incrementally without affecting programs correctness particular cases compiler may even able parallelize program without user assistance hand hpf also potential disadvantages first extension sequential language likely inherit language features either incompatible parallelization difficult compiler analyze second portability program must affected differences technology compilers machines since principal purpose creating standard ensure programs portable hpfs design presents potential conflicts goal portability instance hiding aspects communication programmer convenient forces user rely completely compiler generating efficient communication differences compilers always present however maintain program portability language differences must force users make program modifications accommodate specific compiler words user able use compiler develop program scales option migrating different machine compiler better scalar performance requires tight coupling language specification compiler sense compiler implementations must provide consistent behavior abstractions provided language end language specification must serve consistent contract compiler programmer call contract performance model language 18 robust performance model dual effect program performance 1 predictable user 2 portable across different platforms focus portability issue study indepth performance three nas benchmarks compiled three commercial hpf compilers ibm sp2 benchmarks embarrassingly parallel ep multigrid mg fourier transform ft hpf compilers include applied parallel research portland group ibm evaluate effect data dependences compiler analysis consider two versions benchmark one programmed using loops second using f90 constructs andor hpfs forall statement comparison also consider performance benchmark written mpi zpl 16 data parallel language developed university washington since message passing programs yield scalable performance convenient mpi results represent level performance hpf programs use point reference motivation including zpl results follows zpl data parallel language developed first principles lack parent language allows zpl introduce new language constructs incorporate robust performance model creating concrete delineation parallel sequential execution consequently programming model presented user clear compiler relatively unhindered artificial dependencies complex interactions language features one may expect easier develop zpl compiler write zpl program scales well naturally downside designing new language without legacy challenge gaining user acceptance study zpl measurement gives indication whether consistent scalable performance achieved compiler hampered language features unrelated parallel computation results show programs scale well using particular hpf compiler may perform similarly different compiler indicating lack portability f90forall programs achieve scalable performance results uniform programs results suggest fortrans sequential nature leads considerable difficulties compil ers analysis optimization communication analyzing detail implementations hpf compilers find wide differences parallelization strategies varying degrees success contribute portability problem hpf programs improving compiler optimizations may help reduce performance variations clear robust solution require mature compiler technology results suggest foremost criteria portability concise performance model compiler must adhere users rely performance model serve effective contract users compiler related work apr published performance hpf compiler suite hpf pro grams along detailed descriptions program restructuring process using apr forge tool improve codes 3 11 programs well tuned apr compiler many cases rely use aprspecific directives rather standard hpf directives although approach apr advocates program development followed profilerbased program restructuring successful instances resulting programs may portable respect performance particularly cases employ apr directives fore believe suite apr benchmarks well suited evaluating hpf compilers general similarly papers vendors describing individual hpf compilers typically show performance numbers however remains difficult make comparisons across compilers 8 12 13 lin et al used apr benchmark suite compare performance zpl versions programs corresponding hpf performance published apr found zpl generally outperforms hpf 17 however without access apr compiler time detailed analysis possible limiting comparison aggregate timings paper makes following contributions 1 indepth comparison analysis performance hpf programs three current hpf compilers alternative approaches mpi zpl 2 comparison loop f90 array syntax forall construct 3 assessment parallel programming model presented hpf remainder paper organized follows section 2 describes methodology study including description algorithms benchmark implementations section 3 examine analyze benchmarks performance detailing communication generated implementation quantifying effects data dependences hpf programs section 4 provides observations conclusions 21 zpl overview zpl array language designed university washington expressly parallel ex ecution context paper serves two purposes first sets bound performance expected high level data parallel language extension existing sequential language second illustrates importance performance model parallel language zpl implicitly parallel ie directives concurrency derived entirely semantics array operations array decompositions specified run time partition arrays either 1d 2d blocks processors perform computations values scalars replicated processors kept coherent redundantly computing scalar computations zpl introduces new abstraction called region used allocate distributed arrays specify distributed computation zpl provides full complement operations define regions relative eg east r refer adjacent elements eg awest perform full partial prefix operations eg big max express strided computations establish boundary conditions eg wrap accomplish powerful operations eg flooding zpl also contains standard operators data types control structures using syntax similar modula2 22 benchmark selection emphasize portability issue establish criteria 1 benchmarks derived independent source insure objectivity 2 message passing version included study establish target performance 3 hpf separate versions employ f77 loop f90forall significant difference two types constructs 4 algorithm parallel algorithmic differences versions benchmark 5 tuning must adhere language specification rather specific compiler capability 6 support hpf features uniform benchmarks require feature supported hpf compilers following criteria proves challenging given disparate benchmark availabil ity nas benchmark version 10 npb1 implemented computer vendors intended measure best performance possible parallel machine without regard portability available sources npb1 generally sequential implementations although valid hpf programs sequential nature algorithms may difficult compilers parallelize may reflect natural approach parallel programming instance programmer may simply choose specific parallel algorithm implement hpf nas version 21 npb21 intended measure portable performance better choice since programs implement inherently parallel algorithms use mpi interface compilers study npb21 contains 7 benchmarks 1 ideally included study unfortunately portable hpf version benchmarks available severely limiting independent comparison apr hpf vendors publish benchmarks used acquire performance measurements benchmarks generally tuned specific compiler portable limitation forces us carefully derive hpf versions npb21 sources focus portability avoiding external effects algorithmic differences among benchmarks cg available npb21 sp bt lu included study require block cyclic 3d data distributions supported hpf compilers limitations prevent benchmarks implemented hpf zpl however implementations algorithmic difference cannot factored performance leaves ft mg potential candidates fortunately ep definition highly parallel therefore sequential implementation trivially parallelized 23 benchmark implementation hpf implementations derived reverse engineering mpi programs communication calls removed local loop bounds replaced global loop bounds parallelize programs hpf directives added recreate data partitioning mpi versions hpf compilers thus presented program fully data parallel sequential ready parallelized conceptually task compilers repartition problem specified hpf directives regenerate communication benchmarks chose require basic block distribution supported three compilers use basic hpf intrinsics therefore stress compilers without exceeding capability implementations zpl language derived source hpf implementations following manner sequential computation translated directly fortran corresponding zpl syntax parallel execution expressed using zpls parallel constructs one added recently 231 ep benchmark ep generates n pairs pseudorandom floating point values interval 01 according specified algorithm redistributes value x j j onto range 11 scaling 2x pair tested condition true independent gaussian deviates computed new pair x tested see falls within one 10 square annuli total count tabulated annulus l maxjxj jy pseudorandom numbers generated according following linear congruential recursion values pair consecutive values recursion scale 01 range value x k divided 2 46 computation pair gaussian deviates proceed independently processor would maintain counts gaussian deviates communicate end obtain global sum random number generation however presents challenge two ways compute random value 1 x k computed quickly preceding value x using one multiplication one mod operation leading complexity however major drawback true data dependence value x 2 x k computed independently using k defined values x 0 result overall complexity 2 fortunately property mod operation allows x k computed olog steps using binary exponentiation algorithm 4 goal balance method 1 2 achieve parallelism maintaining cost ep available npb21 suite use implementation provided apr loop version version structured achieve balance 1 2 batching random values generated one sequential batch time saved seed batch computed using expensive method 2 remaining values computed using less expensive method 1 loop iterates compute number batches required constitutes opportunity parallel execution f90forall version derived loop version following modifications ffl variables main loop cause output dependence expanded arrays size loop iteration words output dependence eliminated essentially renaming variables computation expressed fully data parallel manner since iteration count number sequential batches expansion excessive ffl directives added partition arrays onto 1d processor grid ffl loop final summation also recoded using hpf reduction intrinsic complication arises involving subroutine call within forall loop must free side effects order loop distributed slight code rearrangement done remove side effect original subroutine pure directives added assert freedom side effects zpl version translated straightforward manner loop version notable difference use zpl region construct express independent batch computations 232 mg multigrid interesting several reasons first illustrates need data parallel languages hpf zpl npb21 implementation contains 700 lines code communication 30 program eliminated program written data parallel language second since main computation 27points stencil reference pattern requires communication simply shift constant results simple neighbor exchange processor grid compilers zpl hpf recognize pattern well employ optimizations message vectorization storage preallocation nonlocal data 3 8 9 12 therefore although benchmark rather complex initial indication hpf zpl able produce efficient parallel programs benchmark vcycle multigrid algorithm computing approximate solution discrete poisson problem r 2 laplacian operator r 2 algorithm consists 4 iterations following three steps residual correction apply correction trilinear finite element discretization laplace operator r 2 k vcycle multigrid operator defined npb1 benchmark specification 1 4 algorithm implemented npb21 version consists three phases first phase computes residual second phase set steps applies k operator compute correction last phase applies correction hpf loop version derived npb21 implementation follows ffl mpi calls removed ffl local loop bounds replaced global bounds ffl use common block storage hold set hierarchical arrays different sizes incompatible hpf therefore arrays renamed declared explicitly ffl hpf directives added partition arrays onto 3d processor grid array distribution maintained across subroutine calls using transcriptive directives prevent unnecessary redistribution hpf f90forall version requires additional step rewriting data parallel loops f90 syntax zpl version similar structure hpf f90forall version notable difference use strided region express hierarchy 3d grids strided region sparse index set data declared computation specified 233 ft consider partial differential equation point x 3d space ffit ft benchmark solves pde 1 computing forward 3d fourier transform ux 0 2 multiplying result set exponential values 3 computing inverse 3d fourier transform problem statement requires 6 solutions therefore benchmark consists 1 forward fft 6 pairs dot products inverse ffts npb21 implementation follows standard parallelization scheme 5 2 3d fft computation consists traversing applying 1d fft along three dimensions 3d array partitioned along third dimension allow processor independently carry 1d fft along first second dimension array transposed enable traversal third dimension transpose operation constitutes communication program program requires moving third dimension first dimension transpose memory stride favorable 1d fft therefore hpf redistribute function alone sufficient 2 hpf loop implementation derived following modifications 1 hpf directives added distribute arrays along appropriate dimension tran scriptive directives used subroutine boundaries prevent unnecessary redistribution 2 communication transpose step replaced global assignment statement 3 scratch array recast arrays different ranks sizes subroutines replaced multiple arrays constant rank size although passing array section formal argument legitimate hpf hpf compilers difficulty managing array sections data distribution specifies partition processor mapping memory layout hpf f90forall version requires additional step rewriting data parallel loops f90 syntax zpl implementation allocates 3d arrays regions 2d arrays transpose operation realized zpl permute operator 24 platform program portability evaluated across multiple platforms different compilers machines study elect factor difference due machine architecture focus compiler difference although machine architecture sets upper bound possible performance language compiler determine achievable performance targeted parallel platform ibm sp2 cornell theory center hpf compilers used study include ffl portland group pghpf version 21 ffl ibm xlhpf version 10 applied parallel research xhpf version 20 compilers generate mpi calls communication use mpi library ensuring communication fabric identical measurements pgi apr hpf compilers generate intermediate fortran code processed standard ibm fortran compiler xlf ibm hpf compiler generates machine code directly otherwise based xlf compiler zpl compiler generates intermediate c code measurements use compiler options system environment nas apr specified publications using sp2 wide nodes spotchecks confirmed published nas apr performances reproduced 3 parallel performance section examine performance programs figure 1 shows aggregate timing versions mpi hpf zpl small large problem size class class notes ffl execution time may excessive depending success compilers first examine small problem size class programs reasonable performance speedup large problem size class ffl time axis uses log scale wide performance spread ffl ep mpi version used class f90forall version shown ffl mg class apr f90forall version scale included ffl ft class one data point available ibm f90forall version processors timesecs b ep class timesecs ep class processors timesecs c mg class processors timesecs mg class timesecs processors timesecs f ft class mpi aprdo ibmdo pgido zpl aprf90 ibmf90 pgif90 figure 1 performance ep mg ft see notes section 31 nas ep benchmark figure 1a first surprising observation ibm pgi compilers achieve speedup hpf loop version although apr compiler produces program scales well recall ep loop version apr suite inspecting code reveals distribution directives specified arrays resulting default data distribution although default distribution implementation dependent conventional choice replicate array ibm pgi compilers distribute computation strictly ownercomputes rule therefore order program parallelized data structures must distributed since arrays ep replicated default computation partitioned among processors processor executes full program achieves speedup contrast apr parallelization strategy strictly adhere ownercomputes rule allows main loop partitioned despite fact none arrays within loop distributed note hpf language specification specify default distribution data partitioning scheme computation omission likely intended maximize opportunity compiler optimize however observation ep suggests different schemes adopted compilers may result portability problem hpf programs directives inserted distribute arrays found main array ep intended hold pseudorandom values generated sequentially therefore exists true dependence loop computing values array distributed compiler adjust loop bounds local partition computation serialized hpf f90forall version corrects problem explicitly distributing arrays ibm pgi compilers able parallelize class performance figure 1b shows compilers achieve expected linear speedup however expanding arrays express computation data parallel form seems introduce overhead degrade scalar performance possible advanced compiler optimizations loop fusion array contraction remove overhead optimizations either available successful case zpl version scales linearly expected scalar performance slightly better apr version 32 nas mg benchmark compared ep mg allows rigorous test languages compilers first discuss performance class figure 1c p1 column shows considerable variation scalar performance versions showing overhead 1 2 orders magnitude mpi performance base cases original mpi program zpl version scale well zpl compiler partitions problem straightforward manner according region strided region semantics communication vectorized little effort scalar performance however show 6x overhead compared mpi version hpf loop version clearly scale hpf compiler explained pgi compiler performs poorly vectorizing communication computation expressed loops communication calls tend remain innermost loop resulting large number small messages generated addition program uses guards within loop instead adjusting loop bound apr compiler supports 1d processor grid therefore 3d distribution specified hpf directives collapsed default 1d distribution limitation affects asymptotic speedup necessarily limit parallelization 27point stencil computation one subroutine compiler detects interprocedural analysis alias two formal arguments constitutes inhibitor loop parallelization within subroutine however analysis go detect index expressions array references dependence actually exists major loops program apr compiler correctly partitions computation along distributed array dimension generates conservative communication loop obtain latest value rhs loop update lhs result performance degrades number processors ibm compiler parallelize detects output dependence number variables although arrays replicated case compiler appears overly conversative maintaining consistency replicated variables loops parallelize contain statement independent directive treated differently compilers pgi compiler interprets directive literally parallelizes loop directed ibm compiler hand ensures correctness performing rigorous dependence check nevertheless detected dependence parallelize loop hpf f90forall version ibm pgi compilers successful ibm compiler performance scalability approach zpls pgi compiler experiences little problem vectorizing communication indeed scalar performance exceeds ibms apr compiler result slowdown achieve speedup either partitions computation f90forall version similarly loop version able reduce amount communication continues limited 1d distribution well alias problem one subroutine note version mg apr suite employs aprs directives suppress unnecessary communication directives used study part hpf worth noting possible use aprs tools analyze program manually insert aprs directives improve speedup apr compiler given loop version fails scale compiler one may conjecture whether program may written differently aid compilers specific causes compiler described suggest apr compiler would successful aprs directives used pgi compiler may benefit hpf independent directive ibm compiler would require actual removal data dependences therefore appear single solution portable across compilers 33 nas ft benchmark ft presents different challenge hpf compilers terms reference pattern ft consists dot product fft butterfly pattern former requires communication readily parallelized compilers latter index expression far complex optimize communication fortunately index variable limited one dimension time therefore task compiler partition computation along appropriate dimensions intended data distribution 1d thus within capability apr compiler figure 1e shows full set performance results small problem size mg mpi zpl versions scale well scalar performance data parallel implementations shows overhead 1 2 orders magnitude mpi implementation hpf loop version apr compiler exhibits problem mg generates conservative communication many loops addition apr compiler choose correct loop parallelize discrepancy arises aprs strategy choose partitioning based array references within loop case main computation thus array references packaged subroutine called loop intention loop parallelized subroutine operates local data compiler proceeds analyze loops subroutine 1d fft finds loops parallelizable pgi compiler also generates suboptimal communication although principal limitation vectorizing messages ibm compiler parallelize assignments replicated variables hpf f90forall version requires considerable experimentation code restructuring arrive version accepted compilers partly differences supported features among compilers partly nested subroutines structure original program hpf compilers achieve speedup varying degrees apr particularly successful since principal parallel loop moved innermost subroutine scalar performance approaches mpi performance although communication overhead limits speedup pgi shows good speedup ibms speedup limited 34 communication communication generated compilers useful indication effectiveness parallelized programs versions benchmarks scale table 1 shows total number mpi message passing calls differences communication scheme employed compiler apr pgi compilers use generic send receive ibm compiler also uses nonblocking calls collective communication may ramifications portability compiler platforms zpl compiler uses nonblocking mpi calls overlap computation communication well mpi collective communication benchmark version pointtopoint collective type mpi calls ep class allreduce barrier ibm f90 70 120 send recv bcast mg class mpi 2736 40 send irecv allreduce barrier zpl 9504 56 isend recv barrier apr f90 126775 8 send recv barrier bcast ft class mpi 0 104 alltoall reduce apr f90 58877 8 send recv barrier ibm f90 728 258048 send irecv bcast table 1 dynamic communication statistics ep class mg class ft class p8 35 data dependences hpf compilers derive parallelism data distribution loops operate data loops dependences readily parallelized adjusting loop bounds local bounds loops dependences may still parallelizable require analysis instance ibm compiler recognize dependence loop performs reduction generate appropriate hpf reduction intrinsic instances loop distribution may isolate portion containing dependence allow remainder original loop parallelized approximately quantify degree difficulty program presents parallelizing compiler terms dependence analysis use following simple metric count loops dependences count loops value 0 would indicate loops trivially parallelized value 1 would indicate parallelizable loops depend analysis capability compiler using kapf tool collect loop statistics benchmarks major subroutines listed table 2 metric complete since account data distribution instance 3 nested loops 1d distribution 1 loop needs partitioned parallelize program 2 loops may contain dependences ill effects addition metric static may correlate directly dynamic characteristics programs however metric gives coarse indication demand compiler loop dependence statistics show clear trends correlate directly performance data observe expected reduction dependences loop version f90forall version reduction greatly aids compilers parallelizing f90forall programs also highlights difficulty parallelizing programs loops subroutine f90forall ep embar 35 131 get start seed 11 ft fftpde 216 216 cfft3 06 06 cfftz 35 14 subroutine f90forall mg hmg 12 127 psinv 44 06 resid 44 06 rprj3 44 06 norm2u3 33 00 table 2 statistics dependences ep mg ft count loops data dependences subroutine calls n total loop count f90forall counts obtained array statements scalarized mg difference significant array syntax eliminates dependence cases hpf compilers implement optimizations array references affine functions loop indices particularly functions constants optimizations effective mg loop version however appear successful note loops subroutine norm2u3 replaced altogether hpf reduction intrinsics ft low number dependences fftpde comes dotproducts easily parallelized topdown order subroutines listed also represents nesting level subroutines increasing dependences inner subroutine reflect need achieve parallelism higher level explained earlier proves challenge apr compiler focuses analyzing individual loops partition work data parallel applications recent progress languages compilers allows us experimentally evaluate important issue program portability recognize many factors affect development success parallel language study focuses portability factor three nas benchmarks studied across three current hpf compilers examined different styles expressing computation hpf also consider benchmarks written mpi zpl understand interaction performance portability convenient programming hpf compilers show general difficulty detecting parallelism loops compilers successful f90 array syntax forall construct although even case success parallelization uniform significant variation scalar performance also exists compilers hpf directives constructs provide information data computation partitioning sequential semantics fortran leave many potential dependences pro gram hpf compiler must analyze dependences unable must make conservative assumption analysis capability differentiates various vendor imple mentations however difficult compilers parallelize reliably user cannot consistently estimate parallel behavior thus speedup program addition parallelization strategy compilers varies widely different ways express computation lead drastically different performance unpredictable variations reflect shortcoming performance model hpf result user needs continually experiment compiler learn actual behavior user effectively supplementing performance model provided language empirical information yet enhanced model tends platform specific portable zpl programs show consistent scalable performance illustrating possible incorporate robust performance model high level language language design ensures language abstractions behave predictable manner respect parallel performance although zpl supported multiple parallel systems results study directly show zpls portability across platforms multiple independent compiler implementations zpl available however existence performance model evident predictable performance behavior ensures zpl programs portable across independently developed platforms results also show significant overhead scalar performance remains implementations compared mpi programs one source overhead large number temporary arrays generated compiler across subroutine calls parallelized loops require dynamic allocationdeallocation copying generally degrade cache perfor mance index computation also contributes significantly overhead clear become viable alternative explicit message passing compilers data parallel languages must achieve much lower scalar overhead r david klepacki rick lawrence efficient parallel algorithm 3d fft nas parallel benchmark applied parallel research nas parallel benchmarks rob van der wijngaart vienna fortran 90 compiling fortran 90dhpf distributed memory mimd computers compiling high performance fortran factorjoin unique approach compiling array languages parallel machines high performance fortran forum fortran parallelization hand book hpf compiler ibm sp2 compiling high performance fortran distributedmemory systems evaluating compiler optimizations fortran zpl language reference manual zpl array sublanguage zpl vs hpf comparison performance programming style role performance models parallel programming languages influence programming models shared memory computer performance nas parallel benchmark 21 results 896 zpl programming guide cm fortran programming guide tr compiling fortran 90dhpf distributed memory mimd computers evaluating compiler optimizations fortran highperformance parallel implementations nas kernel benchmarks ibm sp2 compiling high performance fortran distributedmemory systems role performance models parallel programming languages factorjoin zpl ctr bradford l chamberlain sungeun choi e christopher lewis lawrence snyder w derrick weathersby calvin lin case highlevel parallel programming zpl ieee computational science engineering v5 n3 p7686 july 1998 bradford l chamberlain steven j deitz lawrence snyder comparative study nas mg benchmark across parallel languages architectures proceedings 2000 acmieee conference supercomputing cdrom p46es november 0410 2000 dallas texas united states govett l hart henderson j middlecoff schaffer scalable modeling system directivebased code parallelization distributed shared memory computers parallel computing v29 n8 p9951020 1 august bradford l chamberlain sungeun choi e christopher lewis calvin lin lawrence snyder w derrick weathersby zpl machine independent programming language parallel computers ieee transactions software engineering v26 n3 p197211 march 2000