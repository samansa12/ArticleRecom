performance prediction evaluation parallel processing numa multiprocessor efficiency basic operations numa nonuniform memory access multiprocessor determines parallel processing performance numa multiprocessor authors present several analytical models predicting evaluating overhead interprocessor communication process scheduling process synchronization remote memory access network contention memory contention considered performance measurements support models analyses several numerical examples done bbn gp1000 numa sharedmemory multiprocessor analytical experimental results give comprehensive understanding various effects important effective use numa sharedmemory multiprocessor results presented used determine optimal strategies developing efficient programming environment numa system b distributed memory multicomputer environment data sharing communication conducted messagepassing network busbased shared memory multiproces sors encore multimax sequent symmetry always make uniform memory access uma shared data exclusive use bus per data access simple bus structure limits size uma multiprocessor small scale example cessors numa architecture shared memory environment built distributed architecture processor local memory also able access memory models switching network therefore numa architecture scale large number processors shared memory multiprocessor design examples current numa architectures include bbn butterfly family see eg 35 16 23 cedar university illinois see eg 14 26 ibm rp3 see eg 21 cm plus carnegiemellon university see eg 9 17 22 hector see 25 paradigm see 11 bbn butterfly machines systems commercially available rest research model architectures 11 programming models numa architectures programming models numa architecture classified three types distributed mem partially shared memory fully shared memory distributed programming model node viewed complete computer supported processor local memory io facilities physically one board connects nodes system important factor efficiency distributed memory model effectiveness data exchanged among many nodes partially shared memory programming model provides noncached access shared memory program code private data stored local memory programming requires partitioning application across nodes load time explores processor locality best provides dynamic process scheduling run time shared memory synchronization primitives barrier supported synchronizing processors end group parallel tasks fully shared memory programming model implies processor access memory module time except one exception single memory module accessed one processor simultaneously users point view processors share single pool memory different memory access time scheduling mechanism supported schedule processes dynamically among processors run time several related studies conducted understand improve parallel processing performance numa multiprocessor larowe ellis see 19 take experimental approach compare widerange memory management policies target numa system bbn gp1000 system experiments conclude placement movement code data crucial numa performance performance general multistage interconnection network omega network evaluated analytically see eg 6 7 15 analysis work independent numa architecture although multistage interconnection network commonly used numa system performance factors numa multiprocessors example bbn gp1000 data access time scheduling overhead others measured various application programs see eg 10 30 memory architectures interconnection networks become complex performance prediction evaluation numa architecture become difficult paper studies three types programming models supported numa multiprocessor architecture presents several analytical models measurements predict evaluate overhead interprocessor communication process synchronization process scheduling remote memory access network contention memory contention considered performance measurement support models analyses different measurements done bbn gp1000 numa shared memory multiprocessor goal provide reasonably accurate numa performance models incorporate interconnection network effects numa system effects analyses experiments 12 performance models performance analyses start amdahls law 1 simplest form execution time versus number processors p model viewed separating program perfectly parallelized time section p strictly sequential time section running p node multiprocessor amdahls law incomplete model evaluating parallel processing performance practice model ignores several significant effects communication synchronization memory management memory network contention others amdahls model may modified adding overhead function term p include practical effects modified amdahls model used evaluating parallel processing performance different architectures vector supercomputers see eg 18 distributed memory multicomputer hypercube see eg 13 uma shared memory multiprocessors see eg 28 overhead function p affected structure application influences necessity communication task dispatching algorithm used control assignment processes processors well hardware software mechanisms communication synchro nization similar distributed memory multicomputers communication delay major overhead distributed programming model applied numa architecture computing bottleneck partially shared memory programming model barrier synchronization set end group parallel tasks typical parallel computation process shared memory programming model numa architecture described following steps system spends amount time generate parallel tasks place shared memory visible processors interconnection network processor turn enters critical region select assigned task end computation time expended waiting last processor finish parallel task another important source runtime overhead numa multiprocessor shared memory models comes remote memory access ie processor access read write memory models local therefore distributed memory programming model overhead function mainly contributed communication cost barrier synchronization remote memory access partially shared memory model process scheduling shared memory programming models respectively figure 1 twolevel interconnection network connecting 16 processors 16 memory modules 13 bbn gp1000 gp1000 see 35 based original butterfly architecture mimd system incorporates 256 motorola 68020 processor nodes connected via multistage interconnection switching network example 16 processor gp1000 system described figure 1 network composed 4 2 4 switches form plog 4 p switching interconnection p number processors connected bandwidth path switch processor node 68851 paged memory management unit virtual memory processing memory machine shared among processors way processor node includes 4 mbytes memory accessed processor system via network memory location physically local host processor although globally accessible processor system memory access processor memory direct path called local access memory access processor memory models network called remote access time ratio local access remote access processors active gp1000 115 depending types access see eg 10 gp1000 provides parallel programming environment called uniform system see 5 parallel tasks may distributed processed without regard physical location data associated tasks uniform system uma uniform memory access processors uniform memory access time data shared memory programming model implemented numa architecture gp1000 thus uniform system uses shared memory data structures called task generators specify parallel operations approach processor management implemented treating processors equivalent workers processor working task finishes current task looks next task performs best possible load balancing dynamically totally ignores processor locality memory management mechanisms uniform system makes globally shared data visible processes allows programs control data allocated provides means copying blocks data one memory another evaluate measure numa performance based four major overhead sources communication barrier synchronization process scheduling remote memory access provide several timing models experimental results section 2 presents performance evaluation distributed memory model communication overhead message passing measured gp1000 multiprocessor section 3 gives two models evaluating static dynamic task scheduling performance synchronization overhead partially fully shared memory programming models synchronization cost dynamic scheduling overhead also measured gp1000 multiprocessor several numerical examples remote access delay studied section 4 along analytical models experimental measurements finally summary given section 5 communication overhead numa multiprocessor system memory modules distributed interconnection network used connect processor local memory processors memory modules type architecture bbn gp1000 also support distributed memory multicomputer environment data sharing communication conducted messagepassing interconnection network message passes pair nodes system routed number switches network contentions switches considered multilevel connection network used see figure 1 ff fi gp1000 1812 240 topology 1000 215 table 1 alpha beta interprocessor communication gp1000 comparing five types distributed memory multicomputers head message passing pair nodes identical thus basic communication timing test distributed memory model measure time required transmit message packet one node another node network test also called echo test test node sends message echo node directly connected test node echo node receives message sends back test node interprocessor communication time required transmit message two directly connected nodes may approximately expressed k number bytes contained message ff overhead startup time sending packet fi bandwidth communication channel sbyte conducted echo tests measuring interprocessor communication overhead bbn gp1000 communication primitive machine supported set mach 1000 system calls used different sizes message packets 1 byte 8k bytes experiment used least square fit approximate ff fi table 1 gives ff fi gp1000 comparing five types distributed memory multicomputers see eg 29 experimental results show messagepassing gp1000 efficient bandwidth communication channel fi one intel ipsc1 representative first generation hypercube multicomputer startup time ff sending packet 4 times longer one ipsc1 connection processor remote memory module remote memory access established two level switches therefore distributed programming model bbn gp1000 preferred unless large data sets need exchanged occasionally see 24 3 process scheduling models synchronization overhead two major process scheduling models available numa multiprocessor prescheduling partially shared memory programming model selfscheduling fully shared memory programming model evaluate overhead two schedulings respectively give quantitative comparisons two models different numerical examples section 31 prescheduling barrier prescheduling tasks prescheduled assigned processors load time processes must wait synchronization point called barrier slowest finishes synchronization barrier defines logical point control flow processes must arrive allowed proceed often used synchronize processors end processing group parallel tasks prescheduling model thus consequences fluctuations execution time imbalanced task load maximized choosing task load normal probability distribution derive simple analytical timing model predicts effect imbalanced task load caused prescheduling scheme model assumes p processors begin work section simultaneously time takes mean standard deviation oe assuming mutual independence instant last processor completes work section w given thus keeping task load balanced among independent processes reducing oe reducing number barriers decrease potentially large performance penalties caused prescheduling process advantage prescheduling dynamic load scheduling performed task distribution decision made deterministically tasks processed efficiently among processors degree imbalanced task load minimized prescheduling preferred simple algorithm barrier synchronize prescheduling accumulating counter barrier see eg 2 implemented bbn gp1000 consists two locks shared counter first lock controls access shared counter records number processes arrived barrier counter must globally shared access serialized shared counter allocated local memory one processor process processor shared counter arrives barrier point simply atomic operation counter local memory atomic operation includes busywait type lock unlock immediately update counter rest processors busywait access counter using remote memory accesses interconnection network processes arrive barrier worst case situation occur processors arrive barrier time shared counter updated sequential order barrier synchronization delay time units numa shared memory architecture may described p number processors used atom time spent atomic operation lock update unlock counter remote memory access conducted processors respectively gp1000 constant words remote memory access equally distant among processors use r represent identical remote access delay 32 becomes linear function 33 simplified fl represents critical section delay protecting update shared counter local memory ffi overhead caused remote update shared counter interconnection network computed matrix dotproduct n 2 n matrix b vector n elements gp1000 n rows matrix distributed blocks processors processor holds copy b vector order measure overhead associated barrier implementation minimize fluctuations execution time multiplication oe described 31 evenly prescheduling tasks among processors experiment run program different number processors 1 12 experiment used least square fit approximate fl ffi ideally barrier synchronization overhead independent sizes evenly distributed dotproduct problems computed dotproduct problems different sizes 120 240 480 1200 variables differences approximated fls ffis using least square fit different size problems trivial get average fl ffi experiments obtain synchronization overhead gp1000 imbalanced load computing cycles consumed barrier two major sources complete synchronization overhead prescheduling may expressed sum second term 31 33 efficient barrier algorithms proposed numa shared memory architectures interested readers may refer 20 8 32 selfscheduling shared memory programming order balance processing load tasks dynamically scheduled selfscheduled processors runtime selfscheduling algorithm consists processor fetching task one time requiring mutually exclusive access shared variable reading modifying appropriate data memory modules uma architecture busbased shared memory multiprocessors processor requires exclusive access shared bus reach shared memory numa architecture gp1000 access shared memory performed interconnection network without network contention problems multistage interconnection switching network provides unique path source processor destination memory module pair however paths different pairs disjoint therefore conflicts may occur simultaneous communication established several sourcedestination pairs may degrade parallel performance consider computing job may divided n tasks requiring average comp units time execute single processor computing job executed p processors selfscheduling routines create initialize task data structures place shared memory visible processors different distances processor numa archi tecture process spends init time units initializing task processor requires extra time units access data scheduled task may also denoted constant r identical remote access delay one gp1000 addition processor requires overhead arri units synchronizing barrier end computation arri units spent notify processors arrival finally first scheduled processor process task last terminate program exit thus special processor needs chek units check processors arrived assume n integral multiple p parallel execution time numa shared memory multiprocessor may described first term 37 time purely spent computing rest terms overhead scheduling figure 2 gives time line selfscheduling 33 comparisons two scheduling models prescheduling selfscheduling provide static dynamic load balancing schemes comparing 36 38 overhead caused prescheduling overhead selfscheduling quantitatively equal oe p 2logp overhead prescheduling quantitatively larger one selfscheduling oe p overhead prescheduling quantitatively less one selfscheduling oe p figure 2 selfscheduling example 8 tasks scheduled run 4 processors oe standard deviation time task used one processor representing degree imbalanced task load based analysis variance distributed task processing expected large dynamic changes execution advantage using selfscheduling according 311 selfscheduling decrease arrival time variance cost scheduling overhead 310 exists selfscheduling gain 34 numerical experiments prescheduling selfscheduling numerical experiment conducted compare two scheduling models group complex nonlinear circuit simulations objective circuit simulation determine accurately voltage current waveforms circuit period time specified user given topology electric elements circuit simulation conducted multiprocessor one method partition circuit processor simulates one set subcircuits concurrently result circuit partitioning leads us solve special class nonlinear systems equations block bordered structure using newtons method n detailed mathematical analyses nonlinear block bordered equations reader may refer 27 circuit equations usually highly nonlinear newton step easily may result increase function norm addition many block bordered equations result nearly singular singular jacobians iteration process thus newton step needs modified dynamically converge solution modifications subsystems line search matrix perturbations see eg 12 require extra computing time cannot predicted statically running program testing circuit problem simulated 741 opamp 27 using prescheduling model block bordered equations 741 opamp circuit distributed among 1 2 4 nodes gp1000 respectively addition function f q1 assigned another node plays control role computation load among different nodes reasonably balanced statically obtained circuit partitioning runtime simulation subsystem perturbed independent computation time node slightly different using selfscheduling model subcircuit systems scheduled runtime system subcircuit system data structures placed shared memory visible processors different distances processor gp1000 used 1 3 processors respectively computation computing performance scheduling models given figure 3 experiments showed computing performance selfscheduling model poor comparing one prescheduling model partitioned systems well balanced runtime therefore dynamic scheduling became real overhead multiprocessing also simulated analog filter connected 3 blocks 741 opamp circuit see 27 figure 3 prescheduling selfscheduling opamp 741 simulation gp1000 12 subcircuit systems generated 12 block equations analog filter distributed among 1 3 6 12 nodes gp1000 respectively nonlinear system harder compute single 741opamp system unbalanced block bordered system due singularity subsystems runtime subsystems perturbed quite often newton iteration applied prescheduling selfscheduling models unbalanced systems figure 4 gives computational curves since prescheduling able handle dynamic load scheduling runtime selfscheduling show effectiveness solving system 4 remote memory access delay 41 background memory architectures become complex interconnection network introduces nonuniform memory access remote memory access prediction evaluation numa system dynamic environment become difficult remote memory access delay measured bbn butterfly systems processor memory module pair without considering memory network contentions without considering random contentions see eg 3 10 however real parallel processing numa system remote memory access delay complicated memory network contentions give analytical model predict average remote memory access delay considerations different network memory architecture effects numa systems figure 4 prescheduling selfscheduling analog filter connected 3 opamp 741 simulation multistage switching interconnection network commonly effectively used numa systems bbn butterfly systems ibm rp3 multiprocessors figure 1 shows twostage switching network connecting 16 processors 16 memory modules switch 4 inputs 4 outputs network contention defined conflict two messages need access portion path time network designed either blocking form nonblocking form blockingnetwork protocol organizes message queue conflicting traffic comes standstill conflicting messages sits holds path path cleared next selected message proceeds problem blocking network called cascade effect new message tends run blocked messages gets blocked ties resources switch increases chance subsequent messages also block nonblocking network greatly reduces traffic conflicts practice see eg 23 conflicts happen switch first message retreat back source free path selects alternative route random delay tries model based nonblockingnetwork architecture used numa systems 42 model remote memory access gelenbe 15 describes behavior remote memory access nonblocking multistage interconnection network state transition diagram called drop approach see figure 5 processor makes remote memory access formulating requests access set switches figure 5 state transition diagram remote memory access nstage interconnection network along path cannot obtain switch abandons request point try later time figure 2 state 0 represents processor quiescent state represents ongoing successful access state b represents processor dropped request switch contention make analytical model tractable certain approximation assumptions necessary assume multiprocessor system processor identical uniform reference model different uma concept urm uniform reference model implies processor makes memory request global memory request directed memory modules probability destination address memory request uniformly distributed among memory modules symmetric property significantly simplifies modeling average remote memory access delay estimated making use semimarkov model length path n request state 1 n successfully obtained first switches path state go b 1th request successful otherwise go state 1 process continues state n 1 reached switch requests granted finally goes state n 1 state 0 processor access memory module releases path assume 0 rate number requests per time unit quiescent processor makes remote memory access 1 0 average time spent processor state making access processor state 1 n assume requests next switch average time probability success q following notation used later mathematical work practice switches network therefore identical average duration access denoted 1oe processor return state 0 similarly use 1oe b denote average time spent state b conflict detected processor returns quiescent state based kirchhoff current law kcl flow balance assumptions steadystate probabilities associated states model may obtained following equations steadystate probabilities state n last equation derived previous n 2 ones solve system equations one additional equation based sum success fail probability success probability ith state q depend traffic conditions expressed based steadystate probabilities inputoutput size switch network steadystate probabilities state k inputoutput size multistage interconnection network use substituting q get n new equations system equations able solve get q probability memory reference request may fail ith stage network average delay unsuccessful memory access caused either network contention memory contention successful one average delay remote access 4 probability success remote access average delay caused remote access one computation 43 experiment remote memory access difficult use limited number experiments cover cases interpreted 414 random contentions various structures application programs however remote memory access delay practice determined two important factors 1 delay establishing connecting path processor remote memory module network switches 2 network contention connection delay constant time units without considering network contention architecture dependent example gp1000 spends 05 establish remote memory access connection 2level switches connection established read write operations performed data communication rate 0125 every 4bits see eg 3 network contention basically determined remote access rate defined number remote access per time unit denoted 41 constructed two programs measuring comparing remote memory access effects gp1000 one matrix multiplication c2b one matrix addition n 2 n square matrices structures programs designed way single operation multiplication addition pair data needs two remote memory access therefore number remote access matrix multiplication addition 2 2 n 3 2 2 n 2 respectively n size matrices computations order compare remote memory access effects computations made number remote memory access two problems identical chosing size matrices addition n nm nm n matrix size addition nm size multiplication figure 6 gives remote access times two computations different number processors size matrix multiplication 80 716 matrix addition computations figure remote access time matrix addition matrix multiplication gp1000 performed total approximate 1024210 3 times remote memory access gp1000 however time spent remote access computing matrix addition almost twice much computing matrix multiplication total times multiplication operations performed matrix multiplication program approximately number additions performed matrix addition program computing time ratio multiplication operation addition operation mc68020 processors gp1000 17 thus remote memory access rate matrix addition program 17 times higher one matrix multiplication gives difference overall remote access times similar factor two programs runing gp1000 plotted figure 6 higher remote memory access rate higher chances network contention occur therefore remote memory access rate simply used predict remote memory access delay 5 summaries examined effects scheduling synchronization remote memory access parallel processing performance numa shared memory multiprocessor analytical models based generic numa machine developed tested verified gp1000 multiprocessor several numerical examples analytical experimental results paper may used advice determine optimal parallel processing strategy effective use numa shared memory multiprocessor developing efficient parallel programming environment running application programs current work includes development graphical tool monitor tune performance numa multiprocessor based analysis models presented paper acknowledgement authors wish thank p srinivasan w wu university texas san antonio j zhou university ohio multiprocessor tesing technical discussions r validity singleprocessor approach achieving large scale computing capabilities effects synchronization barriers multiprocessor performance bbn advanced computer inc bbn advanced computer inc bbn advanced computer inc design performance generalized interconnection network performance multiprocessor interconnection network simple mechanism efficient barrier synchronization mimd machines performance evaluation prediction parallel algorithms bbn gp1000 paradigm highly scalable sharedmemory multicomputer architecture performance parallel processors cedar large scale multiprocessor john wiley sons overview butterfly gp1000 largescale parallel unix computer software management cm distributed multiprocessor measuring parallel processor performance experimental comparison memory management policies numa multiprocessors barrier synchronization multistage interconnection networks ibm research parallel processor prototype rp3 introduction architec ture cm modular multimicroprocessor behavior butterfly parallel processor presence memory hot spots experimental studies different programming models bbn gp1000 hector hierarchically structured shared memory multiprocessor architecture cedar parallel supercomputer parallel partition simulation largescale circuits local memory mul ticomputer performance measurement modeling evaluate various effects shared memory multiprocessor system effects interprocessor communication latency multicomputers distributed task processing performance numa shared memory multiprocessor tr effects synchronization barriers multiprocessor performance design performance generalized interconnection networks performance multiprocessor interconnection networks multiprocessor performance measuring parallel processor performance paradigm hector performance measurement modeling evaluate various effects shared memory multiprocessor performance evaluation prediction parallel algorithms bbn gp1000 plus system effects interprocessor communication latency multicomputers experimental comparison memory management policies numa multiprocessors ctr xiaodong zhang robert castaeda elisa w chan spinlock synchronization butterfly ksr1 ieee parallel distributed technology systems technology v2 n1 p5163 march 1994 l boyd johndavid wellman santosh g abraham edward davidson evaluating communication performance mpps using synthetic sparse matrix multiplication workloads proceedings 7th international conference supercomputing p240250 july 1923 1993 tokyo japan xiaodong zhang robert castaeda elisa w chan spinlock synchronization butterfly ksr1 ieee parallel distributed technology systems technology v2 n1 p5163 march 1994 xiaodong zhang yong yan robert castaeda comparative performance evaluation hot spot contention minbased ringbased sharedmemory architectures ieee transactions parallel distributed systems v6 n8 p872886 august 1995 k harzallah k c sevcik hot spot analysis large scale shared memory multiprocessors proceedings 1993 acmieee conference supercomputing p895905 december 1993 portland oregon united states yongsheng song weiming lin performance prediction based loop scheduling heterogeneous computing environment proceedings 1997 acm symposium applied computing p413421 april 1997 san jose california united states