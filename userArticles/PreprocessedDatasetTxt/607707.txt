algebraic geometrical methods hierarchical learning machines hierarchical learning machines layered perceptrons radial basis functions gaussian mixtures nonidentifiable learning machines whose fisher information matrices positive definite fact shows conventional statistical asymptotic theory cannot applied neural network learning theory example either bayesian posteriori probability distribution converge gaussian distribution generalization error proportion number parameters purpose paper overcome problem clarify relation learning curve hierarchical learning machine algebraic geometrical structure parameter space establish algorithm calculate bayesian stochastic complexity based blowingup technology algebraic geometry prove bayesian generalization error hierarchical learning machine smaller regular statistical model even true distribution contained parametric model b introduction learning artificial neural networks understood statistical estimation unknown probability distribution based empirical samples white 1989 watanabe fukumizu 1995 let pyx w conditional probability density function represents probabilistic inference artificial neural network x input output parameter w consists lot weights biases optimized inference pyx w approximates true conditional probability density training samples taken let us reconsider basic property homogeneous hierarchical learning machine mapping parameter w conditional probability density pyx w onetoone model called identifiable otherwise called nonidentifiable words model identifiable parameter uniquely determined behavior standard asymptotic theory mathematical statistics requires given model identifiable example identifiablity necessary condition ensure distribution maximum likelihood estimator bayesian posteriori probability density function converge normal distribution number training samples tends infinity cramer 1949 approximate likelihood function quadratic form parameter select optimal model using information criteria aic bic mdl implicitly assume model identifiable however many kinds artificial neural networks layered perceptrons radial basis functions boltzmann machines gaussian mixtures nonidentifiable hence either statistical property yet clarified conventional statistical design methods applied fact failure likelihood asymptotics normal mixtures shown viewpoint testing hypothesis statistics hartigan 1985 researches artificial neural networks pointed aic correspond generalization error maximum likelihood method hagiwara 1993 since fisher information matrix degenerate parameter represents smaller model fukumizu 1996 asymptotic distribution maximum likelihood estimator nonidentifiable model analyzed based theorem empirical likelihood function converges gaussian process satisfies donskers condition dacunhacastelle gassiat 1997 proven generalization error bayesian estimation far smaller number parameters divided number training samples watanabe 1997 watanabe 1998 parameter space conic sym metric generalization error maximum likelihood method dierent regular statistical model amari ozeki 2000 log likelihood function analytic parameter set parameters compact generalization error maximum likelihood method bounded constant divided number training samples watanabe 2001b let us illustrate problem caused nonidentifiability layered learning machines pyx w threelayer perceptron k hidden units w 0 parameter pyx w 0 equal machine k 0 hidden units set true parameters consists several submanifolds parameter space moreover fisher information matrix log pyx w log pyx wpyx wqxdxdy qx probability density function input space positive semidefinite positive definite rank rank iw depends parameter fact indicates artificial neural networks many singular points parameter space figure 1 typical example shown example2 section 3 reason almost homogenous hierarchical learning machines boltzmann machine gaussian mixture competitive neural network singularities parameter spaces resulting mathematical foundation analyze learning previous paper watanabe 1999b watanabe 2000 watanabe 2001a order overcome problem proved basic mathematical relation algebraic geometrical structure singularities parameter space asymptotic behavior learning curve constructed general formula calculate asymptotic form bayesian generalization error using resolution singularities based assumption true distribution contained parametric model paper consider threelayer perceptron case true probability density contained parametric model clarify singularities parameter space aect learning bayesian estimation employing algebraic geometrical method show following facts 1 learning curve strongly aected singularities since statistical estimation error depends estimated parameter 2 learning eciency evaluated using blowingup technology algebraic geometry 3 generalization error made smaller singularities bayesian estimation applied results clarify reason bayesian estimation useful practical applications neural networks demonstrate possibility algebraic geometry plays important role learning theory hierarchical learning machines dierential geometry regular statistical models amari 1985 paper consists 7 sections section 2 general framework bayesian estimation formulated section 3 analyze parametric case true probability density function contained learning model derive asymptotic expansion stochastic complexity using resolution singularities section 4 also study nonparametric case true probability density contained clarify eect singularities parameter space section 5 problem asymptotic expansion generalization error considered finally section 6 7 devoted discussion conclusion bayesian framework section formulate standard framework bayesian estimation bayesian stochastic complexity schwarz 1974 akaike 1980 levin tishby solla 1990 mackay 1992 amari fujita shinomoto 1992 amari murata 1993 let pyx w probability density function learning machine input x output parameter w n dimensional vectors respectively let qyxqx true probability density function input space training samples x independently taken paper mainly consider bayesian framework hence estimated probability density n w parameter space defined expnh n ww log z n normalizing constant w arbitrary fixed probability density function parameter space called priori distribution h n w empirical kullback distance note posteriori distribution n w depend qy x constant function w hence written form inference p n yx trained machine new input x defined average conditional probability density function generalization error gn defined kullback distance p n yx qyx qyx log qxdxdy 1 represents expectation value overall sets training samples one important purposes learning theory clarify behavior generalization error number training samples suciently large well known levin tishby solla 1990 amari 1993 amari murata 1993 generalization error gn equal increase stochastic complexity f n arbitrary natural number n f n defined stochastic complexity f n generalized concepts sometimes called free energy bayesian factor logarithm evidence seen statistics information theory learning theory mathematical physics schwarz 1974 akaike 1980 rissanen 1986 mackay 1992 opper haussler 1995 meir merhav 1995 haussler opper 1997 yamanishi 1998 example bayesian model selection hyperparatemeter optimization often carried minimization stochastic complexity averaging called bic abic important practical applications stochastic complexity satisfies two basic inequalities firstly define hw f n respectively qxdxdy note hw called kullback information applying jensens inequality holds arbitrary natural number n opper haussler 1995 watanabe 2001a secondly use notations f n f n f n f n explicitly show priori probability density w f n f n understood generalized stochastic complexity case w nonnegative function w w satisfy immediately follows therefore restriction integrated region parameter space makes stochastic complexity smaller example define expnhwwdw 7 suciently small 0 two inequalities eq4 eq8 give upper bounds stochastic com plexity hand support w compact lower bound proven moreover learning machine contains true distribution holds watanabe 1999b watanabe 2001a paper based algebraic geometrical methods prove rigorously upper bounds f n constants olog n function n satisfies olog n log n mathematically speaking although generalization error gn equal f n natural number n derive asymptotic expansion gn however section 5 show gn asymptotic expansion satisfy inequality suciently large n eq11 main results paper upper bounds stochastic complexity however also discuss behavior generalization errors based eq12 3 parametric case section consider parametric case true probability distribution qyxqx contained learning machine pyx wqx show relation algebraic geometrical structure machine asymptotic form stochastic complexity 31 algebraic geometry neural networks subsection briefly summarize essential result previous paper mathematical proofs subsection see watanabe 1999b watanabe 2001a strictly speaking need assumptions log pyx w analytic function w analytically continued holomorphic function w whose associated convergence radii positive uniformly arbitrary x satisfies qyxqx 0 watanabe 2000 watanabe 2001a paper apply result previous paper threelayer perceptron threelayer perceptron redundant approximate true distribution set true parameters w union several submanifolds parameter space general set zero points analytic function called analytic set analytic function hw polynomial set called algebraic variety well known analytic set algebraic variety complicated singularities general introduce state density function vt diracs delta function 0 suciently small constant definition 0 using vt f n rewritten expnhwwdw dt hence vt asymptotic expansion 0 f n n asymptotic expansion n order examine vt introduce kind zeta function jz sato kullback information hw priori probability density w function one complex variable z hw z wdw 14 jz analytic function z region rez 0 well known theory distributions hyperfunctions hw analytic function w jz analytically continued meromorphic function entire complex plane poles negative part real axis atiyah 1970 bernstein 1972 sato shintani 1974 bjork 1979 moreover poles jz rational numbers kashiwara 1976 let 1 largest pole order jz respectively note eq15 shows jz z c mellin transform vt using inverse mellin transform show vt satisfies c 0 0 positive constant eq13 f n asymptotic expansion o1 bounded function n hence eq8 moreover support w compact set eq9 obtain asymptotic expansion f n first theorem theorem 1 watanabe 1999b watanabe 2001a assume support w compact set stochastic complexity f n asymptotic expansion respectively largest pole order function analytically continued hw z wdw hw kullback information w priori probability density function remark support w compact theorem 1 gives upper bound f n important constants 1 1 calculated algebraic geometrical method define set parameters w proven hironakas resolution theorem hironaka 1964 atiyah 1970 exist manifold u resolution map arbitrary neighborhood arbitrary u u satisfies au 0 strictly positive function k nonnegative even integers figure 2 let decomposition w finite union suitable neighborhoods w applying resolution theorem function jz hw z wdw hw z wdw given recursive blowingups jacobian g u direct product local variables u 1 cu positive analytic function h j nonnegative integers neighborhood u au gu set constant functions calculation poles jz take u small enough hence set loss generality k j depend neighborhood u find jz poles h j rational numbers negative part real axis since resolution map gu found using finite recursive procedures blowingups 1 1 found algorithmically also proven 1 d2 w w 0 1 theorem 2 watanabe 1999b watanabe 2001a largest pole 1 function jz algorithmically calculated hironakas resolution theorem moreover 1 rational number 1 natural number w w 0 dimension parameter note learning machine regular statistical model always also note jereys prior employed neural network learning equal zero singularities assumption w w 0 satisfied even fisher metric degenerate watanabe 2001c example1 regular model let us consider regular statistical model exp2 set parameters assume true distribution exp2 priori distribution uniform distribution w subset w define introduce mapping 2 z pole z 1 show jw 2 z pole way hence resulting f n log n coincides well known result bayesian asymptotic theory regular statistical models mapping eq17 typical example blowingup example2 nonidentifiable model let us consider learning machine pyx b c 2 exp2 assume true distribution eq16 priori probability distribution uniform one set kullback information let us define two sets parameters using blowingups recursively find map defined using transform obtain therefore hw z dw 2 z largest pole jw 1 z 34 order one also shown jww 1 z largest pole 34 order one hence resulting log n o1 32 application layered perceptron apply theory foregoing subsection threelayer perceptron threelayer perceptron parameter defined k b k x fx w h n dimensional vectors x b h dimensional vectors c h real number k numbers input units output units hidden units paper consider machine estimate standard deviation 0 constant assume true distribution say true regression function special case analysis case important following section true regression function contained model theorem 3 assume learning machine given eq18 eq19 trained using samples independently taken distribution eq20 priori distribution satisfies w 0 neighborhood origin proof theorem use notations kullback information b c hp kp purpose find pole function let us apply blowingup technique kullback information ha b c firstly introduce mapping defined let u variables u except u 11 words jacobian g u mapping g define set paramaters 0 assumption exists 0 order obtain upper bound stochastic complexity restrict integrated region parameter space using eq5 6 assumption w 0 gu calculation pole jz assume constant gu du db dc pole function respectively largest poles jz since h 1 zero point interval 1 larger 1 z nk2 pole jz otherwise jz larger pole nk2 hence 1 nk2 secondly consider another blowingup g defined method first half exists analytic function implies therefore combing two results largest pole 1 jz satisfies inequality completes proof theorem 3 end proof theorem 1 moreover gn asymptotic expansion see section 5 obtain inequality generalization error hand well known largest pole regular statistical model equal d2 number parameters threelayer perceptorn 100 input units 10 hidden units 1 output unit employed regular statistical models number parameters emphasized generalization error hierarchical learning machine far smaller regular statistical models use bayesian estimation adopt normal distribution priori probability density shown result theorem 3 direct calculation watanabe 1999a however theorem 3 shows systematically result holds arbitrary priori distribution moreover easy generalize result case learning machine input units k 1 first hidden units k 2 second hidden units k p pth hidden units n output units assume hidden units output units bias parameters using blowingups generalize proof theorem 3 course result holds true regression function special case however following section show result necessary obtain bound general regression function 4 nonparametric case previous section studied case true probability distribution contained parametric model section consider nonparametric case true distribution contained parametric models illustrated figure 3 let w 0 parameter minimizes hw point c figure 3 main purpose clarify eect singular points b figure 3 contained neighborhood w 0 let us consider case threelayer perceptron given eq18 eq19 trained using samples independently taken true probability distribution gx true regression function qx true probability distribution input space let ek minimum function approximation error using threelayer perceptron k hidden units assume 1 k k exists parameter w attains minimum value theorem 4 assume learning machine given eq18 eq19 trained using samples independently taken distribution eq21 priori distribution satisfies w 0 arbitrary w proof theorem 4 jensens inequality eq4 hw kullback distance natural numbers satisfy 0 k 1 k divide parameter also let 1 2 real numbers satisfy 1 1 arbitrary u v r n therefore arbitrary x w hence inequality use definitions f n increasing function hw functions satisfy choose 1 w 1 2 w 2 compact support functions firstly evaluate f 1 n let w 1 parameter minimizes h 1 w 1 eq22 theorem 2 number parameters threelayer perceptron k 1 hidden units secondly applying theorem 3 f 2 n combining eq23 eq24 taking 1 suciently close 1 obtain arbitrary given theorem 4 end proof based theorem 4 gn asymptotic expansion see section 5 gn satisfy inequalities n n 0 suciently large n 0 hence ek n n 0 suciently large n 0 figure 4 illustrates several learning curves corresponding k 0 k k generalization error gn smaller every curve well known barron 1994 murata 1996 gx belongs kind function space suciently large k cg positive constant determined true regression function gx asymptotic property generalization error 22 n k suciently large choosing inequality 27 holds n suciently large n suciently large extensively large gn bounded generalization error middle size model n becomes larger bounded larger model n extensively large bounded largest model complex hierarchical learning machine contains lot smaller models parameter space analytic sets singularities chooses appropriate model adaptively number training samples bayesian estimation applied property caused fact model nonidentifiable quantitative eect evaluated using algebraic geometry 5 asymptotic property generalization er ror section let us consider asymptotic expansion generalization error eq2 f n equal accumulate generalization error g0 defined f 1 hence gn asymptotic expansion f n also asymptotic expansion however even f n asymptotic expansion gn may asymptotic expansion foregoing sections proved f n satisfies inequalities constants determined singularities true distribu tion order mathematically derive inequality gn eq30 need assumption asymptotic property generalization error 23 assumption assume generalization error gn asymptotic expansion q q n q real constants q n 0 positive nonincreasing function n satisfies based assumption following lemma lemma 1 gn satisfies assumption eq30 holds gn satisfies inequality proof assumption shows 1 1 eq35 holds ks 2 k eq32eq33 eq34 tk tk c c 0 tk arbitrary 0 exists k 0 hence contradicts eq36 hence tn c 2 c end proof lemma 1 paper proven inequalities eq30 theorem 1 2 3 4 without assumption obtain corresponding inequalities adopt assumption words gn asymptotic expansion eq30 holds gn satisfy eq35 conjectured natural learning machines satisfy assumption sucient condition assumption f n asymptotic expansion r 1 example learner pyx 2 exp2 priori distribution standard normal distribution true distribution shown direct calculation stochastic complexity asymptotic expansion hence gn asymptotic expansion c 22n expected general case gn asymptotic expansion assumption however mathematically speaking necessary sucient condition yet established important problem statistics learning theory future 6 discussion section universal phenomena observed hierarchical learning machines 61 bias variance singularities consider covering neighborhood parameter space w w j suciently small neighborhood parameter w j number j eq38 finite compact upperbound stochastic complexity rewritten exphwwdw function approximation error parameter w j hw v w j statistical estimation error neighborhood w j log n mw j 1 c 0 0 constant values w j mw j respectively largest pole multiplicity meromorphic function note bw j v w j called bias variance respectively bayesian estimation neighborhood parameter w j minimizes selected largest probability regular statistical models variance depend parameter words w j arbitrary parameter w j hence parameter minimizes function approximation error selected hand hierarchical learning machines variance v w j strongly depends parameter w j parameter minimizes sum bias variance selected number training samples large extensively large parameters among singular point figure 3 represents middle size model automatically selected resulting smaller generalization error n increases larger largest model b selected last n becomes extensively large parameter c minimizes bias selected universal phenomenon hierarchical learning machines indicates essential dierence regular statistical models artificial neural networks 62 neural networks overcomplete basis singularities hierarchical learning machine originate homogeneous structure learning model set functions used artificial neural network example set overcomplete basis words coecients ab c wavelet type decomposition given function gx uniquely determined gx chui 1989 murata 1996 practical applications true probability distribution seldom contained parametric model however adopt model almost approximates true distribution compared fluctuation caused random samples k b k x appropriate number samples choose appropriate learning model expected model almost redundant state output functions hidden units almost linearly dependent expect paper mathematical foundation study learning machines states 7 conclusion considered case true distribution contained parametric models made hierarchical learning machines showed parameters among singular points selected bayesian distribution resulting small generalization error quantitative eect singularities clarified based resolution singularities algebraic geometry even true distribution contained parametric models singularities strongly aect improve learning curves universal phenomenon hierarchical learning machines observed almost artificial neural networks r likelihood bayes procedure universal theorem learning curves four types learning curves neural computation statistical theory learning curves entropic loss resolution singularities division distributions communications pure applied mathematics approximation estimation bounds artificial neural networks analytic continuation generalized functions respect parameter mathematical methods statistics introduction wavelets testing locally conic models generalized functions problem applying aic determine structure layered feedforward neural network failure likelihood asymptotics normal mixtures mutual information resolution singularities algebraic variety field characteristic zero statistical approaches learning generalization layered neural networks bayesian interpolation stochastic complexity learning realizable unrealizable rules integral representation ridge functions approximation bounds threelayered network bounds predictive errors statistical mechanics supervised learning stochastic complexity modeling zeta functions associated prehomogeneous vector space optimization method layered neural networks based modified information criterion essential di generalization error layered statistical model bayesian estimation algebraic analysis nonregular learning machines neural computation probabilistic design layered neural networks based unified framework learning artificial neural networks statistical prespective neural computation decisiontheoretic extension stochastic complexity applications learning tr bayesian interpolation four types learning curves universal theorem learning curves introduction wavelets statistical theory learning curves entropic loss criterion approximation estimation bounds artificial neural networks stochastic complexity learning realizable unrealizable rules regularity condition information matrix multilayer perceptron network integral representation functions using threelayered networks approximation bounds algebraic analysis singular statistical estimation ctr miki aoyagi sumio watanabe stochastic complexities reduced rank regression bayesian estimation neural networks v18 n7 p924933 september 2005 keisuke yamazaki sumio watanabe singularities mixture models upper bounds stochastic complexity neural networks v16 n7 p10291038 september sumio watanabe shunichi amari learning coefficients layered models true distribution mismatches singularities neural computation v15 n5 p10131033 may shunichi amari hiroyuki nakahara difficulty singularity population coding neural computation v17 n4 p839858 april 2005 haikun wei jun zhang florent cousseau tomoko ozeki shunichi amari dynamics learning near singularities layered networks neural computation v20 n3 p813843 march 2008 shunichi amari hyeyoung park tomoko ozeki singularities affect dynamics learning neuromanifolds neural computation v18 n5 p10071065 may 2006