predictorcorrector algorithm class nonlinear saddle point problems interior pathfollowing algorithm proposed solving nonlinear saddle point problem rm minimax ctxphxbtypsiyytax vspace18pt rm subject xyin xti ysu rnti rm noindent phx psy smooth convex functions x boxes hyperrectangles problem closely related models stochastic programming optimal control studied rockafellar wets math programming studies 28 1986 pp 6393 siam j control optim 28 1990 pp 810822 existence errorbound results central path derived starting initial solution near central path duality gap omu algorithm finds epoptimal solution problem osqrtmnlogmuep iterations phx psy satisfy scaled lipschitz condition b introduction paper discusses interior pathfollowing method solving class nonlinear saddle point problems following form find saddle point subject x 2 x ae r n oex c 2 convex functions c 2 r n b superscript represents transpose sets x boxes hyperrectangles according convex analysis 15 problem 11 pair associated optimization problems primal problem dual problem maximize note function fx called primal objective function convex function gy called dual objective function concave nondifferentiable general detailed analysis case oex quadratic see 16 fx gy turn piecewise quadratic convex functions therefore problems 11 13 categorized nonsmooth convex programming problems fundamental duality relationships among 11 12 13 established 15 include existence results saddle points 11 saddle point value common optimal value 12 13 problems stem development beyond conventional formulation optimization problems models provide framework allows penalty representations constraints well accommodates sources nonsmoothness objectives produced multistage optimization problems instance linearly constrained convex optimization problems usually posed form minimize oex subject ax b x 0 corresponding lagrangian saddle point problem special case 11 let convex function 0 add gammay lagrangian function corresponding primal program becomes fy subject x 0 function sup y0 fy b equal zero x satisfying ax b greater equal zero x thus formulation exact constraint replaced penalty representation allows modeler deal flexibly selecting suitable example multistage optimization could fit form 11 consider twostage stochastic programming model studied rockafellar wets 21 first current stage decision x 2 r n made incurring direct cost c xoex subject x 2 x x box second future stage random event observed outcome whereomega probability space decision x outcome determine additional recourse cost ae x practical circumstances soft constraints 1618 simple recourse 27 function expressed optimal value function follows fy principle set vector b matrix function allowed random objective model make best decision x respect present cost constraints well expected cost induced constrains assuming finite discrete distribution decision problem described subject x probability random event let deltac c c c c deltac c c c c problem 15 takes form 12 motivation models 11 13 applications stochastic programming optimal control found series pioneer papers rockafellar wets 161718192122 algorithms linearquadratic cases problem 11 oex linear quadratic studied extensively among lshaped method 26 decomposition methods 15 finite generation method 921 projected gradient method 32 steepest descent method 31 sqp method 14 interior point methods 23232829 special case x boxes oex separable quadratic functions simplexactive set method developed 20 general convex case 11 however yet received enough attention algorithmic development although problem traced back extended fenchel duality model 1970s 15 purpose writing paper three fold 1 develop predictorcorrector algorithm problem 11 together existence errorbound results central path particularly show functions oex satisfy scaled lipschitz condition starting initial solution near central path algorithm converges ffloptimal solution problem number related initial duality gap 2 research community stochastic programming optimal control demonstrate interior point method used one theoretically efficient methods convexconcave saddle point problems 3 research community interior point methods paper contributes polynomial algorithm special class monotone complementarity problems mcp knowledge algorithms nonlinear mcp shown polynomial complexity nesterov nemirovskii 13 tseng 24 different conditions scaled lipschitz condition shown 33 satisfied fairly large class convex functions although problem 11 reduced monotone complementarity problem special type see next section two major differences proposed algorithm algorithms general mcp first convergence analysis based properties monotone mapping rather based specifications two functions oex second proposed algorithm take advantage special structure problem 11 arising stochastic programming optimal control paper organized follows section 2 discuss conditions existence optimal solutions central path establish estimate duality gap x near central path results form foundation interior pathfollowing method problem 11 section 3 devoted proof polynomial convergence predictorcorrector algorithm finally section 4 contains concluding remarks predictorcorrector algorithm studied paper rooted primaldual pathfollowing algorithm kojima mizuno yoshise 10 linear complementarity problems algorithm variants extensively studied global local computational behaviors context linear complementarity problems possible us point references method interested reader may look nice tutorial paper gonzaga6 recent papers 8112930 references therein goal limited proof global convergence result proposed algorithm discussion properties central path computational behaviors issues left studies 2 results feasibility existence errorbounds related central path following analysis assume x nonnegative orthants order simplify statements essential change assuming x boxes elaborate point end section 3 suppose saddle point exists shown 15 x must optimal solutions 12 13 spectively fx vice versa addition saddle point satisfies following variational relationships stands normal cone x x notation n similar meaning introducing auxiliary vectors w conditions 21 written explicit form follows ax ry assume oex finite twice continuously differentiable x respectively proposed algorithm finds approximate saddle point lx x theta finding sequence approximate saddle points following saddle function log log x theta analogous 22 point x saddle point l x exists w x w satisfies system 23 derived similar way deduction system 22 seen system 22 special case system 23 naturally interior point method proposed algorithm needs kind interior points start make following assumption assumption 21 quadruple x first two equations system 23 satisfied assumption prove iterates generated algorithm feasible solutions problems 12 13 solutions problem 23 exists 0 first discuss feasibility problem x feasible 12 x 0 feasible 13 0 gy gamma1 following result assumption weaker assumption 21 proposition 22 suppose following relations valid x k feasible solution problem 11 k feasible solution problem 12 proof second equation 24 convexity imply thus second equation 24 k 0 x k 0 imply x k feasible 12 similarly first equation 24 w k 0 k 0 imply k feasible 13 algorithm ensure 24 valid iterates see details algorithm generate feasible sequence fx k k g problems 11 12 according proposition 22 discuss existence saddle points l x unlike linearquadratic case feasibility primal dual problems enough existence saddle point however show assumption 21 0 saddle point l x exists first prove lemma lemma 23 suppose following relations valid recession functions oex respectively two recession functions invariant regardless choice x 2 x 2 l x saddle point x theta proof according convex analysis 15 theorem 376 sufficient condition l x saddle point x theta convex functions l delta common direction recession 2 ri convex functions gammal x delta common direction recession x 2 ri x ri stands relative interior denote r n nonnegative orthants r n r respectively fixed 2 ri set directions recession l delta given similarly fixed x 2 ri x set directions recession gammalx delta statement 2 ri common direction recession interpreted 25 also get 26 fashion proposition 24 assumption 21 0 l x saddle point x theta proof suppose assumption 21 satisfied quadruple note p 2 p 0 w inequality implies therefore analogously q saddle point x theta proposition 24 says assumption 21 sufficient existence optimal solutions problem 11 13 well existence solutions 23 0 strict convexity sup y0 l x gamma inf x0 l x seen x unique 0 call set fx yj 0g central path problem 11 x saddle point l x x theta noted system 23 viewed monotone complementarity problem mapping several conditions discussed literature existence central path various situations example guler 7 studies conditions monotone complementarity problems shown conditions equivalent one proposition 24 context mapping f since proposition 24 deduced lemma 23 seems conditions lemma 23 weaker gulers conditions context saddle point problem 11 estimate error solution system 23 used approximate solution saddle point problem 11 prove duality gap solutions central path converges zero goes zero basic fact justifies interior pathfollowing algorithms proposition 25 assumption 21 given 0 proof assumption implies existence x together certain w satisfies system 24 definitions fx gy always weak duality therefore need prove second inequality analogous proof proposition 22 last equality based second equation 24 symmetric argument dual problem implies proposition 25 proved subtracting 29 28 proposition 25 provides estimation duality gap points central path denote positive diagonal matrices diagx respectively 0 obtain x exactly practice pathfollowing algorithm generates sequence close x k k closeness defined proximity function wx sy e vector ones compatible dimension notice ffix implies x central path ie x little abuse notations e used 210 matter dimension following result provides error bound approximate solution 23 satisfies 24 may satisfy equations 23 proposition 26 x proof notice proof inequalities 28 29 uses relationships 24 thus following proof proposition 25 hand hence proposition 26 provides estimation duality gap solutions neighborhood center path estimation order find pair ffloptimal solution 11 following sense x feasible 12 feasible 13 0 need find pair primal dual feasible solutions neighborhood central path satisfying ffix 3 convergence analysis predictorcorrector algorithm given point x number 0 satisfying 24 ffix ff consider path following method problem 11 typical iteration algorithm applies one step newtons method system certain constant newton approximation system 31 improving direction deltax deltay deltaw deltas determined deltay deltay associated direction called predictor affinescaling direction called corrector centering direction 1 following algorithm moves solution tight neighborhood central path loose one predictor step order reduce central path parameter draws solution loose neighborhood back tight one corrector step algorithm terminates ffl1 present algorithm algorithm 31 algorithm problem 11 first two equations 31 satisfied ffix ffl user assigned tolerance step 11 solve 32 deltax deltas p resulting directions let positive number predictor step step 12 solve 32 resulting deltax c deltay c deltaw c deltas c let corrector step update k go next iteration step 1 proceed show algorithm polynomial complexity purpose make assumptions functions oex scaled lipschitz condition slc oex given deltay r 2 ydeltay hold x 0 deltax 0 deltay satisfying jjx condition employed analysis interior point methods convex programs 33 functions satisfying condition include many useful functions could note oex necessarily separable general said elegant paper 24 tseng proposes pathfollowing algorithm variational inequality problems include nonlinear complementarity problems ncp worth noting slc condition oex weaker smooth condition 24 mapping smooth condition 24 allows jjx condition smooth constant infinite function like log x 1x deltax happen close gammax fact examples satisfy slc satisfy smooth condition 24 associated mapping f derivations frequently use relationships defined 32 following simple relationship x ffix holds analysis predictor step given superscript k omitted want know choose give us important information complexity algorithm algorithm sets explicit formula given following analysis let let diagonal matrices diagdeltax diagdeltas respectively use conventional notations real number note deltay deltas deltay deltas slc satisfied let deltay deltas estimate j 1 j 2 separately lemma 31 deltax deltaw deltay deltas proof let deltax deltay deltaw deltas therefore get deltaw deltas since apply slc estimate j 2 need prove following lemma lemma 32 predictor step proof let multiply sides x gamma12 w gamma12 therefore using 33 turn implies thus long part jjy gamma1 deltayjj proved similarly slc similarly following estimate related lemma 33 proof setting 32 deltay multiply sides first equation deltax second deltay deltay gammadeltay adding two equations together deltay gammadeltay gamma12 12 12 gamma12 second last inequality due ae jjx gamma12 w 12 deltaxjj gamma2 ae jjy gamma12 12 deltayjj gamma2 jjy 0 proposition 34 taking fl fl fl fl proof 34 lemmas 31 33 deltay deltas quantity greater tff1 gamma long simply solving quadratic equation picking large root replacing estimates lemma 31 33 satisfies corollary 35 predictor step central path parameter reduced least factor positive constant depending choices ff fi smooth coefficient slc analysis corrector step begin corrector step ffix w tff target show step follow clue used analysis predictor step estimate deltay deltas lemma 36 lemma 38 lemma 37 guarantee use slc lemma 36 ffix deltay deltas proof let deltax deltay deltaw deltas similar proof lemma 31 fl fl fl fl deltay deltas lemma 37 corrector step tff fi1 proof set 32 get deltay let multiply sides 35 deltay deltas also formulae deltaw deltas 32 get deltax deltay deltay deltas deltay deltay deltas hand deltay deltay hence get tff tff lemma 38 ffix proof 32 implies deltay deltay add two equations together arrange terms deltay deltay second inequality due ae ae 0 proposition 39 parameters ff satisfy example choose proof using similar derivations 34 invoking lemmas 36 38 deltay deltas thus ffix implies easy see addition choices algorithm ensure condition 24 satisfied k thus x k k feasible problems 11 12 respectively summary algorithm keeps k reducing linear rate proposition 25 readily see order find ffloptimal solution satisfying iterations necessary usually larger polynomial complexity taking advantage problem structure practical problems formulated problem 11 often special structures stance twostage model stochastic programming mentioned section 1 matrix r 2 large blockdiagonal whereas matrix r 2 oex ordinary size key point reduce amount work solving system 32 special feature system 32 allows us reduce amount computations substantially let us elucidate point detail problems large blockdiagonal r 2 small r 2 oex substituting deltaw deltas last two equations canceling deltay obtain equivalent system 32 deltay principal work solve first equation 36 since dimension x low less 100 say solve system 36 exactly even dimension high notice matrix r blockdiagonal structure hence computed blockwise particular computation proceeded parallel solution first equation 36 obtained factorization r might dense matrix must lowdimensional symmetric positive definite matrix deltax obtained first equation deltay calculated second equation 36 using r computed first step fact may store block factorizations r first step later use second step subsequently get deltaw deltas problems large blockdiagonal r 2 r 2 oex blockbanded discretized optimal control problems often large blockdiagonal r 2 r 2 oex described 18 22 addition matrix problem 11 partitioned blocks resulting matrix blocks banded 2829 instance dynamic constraint system expressed additional term saddle function 11 additional dual vectors generate infinite penalty violation constraints 37 problem 12 let primal control vector primal state vector seen submatrix problem 11 corresponding primal vector u x dual vector gammab gammaf computation r require compute sub matrices z auxiliary diagonal matrices note gammab gammaf gammab gammaf according structures block diagonal matrix 21 blockband matrix bandwidth 2 matrix h 22 also blockband one bandwidth 3 therefore possible solve equation system 36 using block operations applied banded system case x boxes explain changing x boxes complicate computation consider simple case x closeend boxes introducing auxiliary variables conditions 21 written explicit form follows corresponding newton equations become deltay certain fixed vectors canceling variables superscript 1 get system whose major equations certain fixed vectors canceling deltay system 38 becomes similar system 36 therefore computations needed solving system 37 roughly solving system 32 hence significant changes algorithm x changed boxes case x halfopenend halfcloseend boxes treated analogously 4 conclusions final remarks shown polynomiality predictorcorrector algorithm class nonlinear saddle point problems model extension traditional lagrange multiplier model optimization existence results central path relationship central path duality gap established section 2 useful developing interior pathfollowing algorithms several issues deserve investigation 1 smooth condition proposed slc smooth condition context convex programming various smooth conditions proposed characterize problems polynomial interior point algorithms refer reader references 4 13 details direction future studies identify different classes practical saddle point problems polynomial interior point algorithms smooth conditions weaker slc 2 infeasible starting point algorithm 31 requires starting point near central path could major obstacle starting point provided recently much research done interior point algorithms starting infeasible interior point see 122529 examples algorithms reduce infeasibility duality gap simultaneously makes practical sense develop infeasible interior point methods saddle point problems like 11 3 computationrelated developments primary studies 232829 show interior point methods might fairly efficient solving linearquadratic problems comparison studies interior point methods existing methods done large scale problems crucial reduce amount computations matrix inverse hessian evaluation meaningful develop algorithms instance use inexact directions eg 23 inaccurate hessians approximate factorizations predictor corrector steps r decomposition partitioning methods multistage stochastic linear programs efficient solution twostage stochastic linear programs using interior point methods computing blockangular karmarkar projections application stochastic programming unifying investigation interiorpoint methods convex programming solution twostage linear programs uncertainty interior point path following algorithms existence interior point interior paths nonlinear monotone complementarity problems predictorcorrector method linear complementarity problems polynomial complexity superlinear convergence implementation lagrange finite generation method primaldual algorithm class linear complementarity problems unified approach interior point algorithms linear complementarity problems superlinearly convergent infeasibleinteriorpoint algorithm geometrical lcps without strictly complementary condition interior point polynomial algorithms convex programming sqp algorithm extended linearquadratic problems stochastic programming convex analysis linearquadratic programming optimal control computational schemes largescale problems extended linearquadratic programming largescale extended linearquadratic programming multistage optimization nonsmooth optimization finite simplexactiveset method monotropic piecewise quadratic programming lagrangian finite generation technique solving linearquadratic problems stochastic programming generalized linearquadratic problems deterministic stochastic optimal control discrete time inexact predictorcorrector method extended linearquadratic programs global linear convergence pathfollowing algorithm monotone variational inequality problems infeasible pathfollowing method monotone complementarity problem lshaped linear programs applications optimal control stochastic linear programs solving stochastic programs simple recourse interior point methods optimal control discretetime systems superlinear infeasibleinteriorpoint algorithm monotone complementarity problems quadratic p nl convergence predictorcorrector algorithm lcp primaldual steepest descent algorithm extended linearquadratic programming primaldual projected gradient algorithms extended linearquadratic programming pathfollowing algorithm class convex programming problems tr