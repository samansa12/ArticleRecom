pac analogues perceptron winnow via boosting margin describe novel family pac model algorithms learning linear threshold functions new algorithms work boosting simple weak learner exhibit sample complexity bounds remarkably similar known online algorithms perceptron winnow thus suggesting wellstudied online algorithms sense correspond instances boosting show new algorithms viewed natural pac analogues online pnorm algorithms recently studied grove littlestone schuurmans 1997 proceedings tenth annual conference computational learning theory pp 171183 gentile littlestone 1999 proceedings twelfth annual conference computational learning theory pp 111 special cases algorithm taking p equals 2 p equals obtain natural boostingbased pac analogues perceptron winnow respectively p equals case algorithm also viewed generalization improved sample complexity bound jackson cravens pacmodel boostingbased algorithm learning sparse perceptrons jackson craven 1996 advances neural information processing systems 8 mit press analysis generalization error new algorithms relies techniques theory large margin classification b introduction one fundamental problems computational learning theory learning unknown linear threshold function labeled examples many different learning algorithms problem considered past several decades particular recent years many researchers studied simple online additive multiplicative update algorithms namely perceptron winnow algorithms variants thereof 3 5 8 14 15 16 25 26 27 28 33 36 paper takes different approach describe natural parameterized family boostingbased pac algorithms learning linear threshold functions weak hypotheses used linear functionals strong classifier obtained linear threshold function although new algorithms supported part nsf graduate fellowship nsf grant ccr9504436 onr grant n000149610550 conceptually algorithmically different perceptron winnow establish performance bounds new algorithms remarkably similar perceptron winnow thus refer new algorithms pac analogues perceptron winnow hope analysis new algorithms yield fresh insights relationship boosting online algorithms give unified analysis perceptron winnow analogues includes many algorithms well grove littlestone schuurmans 16 shown perceptron version winnow viewed cases general online pnorm linear threshold learning algorithm p 2 real number present pacmodel boostingbased analogues online pnorm algorithms value 2 p 1 pacmodel perceptron winnow analogues mentioned respectively cases general algorithm case algorithm also viewed generalization jackson cravens pacmodel algorithm learning sparse perceptrons 20 algorithm boosts using weak hypotheses single boolean lit erals similar case algorithm analysis case generalizes algorithm deal realvalued rather boolean input variables yields substantially stronger sample complexity bound established 20 section 2 paper contains preliminary material including overview online pnorm algorithms 15 16 section 3 present simple pacmodel pnorm algorithm prove weak learning algorithm 2 section 4 apply techniques theory large margin classification show learning algorithm boosted strong learning algorithm small sample complexity finally section 5 compare pac algorithms analogous online algorithms extend algorithm case discuss relationship case algorithm jacksoncraven algorithm learning sparse perceptrons 11 related work several authors studied linear threshold learning algorithms work combining weak predictors freund schapire 14 describe algorithm combines intermediate perceptron algorithm hypotheses using weighted majority vote final classifier depth2 threshold circuit prove bounds generalization error resulting classifier algorithm use boosting combine perceptron hypotheses rather weights according survival time ji 21 propose randomsearchandtest approach find weak classifier linear threshold functions combine simple majority vote thus also obtaining depth2 threshold circuit approach closest jackson craven 20 use boosting combine single literals strong hypothesis linear threshold function described section 5 case algorithm strengthens generalizes results generally also note freund schapire 12 schapire 32 exhibited close relationship boosting online learning start geometric definitions point denote pnorm namely 1norm x kxk qnorm dual pnorm 1 hence 1norm 1norm dual 2 norm dual paper p q always denote dual norms following facts well known eg 37 pp h older inequality ju delta minkowski inequality ku throughout paper example space x subset linear threshold function x function f function signz takes value 1 z 0 takes value note standard definition linear threshold function allows nonzero threshold ie real number however linear threshold function general form n variables equivalent linear threshold function threshold 0 definition incurs real loss generality write kxk p denote sup x2x kxk use symbol ux denote quantity x2x measure separation examples x hyperplane whose normal vector u assume throughout paper kxk p 1 ie set x bounded ffi nonzero lower bound separation hyperplane defined u examples x 21 pac learning denote example oracle queried provides labeled example x drawn according distribution x say algorithm strong learning algorithm u x satisfies following condition function x distribution x makes mffl ffi calls exu probability least outputs hypothesis h 1g pr x2d hx 6 signu delta say hypothesis h fflaccurate hypothesis function mffl ffi x sample complexity algorithm main result describe strong learning algorithm carefully analyze sample complexity must consider algorithms satisfy strong learning property still capable generating hypotheses slight advantage random guessing socalled weak learning algorithms first considered kearns valiant 24 let finite sequence labeled examples x let distribution say say algorithm 12 gamma flweak learning algorithm u following condition holds finite set described distribution given input outputs hypothesis 12 gamma flapproximator u thus purposes weak learning algorithm one always find hypothesis outperforms random guessing fixed sample 22 online learning pnorm algorithms online model learning takes place sequence trials throughout learning process learner maintains hypothesis h maps x fgamma1 1g trial proceeds follows upon receiving example x 2 x learning algorithm outputs prediction associated label learning algorithm given true label 2 fgamma1 1g algorithm update hypothesis h based new information next trial begins performance online learning algorithm example sequence measured number prediction mistakes algorithm makes grove littlestone schuurmans 16 gentile littlestone 15 studied family online algorithms learning linear threshold functions see figure 1 refer algorithm parameterized real value online pnorm algorithm like wellknown perceptron algorithm online pnorm algorithm updates hypothesis making additive change weight vector z however shown steps 45 figure 1 pnorm input parameter real number p 2 initial weight vector positive value 0 1 set 2 examples available 3 get unlabeled example 4 5 predict 6 get label 2 fgamma1 1g 7 8 set 9 enddo figure 1 online pnorm algorithm algorithm use z vector directly prediction rather predicts using vector w transformed version z vector namely w w hence online 2norm algorithm perceptron algo rithm 16 shown p 1 online pnorm algorithm approaches version winnow algorithm precisely following theorem 16 gives mistake bounds online pnorm algorithms theorem 1 let sequence labeled examples every example hx yi 2 2 0 online pnorm algorithm invoked input parameters p z mistake bound example sequence b 2 mistake bound z 0 c let suppose described part b mistake bound given b converges ux log log 23 online pac learning various generic procedures proposed 1 18 22 automatically converting online learning algorithms pacmodel algorithms procedures sample complexity resulting pac algorithm depends mistake bound original online learning algorithm strongest general result type terms minimizing sample complexity resulting pac algorithm longestsurvivor conversion due kearns li pitt theorem 2 let online learning algorithm guaranteed make mistakes pacmodel learning algorithm 0 uses logffi log examples outputs fflaccurate hypothesis probability theorems 1 2 yield sample complexity bounds generic pacmodel conversion online pnorm algo rithm describe completely different pacmodel algorithm remarkably similar sample complexity bounds 3 pacmodel pnorm weak pnorm weak learning algorithm motivated following simple idea suppose collection labeled examples replacing negative example equivalent positive example obtain new collection 0 examples let average location example 0 ie z center mass every example 0 must lie side hyperplane vector u clear z must also lie side hyper plane one might even hope z related vector points approximately direction vector u pnorm weak learning algorithm call wla presented figure 2 online pnorm algorithm wla transforms vector z vector w using mapping show simple algorithm fact weak learner theorem 3 wla 12 gamma flweak learning algorithm 1 littlestone 27 gives conversion procedure yields pac sample complexity bound offl although improves result 22 log factor littlestones procedure requires example space x finite stronger assumption make paper input parameters real number p 2 sequence labeled examples distribution 1 set 1 2 return hypothesis hx j figure 2 pnorm weak learning algorithm wla proof let sequence labeled examples x 2 x x every let distribution show hypothesis h wlap returns see h maps x gamma1 1 note holders inequality implies show inequality 1 section 21 holds thus2 wk q thus suffices show wk q first note x ja hence lefthand side desired inequality equals second equality used fact p gamma p consequently lefthand side simplified kzk p thus goal show kzk p ffi x ja last line follows holder inequality theorem proved shown simple wla algorithm weak learning algorithm halfspace learning problem section use techniques boosting large margin classification obtain strong learning algorithm small sample complexity 41 boosting achieve high accuracy series important papers schapire 31 freund 10 11 given boosting algorithms transform weak learning algorithms strong ones paper use adaboost algorithm 13 shown figure 3 notation algorithm similar 34 35 input adaboost sequence labeled examples weak learning algorithm wl two parameters given distribution outputs hypothesis h adaboost successively generates new distributions uses wl obtain hypotheses h ultimately outputs final hypothesis linear threshold function h 13 freund schapire prove algorithm wl 12 gamma flweak learning algorithm ie call wl adaboost generates hypothesis h ffl fraction examples misclassified final hypothesis h given result one straightforward way obtain strong learning algorithm halfspace learning problem draw sufficiently large specified sample example oracle exu run adaboost using wla weak learning algorithm fl given theorem 3 choice ensures adaboosts final hypothesis makes errors moreover since hypothesis generated wla form h v final hypothesis using wellknown fact vc dimension class zerobias input parameters sequence labeled examples weak learning algorithm wl gamma1 1 two real values 1 set log 1 2 3 4 let h output wld 5 set ffl 7 normalizing factor t1 distribution 9 enddo 10 output final hypothesis hx j signfx figure 3 adaboost algorithm linear threshold functions n n main result implies probability least 1 gamma ffi final hypothesis h fflaccurate hypothesis u provided jsj cffl gamma1 n logffl c 0 analysis though attractively simple yields rather crude bound sample complexity depend particulars learning problem ie u x rest section use recent results adaboosts ability generate largemargin classifier generalization ability largemargin classifiers give much tighter bound sample complexity learning algorithm 42 boosting achieve large margin suppose classifier form say margin h labeled example hx yi yfx note quantity nonnegative h correctly predicts label associated x following theorem extension theorem 5 34 shows adaboost generates large margin hypotheses theorem 4 suppose adaboost run example sequence using weak learning algorithm wl gamma1 1 value 0 theorem stated 34 covers case wl maps fgamma1 1g need general version weak hypotheses theorem 3 map gamma1 1 rather fgamma1 1g proof theorem 4 given appendix results section 3 imply wla used learning algorithm adaboost value ffl always 12 gamma fl upper bound theorem 4 becomes 1gamma2fl 1gamma 12fl 1 easy lemma proved appendix b 14 set apply lemma upper bound theorem 4 becomes obtain following corollary 6 adaboost run sequence labeled examples drawn exu using wla learner fl defined theorem 3 hypothesis h adaboost generates margin least fl2 every example proof bound causes greater 2 log 1 consequently upper bound theorem 4 less 1jsj next subsection use corollary 6 theory large margin classification establish bound generalization error h terms sample size 43 large margins generalization let f collection realvalued functions set x finite set fx said shattered f real numbers r b function f b 2 f 0 fatshattering dimension f scale denoted fat f size largest set shattered f finite infinity otherwise fatshattering dimension useful us following theorem 4 theorem 7 let f collection realvalued functions x let distribution x theta fgamma1 1g let sequence labeled examples drawn probability least 1 gamma ffi choice classifier hx j signfx f 2 f margin least 0 every example pr log 8em 8m noted section 41 final hypothesis h adaboost outputs must form x invocation wla generates hypothesis form x kv k q 1 implies vector must satisfy kvk q 1 consider class functions ae x 7 oe bound fat f given sample size theorem 7 immediately yields corresponding bound pr x2d hx 6 signu delta x halfspace learning prob lem following theorem proved appendix c gives desired bound fat f theorem 8 let x bounded region n let f class functions x defined 2 fat f 2 log 4n combining theorem 3 corollary 6 theorems 7 follows algorithm uses sample size probability least 1 gamma ffi hypothesis h generated satisfy pr x2d log n log 2 log thus established following onotation hides log theorem 9 algorithm obtained applying adaboost wla using parameter settings described corollary 6 strong learning algorithm u x sample complexit sample complexity boostingbased pnorm pac learning algorithm remarkably similar pac transformed online pnorm algorithms section 21 log factors sets bounds depend linearly ffl gamma1 quadratically kuk q kxk p ffi comparing bounds detail see online variant described part theorem 1 extra factor bound present sample complexity algo rithm variant offers advantage though user need know values quantities kxk p kuk q advance order run algorithm turning part b theorem 1 see parameter set appropriately online algorithm online bound differs pac algorithm bound extra factor z 0 ignoring log factors part c theorem 1 shows even z 0 chosen also note omegagamma557 n gentile littlestone 15 given alternative expressions online pnorm bounds terms kxk1 using entirely similar analysis bounds algorithm analogously rephrased case well 51 since case online pnorm algorithm precisely perceptron algorithm case algorithm viewed natural pacmodel analogue online perceptron algorithm note upper bound given lemma 12 appendix c strengthened delta kxk 2 see lemma 13 4 theorem 41 2 proof means fatshattering dimension upper bound theorem 8 improved 2 removes log factor bound theorem 9 however bound still contain various log factors log terms theorem 7 52 algorithm extreme define natural algorithm consider vectors z w computed weak learning algorithm wla let r number coordinates z z jz lim wk q ae signz r jz hence natural consider version wla denote wla 0 vector w defined taking wise analysis continues hold minor modifications described appendix obtain strong learning algorithm theorem 9 holds place wla close relationship work jackson craven learning sparse perceptrons 20 note one coordinate z jz wla 0 hypothesis kxk1 signed variable strongly correlated distribution value signu delta similar weak learning algorithm used jackson craven 20 takes single bestcorrelated literal hypothesis break ing ties arbitrarily proof bestsingleliteral algorithm used 20 weak learning algorithm due goldmann hastad razborov 17 however proof 17 assumes example space x f0 1g n target vector u integer coefficients thus noted jackson craven 20 algorithm learning sparse perceptrons applies learning problems defined discrete input domains contrast algorithm applied continuous input domains restrictions example space x target vector u satisfy also observe theorem 9 establishes tighter sample complexity bound strong learning algorithm given 20 see let suppose target vector coefficients algorithm 20 applied learning problem ffi ux omegagamma20 letting theorem 9 implies learning algorithm sample complexity roughly 2 ffl ig noring log factors substantial improvement roughly 4 ffl sample complexity bound given 20 generally sample complexity bound given 20 learning ssparse kperceptrons roughly ks 4 ffl analysis paper easily extended establish sample complexity bound roughly ks 2 ffl learning sparse kperceptrons 6 open questions results give evidence broad utility boosting algorithms adaboost natural question much utility extends simple boostingbased pac versions standard learning algorithms note context kearns mansour 23 shown various heuristic algorithms topdown decision tree induction viewed instantiations boosting another goal construct powerful boostingbased pac algorithms linear threshold functions algorithms discussed paper inverse quadratic dependence separation parameter ffi linearprogramming based algorithms learning linear threshold functions see eg 6 7 9 29 30 dependence natural boostingbased pac algorithm linear threshold functions performance bounds similar linearprogramming based algorithms acknowledgements warmly thank les valiant helpful comments suggestions r machine learning 2 probabilistic method proc 36th symp found comp sci advances kernel methods support vector learning perceptron algorithm fast nonma licious distributions proc 37th symp found comp sci learnability vapnikchervonenkis dimension proc 38th symp found comp sci boosting weak learning algorithm jority fifth ann work comp learning theory proc ninth ann conf comp learning theory decisiontheoretic generalization online learning application boosting proc eleventh ann conf comp learning theory proc 12th ann conf comp learning theory proc 10th ann conf comp learning theory majority gates vs general weighted threshold gates space efficient learning algorithms probability inequalities sums bounded random variables advances neural information processing systems 8 combinations weak classifiers proc fourth int workshop machine learning proc 28th symp theor comp 21st acm symp theor comp proc eighth ann conf comp learning theory learning quickly irrelevant attributes abound new linearthreshold algorithm mistake bounds logarithmic linearthreshold learning algorithms proc fourth ann conf comp learning theory halfspace learning comput learning theory natural learning systems volume constraints prospects strength weak learnability proc twelfth ann conf comp learning theory proc twelfth ann conf comp learning theory boosting margin new explanation effectiveness voting methods proc eleventh ann conf comp learning theory criteria lower bounds perceptronlike learning rules advanced calculus tr