theory practice vector quantizers trained small training sets examines performance memoryless vector quantizer changes function training set size specifically authors study well training set distortion predicts test distortion training set randomly drawn subset blocks test training images using vapnikchervonenkis vc dimension authors derive formal bounds difference test training distortion vector quantizer codebooks authors describe extensive empirical simulations test bounds variety codebook sizes vector dimensions give practical suggestions determining training set size necessary achieve good generalization codebook authors conclude using training sets comprising small fraction available data one produce results close results obtainable available data used b introduction vector quantization vq 7 8 data compression technique used reduce storage transmission costs binary grayscale images lossy compresseduncompressed image degraded copy original image major part computational cost vq designing codebook used encode image design usually done training codebook set images somehow representative images encoded generally presumed data used design vq codebook better codebook encode test images data consist blocks pixels extracted training image alternatively refer training blocks training vectors training set ten 512 theta 512 pixel images broken 4 theta 4 blocks 163840 blocks available training encoding grayscale image one half bit per pixel requires approximating 4 theta 4 vectors codebook 256 representative vectors statistically distribution vectors image well represented even small random subsample total available training set since computational cost codebook design heavily dependent size training set would like able determine point diminishing returns larger training set size outweighed additional time required train purpose paper investigate size training sets sufficient construction codebooks nearly good codebooks trained available data describe theoretical empirical results theory show vq problem viewed learning problem allows us derive upper bounds bounds already known learning theory size training set needed build good codebooks based size codebook dimension vectors codebook unfortunately bounds general give useful advice practitioners fortunately learning theory analysis vq problem suggested empirical study eventually led us results form basis practical advice paper appears ieee transactions pattern analysis machine intelligence 1615465 suggested learning theory analysis right thing look value test train decreases size training set increases value test train call generalization curve difference test error codebook training error codebook train larger larger samples image set images generalization curve approaches zero surprisingly generalization curve shown empirical studies simple functional form namely ffm ff constant size training set call constant ff learning complexity codebook image set images main determinant large training set needed build good codebook example typical images value ff less 50 codebooks size 256 16 dimensional vectors one desires 1 difference testing training error size training set need larger training set amounts 30 entire 512 theta 512 pixel image training set 10 images rather single image 5000 vectors needed training set represents 3 potential training vectors problem training set size also studied independently stanford university 4 different consistent results also considered informationtheoretic viewpoint david pollard 14 summarize remainder paper follows section 2 provides brief introduction vector quantization details derivation bounds training set sizes vq codebooks section 3 empirically examine generalization error difference test training distortion vq codebooks respect bound find formal worstcase bounds derivable theory tight enough provide practical guidance codebook design describe empiricallyderived average case worst case performance cases generalization error found approach zero inversely proportional size training set section 4 examine case training testing sets differ common practical occurrence find although theory section 2 unable make predictions case empirical results section 3 appear apply finally section 5 discuss practical guidelines deriving work indicating using training sets comprised fraction available data one produce results close results obtainable available data used also suggest methods researchers may use determine appropriate training set sizes vq problems vector quantization vapnikchervonenkis dimension applying standard vq image image first broken typically rectangular pixel blocks eg 4 theta 4 pixels blocks kdimensional vector image quantized assigning blocks closest vector metric small number predetermined vectors reduced set vectors codebook used encode image may simply store transmit index selected codebook vector block rather storing transmitting entire block encoding grayscale image eight bits per pixel codebook 256 4 theta 4pixel blocks require log 2 bits used every 128bit block resulting 16 1 compression ratio indicate compression number bits per pixel b bit rate quantization imposes degradation image quality extent governed distribution blocks kdimensional vector space size codebook care codebook vectors chosen new codebook may designed source image single codebook may designed quantize large number images class given image set images fixed block size fixed codebook size iterative algorithms generalized lloyd algorithm gla select codebook vectors locally optimize image degradation measure 12 typical distortion measures meansquared error weighted meansquared error itakura saito distortion speech 10 first introduce pattern classification problem formal bounds derived using vapnikchervonenkis dimension show bounds may used bound difference training test performance vq codebook 21 pattern classification vcdimension results 1 16 17 concern asymptotic performance learning systems specifically results bound difference empirically observed performance system true performance function number inputs empirical performance observed concerned results apply pattern classification pattern classification problem given domain x associated unknown probability density p unknown subset c x called target concept wish learn indicator function x2c indicates point x 2 x c x2c indicates x c let us consider hypothesis concept c x define generalization error error rate c respect fixed target concept c distribution p x2c x x drawn according p 1 p ra denotes probability event based information training examples attempt choose concept c minimizes error rate note twosided error measure points c c error points c c cases error minimization done making empirical estimate fflc c p choosing concept lowest empirical error simplest way measure empirical estimate measuring empirical error sample points drawn p write ae 0 x2c otherwise 2 involved methods estimating empirical error discussed 18 typically hypothesis chosen according rule points within euclidean distance 1 point z rule defines concept class c diversity hypotheses class class representational power may indexed vapnikchervonenkis dimension vcdimension class vcdimension concept class defined follows consider set points concept c concept classifies point either 1 0 thus imposes labeling set set points 2 possible labelings say concept class c shatters set exists concept c imposes 2 possible labelings vcdimension class size largest set shattered c example vcdimension class balls kdimensional euclidean space 3k 1 given empirical error fflc c selected class vcdimension theorems chervonenkis bound probability fflc c p exceed value specifically 2m ln 2m 2m 16 another popular way expressing bound difference fflc c p fflc c testing training errors last term equation 3 describes bound learning curve example let us assume selected concept c class vcdimension based training examples error training examples 3 fflc c 95 confidence 005 true error c less 1014 detailed description vcdimension use formal learning theory beyond scope paper may found 1 16 22 framing vq classification problem bound previous subsection useful lets us predict generalized performance based observed performance applied vq would allow predicting distortion codebook image based performance small part whole image would great computational advantages designing codebook since training time depends heavily training set size use theorems vector quantization must able frame vq classification problem first step define x domain problem interested encoding binary images using kdimensional vectors domain simply space kdimensional binary vectors interest 8bit grayscale images kdimensional vectors domain would point domain represents kdimensional block might appear image respect memoryless vector quantizer every image set images may viewed unordered set points domain set corresponds formal notion concept learning theory frequency points occur images defines distribution p domain note concept formal model corresponds many possible images one imagine shuffling ordering blocks image memoryless quantizers possible permutations blocks equivalent concern choosing vq codebook encode image least possible error begin describing simple tolerance error measure fits well concept learning framework following subsection extend model include practically useful error measures 23 tolerance measure classification problem one measures error terms probability example classified correctly incorrectly popular vq distortion measures measure far correct given encoding block reconcile two approaches define simple vq distortion measure shall refer tolerance measure simply stated vq codebook said zero error block encodes block within tolerance describe error codebook image probability encode random block image within specified tolerance 231 binary images begin analysis considering binary blackwhite images consider point x 2 x say codebook oe think concept covers x vector v oe hamming distance x v written hv x r bits cover r oe rtolerance error codebook image probability block drawn random image fail encoded within r bits drawn random 4 empirical estimate ffl r oe may made based sample blocks drawn random ae concepts represent sets points covered codebook n kdimensional vectors using rtolerance criterion technically target concept set vectors appear image simplify problem defining target concept entire domain x penalty covering vector occur image penalty covering vector occur since vectors occur image zero probability drawn random sample error measures two concepts identical learning part vector quantization involves finding codebook minimal nearminimal error image moment concern minimizing rtolerance error given r typically one specifies vector dimension number vectors codebook oe advance based desired bit rate encoding complexity tolerated defines hypothesis class select hypothesis given training set set vectors training algorithm attempt minimize empirical error training set sufficiently large representative images wish encode learned codebook provide nearminimal error test images one fixed set images sufficient computational power straightforward use entire image training set investigate remainder paper codebooks trained small fraction available data perform bulk images denote rtolerance errors codebook follows rtolerance error rate codebook oe image rtolerance error rate codebook oe blocks drawn image 232 grayscale images fit grayscale images tolerance model need additional parameter binary images given pixel either correct incorrect compared codeword grayscale image pixel realvalued distortion respect codeword accommodate define parametric threshold grayscale image r ttolerance given block zero error r pixels distortion greater defined arbitrarily whichever pixel distortion measure interest us straightforward extend notation binary images grayscale images ttolerance error rate grayscale codebook oe image ttolerance error rate grayscale codebook oe blocks drawn image 24 determining vcdimension vq codebook speak vcdimension codebook actually referring vcdimension class codebooks meeting specification case mean class kdimensional nvector codebooks using specific tolerance error measure denote class codebooks n kdimensional vectors vcdimension binary codebook class phi nk using rtolerance error measure grayscale codebook class phi nk using r ttolerance error measure say class codebooks shatters set vectors exists codebook class covers 2 jsj possible subsets including empty set vcdimension class codebooks cardinality largest set vectors shattered class brevity remainder paper simply refer vcdimension codebook understanding dimension formally applies class codebook member simplest upper bound place vcdimension binary vq codebook class derived combinatorial argument 1 order shatter blocks must codebook encodes correctly 2 subsets blocks requires class include least 2 distinct codebooks since class kdimensional nvector codebooks contains exactly distinct codebooks class shatter log 2 blocks upper bound vcdimension class assuming 8 bits intensity resolution grayscale images upper bound number distinct grayscale codebooks bounding vcdimension less log 2 slightly tighter upper bounds derive specific tolerance models derivation complex improvements bounds minor remainder paper simply use combinatorial bounds described framed vector quantizer pattern classifier determined numerical bounds complexity classifier allows us achieve goal bounding rtolerance distortion entire image function rtolerance distortion small training set drawn appropriate modification equation 3 2m ln 2m 25 relating tolerance mean distortion rate terms predicting coded image quality neither rtolerance r ttolerance measures appear robust useful measures main utility giving us starting point look commonly used distortion measures binary images common measure distortion vector quantization average bit error grayscale images common measure distortion meansquared error mse demonstrate use equations bound average bit error binary codebook given either average bit error training set rtolerance training errors k similarly bound average mse grayscale codebook given either mse training set r ttolerance training errors 251 binary images begin always considering case binary images deriving bound average bit error let error codebook oe image error codebook oe example blocks drawn image image broken kdimensional blocks distortion expressed average rtolerance bit errors range 0 r k expected bit errors total number bits expected bit error per vector size vector note final sum 0 express average bit error bound function rtolerance training error using equation ffl oe k 2m ln 2m information tolerance errors codebook training set use inequality directly information meansquared error training set must make approximations settle looser bound define dn may substitute dimensions rtolerance model since kg write 2m ln 2m 252 grayscale images grayscale images transition tolerance useful distortion measure simple binary images consider case mse distortion addition summing tolerances r sum thresholds must take account fact pixel errors squared summed follow convention image compression community treating pixel value 8bit integer rather fraction results individual pixel distortions range 0 65025 255 2 rather 0 1 much case binary images using hamming distortion many terms grayscale mse distortion expression telescope give relatively compact equation ffl oe binary case substitution may made bounding equations resulting equation somewhat messy detail 3 empirical results results derived previous section would significant value right proved describe typical behavior vq codebook however shall see theoretical worstcase bounds appear far typically observed performance even far empirical worstcase derived experimentally section explore empirically behavior vq codebooks trained small training sets order compare derived theoretical predictions first describe methodology followed series vector quantizer experiments describe result running experiments set typical images results indicate bounds previous section much loose apply average case describe results search empirical worst case image one allow us define empirical upper bound difference training test performance vq codebook 31 given source image first block image square pixel blocks k pixels partial blocks edges image discarded denote total number blocks image blocks select training set blocks random sampling replacement training set used input program running generalized lloyd algorithm described 12 output program codebook locally minimal distortion training set denote codebook oes measure ffl oes distortion codebook imposes training set ie training error ffl oes distortion codebook imposes entire source image test error determine dependence value repeated procedure fixed k n values ranging 50 blocks size source image value ran number trials ranging 50 500 trials depending variance observed differences figure 1 plots averages ffl oes training set size blocks distortion 120140test train figure 1 typical run one using codebook 128 25dimensional binary vectors error diffused man image experiments drawn replacement training test sets overlap results previous section based assumption sampling done distribution corroborating results requires assumption violated many parts data compression community however call separate training test sets generally disjoint selected without replacement set images reconcile differences difference distortion test set disjoint training set one may described disjoint test set given size image bound difference ffloe ffloe error entire image image minus training set practice difference appears small fixed training set size goes zero image size increases although issue sampling without replacement simple equivalence albeit empirical one may still observed drawing distribution continuous domain issue replacement vs nonreplacement normally moot unless distribution discrete places probability drawing point twice zero however source finite image nonzero probability drawing blocks random may draw block giving us redundancies data set 1 source training examples large drawing without replacement essentially equivalent differences appear sample training set takes appreciable fraction available examples empirically found relationship two sampling paradigms examining difference training test errors specifically find sample drawn replacement sample drawn without replacement test gamma training distortion differs factor 1 difficult find extreme cases relationship breaks holds well typical cases examined allows conversion nonreplacement approach theoretically examined approach sampling replacement detailed treatment relationship see 3 32 relation worstcase bounds singleimage learning experiments examined binary grayscale images three sources photographic images usc database mri brain scans computergenerated line drawings binary images generated halftoning grayscale images ordered dithering 13 15 error diffusion 6 images tested difference observed behavior worstcase bound remarkable true distortion codebook test image rapidly approaches asymptotic value theoretical bound codebooks distortion remains surprisingly high even large training set sizes figure 2 plot bound equation 8 using upper bound vcdimension codebooks involved 2 also plot bound assuming trivial lower bound vcdimension 128 assuming codebook vector covers single vector figure 2 shows bound confidence parameter ffi set 05 indicating bound guaranteed hold least 50 time realistic sample sizes though effect varying ffi minor may consider bound applying almost certainty unfortunately graph indicates even bound using trivially small vcdimension conservative provide realistic guidance typical problem even using tighter tolerance bound equation 7 along tolerance information gleaned training process fails produce upper bound distortion approaches observed behavior determined formal bound provide direct guidance typical case turn information theory guidance well empirical evidence experiments areas examining vcdimension bounds 33 averagecase experiments ease reading shall refer quantity ffl oes remainder section pollard 14 shown certain strong conditions expected value test gamma train decrease o1m codebook designed using optimal kmeans clustering algorithm empirical work artificial neural networks eg 2 observed behavior cases vcdimension theory predicts worst case equation 3 examine test gamma train empirically changes training set size block size k codebook size n typical images determined value test gamma train distortion closely follows firstorder polynomial test 1 easy confuse issue drawing distinct identical blocks clarify imagine block image labeled row column image appears drawing without replacement could still expect draw many blocks consisting vector would never draw twice coordinate 2 value use blog 2 training set size blocks 1000 2000 3000 4000 5000 6000 7000 8000 distortion 1000 bound bound test mean std dev test mean train mean figure 2 theoretical upper bounds test distortion codebook 128 16dimensional binary vectors trained typical image two bounds plotted one using upper bound possible vcdimension codebook 1331 one using trivial lower bound vcdimension codebook 128 disparity even lower two bounds actual test distortion also plotted indicates worstcase bounds may provide useful information image used errordiffused man usc database binary grayscale images remarkably good fit equation 9 sampling done replacement see figure 3 noticeable deviations polynomial model small training set sizes large codebooks used consistent observations made 18 pointing learner sufficiently powerful training set sufficiently small rather learning generalize learner simply memorizes data extent memorization qualitatively different phenomenon generalization beyond scope study drawn without replacement test gamma train distortion follows inverse firstorder polynomial modifying linear factor namely test ff figure 4 plots generalization curves codebooks sampled without replacement well best fit respective equations 331 learning complexity typical images one surprising observation experiments spite fact training test distortion varied widely image image difference relatively constant given fixed training set size block size codebook size see figure 5 photographs used usc database fit equation 9 similar values ff thus similar learning complexities images mri brain scans computergenerated line drawings gave appreciably different lower learning complexities photographs instructive give values concrete example binary vq achieve expected test gamma train distortion less 01 one must train least ff0001 2500 binary blocks appears based experiments one may quantify learning complexity typical class images photographs satellite images line drawings etc describe training set size blocks testtrain distortion 020100data points figure 3 best fit typical run sampled replacement inverse firstorder polynomial dashed line represents standard error mean image used errordiffused man quantized codebook 128 16dimensional binary vectors quantified ff learning complexity class binary grayscale photographs although fi technically also factor equation empirically seems small value thus play major role equation figure 6 plots learning complexity ff binary grayscale images varies block size codebook size note grayscale images value ff significantly dependent block size size codebook binary images small significant increase ff increasing block size brevity extrapolate following characteristic equations 34 learning complexity worstcase images learning complexity indication difficult quantize image well much image need see know well quantize given algorithm must image set images nk test training distortion codebook n kdimensional vectors converge slowly average able describe produce worstcase image would great benefits measuring generalization curve image would provide upper bound learning complexity ff arbitrary image given training set true distortion codebook would less equal ffl oes probability refer ff maximum learning complexity codebook without reference image unable analytically derive ff determined grayscale case learning complexity image appears related nearlinear manner entropy image binary images relationship appears nonmonotonic ff maximum images intermediate entropy see figure 7 although binary case needs study straightforward training set size fraction image testtrain distortion 010rep data norep data replacement noreplacement figure 4 best fits runs sampled without replacement respective models image quantized errordiffused man using codebook 128 25dimensional binary vectors construct grayscale image following entropy plot maximal learning complexity construct completely random image ie one maximal entropy pixel either completely completely note even though image technically binary still qualifies highcontrast grayscale image figure 8 plots empirical worstcase learning complexities image case appears dependence block size k small enough may ignored firstorder approximation empirical worstcase ff may approximated may noted although empirical worstcase order magnitude worse average observed case still far theoretical bounds described previous section 4 multipleimage learning experiments applications codebook designed solely purpose encoding particular image common one codebook trained set images purpose encoding different set images section discuss extension results previous section problem learning encoding multiple images important differences must considered working multiple images test images different source training examples obviously problem difference training test distortions longer asymptotically converge zero second even could guarantee bound difference training distortion test distortion would guarantee performance individual test images guarantee test gamma train set ten test images less 5 could still give 1 distortion nine images 40 distortion tenth although formal theory described section 2 stymied use disjoint training test images found practical performance examined section 3 may still used little modification detail codebook trained image set tested image set training distortion training set size increases test distortion ffl oes approach limit ffl oes since typically ffl oes 6 ffl oes test training dis training set size fraction image testtrain distortion 20 lax 64 lax e 64 man 64 man e 64 figure 5 rate convergence test training distortion roughly halftoned photographs despite widely varying individual test training distortions tortion converge thus cannot fit asymptotically converging form like equations 9 10 thus examine first derivative equation 9 deltatest dm test ff indicates much additional training block improve performance regardless asymptotic error rates ffl oes ffl oes value approach zero increases indicating increasing training set size little effect previous singleimage generalization curves fit equation 9 also fit equation 11 parameter values see values must adjusted accommodate multiple images experiments chose two image sources set seven photographs usc database set eleven goes weather satellite images first series experiments images compared derived values ff singleimage training sources previous section sources comprised many images illustrated figure 9 learning complexities multiimage sources almost single image sources indicating relative unimportance size training image source examined effect ff testing images part training set experiments codebook trained set either ten goes images reserving one day six usc images reserving man codebook tested image training set reserved image training image source reserved image image source results plotted figure 10 generalization curves match equation 11 well indicate convergence rate based ff relatively insensitive source test images implications observations important indicate degree empirical results section 3 applicable multipleimage learning well training source image blocks say fixed finite set weather satellite images attempt encode blocks another source say infinite set weather satellite photos see future minimum distortion possible use available data ffl oes results indicate rate minimum approached relatively independent size sources indicate makes difference data train codebook simply suggests log2codebook 30 40 50 60 70 80 90 100 alpha 36d vectors 25d vectors 16d vectors 9d vectors log2codebook 20 30 40 50 60 70 80 90 100 alpha constant x 1041010036d vectors 25d vectors 16d vectors 9d vectors figure learning complexity ff function vector dimension codebook size binary left grayscale right images image used cases man halftoned binary case errordiffusion nondegenerate cases approach asymptotic performance rate regardless training test image sources tested cases approach asymptotic performance governed learning complexity codebook little variation ff different training test images used indicates degree convergence results described previous section characteristic training algorithm independent images involved last section first discuss theoretical implications results previous two sections discuss practical implications results suggest guidelines users vq techniques may use select appropriate training set sizes conclude briefly recapitulating results paper suggesting directions future research 51 implications empirical results theory bounds derived section 2 worstcase bounds hold regardless input distribution codebookdesign algorithm shown 9 certain circumstances much tighter bounds may derived something known design algorithm consider case arbitrary classifier achieve zero training error training examples using equation 3 bound generalization error ln 2m however know classifier follows bayesoptimal decision rule show expected worstcase behavior greater empirical studies 2 shown even simple zerotrainingerror learning problems expected generalization may close latter bayesoptimal bound tighter bound known case training errors nonzero know nothing optimality bayes otherwise gla used design codebooks experiments however image complexity bits entropy 70 80 90 100 110 120 130 complexity600100014001800 grayscale vq binary vq figure 7 learning complexity function random image entropy plot binary case normalized fit scale grayscale plot simplicity observed test gamma train curves suggests nonzero training error bounds may similar form equation 9 52 implications empirical results practice given perfect knowledge source minimum test distortion source codebook design algorithm achieve fixed bit rate source fixed image fixed set images minimum ffl oes ensure codebook within minimum sufficient ensure test distortion within training distortion since test gamma train ffm sufficient train ff blocks achieve performance another use results relating limited codebook training time test gamma train time train using gla proportional training set size codebook size example decstation 5000 16dimensional codebook vectors training time implementation required approximately 50s per codebook vector per training element knowledge substitute training time training set size appropriate constant factor derived learning complexity give guidance close optimum one come fixed training algorithm given fixed amount training time thus given bounding learning complexity set images make useful decisions appropriate training set sizes training times vq problem section 3 derived parameterized equation ff using set images maximized value source images available using value ff may prove useful practitioners little experimentation may derive values ff appropriate problem domains domains regular noisy images learning complexity may significantly smaller computational savings may realized basing ones training set sizes smaller value practitioners using vq applications eg compressing speech sonar data may likewise find useful perform experiments establish whether results guidelines generalize areas interest practitioner wishes derive learning complexity data set particular block size k bit rate b may estimating ffk b single small value using chosen log2codebook 60 70 80 90 alpha constant 36d vectors 25d vectors 16d vectors figure 8 dependence ff codebook size empirical worst case image settings practitioner may repeatedly extract blocks training set train test another randomly extracted set blocks determine test gamma train trial repeating procedure averaging one may derive increasingly accurate estimate etest gamma train value k b learning complexity ffk b simply etest gamma trainm example let us say want compress set satellite images designing separate codebook minimize distortion let us say image resolution 512 theta 512 pixels 16384 4 theta 4 blocks noted section 3 residual lack fit ffm model small training set sizes run experiments using initial training set size sampled replacement mean training test errors 592 658 respectively giving empirical mean test gamma train distortion 067 standard error 011 gives us estimated learning complexity 234 168 upper lower limits expected value ff let us say want codebook distortion within 01 best given training algorithm values k b represents tiny fraction approximately 6 distortion expect final codebook produce achieve performance need choose training set large enough expected value test gamma train less 01 sampling without replacement solving test suggested training set size running series experiments training set size find average test gamma train difference 0082 confirming expectations bit work may extrapolate different block sizes bit rates well different training set sizes observed form learning complexity codebook approximately block size fixed exponent linear bit rate similarly fixed codebook size exponent linear block size since exponent empirically observed form linear k n determining appropriate coefficients matter determining ffk n two distinct values k n may done single small value performing linear fit logarithm derived ffk n may derive generalized equations like section 331 training set size blocks distortion x 104303003000 train cv test cv31 train cv31 test cv31 figure 9 rate convergence test training distortion roughly independent size source training set drawn codebooks trained blocks drawn full set 10 goes images cv converge asymptote almost rate codebook trained single goes image cv31 53 summary future directions paper examined performance memoryless vector quantizer changes function training set size specifically studied well training set distortion predicts test distortion training set randomly drawn subset blocks test training images demonstrated formal upper bounds test gamma train distortion function vector dimension bit rate training set size bounds turn much loose practical help demonstrated practical guidelines derived empirically range test images guidelines depend codebook size dimension appear somewhat robust across classes images however great deal work remains done area obvious directions 1 inquiry necessary ff worstcase learning complexity section 34 worst case mean producing greatest minimum distortion rather requiring largest training set size average achieve less specified difference test training distortions formal bounds algorithm independent take account limited learning ability gla training therefore worstcase upper bound high simulations described paper demonstrate empirical worstcase lower bounds images used selected arbitrary criteria almost assuredly exist distributions difficult learn ones examined working mathematical framework covering problem rtolerance model may possible formally prove worstcase distribution exists measure generalization analytically empirically distribution would provide exact bound worstcase learning complexity 2 completely ignored implications work achieving minimum distortion fixed bit rate minimum bit rate fixed distortion given fixed allowable training time recent work described 11 elsewhere addresses problem minimizing training distortion bound training distortion work bounding difference test gamma train possible directly bound test distortion image function codebook training set size 3 finally touched problem identifying classes images training testing codebooks work area needed determine set images given learning training set size blocks distortion x 104303003000 train cv test cv31 train cv test cv17 train cv test man figure 10 plot deltatest gamma train codebooks designed drawing random blocks 10 goes images tested one images training set cv31 one image training set cv17 one usc image man complexity valid despite amount research still needs done area work described paper made one thing clear possible design good codebooks using fraction computational power typically used normal exhaustive training paradigm acknowledgments portions research funded national science foundation grant numbers ccr9108314 mip9110508 hewlettpackard laboratories would also like thank professor les atlas university washington department electrical engineering professor david madigan university washington department statistics phil chou xerox parc many helpful discussions suggestions preparation paper portion work done cohn department computer science university oregon r learnability vapnikchervonenkis dimension tight vapnikchervonenkis bounds neural computation separating formal bounds practical performance learning systems training sequence size vector quantizer performance computation onset chaos adaptive algorithm spatial grey scale vector quantization signal compression vector quantization unifying bounds sample complexity bayesian learning theory using information theory vc dimension analysis synthesis telephony based maximum likelihood method algorithm vector quantizer design digital pictures representation compression central limit theorem kmeans clustering digital halftoning estimation dependencies based empirical data uniform convergence relative frequencies events probabilities computer systems learn probability distributions statistics tr digital halftoning learnability vapnikchervonenkis dimension bounds sample complexity bayesian learning using information theory vc dimension vector quantization signal compression eapproximations minimum packing constraint violation extended abstract separating formal bounds practical performance learning systems tight vapnikchervonenkis bounds digital pictures ctr stefano rovetta francesco masulli vector quantization fuzzy ranks image reconstruction image vision computing v25 n2 p204213 february 2007 hasenjger h ritter active learning neural networks new learning paradigms soft computing physicaverlag gmbh heidelberg germany 2002