evolutionary trees learned polynomial time twostate general markov model jstate general markov model evolution due steel stochastic model concerned evolution strings alphabet size j particular twostate general markov model evolution generalizes wellknown cavenderfarrisneyman model evolution removing symmetry restriction requires probability 0 turns 1 along edge probability 1 turns 0 along edge farach kannan showed probably approximately correct paclearn markov evolutionary trees cavenderfarrisneyman model provided target tree satisfies additional restriction pairs leaves sufficiently high probability show remove restrictions thereby obtain first polynomialtime paclearning algorithm sense kearns et al proceedings 26th annual acm symposium theory computing 1994 pp 273282 general class twostate markov evolutionary trees b introduction jstate general markov model evolution proposed steel 1994 14 model concerned evolution strings dna strings alphabet size j model described follows jstate markov evolutionary tree consists topology rooted tree edges directed away root together following parameters root tree associated j probabilities ae sum 1 edge tree associated stochastic transition matrix whose state space alphabet probabilistic experiment performed using markov evolutionary tree follows root assigned letter alphabet according probabilities ae letter chosen probability ae letter propagates edges tree letter passes edge undergoes probabilistic transition according transition matrix associated edge result string length n concatenation letters obtained n leaves tree jstate markov evolutionary tree thus defines probability distribution lengthn strings alphabet size j probabilistic experiment described produces single sample distribution 1 avoid getting bogged detail work binary alphabet thus consider twostate markov evolutionary trees following farach kannan 9 erdos steel szekely warnow 7 8 ambainis desper farach kannan 2 interested problem learning markov evolutionary tree given samples output distribution following farach kannan ambainis et al consider problem using polynomially many samples markov evolutionary tree learn markov evolutionary tree 0 whose distribution close use variation distance metric measure distance two distributions 0 strings length n variation distance 0 nleaf markov evolutionary trees use denote variation distance distribution distribution 0 use probably approximately correct pac distribution learning model kearns mansour ron rubinfeld schapire sellie 11 main result first polynomialtime paclearning algorithm class twostate markov evolutionary trees refer mets theorem 1 let ffi ffl positive constants algorithm given polyn 1ffl 1ffi samples met nleaf topology probability least met 0 constructed algorithm satisfies varm interesting paclearning algorithms biologically important restricted classes mets given farach kannan 9 ambainis desper farach kannan 2 algorithms relation algorithm discussed fully section 11 point simply note algorithms apply mets satisfy following restrictions restriction 1 transition matrices symmetric probability 1 turning 0 along edge probability 0 turning 1 biologists would view n leaves existing species internal nodes hypothetical ancestral species model single experiment described would produce single bit position example dna n species restriction 2 positive constant ff every pair leaves x satisfies explain section 11 restrictions significantly simplify problem learning markov evolutionary trees though certainly make easy main contribution paper remove restrictions used variation distance l 1 distance measure distance target distribution hypothesis distribution 0 kearns et al formulated problem learning probability distributions terms kullbackleibler divergence distance target distribution hypothesis distribution distance defined sum lengthn strings ds logdsd 0 kearns et al point kl distance gives upper bound variation distance sense kl distance 0 omegagamma ard 0 hence class distributions paclearned using kl distance paclearned using variation distance justify use variation distance metric showing reverse true particular prove following lemma appendix class probability distributions domain f0 1g n paclearnable variation distance metric paclearnable kldistance measure lemma proved using method related ffl bayesian shift abe warmuth 3 note result requires discrete domain support target distribution domain f0 1g n use rest section organised follows subsection 11 discusses previous work related general markov model evolution relationship work work subsection 12 gives brief synopsis algorithm paclearning markov evolutionary trees subsection 13 discusses interesting connection problem learning markov evolutionary trees problem learning mixtures hamming balls studied kearns et al 11 11 previous work relation work twostate general markov model 14 study paper generalisation cavenderfarrisneyman model evolution 5 10 13 describing cavender farrisneyman model let us return twostate general markov model fix attention particular twostate alphabet f0 1g thus stochastic transition matrix associated edge e simply matrix e 0 denotes probability 0 turns 1 along edge e e 1 denotes probability 1 turns 0 along edge e cavenderfarrisneyman model simply special case twostate general markov model transition matrices required symmetric special case twostate general markov model restriction 1 page 1 holds e describe past work learning markov evolutionary trees general markov model cavenderfarrisneyman model throughout paper define weight edge e steel 14 showed jstate markov evolutionary tree satisfies ae 0 ii determinant every transition matrix outside fgamma1 0 1g distribution uniquely determines topology case showed recover topology given joint distribution every pair leaves 2state case suffices know exact value covariances every pair leaves case defined weight e edge e node v node w w leaf r 1 steel observed distances multiplicative along path distance two leaves equal covariance since distances multiplicative along path logarithms additive therefore methods constructing trees additive distances method bandelt dress 4 used reconstruct topology steels method show recover parameters markov evolutionary tree even exact distribution known 2 particular quantity obtains edge e onedimensional distance rather twodimensional vector giving two transition probabilities e 0 e 1 method shows recover parameters exactly given exact distribution recover parameters approximately well enough approximate distribution given polynomiallymany samples farach kannan 9 ambainis desper farach kannan 2 worked primarily special case twostate general markov model satisfying two restrictions page 1 farach kannans paper breakthrough prior paper nothing known feasibility reconstructing markov evolutionary trees samples given positive constant ff showed paclearn class mets satisfy two restrictions however number samples required function 1ff taken constant ambainis et al improved bounds given farach kannan achieve asymptotically tight upper lower bounds number samples needed achieve given variation distance results elegant important nevertheless restrictions place model significantly simplify problem learning markov evolutionary trees order explain true explain approach farach et al algorithm uses samples met satisfies restrictions estimate distance two leaves distance related covariance leaves authors relate distance two leaves amount evolutionary time elapses distances thus turned times algorithm 1 used approximate evolutionary times times close form additive metric fitted onto tree finally times turned back transition probabilities symmetry assumption essential approach symmetry relates onedimensional quantity evolutionary time otherwise twodimensional quantity probability going 0 1 probability going 1 0 second restriction also essential probability x differs allowed approach 12 evolutionary time x would tend 1 would mean order approximate interleaf times accurately algorithm would get distance estimates accurately would require many samples ambainis et al 2 generalised results symmetric version jstate evolutionary model subject two restrictions erdos steel szekely warnow 7 8 also considered reconstruction markov evolutionary trees samples like steel 14 unlike paper papers farach et al 9 2 erdos et al interested reconstructing topology met rather parameters distribution interested using samples possible reconstruct topology showed reconstruct topologies jstate general markov model markov evolutionary trees satisfy every root probability bounded 0 ii every transition probability bounded 0 12 iii positive quantities 0 determinant transition matrix along edge number samples required polynomial worst case polylogarithmic certain cases including case met drawn uniformly random one several specified natural distributions note restriction iii erdos et al weaker farach kannans restriction 2 page 1 however erdos et al show reconstruct topology thus work restricted case topology uniquely constructed using samples show reconstruct parameters markov evolutionary tree approximate distribution 12 synopsis method paper describe first polynomialtime paclearning algorithm class twostate markov evolutionary trees mets algorithm works follows first using samples target met algorithm estimates pairwise covariances leaves met second using covariances leaves met partitioned related sets leaves essentially leaves different related sets small covariances always possible use polynomially many samples discover related sets connected target topology nevertheless show closely approximate distribution target met approximating distribution related set closely joining related sets cut edges first step related set discover approximation correct topology since restrict class mets consider cannot guarantee construct exact induced topology target met nevertheless guarantee construct good enough approximation topology constructed looking triples leaves show ensure triple consider large interleaf covariances derive quadratic equations allow us approximately recover parameters triple using estimates interleaf covariances estimates probabilities particular outputs compare outcomes different triples use comparisons construct topology topology use quadratic equations discover parameters tree show section 24 able prevent error estimates accumulating able guarantee estimated parameter within small additive error real parameter normalised target met show variation distance hypothesis target small 13 markov evolutionary trees mixtures hamming balls hamming ball distribution 11 binary strings length n defined center string c length n corruption probability p generate output distribution one starts center flips bit according independent bernoulli experiment probability p linear mixture j hamming balls distribution defined j hamming ball distributions together j probabilities ae sum 1 determine hamming ball distribution particular sample taken fixed j kearns et al give polynomialtime paclearning algorithm mixture j hamming balls provided j hamming balls corruption probability 2 pure distribution binary strings length n defined n probabilities generate output distribution ith bit set 0 independently probability 1 otherwise pure distribution natural generalisation hamming ball distribution clearly every linear mixture j pure distributions realized jstate met starshaped topology thus algorithm given paper shows learn linear mixture two pure distributions furthermore generalisation result jary alphabet would show learn linear mixture j pure distributions 2 algorithm description paclearning algorithm analysis require following defini tions positive constants ffi ffl input algorithm consists polyn 1ffl 1ffi samples met nleaf topology let ffl made effort optimise constants however state explicitly reader verify constants defined consistently define ffl 4 contraction met topology 0 tree formed 0 contracting internal edges e edgedistance e defined steel 14 see equation 1 x leaves topology use notation covx denote covariance indicator variables events bit x 1 bit 1 thus use following observations observation 3 met 0 topology 0 e internal edge 0 root r node v 00 topology 0 except v root e goes v r construct met topology 00 distribution 0 simply set appropriately distribution 0 set e 0 distribution 0 1 set e 1 distribution 0 otherwise set e observation 4 met 0 topology 0 v degree2 node 0 edge e leading v edge f leading v 00 topology 0 except e 2 kind paclearning consider paper generation kearns et al also show evaluation special case mixture j hamming balls described using observation output distributions subtrees node met independent provided bit node fixed also solve evaluation problem mets particular calculate polynomial time probability given string output hypothesis met f contracted form edge g met topology 00 distribution 0 construct simply set observation 5 met 0 topology 0 met 00 topology 0 distribution leaves 0 every internal edge e satisfy e 0 e 1 1 proof observation 5 say edge e good e 0 starting root make edges along path leaf good except perhaps last edge path edge e u v first nongood edge path simply set e 0 makes edge good side effect interchanging meaning 0 1 node v long interchange 0 1 even number times along every path preserve distribution leaves thus make edges good except possibly last one use get parity number interchanges correct 2 describe algorithm subsection 26 prove probability least 1 gamma ffi met 0 constructs satisfies varm 0 ffl thus prove theorem 1 estimate covariances pairs leaves pair x leaves obtain observed covariance covx probability least 1 gamma ffi3 observed covariances satisfy lemma 6 step 1 requires polyn 1ffl 1ffi samples proof consider leaves x let p denote prxy 11 chernoff bound see 12 k samples observed proportion outputs p probability least pair x leaves estimate estimates calculate covx within sigmaffl 3 using equation 2 2 22 step 2 partition leaves related sets consider following leaf connectivity graph whose nodes leaves nodes x connected positive edge connected negative edge covx gamma34ffl 2 connected component graph ignoring signs edges forms set related leaves set related leaves let ss denote leaf smallest index mets property leaves x z covy z positive iff covx covy z sign see use following equation proved algebraic manipulation equation 2 v taken least common ancestor x ff 0 ff 1 transition probabilities along path v x fi 0 fi 1 transition probabilities along path v therefore long observed covariances accurate stated step 1 signs edges leaf connectivity graph partition leaves two sets 1 2 way ss 2 1 covariances pairs leaves positive covariances pairs leaves 2 positive covariances leaf 1 leaf 2 negative set related leaves let denote subtree formed deleting leaves contracting degree2 nodes rooting neighbour ss let ms met topology distribution leaves satisfies following ffl every internal edge e ms e 0 ffl every edge e node 1 e 0 ffl every edge e node 2 e 0 observations 3 4 5 guarantee ms exists observation 7 long observed covariances accurate stated step 1 happens probability least 1 gamma ffi3 related set leaf x 2 leaf 2 jcovx yj ffl 2 2 observation 8 long observed covariances accurate stated step 1 happens probability least 1 gamma ffi3 related set edge e leaves b connected e jcova bj ffl 2 2 observation 9 long observed covariances accurate stated step 1 happens probability least 1 gamma ffi3 related set every internal node v ms proof observation 9 suppose contrary v internal node ms using observation 3 reroot ms v without changing distribution let w child v equation 3 every pair leaves b connected v w satisfy jcova bj observation follows observation 8 2 long observed covariances accurate stated step 1 happens probability least 1 gamma ffi3 related set every edge e ms ffl 2 2 proof observation 10 follows observation 8 using equation 3 recall 23 step 3 related set find ffl 4 contraction 0 section assume observed covariances accurate stated step 1 happens probability least 1 gamma ffi3 let related set probability least 1 gamma ffi3n find ffl 4 contraction 0 since n related sets ffl 4 contractions constructed probability least 1 gamma ffi3 recall ffl 4 contraction ms tree formed contracting internal edges e e start following observation allow us redirect edges convenience observation 11 e internal edge e remains unchanged e redirected observation 3 proof observation proved algebraic manipulation equation 1 observation 3 note observation every endpoint v e satisfies 0 1 thus redirection observation 3 degenerate e defined 2 describe algorithm constructing ffl 4 contraction 0 build 0 inductively adding leaves one one ffl 4 contraction 0 0 subset 0 consider leaf x build ffl 4 contraction 0 precise order leaves added matter add new leaf x unless 0 contains leaf jd covx yj 34ffl 2 add new leaf x proceed follows first consider use method following section section 231 estimate e 0 specifically let u v nodes adjacent 0 show estimate e afterwards section 232 show insert x 231 estimating e section suppose met ms 0 set 0 leaves form single related set 0 topology ms 0 0 0 ffl 4 contraction edge 0 0 edge 0 wish estimate e within sigmaffl 4 16 ensure overall probability estimates range ffi6n proof following equations straightforward typically apply situations z error approximation case 1 e 0 internal edge first estimate e 0 e 1 correct values observation 9 ffl 2 estimate within factor 1 sigma 2ffl 5 correct value similarly estimates within factor 1 sigma ffl 4 2 gamma9 correct values using equation 1 estimate e within sigmaffl 4 16 particular estimate e inequalities used equation 6 fact e 1 similarly equation 7 estimate e least show estimate e 0 e 1 say path node ff node fi met strong jcovff fij ffl 2 2 follows equation 3 node fl path say quartet c b j leaves b c good estimator edge edge 0 following hold 0 see figure 1 1 descendent v 2 undirected path c strong passes u v 3 path u descendent b strong intersects undirected path c node u 4 path v descendent strong intersects path v node v say apparently good estimator e 0 following hold 1 descendent v 0 2 undirected path c strong passes u 0 v 0 3 path u 0 descendent b strong intersects undirected path c node u 0 4 path v 0 descendent strong intersects path v 0 node v 0 e c figure 1 finding observation 12 e edge 0 c b j good estimator e leaves proof observation follows equation 8 9 definition good estimator 2 good estimator e used along polyn 1ffl 1ffi samples ms 0 estimate e 0 e 1 use sufficiently many samples probability estimates within sigmaffl 5 correct value ffi12n 7 proof let q 0 q 1 denote transition probabilities v see figure 1 let transition probabilities u first show estimate loss generality observation 3 assume c descendant u otherwise reroot 0 u without changing distribution nodes p 0 p 1 let fi path u b let fl path u c define quite correspond conditional covariances b c related also define covb c following equations proved algebraic manipulation equation 10 equation 2 definitions f case 1a 2 1 case equation 4 observation 10 equation 13 equations 12 14 imply also since equations clear could find p 0 exactly show polynomiallymany samples approximate values sufficiently accurately using approximations equations obtain approximations within sigmaffl 6 proof lemma 6 use equations 2 10 estimate within sigmaffl 0 ffl 0 whose inverse polynomial n 1ffl note estimate covb c nonzero observation 12 long ffl 0 ffl 2 2 3 able use estimate f definition using definition f equation 5 estimate 2f observation 12 error ffl 00 ffl 00 whose inverse polynomial n 1ffl accomplished making ffl 0 small enough respect ffl 2 according equation 18 similarly bound amount underestimate f use definition estimate estimate using equation 5 covb c observation 12 error made within sigmaffl 000 ffl 000 whose inverse polynomial n 1ffl making ffl 0 ffl 00 sufficiently small follows estimate us upper bound value function ffl 2 estimate within sigmaffl 0000 ffl 0000 whose inverse polynomial n 1ffl implies estimate p 0 p 1 within sigmaffl 6 observation 12 equation 3 imply wp ffl 2 2 3 thus estimate nonzero implies similarly estimate using equation 17 estimates within sigmaffl 6 correct values repeat trick find estimates q 0 q 1 also within sigmaffl 6 use leaf observation 4 implies using equations estimate e 0 equation 5 observation wp ffl 2 2 3 imply error 2 7 ffl 6 ffl 3 similarly estimate e 0 least e estimate e 1 within sigmaffl 5 e 1 estimated e 0 e 1 explained beginning section use estimates estimate case 1b 2 2 case equation 4 observation 10 equation 13 equations 12 19 imply equation 17 remains unchanged process estimating new equations case 1a concludes proof lemma 13 2 observation 14 suppose e 0 edge u 0 v 0 0 0 edge 0 u 2 u 0 v 2 v 0 good estimator c b j e furthermore every good estimator e apparently good estimator e 0 refer figure 2 ae oe ae c figure 2 good estimator apparently good estimator c figure 3 apparently good estimator e good estimator proof leaves c found satisfy first two criteria definition good estimator observation 8 leaf b found satisfy third criterion observation 8 equation 8 fact degree u least 3 see text equation 4 similarly leaf found satisfy fourth criterion apparently good estimator e 0 internal edges 0 contracted ffl 4 contraction 0 observation 15 suppose e 0 edge u 0 v 0 0 0 edge 0 u 2 u 0 v 2 v 0 suppose c b j apparently good estimator e 0 let u 00 meeting point c b 0 let v 00 meeting point c 0 refer figure 3 c b j good estimator path p u 00 v 00 0 also p e proof fact c b j good estimator p follows definition good estimator fact p e follows fact distances multiplicative along path bounded 1 2 observations 14 15 imply order estimate e within sigmaffl 4 16 need estimate e using apparently good estimator e 0 take maximum lemma 13 failure probability given estimator ffi12n 7 probability least 1 gamma ffi12n 3 estimators give estimates within sigmaffl 4 16 correct values since 2n edges e 0 0 0 add new leaf x 0 n times estimates within sigmaffl 4 16 probability least 1 gamma ffi6n case 2 e 0 internal edge case leaf 0 say pair leaves b c good estimator e following holds 0 paths leaves v b c meet u jcovv bj jcovv cj jcovb cj least ffl 2 2 2 say b c apparently good estimator e 0 following holds 0 0 paths leaves v b c meet u 0 jcovv bj jcovv cj jcovb cj least ffl 2 2 2 previous case result follows following observations observation c good estimator e used along polyn 1ffl 1ffi samples ms 0 estimate e 0 e 1 probability estimates within sigmaffl 5 correct value ffi12n 3 proof follows proof lemma 13 2 observation 17 suppose e 0 edge u 0 leaf v 0 0 edge 0 u 2 u 0 good estimator b c e furthermore every good estimator e apparently good estimator e 0 proof follows proof observation 14 equation 9 2 observation suppose e 0 edge u 0 leaf v 0 0 edge 0 u 2 u 0 suppose b c apparently good estimator e 0 let u 00 meeting point b v c 0 b c good estimator path p u 00 v 0 also p e proof follows proof observation 15 2 232 using estimates e return problem showing add new leaf x 0 0 indicated every internal edge e use method section 231 estimate e edge 0 u 2 u 0 v 2 v 0 observed value e exceeds contract e accuracy estimates guarantee contract e definitely contract e add new leaf x 0 0 follows insert new edge either 1 identifying x 0 node already 0 0 2 splicing x 0 middle edge 0 0 show decide attach x 0 0 0 start following definitions let 00 subset 0 every 2 00 jcovx yj ffl 2 2 4 let 00 subtree 0 0 induced leaves 00 let 000 subset 0 every 2 000 jd covx yj ffl 2 000 subtree 0 0 induced leaves 000 observation 19 0 fxg x 0 attached edge edge corresponding e 0 0 e edge 00 proof observation 14 good estimator c b j e since x added 0 using equation 8 jcovx x 0 j ffl 2 2 thus observation 12 equation 9 every leaf 2 fa b c dg jcovx yj ffl 2 2 4 thus b c 00 e 0 00 2 attached edge contained node u 0 0 0 u 0 node 00 proof since u internal node 0 degree least 3 observation 8 equation 8 three leaves 1 2 3 meeting u jcovu j ffl 2 2 similarly jcovu vj ffl 2 2 thus jcovx j ffl 2 2 3 1 2 3 00 2 observation proof follows accuracy covariance estimates step 1 2 use following algorithm decide attach x 0 000 algorithm use following tool triple b c leaves 0 fxg let u denote meeting point paths leaves b c 0 fxg let u met distribution ms 0 fxg rooted u u exists observation 3 let c b c denote weight path u c u observation 11 c b c equal weight path u c ms 0 fxg follows fact rerooting u redirects internal edges follows definition equation 1 equation 3 c b c cova ccovb c b c 000 fxg accuracy covariance estimates equations 8 9 absolute value pairwise covariance pair least ffl 8 section 231 estimate cova c covb c cova b within factor 1 sigma ffl 0 correct values ffl 0 whose inverse polynomial n 1ffl thus estimate c b c within factor 1 sigma ffl 4 16 correct value take sufficiently many samples ensure probability estimates outside required range ffi6n 2 thus probability estimate outside range x ffi6n determine 000 attach x 0 choose arbitrary internal root u 0 000 first see x 0 placed respect u 0 neighbour v 0 u 0 000 pair leaves 1 2 u leaf b v side perform following two tests figure 4 setting internal node 000 v 0 leaf perform tests x f u 2 figure 5 either test succeeds observed value x least 1 test succeeds observed value b 1 1 gamma 3ffl 4 4 make following observations observation 22 x u side u v 000 fxg u u 0 000 v v 0 6 u 0 000 test fails proof since u 0 internal node 000 degree least 3 thus construct test one depicted figure 5 x figure still correct would mean f similarly v 0 leaf simply f 0 edge v b havef however succeed left hand fraction least 1 furthermore succeed right hand fraction estimates accurate within factor 1 sigma ffl 4 16 least one two tests fail 2 observation 23 x u v 000 fxg edge f u x 0 succeed choices 1 2 b x figure succeed choices 1 2 b 1 f x 1 f x figure 7 succeed choices 1 2 b proof every test form depicted figure 6 g might degen erate case g 1 observe x estimate least succeeds furthermore estimate 1 gamma 3ffl 4 4 test2 succeeds 2 observation 24 x v side u v 000 fxg beginning section 232 e 1 gamma 7ffl 4 8 u v different nodes 000 succeed choices 1 2 b proof note case applies v internal node 000 thus every test one forms depicted figure 7 edges may degenerate observe cases x estimate least 1 test1 succeeds also cases estimate 1 gamma 3ffl 4 4 test2 succeeds 2 note using observation 22 node u 0 one neighbour v 0 tests succeed furthermore imply x 0 merged u 0 case dealt case exactly one v 0 tests succeed case v 0 leaf insert x 0 middle edge otherwise either insert x 0 middle edge insert subtree rooted v 0 order decide perform similar tests node v 0 check whether test1v succeed choices 1 2 b put x 0 middle edge otherwise recursively place x 0 subtree rooted v 0 24 step 4 related set construct met 0 close ms set related leaves construct met 0 leafset edge parameter 0 within sigmaffl 1 corresponding parameter ms topology 0 0 assume without loss generality root 0 failure probability ffi3n overall failure ffi3 start observing problem easy one two leaves observation construct met 0 edge parameter 0 within sigmaffl 1 corresponding parameter ms consider case least three leaves edge contracted 0 regarded e 0 e 1 set 0 fact within sigmaffl 1 true values follows following lemma lemma 26 e internal edge ms v w proof first observe observation 9 1g observation 10 using algebraic manipulation one see thus equation 1 proves observation 2 thus need show label remaining parameters within sigmaffl 1 note already shown section 231 total failure probability ffi3n failure probability ffi6n 2 associated 2n edges 25 step 5 form 0 mets 0 make new root r 0 set 1 related set leaves let u denote root 0 let p denote probability u 0 distribution 0 make edge e r u e 26 proof theorem 1 let 00 met formed follows ffl related sets formed step 2 ffl related set copy 00 ms made ffl mets 00 combined step 5 theorem 1 follows following lemmas lemma 27 suppose every set related leaves every parameter 0 within 1 corresponding parameter ms varm proof first observe using crude estimate 5n 2 parameters 0 n mets 0 one root parameter 4n edge parameters show changing single parameter met sigmaffl 1 yields met whose variation distance original 2ffl 1 implies ffl2 suppose e edge u v e 0 changed probability output string leaves v string 0 remaining leaves thus variation distance 00 met obtained changing value e 0 within similarly ae 1 root parameter met probability output variation distance original met one ae 1 changed within 1 x prove lemma 28 provide background material recall weight edge e met define weight w leaf product weights edges path root use following lemma lemma 29 met root r variation distance distribution leaves conditioned distribution leaves conditioned 0 mostp w sum leaves proof proceed induction number edges met base case edges r leaf result holds inductive step let e edge r node x string 1 leaves x string 2 leaves algebraic manipulation formula shows prs 1 follows variation distance sum 1 2 absolute value quantity equation 23 result follows induction 2 lemma met n leaves e edge node u node v let 0 met derived replacing e 0 z maximum pairs x leaves connected via e jcovx yj proof observation 3 assume without loss generality u root string 1 leaves v string 2 remaining leaves find via little algebraic manipulation difference probability outputs 1 2 probability 0 thus variation distance 0 times product variation distance distribution leaves v conditioned distribution leaves v conditioned variation distance distribution remaining leaves conditioned distribution remaining leaves conditioned v equation 3 isx connected via e 4n2 2 lemma 31 two different related sets 0 edge e u v ms 0 e proof definition leaf connectivity graph step 2 leaves 0 2 b b 0 2 0 path 0 path b 0 b go jd cova remaining covariance estimates amongst leaves 0 b b 0 less 34ffl 2 without loss generality using observation 3 assume u root met let denote path u 0 use similar notation leaves equation 3 accuracy estimates step 1 thus equation 1 result follows proof lemma 26 clearly bound statement lemma 31 weaker prove need 2 proof lemma 28 let met except every edge e contained ms ms 0 two different related sets 0 contracted similarly let 00 met 00 except every edge copies contracted 00 clearly varm 00 number edges contracted wish bound varm 0 intersect edge related sets 0 suppose 0 contain node u modify without changing distribution way avoids overlap replace node u two copies u connect two copies edge e e note change affect operation algorithm thus without loss generality assume related sets 0 0 intersect thus 00 identical except edges go submets edge e going two submets property pair leaves x connected via e jcovx yj ffl 2 follows accuracy covariance estimates step 1 thus lemma 30 changing edge according step 5 adds n 2 ffl 2 variation distance thus varm number edges modified according step 5 conclude varm 00 acknowledgements thank mike paterson useful ideas discussions r approximability numerical taxonomy nearly tight bounds learnability evolution polynomial learnability probabilistic concepts respect kullbackleibler divergence reconstructing shape tree observed dissimilarity data elements information theory logs suffice build almost trees inferring big trees short quartets efficient algorithms inverting evolution probability model inferring evolutionary trees learnability discrete distributions method bounded differences molecular studies evolution source novel statistical problems recovering tree leaf colourations generates markov model tr ctr elchanan mossel distorted metrics trees phylogenetic forests ieeeacm transactions computational biology bioinformatics tcbb v4 n1 p108116 january 2007 elchanan mossel sbastien roch learning nonsingular phylogenies hidden markov models proceedings thirtyseventh annual acm symposium theory computing may 2224 2005 baltimore md usa