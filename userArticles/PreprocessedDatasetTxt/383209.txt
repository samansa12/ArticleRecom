scrambling query plans cope unexpected delays accessing data numerous widelydistributed sources poses significant new challenges query optimization execution congestion failures network introduce highlyvariable response times widearea data access paper initial exploration solutions variability introduce class dynamic runtime query plan modification techniques call query plan scrambling present algorithm modifies execution plans onthefly response unexpected delays obtaining initial requested tuples remote sources algorithm reschedules operators introduces new operators query plan present simulation results demonstrate technique effectively hides delays performing useful work waiting missing data arrive b introduction ongoing improvements networking technology infrastructure resulted dramatic increase demand accessing collating data disparate remote data sources widearea networks internet intranets query optimization execution strategies long studied centralized parallel tightlycoupled distributed environments data access across widely distributed sources however imposes significant new challenges query optimization execution two reasons first semantic performance problems arise due heterogeneous nature data sources looselycoupled envi ronment second data access widearea networks involves large number remote data sources intermediate sites communications links vulnerable congestion failures appear proceedings fourth international conference parallel distributed information systems pdis96 miami beach florida december 1996 laurent amsaleg supported postdoctoral fellowship inria rocquencourt france z supported part nsf grant iri9409575 ibm sur award grant bellcore users point view congestion failure components network manifested highlyvariable response time time required obtaining data remote sources vary greatly depending specific data sources accessed current state network time access attempted query processing problems resulting heterogeneity subject much attention recent years eg sad con trast impact unpredictable response time widearea query processing received relatively little attention work presented initial exploration addressing problems responsetime variability widearea data access 11 response time variability high variability makes efficient query processing difficult query execution plans typically generated statically based set assumptions costs performing various operations costs obtaining data ie disk andor network accesses causes highvariability typically failures congestion inherently runtime issues cannot reliably predicted query optimization time even query startup time result execution statically optimized query plan likely suboptimal presence unexpected response time problems worst case query execution may blocked arbitrarily long time needed data fail arrive remote data sources different types response time problems experienced looselycoupled widearea environment categorized follows ffl initial delay unexpected delay arrival first tuple particular remote source type delay typically appears difficulty connecting remote source due failure congestion source along path source destination ffl slow delivery data arriving regular rate rate much slower expected rate problem sult example network congestion resource contention source different slower communication path used eg due failure ffl bursty arrival data arriving unpredictable rate typically bursts data followed long periods arrivals problem arise fluctuating resource demands lack global scheduling mechanism widearea environment problems arise unpredictably runtime cannot effectively addressed static query optimization techniques result investigating class dynamic runtime query plan modification techniques call query plan scrambling approach query initially executed according original plan associated schedule generated query optimizer ever significant performance problem arises execution query plan scrambling invoked modify execution onthefly progress made parts plan words rather simply stalling slowly arriving data query plan scrambling attempts hide unexpected delays performing useful work three ways query plan scrambling used help mask response time problems first scrambling allows useful work done hope cause problem resolved meantime approach useful three classes problems described second data ar riving rate hampers query processing performance eg slow delivery bursty arrival cases scrambling allows useful work performed problematic data obtained background fashion finally cases data simply arriving arriving far slowly scrambling used produce partial results returned users andor used query processing later time trv96 12 tolerating initial delays work present initial approach query plan scrambling specifically addresses problem initial delay ie delay receiving initial requested tuples remote data source describe analyze query plan scrambling algorithm follows first approach outlined namely useful work performed hope problem eventually resolved requested data arrive near expected rate algorithm exploits possible decisions made static query optimizer imposes optimization execution performance overhead absence unexpected delays order allow us clearly define algorithm study performance work assumes execution environment several properties ffl algorithm addresses response time delays receiving initial requested tuples remote data sources initial delay tuples assumed arrive near originally expected rate stated previously type delay models problems connecting remote data sources often experienced internet ffl focus query processing using datashipping hybridshipping approach fjk96 data collected remote sources integrated query source query processing performed query source subject scrambling approach typical mediated database systems integrate data distributed heterogeneous sources eg trv96 query execution scheduled using iterator model gra93 model every runtime operator supports open call getnext call query execution starts calling open topmost operator query execution plan proceeds iteratively calling getnext topmost operator calls propagated tree time operator needs consume data calls getnext child chil dren operators model imposes schedule operators query plan reminder paper organized follows section 2 describes algorithm gives extended example section 3 presents results simulation study demonstrate properties algorithm section 4 describes related work section 5 concludes summary results discussion future work scrambling query plans section describes algorithm scrambling queries cope initial delays obtaining data remote data sources algorithm consists two phases one changes execution order operations order avoid idling one synthesizes new operations execute absence work perform first provide brief overview algorithm describe two phases detail using running example algorithm summarized end section 21 algorithm overview figure 1 shows operator tree complex query plan typically complicated plan would generated static query optimizer according cost model statistics objective functions leaves tree base relations stored remote sites nodes tree binary operators focus study hashbased joins executed query source site 1 unary operators selections sorting partitioning shown figure e2 g figure 1 initial query tree discussed previously describe scrambling algorithm context iteratorbased execution model model imposes schedule operators query drives flow data operators scheduling operators indicated figure 1 numbers associated oper ator figure joins numbered according order would completed iteratorbased scheduler flow data operators follows model discussed sd90 ie left input hash join always materialized right input consumed pipelined fashion schedule implied tree figure 1 would thus begin materializing left subtree root node assuming hash joins used sufficient memory hold hash tables relations c partitioning necessary relations materialization would consist following steps 1 scan relation build hashtable ha using selected tuples 2 pipelined fashion probe ha selected tuples b build hashtable containing result a1b hab 3 scan c build hashtable hc 4 scan build hashtable hd 5 pipelined fashion probe hd hc hab tuples e build hashtable containing result a1b1c1d1e execution thus begins requesting tuples remote site relation stored delay accessing site say site temporarily scan ie step 1 blocked site recovers traditional iteratorbased scheduling discipline delay would result entire execution query blocked pending recovery remote site given unexpected delays highly probable widearea environment sensitivity delays likely result unacceptable performance scrambling algorithm addresses problem attempting hide delays making progress parts query problem resolved scrambling algorithm invoked delayed relation detected via timeout mechanism algorithm iterative iteration selects part plan execute materializes corresponding temporary results used later execution scrambling algorithm executes one two phases phase 1 iteration modifies schedule order execute operators dependent data known delayed example query figure 1 phase 1 might result materializing join relations c e waiting arrival phase 2 iteration synthesizes new operators joins example order make progress example phase 2 iteration might choose join relation b result c1d1e computed previously end iteration algorithm checks see delayed sources begun respond stops iterating returns normal scheduling operators possibly reinvoking scrambling additional delayed relations later detected however delayed data arrived iteration algorithm iterates algorithm moves phase 1 phase 2 fails find existing operator dependent delayed relation phase 2 algorithm unable create new operators scrambling terminates query simply waits delayed data arrive following sections describe detail two phases scrambling interactions 22 phase 1 materializing subtrees 221 blocked runnable operators operators query tree producerconsumer relationships immediate ancestor given operator consumes tuples produced operator conversely immediate descendants given operator produce tuples operator con sumes producerconsumer relationships create execution dependencies operators one operator consume tuples tuples produced example select operator consume tuples base relation relation available case select operator blocked select consume tuples produce tuples consequently consumer select also blocked transitivity ancestors unavailable relation blocked system discovers relation un available query plan scrambling invoked scrambling starts splitting operators query tree two disjoint queues queue blocked operators queue runnable operators queues defined follows definition 21 queue blocked operators given query tree queue blocked operators contains ancestors unavailable relation definition 22 queue runnable operators given query tree queue blocked operators queue runnable operators contains operators queue blocked operators operators inserted runnable blocked queues according order execution would initiated iteratorbased scheduler 222 maximal runnable subtree iteration phase 1 query plan scrambling analyzes runnable queue order find maximal runnable subtree materialize maximal runnable subtree defined follows definition 23 maximal runnable subtree given query tree queues blocked runnable operators runnable subtree subtree operators runnable runnable subtree maximal root first runnable descendant blocked operator none operators belonging maximal runnable subtree depend data known delayed iteration phase 1 initiates materialization first maximal runnable subtree found notion maximal used definition impor tant materializing biggest subtrees iteration tends minimize number materializations performed hence reducing amount extra io caused scrambling materialization runnable subtree completes relations used subtree discovered unavailable execution 2 execution runnable subtree finished result materialized algorithm removes operators belonging subtree runnable queue checks missing data begun arrive missing data others blocked relations still unavailable another iteration begun new iteration analyzes runnable queue find next maximal runnable subtree materialize 223 subtrees data unavailability possible execution runnable subtree one participating base relations discovered unavailable maximal runnable subtree defined respect current contents blocked runnable queues runnable queue guess real availability relations algorithm inserts operators runnable queue know whether associated relations actually available unavailable discovered corresponding relations requested case relation discovered unavailable execution runnable sub tree current iteration stops algorithm updates runnable blocked queues ancestors unavailable relation extracted runnable queue inserted blocked queue queues updated scrambling query plan initiates new phase 1 iteration order materialize another maximal runnable subtree 2 note remainder paper use maxi mal runnable subtree runnable subtree interchangeably except explicitly noted e2 g runnable blocked figure 2 blocked runnable operators relation unavailable 224 termination phase 1 end iteration algorithm checks data arrival discovered unavailable relation begun arrive algorithm updates blocked runnable queues ancestors unblocked relation extracted blocked queue inserted runnable queue note ancestors unblocked relation also depend blocked relations extracted queue phase 1 terminates execution query returns normal iteratorbased scheduling operators relations blocked execution query proceed final result returned user scrambling algorithm reinvoked however query execution blocks phase 1 also terminates runnable queue empty case phase 1 perform iteration remaining operators blocked happens query plan scrambling switches phase 2 purpose second phase process available relations operators query tree blocked present second phase query plan scrambling section 23 first however present example illustrates facets phase 1 described 225 running example example reuses complex query tree presented beginning section 2 discuss cases data need need partitioned joined assume tuples relations need partitioned contrast assume tuples relations f g h partitioned illustrate behavior phase 1 follow scenario given 1 execution query starts relation discovered unavailable 2 third iteration relation g discovered unavailable 3 tuples begin arrive query execution site end fourth iteration 4 time phase 1 terminates tuples g received execution example query begins requesting tuples remote site owning relation following scenario assume relation b ac e2 g ai g figure 3 query tree iterations 1 2 b figure 4 g unavailable x2 materialized unavailable indicated thick solid line figure 2 operators blocked delay depicted using dashed line unavailability invokes phase 1 updates blocked runnable queues initiates first iteration iteration analyses runnable queue finds first maximal runnable subtree consists unary operator selects tuples relation b 3 operator materialized ie selected tuples b local disk stored relation b algorithm checks arrival tuples following scenario assume tuples still unavailable another iteration initiated second iteration finds next maximal runnable subtree one rooted operator 3 note subtree rooted operator 2 maximal since consumer operator blocked figure 3 shows materialization runnable subtrees found first two iterations query scrambling part figure shows effect materializing first runnable subtree local relation b contains materialized selected tuples remote relation b also shows second runnable subtree indicated shaded grey area figure 3b shows query tree materialization second runnable subtree materialized result called x1 x1 materialized another iteration starts since example relation still unavailable third iteration finds next runnable subtree rooted operator 7 joins f g h stated relations need partitioned joined execution runnable subtree starts building left input operator 5 partitioning f f requests relation g order partition probing tuples f scenario however g discovered unavailable triggering update blocked runnable queues figure 4a shows operators 5 7 newly blocked operators operator 8 already blocked due unavailability queues operators updated another iteration scrambling initiated run next runnable sub tree ie one rooted operator 6 indicated shaded grey area figure result execution called x2 3 stated earlier operators inserted queues respect execution order figure 5 illustrates next step scenario ie illustrates case x2 materialized discovered tuples relation begun arrive case algorithm updates runnable blocked queues shown figure 5a operators 1 4 previously blocked unblocked operator 8 remains blocked however phase 1 terminates returns normal iteratorbased scheduling operators materializes left subtree root node see figure 5b resulting relation called x3 x3 materialized query blocked g phase 1 reinvoked phase 1 computes new contents runnable blocked queue discovers runnable queue empty since remaining operators ancestors g phase 1 terminates scrambling query plan enters phase 2 describe phase 2 algorithm next section 23 phase 2 creating new joins scrambling moves phase 2 runnable queue empty blocked queue goal phase 2 create new operators executed specifically second phase creates joins relations directly joined original query tree whose consumers blocked ie blocked queue due unavailability data contrast phase 1 iterations simply adjust scheduling allow runnable operators exe cute iterations phase 2 actually create new joins operations created phase 2 chosen optimizer original query plan generated possible operations may entail significant amount additional work joins created executed phase 2 expensive query scrambling could result net degradation performance phase 2 b figure 5 relation available b c5 gx3 g f x2 figure performing new join phase 2 therefore potential negate even reverse benefits scrambling care taken paper use simple heuristic avoiding cartesian products prevent creation overly expensive joins phase 2 section 3 analyze performance impact cost created joins relative cost joins original query plan one way ensure phase 2 generate overly expensive joins involve query optimizer choice new joins involving optimizer query scrambling one aspect ongoing work 231 creating new joins start phase 2 scrambling algorithm constructs graph g possible joins node g corresponds relation edge g indicates two connected nodes common join attributes thus joined without causing cartesian product unavailable relations placed g g constructed phase 2 starts iteratively create execute new join operators iteration phase 2 performs following steps 1 g find two leftmost joinable ie con nected relations j notion leftmost respect order query plan joinable relations g terminate scrambling 2 create new join operator 3 materialize 1 j update g replacing j materialized result 1 j update runnable blocked queues update query tree 4 test see unavailable data arrived terminate scrambling else begin new iteration figure 6 demonstrates behavior phase 2 continuing example previous section figure divided three parts part shows query tree end phase 1 case g would contain f x2 x3 assume g relations f x2 directly connected relation x3 connected either ie assume shares join attributes unavailable relation g example therefore f x2 two leftmost joinable relations x3 leftmost relation joinable figure 6b shows creation new join f x2 creation join requires removal join number 7 blocked queue replacement ordering execution join number 5 finally figure 6c shows materialization created operator materialized join called x4 point g modified removing f x2 inserting x4 joinable x3 relation g 232 termination phase 2 iteration phase 2 number relations g reduced phase 2 terminates g reduced single relation multiple relations none joinable shown preceding example latter situation arise attributes required join remaining relations contained unavailable relation case relation g phase 2 also terminate due arrival unavailable data data arrive phase 2 iteration end itera tion runnable blocked queues updated accordingly control returned normal iteratorbased scheduling operators mentioned phase 1 query scrambling may reinvoked later cope delayed relations 233 physical properties joins preceding discussion focused restructuring logical nodes query plan restructuring physical plans however raises additional considera tions first adding new join may require introduction additional unary operators process inputs new join correctly ex ecuted example merge join operator requires tuples consumes sorted thus may require sort operators applied inputs second deleting operators done preceding example may also require addition unary operators example relations may need repartitioned order placed children existing hybrid hash node finally changing inputs existing join operator may also require modifica tions new inputs sufficiently different original inputs physical join operators may modified example indexed nested loop join might changed hash join inner relation replaced one indexed join attribute 24 summary discussion query plan scrambling algorithm summarized follows ffl query becomes blocked relations unavailable query plan scrambling initi ated first computes queue blocked operators queue runnable operators phase 1 analyses queue runnable oper ators picks maximal runnable subtree materializes result process repeated ie iterates queue runnable operators empty point system switches phase 2 phase 2 tries create new operator joins two relations available joinable process iterates joinable relations found ffl iteration algorithm checks see unavailable data arrived control returned normal iteratorbased scheduling operators otherwise another iteration performed two additional issues regarding algorithm deserve mention first issue concerns knowledge actual availability lations instead discovering algorithm execution operations performed iteration sources unavailable possible send initial data requests data sources soon first relation discovered unavailable would give algorithm immediate knowledge availability status sources fortunately using iterator model opening multiple data sources force query execution site consume tuples simultaneously iterator model suspend flow tuples consumed consumer operators second issue concerns potential additional work phase described previously phase 1 materializes existing subtrees optimized prior runtime query optimizer relative overhead materialization may less significant depending io pattern scrambled subtree compared unscrambled ver sion example subtree consists single select base relation materialization phase 1 pure overhead since original query plan selecting tuples received without involving io hand overhead materializing operator partitions data comparatively less important case original query plan scrambled plan perform disk ios write partitions disk later pro cessing scrambled plan however writes disk one extra partition would kept memory original nonscrambled query plan phase 2 however costly creates new joins scratch using simple heuristic avoiding cartesian products advantage approach simplicity disadvantage however potential overhead caused possibly suboptimal joins study performance impact varying costs created joins following section costs materializations phase 1 new joins phase 2 may certain cases negate benefits scrambling controlling costs raises possibility integrating scrambling existing query optimizer would allow us estimate costs iterations order skip example costly materializations expensive joins integration one aspect ongoing work parameter value description numsites 8 number sites mips numdisks 1 number disks per site dskpagesize 4096 size disk page bytes netbw 1 network bandwidth mbitsec netpagesize 8192 size network page bytes compare 4 instr apply predicate hashinst 25 instr hash tuple move 2 instr copy 4 bytes table 1 simulation parameters main settings performance section examine main performance characteristics query scrambling algorithm first set experiments shows typical performance query scrambled second set experiments studies sensitivity phase 2 selectivity new joins creates first describe simulation environment used study algorithm 31 simulation environment study performance query scrambling algorithm extended existing simulator fjk96 models heterogeneous peertopeer database system shore cdf 94 simulator used provides detailed model query processing costs system briefly describe simulator focusing aspects pertinent experiments table 1 shows main parameters configuring simulator settings used study every site cpu whose speed specified mips parameter numdisks disks mainmemory buffer pool current study simulator configured model clientserver system consisting single client connected seven servers site except query execution site stores one base relation study disk query execution site ie client used store temporary results disk model includes costs random sequential physical accesses also charges software operations implementing ios unit disk io database clients disk cache pages size dskpagesize unit transfer sites pages size netpagesize network modeled simply fifo queue specified bandwidth netbw details particular technology ethernet atm modeled simulator also charges cpu instructions networking protocol op erations cpu modeled fifo queue simulator charges functions performed query operators like hashing comparing moving tuples memory paper simulator used primarily demonstrate properties scrambling algo rithm rather detailed analysis algo f g 10000 10000 10000 10000 10000 10000 10000 figure 7 query tree used experiments rithm specific settings used simulator less important way delay either hidden hidden algorithm experiments various delays generated simply requesting tuples unavailable source end various iterations query plan scram bling rather stochastically generating delays explicitly imposed series delays order study behavior algorithm controlled manner example simulate arrival blocked tuples say third iteration phase 1 scrambled query 3 times initiated transfer tuples blocked relation final result query could eventually computed 32 query tree experiments experiments described section use query tree represented figure 7 use query tree demonstrates features scrambling allows us highlight impact performance overheads caused materializations created joins base relation 10000 tuples 100 bytes assume join graph fully con nected relation equijoined relation joins use join attribute first set experiments study performance query plan scrambling case joins query tree produce number tuples ie 1000 tuples second set experiments however study case joins query tree different selectivities thus produce results various sizes experiments study performance approach case single relation unavailable relation leftmost relation ie relation represents case query scrambling beneficial examining cases others unavailable relations would change basic lessons study experiment described evaluate algorithm cases executes context small large memory case large memory none relations used query tree either base relation intermediate result need partitioned processed case small memory every relation including intermediate results must partitioned note since joins test query use join attribute repartitioning relations required new joins created case 33 experiment 1 step phenomenon figure 8 shows response time scrambled query plans generated delay relation leftmost relation plan var ied delay shown along xaxis also represented lower grey line figure higher grey line shows performance unscrambled query execution query simply delayed tuples relation begin arrive distance two lines therefore constant equal response time original unscrambled query plan 8003 seconds case experiment memory size query execution site small set ting hashtables inner relations joins entirely built memory partitioning required middle line figure 8 shows response time scrambled query plans executed various delays case six possible scrambled plans could generated stated sections 22 23 scrambling algorithm iterative end iteration checks see delayed data begun arrive stops scrambling normal query execution resumed however end iteration delayed data still arrived another iteration scrambling algorithm initiated result execution model step shape observed figure 8 width step equal duration operations performed current iteration scrambling algorithm height step equal response time query normal processing resumed end iteration example experiment first scrambling iteration results retrieval partitioning relation b operation requires 1223 seconds end iteration tuples relation begun arrive scrambling done normal query execution resumes resulting execution case response time 8010 seconds thus first step shown figure 8 width 1223 seconds height 8010 seconds note case scrambling effective hiding delay response time scrambled query nearly identical original query delay tuples arrived end first iteration another iteration performed case second iteration retrieves partitions joins relations c shown figure 8 iteration requires additional 2638 seconds begins arrive iteration resulting query plan total response time 8090 seconds thus experiment scrambling able hide delays 3861 seconds penalty 080 seconds ie 1 response response time secdelay sec delay scrambling figure 8 response times scrambled query plans small memory varying delay time original query delay corresponds response time improvement 32 compared scrambling end second iteration tuples still failed arrive third iteration initiated case however runnable subtrees scrambling switches phase 2 results creation new joins see section 23 third iteration result c1d partitioned joined relation b iteration width 201 seconds inputs already present b already partitioned result c1d fairly small response time resulting plan 8222 seconds represents response time improvement 32 compared scrambling remaining query plans exhibit similar behavior table 2 shows additional operations overall performance possible scrambled plans experiment largest relative benefit approximately 44 scrambling obtained delay 6979 seconds time required complete six iterations point work query scrambling scrambled plan must also wait ar rive seen figure 8 end iteration six response time scrambled plan increases linearly delay distance delay response time scrambled plan time required complete query arrives although apparent figure 8 first scrambled query slightly slower unscrambled query plan delayed short amount time delay 007 seconds response time scrambled query 8010 seconds 8003 seconds nonscrambled query joining b unscrambled query b partitioned join allowing one partitions b stay memory partitioning b joining first scrambled query plan forces partition written back disk read later join delayed less time needed perform additional ios cheaper stay idle waiting 34 experiment 2 sensitivity phase 2 previous experiment joins produced number tuples result operations performed phase 2 beneficial section examine sensitivity phase 2 changes selectivities joins creates varying selectivities changes number tuples produced joins affects width height step goal show cases benefits scrambling vary greatly clear improvements cases scrambling performs worse noscrambling test query first join created phase 2 join relation b result c1d materialized phase 1 set experi ments vary selectivity new join create result variable size selectivity join adjusted produces 1000 tuples several thousand tuples joins phase 2 may create behave like functional joins simply carry tuples created b1c1d query tree time tuples joined number tuples carried along query tree returns normality drops 1000 varying selectivity first join produced phase 2 sufficient generate variable number tuples carried along tree joins phase 2 may create two next sections present results sensitivity analysis small large memory case scrambled performed total response savings plan iteration delay time partition b 01223 8010 1318 table 2 delay ranges response times scrambled query plans 50000 1000 10000 delay sec response time delay scrambling phase 1 figure 9 response times scrambled query plans small memory varying selectivity delay stated previously memory small relations partitioned joined previous experiment partitioning adds potential cost scrambled plans results additional io would present unscrambled plan memory large however hashtables built entirely memory relations need partitioned thus large memory potential overhead scrambled plans lessened 341 small memory case experiment examine effectiveness query scrambling selectivity first join created phase 2 varied figure 9 shows response time results 3 different selectivities previous experiment delay shown along xaxis also represented lower grey line figure higher grey line shows response time unscrambled query fore increases linearly delay two lines exactly ones presented previous experiment solid line middle figure shows performance scrambled query plan stops scrambling right end phase 1 case two iterations performed phase 1 without initiating phase 2 iterations note line becomes diagonal end phase 1 since system simply waits tuples arrive computing final result query intuitively useful perform second phase scrambled queries resulting response time would located line costly joins would created phase 2 would consume lot resources little improvement hand phase 2 would beneficial scrambled queries whose resulting response time would line since additional overhead would small gain large dashed dotted lines figure illustrate tradeoffs lines show response time scrambled query plans executed various delays various selectivities note scrambled query plans share response times iterations performed phase 1 two first iterations correspond exactly scrambled plans 1 2 described previous ex periment end second iteration 3861 seconds however tuples still failed arrive third iteration initiated query scrambling enters phase 2 creates new joins dotted line shows performance selectivity new join produces result 1000 tuples line identical one showed previous experiment since joins producing 1000 tuples second selectivity first join created second phase produces 10000 tuples end iteration tuples still arrived another iteration initiated iteration process produce 10000 tuples corresponding line figure lowest dashed line case 10 times tuples carried along scrambled query plans step higher roughly 12 seconds wider since tuples manipulated case 1000 tuples created even additional overhead 10000 tuples however response times scrambled query plans far response times unscrambled query equivalent delay new join produces 50000 tuples higher dashed line figure response time scrambled plans almost equal even worse original unscrambled query including delay case costly carry large number tuples query tree simply wait blocked data arrive response time delay sec scrambling delay 10000 50000 80000 1000 phase 1 figure 10 response times scrambled query plans large memory varying selectivity delay 342 large memory case figure shows experiment case memory large enough allow inner relations joins built entirely main memory large memory partitioning relations needs done large memory case lines showing increasing delay response time unscrambled query delay increases separated 6503 seconds phase 2 starts delayed 1895 seconds four different selectivities represented figure contrast previous experiment 50 times tuples negated benefits scrambling case 80 times tuples carried scrambled query plans benefits become close zero large memory results computed iteration need materialized consumed contrast memory small materialized results partitioned consumed respect small memory case partitioning relation memory large reduces number ios allows scrambled plans manipulate tuples overhead experiments presented section shown query scrambling effective technique able improve response time queries data delayed improvements come fact iteration scrambled query plan hide delay data improve ment however depends overhead due materializations created joins improvement scrambling bring also depends amount work done original query bigger ie longer costly original query improvement technique bring since able hide larger delays computing costly operations improvement also depends shape query tree bushy trees offer options scrambling deep trees respect figures 9 10 presented many iterations done phase 1 point phase 2 starts shifts right increases distance phase 1 diagonal line response time unscrambled query turn scrambling algorithm handle wider range bad selectivities joins creates phase 2 4 related work section consider related work respect point time optimization decisions made ie compile time query startup time query runtime b variables used dynamic decisions ie response time remote source con sidered c nature dynamic optimization ie entire query rewritten basis optimization ie costbased heuristic based volcano optimizer cg94 gra93 dynamic optimization distributed query processing optimization cost comparison returns comparable choice part search space encoded chooseplan operator query start time incomparable cost comparisons evaluated according result reevaluation chooseplan operator selects particular query execution plan final decisions regarding query execution thus made query startup time work complimentary volcano optimizer since volcano adapt changes evaluation query started work dynamic query optimization either consider distributed case dmp93 ohms92 optimizes access path selection cannot reorder joins hs93 thus direct considerations problems response times remote sources accounted articles ever rich source optimizations carried work novel approach dynamic query optimization used rdbvms described ant93 ap proach multiple different executions logical operator occur time compete producing best execution one execution operator determined probably better execution terminated dsd95 response time queries improved reordering leftdeep join trees bushy join trees several reordering algorithms presented work assumes reordering done entirely compile time work cannot easily extended handle runtime reordering since reorderings restricted occur certain locations join tree acps96 tracks costs previous calls remote sources addition caching results use tracking estimate cost new calls volcano system optimizes query query compile query startup time change query plan query runtime research prototype mermaid cbty89 commercial successor interviso thmb95 heterogeneous distributed databases perform dynamic query optimization mermaid constructs query plan entirely runtime thus step query optimization based dynamic information intermediate join result sizes network performance mermaid neither takes advantage statically generated plan dynamically account source respond runtime sage system kno95 ai planning system query optimization heterogeneous distributed sources system interleaves execution optimization responds unavailable data sources 5 conclusion future work query plan scrambling novel technique dynamically adjust changes runtime environ ment presented algorithm specifically deals variability performance remote data sources accounts initial delays response times algorithm consists two phases phase 1 changes scheduling existing operators produced result query optimization phase 1 iteratively applied changes scheduling possible point algorithm enters phase 2 creates new operators process available data new operators iteratively created work query plan scrambling performance experiments demonstrated technique hides delays receiving initial requested tuples remote data sources examined sensitivity performance scrambled plans selectivity joins created phase 2 work represents initial exploration development flexible systems dynamically adapt changing properties environment among ongoing future research plans developing algorithms scramble different failure models handle environments data arrives bursty rate steady rate significantly slower expected also studying use partial results approximate final results also plan study potential improvement basing scrambling decisions costbased knowledge finally query plan scrambling promising approach addressing many concerns addressed dynamic query optimization adapting query plan runtime account actual costs operations could compensate often inaccurate unreliable estimates used query optimizer moreover could account remote sources export cost information especially important remote sources run complex subqueries thus plan investigate use scrambling complimentary approach dynamic query optimization acknowledgments would like thank praveen seshadri bjorn jonsson jeanrobert gruser helpful comments work would also like thank alon levy pointing related work r query caching optimization distributed mediator systems dynamic query optimization rdbvms distributed query processing multiple database system shoring persistent applications optimization dynamic query execution plans semantic data caching replacement design implementation gluenail database system reducing multidatabase query response time tree balancing performance tradeoffs client query evaluation techniques large databases optimization parallel query execution plans xprs query processing objectstore database system tradeoffs processing complex join queries via hashing multiprocessor database machines modern database systems object model dealing complexity federated database access scaling heterogeneous databases design disco tr ctr navin kabra david j dewitt efficient midquery reoptimization suboptimal query execution plans acm sigmod record v27 n2 p106117 june 1998 henrique paques ling liu calton pu ginga selfadaptive query processing system proceedings eleventh international conference information knowledge management november 0409 2002 mclean virginia usa ihab f ilyas walid g aref ahmed k elmagarmid hicham g elmongui rahul shah jeffrey scott vitter adaptive rankaware query optimization relational databases acm transactions database systems tods v31 n4 p12571304 december 2006 amol deshpande zachary ives vijayshankar raman adaptive query processing foundations trends databases v1 n1 p1140 january 2007 memoryadaptive scheduling large query execution proceedings seventh international conference information knowledge management p105115 november 0207 1998 bethesda maryland united states avigdor gal obsolescent materialized views query processing enterprise information systems proceedings eighth international conference information knowledge management p367374 november 0206 1999 kansas city missouri united states laurent amsaleg michael j franklin anthony tomasic dynamic query operator scheduling widearea remote access distributed parallel databases v6 n3 p217246 july 1998 qiang zhu jaidev haridas wenchi hou query optimization via contention space partitioning cost error controlling dynamic multidatabase systems distributed parallel databases v23 n2 p151188 april 2008 henrique paques ling liu calton pu distributed query adaptation tradeoffs proceedings acm symposium applied computing march 0912 2003 melbourne florida yongluan zhou beng chin ooi kianlee tan wee hyong tok adaptable distributed query processing architecture data knowledge engineering v53 n3 p283309 june 2005 kainlee tan pin kwang eng beng chin ooi ming zhang join multijoin processing data integration systems data knowledge engineering v40 n2 p217239 february 2002 tolga urhan michael j franklin laurent amsaleg costbased query scrambling initial delays acm sigmod record v27 n2 p130141 june 1998 qiang zhu jaidev haridas wenchi hou query optimization via contention space partitioning cost error controlling dynamic multidatabase systems distributed parallel databases v23 n2 p151188 april 2008 jeanrobert gruser louiqa raschid vladimir zadorozhny tao zhan learning response time websources using query feedback application query optimization vldb journal international journal large data bases v9 n1 p1837 march 2000 ron avnur joseph hellerstein eddies continuously adaptive query processing acm sigmod record v29 n2 p261272 june 2000 yong yao johannes gehrke cougar approach innetwork query processing sensor networks acm sigmod record v31 n3 september 2002 bertram ludscher pratik mukhopadhyay yannis papakonstantinou transducerbased xml query processor proceedings 28th international conference large data bases p227238 august 2023 2002 hong kong china alon halevy anand rajaraman joann ordille data integration teenage years proceedings 32nd international conference large data bases september 1215 2006 seoul korea bret hull vladimir bychkovsky yang zhang kevin chen michel goraczko allen miu eugene shih hari balakrishnan samuel madden cartel distributed mobile sensor computing system proceedings 4th international conference embedded networked sensor systems october 31november 03 2006 boulder colorado usa anthony tomasic rmy amouroux philippe bonnet olga kapitskaia hubert naacke louiqa raschid distributed information search component disco world wide web acm sigmod record v26 n2 p546548 june 1997