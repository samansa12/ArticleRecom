lower bounds rate convergence nonparametric pattern recognition show exist individual lower bounds corresponding upper bounds rate convergence nonparametric pattern recognition arbitrarily close yangs minimax lower bounds certain cubic classes regression functions used stone others rates equal ones corresponding regression function estimation problem thus classes classification easier regression function estimation b introduction independent identically distributed r f0 1gvalued random variables pattern recognition classication one wishes decide whether value label 0 1 given ddimensional value x observation one wants nd decision function g dened range x taking values 0 1 gx equals high probability assume main aim analysis minimize probability error xg posteriori probability regression function introduce bayesdecision let bayeserror denote distribution x introduce kfk q r 1q wellknown see devroye gyor lugosi 8 measurable function relation z 1 fg 6g g 2 holds denotes indicator function event therefore function g achieves minimum 1 minimum l classication problem consider distribution x therefore also g unknown given independent sample distribution x one wants construct decision rule r f0 1g n 7 f0 1g close l paper study asymptotic properties el n l estimate n regression function derive plugin rule g n n quite naturally 2 get easily see 8 shows k sense latter least rate sense classication complex regression function estimation wellknown exist regression function estimates classication rules universally consistent satisfy distributions x rst shown stone 14 nearest neighbor estimates see also 8 list references classication actually easier regression function estimation sense plugin rule see 8 chapter 6 relative expected error g n decreases faster expected error n moreover k plugin rule see antos 1 relation also holds strong consistency however value ratio cannot universally bounded convergence arbitrary slow depends behavior near 12 rate convergence f n g unfortunately exist rules el n l tends zero guaranteed rate convergence distributions x theorem 72 problem 72 devroye et al 8 imply following slowrateofconvergence result let fa n g positive sequence converging zero 116 1 every sequence fg n g decision rules exists distribution x x uniformly distributed 0 1 2 f0 1g n therefore order obtain nontrivial rateofconvergence results one restrict class distributions natural ask fastest achievable rate given class distributions usually done considering minimax rateofconvergence results one derives lower bounds according following denition denition 1 positive sequence fa n g called lower rate convergence class distributions x lim sup sup xy 2d n remark many cases limit superior denition could replaced limit inferior inmum lower bound minimax loss holds suciently large n since determine distribution x class distributions often given product class h allowed distributions x class f allowed regression functions example h may consist absolute continuous distributions distributions distributionfree approach one particular distribution distribution sensitive approach paper give lower bounds classes last strongest mat h contains one uniform distribution class functions parameter dened later minimax lowerrate results types distribution classes eg vapnikchervonenkis classes see devroye et al 8 references therein related results general minimax theory statistical estimates see ibragimov khasmiskii 9 10 11 korostelev tsybakov 12 yang 16 points 3 holds every xed distribution f n g consistent optimal rate convergence many usual classes classication regression function estimation shows many examples counterexamples phenomenon rates convergence terms metric entropy classication seems complexity regression function estimation classes rich near 12 see also mammen tsybakov 13 example shown yang 16 distribution classes optimal rate convergence fn 2d g regression function estimation see also stone 15 sense lower bounds satisfactory tell us anything way probability error decreases sample size increased given classi cation problem bounds n give information maximal probability error within class behavior probability error single xed distribution sample size n increases words bad distribution causing largest probability error decision rule may dierent n example previous lower bounds classes exclude possibility exists sequence fg n g every distribution expected probability error el n l decreases exponential rate n paper interested also individual minimax lower bounds describe behavior probability error xed distribution x sample size n grows denition 2 positive sequence fa n g called individual lower rate convergence class distributions x fgng sup xy 2d lim sup n inmum taken sequences fg n g decision rules concept individual lower rate introduced birge 7 concerning density estimation individual lowerrate results concerning pattern recognition see antos show every sequence fb n g tending zero fb n n 2d g individual lower rate convergence classes hence exist individual lower rates classes arbitrarily close optimal lower rates rates individual lower rates expected l 2 error regression function estimation classes see also antos gyor kohler 4 antos 3 regression function estimation pattern recognition individual lower rates optimal hence extend yangs observation individual rates classes results also imply ratio eln l tend zero arbitrary slowly even xed sequence f n g next give denitions function classes derive lower rates convergence let denote l q norm regarding lebesguemeasure r k k lq denition 3 given 1 q 1 r 2 class functions f r kd moreover every kd denotes partial derivative respect q 1 assume r 1 rateofconvergence results hold 0 mr 1 innite omit conditions rst kind classes generalizations lipschitz classes example 2 yang 16 antos gyor kohler 4 birge 7 stone 15 also generalizations special case example 4 yang 16 polinomial modulus continuity denition 4 given 0 let v class functions f kfk1 0 r r r e th unit vector r classes example 5 yang 16 assume mm 0 12 denition 5 denote one classes lip v f let class distributions x x uniformly distributed 0 1 wellknown exist regression function estimates f n g satisfy lim sup sup xy 2d see eg 16 barron birge massart 6 remains true replacing condition denition 5 eg assumption class distributions uniformly bounded density funtions note rate depends thus plugin rules fg n g lim sup sup xy 2d 2d sup xy 2d el n l 2d rate n 2d might formulated exponent dimension exponent 1 metric entropy classes see 7 handle two types classes together let classes lip thus holds cases may call exponent global smoothness following 7 give conditions general function class f assure lower rates convergence corresponding distribution class subclass regression functions indexed vectors 1 1 components introduced denote set vectors c let uniform distribution 0 1 assumption 1 satisfying 6 probability distribution fp j g every j 2 n function support brick disjoint even dierent j j 0 every c 2 c c independent fp j g j note 6 theorem 1 assumption 1 holds function class f sequence lower rate convergence corresponding distribution class assumption 1 similar a2k birge 7 form required individual lower rates seems proof choice fp j g minimax rates use weaker form assumption 2 satisfying 6 2 0 1 function support brick 0 1 every c r also km k 2 almost assumption 3 yang 16 a2k birge 7 case polinomial metric entropy theorem 1 analogous yangs theorem 2 yangs theorems give special case corollary 1 f lip v sequence 2d lower rate convergence class main result following extention theorem 1 individual lower rates see antos 2 lip theorem 2 let fb n g arbitrary positive sequence tending zero assumption 1 holds function class f sequence b n 2d individual lower rate convergence corresponding distribution class remark 1 applying sequence f b n g theorem 2 implies fg n g lim sup b n n remark 2 certainly theorems 1 2 hold increase class leaving condition denition 5 focusing classes lip corollary 2 let fb n g arbitrary positive sequence tending zero f lip sequence b n 2d individual lower rate convergence class call sequence fc n g upper rate convergence class exist rules fg n g satisfy lim sup sup xy 2d sup xy 2d el n l individual upper rate convergence class exist rules fg n g satisfy sup xy 2d lim sup implies every distribution el n l possibly dierent constants 5 implies n 2d upper rate convergence thus also individual upper rate convergence theorem 1 shows upper rate convergence better n follows theorem 2 n even optimal individual upper rate sense doesnt exist individual upper rate c n convergence satises lim moreover 3 4 imply fgng sup xy 2d lim sup shows theorem 2 cannot improved dropping b n shows strange nature individual lower bounds every sequence tending zero faster 2d individual lower rate n 2d 3 proofs proofs theorems apply following lemma ldimensional real vector taking values 14 14 l let c zero mean random variable taking values f11g let l inde pedent binary variables given c error probability bayes decision c based proof bayes decision 1 see 8 one verify fy 1g 2 fy 0g arbitrary 0 q 12 q j log j log 1 q markovs inequality log 1 q moreover j log get log using inequality 14 x 14 one hand hand hence log jg thus efg q 1 efj log jg log 1 q qa q 1q p log 1 q choosing proof theorem 1 method proof diers yang easily modied individual lower bound theorem 2 assumption 1 0 c 1 imply distribution x x unif0 1 2 f0 1g efy x 2 0 1 c 2 c contained implies lim sup gn sup xy 2d n lim sup sup xy xunif01 efy jxxg c xc2c n let g n arbitrary rule denition fi jk 2 j kg orthogonal system measure r therefore projection g given jk x r g n 12i jk 2 r jk 2 r jk r jk r jk r jk r jk r jk note arbitrary note g 1 jk 2 z c 1 fgn 6g g z z kx let c njk otherwise jc njk c jk j fc njk 6c jk g get fc njk 6c jk g p proves equations 8 imply lim sup gn sup xy 2d n k gn sup n bound last term x rules fg n g choose c 2 c randomly let sequence independent identically distributed random variables independent x 1 satisfy pfc 12 next derive lower bound sign c njk interpreted decision c jk using n error probability minimal bayes decision c njk 1 pfc therefore l x 2 jk given distributed conditions lemma 1 u depends c n fc jk g x r r 62 fi therefore independent conditioning x error conditional bayes decision c jk based depends pf jenseninequality pf 1e 10e k1np 2d independently k thus np 2d pf 2d j n 1 2d 2d cn 1 lim sup sup n lim sup n together 11 implies assertion 2 proof theorem 2 use notations results proof theorem 1 fgng sup xy 2d lim sup b n n k fgng sup lim sup b n n case choose fp j g independently n since b n n tend zero take subsequence fn g t2n fng n2n b n 2 1 1 choose fp j g q repeated 2 q times td2 1 1 tan 1 1 1 specially er ns c k 3 ts b ns ns 15 nish proof spirit lemma 1 antos lugosi 5 using 15 one gets fgng sup lim sup b n n fgng sup lim sup r ns c b ns ns k 3 fgng sup lim sup r ns c er ns c k 3 fgng lim sup r ns c er ns c 12 fact c 2 c sequence fr ns cer ns cg uniformly bounded apply fatous lemma get fgng sup lim sup b n n k 3 fgng lim sup r ns c er ns c together 14 implies assertion 2 proof corollary 1 2 must prove classes lip v satisfy assumption 1 parameters given text satisfying 6 fp j g give required functions j sets jk first pack disjoint sets jk 0 1 following way assume simplicity index minimal takes role rst dimension construction given fp j g let fb j g partition 0 1 b j interval length p j pack disjoint translates j brick gives since x 1 bxc x2 p j let uniform distribution 0 1 choose function 14 support subset 0 1 denotes diagonal matrix thus j contraction 0 1 j km 2 check iii every c 2 c c case f note functions jk disjoint support holds also derivatives q 1 r q thus kd c k q c jk kd moreover every kd c c xk lq c jk c jk c jk choose support 13 23 kk p j 3 support jk jk hence disjoint thus rst term c jk second term c jk c jk c jk c jk c jk c jk kd gives kd c r p kd c jk kd moreover every 2 r introduce x 0 x 00 function x following way x x fall jk let x dierent bricks consider segment xx let x 0 x 00 intersections segment borders bricks x x vanishing outside jk partial derivatives order r zero border jk thus case c c kd c c xk 1 c c c c xk 1 kd c c x 00 c c using fact x x 0 brick support jk x jk second term kd c c c jk kp r sup get bound similarly rst term gives kd c see also 15 p 1045 case f k c k1 c jk r c jk r c jk r choose support 12 1 r support r jk jk hence disjoint rst term c jk r second term c jk r c jk r r r r r r r gives r lower bounds rate convergence nonparametric pattern recog nition performance limits nonparametric estimators strong minimax lower bounds learning nonparametric estimation regression statistical estimation asymptotic theory bounds quality nonparametric minimax theory image reconstruction smooth discrimination analysis consistent nonparametric regression optimal global rates convergence nonparametric regression minimax nonparametric classi tr characterizing rational versus exponential learning curves strong minimax lower bounds learning minimax methods image reconstruction lower bounds rate convergence nonparametric pattern recognition