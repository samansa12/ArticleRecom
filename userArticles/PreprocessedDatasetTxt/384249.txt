fast priority queues cached memory cache hierarchy prevalent todays high performance processors taken account order design algorithms perform well practice paper advocates adaption external memory algorithms purpose idea practical issues involved exemplified engineering fast priority queue suited external memory cached memory based ikiway merging improves previous external memory algorithms constant factors crucial transferring cached memory running cache hierarchy workstation algorithm least two times faster optimized implementation binary heaps 4ary heaps large inputs b introduction mainstream model computation used algorithm designers last half century 18 assumes sequential processor unit memory access cost however mainstream computers sitting desktops increasingly deviated model last decade 10 11 13 17 19 particular usually distinguish least four levels memory hierarchy file multiported registers accessed parallel every clockcycle firstlevel cache still accessed every one two clockcycles parallel ports achieves high throughput pipelining therefore instruction level parallelism superscalar processors works best instructions use registers currently firstlevel caches quite small 864kb order able keep chip close execution unit secondlevel cache considerably larger also order magnitude higher latency offchip size mainly constrained high cost fast static ram main memory build high density low cost dynamic ram including overheads cache miss memory latency translation logical virtual physical memory addresses main memory access two orders magnitude slower first level cache hit machines separate caches data code disregard instruction reads long programs remain reasonably short although technological details likely change future physical principles imply fast memories must small likely expensive slower memories live memory hierarchies talking sequential algorithms large inputs general approach paper model one cache level main memory single disk single processor variant external memory model 22 model assumes internal memory size access external memory transferring blocks size b use word pairs cache line memory block cache internal memory main memory external memory io cache fault synonyms context indicate otherwise formal limitation compared external memory caches fixed replacement strategy another paper show relatively little influence algorithm kind considering nevertheless henceforth use term cached memory order make clear different model despite farreaching analogy external memory cached memory number additional differences noted since speed gap caches main memory usually smaller gap main memory disks careful also analyze work performed internally ratio main memory size first level cache size much larger disk space internal memory therefore prefer algorithms use cache economically possible finally also discuss remaining levels memory hierarchy informally order keep analysis focussed important aspects section 2 present basic algorithm sequence heaps data structure priority queues 1 algorithm analyzed section 3 using external memory model thetam k thetamb constant perform insertions deletemins using i2rb log mo1 key comparisons another paper show similar bounds hold cached memory away associative caches k reduced ob 1a section 4 present refinements take levels memory hierarchy account ensure almost optimal memory efficiency amortized work performed operation depends current queue size rather total number operations section 5 discusses implementation algorithm several architectures compares results priority queue data structures previously found efficient practice namely binary heaps 4ary heaps related work external memory algorithms well established branch algorithmics eg 21 20 external memory heaps teuhola wegner 23 fish spear data structure 9 need thetab less ios traditional priority queues like binary heaps buffer search trees 1 first external memory priority data structure representing totally ordered set supports insertion elements deletion minimal element queue reduce number ios another factor thetalog lower bound oib log mb im ios operations amortized using fullfledged search tree implementing priority queues may considered wasteful heaplike data structures brodal katajainen crauser et al fadel et al 3 7 8 directly geared priority queues achieve asymptotic bounds one 3 even per operation amortized sense sequence heap similar particular considered simplification reengineering improved arrayheap 7 however sequence heaps ioefficient factor three 1 3 7 8 need factor two less memory 1 7 8 2 algorithm merging k sorted sequences one sorted sequence kway merging io efficient subroutine used sorting external 14 cached memory 16 basic idea sequence heaps adapt kway merging related dynamical problem priority queues let us start simple case km insertions take place size buffer fits fast memory data structure could consist k sorted sequences length use kway merging deleting batch smallest elements k sorted sequences next deletions served buffer constant time allow arbitrary mix insertions deletions maintain separate binary heap size holds recently inserted elements deletions check whether smallest element come insertion buffer buffer full sorted resulting sequence becomes one sequences kway merge point sequence heaps earlier data structures 3 7 8 almost identical differences related question handle km elements cannot increase beyond since insertion heap would fit fast memory cannot arbitrarily increase k since eventually kway merging would start incur cache faults sequence heaps use approach make room merging k sequences producing larger sequence size km 3 7 question arises handle larger sequences adopt approach used improved arrayheaps 7 employ r merge groups g holds k sequences size mk igamma1 group g overflows sequences merged resulting sequence put group g i1 group equipped group buffer size allow batched deletion sequences smallest elements buffers deleted batches size 0 stored deletion buffer fig 1 summarizes data structure enough information explain deletion works deletemin smallest elements deletion buffer insertion buffer compared smaller one deleted returned empties deletion buffer refilled group buffers using rway merge kmerge kmerge kmerge rmerge insert heap mk buffer 3 group group buffer 1 mt group buffer 2 deletion buffer fig 1 overview data structure sequence heaps refill group buffers less 0 elements refilled sequences group group nonempty deletemin works correctly provided data structure fulfills heap prop erty ie elements group buffers smaller elements deletion buffer turn elements sorted sequence smaller elements respective group buffer maintaining invariant main difficulty implementing insertion insert new elements inserted insert heap size reaches elements sorted eg using merge sort heap sort result merged concatenation deletion buffer group buffer 1 smallest resulting elements replace deletion buffer group buffer 1 remaining elements form new sequence length new sequence finally inserted free slot group g 1 free slot initially g 1 emptied merging sequences single sequence size km put g 2 strategy used recursively necessary group gr overflows r incremented new group created sequence moved one group heap property may violated therefore g 1 g emptied group buffers 1 i1 merged put g 1 latter measure one differences improved array heap invariant maintained merging new sequence group buffer measure almost halves number required ios cached memory speed internal computation matters also crucial implement operation kway merging propose use loser tree variant selection tree data structure described knuth 14 section 541 k 0 nonempty sequences consists binary tree k 0 leaves leaf stores pointer current element sequence current keys sequence perform tournament winner passed tree key loser index leaf stored inner node overall winner stored additional node root using data structure smallest element identified replaced next element sequence using dlog comparisons less heap size k assumed 7 8 would require address calculations memory references similar needed binary heaps noteworthy difference memory locations accessed loser tree predictable case deleting binary heap instruction scheduler compiler place accesses well data needed thus avoiding pipeline stalls particular combined loop unrolling 3 analysis start analysis number ios terms b parameters arbitrary sequence insert deletemin operations insertions deletemins continue number key comparisons measure internal work discuss k 0 chosen external memory cached memory respectively adaptions memory efficiency many accesses relatively small queues postponed section 4 need following observation minimum intervals tree emptying operations several places lemma 1 group g overflow every mk proof complication slot group g 1 used invalid group buffers nevertheless groups g 1 g contain k sequences happen insertions taken place particular since room insertions insertion buffer simple upper bound number groups needed corollary 1 log k upsilon groups suffice analyze number ios based assumption following information kept internal memory insert heap deletion buffer merge buffer size group buffers 1 r loser tree data groups gr grgamma1 assume kb units memory suffice store blocks k sequences currently accessed loser tree information corresponding amount space shared remaining r gamma 2 groups data merging r group buffers 2 theorem 1 log k ios suffice perform sequence inserts deletemins sequence heap proof let us first consider ios performed element moving following canonical data path first inserted insert buffer written sequence group g 1 batched manner ie charge 1b ios insertion element involved emptying groups arrives group gr emptying operation involved one batched read one batched write ie charge 2r gamma 1b ios tree emptying operations eventually read group buffer r charge 1b ios get charge 2rb ios insertion remains shown remaining ios contribute lower order terms replace ios done canonical path element travels group grgamma1 2b ios must charged writing group buffer later reading refilling deletion buffer however 2b ios saved element moved group gr pay charge element travels group buffer r gamma 2 additional saved compared canonical path also pay cost swapping loser tree data group g latter costs 2kb divided among least removed one batch group buffer 2 becomes invalid must merged group buffers put back group g 1 causes direct cost omb ios must charge cost oimb ios elements thrown back oi steps path deletion buffer although element may move r groups need charge ormb ios small since means shortcut originally taken element compared canonical path missed remaining overhead charged mk gamma 1k j gamma2 insertions filled group g igamma1 summing groups insertions gets additional charge similarly invalidations group buffer 1 give charge o1k per insertion need olog inserting new sequence loser tree data structure done tree 1 amortized insertions amortized mk lemma 1 2 accept o1b ios per operation would suffice swap insertion buffer plus constant number buffer blocks one loser tree k sequence buffers internal memory element moving canonical path get overall charge olog km overall get charge 2rbo1klogkm per insertion estimate number key comparisons performed believe good measure internal work since efficient implementations priority queues comparison model number close number unpredictable branch instructions whereas loop control branches usually well predictable hardware compiler number key comparisons also proportional number memory accesses two types operations often largest impact execution time since severe limit instruction parallelism superscalar processor order avoid notational overhead rounding also assume k powers two divisible mk rgamma1 general bound would larger small additive term theorem 2 assumptions theorem 1 ilog dlog log 4 0 molog kk key comparisons needed average case inputs log replaced o1 proof insertion insertion buffer takes log comparisons worst o1 comparisons average every deletemin operation requires comparison minimum insertion buffer deletion buffer remaining comparisons charged insertions analogous way proof theorem 1 sorting insertion buffer eg using merge sort takes log comparisons merging result deletion buffer group buffer 1 takes comparisons inserting sequence loser tree takes olog comparisons emptying groups takes r gamma 1 log k ork comparisons per element elements removed insertion buffer take log comparisons need counted since save comparisons similarly refills group buffers r already accounted conservative estimate group emptying cost group gr degree imk comparisons per element suffice using similar arguments proof theorem 1 shown inserting sequences loser trees leads charge olog km comparisons per insertion invalidating group buffers costs olog kk comparisons per insertion summing charges made yields bound proven external memory one would choose another paper show k factor ob 1a ffi smaller away associative caches order limit number cache faults 1 ffi times number ios performed external memory algorithm requirement together small size many first level caches tlbs 3 explains ranslation lookaside buffers store physical position recently used virtual memory pages may live quite small k observation main reason pursue simple variant array heap described 7 needs single merge group sequences merge group would factor r larger however refinements memory management sequence heap implemented memory efficient way representing sequences groups singly linked lists memory pages whenever page runs empty pushed stack free pages new page needs allocated popped stack necessary stack maintained externally except single buffer block using pages size p external sequences sequence heap r groups n elements occupy n kpr memory cells together measures described keeping number groups small becomes n kp log k nm page size particularly easy implement since also size group buffers insertion buffer long guarantees asymptotically optimal memory efficiency ie memory requirement many operations small queues let n denote queue size ith operation executed earlier algorithms 3 7 8 number ios bounded ii log k n certain classes inputs ii log k n considerably less log k im however believe applications require large queues difference large enough warrant significant constant factor overheads algorithmic complications therefore chosen give detailed analysis basic algorithm first outline adaption yielding refined asymptotic bound similar 7 new sequence inserted group g free slot first look two sequences g whose sizes sum less mk elements found sequences merged yielding free slot merging cost charged deletemins caused sequences get small g emptied contains least mk 2 elements ios involved charged elements inserted g least size mk igamma1 4 similarly tidy shrinking queue r groups total size queue falls mk group gr insert resulting sequence group grgamma1 free slot group grgamma1 merge two sequences first registers instruction cache realistic cases r 4 groups therefore instruction cache register file likely large enough efficiently support fast rway merge routine refilling deletion buffer keeps current keys stream registers second level cache far analysis assumes single cache level still assume level first level cache second level cache may influence first note group buffers loser trees group buffers likely fit second level cache second level cache may also large enough accommodate group g 1 reducing costs 2b ios per insert get interesting use second level cache assume bandwidth sufficiently high bottleneck look inputs deletions insertion buffer rare eg sorting choose size second level cache insertions high locality log cache lines currently accessed fit first level cache operations deletion buffers group buffers use random access high bandwidth disks sequence heap data structure viewed classical external memory algorithm would simply use main memory size measurements section 5 indicate large binary heaps insertion buffer may slow match bandwidth fast parallel disk subsystems case better modify sequence heap ooptimized cache main memory using specialized external memory implementations larger groups may involve buffering disk blocks explicit asynchronous io calls perhaps prefetching code randomization supporting parallel disks 2 also number ios may reduced using larger k inside external groups degrades performance loser tree data structure much insert another heap level ie split high degree group several low degree groups connected together sufficiently large level2 group buffers another merge data structure deletions nonminimal elements performed maintaining separate sequence heap deleted elements deletemin smallest element main queue deletequeue coincide discarded hereby insertions deletions cost one comparison charge delete costs one insertion two deletemins note latter much cheaper insertion memory overhead kept bounds completely sorting queues whenever size queue deleted elements exceeds fraction size main queue sorting operation deleted keys discarded resulting sorted sequence put group gr sequences deletion heap empty 5 implementation experiments implemented sequence heaps portable c template class arbitrary keyvaluepairs currently sequences implemented single array performance sequence heap mainly stems efficient implementation kway merge using loser trees special routines 2way 3way 4way merge binary heaps insertion buffer important optimizations turned roughly order making live compiler use sentinels ie dummy elements ends sequences heaps save special case tests loop unrolling 51 choosing competitors author new code wants demonstrate usefulness experimen tally great care must taken choose competing code uses one best known algorithms least equally well tuned chosen implicit binary heaps aligned 4ary heaps recent study 15 two algorithms outperform pointer based data structures splay tree skew heap factor two although latter two performed best older study 12 least need code insertion buffer binary heaps coded perhaps even carefully remaining components binary heaps part code took care assembler code contains unnecessary memory accesses redundant computations reasonable instruction schedule also use bottom heuristics deletemin elements first lifted minpath root leaf leftmost element put freed leaf finally bubbled note binary heaps heuristics perform log comparisons insertions plus deletemin average close lower bound flat memory hard find comparison based algorithm performs significantly better average case inputs small queues binary heaps factor two faster straightforward nonrecursive adaption textbook formulation used cormen leiserson rivest 5 aligned 4ary heaps developed end using basic approach binary heaps particular bottom heuristics also used main difference data gets aligned cache lines complex index computations needed source codes available electronically httpwwwmpisbmpg desandersprograms 52 basic experiments although programs developed tuned sparc processors sequence heaps show similar behavior recent architectures available measurements run code sparc mips alpha intel processor even turned single parameter setting works well machines 4 figures 2 3 4 5 respectively show results measurements use random values maximal heap size n operation sequence insert deletemin insert deletemin insert deletemin n executed normalize amortized execution time per insertdeleteminpair t6n dividing log n since algorithms flat memory execution time c log n o1 constant c would expect curves hyperbolic form converge 4 tuning k performance improvements around 10 possible eg ultra pentiumii better tinsertlog bottom binary heap bottom aligned 4ary heap sequence heap fig 2 performance sun ultra10 desktop workstation 300 mhz ultra processor 1stlevel cache using sun workshop c 42 options fast o4501501024 4096 16384 65536 2 tinsertlog bottom binary heap bottom aligned 4ary heap sequence heap fig 3 performance 180 mhz mips r10000 processor compiler cc r10000 tinsertlog bottom binary heap bottom aligned 4ary heap sequence heap fig 4 performance 533 mhz decalpha21164 processor compiler g o6501501024 4096 16384 65536 2 tinsertlog bottom binary heap bottom aligned 4ary heap sequence heap fig 5 performance 300 mhz intel pentium ii processor compiler g o6 constant large n values shown averages least 10 trials small inputs avoid problems due limited clock resolution order minimize impact processes virtual memory management warmup run made measurement programs run almost unloaded machines sequence heaps show behavior one would expect flat memory cache faults rare influence execution time much section 54 see decrease time per comparison quite strong inputs machines binary heaps equally fast slightly faster sequence heaps small inputs heap still fits second level cache performance remains rather stable even larger queues performance degradation accelerates time per comparison growing linearly log n easy explain whenever queue size doubles another layer heap fit cache contributing constant number cache faults per deletemin sequence heaps 21 38 times faster binary heaps consider difference large enough considerable practical interest furthermore careful implementation algorithms makes unlikely performance difference reversed tuning use different compiler 5 binary heaps sequence heaps could slightly improved replacing index arithmetics arithmetics address offsets would save single registertoregister shift instruction per comparison likely little effect superscalar machines furthermore satisfactory performance binary heaps small inputs shows large inputs time spent memory access overhead coding details little influence 53 4ary heaps measurements figures 2 5 largely agree important observation lamarca ladner 15 since number cache faults halved compared binary heaps 4ary heaps robust behavior large queues still sequence heaps another factor 25 29 faster large heaps since reduce number cache faults even however relative performance binary heaps 4ary heaps seems complicated issue 15 although main concern paper would like offer explanation although bottom heuristics improves binary heaps 4ary heaps binary heaps profit much binary heaps need less instead comparisons 4ary heaps concerning instruction counts 4ary 5 example older studies heaps loser trees may looked bad compared pointer based data structures compiler generates integer division operations halving index integer multiplications array indexing heaps save memory write instructions need complicated index computations apparently alpha highest clock speed machines considered saved write instructions shorten critical path index computations done parallel slow memory accesses spill code machines balance turns direction partic ular intel architecture lacks necessary number registers compiler generate large number additional memory accesses even large queues handicap never made confusing effect jump execution time 4ary heaps sparc n 2 20 nothing like observed machines effect hard explain cache effects alone since input size already well beyond size second level cache suspect problems virtual address translation also haunted binary heaps earlier version 54 long operation sequences worst case analysis predicts certain performance degradation number insertions much larger size heap n however fig 6 seen contrary true random keys2060100 tinsertlog fig 6 performance sequence heaps using setup fig 2 using different operation sequences insert deletemin insert deletemin insert deletemin 16g essentially get heapsort overhead maintaining useless group deletion buffers fig 2 used family instances 33n heap grows shrinks slowly almost two times faster n reason new elements tend smaller old elements smallest old elements long removed therefore many elements never make group g 1 let alone groups larger sequences since work performed emptying groups work saved similar locality effect observed analyzed fishspear data structure 9 binary heaps 4ary heaps property even seem get slightly slower locality effect cannot work instances come close worst case 6 discussion sequence heaps may currently fastest available data structure large comparison based priority queues cached external memory particularly true queue elements small need deletion arbitrary elements decreasing keys implementation approach particular kway merging loser trees also useful speed sorting algorithms cached memory cases sequence heaps still look promising need experiments encompassing wider range algorithms usage patterns decide algorithm best example monotonic queues integer keys radix heaps look promising either simplified average case efficient form known calendar queues 4 adapting external memory radix heaps 6 cached memory order reduce cache faults outlined algorithm adapted multiple levels memory parallel disks shared memory multiprocessor also possible achieve moderate speedup parallelization eg one processor insertion deletion buffer one group refilling group buffers processors collectively work emptying groups acknowledgements would like thank gerth brodal andreas crauser jyrki katajainen ulrich meyer valuable suggestions ulrich rude university augsburg provided access alpha processor r buffer tree new technique optimal ioalgorithms simple randomized mergesort parallel disks calendar queues fast o1 priority queue implementation simulation event set problem introduction algorithms performance ledasm efficient priority queues external memory external heaps combined effective buffering priority queue algorithm computer architecture quantitative ap proach empirical comparison priorityqueue event set implementa tions 21264 superscalar alpha processor outoforder execution art computer programming sorting searching influence caches performance heaps influence caches performance sorting first draft report edvac tpie user manual reference external memory algorithms algorithms parallel memory two level memories external heapsort tr empirical comparison priorityqueue eventset implementations calendar queues fast 01 priority queue implementation simulation event set problem external heapsort bottomupheapsort new variant heapsort beating average quicksort italicnitalic small fishspear priority queue algorithm influence caches performance heaps simple randomized mergesort parallel disks influence caches performance sorting worstcase externalmemory priority queues buffer tree accessing multiple sequences set associative caches experimental study priority queues external memory external memory algorithms ctr peter sanders presenting data experiments algorithmics experimental algorithmics algorithm design robust efficient software springerverlag new york inc new york ny 2002 roman dementiev peter sanders asynchronous parallel disk sorting proceedings fifteenth annual acm symposium parallel algorithms architectures june 0709 2003 san diego california usa james fix setassociative cache performance search trees proceedings fourteenth annual acmsiam symposium discrete algorithms january 1214 2003 baltimore maryland joonsang park michael penner viktor k prasanna optimizing graph algorithms improved cache performance ieee transactions parallel distributed systems v15 n9 p769782 september 2004 bernard e moret david bader tandy warnow highperformance algorithm engineering computational phylogenetics journal supercomputing v22 n1 p99111 may 2002 gerth stlting brodal rolf fagerberg kristoffer vinther engineering cacheoblivious sorting algorithm journal experimental algorithmics jea 12 2007