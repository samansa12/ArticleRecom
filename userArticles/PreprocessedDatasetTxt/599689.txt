logistic regression adaboost bregman distances give unified account boosting logistic regression learning problem cast terms optimization bregman distances striking similarity two problems framework allows us design analyze algorithms simultaneously easily adapt algorithms designed one problem problems give new algorithms explain potential advantages existing methods algorithms iterative divided two types based whether parameters updated sequentially one time parallel also describe parameterized family algorithms includes sequential parallelupdate algorithm special cases thus showing sequential parallel approaches unified algorithms give convergence proofs using general formalization auxiliaryfunction proof technique one sequentialupdate algorithms equivalent adaboost provides first general proof convergence adaboost show algorithms generalize easily multiclass case contrast new algorithms iterative scaling algorithm conclude experimental results synthetic data highlight behavior old newly proposed algorithms different settings b introduction give unified account boosting logistic regression show learning problems cast terms optimization bregman distances frame work two problems become extremely similar real difference choice bregman distance unnormalized relative entropy boosting binary relative entropy logistic regression fact two problems similar framework allows us design analyze algorithms si multaneously able borrow methods maximumentropy literature logistic regression apply exponential loss used adaboost especially convergenceproof techniques conversely easily adapt boosting methods problem minimizing logistic loss used logistic regression result family new algorithms problems together convergence proofs new algorithms well adaboost adaboost logistic regression attempt choose parameters weights associated given family functions called features weak hypotheses adaboost works sequentially updating parameters one one whereas methods logistic regressionmost notably iterative scaling 9 10 iterative update parameters parallel iteration first new algorithm method optimizing exponential loss using parallel updates seems plausible parallelupdate method often converge faster sequentialupdate method provided number features large make parallel updates infeasible preliminary experiments suggest case second algorithm parallelupdate method logistic loss although parallelupdate algorithms well known function updates derive new preliminary experiments indicate new updates may also much faster unified treatment give exponential logistic loss functions able present prove convergence algorithms two losses simultaneously true algorithms presented paper well next describe analyze sequentialupdate algorithms two loss functions exponential loss algorithm equivalent adaboost algorithm freund schapire 13 viewing algorithm frame work able prove adaboost correctly converges minimum exponential loss function new result although kivinen warmuth 16 mason et al 19 given convergence proofs adaboost proofs depend assumptions given minimization problem may hold cases proof holds general without assumptions unified view leads instantly sequentialupdate algorithm logistic regression minor modification adaboost similar one proposed duffy helmbold 12 like adaboost algorithm used conjunction classification algorithm usually called weak learning algorithm accept distribution examples return weak hypothesis low error rate respect distribution ever new algorithm provably minimizes logistic loss rather arguably less natural exponential loss used adaboost another potentially important advantage new algorithm weights places examples bounded 0 1 suggests may possible use new algorithm setting boosting algorithm selects examples present weak learning algorithm filtering stream examples large dataset pointed watanabe 22 domingo watanabe 11 possible adaboost since weights may become extremely large provide modification adaboost purpose weights truncated 1 new algorithm may viable cleaner alternative next describe parameterized family iterative algorithms includes parallel sequentialupdate algorithms also interpolates smoothly two extremes convergence proof give holds entire family algorithms although paper considers binary case two possible labels associated example turns multiclass case requires additional work algorithms convergence proofs give binary case turn directly applicable multiclass case without modification comparisonwe also describe generalized iterative scaling algorithm darroch ratcliff 9 rederiving procedure setting able relax one main assumptions usually required algorithm paper organized follows section 2 describes boosting logistic regression models usually formulated section 3 gives background optimization using bregman distances section 4 describes boosting logistic regression cast within framework section 5 gives parallelupdate algorithms proofs convergence section 6 gives sequentialupdate algorithms convergence proofs parameterized family iterative algorithms described section 7 extension multiclass problems given section 8 section 9 contrast methods iterative scaling section 10 give preliminary experiments previous work variants sequentialupdate algorithms fit general family arcing algorithms presented breiman 4 3 well mason et als anyboost family algorithms 19 informationgeometric view take also shows algorithms study including adaboost fit family algorithms described 1967 bregman 2 satisfying set constraints work based directly general setting laf ferty della pietra della pietra 18 one attempts solve optimization problems based general bregman distances gave method deriving analyzing parallelupdate algorithms setting use auxilliary functions algorithms convergence proofs based method work builds several previous papers compared boosting approaches logistic regression fried man hastie tibshirani 14 first noted similarity boosting logistic regression loss functions derived sequentialupdate algorithm logitboost logistic loss however unlike algorithm requires weak learner solve leastsquares problems rather classification problems another sequentialupdate algorithm different related problem proposed cesabianchi krogh warmuth 5 duffy helmbold 12 gave conditions loss function gives boosting algorithm showed minimizing logistic loss lead boosting algorithm pac sense suggests algorithm problem close may turn also pac boosting property lafferty 17 went studying relationship logistic regression exponential loss use family bregman distances however setting described paper apparently cannot extended precisely include exponential loss use bregman distances describe important differences leading natural treatment exponential loss new view logistic regression work builds heavily kivinen warmuth 16 along lafferty first make connection adaboost information geometry showed update used adaboost form entropy projection however bregman distance used differed slightly one chosen normalized relative entropy rather unnormalized relative entropy adaboosts fit model quite complete particular convergence proof depended assumptions hold general kivinen warmuth also described updates general bregman distances including one examples bregman distance use capture logistic regression boosting logistic models loss functions set training examples instance x belongs domain instance space x label assume also given set realvalued functions following convention maxent literature call functions features boosting literature would called weak base hypotheses study problem approximating using linear combination features interested problem finding vector parameters 2 r n f good approximation measure goodness approximation varies task mind classification problems natural try match sign f attempt minimize true 0 otherwise although minimization number classification errors may worthwhile goal general form problem intractable see instance 15 therefore often advantageous instead minimize nonnegative loss function instance boosting algorithm adaboost 13 20 based exponential loss exp gammay verified eq 1 upper bounded eq 2 however latter loss much easier work demonstrated adaboost briefly series rounds adaboost uses oracle subroutine called weak learning algorithm pick one feature weak hypothesis h j associated parameter j updated noted breiman 3 4 various later authors steps done way approximately cause greatest decrease exponential loss paper show first time adaboost fact provably effective method finding parameters minimize exponential loss assuming weak learner always chooses best h j also give entirely new algorithm minimizing exponential loss round parameters updated parallel rather one time hope parallelupdate algorithm faster sequentialupdate algorithm see section 10 preliminary experiments regard instead using f classification rule might instead postulate generated stochastically function x attempt use f x estimate probability associated label natural wellstudied way pass f logistic function use estimate likelihood labels occuring sample gammay maximizing likelihood equivalent minimizing log loss model gammay generalized improved iterative scaling 9 10 popular parallelupdate methods minimizing loss paper give alternative parallelupdate algorithm compare iterative scaling techniques preliminary experiments section 10 section give background optimization using bregman distances form unifying basis study boosting logistic regression particular setup follow taken primarily lafferty della pietra della pietra 18 r continuously differentiable strictly convex function defined closed convex set bregman distance associated f defined instance bf unnormalized relative entropy du shown general every bregman distance nonnegative equal zero two arguments equal natural optimization problem associated bregman distance namely find vector closest given vector q 0 2 subject set linear constraints constraints specified theta n matrix vector vectors p satisfying constraints problem find arg min convex dual problem gives alternative formulation problem find vector particular form closest given vector p form vectors defined via legendre transform written simply v ffi q f clear context using calculus seen equivalent instance bf unnormalized relative entropy verified using calculus v eq 6 useful note given theta n matrix vector q 0 2 consider vectors obtained taking legendre transform linear combination columns vector q 0 vectors set dual optimization problem stated problem finding arg min q closure q remarkable fact two optimization problems solutions moreover solution turns unique point intersection p q take statement theorem lafferty della pietra della pietra 18 result appears due csisz ar 6 7 topsoe 21 proof case normalized relative entropy given della pietra della pietra lafferty 10 see also csisz ars survey article 8 theorem 1 let q assume bf 1 exists unique 2 bf delta 3 4 q moreover one four properties determines q uniquely theorem extremely useful proving convergence algorithms described show next section boosting logistic regression viewed optimization problems type given part 3 theorem prove optimality need show algorithms converge point p q regression revisited return boosting logistic regression problems outlined section 2 show cast form optimization problems outlined recall boosting goal find exp gammay minimized precisely minimum attained finite seek procedure finding sequence causes function converge infimum shorthand call exploss problem view problem form given section 3 0s 1s vectors follows finally take f eq 4 bf unnormalized relative entropy noted earlier case v ffi q given eq 7 particular means furthermore trivial see du du delta equal eq 10 thus minimizing du equivalent minimizing eq 10 theorem 1 equivalent finding satisfying constraints logistic regression reduced optimization problem form nearly way recall goal find sequence minimize shorthand call logloss problem define exactly exponential loss vector q 0 still constant defined 121 space restricted 0 1 minor differences however important difference choice function f namely resulting bregman distance trivially choice f verified using calculus v delta equal eq 13 minimizing db equivalent minimizing eq 13 finding q 2 q satisfying constraints eq 12 section describe new algorithm exploss logloss problems using iterative method weights j updated iteration algorithm shown fig 1 algorithm used function f satisfying certain conditions described particular see used choices f given section 4 thus really single algorithm used lossminimization problems setting parameters appropriately note without loss generality assume section instances algorithm simple iteration vector ffi computed shown added parameter vector assume algorithms inputs infinitevalued updates never occur algorithm new minimization problems optimization methods exploss notably adaboost parameters assumptions 1 2 bf lim ffl ffl update parameters figure 1 parallelupdate optimization algorithm generally involved updates one feature time parallel update methods logloss well known see exam ple 9 10 however updates take different form usual updates derived logistic models useful point distribution q t1 simple function previous distribution q eq 8 gives exploss logloss respectively prove next algorithm given fig 1 converges optimality either loss prove abstractly matrix vector q 0 function f satisfying following assumptions assumption 1 v 2 r q 2 assumption 2 c 1 set bounded show later choices f given section 4 satisfy assumptions allow us prove convergence exploss logloss prove convergencewe use auxiliaryfunction technique della pietra della pietra lafferty 10 roughly idea proof derive nonnegative lower bound called auxiliary function much loss decreases iteration since loss never increases lower bounded zero auxiliary function must converge zero final step show auxiliary function zero constraints defining set p must satisfied therefore theorem 1 must converged optimality formally define auxiliary function sequence continuous function satisfying two conditions proving convergence specific algorithms prove following lemma shows roughly sequence auxiliary function sequence converges optimum point q thus proving convergence specific algorithm reduces simply finding auxiliary function auxiliary function matrix assume q lie compact subspace q q eq 9 particular case assumption 2 holds bf lim proof condition 18 bf delta nonincreasing sequence bounded zero therefore sequence differences bf must converge zero condition 18 means aq must also converge zero assume q lie compact space sequence q must subsequence converging point continuity p eq 5 hand q limit sequence points q theorem 1 argument uniqueness q show single limit point q suppose entire sequence converge q could find open set b containing q fq 1 contains infinitely many points therefore limit point must closed set must different q already argued impossible therefore entire sequence converges q apply lemma prove convergence algorithm fig 1 theorem 3 let f satisfy assumptions 1 2 assume bf 1 let sequences generated algorithm fig 1 lim q eq 9 lim proof let w claim function auxiliary function clearly continuous nonpositive upper bound change delta round aq follows eqs 20 21 follow eq 16 assumption 1 respectively eq 22 uses fact x j jensens inequality applied convex function e x eq 23 uses definitions w tj w gamma tj eq 24 uses choice ffi indeed ffi chosen specifically minimize eq 23 j q thus auxiliary function theorem follows immediately lemma 2 apply theorem exploss logloss prob lems need verify assumptions 1 2 satisfied exploss assumption 1 holds equality logloss first second equalities use eqs 14 15 respec tively final inequality uses 1 assumption 2 holds trivially logloss since bounded exploss du clearly defines bounded subset r 6 sequential algorithms section describe another algorithm minimization problems described section 4 however unlike algorithm section 5 one present updates weight one feature time parallelupdate algorithm may give faster convergence many features sequentialupdate algorithm used large number features using oracle selecting feature update next instance adaboost essentially equivalent sequentialupdate algorithm exploss uses assumed weak learning algorithm select weak hypothesis ie one features sequential algorithm present logloss used exactly way algorithm shown fig 2 theorem 4 given assumptions theorem 3 algorithm fig 2 converges optimality sense theorem 3 proof theorem use auxiliary function parameters fig 1 output fig 1 ae ff else ffl update parameters figure 2 sequentialupdate optimization algorithm function clearly continuous nonpositive eq 27 uses convexity e gammaff x eq 29 uses choice ff chose ff minimize bound eq 28 thus auxiliary function theorem follows immediately lemma 2 mentioned algorithm essentially equivalent adaboost specifically version adaboost first presented freund schapire 13 adaboost iteration distribution training examples computed weak learner seeks weak hypothesis low error respect distribution algorithm presented section assumes space weak hypotheses consists features h learner always succeeds selecting feature lowest error accurately error farthest 12 translating notation weight assigned example adaboost exactly equal q ti z weighted error tth weak hypothesis equal theorem 4 first proof adaboost always converges minimum exponential loss assuming idealized weak learner form note theorem also tells us exact form lim however know limiting behavior q know limiting behavior parameters whether q also presented section new algorithm logistic regression fact algorithm one given duffy helmbold 12 except choice ff practical terms little work would required alter existing learning system based adaboost uses logistic loss rather exponential lossthe difference manner q computed even systems based confidencerated boosting 20 ff j chosen together round minimize eq 26 rather approximation expression used algorithm fig 2 note proof theorem 4 easily modified prove convergence algorithm using auxiliary 7 parameterized family iterative algorithms previous sections described separate parallel sequentialupdate algorithms section describe parameterized family algorithms includes parallel update algorithm section 5 well sequentialupdate algorithm different one section 6 family algorithms also includes algorithms may appropriate either certain situations explain algorithm shown fig 3 similar parallelupdate algorithm fig 1 round quantities tj w gamma tj computed vector computed ffi computed fig 1 however vector added directly instead another vector selected provides scaling features vector chosen maximize measure progress restricted belong set allowed form scaling vectors given set parameter algorithm restriction vectors satisfying constraint parallelupdate algorithm fig 1 obtained choosing assuming equivalently make assumption choose parameters fig 1 r n mthetan satisfying condition output fig 1 ffl tj 2 ffl j ffl update parameters figure 3 parameterized family iterative optimization algorithms obtain sequentialupdate algorithm choosing set unit vectors ie one component equal 1 others equal 0 assuming ij 2 j update becomes ae tj else another interesting case assume natural choose ensures maximization solved analytically giving update idea generalizes easily case dual norms p q final case restrict scaling vectors ie choose r case maximization problem must solved choose linear programming problem n variables constraints prove convergence entire family algorithms theorem 5 given assumptions theorem 3 algorithm fig 3 converges optimality sense theorem 3 proof use auxiliary function j theorem 3 function continuous nonpositive bound change using technique given theorem 3 tj tj tj finally j 0 since every j exists 2 j 0 implies applying lemma 2 completes theorem section show results extended multiclass case generality preceding results see new algorithms need devised new convergence proofs need proved case rather preceding algorithms proofs directly applied multiclass case multiclass case label set cardinality k feature form h logistic regression use model e f xy 6y e f xgammaf xy loss training set e f transform framework follows let fy gg vectors p q etc work r 1mdimensional indexed pairs b let convex function f use case defined space resulting bregman distance 6y clearly shown v assumption 1 verified noting i2b let 1k1 plugging definitions gives delta equal eq 31 thus algorithms sections 5 6 7 used solve minimization problem corresponding convergence proofs also directly applicable several multiclass versions adaboost ada boostm2 13 special case adaboostmr 20 based loss function i2b exp loss use similar set except choice f instead use i2b fact actually f used binary adaboost merely changed index set b thus i2b v choosing multiclass logistic regression bf delta equal loss eq 33 thus use preceding algorithms solve multiclass problem well particular sequentialupdate algorithm gives adaboostm2 adaboostmh 20 another multiclass version ada boost adaboostmh replace b index set example label 2 define ae loss function adaboostmh exp let use f binary adaboost q obtain multiclass version adaboost 9 comparison iterative section describe generalized iterative scaling gis procedure darroch ratcliff 9 comparison algorithms largely follow description gis given berger della pietra della pietra 1 multiclass case make comparison stark possible present gis notation prove convergence using methods developed previous sections also able relax one key assumptions traditionally used studying gis adopt notation setup used multiclass logistic regression section 8 knowledge analog gis exponential loss consider case logistic loss also extend notation defining q iy q defined moreover verified q defined eq 30 gis following assumptions regarding features usually made section prove gis converges second condition replaced milder one namely since multiclass case constant added features h j without changing model loss function since features scaled constant two assumptions consider clearly made hold without loss generality improved iterative scaling algorithm della pietra della pietra lafferty 10 also requires milder assumptions much complicated implement requiring numerical search newton raphson feature iteration gis works much like parallelupdate algorithm section 5 f q 0 defined multiclass logistic regression section 8 difference computation vector updates ffi gis requires direct access features h j specifically gis ffi defined clearly updates quite different updates described paper using notation sections 5 8 reformulate framework follows theta h j i2b prove convergence updates using usual auxiliary function method theorem 6 let f q 0 modified gis algorithm described converges optimality sense theorem 3 proof show auxilliary function vectors q computed gis clearly continuous usual nonnegativity properties unnormalized relative entropy imply eq 35 h w thus implies constraints q proof theorem 3 remains shown introduce notation rewrite gain follows using eq 32 plugging definitions first term eq 38 written next derive upper boundon second term eq 38 09training loss seq seq2 par 403050709training loss seq seq2 par figure 4 training logistic loss data generated noisy hyperplane many left right relevant features eq 40 follows log bound ln x x gamma 1 eq 42 uses eq 25 assumption form h j eq 43 follows definition update ffi finally combining eqs 36 38 39 44 gives eq 37 completing proof clear differences gis updates given paper stem eq 38 derived ith term sum choice c effectively means log bound taken different point ln 1 general case bound exact varying c varies bound taken thereby varies updates section briefly describe experiments using synthetic data experiments preliminary intended suggest possibility algo rithms practical value systematic experiments clearly needed using realworld synthetic data comparing new algorithms commonly used procedures first tested effective methods minimizing logistic loss training data first ex periment generated data using noisy hyperplane specifically first generated random hyperplane 100dimensional space represented vector w 2 r 100 chosen uniformly random unit sphere chose 300 points x 2 r 100 point normally distributed x n0 next assigned label point depending whether fell chosen hyperplane ie label chosen perturbed point x adding random amount n0 08 effect causing labels points near separating hyperplane noisy points farther features identified coordinates x ran parallel sequentialupdate algorithms sections 5 6 denoted par seq figures data also ran sequentialupdate algorithm special case parameterized family described section 7 denoted seq2 finally ran iterative scaling algorithm described section 9 results experiment shown left fig 4 shows plot logistic loss training set four methods function number iterations loss normalized 1 methods well comparison iterative scaling parallelupdate method clearly best followed closely second sequentialupdate algorithm parallelupdate method much times faster terms number iterations iterative scaling right fig 4 shown results similar experiment four components w forced zero words four relevant variables features experiment sequentialupdate algorithms perform kind feature selection initially significant advantage test error log seq exp seq log par exp par figure 5 test misclassification error data generated noisy hyperplane boolean features parallelupdate algorithm eventually overtaken last experiment tested effective new competitors adaboost minimizing test misclassification error experiment chose separating hyperplane w first experiment however chose 1000 points x uniformly random boolean hypercube fgamma1 1g 100 labels computed labels chosen flipped coordinate point x independently probability 005 noise model effect causing examples near decision surface noisier far experiment used parallel sequential update algorithms sections 5 6 denoted par seq cases used variants based exponential loss exp logistic loss log case sequentialupdate algorithms sections 6 7 identi cal fig 5 shows plot classification error separate test set 5000 examples large difference performance exponential logistic variants algorithms however parallelupdate variants start much better although eventually methods converge roughly performance level acknowledgments many thanks manfred warmuth first teaching us bregman distances many comments earlier draft thanks also nigel duffy david helmbold raj iyer helpful discussions suggestions research done yoram singer att labs r della pietra relaxation method finding common point convex sets application solution problems convex programming arcing edge prediction games arcing classifiers generalized iterative scaling loglinear models inducing features random fields scaling boosting based learner via adaptive sampling potential boosters decisiontheoretic generalization online learning application boosting additive logistic regression statistical view boosting boosting entropy projection additive models statistical learning algorithms based bregman dis tances functional gradient techniques combining hypothe ses improved boosting algorithms using confidencerated predictions information theoretical optimization techniques computational learning theory discovery science tr ctr stefan riezler new developments parsing technology computational linguistics v32 n3 p439442 september 2006 nir krause yoram singer leveraging margin carefully proceedings twentyfirst international conference machine learning p63 july 0408 2004 banff alberta canada hoiem alexei efros martial hebert automatic photo popup acm transactions graphics tog v24 n3 july 2005 zhihua zhang james kwok dityan yeung surrogate maximizationminimization algorithms adaboost logistic regression model proceedings twentyfirst international conference machine learning p117 july 0408 2004 banff alberta canada cynthia rudin ingrid daubechies robert e schapire dynamics adaboost cyclic behavior convergence margins journal machine learning research 5 p15571595 1212004 steven j phillips miroslav dudk robert e schapire maximum entropy approach species distribution modeling proceedings twentyfirst international conference machine learning p83 july 0408 2004 banff alberta canada amir globerson terry koo xavier carreras michael collins exponentiated gradient algorithms loglinear structured prediction proceedings 24th international conference machine learning p305312 june 2024 2007 corvalis oregon zhihua zhang james kwok dityan yeung surrogate maximizationminimization algorithms extensions machine learning v69 n1 p133 october 2007 taneli mielikinen evimaria terzi panayiotis tsaparas aggregating time partitions proceedings 12th acm sigkdd international conference knowledge discovery data mining august 2023 2006 philadelphia pa usa joshua goodman sequential conditional generalized iterative scaling proceedings 40th annual meeting association computational linguistics july 0712 2002 philadelphia pennsylvania stefano merler bruno caprile cesare furlanello parallelizing adaboost weights dynamics computational statistics data analysis v51 n5 p24872498 february 2007 gokhan tur extending boosting large scale spoken language understanding machine learning v69 n1 p5574 october 2007 hoiem alexei efros martial hebert recovering surface layout image international journal computer vision v75 n1 p151172 october 2007 w john wilbur lana yeganova kim synergy pav adaboost machine learning v61 n13 p71103 november 2005 heinz h bauschke duality bregman projections onto translated cones affine subspaces journal approximation theory v121 n1 p112 march joseph turian dan melamed advances discriminative parsing proceedings 21st international conference computational linguistics 44th annual meeting acl p873880 july 1718 2006 sydney australia michael collins parameter estimation statistical parsing models theory practice distributionfree methods new developments parsing technology kluwer academic publishers norwell 2004 michael collins terry koo discriminative reranking natural language parsing computational linguistics v31 n1 p2570 march 2005 ron meir gunnar rtsch introduction boosting leveraging advanced lectures machine learning springerverlag new york inc new york ny