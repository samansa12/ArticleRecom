synergetic effect compiler architecture manual optimizations performance cfd multiprocessors paper discusses comprehensive performance profiling improvement benchmarking computational fluid dynamics code one grand challenge applications three popular multiprocessors process analyzing performance considered language compiler architecture algorithmic changes quantified incremental contribution bottomline performance demonstrate parallelization alone cannot result significant gains granularity parallel threads effect parallelization data locality taken account unlike benchmarking studies often focus performance effectiveness parallelizing compilers specific loop kernels used entire cfd code measure global effectiveness compilers parallel architectures probed performance bottlenecks case derived solutions eliminate neutralize performance inhibiting factors major conclusion work overall performance extremely sensitive synergetic effects compiler optimizations algorithmic code tuning architectural idiosyncrasies b introduction despite continuing quest achieve high performance complete scientific applications obtain fraction expected speedup modern multiprocessors hand proliferation parallel architectures throughout spectrum computer systems indisputable would argue significance parallelism even fewer would suggest parallel machines deliver performance users come expect paper means grand challenge application show compiler architectural limi tations algorithmic characteristics restrict performance severely amdahls law suggests even cases parallelism may abundant results indicate synergetic approach simultaneously addressing limitations result significant performance payback work focused comprehensive multilevel analysis complete commercial scientific application computational fluid dynamics cfd code based simple semiimplicit method pressurelinked equation 8 paper reports performance bottlenecks hardware architectures compilers operating systems importance manual optimizations code tuning hand optimizations used work improve performance well isolation hardware bottlenecks point possible architectural software improvements alleviate performance limitations multiprocessor machines experiments used three commercial multiprocessors sgi challenge alliant fx2800 alliant fx80 used two commercial two experimental parallelizing compilers results suggest user cfd application expert code optimization andor computer architecture expect see little performance improvement running code multiprocessor true even presence powerful automatic parallelizing compilers tools effective expert intervention even though results single commercial application cannot generalized across spectrum scientific applications size type codes make cfd programs representative converge time iteration24solve continuous equation solve nseqz component solve nseqy component solve nseqx component boundary condition boundary condition boundary condition boundary condition set time input data output result coefv coefv coefv corect coefp solve system set coefficient system set coefficient system set coefficient system set coefficient system solve system solve system solve system add p p6 figure 1 algorithm cfd application simple linear system solvers pde solvers core majority numerical applications many performance evaluation benchmarking studies employ selected kernels complete applications may amenable automatic parallelization parallel execution resisted approach work considered performance entire application profiling performance components cfd code weighted contribution overall speedup result hand optimizations targeted compilation code tuning avoid architectural bottlenecks significant improvement overall performance findings also underscore need predict maximum possible theoretical performance gains given application upper bounds performance could useful reference points automatic manual parallelization comes surprise runtime overhead plays important role performance serial bottlenecks amdahls law 2 cfd code experimental environment inflow1ms x z density 0001kgm 5 figure 2 model cube divided 40 meshes direction cfd program based simple method 8 outlined figure 1 algorithm first increments time step step 1 figure 1 iteratively solves navierstokes equations compute velocity steps 24 continuity equations step 5 converge steps 2 5 figure 1 consists three modules assemble system linear equations discretization ii modify equations according boundary conditions iii solve system linear equations employ miccg modified incomplete cholesky conjugate gradient 5 solve continuity equation module solvep ilucr incomplete lu conjugate residual 6 solve navierstokes equations module solvev performance evaluated using 3dimensional space shown figure 2 experiments measured entire cfd code except input output data identified steps 0 6 figure 1 performance resulting codes measured compared three multiprocessor systems alliant fx2800 alliant fx80 sgi challenge alliant fx80 8processor mimd system processor vector unit processors share 4quadrant cache 2 alliant fx2800 next generation mimd machines alliant 20processor machine using intel i860 processor finally sgi system used experiments sgi challenge configurations 12 8 processors machine used measurements specified explicitly experimental sections execution times paper measured dedicated mode three machines measurement repeated several times average taken cases standard deviation equivalent 1 experiments repeated thus measurements reported standard deviation almost zero serial parallel versions cfd code used measurements machine use terms serial parallel refer serial parallel execution times respectively serial parallel versions cfd compiled using two experimental parallelizing compilers manufacturersupplied parallelizers machine explained serial serial version cfd code used measure serial execution time compiled using native nonparallelizing compiler optimizations option turned parallel case parallel execution time computed running parallel version cfd obtained follows original code compiled systems native parallelizing compiler alliant used alliant compiler automatic parallelizationvectorization options 1 sgi challenge used sgi parallel fortran accelerator pfa 10 parafrase2 polaris original code compiled experimental sourcetosource parallelizing compiler parafrase2 7 polaris 9 parallelized source output two parallelizers compiled backend fortran compiler machine sgi alliant backend compilers parallelize loops explicitly indicated following sections incremental effects compilers system architecture manual optimizations limitations overall performance discussed 3 effect compiler parallelization optimization section discuss effectiveness automatic parallelization cfd code moreover consider specific limitations compilers tested identify several areas potential improvement allt coefv coefp corect normalized execution serial parallel ppolaris alliant fx280016161616135789 90serial parallel ppolaris alliant fx8016161616 serial parallel ppolaris figure 3 performance automatic parallelizing executed 8 processors parallel indicates execution time running application using computers native compilers terms functionality performance four compilers tested cfd code machine figure 3 shows normalized execution time cfd code machine using four compilers took serial execution time defined normalized time native parallelizer system used compute time labeled parallel figure 3 different shades bar correspond execution time modules cfd code illustrated figure 1 first clear observation none four compilers improved performance significantly best case speedup automatically parallelized cfd 119 dismal performance circumstances course could due serial nature underlying algorithm however shown later case cfd code exposes significant amount parallelism none parallelizing compilers able exploit due number reasons figure 3 evident four compilers performed uniformly poorly hardly significant difference observed closer observation output codes revealed parafrase2 polaris local variables else endif b statements f reduction figure 4 code samples illustrating compiler bottlenecks fact parallelize significant number loops however much exposed parallelism expensive exploit three machines although experimental compilers powerful pfa alliant compilers tend extract maximal parallelism regardless impact performance result parallel execution small loops resulted execution time worse serial time clearly points need compiletime determination exploitable nonexploitable paral lelism existing compilers provide even rudimentary support determining quality exposed parallelism group four compilers pfa provides capability computing tradeoff runtime overhead amount parallelism although cfd loops parallelized experimental compilers performance sgi challenge noticeably worse two alliants due mostly architectural bottlenecks sgi challenge whose effect profound alliants examine architectural bottlenecks later sections order better understand characterize poor performance compilers used cfd code isolated largest loops characterized serial parallel undetermined serial undetermined versions examined loops hand determine cause serialization partial parallelization following discuss limitations compiler identify highyield optimizations significantly improve performance table characterizes parafrase2 polaris terms important optimizations gives percentage improvement serial execution time 1 cfd 8processor sgi challenge time normalized serial execution time taken 100 observe significant payoff due local variable recognition allocation loops following comparison limited analysis sgi considering two experimental compilers sgis pfa local variables loops present parafrase2 cannot parallelize loops localizable variables one shown figure 4a unless invoked scalar expansion pass however scalar expansion pass powerful enough handle many loops code polaris effective identifying local variables resulted parallelization simple loops privatarizing localizable variables b statements loops parafrase2 much effective parallelization loops whose body contains statements loop figure 4b polaris however cannot parallelize loops since large loops local variables branches neither parafrase2 polaris could parallelize loops branches local variables although experimental compilers deal effectively either local variable recognition branches inside loops neither could handle combination local variables control flow statements inside loop body result applied optimizations manually original code code involved index expressions order two missed compilers pfa alliant compilers even weaker respect dependence analysis powerful dependence analysis schemes necessary order improve effectiveness automatic parallelization previously false table 1 characteristics parafrase2 polaris bottleneck parafrase2 polaris impact local variable loop weak yes b statement loop yes c index expression higher order e distributing strongly connected components reduction optimization 52 dependences due higher order index expressions manually removed loop distribution although parafrase2 effective distributing loops applies loop distribution possible cases resulting large number relatively small loops even though may desirable generating vector code may counter productive case parallel loops ideally distribute loop enables parallelization loop figure 4d typical case reason distribute pass used experiments handoptimized version distributed loops manually helpful distributing strongly connected components loop shown figure 4e cannot parallelized polaris parafrase2 however two statements body loop distributed find distributed loops parallelizable currently way determine legality distribution automatically compiler would able execute loop symbolically order determine legality distribution thus automatically parallelize loop typical reduction shown figure 4f loops reductions cannot parallelized parafrase2 polaris parallelized back end compiler handle reduction reduction operation parallelized breaking loop several parts summing part finally calculating total sum 1 1 parallel version code adds elements together different order sequential version roundoff errors accumulate differently two versions code thus answer may differ slightly impact optimization shown table 1 expressed improvement 1 related optimizations thus aggregate improvement applying optimizations necessarily equal sum individual improvements example first row table shows 30 improvement execution time specifying local variables parallel loops include loops branches contribute improvement shown second row table incorporating optimizations discussed section performance cfd code improved 52 achieving speedup 21 automatic parallelization proved ineffective case cfd code especially sgi challenge incorporating improving particular transformations may result significant payoff implementationimprovement transformations would necessary achieve improvement suggested table 1 perhaps one least looked issue parallelizing compilers useful versus useless parallelism equivalently coarse fine grain parallelism compilers need ability quantitatively analyze sections code determine tradeoff payoff cost particular trans formation symbolic program analysis 3 provides powerful means computing symbolic size code sections however exception parafrase2 compiler provides capability 4 architectural bottlenecks compiler transformations manually incorporated code performance parallelized version compared sgi challenge alliant machines impact architectural bottlenecks profound sgi multiprocessor shown section 6 alliants mostly due cost enforcing cache coherence nonexisting problem alliants use shared cache limited bus bandwidth therefore experiments solutions architectural bottlenecks focused sgi challenge 41 runtime overhead table 2 cost scheduling synchronization operations sgi operation initialization subsequent use createuse thread 446741 20 barrier entry 103 63 barrier exit 17 7 sec order put following experimental results perspective first measured overhead associated primitive operations bookkeeping functions sgi challenge cost issuing parallel loop iteration barrier synchronization setting threads used parallel loop execution used sgisupplied timers make detailed performance measurements table 2 shows cost creating thread cost reusing existing threads loop iteration assigned empty thread cost 20 secs cost creating threads extremely high indicated table 2 approaches 05sec however cost paid per thread threads recycled subsequent parallel loops nevertheless accumulated runtime overhead thread creation thread assignment loop iterations high detrimental performance applications small number loops andor loops small loop bodies barrier synchronization overhead measured pair numbers barrier entry exit former computed time difference completion last first processor includes skew due load balancing latter measured difference time last iteration completed execution time loop exited figure 5 shows spacetime diagram execution parallel loop 8 iterations large loop body figure shows startup phase existing threads completion phase processor assigned one iteration parallel loop top horizontal bar indicates time execution entered loop time loop completed execution although right part figure 5 shows noticeable unbalance processors completed within interval 57 total execution time loop although sgi supports gang scheduling parallel loops processors start loop simultaneously observe noticeable difference among processors starting execution difference due l iteration number time sec 1000 1010 iteration number time sec figure 5 diagram 8 processors cdoacross localj i18 j12000000 program emulate false sharing cdoacross localj r i18 j12000000 air b eliminate local variable dimension ad8 j12000000 c eliminate keeping arrays separate figure sharing synchronization overhead associated exclusive access loop index case 8 processors total approximately 10secs spent locking updating loop index 1sec lock operation approximate terms total overhead associated processor participating execution parallel loop lower bound 60secs equivalently approximately 10000 clock cycles therefore parallel execution starts paying loops whose body contains 10000 instructions 42 false sharing false sharing occurs multiple processors access cache small section memory although memory accesses may truly independent may treated accesses shared data figure 6a shows loop gives rise false sharing like many complicated loops cfd code false l timesec iteration number time stamps figure 6a procs local time sec b execution time figure 6b c figure 7 performance sharing false sharing enforced cache line level byte word level thus processor writes byte cache line entire line invalidated modified byte discuss two techniques avoid false sharing measure effect performance sample codes first solution false sharing use local variables wherever possible shown figure 6b loop manually rewritten order make explicit use local variables execution time loop figure 6b varying number processors shown second column figure 7b header local second approach spreading allocation shared data memory separated addresses differ cache line size improvement illustrated code example 6c last three columns figure 7b show execution time loop figure 6c three different allocations shared array distance number array elements variables accessed successive loop iterations notice worst performance observed d1 corresponds maximum amount false sharing since cache line size sgi challenge 128 bytes eliminate false sharing keeping elements threads numbers shown italics display poor performance shared array spread wide enough avoid false sharing worth noting execution times local variable version better shared array version due effect local variables whose values written back memory suspect registers effectively used local variables allowed supported write back memory dimension anmax8 store data local cache c cdoacross localjji sharea 100 j18 110 i1nmax2 execution time loop figure 8 program emulate bus contention iadd switching parameter iadd0 bus contention iadd1 bus contention values thus reducing performance implicitly suggested 10 although avoid false sharing either method using local variables appears effective possibly due opportunity extensive register reuse none compilers tested able handle effectively problem false sharing 43 bus contention clearly locality data important cache performance also minimizing memory accesses network traffic busbased multiprocessors sgi challenge bus contention serious performance bottleneck modules cfd code performed poorly due combination false sharing bus contention bus contention measured comparing performance two almost identical loop kernels figure 8 first version loop needs data local cache iadd0 iadd1 loop accesses data present local caches processors request significant amount data shared memory remote caches anticipate bus contention varies number processors attempting data transfers read local cache read also caches1 proc time msec 16kb read local cache read also caches 1 proc time msec b 4mb figure 9 influence bus contention performance simultaneously measured performance effects bus contention changing size array number processors used execute kernel figure 8 size array nmax taken 4096 1048576 elements equivalently 16kb 4mb respectively execution time parallel loop shown figure 9 array size 16kb worst case scenario 30 increase execution time 8 processors cache misses account difference array size 4mb execution time 900 longer compared case caching entire array local caches performance difference cannot attributed cache misses alone false sharing play role case since data skewed appropriately shared memory order avoid false sharing thus bulk slow due bus contention although attempted separate performance loss due cache misses due bus contention possible measure events level clock period software timers alliants avoid network contention due false sharing coherence general since neither applicable performance alliants limited raw bus bandwidth saturated large arrays accessed cached simultaneously different processors x z b line executed parallel allow executed serially parallel loop executed serially c plane executed parallel figure 10 computing order original serial code b 3dimensional hyperplane method c 2dimensional hyperplane projection method 5 algorithmic changes section discuss extensive hand recoding cfd code bordered algorithmic changes although current compiler technology insufficient performing type code restructuring future parallelizing compilers could potentially perform restructuring computation language level order improve degree quality exploitable parallelism order improve performance timeconsuming modules cfd code changed order calculations performed without violating algorithmic dependences addition exploited nonloop functional parallelism computation velocities latter involved change algorithm simple among seven modules shown figures 1 3 solvep solvev coefv timeconsuming modules coefv extensively parallelized compiler optimizations discussed section 3 structure calculations solvep solvev similar thus limit discussion solvep similar approach used solvev details handrestructuring code given 4 paper outline changes highest possible level computations solvep sweep 3dimensional structure shown figure 10a parallelization difficult case due dependences point computed depends three immediate neighbors x z dimensions figure 10b shows one alter order computations respect converge time iteration set time input data output result solve continuous equation solve nseq solve nseq z component solve nseq component figure parallelized simple algorithm velocity certain subcomputations step 5 distributed steps 24 three dimensions without changing underlying algorithm order shown figure 10b yields maximum amount parallelism since elements within crossdiagonal plane computed parallel refer 3dimensional hyperplane method however planes close beginning end cube contain elements translates parallel loops iterations provide little opportunity exploiting cache locality sensitive runtime overhead figure 10c shows alternative order calculations solvep organizing computation across diagonal planes adopted optimized cfd code refer approach 2dimensional hyperplane projection method although parallelism less case 10b results parallel loops longer bodies sweep across elements plane direction arrows thus parallel loop iteration perform computations elements local vector locality data profound effect performance case addition larger granularity parallel loop iterations contributed amortization overhead result restructuring based figure 10c outperformed one based 10b speedup achieved solvep versions figure 10b c 8 processor sgi challenge 17 74 respectively final manual change code targeted exploitation nonloop parallelism figure 11 shows alternative order performing computations within iteration algorithm illustrated figure 11 code restructured order take advantage simultaneous computation across x table 3 performance improvements parallelizing components velocity part serial parallel speedup 1000 625 16 24 23 10 coefv coefp corect normalized execution serial parallel ppolaris mcom mcomalg mall alliant fx2800161616161 serial parallel ppolaris mcom mcomalg mall alliant fx80161616161 100 969841serial parallel ppolaris mcom mcomalg mall sgi challenge16040471611 figure 12 performance 8 processors parallel refers parallelized native compiler z components model done structuring calculations across dimension shown figure 11 addition highlevel parallelism resulting simultaneous execution three solvers parallelism loop level still exploited within component approach expected yield better performance due significantly higher degree data locality table 3 shows speedup new cfd code nonloop parallelism exploited unable exploit functional loop parallelism sgi challenge due fact system allows one level parallelism nested parallelism supported pfa compiler runtime library 6 performance analysis figure 12 gives comprehensive account performance cfd code three multiprocessors set automatic manual optimizations incorporated three groups execution time bars one three multiprocessors used experiments alliant fx2800 fx80 sgi challenge respectively shadeskey right handside figure 12 shows correspondence various shades bar major modules cfd code defined figure 1 case seven performance bars shown following data left right leftmost bar group corresponds normalized serial execution time cfd code machine serial optimizations turned execution times normalized respect serial execution time next three bars labeled parallel p 2 polaris correspond parallel execution time code machine code compiled manufacturersupplied parallelizer parafrase2 polaris respectively identical timings shown figure 3 illustrated figure 3 automatic parallelization failed major way worth noting compilers effective parallelizing specific loops however combination bias toward specific transformations lack quantitative analysis resulted bottomend improvement example loop parallelized misses cache iteration hardly benefit parallel execution bars labeled mcom correspond parallel execution time code manual compiler optimizations parallelization techniques described section 3 carried reflect optimizations restructuring techniques automatable integrated existing parallelizing compilers notice significant improvement manual parallelization three machines resulting speedup 2 3 bars labels mcomalg show resulting execution times compiler optimizations algorithmic code changes discussed section 5 incorporated case alliants algorithmic changes resulted yet significant improvements corresponding additional speedups approximately 3 however algorithmic changes increased parallelism eg 3dimensional hyperplane method section 5 resulted little improvement sgi challenge case latter case performance topped due lack additional number processors speed figure 13 scalability cfd code parallelism due cache misses saturation bus bandwidth interference coherence overhead bus saturation challenge become evident rightmost bar figure 12 bars labeled mall show parallel execution times cfd code addition automatic manual optimizations algorithmic changes code altered eliminate interference architectural bottlenecks false sharing bus bandwidth fact 2 dimensional hyperplane projection method outlined section 5 restricted amount parallelism promoted data locality eliminated false sharing mall bar sgi data figure 12 reflects improvements achieved due elimination bottlenecks obtained 2dimensional hyperplane projection major improvement came case sgi additional speedup almost three resulting elimination false sharing increased data locality reduction bus traffic optimizations discussed section 4 cfd code achieved total speedup approximately 7 three systems figure 13 shows speedup cfd benchmark sgi three different problem sizes cases computing speedup took serial execution time execution time parallelized code single processor optimized sequential code thus speedups reported fall conservative side least sgi fx2800 2 figure 13 attests scalability cfd code problem size 2 sequential execution times alliant fx80 obtained running parallel version code single processor without vectorization vectorization used parallel runs discussed laborious timeconsuming analysis optimization cfd code resulted total speedup approximately 7 however result tell us anything regarding maximum potential parallelism maximum performance attainable cfd benchmark machines tested order determine position delivered speedup respect ideal used two ap proaches first used amdahls law compute maximum speedup measuring serial parallel parts code actual execution sgi measured execution time cfd several times different numbers processors using least square approximation estimated parallel serial fractions code used estimate upper bound performance using amdahls law table 4 shows maximum attainable speedup estimated measurements sgi challenge three problem sizes indicated speedup 156 corresponds problem size used obtain measurements reported figure 12 delivered speedup approximately half amdahls upper bound maximum speedup believe factor two difference attributed network bandwidth interference cache misses addition estimated maximum speedup using properties underlying algorithms handcarried analysis code amdahls law used obtain upper bound however serial parallel fractions code estimated inspection code static analysis approach yielded maximum speedup 40 100 3dimensional 2dimensional hyperplane projection methods respectively problem size fixed 40x40x40 factor 6 ideal maximum measured maximum speedup discrepancy attributed architectural bottlenecks thus powerful parallelizerbackend combination multiprocessor architecture without bottlenecks would expected deliver speedups 40100 cfd application factor almost highly optimized cfd version optimized version running bottleneckfree architecture infinite number processors table 4 scalability maximum speedup elements maximum speedup 7 conclusion paper presented comprehensive performance profiling analysis optimization tuning commercial computational fluid dynamics code cfd one important applications running highperformance computers one demanding terms computational resources findings underscore severe limitations commercial experimental parallelizing compilers well architectural bottlenecks performance implications popular highperformance multiprocessors sgi challenge importantly findings stress importance synergetic effect compiler algorithmic optimizations overall performance prowess compilers individual transformations optimizations little effect bottomline performance however global approach optimization considers interdependencies among various optimizations result significant performance improvements experiments based performance complete cfd code opposed selected kernels although individual compilers performed excellently specific loops performance entire application proved dismal true delivered performance performance cfd code sgi challenge striking case mismatch compiler optimizations architectural idiosyncrasies parallelization may result far less expected delivered performance attempt customize parallelization underlying architecture alternatively order parallelism work effectively architectural bottlenecks eliminated taken consideration compilation finally sophisticated program restructuring capture alter order type computations result certain cases improvements similar parallelization code optimization r alliant computer systems corporation alliant computer systems corporation symbolic analysis parallelizing compilers parallelization cfd code guidelines usage incomplete decompositions solving sets linear equations occur practical problems new generation parallelizing compiler mpps numerical heat transfer fluid flow silicon graphics inc tr symbolic analysis parallelizing compilers