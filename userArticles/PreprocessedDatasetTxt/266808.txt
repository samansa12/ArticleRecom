design performance conflictavoiding cache high performance architectures depend heavily efficient multilevel memory hierarchies minimize cost accessing data dependence increase expected increases relative distance main memory number published proposals cache conflictavoidance schemes investigate design performance conflictavoiding cache architectures based polynomial modulus functions earlier research shown highly effective reducing conflict miss ratios examine number practical implementation issues present experimental evidence support claim pseudorandomly indexed caches effective performance terms practical implementation viewpoint b introduction current projections next 10 years could see cpu clock frequencies increase factor twenty whereas dram rowaddressstrobe delays projected decrease factor two potential tenfold increase distance main memory serious implications design future cachebased memory hierarchies well architecture memory devices many options architect consider battle memory latency partitioned two broad categories latency reduction latency hiding latency departament darquitectura de computadors universitat politcnica de catalunya c jordi girona 13 08034 barcelona spain emailantonio josegacupces department computer science university edinburgh jcmb kings buildings edinburgh uk reduction techniques rely caches exploit locality objective reducing latency individual memory reference latency hiding techniques exploit parallelism overlap memory latency operations thus hide programs critical path paper addresses issue latency reduction degree future cache architectures isolate processor increasing memory latency discuss theory evaluate practice using particular class conflictavoidance indexing functions demonstrate cache could constructed provide practical solutions previously unreported problems well known problems associated unconventional indexing schemes section 2 present overview causes conflict misses summarise previous techniques proposed minimize effect performance propose method cache indexing demonstrably lower miss ratios alternative schemes summarise known characteristics method section 3 discuss number implementation issues effect using novel indexing scheme processor cycle time present experimental evaluation proposed indexing scheme section 4 results show ipc outoforder superscalar processor improved use proposed indexing scheme finally section 5 draw conclusions study 2 problem cache conflicts whenever block main memory brought cache decision must made block set blocks cache candidates storing data referred placement policy conventional caches typically extract field bits address use select one block set whilst simple easy implement indexing function robust principal weakness function susceptibility repetitive conflict misses example cache capacity block size addresses map cache set condition 1 repetitive collisions collide cache block addresses also collide cache integer except two common cases happens accessing stream addresses collides may conflict misses stream accessing elements two distinct arrays collides collides way setassociativity help alleviate conflicts however working set contains conflicts cache block set associativity eliminate conflicts following section proposes remedy problem cache conflicts defining improved method block placement 21 conflictresistant cache placement functions objective conflictresistant placement function avoid repetitive conflicts defined condition 1 analogous finding suitable hash function hash table perhaps wellknown alternative conventional cache indexing skewed associative cache 21 involves two indexing functions derived xoring two bit fields address produce bit cache index field interleaved memories well known bank k conflicts reduced using bank selection functions simple modulopower oftwo lawrie vora proposed scheme using primemodulus functions 16 harper jump 11 sohi 24 proposed skewing functions use xor functions proposed frailong et al 5 pseudorandom functions proposed raghavan hayes 17 rau et al 18 19 schemes yield less uniform distribution requests banks varying degrees theoretical predictability implementation cost principle schemes could used construct conflictresistant cache using indexing function however considering conflict resistance cache architectures two factors critical firstly chosen placement function must logically simple implementation secondly would like able guarantee good behavior regular address patterns even pathological conventional placement function respects irreducible polynomial modulus ipoly permutation function proposed rau 19 ideal candidate ipoly scheme effectively defines families pseudorandom hash functions implemented using exclusiveor gates also useful behavioral characteristics discuss later 10 miss ratio ipoly indexing scheme evaluated extensively context cache indexing compared number different cache organizations including directmapped setassociative victim hashrehash columnassociative skewedassociative results study suggest ipoly function particularly robust example spec95 8kb twoway associative cache average miss ratio 1384 ipoly cache identical capacity associativity reduces miss ratio 714 compares well fullyassociative cache miss ratio 680 211 polynomialmodulus cache placement define general form conflict resistant cache indexing scheme let placement block data bit address ways way associative cache sets determined set indices ipoly indexing given function scheme member set possibly distinct integer values range choose use distinct values cache skewed though skewing obligatory feature scheme function defined follows consider integers terms binary representations example similarly consider polynomial defined field gf2 similarly best performance irreducible polynomial though need value also defined gf2 given order less effectively polynomial modulus function ignoring higher order terms bit index computed using xor tree constant andxor tree one requires configurable index function best performance close possible though may small scheme distinct conventional block placement 212 polynomial placement characteristics class polynomial hash functions described studied previously context strideinsensitive interleaved memories see 18 19 functions certain p n 1 n 2 0 x n 2 0 1 x 1 0 x 0 r x provable characteristics significant value context cache indices example strides form produce address sequences free conflicts ie condition 1 set section 1 fundamental result polynomial indexing addresses strided sequence partitioned long subsequences number cache blocks guarantee cache conflicts within sub sequence conflicts subsequences due capacity problems solved larger caches tiling iteration space strideinsensitivity ipoly index function seen figure 1 shows behavior four cache configurations identical except indexing functions capacity twoway associativity driven address trace representing repeated accesses vector 64 8byte elements elements separated stride conflicts sequence would use half 128 sets cache experiment repeated strides range determine many strides exhibited bad behavior indexing function experiment compares three different indexing schemes conventional modulo powerof2 labelled a2 function proposed 21 skewedassociative cache a2hxsk two ipoly functions ipoly scheme simulated without skewed index functions a2hp a2hpsk respectively apparent ipoly scheme skewing displays remarkable ability tolerate strides equally without pathological behavior schemes majority strides yield low miss ratios however conventional skewed xor functions display pathological behavior miss ratio 50 6 strides 3 implementation issues logic polynomial modulus operation gf2 defines class hash functions compute cache placement address combining subsets address bits using xor gates means example bit 0 cache index may computed exclusiveor bits 0 11 14 19 original address choice polynomial determines bits included set implementation function cache 8bit index would require eight xor gates fanin 3 4 whilst appears remarkably simple consider placement function firstly function uses address bits beyond normal limit imposed typical minimum page size restriction secondly use pseudorandom placement multilevel memory hierarchy implications maintenance inclusion briefly examine two issues show virtualreal twolevel cache hierarchy proposed wang et al 25 provides clean solution problems 31 overcoming page size restrictions typical operating systems permit pages small 8 16 kbytes conventional cache places limit firstlevel cache size address translation proceed parallel tag lookup similarly novel cache indexing scheme uses address bits beyond figure 1 frequency distribution miss ratios conventional pseudorandom indexing schemes columns represent ipoly indexing lines represent conventional skewedassociative indexing minimum page size boundary cannot use virtuallyindexed physicallytagged cache four alternative options consider 1 perform address translation prior tag lookup ie physical indices 2 enable ipoly indexing data pages known large enough 3 use virtuallyindexed virtuallytagged level1 cache 4 index conventionally use polynomial rehash level1 miss option 1 attractive existing processor pipeline performs address translation least one stage prior tag lookup might case processor able hide memory latency dynamic execution multithreading example however many systems performing address translation prior tag lookup either extend critical path critical pipeline stage introduce extra cycle untolerated latency via additional pipeline stage option 2 could attractive high performance systems large data sets large physical memories norm circumstances processes may typically data pages 256kbytes os able track page sizes segments currently use process kernel enable polynomial cache indexing firstlevel cache segments page sizes certain threshold provide unmapped bits hash function possible revert conventional indexing possible example threshold 256kbytes cache 8kbytes twoway associative may implement polynomial function hashing 13 unmapped physical address bits 7 cache index bits sufficient produce good conflictfree behavior provided level 1 cache flushed indexing function changed reason indexing function needs remain constant third option currently popular primarily potential difficulties aliases virtual address space well difficulty shooting level1 virtual cache line physicallyaddressed invalidation operation received another processor twolevel virtualreal cache hierarchy proposed wang et al 25 provides way implementing virtuallytagged l1 cache thus exposing address bits indexing function without incurring address translation delays fourth option would appropriate physicallytagged directmapped cache similar principle hashrehash 1 columnassociative caches 2 idea make initial probe conventional integermodulus indexing function using unmapped address bits probe hit probe different index time second probe begins full physical address available used polynomial hashing function compute index second probe addresses coresident conventional index function collide first probe conversely sets addresses collide conventional indexing function collide second probe negligible probability due pseudorandom distribution polynomial hashing function provides kind pseudofull associativity effectively directmapped cache hit time cache first probe would good directmapped physicallyindexed cache however average hit time lengthened slightly due occasional need second probe investigated style cache devised scheme swapping cache lines conventional modulo indexed location alternative polynomiallyindexed location leads typical probability around 90 hit detected first probe however slight increase average hit time due occasional double probes means columnassociative cache attractive miss penalties comparatively large space restrictions prevent coverage option 32 requirements inclusion coherent cache architectures normally require property inclusion maintained levels memory hierarchy thus represents set data present cache level property inclusion demands level memory hierarchy whenever property maintained snooping bus protocol need compare addresses global write operations tags lowest level private cache line index l2 cache replaced line index l1 cache replaced data address already present l2 line contains valid data must sure replacement data still present l1 conventionallyindexed cache issue relatively easy guarantee data l2 index always located l1 index thus ensuring l1 replacement automatically preserve inclusion pseudorandomly indexed cache general way make guarantee instead cache replacement protocols must explicitly enforce inclusion invalidating data l1 required guaranteed twolevel virtualreal cache leads creation holes upper level cache turn leading possibility additional cache misses 33 performance implication holes twolevel virtualreal cache hierarchy three causes holes l1 1 replacements l2 2 removal virtual aliases l1 3 invalidations due external coherency actions probable frequency item 2 occurring low kind hole cause performance problem process must issue interleaved accesses two segments distinct virtual addresses map physical address preserve consistent copy data virtual addresses ensuring one alias may present l1 instant prevent physical copy residing undisturbed l2 simply increases traffic l1 l2 accesses virtual aliases interleaved invalidations external coherency actions occur regardless cache architecture 2 consider analysis events primary importance invalidations l1 due maintenance inclusion l1 l2 important quantify frequency effect hit ratio l1 recall index function l2 based physical address whereas index function l1 uses virtual address also number bits included index function function different cases functions pseudorandom correlation indices l1 l2 particular datum consequently line replaced l2 data replaced also exist l1 probability 1 number bits indices l1 l2 respectively data replaced l2 exist l1 possible l1 index coincidentally equal index data brought l1 l2 replacement actually caused l1 replacement occurs hole created thus probability elimination line l1 preserve inclusion result hole given 2 net probability miss l2 cause hole appear l1 given product thus size ratio l1 l2 large value small example 8kb l1 cache 256kb l2 cache lines yield slightly 3 l2 result creation hole expected increase compulsory miss ratio l1 modelled product l2 miss ratio compared simulated miss ratios find approximation accurate l2l1 cache size ratios 16 instance simulations whole spec95 suite 8kb twoway skewed ipoly l1 cache backed 1 mb conventionallyindexed twoway setassociative l2 cache showed effect holes l1 miss ratio negligible percentage l2 misses created hole averaged less 01 never greater 12 program twolevel virtualreal cache described 25 implements protocol l1 l2 cache effectively provides mechanism ensuring inclusion maintained coherence maintained without reverse address translation case holes created level1 required inclusion property 34 effect polynomial mapping critical path cache memory access conventional organization normally computes effective address adding two registers register plus displacement ipoly indexing implies additional circuitry compute index effective address circuitry consists several xor gates operate parallel therefore total delay delay one gate xor gate number inputs depend particular polynomial used experiments reported paper number inputs never higher 5 therefore delay due gates low compared delay complete pipeline stage depending particular design may happen additional delay hidden instance memory access begin complete effective address computed xor delay hidden since address computed right left gates use leastsignificant bits address 19 experiments reported paper notice true even carry lookahead adders cla cla lookahead blocks size b bits computes first b leastsignificant bits available delay approximately one lookahead block threeblock delay b 2 leastsignificant bits available general b leastsignificant bits delay approximately 2i1 blocks instance 64bit addresses binary cla 19 bits required ipoly functions used experiments paper delay 9 blocks whereas whole address computation requires 11 blockdelays 19 leastsignificant bits computed reasonable assume xor gate delay shorter time required compute remaining bits however since cache access time usually determines pipeline cycle fact leastsignificant bits available early sometimes exploited designers order shorten latency memory instructions overlapping part cache access requires leastsignificant bits computation significant address bits approach results pipeline structure similar shown figure 2 notice organization requires pipelined memory example assumed twostage pipelined memory case polynomial mapping may cause additional delay critical path show later even additional delay induces one cycle penalty cache access time polynomial mapping provides significant overall performance improvement additional delay load instruction may negative impact performance processor issue dependent instructions may delayed accordingly hand delay negligible effect store instructions since instructions issued memory committed order precise exceptions therefore xor functions usually performed instruction waiting store buffer besides load instructions may depend stores dependencies resolved current microprocessors eg pa8000 12 forwarding technique compares effective address load store instructions order check possible match cache index involves use xor gates required operation memory address prediction also used avoid penalty introduced xor delay lengthens critical path effective address memory references shown highly predictable instance 9 shown address 75 dynamically executed memory instructions spec95 suite predicted simple scheme based table keeps track last address seen given instruction last stride propose use similar scheme predict early pipeline line likely accessed given load instruction particular scheme works follows processor incorporates table indexed instruction address entry stores last address predicted stride recently executed load instruction fetch stage figure 2 pipeline overlaps part address computation memory access leastsignificant bits mostsignificant bits memory access memory access alu stage stage critical path table accessed program counter decode stage predicted address computed xor functions performed compute predicted cache line notice done one cycle since xor performed parallel computation mostsignificant bits discussed time perform integer addition higher one cycle vast majority processors instruction subsequently issued memory unit uses predicted line number access cache parallel actual address line computation predicted line turns incorrect cache access repeated actual address otherwise data provided speculative access loaded destination register scheme predict effective address early pipeline previously used purposes 7 load target buffer presented predicts effective address adding stride previous address 3 4 fast address calculation performed computing load addresses early pipeline without using history information proposals memory access overlapped nonspeculative effective address calculation order reduce cache access time though none execute speculatively subsequent instructions depend predicted load authors proposed use memory address prediction scheme order execute memory instructions speculatively well instructions dependent case missspeculation recovery mechanism similar used branch prediction schemes utilized squash missspeculated instructions noteworthy papers topic 8 9 20 4 experimental evaluation order verify impact polynomial mapping realistic microprocessor architecture developed parametric simulator outoforder execution processor fourway superscalar processor simulated table 1 shows different functional units latency considered experiment size reorder buffer entries two separate physical register files fp integer one 64 physical registers processor lockupfree data cache 14 allows 8 outstanding misses different cache lines cache size either 8kb 16 kb 2way setassociative 32byte line size cache writethrough nowriteallocate hit time cache two cycles miss penalty 20 cycles infinite l2 cache assumed 64bit data bus l1 l2 considered ie line transaction occupies bus four cycles two memory ports dependencies thorough memory speculated using mechanism similar arb multiscalar 6 pa8000 12 branch history table 2k entries 2bit saturating counters used branch prediction memory address prediction scheme implemented means directmapped table 1k entries without tags order reduce cost expense interference table entry contains last effective address last load instruction used entry last observed stride addition entry contains 2bit saturating counter assigns confidence prediction mostsignificant bit counter set prediction considered correct address field updated new reference regardless prediction whereas stride field updated counter goes functional unit latency repeat rate multiply effective address 1 1 table 1 functional units instruction latency table 2 shows ipc instructions committed per cycle miss ratio different configurations baseline configuration 8 kb cache ipoly indexing address prediction 4th column average ipc configuration 130 average miss ratio 6th column 1653 1 ipoly indexing used average miss ratio goes 968 8th column xor gates critical path implies increase ipc 135 7th column hand xor gates critical path assume one cycle penalty cache access time resulting ipc 132 9th column however use memory address prediction scheme xor gates critical path 10th column provides performance cache xor gates critical path 7th column thus main conclusion study memory address prediction scheme offset penalty introduced additional delay xor gates critical path finally table 2 also shows performance 16 kb 2way setassociative cache 2nd 3rd columns notice addition ipoly indexing 8kb cache yields 60 ipc increase obtained doubling cache size main benefit polynomial mapping reduce conflict misses however benchmark suite many benchmarks exhibit relatively low conflict miss ratio fact spec95 conflict miss ratio 2way associative cache less 4 programs except tomcatv swim wave5 focus three benchmarks highest conflict miss ratios observe ability polynomial mapping reduce miss ratio significantly boost performance problem cases shown table 3 case see polynomial mapping provides significant improvement performance even xor gates critical path memory address prediction scheme used 27 increase ipc memory address prediction used ipc 33 higher conventional cache capacity 16 higher conventional cache twice capacity notice polynomial mapping scheme 1 benchmark considered 100m instructions skipping first 2000m prediction even better organization xor gates critical path without prediction due fact memory address prediction scheme reduces one cycle effective cache hit time predictions correct since address computation overlapped cache access computed address used verify prediction correct however main benefits observed table 3 come reduction conflict misses isolate different effects also simulated organization conventional indexing ipoly indexing xor cp xor cp ipc miss pred pred ipc miss pred pred ipc miss ipc ipc go 100 545 087 088 1087 087 1060 083 084 compress 113 1296 112 113 1363 111 1417 107 110 li 140 472 130 132 801 133 710 126 131 ijpeg 131 094 128 128 372 129 217 128 130 perl 145 452 126 127 947 124 1026 119 121 vortex 139 497 127 128 837 130 787 125 127 su2cor 128 1374 124 126 1469 124 1466 121 125 hydro2d 114 1540 113 115 1723 113 1722 111 115 applu 163 554 161 163 616 157 684 155 159 mgrid 151 491 150 153 505 150 531 146 152 turb3d 185 467 180 182 605 181 538 178 182 apsi 113 1003 108 109 1519 108 1336 107 109 fpppp 214 109 200 200 266 198 247 193 194 wave5 137 2772 126 128 4276 151 1467 148 154 average 138 1047 130 131 1653 135 968 132 135 table 2ipc load miss ratio different cache configurations memory address prediction scheme conventional indexing 8kb cache column 5 compare ipc column 4 table 3 see benefits memory address prediction scheme due reduction hit time almost negligible confirms improvement observed ipoly indexing scheme address prediction derives reduction conflict misses conclusions paper described pseudorandom indexing scheme robust enough eliminate repetitive cache conflicts discussed main implementation issues arise use novel indexing schemes example ipoly indexing uses address bits conventional cache compute cache index also use different indexing functions l1 l2 results occasional creation hole l1 problems solved using twolevel virtualreal cache hierarchy finally proposed memory address prediction scheme avoid penalty due potential delay critial path introduced ipoly indexing function detailed simulations ooo superscalar processor demonstrated programs significant numbers conflict misses conventional 8kb 2way setassociative cache conventional indexing polynomial mapping xor cp xor cp ipc miss pred pred ipc miss pred pred ipc miss ipc ipc wave5 137 2772 126 128 4276 151 1467 148 154 average 128 3080 112 113 5461 146 1440 142 149 table 3 ipc load miss ratio different cache configurations selected bad programs perceive ipc improvements 33 address prediction 27 without address prediction 16 higher ipc improvements obtained simply doubling cache capacity furthermore programs experience significant conflict misses see less 1 reduction ipc ipoly indexing used conjunction address prediction interesting byproduct ipoly indexing increase predictability cache behaviour experiments see ipoly indexing reduces standard deviation miss ratios across spec95 1849 516 conflict misses eliminated miss ratio depends solely compulsory capacity misses general easier predict control systems incorporate ipoly cache could useful realtime domain cachebased scientific computing iterationspace tiling often introduces extra cache conflicts 6 r cache performance operating systems multiprogramming columnassociative caches technique reducing miss rate directmapped caches streamling data cache access fast address calculation zerocycle loads microarchitecture support reducing load latency xorschemes flexible data organization parallel memories arb hardware mechanims dynamic reordering memory references hardware support fot hiding cache latency memory address prediction data speculation speculative execution via address prediction data prefetching eliminating cache conflict misses xorbased placement functions vector access performance parallel memories using skewed storage scheme advanced performance features 64bit pa8000 improving directmapped cache performance addition small fullyassociative cache prefetch buffers lockupfree instruction fetchprefetch cache organization cache performance optimization blocked algorithms prime memory system array access randomly interleaved memories cydra 5 strideinsensitive memory system pseudorandomly interleaved memories performance potential data dependence speculation collapsing case twoway skewedassociative caches skewedassociative caches cache memories logical data skewing schemes interleaved memories vector processors organization performance twolevel virtualreal cache hierarchy tr vector access performance parallel memories using skewed storage scheme cache performance operating system multiprogramming workloads organization performance twolevel virtualreal cache hierarchy cache performance optimizations blocked algorithms randomly interleaved memories pseudorandomly interleaved memory case twoway skewedassociative caches columnassociative caches streamlining data cache access fast address calculation zerocycle loads performance potential data dependence speculation myampersandamp collapsing eliminating cache conflict misses xorbased placement functions speculative execution via address prediction data prefetching cache memories skewedassociative caches memory address prediction data speculation advanced performance features 64bit pa8000 lockupfree instruction fetchprefetch cache organization ctr hans vandierendonck koen de bosschere highly accurate efficient evaluation randomising set index functions journal systems architecture euromicro journal v48 n1315 p429452 may steve carr soner nder case workingsetbased memory hierarchy proceedings 2nd conference computing frontiers may 0406 2005 ischia italy mazen kharbutli yan solihin jaejin lee eliminating conflict misses using prime numberbased cache indexing ieee transactions computers v54 n5 p573586 may 2005 nigel topham antonio gonzlez randomized cache placement eliminating conflicts ieee transactions computers v48 n2 p185192 february 1999 rui min yiming hu improving performance large physically indexed caches decoupling memory addresses cache addresses ieee transactions computers v50 n11 p11911201 november 2001 jaume abella antonio gonzlez heterogeneous waysize cache proceedings 20th annual international conference supercomputing june 28july 01 2006 cairns queensland australia