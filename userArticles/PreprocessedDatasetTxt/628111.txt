finding interesting associations without support pruning abstractassociationrule mining heretofore relied condition high support work efficiently particular wellknown priori algorithm effective rules interest relationships occur frequently however number applications data mining identification similar web documents clustering collaborative filtering rules interest comparatively instances data cases must look highly correlated items possibly even causal relationships infrequent items develop family algorithms solving problem employing combination random sampling hashing techniques provide analysis algorithms developed conduct experiments real synthetic data obtain comparative performance analysis b introduction prevalent problem largescale data mining associationrule mining first introduced agrawal imielinski swami 1 challenge sometimes referred marketbasket problem due origins study consumer purchasing patterns retail stores although applications extend far beyond specific setting suppose relation r containing n tuples set boolean attributes 1 l g two sets attributes say j association rule following two conditions satisfied support set j appears least sfraction tuples confidence amongst tuples appears least cfraction also j appearing goal identify valid association rules given relation extent relative popularity problem attributed paradigmatic nature simplicity problem statement wide applicability identifying hidden patterns data general applications original marketbasket motivation arguably though success much availability surprisingly efficient algorithm lack stymied models patterndiscovery data mining algorithmic efficiency derives idea due agrawal et al 1 2 called apriori exploits support requirement association rules key observation set attributes appears fraction tuples subset also appears fraction tuples principle enables following approach based pruning determine list l k ksets attributes high support first compute list l kgamma1 1sets attributes high support consider candidates l k ksets subsets l kgamma1 variants enhancements approach underlie essentially known efficient algorithms computing association rules variants note worst case problem computing association rules requires time exponential apriori algorithm avoids pathology real data sets observe also confidence requirement plays role algorithm indeed completely ignored endgame highsupport sets screened high confidence work motivated longstanding open question devising efficient algorithm finding rules extremely high confidence extremely support example marketbasket data standard associationrule algorithms may useful commonlypurchased ie highsupport items beer diapers essentially useless discovering rules beluga caviar ketel vodka always bought together people purchase either two items develop body techniques rely confidence requirement alone obtain efficient algorithms two possible objections removing support requirement first may increase number rules produced make difficult user pinpoint rules interest note applications described next output rules intended human analysis rather automated analysis case intent seek lowsupport rules confidence extremely close 100 latter substantially reduce output size yet leave rules interest second may argued rules low support inherently uninteresting may true classical marketbasket applications many applications essential discover rules extremely high confidence without regard support discuss applications briefly give supporting experimental evidence turning detailed description results one motivation seeking associations high confidence without support renames phrases misc relations dalai lama pneumocystis carinii avant garde encyclopedia britannica meryl streep meseo oceania mache papier salman satanic bertolt brecht fibrosis cystic cosa nostra mardi gras buenos aires hors oeuvres emperor hirohito darth vader presse agence figure 1 examples different types similar pairs found news articles quirement rules high support obvious wellknown rules lowsupport provide interesting new insights supportfree associations natural class patterns data mining right also arise variety applications copy detection identifying identical similar documents web pages 4 13 clustering identifying similar vectors highdimensional spaces purposes clustering data 6 9 collaborative filtering tracking user behavior making recommendations individuals based similarity preferences users 8 16 note applications formulated terms table whose columns tend sparse goal identify column pairs appear similar without support requirement also forms data mining eg detecting causality 15 important discover associated columns natural notion support describe experimental results one application mining pairs words occur together news articles obtained reuters goal check whether lowsupport highconfidence pairs provided interesting information indeed similar pairs proved extremely interesting illustrated representative samples provided figure 1 large majority output pairs names famous international personalities cities terms medicine fields phrases foreign languages miscellaneous items like authorbook pairs organization names also obtained clusters words ie groups words pairs high similarity example cluster chess timman karpov soviet ivanchuk polger represents chess event noted pairs discovered low support would discovered standard definition association rules course run apriori algorithm low support definition would really slow indicated running time comparison provided section 5 2 summary results notion confidence asymmetric unidirectional convenient purpose work symmetric bidirectional measure interest conceptual level view data 01 matrix n rows columns typically matrix fairly sparse assume average number 1s per row r r applications mind n could much could large 10 6 r could small set rows 1 column c also define density column c define similarity two columns c c j similarity c c j fraction rows amongst containing 1 either c c j contain 1 c c j observe definition similarity symmetric respect c c j contrast confidence rule fc g fc j g given identify pairs columns similarity exceeding prespecified threshold easy matrix small fits main memory since bruteforce enumeration algorithm requires om 2 n time interested case large data diskresident paper primary focus problem identifying pairs columns similarity exceeding prespecified threshold restricting basic version problem enable us clearly showcase techniques dealing main issue achieving algorithmic efficiency absence support requirement possible generalize techniques complex settings discuss briefly moving techniques easy verify basic approach generalizes problem identifying highconfidence association rules pairs columns discussed section 6 omit analysis experimental results version paper results practically highsimilarity pairs noted several recent papers 3 14 15 expressed dissatisfaction use confidence measure interest association rules suggested various alternate measures ideas applicable new measures interest well major restriction work deal pairs columns however believe possible apply techniques identification complex rules matter discussed detail section 6 algorithms identifying pairs similar columns follow natural threephase compute signatures generate candidates prune candidates first phase make pass table generating small hashsignature column goal deal largescale tables sitting secondary memory phase produces summary table fit main memory second phase operate main memory generating candidate pairs column signatures finally third phase make another pass original table determining candidate pair whether indeed high similarity last phase identical algorithms scanning table data maintain candidate columnpair counts number rows 1 least one two columns also number rows 1 columns consequently limit ensuing discussion proper implementation first two phases key ingredient course hashing scheme computing signatures one hand needs extremely fast produce small signatures able single pass data competing goal requirement many falsepositives ie candidate pairs really highlysimilar since time required third phase depends number candidates screened related requirement extremely ideally none falsenegatives ie highlysimilar pairs make list candidates section 3 present family schemes based technique called minhashing mh inspired idea used cohen 5 estimate size transitive closure reachability sets see also broder 4 idea implicitly define random order rows selecting column signature consists first row index ordering column 1 show probability two columns signature proportional similarity reduce probability falsepositives falsenegatives collect k signatures independently repeating basic process picking first k rows column 1s main feature minhashing scheme suitably large choice k number falsepositives fairly small number falsenegatives essentially zero disadvantage k rises space time required second phase candidate generation increases second family schemes called localitysensitive hashing lsh presented section 4 inspired ideas used gionis indyk motwani 7 highdimensional nearest neighbors see also indyk motwani 11 basic idea implicitly partition set rows computing signature based pattern 1s column subtable example could compute bit column subtable denoting whether number 1s column greater zero family schemes suffers disadvantage reducing number falsepositives increases number falsenegatives vice versa unlike previous scheme tends produce falsepositives falsenegatives advantage much lower space time requirements minhashing conducted extensive experiments real synthetic data results presented section 5 expected experiments indicate schemes outperform apriori algorithm orders magnitude also illustrate point made tradeoff accuracy speed two algorithms important avoid falsenegatives recommend use minhashing schemes tend slower however speed important complete accuracy generating rules locality sensitive hashing schemes preferred conclude section 6 discussing extensions work alluded earlier providing interesting directions future work minhashing schemes minhashing scheme used idea due cohen 5 context estimating transitive closure reachability sets basic idea minhashing scheme randomly permute rows column c compute hash value hc index first row permutation 1 column reasons efficiency wish explicitly permute rows indeed would like compute hash value column single pass table end scanning rows simply associate row hash value number chosen independently uniformly random range r assuming number rows 2 suffice choose hash value random 32bit integer avoiding birthday paradox 12 two rows get identical hash value furthermore scanning table assigning random hash values rows column c keep track minimum hash value rows contain 1 column thus obtain minhash value hc column c single pass table using om memory proposition 1 column pair easy see since two columns minhash value random permutation rows defined hash values first row 1 column c also first row 1 column c j words hc restriction permutation rows c c j first row belongs c order able determine degree similarity columnpairs necessary determine multiple say independent minhash values column end single pass input table select parallel k independent hash values row defining k distinct permutations rows using omk memory single pass also determine corresponding k minhash values say h c j k row permutations effect obtain matrix c k rows columns c entries column minhash values matrix c viewed compact representation matrix show theorem 1 similarity columnpairs captured similarity c fraction minhash values identical c c j ie defined b fraction rows b minhash entries columns c c j identical show b good estimator sc recall set threshold two columns said highlysimilar sc assume lower bounded constant c following theorem shows unlikely get many falsepositives falsenegatives using b determine similarity columnpairs original matrix theorem 1 let columns c following two properties sc probability least 1 gamma ffl probability least 1 gamma ffl sketch proof first part theorem proof second part quite similar omitted fix two columns c c j similarity sc l random variable takes value 1 h l proposition 1 ex l ks applying chernoff bound 12 random variable x obtain ks establish first part theorem simply notice b theorem 1 establishes sufficiently large k two columns high similarity least agree correspondingly large fraction minhash values c conversely similarity low c agree correspondingly small fraction minhash values c since c computed single pass data using okm space obtain desired implementation first phase signature computation turn task devising suitable implementation second phase candidate generation 31 candidate generation minhash values computed signatures first phase discussed previous section wish generate candidate columnpairs second phase point k theta matrix c containing k minhash values column since k n assume c much smaller original data fits main memory goal identify columnpairs agree large enough fraction least 1 gamma ffis minhash values c bruteforce enumeration require ok time columnpair total okm 2 present two techniques avoid quadratic dependence considerably faster typically case average similarity rowsorting algorithm view rows c list tuples containing minhash value corresponding column number sort row basis minhash values groups together identical minhash values sequence runs maintain column index position minhash value sorted row estimate similarity column c columns use following algorithm use counters column c jth counter stores number rows minhash values columns c c j identical row run containing min hash value c column represented run increment corresponding counter avoid om 2 counter initializations reuse om counters processing different columns remember reinitialize counters incremented least estimate running time algorithm follows sorting rows requires total time okm log thereafter indexes columns built time okm remaining time amounts total number counter increments processing row column c number counter increments fact length run expected length run equals sum similarities hence expected counterincrement cost processing c ok expected combined increments cost ok oksm 2 thus expected total time required algorithm okm log note average similarity typically small fraction latter term running time really quadratic appears hashcount next section introduces kminhashing algorithm signatures column c set sig exactly k minhash values similarity columnpair estimated computing size sig clearly suffices consider ordered pairs task accomplished via following hashcount algorithm associate bucket minhash value buckets indexed using hash function defined minhash values store columnindexes columns c element sig hashing bucket consider columns c column c use counters jth counter stores minhash value v 2 sig access hashbucket find indexes columns c j j v 2 sig j column c j bucket increment counter finally add c bucket hashcount easily adapted use original minhash scheme instead want compute pair columns number c rows two columns agree end use different hash table set buckets row matrix c execute process kminhash argument used rowsorting algorithm shows hashcount minhashing takes oksm 2 time running time hashcount kminhash amounts number counter increments number increments made counter exactly size jsig sig j j simple argument see lemma 1 shows expected size efjsig thus expected total running time hashtable scheme oksm 2 cases 32 kminhashing algorithm one disadvantage minhashing scheme outlined choosing k independent min hash values column entailed choosing k independent hash values row negative effect efficiency signaturecomputation phase hand using k minhash values per column essential reducing number falsepositives false negatives present modification called kminhashing kmh use single hash value row setting k minhash values column hash values first k rows induced row permutation containing 1 column approach also mentioned 5 without analysis words column pick k smallest hash values rows containing one column column c fewer 1s k assign minhash values hash values corresponding rows 1s column resulting set k minhash values forms signature column c denoted sig proposition 2 kminhashing scheme column c signature sig consists hash values uniform random sample distinct rows c remark number 1s column significantly larger k hash values may considered independent analysis minhashing applies situation slightly complex columns sparse case interest us let sig ij denote k smallest elements c view sig ij signature column would correspond c c j observe sig ij obtained ok time sig sig j since fact set smallest k elements corresponds set rows selected uniformly random elements c c j expected number elements sig ij belong subset c exactly jsig ij jthetajc since signatures smallest k elements hence obtain following theorem theorem 2 unbiased estimator similarity sc given expression consider computational cost algorithm scanning data generate one hash value per row column maintain minimum k hash values corresponding rows contain 1 column maintain k minimum hash values column simple data structure allows us insert new value smaller current maximum delete current maximum olog time data structure also makes maximum element amongst k current minhash values column readily available hence computation row constant time 1 entry additional log k time column 1 entry hash value row amongst k smallest seen far simple probabilistic argument shows expected number rows kminhash list column c gets updated ok log jc log n follows total computation cost single scan data ojm log k jm j number 1s matrix second phase generating candidates need compute sets sig ij columnpair using merge join ok operations merging also find elements belong sig hence total time phase okm 2 quadratic dependence number columns prohibitive caused need compute sig ij columnpair instead first apply considerably efficient biased approximate estimator similarity biased estimator computed pairs columns using hashcount time next perform mainmemory candidate pruning phase unbiased estimator theorem 2 explicitly computed pairs columns approximate biased estimator exceeds threshold choice threshold biased estimator guided following lemma alternatively biased estimator choice threshold derived following analysis let column c choose set sig k minhash values let sig expected sizes sig ij sig ji given kjc ij j hence compute expected value assume probjsig ij j jsig ji j 0 equation becomes thus obtain estimator ejsig use estimate calculate use estimate similarity since know jc j jc j j compute jsig using hash table technique described earlier section 31 time required compute hash values ojm j mk log n log described earlier time computing 4 localitysensitive hashing schemes section show obtain significant improvement running time respect previous algorithms resorting locality sensitive hashing lsh technique introduced indyk motwani 11 designing mainmemory algorithms nearest neighbor search highdimensional euclidean spaces subsequently improved tested 7 apply lsh framework minhash functions described earlier section obtaining algorithm similar columnpairs problem differs nearest neighbor search data known advance exploit property showing optimize running time algorithm given constraints quality output optimization inputsensitive ie takes account characteristics input data set key idea lsh hash columns ensure hash function probability collision much higher similar columns dissimilar ones subsequently hash table scanned columnpairs hashed bucket reported similar since process probabilistic false positives false negatives occur order reduce former lsh amplifies difference collision probabilities similar dissimilar pairs order reduce false negatives process repeated times union pairs found iterations reported fraction false positives false negatives analytically controlled using parameters algorithm although main focus paper mention lsh algorithm adapted online framework 10 particular follows analysis iteration algorithm reduces number false negatives fixed factor also add new false positives removed small additional cost thus user monitor progress algorithm interrupt process time satisfied results produce far moreover higher similarity earlier pair likely discovered therefore user terminate process output produced appears less less interesting 41 minlsh scheme present minlsh mlsh scheme finding similar columnpairs matrix c minhash values mlsh algorithm splits matrix c l submatrices dimension r theta recall c dimension k theta assume l submatrices repeat following column represented r minhash values current submatrix hashed table using hashing key concatenation r values two columns similar high probability agree r minhash values hash bucket end phase scan hash table produce pairs columns hashed bucket amplify probability similar columns hash bucket repeat process l times let p rl probability columns c c j hash bucket least since value p depends upon notation writing p assume columns c c j similarity also let similarity threshold 0 1 choose parameters r l ffl ffl proof proposition 1 probability columns c c j agree one minhash value exactly probability agree group r values r repeat hashing process l times probability hash least bucket would lemma follows properties function p states large values r l function p approximates unit step function translated point used filter pairs similarity hand timespace requirements algorithm proportional increase values r l subject qualityefficiency tradeoff practice willing allow number false negatives n gamma false positives optimal values r l achieve quality specifically assume given estimate similarity distribution data defined ds number pairs similarity unreasonable assumption since approximate distribution sampling small fraction columns estimating pairwise similarity expected number false negatives would expected number false positives would therefore problem estimating optimal parameters turns following minimization problem subject easy problem since two parameters optimize feasible values small integers also histogram ddelta typically quantified 1020 bins one approach solve minimization problem iterating small values r finding lower bound value l solving first inequality performing binary search second inequality satisfied experiments optimal value r 5 20 42 hamminglsh scheme propose another scheme hamminglsh hlsh finding highlysimilar columnpairs idea reduce problem searching columnpairs small hamming distance order solve latter problem employ techniques similar used 7 solve nearest neighbor problem start establishing correspondence similarity hamming distance proof easy follows consider pairs sum high value sc corresponds small values dh versa hence partition columns groups similar density group find pairs columns small hamming distance first briefly describe search pairs columns small hamming distance scheme similar technique 7 analyzed using tools developed scheme finds highlysimilar columns assuming density columns roughly done partitioning rows database p subsets partition process previous algorithm declare pair columns candidate agree subset thus scheme exactly similar earlier scheme except dealing actual data instead minhash values however two problems scheme one problem matrix sparse subsets contain zeros also columns similar densities assumed following algorithm call hlsh improves basic algorithm basic idea follows perform computation sequence matrices increasing densities denote matrix i1 obtained matrix randomly pairing rows placing i1 pair 1 one see i1 contains half rows illustration purposes assume initial number rows power 2 algorithm applied matrices set pair columns become candidate matrix sufficiently dense densities belong certain range false negatives controlled repeating 1 notice operation gives similar results hashing columns set increasingly smaller hash table provides alternative view algorithm number similar pairs histogram sun data set number similar pairs histogram sun data set figure 2 first figure shows similarity distribution sun data second shows distribution focuses region similarities interested sample l times taking union candidate sets across l runs hence kr rows extracted compressed matrix note operation may increase false positives present algorithm implemented experiments show scheme better minhashing algorithms terms running time number false positives much larger moreover number false positives increases rapidly try reduce number false negatives case minhashing algorithms decreased number false negatives increasing k number false positives would also decrease algorithm described 2 0 select k sets r sample rows 3 column pair candidate exists column pair density 1t gamma 1t ii identical hash values essentially identical rbit representations least one k runs note parameter indicates range density candidate pairs use experiments 5 experiments conducted experiments evaluate performance different algorithms section report results different experiments use two sets data namely synthetic data real data synthetic data data contains 10 4 columns number rows vary 10 4 column densities vary 1 5 every 100 columns pair similar columns 20 pairs similar columns whose similarity fall ranges 85 95 75 85 65 75 55 65 45 55 real data real data set consists log http requests made period 9 days sun microsystems web server wwwsuncom columns case urls support number columns apriori mh kmh hlsh mlsh threshold support pruning sec sec sec sec sec 01 15559 714 876 156 107 015 11568 9605 448 520 67 97 02 9518 7994 258 360 60 51 figure 3 running times news articles data set rows represent distinct client ip addresses recently accessed server entry set 1 least one hit url particular client ip data set thirteen thousand columns 02 million rows columns sparse density less 001 histogram figure 2 shows number column pairs different values similarity typical examples similar columns extracted data urls corresponding gif images java applets loaded automatically client ip accesses parent url compare algorithms existing techniques implemented executed apriori algorithm 1 2 course apriori designed setting low support existing technique gives us benchmark compare improvements afforded algorithms comparison done news articles data mentioned section 1 conducted experiments news article data results summarized figure 3 apriori algorithm cannot run original data since runs memory therefore performed support pruning remove columns ones evident techniques give nearly order magnitude improvement running time support threshold 01 apriori runs memory systems lot thrashing note although algorithms probabilistic report set pairs reported apriori 51 results implemented four algorithms described previous section namely mh kmh hlsh mlsh algorithms compared terms running time quality output due lack space report experiments give graphs sun data case interesting also performed tests synthetic data algorithms behave similarly quality output measured terms false positives false negatives generated algorithm plot curve shows ratio number pairs found algorithm real number pairs computed offline given similarity range eg figure 7 result typically sshaped curve gives good visual picture false positives negatives algorithm intuitively area curve left given similarity cutoff corresponds number false positives area curve right cutoff corresponds number false negatives describe behavior algorithm parameters varied mh kmh algorithms two parameters user specified similarity cutoff k number minhash values extracted represent signature column fig fraction pairs found performance mh sun data set f80 total time sec running time mh sun data setf80 b fraction pairs found performance mh sun data setk500 total time sec running time mh sun data setk500 c figure 4 quality output total running time mh algorithm k varied ures 4a 5a plot scurves different values k mh kmh algorithms k value increases curve gets sharper indicating better quality figures 4c 5c fixed change value similarity cutoff expected curves shift right cutoff value increases figures 4d 5d show given value k total running time decreases marginally since generate fewer candidates figure 4b shows total running time mh algorithm increases linearly k however case kmh algorithm depicted figure 5b sublinear increase running time due sparsity data specifically number hash values extracted column upper bounded number ones column therefore hash values extracted increase linearly k similar exploration parameter space mlsh hlsh algorithms parameters algorithm r l figures 7a 6a illustrate fact r increases probability columns mapped bucked decreases therefore number false positives decreases tradeoff consequence number false negatives increases hand figure 7c 6c shows increase l corresponds increase collision probability therefore number false negatives decrease number false positives increases figures 7d 6d show total running time increases l since fraction pairs found performance kmh sun data setf80 total time sec running time kmh sun data setf80 b fraction pairs found performance kmh sun data setk500 total time sec running time kmh sun data setk500 c figure 5 quality output total running time kmh algorithm k varied hash column times also results increase number candidates implementation mlsh extraction min hash values dominates total computation time increases linearly value r shown figure 7b hand implementation hlsh checking candidates dominates running times result total running time decreases r increases since less candidates produced shown figure 6b compare different algorithms implemented comparing time requirements algorithm compare cpu time algorithm since time spent io algorithms important note algorithms number false negatives important quantity requires kept control long number false positives large ie candidates fit main memory always eliminate pruning phase compare algorithms fix percentage false negatives tolerated algorithm pick set parameters number false negatives within threshold total running time minimum plot total running time number false positives false negative threshold consider figures 8a 8c figures show total running time false fraction pairs found performance hlsh sun data set 5680120160200value parameter r total time sec running time hlsh sun data set b fraction pairs found performance hlsh sun data set value parameter l total time sec running time hlsh sun data set c figure quality output total running time hlsh algorithm r l varied negative threshold see hlsh algorithm requires lot time false negative threshold less better limit high general mlsh hlsh algorithms better mh kmh algorithms however noted hlsh algorithm cannot used interested similarity cutoffs low graph shows best performance shown mlsh algorithm figure 8 gives number false positives generated algorithms tolerance limit false positives plotted logarithmic scale case hlsh mlsh algorithms number false positives decreases ready tolerate false negatives since case hash every column fewer times however false positive graph kmh mh monotonic exists tradeoff time spent candidate generation stage pruning stage maintain number false negatives less given threshold could either increase k spend time candidate generation stage else decrease similarity cutoff spend time pruning stage get false positives hence points graph correspond different values similarity cutoff algorithms run get candidates similarity certain threshold result observe monotonic behavior case algorithms fraction pairs found performance mlsh sun data time sec value parameter r running time mlsh sun data set b fraction pairs found performance mlsh sun data set time sec value parameter l running time mlsh sun data set c figure 7 quality output total running time mlsh algorithm r l varied would like comment results provided analyzed caution reader note whenever refer time refer cpu time expect io time dominate signature generation phase pruning phase aware nature data smart choice algorithms instance kmh algorithm used instead mh sparse data sets since takes advantage sparsity 6 extensions work briefly discuss extensions results presented well directions future work first note results presented discovery bidirectional similarity measures however minhash technique extended discovery columnpairs form highconfidence association rule type fc without support requirements basic idea generate set minhash values column determine whether fraction values identical c c j proportional ratio densities j analytical experimental results qualitatively similar columnpairs total time sec false negative threshold time vs false negatives mh kmh hlsh mlsh false negative threshold false positives vs false negatives mh kmh hlsh mlsh b time sec false negative threshold time vs false negatives mh kmh hlsh mlsh number false positives false negative threshold false positives vs false negatives mh kmh hlsh mlsh c figure 8 comparison different algorithms terms total running time number false positives different negative thresholds also use minhashing scheme determine complex relationships eg c highlysimilar c j c j 0 since hash values induced column c j c j 0 easily computed taking componentwise minimum hash value signature c j c j 0 extending difficult works follows first observe c implies c j c means c implies latter two implications generated conclude c implies c j c cardinality c roughly c j c j 0 presents problems cardinality c really small difficult otherwise case small c may interesting anyway since difficult associate statistical significance similarity case also possible define anticorrelation mutual exclusion pair columns however statistical validity would require imposing support requirement since extremely sparse columns likely mutually exclusive sheer chance interesting note hashing techniques extended deal situation unlike apriori effective even support requirements extensions three columns complex boolean expressions possible suffer exponential overhead number columns r mining association rules sets items large databases fast algorithms mining association rules dynamic itemset counting implication rules market basket data resemblance containment documents pattern classification scene analysis similarity search high dimensions via hashing using collaborative filtering weave information tapestry online aggregation approximate nearest neighbor towards removing curse dimensionality randomized algorithms building scalable accurate copy detection mechanism beyond market baskets generalizing association rules dependence rules scalable techniques mining causal structures cacm special issue recommender systems tr ctr masao nakada yuko osana document clustering based similarity subjects using integrated subject graph proceedings 24th iasted international conference artificial intelligence applications p410415 february 1316 2006 innsbruck austria yan huang hui xiong shashi shekhar jian pei mining confident colocation rules without support threshold proceedings acm symposium applied computing march 0912 2003 melbourne florida girish k palshikar mandar kale manoj apte association rules mining using heavy itemsets data knowledge engineering v61 n1 p93113 april 2007 yiping ke james cheng wilfred ng mining quantitative correlated patterns using informationtheoretic approach proceedings 12th acm sigkdd international conference knowledge discovery data mining august 2023 2006 philadelphia pa usa zahid hossain sk ahad ali unified descriptive language association rules data mining second international workshop intelligent systems design application p227232 august 0708 2002 atlanta georgia sunita sarawagi alok kirpal efficient set joins similarity predicates proceedings 2004 acm sigmod international conference management data june 1318 2004 paris france yishan jiao maintaining stream statistics multiscale sliding windows acm transactions database systems tods v31 n4 p13051334 december 2006 edith cohen amos fiat haim kaplan case associative peer peer overlays acm sigcomm computer communication review v33 n1 p95100 january jian zhang joan feigenbaum finding highly correlated pairs efficiently powerful pruning proceedings 15th acm international conference information knowledge management november 0611 2006 arlington virginia usa david johnson shankar krishnan jatin chhugani subodh kumar suresh venkatasubramanian compressing large boolean matrices using reordering techniques proceedings thirtieth international conference large data bases p1323 august 31september 03 2004 toronto canada wenyang lin mingcheng tseng automated support specification efficient mining interesting association rules journal information science v32 n3 p238250 june 2006 changhung lee chengru lin mingsyan chen sliding window filtering efficient method incremental mining timevariant database information systems v30 n3 p227244 may 2005 edith cohen haim kaplan bottomk sketches better efficient estimation aggregates acm sigmetrics performance evaluation review v35 n1 june 2007 zan huang wingyan chung hsinchun chen graph model ecommerce recommender systems journal american society information science technology v55 n3 p259274 february 2004 edith cohen amos fiat haim kaplan associative search peer peer networks harnessing latent semantics computer networks international journal computer telecommunications networking v51 n8 p18611881 june 2007