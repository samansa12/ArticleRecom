learning design principal curves abstractprincipal curves defined selfconsistent smooth curves pass middle ddimensional probability distribution data cloud give summary data also serve efficient feature extraction tool take new approach defining principal curves continuous curves given length minimize expected squared distance curve points space randomly chosen according given distribution new definition makes possible theoretically analyze principal curve learning training data also leads new practical construction theoretical learning scheme chooses curve class polygonal lines k segments given total length minimize average squared distance n training points drawn independently convergence properties learning scheme analyzed practical version theoretical algorithm implemented iteration algorithm new vertex added polygonal line positions vertices updated minimize penalized squared distance criterion simulation results demonstrate new algorithm compares favorably previous methods terms performance computational complexity robust varying data models b introduction principal component analysis perhaps bestknown technique multivariate analysis used dimension reduction feature extraction image coding enhancement consider ddimensional random vector moments first principal component line x straight line property expected value squared euclidean distance x first principal component line minimum among straight lines property makes first principal component concise onedimensional approximation distribution x projection x line gives best linear summary data elliptical distributions first principal component also self consistent ie point line conditional expectation x points space project point hastie 1 hastie stuetzle 2 hereafter hs generalized self consistency property principal components introduced notion principal curves let smooth infinitely differentiable curve r parametrized 2 r x 2 r let f x denote parameter value distance x ft minimized see figure 1 formally projection index f x defined denotes euclidean norm r figure 1 projecting points curve hs definition smooth curve ft principal curve f intersect ii f finite length inside finite ball r iii f selfconsistent ie intuitively speaking selfconsistency means point f average distribution x points project thus principal curves smooth selfconsistent curves pass middle distribution provide good onedimensional nonlinear summary data based self consistency property hs developed algorithm constructing principal curves similar spirit generalized lloyd algorithm gla vector quantizer design 3 hs algorithm iterates projection step expectation step probability density x known hs principal algorithm constructing principal curves following first principal component line x set psi x 2 r step 4 evaluate expected squared distance deltaf j f j stop deltaf j less certain threshold otherwise increase j 1 go step 1 practice distribution x often unknown data set consisting n samples underlying distribution known instead hs algorithm data sets expectation step 1 replaced smoother locally weighted running lines 4 nonparametric regression estimate cubic smoothing splines hs provide simulation examples illustrate behavior algorithm describe application stanford linear collider project alternative definitions methods estimating principal curves given subsequent hastie stuetzles groundbreaking work banfield raftery 5 modeled outlines ice floes satellite images closed principal curves developed robust method reduces bias variance estimation process method clustering principal curves led fully automatic method identifying ice floes outlines theoretical side tibshirani 6 introduced semiparametric model principal curves proposed method estimating principal curves using em algorithm close connections principal curves kohonens selforganizing maps pointed mulier cherkassky 7 recently delicado 8 proposed yet another definition based property first principal components multivariate normal distributions remains unsatisfactory aspect definition principal curves original hs paper well subsequent works although principal curves defined nonparametric existence given distribution probability density open question except special cases elliptical distributions also makes difficult theoretically analyze estimation scheme principal curves paper propose new definition principal curves resolve problem new definition principal curve continuous curve given length l minimizes expected squared distance x curve section 2 lemma 1 prove x finite second moments always exists principal curve new sense also discuss connections newly defined principal curves optimal vector quantizers propose theoretical learning scheme model classes polygonal lines ksegments given length algorithm chooses curve class minimizes average squared distance n training points theorem 1 prove k suitably chosen function n expected squared distance curve trained n data points converges expected squared distance principal curve rate 13 n 1 two main features distinguish learning scheme hs algorithm first polygonal line estimate principal curve determined via minimizing data dependent criterion directly related definition principal curves facilitates theoretical analysis performance second complexity resulting polygonal line determined number segments k typically much less n optimal choice k 1 agrees mental image principal curves provide concise summary data hand data points hs algorithm scatterplot smoothing produces polygonal lines n segments though amenable analysis theoretical algorithm computationally burdensome implementation section 3 develop suboptimal algorithm learning principal curves practical algorithm produces polygonal line approximations principal curve theoretical method global optimization replaced less complex iterative descent method section 4 give simulation results compare algorithm previous work general examples considered hs performance new algorithm comparable hs algorithm proves robust changes data generating model note choice k made automatic principle using method structural risk minimization 9 learning principal curves length constraint curve ddimensional euclidean space continuous function f r closed interval real line let expected squared distance x f defined theta inf projection index f x given 1 let f smooth infinitely differentiable curve 2 r consider perturbation f g f smooth curve g proved f principal curve f critical point distance function sense g 0 hard see analogous result holds principal component lines perturbation g straight line sense hs principal curve definition natural generalization principal components also easy check principal components fact principal curves distribution x elliptical unfortunate property hs definition general known principal curves exists given source density resolve problem go back defining property first principal component straight line st first principal component theta min theta min straight line st wish generalize property first principal component define principal curves minimize expected squared distance class curves rather critical points distance function necessary constrain length 2 curve since otherwise x density ffl 0 exists smooth curve f deltaf ffl thus minimizing f infinite length hand distribution x concentrated polygonal line uniform infimum distances deltaf 0 class smooth curves smooth curve achieve infimum reason 2 definition length nondifferentiable curves see appendix basic facts concerning curves r collected 10 relax requirement f differentiable instead constrain length f note definition curves f still continuous give following new definition principal curves curve f called principal curve length l x f minimizes deltaf curves length less equal l useful advantage new definition principal curves length l always exist x finite second moments next result shows assume ekxk 2 1 l 0 exists curve f deltaf proof lemma given appendix note dropped requirement hs definition principal curves nonintersecting fact lemma 1 would hold nonintersecting curves length l without restricting distribution x since distributions minimum deltaf achieved intersecting curve even though nonintersecting curves arbitrarily approach minimum remark connection vector quantization new definition principal curves inspired notion optimal vector quantizer points called codepoints optimal kpoint vector quantizer theta min theta min collection k points words points k give best kpoint representation x mean squared sense optimal vector quantizers great interest lossy data compression speech image coding 11 clustering 12 strong connection definition optimal vector quantizers definition principal curve minimize expected squared distance criterion vector quantizer constrained k points constrain length principal curve connection illuminated recent work tarpey et al 13 define k points self consistent voronoi regions associated kg thus principal curves correspond optimal vector quantizers principal points terminology 13 hs principal curves correspond self consistent points principal curves given length always exist appears difficult demonstrate concrete examples unless distribution x discrete concentrated curve problem occurs theory optimal vector quantizers except scalar case structure optimal quantizers unknown even common multivariate densities eg see 11 suppose n independent copies x given called training data assumed independent x goal use training data construct curve length l whose expected squared loss close principal curve x method based common model statistical learning theory eg see 9 consider classes curves increasing complexity given n data points drawn independently distribution x choose curve estimator principal curve kth model class k minimizing empirical error choosing complexity model class appropriately size training data grows chosen curve represents principal curve increasing accuracy assume distribution x concentrated closed bounded convex set k ae r basic property convex sets r shows exists principal curve length l inside k see lemma 2 appendix consider curves k let denote family curves taking values k length greater l k 1 let k set polygonal curves broken lines k k segments whose lengths exceed l note k ae k let denote squared distance point x 2 r curve f f 2 empirical squared error f training data sample average suppressed notation dependence delta n f training data let theoretical algorithm choose f kn 2 k minimizes empirical error ie measure efficiency f kn estimating f difference jf kn expected squared loss f kn optimal expected squared loss achieved f ie let deltaf main result section proves number data points n tends infinity k chosen proportional n 13 jf kn tends zero rate jf kn theorem 1 assume pfx 2 bounded closed convex set k let n number training points let k chosen proportional n 13 expected squared loss empirically optimal polygonal line k segments length l converges n 1 squared loss principal curve length l rate proof theorem given appendix b establish result use techniques statistical learning theory eg see 14 first approximating capability class curves k considered estimation generalization error bounded via covering class curves k ffl accuracy squared distance sense discrete set curves two bounds combined one obtains r term cl depends dimension length l diameter support x independent k n two error terms balanced choosing k proportional n 13 gives convergence rate theorem 1 note although constant hidden notation depends dimension exponent n dimensionfree surprising view fact class curves equivalent certain sense class lipschitz functions f appendix known fflentropy defined logarithm ffl covering number roughly proportional 1ffl function classes 15 using result convergence rate gamma13 obtained considering fflcovers directly without using model classes k picking empirically optimal curve cover however use classes k advantage directly related practical implementation algorithm given next section 3 polygonal line algorithm given set data points x task finding polygonal line k segments length l minimizes 1 computationally difficult propose suboptimal method reasonable complexity basic idea start straight line segment f 1n iteration algorithm increase number segments k adding new vertex polygonal line f kn produced previous iteration adding new vertex positions vertices updated inner loop 04 b 04 c 04 04 figure 2 curves f kn produced polygonal line algorithm data points data generated adding independent gaussian errors coordinates point chosen randomly half circle f 1n b f 2n c f 4n f 11n output algorithm inner loop consists projection step optimization step projection step data points partitioned nearest neighbor regions according segment vertex project optimization step new position vertex v determined minimizing average squared distance criterion penalized measure local curvature vertices kept fixed two steps iterated optimization step applied vertex v cyclic fashion v k1 procedure starts v 1 convergence achieved f kn produced new vertex added algorithm stops k exceeds threshold cn delta stopping criterion based heuristic model complexity measure determined number segments k number data points n average squared distance delta n f kn flowchart algorithm given figure 3 evolution curve produced algorithm illustrated figure 2 note objective function minimized vertex optimization procedure based partly heuristic considerations explained section 33 algorithm step searches local minimum average squared distance penalized local curvature heuristic lies data dependent form penalty factor similarly hs algorithm formal proof practical algorithm converge practice extensive testing observed converge convergence end projection initialization add new vertex vertex optimization figure 3 flow chart polygonal line algorithm 31 initialization step obtain f 1n take shortest segment first principal component line contains projected data points keep computational complexity low compute first principal component constant number points randomly chosen n data points choice suffices purposes since algorithm needs reasonable approximation first principal component 32 projection step let f denote polygonal line vertices closed line segments connects vertices v v i1 step data set x n partitioned nearest neighbor regions vertices segments f following manner x 2 r let deltax squared distance x see definition 3 let deltax let upon setting defined resulting partition illustrated figure 4001101 figure 4 nearest neighbor partition r 2 induced vertices segments f 33 vertex optimization step step new position vertex v determined theoretical algorithm average squared distance delta n x f minimized subject constraint f polygonal line k segments length exceeding l one could use lagrangian formulation attempt find new position v vertices fixed penalized squared error delta minimum however observed approach sensitive choice hand principal curve applications require smooth curve solution avoid overfitting hs used scatterplot spline smoothing chose penalize local curvature obtain smoother curves due fact one vertex moved time penalizing curvature also implicitly penalize length curve considering several possibilities found following measures local curvature work especially well inner vertices v penalize sum cosines three angles vertices v endpoints immediate neighbors v 1 penalty nonexistent angle replaced squared length first last segment formally let fl denote angle vertex v let v let penalty p v vertex v given local measure average squared distance calculated data points project v line segments starting v see projection step accordingly let define local average squared distance function v use gradient steepest descent method minimize part algorithm modular ie simple procedure using substituted sophisticated nonlinear programming procedure expense increased computational complexity one important issue amount smoothing required given data set hs algorithm one needs determine penalty coefficient spline smoother span scatterplot smoother algorithm corresponding parameter curvature penalty factor p priori knowledge distribution available one use determine smoothing parameter however absence knowledge coefficient datadependent intuitively p increase number segments size average squared error decrease data size based heuristic considerations carrying practical experiments set p parameter algorithm 34 adding new vertex start optimized f kn choose segment largest number data points projecting midpoint segment selected new vertex formally let j new vertex v new stopping condition according theoretical results section 2 number segments k proportional n 13 achieve 13 convergence rate expected squared dis tance though theoretical bounds tight enough determine optimal number segments given data size found k n 13 also works practice achieve robustness need make k sensitive average squared distance stopping condition blends two considerations algorithm stops k exceeds 36 computational complexity complexity inner loop dominated complexity projection step onk increasing number segments one time described section 34 complexity algorithm obtain f kn onk 2 using stopping condition section 35 computational complexity algorithm becomes 56 slightly better 2 complexity hs algorithm complexity dramatically decreased certain situations one possibility add one vertex time example instead adding one vertex new vertex placed midpoint every segment reduce computational complexity producing f kn onk log k one also set k constant data size large since increasing k beyond certain threshold brings diminishing returns simplifications work well certain situations original algorithm robust 4 experimental results extensively tested algorithm twodimensional data sets experiments data generated commonly used see eg 2 6 7 additive model uniformly distributed smooth planar curve hereafter called generating curve e bivariate additive noise independent since true principal curve known note generating curve model e general principal curve either hs sense definition hard give objective measure performance reason follows performance judged subjectively mainly basis closely resulting curve follows shape generating curve general simulation examples considered hs performance new algorithm comparable hs algorithm due datadependence curvature penalty factor stopping condition algorithm turns robust alterations data generating model well changes parameters particular model use varying generating shapes noise parameters data sizes demonstrate robustness polygonal line algorithm plots show generating curve generator curve curve produced polygonal line algorithm principal curve curve produced hs algorithm spline smoothing hs principal curve found perform better hs algorithm using scatterplot smoothing closed generating curves also include curve produced banfield raftery br algorithm 5 extends hs algorithm closed curves br principal curve two coefficients polygonal line algorithm set experiments constant values plots normalized fit 2 theta 2 square parameters given refer values normalization figure 5 generating curve circle radius bivariate uncorrelated gaussian variance ee 2 2 performance three algorithms hs br polygonal line algorithm comparable although hs algorithm exhibits bias two note br algorithm 5 tailored fit closed curves reduce model bias figure 6 half circle used generating curve parameters remain hs algorithm behave similarly depart usual settings polygonal line algorithm exhibits better behavior hs algorithm figure 7a data set figure 6 linearly transformed using matrix 06 06 gamma10 12 figure 7b transformation 10 gamma02 used original data set generated sshaped generating curve consisting two half circles unit radii gaussian noise added figure 6 cases polygonal line algorithm produces curves fit generator curve closely especially noticeable figure 7a hs principal curve fails follow shape distorted half circle two situations expect algorithm perform particularly well distribution concentrated curve according hs definitions principal curve generating curve thus noise variance small expect algorithms closely approximate generating curve data figure 8a generated using additive gaussian model figure 5 noise variance reduced ee 2 2 case found polygonal line algorithm outperformed hs br algorithms second case sample size large although generating curve necessarily principal curve distribution natural expect algorithm well approximate generating curve sample size grows case shown figure 8b data points generated small subset actually plotted polygonal line algorithm approximates generating curve much better accuracy hs algorithm 5 conclusion new definition principal curves offered new definition significant theoretical appeal existence principal curves definition proved general conditions learning method constructing principal curves finite data sets yields theoretically analysis inspired new definition theoretical learning scheme introduced new practical polygonal line algorithm designing principal curves lacking theoretical results concerning hs polygonal line algorithm compared two methods simulations found general algorithm performance either comparable performance original hs algorithm exhibits better robust behavior data generating model varied mentioned findings cannot called entirely conclusive due mainly absence objective performance measure practical applications method different advantages respect believe new principal curve algorithm may prove useful compact accurate description pattern image required eg skeletonization handwritten characters feature extraction issues future work appendix curves r continuous mapping curve length f interval denoted lf ff fi defined supremum taken finite partitions ff fi arbitrary subdivision points 1 length f entire domain b denoted lf lf 1 f said rectifiable well known rectifiable iff coordinate function r bounded variation two curves said equivalent exist two nondecreasing continuous real functions oe case write f g easy see equivalence relation curve g b said parametrized arc length b let f curve b length l hard see exists unique arc length parametrized curve g 0 l f g let f curve length l 0 l consider arc length parametrized curve f parameter interval 0 l 0 definition a1 g satisfies following lipschitz condition hand note g curve 0 1 satisfies lipschitz condition a2 length l let f curve b denote squared euclidean distance x 2 r f atb note lf 1 continuity f graph compact subset r infimum achieved also since g f f g also deltax f deltax g g f proof lemma 1 define first show infimum change add restriction f lie inside closed sphere large enough radius r centered origin indeed without excluding nontrivial cases assume delta ekxk 2 denote distribution x choose r 3l large enough z ffl 0 f g f entirely contained sr x 2 sr3 deltax f kxk 2 since diameter g f l a3 implies deltaf z thus ae srg a4 view a4 exists sequence curves ff n g lf n ae sr n deltaf n discussion preceding a2 assume without loss generality f n defined 0 1 consider set curves c 0 1 f 2 c iff kft 1 ae sr easy see c closed set uniform metric df also c equicontinuous family functions sup kftk uniformly bounded c thus c compact metric space arzelaascoli theorem see eg 16 since f n 2 c n follows exists subsequence f n k converging uniformly f 2 c simplify notation let us rename ff n k g ff n g fix x 2 r assume deltax f n deltax f let x deltax f triangle inequality k symmetry similar inequality holds deltax f n ekxk 2 finite exists 0 therefore since lipschitz condition f guarantees lf l proof complete assume pfx 2 closed convex set k let f curve lf l exists curve f g f ae k l f l proof domain f let ft unique point k well known ft satisfies hdelta deltai denotes usual inner product r see eg 17 inequality follows a6 since ft 1 continuous curve similar inequality shows x 2 k delta f appendix proof theorem 1 let f k denote curve k minimizing squared loss ie f deltaf existence minimizing f k easily shown using simpler version proof lemma 1 jf kn decomposed using standard terminology deltaf kn k called estimation error deltaf deltaf called approximation error consider terms separately first choose k function training data size n balance obtained upper bounds asymptotically optimal way approximation two curves f g finite length define nonsymmetric distance min note ae f g ie aef g independent particular choice parametrization within equivalence classes next observe diameter k g f g g 2 k x 2 k therefore prove b1 let x 2 k choose 0 0 deltax f arbitrary arc length parametrized curve 0 l 0 l 0 l polygonal curve vertices min note lg l 0 construction thus g 2 k thus every f 2 exists aef g l2k let g 2 k aef g l2k b2 conclude approximation error upper bounded deltaf estimation ffl 0 k 1 let kffl finite set curves k form fflcover k following sense f 2 k f 0 2 kffl satisfies sup explicit construction kffl given appendix c since f kn 2 k see 5 exists f 0 introduce compact notation x training data thus write b5 follows approximating property f 0 kn fact distribution x concentrated k b6 holds f kn minimizes delta n f f 2 k ordinary expectation type e deltax f f 2 kffl thus 2ffl union bound implies js kffl j denotes cardinality kffl recall hoeffdings inequality 18 states independent identically distributed real random variables 0 probability one u 0 since diameter k k thus 0 deltax f 2 probability one hoeffdings inequality implies b8 2ffl using identity nonnegative random variable write u 0 deltaf kn z 1p dt r b10 follows inequality follows setting log denotes natural logarithm following lemma proved appendix c demonstrates existence suitable covering set kffl lemma 3 ffl 0 exists finite collection curves kffl k sup ld v volume ddimensional unit sphere diameter k hard see setting gives upper bound logjs cl depend k combining b11 approximation bound given b3 results deltaf kn r rate deltaf kn approaches deltaf optimized setting number segments k proportional n 13 choice jf kn asymptotic convergence rate proof theorem 1 complete appendix proof lemma 3 consider rectangular grid side length ffi 0 r point grid associate voronoi region hypercube side length ffi defined set points closer points grid let k ffi ae k denote collection points grid fall k plus projections points grid k whose voronoi regions nonempty intersections k clearly min dffi c1 define kffl family polygonal curves f k vertices satisfying length constraint dffi c2 see kffl desired covering property let f 2 k arbitrary vertices f polygonal curve vertices definition k triangle inequality implies f satisfies c2 thus f 2 kffl hand without loss generality assume line segment connecting igamma1 line segment connecting linearly parametrized 0 1 dffi shows dffi2 follows b1 kffl fflcover k since x 2 k denote length ith segment f let dxe denotes least integer less x fix sequence define set f 2 kffl whose segment lengths generate particular sequence bound js kffl note first vertex 0 f 2 kffl l points k ffi contains many points voronoi cells intersecting k since diameter k exists sphere radius contains voronoi cells thus cardinality k ffi upper bounded v volume unit sphere r assume chosen since k possibilities choosing therefore c2 definition therefore arithmeticgeometric mean inequality implies thus hand c3 2k therefore number distinct sequences l kis upper bounded substituting obtain ld r principal curves surfaces principal curves algorithm vector quantizer design robust locally weighted regression smoothing scatterplots ice floe identification satellite images using mathematical morphology clustering principal curves principal curves revisited selforganization iterative kernel smoothing pro cess another look principal curves surfaces nature statistical learning theory introductory real analysis clustering algorithms principal points selfconsistent points elliptical distributions probabilistic theory pattern recognition fflentropy fflcapacity sets function spaces real analysis probability optimization vector space methods probability inequalities sums bounded random variables tr ctr balzs kgl adam krzyak piecewise linear skeletonization using principal curves ieee transactions pattern analysis machine intelligence v24 n1 p5974 january 2002 peter meinicke stefan klanke roland memisevic helge ritter principal surfaces unsupervised kernel regression ieee transactions pattern analysis machine intelligence v27 n9 p13791391 september 2005 b bhushan j romagnoli strategy feature extraction high dimensional noisy data proceedings 25th iasted international conference modeling indentification control p441445 february 0608 2006 lanzarote spain zhiguo cheng mang chen yuncai liu robust algorithm image principal curve detection pattern recognition letters v25 n11 p13031313 august 2004 j j verbeek n vlassis b krse ksegments algorithm finding principal curves pattern recognition letters v23 n8 p10091017 june 2002 jos koetsier ying han colin fyfe twinned principal curves neural networks v17 n3 p399409 april 2004 b lam h yan curve tracing algorithm using level set based affine transform pattern recognition letters v28 n2 p181196 january 2007 hujun yin data visualisation manifold mapping using visom neural networks v15 n89 p10051016 october 2002 jochen einbeck gerhard tutz ludger evers local principal curves statistics computing v15 n4 p301313 october 2005 alexander j smola sebastian mika bernhard schlkopf robert c williamson regularized principal manifolds journal machine learning research 1 p179209 912001 kuiyu chang j ghosh unified model probabilistic principal surfaces ieee transactions pattern analysis machine intelligence v23 n1 p2241 january 2001