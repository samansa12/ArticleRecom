interpolation models multiple hyperparameters traditional interpolation model characterized choice regularizer applied interpolant choice noise model typically regularizer single regularization constant noise model single parameter ratio alone responsible determining globally attributes interpolant complexity flexibility smoothness characteristic scale length characteristic amplitude suggest interpolation models able capture one flavour simplicity complexity describe bayesian models interpolant smoothness varies spatially emphasize importance practical implementation concept conditional convexity designing models many hyperparameters apply new models interpolation neuronal spike data demonstrate substantial improvement generalization error b introduction paper philosophy generalization follows best generalization achieved bayesian model wellmatched problem accurately implemented aim obtaining best generalization thus subsumed aim searching good models paper expand space interpolation models introducing additional hyperparameters demonstrate generalization performance real problem substantially improved traditional linear interpolation model h 1 characterized choice regularizer r prior probability distribution applied interpolant choice noise model n choice basis functions used represent interpolant may also important small number basis functions used typically regularizer quadratic functional interpolant single associated regularization constant ff noise model also quadratic single parameter fi example splines prior function yx kimeldorf wahba 1970 1 log ff z dx p p denotes pth derivative probability data measurements assuming independent gaussian noise log constants equations 1 2 functions ff fi respectively use distributions find probable yx obtain cubic splines interpolant quadratic regularizer quadratic 1 strictly prior improper since addition arbitrary polynomial degree yx constrained made proper adding terms corresponding boundary conditions 1 present implementations models enforce boundary conditions figure 1 inferred spike signal zebra finch neuron courtesy lewicki doupe california institute technology log likelihood probable interpolant depends linearly data values property define linear interpolation model models may optimized compared using bayesian methods reviewed mackay 1992 models fixed fi ratio fffi alone determines globally following attributes interpolant complexity flexibility smoothness characteristic scale length characteristic amplitude whilst terms may synonyms surely others describe distinct properties models able capture one flavour simplicity complexity interpolants smooth ness example able vary spatially 11 example neural spike modelling example function real system shown figure 1 action potential neuron deduced recordings 40 distinct events lewicki 1994 graph created fitting simple spline model data function one spiky region large characteristic amplitude short spatial scale elsewhere true function smooth however fitted function shown figure 1 controlled one regularization constant ff overfits noise right rough appearance plausibly smooth value ff appropriate fitting spiky region small rest curve would useful model capable capturing concepts local smoothness model prior better matched real world would require less data yield information quality furthermore different hypotheses compared broad priors introduce bias toward simpler hypotheses example ask whether one two distinct spike functions present data set traditional models prior small ff bias conclusion favour single spike function wellmatched priors results bayesian hypothesis comparison trusted paper discuss methods introducing multiple flavours simplicity complexity hierarchical probabilistic model computationally tractable way demonstrate new interpolation models multiple hyperparameters capture spatially varying smoothness prior work making use variable hyperparameters includes modelling data nongaussian innovations observation noise see eg west 1984 carter kohn 1994 shephard 1994 interpolation models propose might viewed bayesian versions variable bandwidth kernel regression technique muller stadtmuller 1987 aim new model also similar goal inferring locations discontinuities function studied blake zisserman 1987 traditional interpolation models difficulty discontinuities value fffi set high edges blurred model fffi lowered edge captured ringing appears near edge noise overfitted everywhere blake zisserman introduce additional hyperparameters defining locations edges models use computationally nonconvex finding good representatives posterior distribution challenging use graduated nonconvexity techniques find good solutions contrast attempt create new hierarchical models practical purposes convex tractable hierarchical modelling convexity bayesian statistical inference often implemented either gaussian approximations modes distributions markov chain monte carlo methods smith 1991 methods clearly better chance success posterior probability distribution model parameters hyperparameters dominated multiple distinct optima know probability mass one hump know need engage timeconsuming search probable optima might hope approximating distribution eg involving mode distribution might able capture key properties hump furthermore convex conditional distributions may easier sample say gibbs sampling methods gilks wild 1992 would useful conditional marginal probability distributions models log convex probability distribution log convex representation x variables matrix defined log p x 3 everywhere positive definite hard however make interesting hierarchical models conditional marginal distributions log convex introduce weaker criterion model conditionally convex variables divided groups every group distribution conditioned values variables log convex example conditionally convex model traditional interpolation model three groups variables data w parameters ff one hyperparameter probability distribution p djw convex gaussian distribution p wjd ff log convex w gaussian distribution p ffjw ff gamma distribution model conditionally convex guarantee marginal distributions variables unimodal example traditional models posterior marginals p wjd p ffjd necessarily unimodal good unimodal approximations often made mackay 1996 conjecture conditional convexity desirable property tractable model generalize spline model equation 1 model multiple hyperparameters conditionally convex demonstrate neural spike data discuss general principles hierarchical modelling multiple hyperparameters 3 new interpolation model replace regularizer equation 1 log z dx ffxy p ffx written terms hyperparameters thus constant equation 4 function ffx u becomes important ffx u inferred exponentiated quantity form linear interpolant using basis functions h x special case obtain traditional single alpha model representation chosen 1 embodies prior belief ffx smooth function x 2 model conditionally convex partial proof given section 4 implementing model optimize hyperparameters u fi maximizing marginal likelihood evidence z k dimensionality representation yx authors view empirical bayes approach controversial inaccurate wolpert 1993 widely used various names mlii closely related generalized maximum likelihood gu wahba 1991 ideal bayesian method would put proper prior hyperparameters marginalize optimization hyperparameters computationally convenient often gives predictive distributions indistinguishable mackay 1996 use discrete representation yx ffx finely spaced grid ffx u fff c j ffx c ug representation hessian log posterior sum banddiagonal terms log prior diagonal matrix log likelihood j gammarr logp yjd fffg fii gradient log evi dence use optimization log log p djfff c g 7 log p djfff c trace 2000 2000 figure 2 traditional models 2 diamondshaped points upper plots artifical data solid line shows probable interpolant found using traditional single alpha model predictive error bars dotted lines onestandarddeviation error bars lower row shows errors interpolant original function noise added make artificial data predictive error bars also shown contrast figure 3 2000 2000 figure 3 new models multiple hyperparameters 2 top row diamondshaped points artifical data solid line shows probable interpolant predictive error bars dotted lines onestandarddeviation error bars second row inferred ffx log scale contrast values 59 theta10 gamma7 20 theta 10 gamma6 inferred traditional models third row shows nine basis functions used represent ffx bottom row shows errors interpolant original function noise added make artificial data predictive error bars also shown top bottom graphs compared figure 2 9 table 1 comparison models artificial data first three columns give evidence effective number parameters rms error model applied data shown figures 23 fourth column gives rms error averaged four similar data sets model log fl rms avg rms evidence error error 31 demonstration made artificial data set adding gaussian noise standard deviation 1000 function depicted figure 1 function plays role experiments true underlying function presence actual roughness function believed unimportant since chosen noise level substantially greater apparent size roughness figure 2 shows data interpolated using traditional single alpha models 2 hyperparameter ff optimized maximizing evidence lewicki 1994 noise level oe set known noise level order spiky part data fitted ff set small value probable interpolant able models go close data points considerable overfitting everywhere predictive error bars large everywhere interpolated data two new models defined equations 4 5 2 set basis functions humpshaped functions shown figure 3 functions define scale length smoothness permitted vary scale length optimized roughly maximizing evidence new models nine hyperparameters u hyperparameters set maximizing evidence using conjugate gradi ents new models conditionally convex hoped maximization evidence would lead unique optimum ump however multiple optima evidence function hyperparam eters cause insurmountable problems found different optima using different initial conditions u optimization best evidence optima found initializing u way corresponded prior knowledge neuronal spike functions start end smooth gion set u initially fu h 0g prior knowledge formulated informative prior u optimization though would probably good idea practical purposes figure 3 shows solutions found using new interpolation models 2 inferred value ff small region spike elsewhere larger value ff inferred interpolant correspondingly smoother log evidence four models shown table 1 reported evidence values log e p djff mp make proper model comparison would integrate hyperparameters integration would introduce additional small subjective occam factors penalizing extra hyperparameters h 2 cf mackay 1992 root mean square errors interpolant original function noise added make artificial data given table 1 errors displayed bottoms figures 23 evidence value rms error values new models significantly superior traditional model table 1 also displays value effective number welldetermined parameters gull 1989 mackay 1992 fl hyperparameters optimized given z dx ffxy p smaller effective number parameters less overfitting noise smaller error bars interpolant become total number parameters used represent interpolant cases 100 32 model criticism interesting assess whether observed errors respect original function compatible onestandarddeviation error bars obtained shown together bottom figure 3 errors significantly larger error bars leftmost five data points small amount noise original function incompatible assumed boundary conditions omitting five data points find new model 95 errors expectation 95 sigma 14 95 errors either case exceed 25 standard deviations therefore see significant evidence observed errors incompatible predictive error bars 33 discussion new models offer two practical benefits first new models still fit spiky region well indeed errors slightly reduced give smoother interpolant elsewhere reduction overfitting allows information extracted given quantity experimental data neuronal spikes distinguishable given fewer samples quantify potential savings data fitted four models fake data equivalent independent observations function shown figure 1 data points noise level oe traditional p1 traditional p2 figure 4 average rms error traditional new models function amount data decreasing actual noise level artificial data figures tables shown thus far correspond case one observation figure 4 show rms error model function number data points averaged four runs different artificial noise achieve performance rms error new models traditional models require three times much data second new models greater values evidence mean probable models assuming omitted occam factors hyperparameters smaller evidence differ ences also means model comparison questions answered reliable way example wish ask two distinct spike types present several data sets one must compare two hypotheses hb explains data terms two spike functions ha uses one function model comparisons occam factors penalize extra parameters hb important used traditional interpolation model would obtain occam factors e 20 bigger obtained using new interpolation model broad priors bias model comparisons toward simpler models new interpolation model optimized produces prior effective number degrees freedom interpolant reduced prior less broad course inference openended expect models turn superceded even better ones close inspection figure 3 reveals smoothness assumption regularizer may imperfect know prior experience true functions spikiness confined small time interval new model gives jagged interpolant time interval spike function ffx assumed vary smoothly future models might include continuum alternative values p noninteger values p implemented fourier representation might also make sense characteristic length scale basis functions ffx represented shorter ff small advantages conferred new models accompanied significant increase computational cost optimization hyperparameters requires hessian matrix inverted small number times approaches implementation models multiple hyperparameters could considered confidence intervals present approach hyperparameters optimized likely small one could use markov chain monte carlo methods gibbs sampling hybrid monte carlo would involve similar computational load see neal 1993 excellent review used gibbs sampling software bugs thomas et al 1992 implement similar interpolation model gaussian noise level spatially varying function fix mackay 1995 4 generalizations 41 strategies making models multiple hyperparameter discuss generally construction hierarchical models multiple hyperparameters consider gaussian prior parameters w equivalent function yx earlier example various ways defining model multiple hyperparameters hyperparameter controls different flavour simplicity complexity w sum model firstly one might define inverse covariance matrix sum exp gamma2 fc c g arbitrary positive semidefinite matrices ff c 0 8c covariance sum model secondly one might define covariance matrix sum z exp gamma2 w hyperparameters c 0 8c exponential sum model thirdly take sum model form 10 though necessarily using matrices fc c g rewrite coefficients exponential sum hyperparameters u h 2 gamma1 1 exp gamma2 exp models different capabilities sum model implements paradigm starting flexible distri bution adding extra terms ff c c c kill degrees freedom model way introducing selective flexibility one hyperparameter ff c large way hyperparameters set undo stiffness introduced covariance sum model uses alternative paradigm starting stiff distribution introducing lacunae flexibility important difference two paradigms whereas sum model conditionally convex covariance sum model possible multiple optima hyperparameters even limit perfect data demonstrated explained subsequently exponential sum model interpolation model section 3 example intended combine best worlds consider case matrix elements ch nonnegative one hyperparameter u h increased introduces selective stiffness decreased introduces selective flexibility model reparameterization sum model still conditionally convex long pathological properties 42 convexity sum model give partial proof conditional convexity sum model straightforward confirm conditional distributions p djw fffg log convex nontrivial property p fffgjw fffgp wjfffg convex assume prior fffg defined sum model b covariance sum c exponential sum figure 5 toy problem probability contours figure shows likelihood two hyperparameters given hyperparameters ff ca ua horizontal axes ff b cb ub vertical axes figures top e 8 function shown contours equally spaced log probability convex examine second factor defining ff c ff log second derivative negative definite proof arbitrary x cd 43 toy illustration illustration examine conditional convexity model assigns zeromean gaussian distribution three component vector w distribution parameterized two hyperparameters simplicity assume w directly observed choice w favours priors give flexibility component 2 components 1 3 call flexibility sum model build sum two matrices diag110 diag011 figure 5a shows log probability log p wjfffg function log ff log ff b function convex covariance sum model build gamma1 sum diag110 diag011 letting diag c figure 5b shows log probability log p wjfcg function log c log c b function convex two alternative flavours flexibility compete give required variance component 2 w either may switch c large value may switch c b may switch intermediate degree exponential sum model build sum three matrices diag100 diag010 diag001 aid basis functions diag model number hyperparameters previous two models uses differently figure 5c shows log probability log p wjfug function u u b function convex two alternative flavours flexibility embodied compete destructively sum model starts flexibility adds constraints stiffness kill degrees freedom w covariance sum representation starts stiffness adds selective flexibility create required degrees freedom covariance sum model convex different forms flexibility compete account data struggle existence potential piece flexibility penalized occam factors det term encouraging stay switched contrast alternative ways introducing stiffness sum model exponential sum model compete two sorts stiffness compatible data switched without incurring penalty sum model convex exponential sum model conjecture pushes flexibility limits convexity believe ideas may relevant design computationally tractable gaussian process models nonlinear regression williams rasmussen 1996 44 represent covariance matrix paper used interpolation neural spike data test bed new models discuss another task general principles discussed may apply imagine wish model correlations k variables assumed gaussian covariance matrix v varies variables x varying covariance matrix vx param eterized assume representation vux used would like parameterization vu satisfy following desiderata 1 setting parameters u produce valid positive definite matrix v 2 positive definite matrix v realizable unique value parameters u 3 parameterization inverse continuous differentiable 4 representation treat indices covariance matrix sym example first row v treated differently second row 5 u kk degrees freedom number independent elements symmetric matrix v 6 finally would like representation conditionally convex given one vectors conditional probability u log convex desiderata rule obvious representations v raw matrix v permitted violates desideratum 1 triangular decomposition violates 4 eigenvector eigenvalue representation violates 235 variance component model representation used example gu wahba 1991 covariance sum representation violates desiderata 5 6 ideas paper motivate following representation conditionally convex let k dimensional let r kgamma1 unit spherical surface v unit vector space parameters introduce symmetric matrix u constrained positive definite inverse sum outer products thus representation satisfies desiderata since may self evident include sketch proof half property 2 namely mapping u v one one first transform eigenvector basis u orthogonal transformation leaves r kgamma1 invariant prove eigenvectors feg u also eigenvectors v let fw g components v eigenvector basis eigenvectors eigenvalues u satisfy ue equation 20 write z integrand 6 j antisymmetric w w j integral zero cases thus e e z v eigenvectors u eigenvalues given z mapping u v one one mapping eigenvalues u f u g eigenvalues v f v g one one differentiate equation 23 obtain jacobian jacobian fullrank mapping one one u z jacobian sum outer products positive vectors z given z either defines positive semidefinite positive definite matrix matrix positive semidefinite direction h nonzero measure integral r kgamma1 integral r kgamma1 vector h thus matrix full rank mapping u v one one problem representation involves highdimensional integral propose practical purposes following approximation c expv c fv c g c fixed unit vectors lying r kgamma1 selected either random systematically representation conditionally convex able limit c 1 conclusions work builds data modelling philosophy previously illustrated work automatic relevance determination model neural networks mackay 1994 neal 1996 use huge flexible model essentially infinite number parameters control complexity model sophisticated regularizers models large numbers hyperparameters carefully designed practically implemented hyperparameters reduce effective number degrees freedom model manner appropriate properties data leading substantial improvements generalization error acknowledgements djcm thanks isaac newton institute matsumoto waseda uni versity hospitality radford neal mike lewicki david mumford brian ripley helpful discussions rt thanks matsumoto support also thank referees helpful feedback r visual reconstruction gibbs sampling statespace models applied statistics 41 minimizing gcvgml scores multiple smoothing parameters via newton method developments maximum entropy data analysis correspondence bayesian estimation stochastic processes smoothing splines bayesian modeling classification neural signals neural computation 6 5 bayesian interpolation bayesian nonlinear modelling prediction competition probabilistic networks new models new meth ods hyperparameters optimize variable bandwidth kernel estimators regressioncurves bayesian learning neural networks partial nongaussian statespace bayesian computational methods bugs program perform bayesian inference using gibbs sampling outlier models prior distributions bayesian linear regression gaussian processes regression use evidence neural networks tr