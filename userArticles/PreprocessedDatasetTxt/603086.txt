efficient approximate planning continuous space markovian decision problems montecarlo planning algorithms planning continuous statespace discounted markovian decision problems mdps smooth transition law finite action space considered prove various polynomial complexity results considered algorithms improving upon several known bounds b introduction mdps provide clean simple yet fairly rich framework studying various aspects intelligence eg planning wellknown practical limitation planning mdps called curse dimensionality 1 referring exponential rise resources required compute even approximate solutions mdp size mdp number state variables increases example conventional dynamic programming dp algorithms value policyiteration scale exponentially size even used subroutines sophisticated multigrid algorithms 4 moreover curse dimensionality akin kind special algorithm shown result chow tsitsiklis 3 recently kearns et al shown certain online tree building algorithm avoids curse dimensionality discounted mdps 8 recently result also extended partially observable mdps pomdps authors 7 bounds homepage httpvictoriamindmakerhuszepes two papers independent size state space scale exponentially 1 eective horizontime discount factor mdp paper consider another online planning algorithm shown scale polynomially horizontime well price assume regularity class models new results true particular restrict stochastic mdps nite action spaces state space assume transition probability kernel mdps subject lipschitzcondition jpx states action 2 l p 0 given xed number upper bounds polylogarithmic l p kk 1 denotes 1 norm vectors another restriction quite common literature assume uniform boundedness transition probabilities bound shall denoted k p immediate rewards bound denoted k r bounds depend dimension state space 1 idea considered algorithms largely originates algorithm considered rust 12 studied even restricted class problems proved following result fix mdp call random realvalued function v domain optimal mean e v optimal value function underlying mdp kk maximumnorm expectation taken random function given 0 algorithm builds random cache c given x 2 x using cache c algorithm draws samples optimal mean rust shown phases algorithm polynomial jaj k r l p l r k r k p 11 lipschitz factor immediate rewards note rusts bound scales polynomialy eective horizontime need extend algorithm planning way property kept following algorithm yielding almost optimal policies high probability algorithm fix random sample n consider v state x using markovs inequality get pk could compute r dy contraction argument would show drawing samples sucient ensuring optimality probability least 1 note bound number samples depends polynomially 1 ideally one would like polylogarithmic dependence 1 furthermore carrying calculations requires evaluation jaj integrals state space must approximated 2 clearly bound follows way majorize bound obtained markovs inequality still depend polynomially 1 one also obtain uniformly optimal policy redrawing random samples 1 bounds developed kearns et al exhibit dependence state space one might either want reuse samples drawn earlier draw new samples second approach easier analyze whilst rst one may appear elegant query algorithm oline phase bounds number samples number steps computation computed tedious calculations case well manner main result article sharpening estimates sketched application maximal inequalities particular estimate polylogarithmic 1 l p instead polynomial drop lipschitz continuity restriction immediate rewards bound number samples polylogarithmic size action space well also derive novel bounds complexity calculating uniformly optimal policies using algorithm outlined organization paper follows section 2 provide necessary background algorithm given section 3 main result paper formulated section 4 proof main result given section 5 conclusions drawn section 6 preliminaries assume reader familiar basics theory mdps readers lack necessary background referred book dynkin yuskevich 6 recent books 2 10 21 notation refers p norm vectors l p norm functions depending type argument lip p denotes set mappings lipschitzcontinuous norm kk means exists positive constant l 0 st kfx fyk p l kx yk p domains mappings suppressed l called k k p lipschitz constant f lip p denotes set mappings whose kk p lipschitz constant larger mapping called contraction norm kk p 1 brevity maximumnorm kk 1 denoted kk let v set vv vv mapping v set natural numbers denoted n set reals r 2 n denotes map product ttimes say holds v 2 v general denote elementary event probability space consideration lhs means lefthandside rhs means righthandside dene bx k 0 bk x g 22 model let us consider continuous space discounted mdp given x p state space action space p measurable transition table 1 pseudocode algorithm model parameters 1 compute n dened theorem 518 2 draw independent samples uniformly distributed x 3 compute according 2 4 let v 5 repeat times v g 6 let g 7 return law measurable function called reward function 0 1 discount factor assume followings assumption 21 nite assumption 22 exist constants k assumption 23 exists constant k r 0 st krk 1 k r 3 algorithm first give pseudocode online planning algorithm yields uniformly optimal policies introduce formalism required present main results sketches proofs pseudocode algorithm seen table 1 note expense increasing computation time one may downscale storage requirement algorithm 2 step 3 algorithm omitted equation 2 must used steps 5 6 note one may still precompute normalizing factor 2 speeding computations since storage requirements normalizing factors depend linearly n introduce notation necessary state main results let bx dened z 2 arbitrary integral understood follows x stationary policy dened finally let bellmanoperator dened a2a assumptions known unique xedpoint v called optimal value function v known uniformly bounded also known sta optimal sense given initial state total expected discounted return resulting execution maximal execution policy means execution action x whenever actual state x policy called myopic greedy wrt function case action set nite existence myopic policy guaranteed given uniformly bounded function v let x us denote dened 0 otherwise 2 similarly a2a dened throughout paper going work independent random variables uniformly distributed x 3 x 1n used denote x random operators tn tn tn respective equations tn tn called random bellmanoperator intended approximate true bellmanoperator 3 uniform distribution used simplicity sampling distribution support covering x could used algorithm modied appropriately importance sampling form ideal sampling distribution far clear since single sampleset used estimate innite number integrals form ideal distribution subject future research dened dened dened a2a nally let dened a2a following proposition heart proposed computational mechanism proposition 31 integer 0 x 1n x 1n particular proof inspection remark 32 according proposition 31 one compute two phases rst could call oline phase second could call online phase oline phase one computes n dimensional vector v timewhilst second phase one computes value evaluating n x second step takes thus whole procedure takes ot time easy see procedure takes algorithm whose pseudocode given formulated follows assume given xed 0 basis l compute integer 0 another integer n 0 time need compute action randomized policy state x rst draw random sample 4 assume basic algebraic operations reals take o1 time storage realnumber takes o1 storagespace also assume 1n stored x 1n compute v random action x computed evaluating resulting policy shown optimal another computationally less expensive method hold random sample x 1n xed accordingly compute v computation x using 4 costs ojajn 2 steps policy shown optimal high probability theorem 513 rst result shows algorithm described yields uniformly approximately optimal policy high probability polynomial complexity theorem 41 let log1 let log log let stationary policy dened easy see complexity algorithm meets indeed said next result shows modied fully online algorithm given table 1 yields uniformly approximately optimal policy also polynomial complexity theorem 42 let let n smallest integer larger log let stochastic stationary policy x 0 1 dened 1n policy dened optimal given state x random action computed time space polynomial k r k r k p log l p jaj 11 rough outline proof follows assumptions pollards maximal inequality cf 9 ensures given xed function v 0 small high probability 5 using triangle inequality one reduces comparison varies zero n 1 precisely one shows dierences small small well using proposition easy prove maximal inequality one use standard contraction arguments prove inequality bounds dierence value policy approximately greedy wrt function terms bellmanresiduals see eg 14 plan use inequality tn calculations yield theorem 41 proven policy selects good actions ie actions f suitable good close optimal next relax condition selecting good actions selecting good actions high probability policies shown good well cf lemma 5 8 finally shown policy good high probability selects good actions high probability thus turn must good nish proof theorem 518 5 proof prove theorem next three sections first prove maximal inequalities random bellmanoperators tn next show extended powers tn nally apply prove main results 5 must rely pollards maximal inequality instead simpler chernobounds state space continuous supnorm involves supremum state space result derived two steps using idea rust 12 51 maximal inequalities random bellman operators shall need auxilliary operators easier approach maximal inequal ities let tn v a2a tn v xg need denitions results theory empirical processes cf 9 tn v a2a tn v xg need denitions results theory uniform deviations cf 9 denition 51 let r set epsiloncover 2 exists element st 1 set covers denoted denition 52 covering number set dened number log n called metric entropy let z r n let f r r dene following theorem due pollard see 9 theorem 53 pollard 1984 let n 0 integer set measurable functions let x iid random variables sup n 8e e elegant proof theorem found 5pp 492 general assumptions needed make result sup measurable measurability problems however well understood shall worry detail readers keep worrying understood probability bounds except main result outerinnerprobability bounds whichever appropriate note nal result work measurable sets therefore need refer outerinner probability measures firstly extend theorem functions mapping r mm corollary 531 let n 0 integer set measurable functions let x iid random variables sup n 8e proof let denote positive negativ parts f respectively f theorem 53 sucient prove denote lhs 8 let 2 since n f proving corollary denition 54 let 2 n 0 let 0 let let grid dened ties broken favor points smaller coordinates remark 55 kx p xk 1 jgridj lemma 56 let k log log n a2a tn v v proof shall make use corollary 531 let fx 1n z n v easily z n v x order estimate n fx 1n construct cover fx 1n claim cover fx 1n chosen appropriately order prove let us pick arbitrary element z n v x fx 1n thenn therefore cover fx 1n remark 55 n fx 1n corollary 531 log log 10 holds shall prove similar result tn using ideas proof corollary theorem 34 12 lemma 57 let k log log log a2a tn v v proof let us pick triangle inequality tn v v tn v tn v tn v v let tn v x tn v x simple algebraic manipulations get tn v x tn v x since assumption jv x j k tn v x tn v x r dened observe tn ex therefore 14 tn v x tn v x k ex tn ex note inequality holds also p n x 0 taking supremum x yields tn v tn v k e tn e 13 tn v v k tn e e tn v v therefore a2a tn v v a2a tn v v statement lemma follows using lemma 56 one lets n 52 maximal inequalities powers random bellman operator first need proposition relates xed point contraction operator operator approximating contraction proposition 58 let b banachspace x operators b 1 0 proof prove statement induction namely prove holds 0 statement obvious assume already proven 17 1 triangle inequality kt rst term rhs bounded turn bounded induction hypothesis second term hand bounded 15 since inequality 17 holds well thus proving proposition cite next proposition without proof proof elementary well known proposition 59 let bellmanoperator maps bk x follows main result section lemma 510 let 0 integer 0 0 log log log n p 3 a2a proof let g proposition 59 b 0 bk x lemma 57 n p 2 1 a2a tn v v 1 let elementary event a2a tn v v 1 show a2a tn proof nished obviously a2a tn construction b 0 since 1 1 note a2a tn v v holds v 2 bx since choice n a2a tn 1 also 1 moreover since proposition 58 applied choice together 20 yields 19 thus proving theorem 53 proving optimality algorithm first prove inequality similar 14 use approximate value functions approximate operators note since nite policy dened lemma exists proof compare k since known converge v v spectively firstly write dierence k form telescoping sum using triangle inequality 2 lip 1 using thus a2a hand kt therefore 22 combined 23 yields a2a taking limes superior sides k 1 yields lemma note get back tight bounds 14 6 next lemma exploits v bellman error related quality approximation 6 note lemma still holds replace special operators operators lemma 512 let log1 assume a2a proof use lemma 511 let us bound bellmanerror rst a2a second term bounded used therefore lemma 511 a2a using denition 24 get kv v k proving lemma position prove following theorem theorem 513 let let log log log let stationary policy dened proof proof combines lemmas 512 510 firstly bound a2a tn let dened tn depend x tn tn a2a a2a therefore lemma 510 lemma 512 kv v k probability least 1 order nish proof main theorem prove discounted problems stochastic policies generate optimal actions high probability uniformly good result appears context nite models 8 though due space limits without proof completeness since model innite state space also sake completeness present proof start denition optimal actions prove three simple lemmas denition 514 let 0 consider discounted mdp x p call set set optimal actions elements set called optimal lemma 515 let x 0 1 stationary stochastic policy selects optimal actions x 2 x 2 x 0 follows 2 x proof denition immediate kt a2a consider telescopicing sum therefore next lemma applied show two policies close evaluation functions lemma proof similar proposition 58 lemma 516 let b banachspace bk f g assume 0 kt holds v 2 bk 1 kt xed points 1 2 also satisfy kv proof proof almost identical proposition 58 one proves induction kt holds 0 v 2 bk xed indeed inequality holds assuming holds 1 1 one gets showing rst part statement second part proven taking limes superior sides 1 ready prove lemma showing policies choose optimal actions high probability uniformly good stochastic policy selects optimal actions probability least 1 kv v k proof let denote probability selecting nonoptimal actions state x policy dened 0 otherwise claim 0 close let a2a since kt v k k a2a a2a 2 therefore bk x satisfy assumptions lemma 516 xed point 0 v v 0 respectively construction 0 selects optimal actions thus lemma 515 combining 27 get kv v k nishing proof ready prove main result paper theorem 518 let log1 let log log choose let let stochastic stationary policy x 0 1 dened x 1n policy dened tn v optimal given state x random action computed time space polynomial 1 k log l p jaj 11 proof second part statement immediate cf remark 32 bound time computation space requirement algorithm 7 rst part x x 1n theorem 513 claim kv v k 0 x 1n 2 us pick let note therefore using denition x get x 1n 2 0 1 shows lemma 517 policy dened 31 optimal ie substituting denitions 0 yields result 6 conclusions work article considered online planning algorithm shown avoid curse dimensionality bounds following rusts original result markovs inequality improved several ways bounds depend polylogarithmically lipschitz constant transition probabilities depend lipschitz constant immediate rewards dropped assumption lipschitzcontinuous immediate 7 assuming normalization factors transition probabilities stored reward functions number samples depends cardinality action set polylogarithmic way well interesting note although bounds depend polylogarithmically lipschitz constant transition probabilities characterizing fast dynam ics depend polynomially bound transition probabilities characterizing randomness mdp therefore perhaps surprisingly kind montecarlo algorithms faster dynamics easier cope less random dynamics transition probability functions consequence result many interesting questions arise example dierent variants proposed algorithm could compared multigrid versions versions using quasirandom numbers versions use importance sampling prac tice one would course work truncated probabilities save computational resources eect truncation needs explored well note lipschitz condition p replaced appropriate condition metricentropy pxj proofs still go therefore proofs extended holderclasses transition laws local lipschitz classes eg case one would need use bracketing smooth functions sobolev classes etc one interesting problems extend results innite action spaces extension surely needs regularity assumptions put dependence transition probability law reward function actions would also interesting prove similar results discrete mdps factorized representation presented algorithm may nd applications economic problems without mod ications 11 also work applications deterministic continuous statespace nite action space control problems partially observable mdps discrete spaces also combination lookahead search interesting practical point view algorithm tried practice standard problems caronthehill acrobot observed yield quite good performance even number samples quite law also observed boundary eects interfere negatively algorithm details experiments however described elsewhere r dynamic programming dynamic programming deterministic stochastic models complexity dynamic programming optimal multigrid algorithm continuous state discrete time stochastic control controlled markov processes approximate planning large pomdps via reusable trajectories sparse sampling algorithm nearoptimal planning large markovian decision processes convergence stochastic processes markov decision processes structural estimation markov decision processes using randomization break curse dimensionality tr dynamic programming deterministic stochastic models complexity dynamic programming markov decision processes sparse sampling algorithm nearoptimal planning large markov decision processes dynamic programming ctr jared go thuc vu james j kuffner autonomous behaviors interactive vehicle animations proceedings 2004 acm siggrapheurographics symposium computer animation august 2729 2004 grenoble france jared go thuc vu james j kuffner autonomous behaviors interactive vehicle animations graphical models v68 n2 p90112 march 2006