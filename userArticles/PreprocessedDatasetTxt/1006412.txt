automatic performance evaluation web search engines measuring information retrieval effectiveness world wide web search engines costly human relevance judgments involved however business enterprises people important know effective web search engines since search engines help users find higher number relevant web pages less effort furthermore information used several practical purposes study introduce automatic web search engine evaluation method efficient effective assessment tool systems experiments based eight web search engines 25 queries binary user relevance judgments show method provides results consistent humanbased evaluations shown observed consistencies statistically significant indicates new method successfully used evaluation web search engines b table desirable features web search evaluation according gordon pathak 1999 features 17 hawking et al features 811 1 searches motivated genuine informationneeds web users 2 search intermediary employed primary searchers informationneed captured fully much context possible transmitted intermediary 3 suciently large number searches must conducted obtain meaningful evaluations search engine eectiveness 4 major search engines considered 5 eective combination specic features search engine exploited ie queries submitted engines may dierent 6 user needs information must make relevance judgments hawking et al 2001 assumes independent judges 7 experiments prevent bias towards search engines eg blinding randomizing search outputs b use accepted information retrieval measures c employ statistical tests measure performance dierences search engines 8 search topics represent range informationneeds desired draw conclusions 9 result judging appropriate type query submitted eg queries may need oneline answer 10 document presentation like web browser images viewable necessary possible follow links 11 dead links count useless answers addition seven items specied gordon pathak 1999 study also provided table 1 note approach dierent two studies since aim automate evaluation process study satisfy features given table 1 except features 2 5 feature 2 apply us furthermore contrary suggestion authors studies believe genuine user queries used way true everyday behavior search engines measured feature 5 requires expert searchers claiming best query formulation used specic search engine disagree since web searchers real life casual users experiment conformant real life sit uations queries modied expert searchers used slight syntactic stylistic modications query allowed make use features specic search engine boolean operators although satisfy feature 6 acknowledge need make relevance judgment original provider informationneed nd time consuming costly web environment standardized test collection complete relevance judgments hawking et al 2001 dynamic huge environment makes almost impossible determine collection obtain human relevance judgments since would cumbersome people judge many web pages therefore study suggest automatic way determine relevance judgments studies mentioned perform evaluations using human relevance judgments dierent kind work study mowshowitz kawaguchi 2002 measures performance search engines using overlap urls matching pages study uses similarity response vector collection search engines response vector particular search engine dened bias evaluate performance search engine study denes response vector particular search engine vector urls making result sets returned search engine response vector collection search engines union response vectors search engines order calculate bias norm vectors response vector determined using number occurrences url two ways calculating bias considered study one taking account another one ignoring order urls mowshowitz kawaguchi 2002 study considers urls retrieved search engines number occurrences url consider content urls believe necessary performance evaluation search engines another study chowdhury soboro 2002 work presents method comparing search engine performance automatically based rank known item search result study initial querydocument pairs constructed randomly search engine mean reciprocal rank computed querydocument pairs query document pairs reasonable unbiased method could useful however construction querydocument pairs requires given directory may always possible also soboro nicholas cahan 2001 study involves ranking retrieval systems without relevance judgments proposes new methodology trec environment however related work replaces human relevance judgment randomly selected documents pool study number documents selected randomly treated relevant number taken average number relevant documents appearing trec pool per topic year consistency method human relevance judgment measured experimenting factors eg pool depth ranking retrieval systems using methodology correlates positively ocial trec rankings performs relevance judgment automatically like study however unlike study relevant document specication done random selection ie contents documents considered relevance judgments 3 experimental environment 31 user queries informationneeds process measuring retrieval eectiveness requires user queries construction queries asked participantsmore technically known subjectsfall 2001 students cs533 students two professors computer engineering department bilkent university dene informationneeds english purpose stated needs form writing query query subject query description related keywords method obtained 25 informationneeds prepared 19 participants paper also referred users query evaluations completed early spring 2002 queries covered broad range topics topics number queries topic follows computer science 16 education 1 internet 2 literature 1 music 2 plants 1 sports 1 travel 1 average query length 380 words average number boolean operators per query 168 participants used boolean operators eight queries boolean operator used 32 selection search engines eight search engines selected conduct experiment alltheweb altavista hotbot infoseek lycos msn netscape yahoo rst search engines selected gordon pathak 1999 study engines used study caused problems opentext example could reached search engines infoseek magellan found using overturecom returned results therefore eliminated one chose infoseek case excite observed responding queries submitted software also eliminated chose three extra search engines alltheweb msn netscape choosing three search engines consider northernlight search engine queries addition others returned links pages required access fee considered choosing google popularity however googles search engine modied prohibit outside access time experiment due software general prohibition example software previous study mowshowitz kawaguchi 2002 also unable retrieve results search engine however googles results evaluated yahoo partner time experiments since yahoo presents googles results displaying results human compiled directory fact results google used results yahoo directory googles results also retrieved netscape however time experiment netscapes results also included results dierent resources netscape returned results dierent sections following popular related searches partner search results netscape recommends reviewed web sites web site categories google results results rst three sections considered rst section displayed alternative search terms second section included paid listings overture third section presented tools services results found particular section section appear netscapes results cannot used evaluate google thus dierent yahoos results 33 search process step used queries collected participants submitted chosen search engines software designed interact specied search engines return results boolean queries slightly modied search engines would recognize operators example query polygon triangulation code algorithm submitted alltheweb polygon triangulation code algorithm since settings search engine equates words parentheses boolean operator case humanbased evaluation participants decided relevance top 20 pages search engine binary fashion asked specify degree relevance prevent possible bias users given list urls obtained taking union 20 top urls returned search engines performed judgment task using cgi based user interface using web browsers cgi programs lists combined search results saves user relevance judgments participants unable associate search results search engines deciding relevancy subjects asked judge given web page pay attention contents web pages linked original one ie follow links page 4 automatic method awseem awseem rst user queries submitted search engines consideration query top b 200 web pages search engine determined content pages pages available downloaded saved build separate document collection web abstraction query process deadlinks ie unavailable pages regarded useless answers page selected one search engine one copy downloaded kept corresponding document collection accordingly total number web pages collection query less maximum value n b downloaded pages converted plain text ascii representation nonword tokens page structure information eg html pdf ps tags deleted awseem works like metasearch engine ranks pages query according similarity dened user informationneeds note compared query informationneeds provide better description users looking future implementations awseem selected terms user designated favorite web page used purpose ranking awseem processes search engine one time examines top returned search engine consideration checks pages also appear similar top number downloaded pages ds document assumed relevant eectiveness measures computed corresponding value also known dcv awseem precision recall measures calculated dierent values consistencies compared humanbased evaluations values fig 1 provides graphical representation working principles awseem experiments value 50 100 used fifty chosen approximation maximum number relevant pages determined query humanbased evaluations 44 also use 100 examine eect using larger value values elaborated future experiments nd appropriate value achieving higher consistency humanbased evaluations awseem uses vector space model querydocument matching ie similarity calcu lation ranking salton mcgill 1983 according model document loaded web page represented vector terms found document query represented vector terms found informationneed specication user ie query query subject description query search keywords process stemming employed stop word list used eliminate frequent words document ranking similarity query actually case user informationneed document calculated using conventional vector product formula salton mcgill 1983 summation product corresponding term weights query document weight term calculated using product term frequency tf inverse document frequency idf term frequency occurrence count term document query whereas inverse document frequency collection case individual query document collection created using downloaded pages dependent factor favors terms fig 1 automatic search engine evaluation process generalized description search engine sei concentrated documents collection multiplying normalization factor normalizes value normalization factor used equalize length document vectors study reported salton buckley 1988 suggests dierent choices calculating term weights dierent document query vector sizes chose query matching function dened retrieval environments long documents short queries salton buckley 1988 study signied tfcnfx 5 experimental results evaluation 51 evaluation measures calculation eectiveness search engines measured terms precision recall done human judgments awseem results compared test consistency two approaches study measure relative recall relative recall calculation denominator term total number relevant documents database replaced total number relevant documents retrieved search engines either automatic judgment human judgment case automatic human handled within precision recall values computed various top dcvs ranging 1 20 rest paper use acronym dcv rather clarity known web users usually look result pages search engines therefore 20 suitable choice maximum dcv spink jansen wolfram saracevic 2002 due small number documents examined users precision gains importance recall search engine eectiveness evaluation however cases measuring recall important recall shows much known relevant documents search engine managed retrieve number ratio documents acceptable case consider rst 20 documents observe recall performance search engines typical web search session suggested hull 1993 average precision recall search engine computed xed dcv using computed precision recall values query point eliminates bias could caused calculating precision recall single chosen dcv reminded average precision calculation used dierent average precision seen relevant documents baezayates ribeironeto 1999 compute average precision values point chosen dcv average precision computed following way first search engine average precisions query pdcv computed every dcv consideration using average precision individual queries able calculate average precision search engine given dcv average precision around dcv padcv calculated taking average p1 p2 pdcv values example want compute average precision search engine around dcv 3 pa3 compute average average precision values 1 2 3 ie pa3 p1 example hypothetical search engines b c given tables 2 3 procedure also applied compute average recall around dcv table example showing precision values search engines b c query search engine precision various dcvs 3a000 precision values three queries 1 6 dcv 6 3 indicates relevant document retrieved rank dcv table average precision dcv padcv average precision around dcv values search engines b c computed precision values given table 2 1 6 dcv 6 3 search p1 p2 p3 pa1 pa2 pa3 engine p 1 p 2 p around 13 16 13 13 14 518 52 precision recall values various dcvs fig 2a c show respectively pa10 ra10 results awseem50 ie 50 terms fig 1 humanbased evaluations gures search engines sorted according pa10 ra10 values calculated using humanbased evaluations also note average precision recall values human awseem shown dierent scales illustrate strong association ranks search engines according eec tiveness two methods methods display similar results determining rank search engines recall calculations whereas correlation precision calculations weaker precision calculations awseem ranks rst search engine dierently results search engines close fig 2b average precision around dcv 20 search engine using awseem 50 displayed search engines sorted average precision around dcv using humanbased evaluations terms human judgments top precision performers altavista yahoo lycos case awseem 50 best performing search engines yahoo msn altavista lycos altavista lycos performance worst performer cases netscape similar results displayed humanbased evaluation awseem terms ranking search engines based precision consider average recall calculations humanbased evaluation awseem 50 also display similar results seen fig 2d around dcv 20 search engines gure sorted according average recall values around dcv 20 humanbased evaluation methods show top two performing search engines altavista yahoo worst performer netscape calculations presented awseem 50 also computed using awseem 100 fig 3a c show pa10 ra10 values awseem 100 humanbased evaluation search engines sorted according pa10 ra10 values calculated using humanbased evaluations fig 3b pa20 ra20 values displayed respectively gures show methods determine top performing search engines altavista yahoo worst performer netscape look overall results say considering pages awseem ie 100 shown fig vs 50 shown fig 2 positive impact consistency humanawseem results gures help explain statistical correlation two methods presented following two sections 53 statistical consistency human automatic judgments table 4 shows number relevant pages per query returned top 20 results search engine table provided give insight actual numbers involved experiments explained relevancy documents separately decided method h column represents number relevant documents top 20 based human table number relevant pages query top 20 results search engine query alltheweb hotbot infoseek lycos msn netscapealtavista yahoo 9 total 134 80 167 115 85 76 118 101 145 95 123 110 70 42 164 134 judgment likewise column represents results awseem 100 example alltheweb search engine query 1 11 pages found relevant human judgment three pages found relevant awseem cannot say performance evaluation set documents used two approaches however previous research shows dierences human relevance judgments aect relative performance retrieval systems voorhees 2000 purpose provide automatic pseudorelevance judgments relative performance assessment full details experimental data please refer web site httpwwwcsbilkentedutrrabianwebdata web site contains various excel tables consistency awseem human judgment intuitively compared using rank search engines according total number relevant pages retrieve queries refer last row table 4 according values ranking search engines using humanbased evaluation best worst follows 1 altavista total 167 retrieved relevant documents 2 yahoo 164 3 lycos 145 4 alltheweb 134 5 msn 123 6 infoseek 118 7 hotbot 85 8 netscape 70 ranking according awseem follows 1 yahoo 134 2 altavista 115 3 msn 110 4 infoseek 101 5 lycos 95 6 alltheweb 80 7 hotbot 76 8 netscape 42 intuitive comparison shows approaches consistent terms determining best altavista yahoo worst hotbot netscape performers see correlation results two methods ie test consistency human automatic results one may use average rank search engines based precisions individual queries using information spearmans rank correlation method indicates high level consistency human awseem judgments correlation value 097 signicant 001 level want see relationship pairs padcv radcv measures human judgment awseem results spearmans correlation suitable variables relationship among ranks interest pearson r correlation suitable estimating relationships variables variables reasonably normal since precision values lie along continuous scale 0 1 appear skewed pearson correlations used determine relationships hence correlation pairs padcv radcv measures various dcvs preferred use pearson r correlation assess signicance r conduct hypothesis test null hypothesis human awseem precision correlation zero let denote signicance level test error probability rejecting null hypothesis null hypothesis correct experiments high pearson rvalues allow us reject null hypothesis lower values reject null hypothesis 005 r 07070 strong correlation reject 001 r 08340 strong correlation fig 4a provides pearson r correlation human judgment awseem 100 results pa20 scattergram shows high degree linear relationship human awseem precision results least squares regression line provided plot illustrates variability data consistency humanbased evaluation awseem 50 awseem 100 based precision measured pearson r signicant dcv 20 correlation human judgment awseem 50 observed 07330 sig nicant 005 correlation human judgment awseem 100 08675 fig 4 ac pearson r correlation scattergram human judgment awseem 100 pa20 ra20 bd pearson r correlation human judgment awseem 50 100 precision recall various dcvs signicant 001 fig 4b shows pearson rvalues human judgment awseem 50 100 based average precision values around various dcvs gure shows considering pages awseem ie 100 vs 50 positive impact consistency humanawseem results furthermore dcv 20 highest consistency observed example awseem 100 shows signicant correlation 07070 r 08340 pa10 signicant correlation pa15 pa20 r 08340 case awseem 50 signicant correlation value observed pa20 consider average recall correlations human judgment awseem 100 strongly positive 09258 signicant 001 illustrated fig 4c using ra20 ie average recall around dcv 20 example pearson rvalues human judgment awseem 50 100 based average recall values around various dcvs seen fig 4d awseem shows strongly signicant correlation dcv 5 r 08340 seen number pages ie value considered awseem need increased since correlation humanawseem results 50 100 pages almost indistinguishable starting dcv 5 54 statistical distinction among search engines finally performed analysis variance among search engines based precision recall scores calculated tukeys highest signicant dierences hsd determine subsets search engines statistically indistinguishable 005 methods using precision recall values dcv 10 ie p10 r10 20 ie p20 r20 general tukeys hsd could distinguish search engines awseem humanbased evaluations using precision recall values around dcv 10 20 pa10 pa20 ra10 ra20 therefore used precision recall values dcv 10 20 p10 p20 r10 r20 checked agreement human awseem results based tukeys test tukeys hsd dcv 10 using average precision values p10 humanbased evaluation clustered search engines two dierent subsets distinguishing best performing search engine altavista worst performing search engine netscapesee table 5 cluster members shown letters b according tukeys results interpretation cluster table anova would nd dierence among search engines 005 although tukeys hsd awseem using top 50 ie 50 pages cannot distinguish search enginessee table 6using top 100 pages forms two clusters yahoo best performer netscape worstsee table 7 best performers humanbased evaluation awseem dierent percentage average precision shows top two performers yahoo altavista methodssee tables 5 7 tukeys hsd dcv 20 using average precision values p20 humanbased evaluation clustered search engines two dierent subsets distinguishing best performing search engines yahoo altavista worst performing search engine netscapesee table 8 behavior observed awseem using top 50 pagessee table 9and table tukeys hsd precision dcv 10 p10 005 humanbased evaluation search n average subset 005 engines precisionnetscape 25 01600 hotbot msn infoseek alltheweb altavista sig r engines precision 005netscape 25 7200e02 alltheweb hotbot infoseek altavista msn sig 0072 table tukeys hsd precision search n average subset 005 engines precisionnetscape 25 01200 hotbot alltheweb infoseek msn altavista yahoo sig table tukeys hsd precision dcv 20 p20 005 humanbased evaluation search n average subset 005 engines precisionnetscape 25 01400 hotbot infoseek msn alltheweb yahoo altavista sig awseem using top 100 pagessee table 10 actually awseem 50 distinguishes yahoo top performer results tukeys hsd humanbased evaluation search n average subset 005 engines precisionnetscape 25 4200e02 hotbot infoseek msn alltheweb altavista yahoo sig table tukeys hsd precision dcv 20 p20 005 awseem 100 search n average subset 005 engines precisionnetscape 25 8400e02 hotbot infoseek msn alltheweb altavista yahoo sig awseem dcv 10 20 using precision p10 p20 reveal top two search engines yahoo altavista worst performing search engine netscape analysis based recall dcv 10 20 r10 r20 humanbased evaluation forms subsets distinguishing search engines whereas analysis awseem cannot statistically distinguish search engines top 50 100 pages however looking rankings consistency awseem humanbased evaluation observed determining top two performers altavista yahoo worst two performers netscape hotbot summary tukeys hsd shows humanbased awseem results agreement determining best worst performing search engines recommended dcv tukeys hsd 20 since shown fig 4 dcv pearson rvalues saturate reach maximum value precision recall performance measures statistically signicant high pearson r correlation among precision recall values methods shows humanbased awseem evaluations used interchangeably measuring eectiveness search engines additional tests reported repeated experiments limiting number terms 1000 per document average document size original experiment 7707 terms used two approaches one considering rst 1000 words one randomly selecting 1000 words downloaded pages page size smaller 1000 words used words pearson r correlation tests show results consistent full text web pages words automatic method applicable smaller system resources 6 conclusions study present automatic method performance evaluation web search engines measure performance search engines examining various numbers top pages returned search engines check consistency human automatic evaluations using observations experiments use 25 queries look performance eight dierent search engines based binary relevance judgments users experiments show high level statistically signicant consistency automatic humanbased assessments terms eectiveness also terms selecting best worst performing search engines knowing eective web search engines satisfying current informationneed important personal business enterprise level however denition best changes due changing informationneeds users changing quality nature search engines example people business enterprises may work solution dierent problems dierent time instances search engines may change indexing ranking strategies web coverage accordingly dierent informationneeds dierent search technologies eective search engine may dierent hence search engine performance needs tested done quite often dynamically changing environment ecient eective automatic evaluation method awseem valuable tool since evaluations expensive due human time involved method several practical implications example used implementation recommenders set sample queries collected individual users similar interests search engines best results determined recommended users betterquality search engines performance measurement web search engines provides motivation vendors systems improve technology custommade metasearch engines best performing search engines selected according informationneeds set users individual user used implementation specialized metasearch engines meng yu liu 2002 benchmarking tools test search engines dierent subject areas subject areas interest set queries pooled search performance particular search engine measured approach also used compare performance dierent search engines subject areas respect note awseem measures eectiveness search engines also eectively denes metasearch engine ie ranking intersecting downloaded pages top pages search engines appears metasearch engine implementation promising looking consistent awseem humanbased evaluation results human participants experiments represent small portion search engine user community large similar argument also true queries used experiments however since aim show consistency humanbased awseem results signicant eect results similar outcomes expected different user groups voorhees 2000 future experiments algorithms may considered examine consistency humanrelevance judgments since better ranking procedure may produce consistent results user assessments one concern ranking algorithm used awseem one might argue using ranking algorithm search engine evaluated would cause bias however since collected documents dierent search engines overlap documents indexed search engine low lawrence giles 1999 search engine uses algorithm retrieve documents found relevant awseem fur thermore awseem document ranking use user information needs much longer version user queries term weights used awseem search engine would dierent factors lead dierent rankings even set docu ments however best evaluate search engine using ranking algorithm awseem number parameters awseem relies n number search engines b number pages downloaded search engine number top awseem pages used measure eectiveness dcv number top search engine pages used measure eec tiveness choice dcv reects current web search engine use practice choice number pages downloaded search engine b simulated approach used gordon pathak 1999 study current choice 200 parameter appears reasonable experiments number top awseem pages used measure eec tiveness show 100 good choice b n values used since stabilizes correlation values human awseem results choices used experiments researchers may also perform sensitivity analysis regarding parameters making nal choices also future experiments increasing number queries might give accurate results awseem provides generic search engine evaluation approach used context currently using trec data performing various sensitivity analysis experiments environment related results published upcoming article acknowledgements would like thank fall 2001 cs533 information retrieval systems students david davenport bilkent university writing evaluating queries used study would also like thank anonymous referees jon patton pedrito uriah maynardzhang valuable comments r inventing internet modern information retrieval methods measuring search engine performance time anatomy large scale hypertextual web search engine finding information world wide web retrieval e using statistical testing evaluation retrieval experiments review web searching framework future research information retrieval web accessibility information web building e assessing bias search engines weighting approaches automatic text retrieval introduction modern information retrieval searching web public queries variations relevance judgments measurement retrieval e tr termweighting approaches automatic text retrieval using statistical testing evaluation retrieval experiments anatomy largescale hypertextual web search engine inventing internet finding information world wide web variations relevance judgments measurement retrieval effectiveness information retrieval web searching web review web searching studies framework future research ranking retrieval systems without relevance judgments building efficient effective metasearch engines methods measuring search engine performance time modern information retrieval automatic evaluation world wide web search services assessing bias search engines introduction modern information retrieval measuring search engine quality esex ecommerce ctr huyentrang vu patrick gallinari machine learning based approach evaluating retrieval systems proceedings main conference human language technology conference north american chapter association computational linguistics p399406 june 0409 2006 new york new york frank mccown joan smith michael l nelson lazy preservation reconstructing websites crawling crawlers proceedings eighth acm international workshop web information data management november 1010 2006 arlington virginia usa rabia nuray fazli automatic ranking information retrieval systems using data fusion information processing management international journal v42 n3 p595614 may 2006 ziv baryossef maxim gurevich random sampling search engines index proceedings 15th international conference world wide web may 2326 2006 edinburgh scotland marcos andr gonalves edward fox aaron krowne pvel calado alberto h f laender altigran da silva berthier ribeironeto effectiveness automatically structured queries digital libraries proceedings 4th acmieeecs joint conference digital libraries june 0711 2004 tuscon az usa bernard j jansen paulo r molina effectiveness web search engines retrieving relevant ecommerce links information processing management international journal v42 n4 p10751098 july 2006