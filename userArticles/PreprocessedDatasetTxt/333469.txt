subspace interior conjugate gradient method largescale boundconstrained minimization problems subspace adaptation colemanli trust region interior method proposed solving largescale boundconstrained minimization problems method implemented either sparse cholesky factorization conjugate gradient computation reasonable conditions convergence properties subspace trust region method strong fullspace versioncomputational performance various large test problems reported advantages approach demonstrated experience indicates proposed method represents efficient way solve large boundconstrained minimization problems b introduction recently coleman li 1 2 3 proposed two interior reflective newton methods solve boundconstrained minimization problem ie min algorithms interior methods since iterates fx k g strict interior feasible region ie ug two methods differ line search update iterates used 2 3 trust region idea used 1 however cases convergence accelerated use novel reflection technique line search method version appears computationally viable largescale quadratic problems 3 main objective investigate solving largescale boundconstrained nonlinear minimization problems 11 using largescale adaptation trustregion interior reflective tir approach proposed 1 tir method 1 outlined fig 1 elegantly generalizes trust region idea unconstrained minimization boundconstrained nonlinear minimization g k crucial role diagonal affine scaling matrices k c k become clear x2 attractive feature tir method 1 main computation per iteration solving research partially supported applied mathematical sciences research program kc0402 office energy research us department energy grant defg0290er25013a000 nsf afosr onr grant dms8920550 advanced computing research institute unit cornell theory center receives major funding national science foundation ibm corporation additional support new york state members corporate research institute computer science department cornell university ithaca ny 14850 z computer science department center applied mathematics cornell university ithaca ny 14850 tir method 1 1 compute define quadratic model 2 compute step k based subproblem 3 compute 4 ae k set x 5 update k specified updating trust region size k 1 ae k set k1 2 0 2 ae k 2 3 ae k j set otherwise set fig 1 tir method minimization subject bounds standard unconstrained trust region subproblem min method mor e sorensen 4 directly applied 12 cholesky factorizations matrices structure h k computed efficiently however method unsuitable largescale problems hessian h k explicitly available sparse cholesky factorizations expensive recently sorensen 5 proposed new method solving subproblem 12 using matrix vector multiplications nonetheless effectiveness approach largescale minimization particularly context trust region algorithm yet investigated take view solving full space trust region subproblem 12 costly largescale problem view shared steihaug 6 proposes approximate conjugate gradient approach steihaugs approach 12 seems viable although computational experience see table indicates important negative curvature information missed causing significant increase number minimization iterations paper propose alternative approximate subspace trust region approach stir verify reasonable conditions convergence properties stir method strong fullspace version explore use sparse linear algebra techniques ie sparse factorization preconditioned conjugate gradients context approach addition demonstrate benefits affine scaling reflection subspace techniques computational results first 11 affine scaling technique outperforms classical dikin scaling 7 least context algorithm second examine method without reflection show reflection technique substantially reduce number minimization itera tions third computational experiments support notion subspace trust region method promising way solve largescale boundconstrained nonlinear minimization problems compared steihaug 6 approach subspace approach likely capture negative curvature information consequently leads better computational performance finally subspace method competitive often superior active set method lancelot 8 paper organized follows x2 briefly summarize existing tir method provide computational comparison subspace trust region method steihaug algorithm context unconstrained minimization x3 introduce subspace method stir discuss convergence properties x4 issues concerning computation negative curvature directions inexact newton steps discussed x5 computational results provided indicating performance typically impaired using inexact newton step concluding remarks appear x7 convergence analysis stir method included appendix 2 tir method section briefly review fullspace tir method 1 sketched fig 1 method closely resembles typical trust region method unconstrained minimization min x2 n fx key difference presence affine scaling diagonal matrices k c k next briefly motivate matrices tir algorithm trust region subproblem 12 affine scaling matrices k c k arise naturally examining firstorder kuhntucker conditions 1 feasible point l x local minimizer x bounds characterization expressed nonlinear system equations vector vx 2 n defined 1 n ii iii iv g 0 l nonlinear system 21 differentiable everywhere nondifferentiability occurs v hence avoid points maintaining strict feasibility ie restricting x k 2 intf newton step 21 defined satisfies j v nthetan corresponds jacobian jvxj diagonal component diagonal matrix j v equals zero sigma1 components l u finite j define j v equation 23 suggests use affine scaling transformation x transformation reduces constrained problem 11 unconstrained problem local minimizer 11 corresponds unconstrained minimizer new coordinates x details see 1 therefore reasonable way improve x k solve trust region subproblem min subproblem 25 equivalent following problem original variable space min addition close resemblance unconstrained trust region method tir algorithm strong convergence properties explicit conditions steps optimality describe conditions tir algorithm requires strict feasibility ie x use ff denote step obtained k possible stepback strict feasibility let k denote minimizer along k within feasible trust region ie ff definition implies explicit conditions yield first secondorder optimality analogous trust region methods unconstrained minimization 1 assume p k solution min s2 nf k fi q fi q two positive constants condition as3 necessary firstorder convergence as4 together as3 necessary secondorder convergence conditions as3 as4 extensions convergence conditions unconstrained trust region methods particular assumptions exactly required trust region methods unconstrained minimization problems satisfaction conditions as3 as4 difficult example one choose k k k minimum values however lead efficient computation process 3 2 utilized reflection technique permit possible reduction objective function along reflection path boundary found 3 2 reflection process significantly enhances performance minimizing general quadratic function subject simple bounds x pr fig 2 reflection technique computational results paper k determined best three points corresponding k p r k denotes piecewise direction path p k reflected first boundary encounters see fig 2 appreciate convergence results approach observing role affine scaling matrix k components x approaching correct bounds sequence directions fgammad gamma2 becomes increasingly tangential bounds hence bounds prevent large step size along fgammad gamma2 k g taken components x approaching incorrect bounds fgammad gamma2 points away bounds relatively large angles corresponding diagonal components k relatively large g k points away bounds hence reduction least implies scaled gradient fd gamma2 converges zero ie firstorder optimality scaling matrix used approach related different scaling typically used affine scaling methods linear programming affine scaling matrix affine commonly used affine scaling methods linear programming formed distance variables closest bounds scaling matrix 2 k equals affine j note even case employ square root quantities used define affine investigate subspace adaptation tir demonstrate effectiveness reflection idea affine scaling technique consider random problem instances molecule minimization 9 10 minimize quartic subject bounds variables table 1 2 list average number iterations ten random test problem instances entry required different techniques comparison notation front number indicates average number least number iteration number exceeds 1000 maximum allowed instance details algorithm implementation given x6 table 1 demonstrates significant difference made single reflection difference reflection 341 417 668 834 936 reflection 714 2101 4254 3022 4085 stir algorithm without reflection number iterations 100 200 400 800 1000 unconstrained k 386 473 614 727 936 affine affine k 5174 6176 5173 1000 1000 comparison stir scaling dk dikin scaling affine number iterations rows without reflection following without reflection k determined best two points based determined best three points based k p r reflection superiority using reflection technique clearly demonstrated problem table 2 compare computational advantage selection k affine difference scaling matrix differentiate problems unconstrained solution bounds active solution constrained solution observe unconstrained problems significant difference two scaling matrices however constrained problems tested choice k clearly superior observe k used number iterations constrained problem roughly corresponding unconstrained problem affine k hand number iterations constrained problem much larger corresponding unconstrained problem 3 approximation trust region solution unconstrained minimization two possible ways approximate fullspace trust region solution unconstrained minimization byrd schnabel schultz 11 suggest substituting full trust region subproblem unconstrained setting min k lowdimensional subspace implementation employs twodimensional choice another possible consideration approximation 12 steihaug idea 6 also proposed largescale unconstrained minimization setting nutshell steihaug proposes applying method preconditioned conjugate gradients pcg current newton system either negative curvature revealed current approximate solution reaches boundary trust region newton system residual sufficiently reduced believe subspace trust region approach better captures negative curvature information compared steihaug approach 6 justify conducted limited computational study unconstrained minimization setting implement subspace method subspace k defined gradient direction g k output modified preconditioned conjugate gradient mpcg method applied linear newton output either inexact newton step defined direction negative curvature detected mpcg algorithm mpcg given greater detail fig 11 appendix b implementation steihaug method also found appendix b steihaug subspace implementations wrapped standard trust region framework unconstrained minimization problem methods preconditioning matrix used g k diagonal matrix computed g k strategy used update k see x6 details let used subspace method k delta k g steihaug method 6 used twenty different unconstrained nonlinear test problems four test problems described 12 bound constraints removed problems erosenbrock epowell taken 13 last two problems molecule problems mole1 mole3 described 9 10 problems number variables n 260 minimization algorithm terminates kgk use parameter fig 11 fig 12 tables 3 4 compare steihaug subspace methods described terms number minimization iterations total number conjugate gradient cg iterations table 3 shows problems negative curvature detected table 4 shows problems negative curvature detected although included function values gradient norms upon termination virtually methods problems since values essentially among two methods discuss difference iterations counts difference minimization cg iteration counts plotted fig 3 fig 4 notable table 3 graphs fig 3 strikingly similar results steihaug subspace methods minimization method stops within two iterations cases furthermore methods take identical number total cg iterations except problem brown1 steihaug method takes four iterations negative curvature encountered shown table 4 fig 4 iteration counts method similar problems problems however steihaug method takes iterations problems difference substantial particularly true problems chainwood mole1 mole3 chainwood problem 3 fig 4 total difference iteration counts minimization cg problem subspace steihaug subspace steihaug 1 brown1 27 29 39 43 2 brown3 6 6 6 6 3 broyden1a 11 11 81 81 4 broyden1b 5 5 34 34 5 broyden2b 7 7 71 71 6 chainsing 22 22 188 188 7 cragglevy 21 21 125 125 8 degensing 22 22 188 188 9 epowell 10 gensing 22 22 83 83 11 tointbroy 7 7 58 58 12 var 43 43 5590 5590 comparison positive curvature encountered number iterations minimization cg problem subspace steihaug subspace steihaug 1 augmlagn 36 29 267 228 2 broyden2a 22 19 247 196 3 chainwood 156 988 3905 3878 4 erosenbrock 44 46 52 86 5 gerose 23 33 166 165 6 genwood 58 63 304 275 7 mole1 46 119 460 376 8 mole3 125 186 6311 5356 comparison negative curvature encountered number iterations positive curvature problems minimization iterations excess steihaug iterations excess subspace iterations positive curvature problems iterations fig 3 comparison subspace steihaug trust region methods unconstrained problems negative curvature problems minimization iterations negative curvature problems iterations fig 4 comparison subspace steihaug trust region methods unconstrained problems explicitly noted beyond scale graph general subspace method take cg iterations problems negative curvature extra relatively inexpensive cg iterations reduce total number minimization iterations problem mole3 difference cg iterations explicitly noted fig 4 beyond scale graph closer examination behavior two algorithms indeed shows negative curvature encountered methods take similar steps case trust region large enough methods fig 11 fig 12 stop conditions number cg iterations displayed table 3 nature algorithms steihaug method detects negative curvature subspace approach however subspace algorithm detects negative curvature steihaug method may terminate finds negative curvature converge local minimizer quickly subspace method important role negative curvature plays supported fact subspace method often moves substantial negative curvature direction steihaug method overlooks negative curvature furthermore trust region radius k small steihaug method likely stop early miss negative curvature thus appears effectiveness steihaug idea decreases nonlinearity increases 4 stir method supported discussion x3 propose largescale subspace adaptation tir method 1 bound constrained problem 11 moving unconstrained subspace approach boxconstrained setting seems natural replace full trust region subproblem 12 following subspace subproblem min k smalldimensional subspace n eg twodimensional subspace twodimensional subspace trust region subproblem 25 selected span two vectors k g negative curvature direction k suggests form k directions fd gamma2 g subspace formulations succeed achieving optimality examine issue detail clear including scaled gradient direction gamma2 k k satisfying as3 guarantee convergence point satisfying firstorder optimality conditions let us assume fx k g converges firstorder point x guarantee x also secondorder point following conditions must met firstly clear sufficient negative curvature condition must carried unconstrained setting 14 end require sufficient negative curvature matrix captured indefinite ie k must contain direction w w k secondly important solution 41 lead sufficiently large step potential difficulty running bound constraint immediately difficulty avoided stepsize sequence along trust region solution direction bounded away zero subsequently define definition 41 direction sequence fs k g largestepsize lim inf k1 jd 2 fast local convergence desired subspace k also contain sufficiently accurate approximation newton direction gamma1 k positive definite k inexact newton step k problem 11 defined approximate solution accuracy kr k kk select twodimensional subspaces satisfying three properties thus guarantee quadratic superlinear convergence secondorder point answer theory yes subspace adaptation tir algorithm stir fig5 example subspace method capable achieving desired properties ensure convergence solution solution sequence subspace trust region subproblems 41 need largestepsize lemma 1 indicates achieved set two sequences uniformly independent vectors sense lim inffkz largestepsize lemma 1 assume fw k g fz k g largestepsize kd k w moreover lim inf k1 fkz solution sequence fp k g subproblem 41 largestepsize proof proof straightforward omitted stir method natural extension condition as4 necessary secondorder optimality following assume p k solution min s2 nf k fi q two positive constants theorem 2 proof provided appendix formalizes convergence properties stir theorem 2 let level set compact f twice continuously differentiable l let fx k g sequence generated stir algorithm fig5 1 as3 satisfied kuhntucker condition satisfied every limit point stir method 1 compute define quadratic model 2 compute step k based subspace subproblem subspace k set 3 compute 4 ae k set x 5 update k specified fig1 determine subspace assume w largestepsize let small positive constant k positive definite k positive definite gamma2 gamma2 end end fig 5 stir method minimization subject bound constraints 2 assume as3 as5 satisfied w k fig 5 contains sufficient negative curvature information whenever k indefinite ie every limit point fx k g nondegenerate limit point x first secondorder necessary conditions satisfied b x isolated nondegenerate limit point first secondorder necessary conditions satisfied x c nonsingular limit point x fx k g k positive definite positive definite fx k g converges x iterations eventually successful fd k g bounded away zero degeneracy definition 1 definition 42 point x 2 f nondegenerate index established principle possible replace fulldimensional trust region subproblem twodimensional variation however equally strong convergence properties stir hinges obtaining guaranteed sufficient negative curvature direction largestepsize discuss next 5 computing negative curvature directions largestepsize possible principle satisfy sufficient negative curvature requirement 42 largestepsize property answer yes let u k unit eigenvector k corresponding negative eigenvalue ie easily verified convergent subsequence lim k1 min sequence fd gamma1 largestepsize however computationally feasible compute exact eigenvector u k therefore approximations short cuts order compute approximate eigenvectors largestep good approximation eigenvector corresponding extreme eigenvalue usually obtained lanczos process 15 using lanczos method k initial vector approximate eigenvectors jth step computed krylov space context algorithm vectors gamma1 natural choices initial vector applying lanczos method key observation following sequence fd gamma1 k g largestepsize sequence gamma1 retains property assume w k computed vector lanczos method contains sufficient negative curvature information respect k verified based recurrence relation fd gamma1 k g largestepsize lanczos vectors f retain orthogonality w k krylov space k clear fw largestepsize words order generate negative curvature direction sequence largestepsize orthogonality needs maintained lanczos process fortunately discussed 16 quite reasonable assume distinct eigenvalues original matrix approximated well orthogonality lanczos vectors well maintained since interested direction sufficient negative curvature expect computed loss orthogonality occurs second cheaper strategy employ modified preconditioned conjugate gradient scheme eg mpcg fig12 unfortunately process guaranteed generate sufficient negative curvature nonetheless indicated 17 mpcg output satisfy largestepsize property finally consider modified cholesky factorization eg 18 obtain negative curvature direction assume f indefinite fd k g obtained modified cholesky method demonstrate fd k g largestepsize nondegeneracy assumption negative curvature direction computed modified cholesky method see 18 page 111 satisfies l k lower triangular matrix p k permutation matrix e j k j k th elementary vector j bounded nonnegative diagonal matrix without loss generality assume p argue contradiction fd k g largestepsize property assume fd k g property l l k lower triangular matrix unit diagonals clear moreover definition 24 k first components fd k bounded implies fv j k converges zero modified cholesky factorization matrix indefinite positive definite impossible sufficiently large k using definition 24 converges matrix form positive nondegeneracy assumption therefore conclude fd k g largestepsize 6 computational experience demonstrate computational performance stirmethod given fig5 report experience modified cholesky conjugate gradient implementations examine sensitivity stir method starting point finally limited comparisons sbmin lancelot 8 also made implementation stir compute k using reflective technique shown fig2 exact trust region updating procedure given fig6 updating trust region size k 1 ae k 0 set 2 ae k 2 0 set 3 ae k 2 j set 4 ae k j otherwise fig 6 updating trust region size experiments carried sun sparc workstation using matlab environment stopping criteria used follows stop either negative curvature detected define also impose upper bound 600 number iterations first report results stir method using modified cholesky factorization table 5 lists number iterations required standard testing problems details problems see 12 results paper number iterations number objective function evaluations problem sizes vary 100 10 000 results table 5 indicate testing problems least number iterations increases slightly problem size moreover comparison unconstrained problems presence bound restrictions seem increase number iterations depicted pictorially fig 7 graph problem size plotted versus iteration count problem corresponding points connected show iteration count relates problem size second set results stir algorithm using conjugate gradient implementation use algorithm mpcg fig12 find directions needed form subspace k stopping condition applied relative residual mpcg 0005 results shown table 6 fig 8 problems iteration counts low steady exception problem var c 10 000 variables iteration count jumps 86 one problem 100 200 500 1000 10000 genrose c 11 11 gensing u 24 25 25 26 27 gensing c degensing c 28 28 28 28 29 genwood broyden2a c 14 19 17 19 19 cragglevy u var c stir method exact newton steps number iterations problem size iterations fig 7 stir performance exact newton steps 100 1000 1000050problem size iterations fig 8 stir method inexact newton steps several degenerate problems included test set tighter bound j relative residual mpcg could decrease number minimization iterations problem note stir exact newton steps takes 38 iterations however change would also increase amount computation conjugate gradient iterations next include results indicate stir method fairly insensitive starting point results table 7 obtained using exact newton steps problems dimension 1000 results table 8 obtained using conjugate gradient implementation also problems 1000 variables starting points follows original suggested starting point according 12 upper starts variables upper bounds lower starts variables lower bounds middle starts midpoint bounds zero starts variable zero origin upperlower starts odd variables upper even variables lower bounds lowerupper reverse perturb starting point slightly necessary strictly feasible note problem brown3 c iteration count shown starting middle origin gradient undefined starting points results also shown graphically fig 9 fig 10 graphs clear implementations stir fairly robust comes starting problem 100 200 500 1000 10000 genrose u 21 21 21 gensing u 23 23 24 24 25 gensing c chainsing u 21 21 21 cragglevy u 26 26 cragglevy c 26 26 26 26 27 stir method inexact newton steps krkkgk 0005 number iterations starting point problem original upper lower middle zero uplow lowup genrose c 11 27 33 15 16 43 27 cragglevy c 26 26 34 37 stir method exact newton steps number iterations points contrast active set methods starting point dramatic effect iteration count last contrast performance stir method using conjugate gradient option sbmin algorithm active set method lancelot software package 8 particular choose problems negative curvature present appears active set solution may difficult find expect stir method outperform active set method situations indeed found case problems use default settings lancelot adjusted stir stopping conditions comparable stringent first consider constrained convex quadratic problem results given table 9 show proposed stir method markedly superior order magnitude sbmin problem cg total number conjugate gradient iterations sbmin takes many iterations problem starting point near bounds method misidentifies correct active set solution takes many iterations recover proposed stir method strictly interior method moves directly solution without faltering started point table summarizes performances stir sbmin set constrained problems exhibiting negative curvature problems 12 except last two constrained differently display negative curvature stir significantly better problems probably due fact negative curvature better exploited subspace trust region approach starting point problem original upper lower middle zero uplow lowup gensing c 28 degensing c 33 43 37 42 37 37 44 genwood c 28 stir method inexact newton steps number iterations orig low mid zero uplo loup50iterations fig 9 stir method exact newton steps varied starting points inexact stir sbmin iteration cg iteration cg stir inexact newton steps vs lancelot sbmin convex quadratic number iterations orig low mid zero uplo loup50iterations fig 10 stir method inexact newton steps varied starting points inexact stir sbmin problem 100 1000 10000 100 1000 10000 augmlagn u 34 genwood u 62 67 63 439 952 554 genwood nc stir inexact newton steps vs lancelot sbmin negative curvature exists number iterations steihaug trust region method sbmin employs consistent results presented x3 eg see table 4 7 conclusion based trustregion interior reflective tir method 1 proposed subspace tir method stir suitable largescale minimization bound constraints variables particular consider twodimensional stir subspace formed scaled gradient inexact exact newton steps negative curvature direction designed reported variety computational experiments results strongly support different components approach subspace idea use novel affine scaling matrix modified cholesky factorization conjugate gradient variations reflection technique moreover preliminary experimental comparisons code sbmin lancelot 8 indicate proposed stir method significantly outperform activeset approach largescale problems r interior convergence reflective newton methods largescale nonlinear minimization subject bounds reflective newton method minimizing quadratic function subject bounds variables minimization large scale quadratic function subject ellipsoidal constraint conjugate gradient methods trust regions large scale optimization iterative solution problems linear quadratic programming lancelot fortran package largescale nonlinear optimization release molecule problem determining conformation pairwise distances family trustregionbased algorithms unconstrained minimization strong global convergence properties testing class methods solving minimization problems simple bounds variables approximate solution trust region problem minimization twodimensional subspaces matrix computations lanczos algorithms large symmetric eigenvalue computations inexact reflective newton methods largescale optimization subject bound constraints practical optimization tr ctr detong zhu new affine scaling interior point algorithm nonlinear optimization subject linear equality inequality constraints journal computational applied mathematics v161 n1 p125 1 december jiaju zheng shuying cao hongli wang wenmei huang hybrid genetic algorithms parameter identification hysteresis model magnetostrictive actuators neurocomputing v70 n46 p749761 january 2007 l n vicente local convergence affinescaling interiorpoint algorithm nonlinear programming computational optimization applications v17 n1 p2335 oct 2000 amit jain david blaauw slack borrowing flipflop based sequential circuits proceedings 15th acm great lakes symposium vlsi april 1719 2005 chicago illinois usa manfred weiler ralf botchen simon stegmaier thomas ertl jingshu huang yun jang david ebert kelly p gaither hardwareassisted feature analysis visualization procedurally encoded multifield volumetric data ieee computer graphics applications v25 n5 p7281 september 2005 r deng p davies k bajaj nonlinear fractional derivative model large uniaxial deformation behavior polyurethane foam signal processing v86 n10 p27282743 october 2006