exactly learning automata small cover time present algorithms exactly learning unknown environments described deterministic finite automata learner performs walk target automaton step observes output state chooses labeled edge traverse next state learner means reset access teacher answers equivalence queries gives learner counterexamples hypotheses present two algorithms first case outputs observed learner always correct second case outputs might corrupted random noise running times algorithms polynomial cover time underlying graph target automaton b introduction paper study problem actively learning environment described deterministic finite state automaton dfa learner viewed robot performing walk target automaton beginning start state step observes output state chooses labeled edge traverse next state learner means reset returning start state particular investigate exact learning algorithms access teacher answer equivalence queries give learner counterexamples hypotheses also study case environment noisy sense fixed probability j learner observes incorrect output state angluin 2 shown general problem exactly learning finite automata performing walk target automaton without access equivalence oracle hard information theoretic sense even learner means reset due existence subclass automata often referred combinationlock automata 1 central property combination lock automata used angluins hardness result hardtoreach states particular single accepting state reachable learner performs particular walk length n called combination n number states automaton walks result zero sequence outputs therefore every exact learning algorithm combination lock automaton algorithm requires exponential time algorithm randomized require exponential expected time thus natural question arises whether exact learning automata remains hard assume underlying graph target automaton certain combinatorial properties small cover time cover time defined smallest integer every state q random walk length starting q visits every state probability least 12 automaton low cover time cannot hardtoreach states since random walk whose length order cover time likely reach states known graph polynomial cover time exactly probability assigned stationary distribution edge 2 least inverse polynomial size graph inferred results described 21 several natural classes directed graphs known property one important class class graphs indegree node equal outdegree 1 class includes underlying graphs permutation automata automata simulate undirected environments replacing undirected edge states q q 0 two oppositely directed edges q q 0 q 0 q labels directed edges arbitrary necessary learning algorithm given upper bound cover time target automaton turn bound number states impossible learning algorithm even approximate cover time unknown automaton efficient automata n states output label q output label qn 1 start state q1 q one outgoing edge labeled 0 one labeled 1 one outgoing edges goes q1 goes q i1 state qn outgoing edge directed q1 another directed sequence inputs cause automaton traverse state sequence q1 order called combination single accepting state reachable learner performs walk corresponds combination walks result zero sequence outputs refer stationary distribution determined markov chain corresponding graph state outgoing edges assigned equal probability manner due difficulty distinguishing combination lock automaton exponential cover time one state automaton selfloops cover time 1 paper show automata polynomial cover time exactly learned polynomial time noisefree noisy settings described previously present probabilistic learning algorithms following holds high probability performing single walk target automaton algorithm constructs hypothesis automaton used correctly predict outputs states path starting state hypothesis completed algorithms run time polynomial cover time noisy setting allow running time algorithm depend polynomially 1ff ff lower bound 12 gamma j restrict attention case edge labeled either 0 1 output state either 0 1 results easily extendible larger alphabets algorithms apply ideas noreset learning algorithm rivest schapire 22 turn uses angluins algorithm 3 subroutine angluins algorithm algorithm exactly learning automata teacher answer membership queries equivalence queries note teacher answers membership queries equivalent means reset use subroutine algorithm variant angluins algorithm similar one described 2 procedure learning means reset lies first key overcoming need teacher answers equivalence queries start procedure learner performs single random walk whose length order cover time target automaton proceeds performing additional walks starting start state determined initial random walk using simple argument similar argument used 2 show needed procedure terminate polynomial time hypothesis automaton equivalent target automaton state target automaton passed initial walk 22 use homing sequence overcome absence reset informally homing sequence sequence whenever executed corresponding output sequence observed uniquely determines final state reached shown 22 homing sequence known learning algorithms use reset easily converted learning algorithms use reset idea homing sequence executed two different stages algorithm use reset two possibly different unknown states output sequence observed know stages reached state thus executing homing sequence essentially plays role performing reset problem remains construct homing sequence sequence known able construct homing sequence without aid teacher rivest schapires learner needs teacher answer equivalence queries order construct homing sequence rough idea performing random walk whose length bounded cover time prior execution current candidate homing sequence repeating step subroutine learns reset enough times discover current candidate true homing sequence improve thus pay absence teacher giving algorithm whose running time depends cover time hence algorithm efficient cover time polynomial number states noisy setting learning problem becomes harder since outputs observed may erroneous learner means reset problem easily solved 26 running noisefree algorithm repeating walk large enough number times majority output observed correct output however learner means reset encounter several difficulties one major difficulty clear learner orient since executing homing sequence high probability observe correct output sequence order overcome difficulty adapt looping idea presented dean et al 9 dean et al study similar setting noise rate fixed function current state present learning algorithm problem however assume algorithm either given distinguishing sequence target automaton generate one efficiently high probability 3 known simple examples illustrating automata distinguishing sequence remains true restrict attention automata small cover time natural question arises whether results improved require learner learn target automaton approximately learner means reset may natural assume allow learner actively explore environment goal perform well respect underlying distribution walks starting starting state model equivalent pac learning membership queries since angluins algorithm 3 modified pac learning algorithm membership queries dfas efficiently learnable model however learner means reset thus performs single walk know natural notion approximately correct learning recent work freund et al 13 results improved follows freund et al consider problem learning probabilistic output automata finite automata whose transition function deterministic whose output function probabilistic namely given string whenever performing walk corresponding string certain state reach state however similarly model studied dean et al 9 output observed time determined probabilistic process flipping coin bias depends state reached case biases state either j 1 gamma j essentially problem learning deterministic automata presence noise give algorithm paper 13 learning algorithm given runs time polynomial cover time target automaton restrictions biases state repeated games computationally bounded opponents another motivation work game theoretical problem finding optimal strategy playing repeated games computationally bounded opponent scenario two players refer one player second opponent step player opponent choose action predefined set actions according strategy strategy possibly probabilistic mapping history play next action player receives payoff determined pair actions played using fixed game matrix goal player maximize average expected payoff particular interested finding good strategies play player opponents strategy computed computationally bounded machine dfa namely starting starting state opponent outputs action labeling state action played player determines opponents next state 4 3 distinguishing sequence sequence input symbols following property automaton unknown starting state given sequence input output sequence observed determines unknown starting state 4 slight difference learning scenario game playing scenario since latter player sees action chosen opponent choosing action however algorithms easily known 15 exist optimal strategies player simply forces opponent dfa follow cycle along nodes underlying graph known player hard prove player find optimal cycle strategy efficiently using dynamic programming however known player fortnow whang 11 show using combinationlock automata argument angluin 2 hard find optimal strategy case general game 5 clearly class automata learned exactly efficiently without reset optimal cycle strategy found efficiently however important learning algorithm use additional source information regarding target automaton counterexamples hypotheses otherwise learning algorithm cannot used game playing scenario related work several researchers considered problem learning dfas limit setting learner presented infinite sequence examples labeled according unknown dfa required output hypotheses converge limit number examples target dfa refer reader survey angluin smith 6 briefly survey known efficient learning algorithms dfas start problem exactly learning dfas addition work angluin 2 3 rivest schapire 22 discussed previously following also known rivest schapire 23 show permutation automata exactly learned efficiently without means reset without making equivalence queries since permutation automata property indegree outdegree node equal underlying automata small cover time thus result viewed generalization angluin 4 proves problem exactly learning dfas equivalence queries alone hard ibarra jiang 17 show subclass kbounded regular languages exactly learned polynomial number equivalence queries bender slonim 7 study related problem exactly learning directed graphs outputs associated nodes show task performed efficiently two cooperating robots robot performs single walk target graph contrast show task cannot performed efficiently one robot perform single walk even robot may use constant number pebbles mark states passes also show algorithm modified made efficient graph high conductance 28 conductance measure expansion properties graph bergando varricchio 8 show automata multiplicity exactly learned multiplicity equivalence queries particular implies learnability probabilistic automata input string may correspond many paths assigned probability product probabilities edges path automata exactly learned given access equivalence oracle oracle given string returns probability string reaches accepting state nonexact approximate learning without aid queries kearns valiant 18 show certain number theoretical assumptions problem pac learning dfas modified adapt difference 5 certain games penny matching player gets positive payoff matches opponents action combinationlock argument cannot applied underlying game penny matching fortnow whang 11 describe algorithm finds optimal strategy efficiently using ideas rivest schapires 22 learning algorithm without actually learning automaton hard given access random examples learning algorithms several special classes automata studied setting li vazirani 20 give several examples regular languages learned efficiently including 1letter languages 10 learning algorithm given languages accepted width2 branching programs readonce leveled special case dfas schapire warmuth 27 shown see also 10 problem learning width3 readonce leveled branching programs hard learning dnf also observe learning width5 readonce leveled branching programs hard certain number theoretical assumptions 14 shown learn typical automata automata underlying graph arbitrary acceptreject labels states chosen randomly passive learning edge traversed robot chosen randomly type mistake bound model addition work dean et al 9 previously mentioned following works consider case labels examples assumed noisy 25 algorithm given paclearning dfas membership queries presence persistent noise 12 algorithm given learning dfas blurry concepts preliminaries 21 basic definitions let deterministic finite state automaton dfa would like learn 4tuple finite set n states transition function q starting state output function transition function extended defined q theta f0 1g usual manner output state q flq output associated string u 2 f0 1g defined output state reached u ie output q 0 u denoted mu unless stated otherwise strings referred alphabet f0 1g cover time denoted cm defined follows every state q 2 q probability least 12 random walk length cm underlying graph starting q passes every state two strings 1 2 let 1 delta 2 denote concatenation 1 2 string integer let denote concatenations two sets strings 1 2 let g let empty string denoted set strings said prefix closed every string 2 prefixes including suffix closed set strings defined similarly string length prefix length 0 prefix defined 22 learning models 221 noise free model problem study exactly learning deterministic finite state automaton learning algorithm means resetting automaton learning algorithm viewed performing walk automaton starting q 0 step algorithm state q observe qs output algorithm chooses symbol oe 2 f0 1g upon moves state q oe course walk constructs hypothesis dfa algorithm exactly learned target dfa hypothesis used correctly predict sequence outputs corresponding given walk target dfa starting current state learning algorithm exact learning algorithm every given ffi 0 probability least 1 gamma ffi exactly learns target dfa exact learning algorithm efficient runs time polynomial n log1ffi assume algorithm given upper bound cover time also assume without loss generality irreducible namely every pair states q q 0 q distinguished string output state reaches executing starting q differs output state reached performing walk starting q 0 also consider easier setting learning algorithm means resetting machine performing new walk starting start state require given performing polynomial n log1ffi number walks polynomial length output hypothesis c equivalent ie every string c 222 noisy model assumptions noise follow classification noise model introduced angluin laird 5 assume fixed noise rate j 12 step probability algorithm observes correct output state reached probability j observes incorrect output observed output state q reached algorithm thus independent random variable flq probability assume j known assume lower bound ff 12 gamma j known algorithm noise free model algorithm performs single walk target dfa required exactly learn defined predictions based final hypothesis must agree correct outputs since task learning becomes harder j approaches 12 ff approaches 0 allow running algorithm depend polynomially 1ff well n log1ffi 3 exact learning reset section describe simple variant angluins algorithm 3 learning deterministic finite automata algorithm works setting learner means reset analysis similar 2 shows target automaton cover time cm high probability algorithm exactly learns target automaton performing oncm walks length ocm name algorithm exactlearnwithreset used subroutine learning algorithm means reset described section 4 algorithm described following sections assume simplicity algorithms actually know cm n upper bounds c b cm n b n known algorithms simply use bounds instead exact values running times resulting algorithms bounded running times original algorithms know cm n occurrence cm replaced c b occurrence n n b following angluin algorithm constructs observation table observation table table whose rows labeled prefix closed set strings r whose columns labeled suffix closed set strings entry table corresponding row labeled string algorithm exactlearnwithresetffi 1 let r random string length 2 let r 1 set prefixes 3 initialize table strings r ffi fill 4 consistent ffl exist r row let k 2 r new entries table performing corresponding walks ffl else table consistent 5 exists r 2 r 2 r j 2 r 1 row r closed return 1 rerun algorithm though assume high probability event table closed occur add last statement completeness could course solve situation angluins algorithm choose solution sake brevity figure 1 algorithm exactlearnwithreset r column labeled string j mr deltas j also refer mr deltas j behavior r j observation table induces partition strings r according behavior suffixes strings reach state equivalence class partition aim refine partition strings reaching state equivalence class case show set r certain property construct automaton based partition equivalent target automaton formally observation table string r 2 r let row r denote row labeled r namely say two strings r belong equivalence class according row r given observation table say consistent following condition holds every pair strings r r r r j equivalence class r deltaoe r j deltaoe 2 r oe 2 f0 1g r delta oe r j delta oe belong equivalence class well say closed every string r 2 r oe 2 f0 1g r 2 r exists string r j 2 r r r j belong equivalence class according every oe 2 f0 1g given closed consistent table define following automaton equivalence class corresponds state hard verify see 3 consistent sense every r 2 r every idea algorithm follows first use random walk construct set r 1 strings high probability reach every state namely r 1 every state q exists string r 1 take walk corresponding starting q 0 end state q given r 1 extend set r strings traverse every edge show use r construct observation table equivalence class state let r 2 f0 1g random string length cm log1ffi ffi confidence parameter given algorithm let r prefix rg r learning algorithm initializes include empty string fills single columned table performing walks corresponding strings r let us first observe definition cm probability random walk length cm log1ffi pass state q 12 therefore probability least every state q 2 q exists string r 2 r 1 q assume fact case directly follows always closed hence learning algorithm must ensure consistent done follows exists pair strings r r row r string oe delta k added k 2 r new entries filled pseudocode algorithm appears figure 1 clear inconsistency resolving process stage 4 algorithm given figure 1 ends steps true since every string added refines partition induced hand number equivalence classes cannot exceed n since every pair strings r r row r reach two different states hence adding oncm log1ffi entries table corresponding string length ocm log1ffi n algorithm constructed consistent table make following claim lemma 31 every state q 2 q exists string r 2 r 1 q proof order prove j show exists mapping oe following properties 1 2 8q 2 q 8oe 2 f0 1g oe q 3 8q 2 q since assumed without loss generality irreducible oe output preserving clearly existence function suffices prove equivalence since properties every 2 f0 1g let oe defined follows q 2 q q assumption statement lemma every state q 2 q exists string r 2 r 1 q definition deterministic finite automata r 6 r j r q necessarily r follows oe well defined next show oe satisfies three properties defined oe first property since q oe third property since remains prove second property let r 2 r 1 q assumption statement lemma know exists string thus q definition oe oe q thus following theorem theorem 1 every target automaton probability least withreset outputs hypothesis dfa equivalent furthermore running time algorithm 4 exact learning without reset section describe efficient exact learning algorithm defined subsection 22 automata whose cover time polynomial size algorithm closely follows rivest schapires learning algorithm 22 however use new techniques exploit small cover time automaton place relying teacher supplies us counterexamples incorrect hypotheses name algorithm exactlearn pseudocode appears figure 3 main problem encountered learner means reset cannot simply orient whenever needed returning starting state thus need alternative way learner orient 22 overcome absence reset use homing sequence homing sequence sequence whenever executed corresponding output sequence observed uniquely determines final state reached formally definition 41 state q sequence homing sequence h 2 f0 1g sequence symbols every pair states hard verify cf 19 every dfa homing sequence length quadratic size moreover given dfa homing sequence found efficiently 41 learning homing sequence known assume homing sequence h length n 2 target dfa remove assumption shortly could run algorithm exactlearngivenhomingsequence whose pseudocode appear figure 2 algorithm creates n copies algorithm exactlearnwithreset corresponding different output sequence may observed h executed stage algorithm walks according h observes output sequence performs next walk elr would performed starting q 0 current state reached since h homing sequence given output sequence whenever h executed observed reached state refer state effective starting state elr thus copy elr constructs observation table entries filled performing walks start effective starting state elr algorithm terminates one copies completes completed copys hypothesis automaton used predict correctly outcome walk described pseudocode exactlearngivenhomingsequence see figure 2 run copy exactlearnwithreset confidence parameter ffi n theorem 1 fact n copies exactlearnwithreset probability least hypothesis completed copy correct running time algorithm exactlearn givenhomingsequence bounded running time copy multiplied number copies executed length homing sequence thus algorithm exactlearngivenhomingsequenceffi ffl copy exactlearnwithreset completed 1 perform walk corresponding h let corresponding output sequence 2 exist copy elr exactlearnwithresetffin create new copy 3 simulate next step elr fill entry performing corresponding walk starting current state 4 observation table elr consistent closed elr completed 5 consistent closed discard elr figure 2 algorithm exactlearngivenhomingsequence 42 learning homing sequence unknown homing sequence unknown consider case guess sequence h homing sequence run algorithm exactlearngivenhomingsequence h instead true homing sequence since h homing sequence exist least two states q 1 6 q 2 pair states q 0 2 walk starting q 0 reaches q 1 upon executing h walk starting q 0reaches q 2 upon executing h output sequence cases let output sequence hence elr one effective starting state simulate elr walks performed fill entries might performed starting q 1 might performed starting q 2 first two possible consequences event observation table becomes consistent closed hypothesis incorrect namely exists walk starting current state whose outcome predicted incorrectly second possible consequence grows without becoming consistent number equivalence classes partition induced become larger n follows describe modify exactlearngivenhomingsequence homing sequence detect copy one effective starting state thus avoid two consequences furthermore procedure detection helps us improve h extending extensions becomes homing sequence initially set effective starting states namely q 2 q holds every state q 0 q exists row labeled string reaches q 0 starting q following true time add columns pair states q 1 q 2 q must exist least one entry distinguishes two states true since otherwise following lemma 31 q 1 q 2 would equivalent contradiction assumption irreducible discover one entry evidence elr one effective starting state therefore h homing sequence moreover concatenate string corresponding entry h restart algorithm extended h 6 n gamma 1 extensions h must become homing sequence algorithm exactlearnffi 1 n 2cm 2 h 3 copy exactlearnwithresetr completed choose uniformly length perform random walk length b perform walk corresponding h let corresponding output sequence c exist copy elrr exactlearnwithresetrnffi2n 2 create new copy simulate next step elrr fill entry performing corresponding walk w starting current state first execution w fill corresponding entry final output observed f else output state reached different output previous state reached performing w ii discard existing copies exactlearnwithresetr go 3 restart algorithm extended h g observation table elrr consistent closed elrr completed consistent closed discard elrr figure 3 algorithm exactlearn algorithm exactlearnwithresetr variant exactlearnwithreset given integer n walk fill entry table repeated n times single output observed output entered 6 22 actually need discard copies restart algorithm may discard copy disagreement found construct adaptive homing sequence results efficient algorithm sake simplicity presentation continue use preset homing sequence 421 detecting distinguishing entries next show detect entries distinguish two effective starting states let exactlearnwithresetr variant exactlearnwithreset walk fill entry table repeated n consecutive times given n n walks give output entry filled output otherwise found distinguishing entry thus algorithm exactlearn instead simulating copies elr exactlearnwithreset exactlearngivenhomingsequence simulate copies elrr exactlearnwithresetr parameter n set subsequently confidence ffi copy find observation table includes distinguishing entry described previously extend h string corresponding entry restart algorithm new h continue way one copy terminates order ensure never fill distinguishing entry without identifying one need ensure every entry r need fill r distinguishing entry following holds pair effective starting states distinguished r least one n executions r delta j starts 1 least one starts q 2 end following time executing h randomly choose length perform random walk length idea behind random walk every state nonnegligible probability reaching upon performing random walk precisely distinguishing entry r consider n executions h whose outcome followed performing walk corresponding r deltas j state set states q reached upon executing h ie given q 2 q probability reach q one n executions h observed equals probability following preceding random walks reach state bq probability bounded follows assume instead choosing random length performing random walk length first randomly choose string length cm choose random length finally perform walk corresponding length prefix clearly distribution states reached end walk equivalent distribution states reached original randomized procedure random strings probability passes state bq least 12 given passes state bq probability randomly chosen prefix ends state least 1cm together probability reach given state bq least 12cm thus given state q 2 q probability q reached corresponding n executions h bounded 422 bounding error running time algorithm remains set n total error probability exactlearn ffi bound algorithms running time two types events want avoid ensure algorithm constructs correct hypothesis shall bound probability type event occurs ffi2 first type event copy elrr one effective starting states q exists state q 0 q row labeled string reaches q 0 starting q course algorithm h takes n values value n effective starting states existing copies elrr even though single state may effective starting state copy since simulate copy error parameter ffi2n 2 probability least 1 gamma ffi 2 type event occur case follows lemma 31 h finally turns homing sequence n gamma 1 extensions table becomes consistent correct hypothesis second type bad event filling entry table detect distinguishing entry value h consider first entry filled table distinguishing entry since h takes n values n first entries 7 entry exists least one pair effective starting states dis tinguishes let equation 2 given distinguishing entry probability reach states pair effective starting states distinguishes ffi2n follows probability least 1 gamma ffi 2 first distinguishing entry perform walk corresponding entry starting two effective starting states distinguishes therefore probability detect first distinguishing entry every value h thus output hypothesis copy elrr corresponds one effective starting state running time algorithm bounded product number phases algorithm one value h n running time phase running time phase product ffl number copies exactlearnwithresetr phase n ffl number entries added table oncm log2n 2 ffi ffl number times walk corresponding entry repeated ffl sum maximum length walk fill entry ocm log2n 2 ffi maximum length h 2 cm log2n 2 ffi maximum length random walk performed prior execution h cm total running time hence thus proven theorem 2 algorithm exactlearn exact learning algorithm dfas running time mentioned previously rivest schapire 22 give exact learning algorithm runs time polynomial n log1ffi depend parameter related 7 note entry uniquely determined current h initial random walks label rows corresponding tables random walks executed prior executions h target automaton however rely teacher gives learner counterexamples incorrect hypotheses output learner interesting note tempting idea simply run rivest schapires algorithm instead making equivalence queries try randomly guess counterexample whenever learner hypothesis work even case automata small cover time rivest zuckerman 24 construct pair automata small cover time probability randomly guessing sequence distinguishes automata exponentially small automata described appendix 5 exact learning presence noise section describe modify learning algorithm described section 4 order overcome noisy environment name new algorithm exactnoisylearn pseudocode appears figure 6 start showing compute good estimate noise rate show use estimate learn target dfa homing sequence known finally describe learning algorithm given homing sequence 51 estimating noise rate according learning model algorithm given upper bound 12 gamma ff noise rate j since need good approximation j j first show j efficiently approximated high probability within small additive error done running procedure estimatenoiserate whose pseudocode appears figure 4 analyzed following lemma similar procedure described 25 lemma 51 given ffi 0 0 0 time polynomial log1ffi 0 1 n 1ff procedure estimatenoiserate outputs approximation j j probability least proof going details procedure describe idea based consider pair states q 1 q 2 string z 2 f0 1g let observed behavior q z output observed learner executing walk corresponding z starting q let actual behavior q z correct output state reached q every string z q z thus observed difference behavior q 1 q 2 set strings entirely due noise process q 1 6 q 2 difference observed behavior set strings z due difference actual behavior z well noise thus order estimate noise rate look strings seem reach state deduce noise rate difference observed behavior precisely done follows let arbitrary string length l l set subsequently suppose executed n1 times state reached performing exactly times let l sequence outputs corresponding th execution clearly pair indices 6 j q every k thus ij fraction indices sequences differ equivalently fraction strings among prefixes observed difference behavior q q j key observation q expected value ij least large precisely fraction prefixes procedure 1 2 let arbitrary string length l 3 perform walk corresponding n1 4 let l sequence outputs corresponding th execution ie complete output sequence corresponding n1 5 7 dmin 12 goto 8 let j solution 9 return j figure 4 procedure estimatenoiserate q q j actually differ oe expected observed difference behavior states therefore define min minimum value ij let solution quadratic equation since less n 2 pairs hoeffdings inequality 16 probability least every pair directly follows see 25 thus assume good approximation j j particular assume j ff8cm away j 52 learning homing sequence known noise free case first assume algorithm means reset assumption define slight modification exactlearnwithreset named exactnoisylearn withreset given large enough integer n procedure simply repeats walk fill entry table n times fills corresponding entry majority observed label thus high probability appropriate choice n majority observed label fact correct label state reached next assume algorithm means reset instead homing sequence h clearly single execution h high probability output sequence erroneous thus adapt technique used 9 idea construct new robust homing sequence h see many samples bit output h thus infer correct output h majority vote assume execute h consecutive times set subsequently order gain intuition consider first directed graph h whose set vertices q edge q 1 q 2 q 2 reached q 1 upon execution h executions h correspond walk length h clearly n n steps walk start following simple cycle h return executions h pass states cycle h hence follow cycle underlying graph noted jhj 1 cycle may simple show use existence cycle order estimate output sequence corresponding last th execution h let state start th execution h denoted q idea since executions h follow cycle assuming large enough h executed starting q many times assume able identify occurrence q equivalently find length simple cycle h could use executions whose outputs noisy infer majority vote high probability correct output sequence corresponds execution h starting q reaches current state formally state reached th execution h let jhj noisy output sequence corresponding execution possible length exists minimal period p 1 p n every 1 k b p words every p executions h executed starting q p simply length simple cycle h thus know p compute high probability correct output sequence corresponding last execution h started q considering previous executions started q every 1 j jhj follows high probability appropriate choice sequence correct output sequence corresponding last execution h case could proceed exactlearngivenhomingsequence simulating copies exactnoisylearnwithreset instead copies exactlearnwithreset find period p let q j state reached executions h followed length j prefix h definition p every k k 0 every j q mgammakp states reached step numbers differ multiples p let v jhj dimensional vector defined follows b v defined equation 4 holds every k outputs generated state since noise added independently appropriate choice n high probability every j p j either within ffl 1 gamma j within ffl j small additive error ffl particular shall choose ensure ffl ff2n assumption either v 6 p two possibilities j every k k 0 flq mgammakv might differ q mgammak 0 v following still true define j 1 v j greater 12 0 12 high probability jhj correct output sequence corresponding last execution h case v effectively behaves period otherwise let j index hold let k claim least 1p least 1n true since v delta p must period well hence every k k 0 multiples p q mgammakv let e v e v written two equivalent forms e v e v since fi 1n equation 9 e v hand since implies 1 gamma fi 1n equation 10 e v 2ff since assuming thus v j ff2n away expected value every j since shown see equations 6 7 high probability p j within ffn either j j able detect whether v minimal period p least effectively behaves consequently compute correct output sequence corresponding homing sequence h pseudocode procedure described appears figure 5 note actually use fact h homing sequence hence procedure used compute correct output sequence corresponding given sequence 53 learning homing sequence unknown remains treat case homing sequence known similarly noise free case correct output sequence corresponding candidate homing sequence h let q states q 2 q reached state following execution h procedure executehomingsequenceh 1 100 nff 2 logncm ffi 2 choose uniformly length perform random walk length 3 perform walk corresponding h 1 output sequence corresponding th execution h 4 length 1 5 let v every j either j v v return 1 7 return figure 5 procedure executehomingsequence corresponding output exists state q 0 2 q q state q 2 q let bq set states q 00 q 00 entry table corresponding exist argued noise free case entry possible output sequences h homing sequence let let q defined analogously noise free case walk corresponding given entry repeated n times random walk length chosen uniformly range performed prior executions h let fi 1 fraction times state q 2 q 1 reached let fi 0 defined analogously argument used noise free case discussion preceding equation 2 applying chernoff bound 2 f0 1g pr let fw defined exactnoisylearn fw fraction 1s observed among n repetitions walk executed fill distinguishing entry similarly calculations performed equations 8 14 hand distinguishing entry efw equals either j 1 gamma j hence within ff8cm either j 1 gamma j choose n ensure high probability determine entry distinguishing entry extend h string corresponding entry algorithm exactnoisylearnffi 1 n 100 ff log 2 ncm ffi 2 3 h 4 copy exactnoisylearnwithreset completed executehomingsequenceh b copy enlr exactnoisylearnwithresetn exist create new copy c simulate next step enlr performing corresponding walk w let w output state reached number times w executed ii discard existing copies exactnoisylearnwithreset go 4 restart algorithm extended h otherwise value entry set majority observed label enlr observation table enlr consistent closed output enlr completed consistent closed discard enlr figure algorithm exactnoisylearn algorithm exactnoisylearnwithreset variant exactlearnwithreset given integer n walk fill entry table repeated n times majority valued entered 531 bounding error running time algorithm start bounding error algorithm following 5 types events need prevent occurring shall bound probability type occurs ffi5 whenever bounding probability bad event occurs assume bad event occurred previously 1 estimation j j good enough call procedure estimatenoiserate confidence parameter estimation parameter know lemma 4 probability least 1 gamma ffi5 2 copy enlr one effective starting states q exists state q 0 q row labeled string reaches q 0 starting q noisefree case course algorithm h takes n values value n effective starting states existing copies enlr since simulate copy parameter ffi5n 2 probability least 1 gamma ffi5 type event occur 3 candidate homing sequence h first distinguishing entry filled detected order ensure event occur probability larger ffi 5 following first ensure probability least entry pair effective starting states distinguished entry fraction times execute corresponding walk starting states least 14cm ensure probability least fraction 1s observed differ expectation ff8cm argued opening subsection case distinguishing entries always detected start former requirement equation 15 n omega cm lognffi probability given distinguishing entry detected ffi10n probability event occurs h ffi10 second requirement hoeffdings inequality suffices ff lognffi 4 table nondistinguishing entry table majority observed output incorrect entry thought distinguishing avoid latter type error also means avoid former need ensure entries tables fraction 1s observed filling entry differ ff8cm expected value either j 1 gamma j construct n 2 tables size oncm log5n 2 ffi thus simply need set n larger previous bound factor ofomegagamma161 ncmffi order ensure type event occur probability greater ffi5 thus require ff log 2 ncmffi 5 execution candidate sequence h execution actually denote consecutive executions h output computed h incorrect maximum length h 2 cm log5n 2 ffi number values taken v computing v j n h takes n values value h executed njt jn times denotes maximum size table oncm log5n 2 ffi hoeffdings inequality ff log ncmn lognffi probability least 1 gamma ffi 5 every v j ff2n away expected value discussion following equation 13 suffices correct computation output sequence corresponding h running time algorithm bounded sum 1 running time procedure estimatenoiserate 2 number phases algorithm one value h n multiplied running time phase running time procedure estimatenoiserate oln 2 l defined figure 4 running time phase product ffl number copies exactnoisylearnwithreset phase n ffl number entries added table ncm log5n 2 ffi ffl number times walk corresponding entry repeated ff log 2 ncmffi ffl sum maximum length walk fill entry cm log5n 2 ffi maximum length h n 2 cm log5n 2 ffi times maximum length random walk performed prior execution h cm thus proven following theorem theorem 3 algorithm exactnoisylearn exact learning algorithm presence noise dfas running time acknowledgments wish thank michael kearns rob schapire conversations stimulated line research dana ron would like thank eshkol fellowship support part work done authors visiting att bell laboratories r charles rack note number queries needed identify regular languages learning regular sets queries counterexamples negative results equivalence queries learning noisy examples inductive inference theory methods power team exploration two robots learn unlabeled directed graphs learning behaviors automata multiplicity equiv alnece queries inferring finite automata stochastic output functions application map learning learning boundedwidth branching programs optimality domination repeated games bounded players learning consistently ignorant teacher efficient algorithms learning play repeated games computationally bounded adversaries efficient learning typical finite automata random walks bounded versus unbounded rationality tyranny weak probability inequalities sums bounded random variables learning regular languaages counterexamples cryptographic limitations learning boolean formulae finite automata switching finite automata theory learnability finite automata randomized algorithms inference finite automata using homing sequences private communication learning fallible finite state automata learning queries couterexamples presence noise presented colt90 rump session approximate counting tr learning regular sets queries counterexamples approximate counting uniform generation rapidly mixing markov chains negative results equivalence queries learnability finite automata learning regular languages counterexamples learning queries counterexamples presence noise efficient learning typical finite automata random walks cryptographic limitations learning boolean formulae finite automata diversitybased inference finite automata inference finite automata using homing sequences learning consistently ignorant teacher learning behaviors automata multiplicity equivalence queries optimality domination repeated games bounded players learning fallible deterministic finite automata randomized algorithms inferring finite automata stochastic output functions application map learning learning boundedwidth branching programs inductive inference theory methods learning noisy examples efficient algorithms learning play repeated games computationally bounded adversaries ctr michael bender antonio fernndez dana ron amit sahai salil vadhan power pebble exploring mapping directed graphs proceedings thirtieth annual acm symposium theory computing p269278 may 2426 1998 dallas texas united states ivan gabrijel andrej dobnikar online identification reconstruction finite automata generalized recurrent neural networks neural networks v16 n1 p101120 january michael bender antonio fernndez dana ron amit sahai salil vadhan power pebble exploring mapping directed graphs information computation v176 n1 p121 july 2002