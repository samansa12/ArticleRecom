bayesian function learning using mcmc methods abstractthe paper deals problem reconstructing continuous onedimensional function discrete noisy samples measurements may also indirect sense samples may output linear operator applied function linear inverse problem deconvolution cases linear operator could even contain unknown parameters estimated second experiment joint identificationdeconvolution problem bayesian estimation provides unified treatment class problems practical calculation posterior densities leads analytically intractable integrals paper shown rigourous bayesian solution efficiently implemented resorting mcmc markov chain monte carlo simulation scheme particular discussed structure problem exploited order improve computational convergence performances effectiveness proposed scheme demonstrated two classical benchmark problems well analysis ivgtt intravenous glucose tolerance test data complex identificationdeconvolution problem concerning estimation insulin secretion rate following administration intravenous glucose injection b introduction problem reconstructing learning unknown function set experimental data plays fundamental role engineering science present paper attention restricted scalar functions scalar variable main ideas may apply general maps well favourable case possible directly sample function rather frequently however indirect measurements available obtained sampling output linear operator applied function instance deconvolution problems sampled convolution unknown function known kernel see eg 1 2 3 4 5 approaches used solve function learning problem classified according three major strategies parametric methods assume unknown function belongs set functions parameterized finitedimensional parameters vector instance function modelled output multilayer perceptron given topology completely characterized values weights 6 another possibility use polynomial spline fixed knots cases function learning reduces problem estimating model parameters task performed solving possibly nonlinear least squares problem true unknown function belongs given function space statistical estimation theory could invoked order find minimum variance estimators compute confidence intervals 7 moreover would also possible compare parametric models increasing complexity using statistical tests ftest complexity criteria akaikes criterion second strategy namely regularization 8 9 10 11 avoids introducing heavy assumptions nature fdelta rather classifies potential solutions according regularity typically using index smoothness integral squared kth derivative function relative importance sum squared residuals regularity index controlled socalled regularization parameter key problem finding optimal criterion selection regularization parameter although empirical criteria ordinary cross validation generalized cross validation 12 13 perform satisfactorily many practical cases moreover regularization provide confidence intervals possible assess reliability reconstructed function present paper deals third strategy based bayesian estimation unknown function seen element probability space whose probability distribution reflects prior knowledge instance prior knowledge fdelta smooth translated probability distribution assigns higher probabilities functions whose derivatives small absolute values practical way describe fdelta gaussian stochastic process whose kth derivative white noise process intensity 2 provided 2 variance oe 2 gaussian measurement noise known bayes formula used work posterior distribution fdelta given data 2 14 15 16 posterior provides complete description state knowledge particular mean posterior used point estimate bayes estimate whereas variance helps assessing accuracy notable regularization parameter taken equal ratio oe 2 2 regularized estimate coincides bayes one main advantage bayesian approach possibility address selection regularization parameter rigourous probabilistic framework fact 2 known modelled random variable two different approaches possible simpler one based following observation prior distribution 2 flat maximum posterior given data close maximum likelihood estimate 2 ml posterior 2 peaked around max imum reasonable estimate fdelta 2 ml true value 2 14 17 4 magni et al bayesian function learning using mcmc methods 3 however posterior 2 peaked likely happen especially medium small data sets neglecting uncertainty 2 would lead underestimated confidence margins fdelta truly bayesian approach conversely calls computation posterior fdelta taking account also random nature since involved integrals analytically intractable one resort monte carlo methods first purpose present paper show truly bayesian solution function learning problem efficiently worked discuss various stages procedure starting discretization fdelta arrive practical implementation markov chain monte carlo mcmc methods 18 approach similar one proposed 15 however case indirect measurements deconvolution problem treated issue addressed paper joint identificationdeconvolution problem arises convolution kernel priori known identified separate experiment standard suboptimal approach identify convolution kernel use perfectly known order learn unknown function fdelta shown paper use mcmc methods allows learn functions jointly paper organized follows section ii contains statement problem section iii concise review mcmc methods numerical procedure solving bayesian function learning problem worked section section iv proposed method illustrated means simulated well realworld data coming analysis metabolic systems conclusions section v end paper ii problem statement paper consider problem reconstructing function discrete noisy samples k denotes measurement error l k linear functional increasing order generality denote sampling instants initial time first definition corresponds function approximation problem based samples function using second third definitions problem 1 becomes deconvolution problem integral equation first kind also called fredholm equation respectively noise v k letting v v 1 assumed known matrix oe 2 possibly unknown scalar following 1 f non parametric estimator based tychonov regularization given f p suitable operator k delta k norm suitable function space typical choice addition l k 2 l 2 norm used turns smoothing spline 13 basic idea behind 5 find balance data fit smooth magni et al bayesian function learning using mcmc methods 5 ness solution relative weight controlled socalled regularization parameter fl various criteria proposed tune fl among may mention ordinary crossvalidation 1 generalized crossvalidation 12 lcurve 19 worth noting without aid smoothness penalty flk would impossible learn function f belonging infinite dimensional space finite data set fy k g rather interestingly 5 also interpreted bayesian estimator pur pose assume f stochastic process f white noise intensity wt0 8t e wt assuming v k f normally distributed letting regularized estimate coincides conditional expectation f given observations k ie e f fl according bayesian paradigm probabilistic assumptions unknown function f reflect prior knowledge instance operator p 6 f double integral white noise relatively smooth signal smaller smoother f considerations suggest f fl optimal estimator provided unfortunately 2 sometimes also oe 2 unlikely known priori literature 20 14 4 proposed regard unknown parameter compute maximum likelihood estimate way f fl becomes suboptimal estimator paper conversely pursue rigourous bayesian approach unknown parameter 2 modelled hyperparameter suitable prior distribution taken account computation posterior density z fp 6 ieee transactions pattern analysis machine intelligence since analytic evaluation posterior made intractable presence hyperparameter calculations carried means monte carlo sampling algorithms context convenient discretize original problem without loss generality assumed 0 0 given sufficiently small discretization interval consider f simplicity assumed sampling instants k multiples ie operator p suitably discretized instance approximated moreover f approximated lf l suitable n theta n matrix l k 2 l kj l k 3 l r l k 4 l r approximations obtained assumption unknown function f constant sampling instants discretization 5 approximated f denotes usual norm n whose closed form solution magni et al bayesian function learning using mcmc methods 7 bayesian setting assumed entries vector taken realization discretetime white noise variance 2 ew 0 first purpose present paper investigate applicability markov chain monte carlo 18 sampling methods estimate pf j assuming 2 parameter suitable prior distribution another issue concerns bayesian solution joint identificationdeconvolution problem fact many deconvolution problems impulse response h enters convolution integral 3 depends one parameters estimated separate experiment putting together two experiments obtain known function z z data vector used identify ffl corresponding measurement error standard approach estimate using 10 estimate f using true value hand truly bayesian approach describes random variable pf j evaluated considering 9 10 simultaneously particular case possible consider 9 alone modelled random variable allow uncertainty standard suboptimal approach compute f fl using nominal value 4 assess sensitivity estimate respect parameters uncertainty iii markov chain monte carlo methods bayesian estimation problems probabilistic inference involves integration possibly highdimensional probability distributions since operation often analytically intractable common 8 ieee transactions pattern analysis machine intelligence resort monte carlo techniques requires sampling probability distribution integrated unfortunately sometimes impossible extract samples directly distribution markov chain monte carlo mcmc methods 18 provide unified framework solve problem mcmc methods based two steps markov chain monte carlo integra tion sampling suitable probability distributions generated markov chain converges distribution target distribution ie distribution tegrated expectation value calculated monte carlo integration obtained samples mcmc methods differ way markov chain generated however different strategies proposed literature special cases metropolishastings 21 22 framework also wellknown gibbs sampler 23 fits metropolishastings scheme following subsections describe application metropolishastings algorithm bayesian function learning well discuss possible variants method order describe metropolishastings algorithm use following notation vector model parameters vector data ie observations ffl theta th samples drawn target distribution proportional posterior distribution p prior distribution model parameters p likelihood magni et al bayesian function learning using mcmc methods 9 markov chain derived metropolishastings method obtained following steps 1 time candidate sample theta drawn proposal distribution 2 candidate point theta accepted probability 3 candidate point theta accepted next sample markov chain theta else chain move theta important remark stationary distribution chain ie distribution chain converges independent proposal distribution 18 coincides target distribution p although proposal distribution longrun deliver samples target distribution rate convergence stationary distribution generated markov chain crucially depends relationships proposal target distributions moreover number samples necessary perform monte carlo steps depends speed algorithm mixes ie spans support target distribution vector model parameters large often convenient divide k components update samples theta components one one 24 scheme called singlecomponent metropolishastings let theta th component theta time theta gammai theta k g metropolishastings scheme turns 1 time next sample theta t1 derived sampling candidate point theta proposal distribution q 2 candidate point theta accepted probability fftheta 3 candidate point theta accepted next sample markov chain theta theta else chain move theta gibbs sampler gs special case singlecomponent metropolis hastings gs scheme exploits full conditional product prior distribution likelihood proposal distribution case easy verify candidate point always accepted markov chain moves every step full conditionals standard distributions easy sample gs represents suitable choice contrary possible draw samples directly fullconditional distributions convenient resort mixed schemes metropolishastings setting portion model parameters estimated using gibbs sampler ones treated using adhoc proposal distributions algorithms extensively used field probabilistic graphical modelling 25 using kind models suitable partition blocking vector model parameters naturally obtained also easy derive full conditional distribu tion convergence rate strategies choosing proposal distribution described 26 27 b mcmc function reconstruction subsection describe problems defined section ii tackled using mcmc methods order explain probabilistic models used different sampling schemes resort bayesian network bn representation bns directed acyclic graphs dags nodes represent variables arcs express direct dependencies variables models quantified specifying conditional probability distribution node given parents help us magni et al bayesian function learning using mcmc methods 11 expressing conditional independence assumptions underlying different function reconstruction problems details see 28 29 b1 function approximation based direct sampling consider function approximation problem based samples function smoothing problem formal definition given equations 1 2 goal provide bayesian estimate vector f ie discretized unknown function shown section ii discretized form problem may written referring section ii f p gamma1 w ew0 eww apply mcmc strategy described section iiia must assign suitable probabilistic model parameter set g exploiting set standard choices bayesian estimation problems assumed w normally distributed given 2 precision parameter 1 gamma distribution formally moreover suppose noise v normal distribution covariance matrix oe 2 psi implies data model written py setting target distribution becomes model described simple bn fig 1 easy see order apply mcmc integration useful adopt partition wg context convenient adopt gibbs sampler since full conditional distributions assume following standard form fig 1 function approximation probabilistic model smoothing problem deconvolution problem fredholm equation problem point estimate derived mcmc algorithm trivial reconstruct unknown function f moreover samples joint posterior distribution possible derive statistics interest including confidence intervals appropriately bayes intervals b2 function approximation inverse problems deconvolution problems unknown function reconstructed basis indirect measurements convolution integral expresses relationships samples unknown function see 1 3 structure problem analogous considering integral equations first kind fredholm equations see 1 4 goal provide bayesian estimate discretized unknown f func magni et al bayesian function learning using mcmc methods 13 tion discretized form problem still form 11 since functions h ht enter definitions 3 4 assumed known matrix l completely specified also case f p gamma1 w parameter set estimated smoothing problem g thus probabilistic model inverse problems 13 14 described bn fig 1 fact setting smoothing problem deconvolution problem fredholm equation problem differ computation matrix l b3 deconvolution problems uncertain impulse response interesting extension problem described previous section relax assumption complete knowledge impulse response h appearing equation 3 anticipated section ii suppose function set unknown parameters estimated experimental data way problem becomes simultaneous estimation unknown function f parameter set given model described 9 10 f p gamma1 w case however probabilistic model specified becomes complex parameter set estimated g corresponding probabilistic model assumed 0 sigma moreover suppose noise signals v ffl independent normally distributed known covariance matrices namely v n0 oe 2 psi ffl data model written py j w setting target distribution becomes resulting model described bn fig 2 14 ieee transactions pattern analysis machine intelligence x x x z nm 1 l yfig 2 function approximation deconvolution problems presence uncertainty impulse response probabilistic model order devise mcmc scheme estimating convenient partition g full conditional distributions follows full conditional 16 clearly nonstandard distribution cannot sampled directly order use gs strategy necessary apply sampling algorithms general distributions like rejection sampling adaptive rejection sampling magni et al bayesian function learning using mcmc methods 15 30 unfortunately algorithms may impair overall efficiency stochastic simulation machinery valuable alternative resort mixed mcmc scheme different proposal distributions used extract samples different partitions ffl proposal 1 2 full conditional distributions gs way candidate point partitions always accepted ffl proposal distribution 3 chosen prior distribution case proposal distribution independent past sample drawn markov chain scheme also known independence sampler 31 acceptance probability candidate sample theta 3 simplifies fftheta 3 acceptance rate depends ratio likelihood candidate point likelihood current one basis specific problem proposal distributions chosen order obtain best performance computational speed example possible good choice use standard distribution good approximation full conditional distribution way preserve advantages singlecomponent metropolishastings avoiding additional computational burden would entailed gs presence nonstandard full conditional distributions iv bayesian function learning work section show presented methodology able cope three different benchmark problems taken literature function approximation based direct sampling test performance mcmc function approximator smoothing problem consider example proposed wahba 13 function approximated noisy samples k since interested reconstructing function correspondence measurement times equation 11 l identity matrix following section iiib1 take following prior identity matrix choice prior distribution parameters 2 reflects absence reliable prior information regularization parameter fact choice probability 09 13 assume second derivative function regular taking account discretization operator p chosen magni et al bayesian function learning using mcmc methods 17 0505t arbitrary units f arbitrary units fig 3 function approximation using mcmcsmoother noisy data stars true function dashdot line reconstructed function continuous line starting point markov chain extracted prior distribution parameters fw 2 g 1000 samples run mcmc scheme convergence estimates verified method described 26 particular choosing quantiles 0975g estimated precision respectively probability burnin n samples number required samples 870 calculated results shown fig 3 although samples rather noisy smoothed signal close true function rmse root mean square error obtained 0073 moreover performances mcmc smoother similar ones obtained 13 cubic smoothing spline used regularization parameter tuned ordinary crossvalidation ocv hence mcmc smoother good ocvtuned smoothing splines avoiding oversmoothing problems main advantage mcmc smoother provides also aposteriori sampling distribution regularization parameter confidence intervals reconstructed function rigourous bayesian setting b function approximation deconvolution problems order test performances mcmc deconvolution scheme consider wellknown benchmark problem 32 2 4 input signal given convoluted impulse response adding measurement errors v k simulated zeromean white gaussian noise sequence variance equal 9 52 noisy samples generated time fig 4a shows true function fig 4b depicts convoluted function together 52 noisy samples goal reconstruct unknown function sufficiently fine resolution means interested function estimates correspondence measurement times also samples time points particular consider 208 points evenlyspaced time grid interval 0 1035 entries l r take following prior distributions similar ones used previous section 2 gamma025 5e gamma 7 magni et al bayesian function learning using mcmc methods 19 arbitrary units f arbitrary units arbitrary units arbitrary units fig 4 simulated deconvolution problem panel function reconstructed panel b convoluted function dashdot line noisy samples stars moreover operator p selected choice corresponds penalty squared norm first derivative approximated first difference discretized signal bottleneck mcmc scheme computation b gamma1 equation 15this matrix inverse performed step mcmc scheme however proper change coordinates possible reduce size matrix inverted n theta n n theta n ie 208 theta 208 52 theta 52 problem goal achieved following steps 1 let hence n theta n matrix compute svd singular value n v n theta n orthogonal matrices uu n theta n diagonal matrix 2 view 11 easy verify v distributed n0 n w n0 2 n 3 apply mcmc scheme described previous section reformulated problem 17 new coordinates b block diagonal matrix first one n theta n block ones 1 theta 1 blocks 4 final estimate obtained retransforming variables original co ordinates particular need compute starting point markov chain extracted prior distribution parameters fw 2 g 5000 steps mcmc scheme convergence estimates verified using method described 26 particular choosing quantiles 0975g estimated precision respectively probability burnin n 132 samples number required samples 3756 calculated results shown fig 5 performance approach comparable one proposed 4 regularization parameter estimated according maximum likelihood criterion see fig 6 fig 7 magni et al bayesian function learning using mcmc methods 21 arbitrary units f arbitrary units arbitrary units arbitrary units fig 5 simulated deconvolution problem panel true function dashdot line reconstructed one continuous line 95 confidence interval dashed line obtained mcmc scheme panel b true noiseless output dashdot line estimated output continu ous line noisy samples rmse obtained approach 0065 one obtained method 4 0059 advantage mcmc scheme ability provide aposteriori sampling distribution regularization parameter confidence intervals reconstructed function rigourous bayesian setting c deconvolution uncertain impulse response subsection mcmc scheme applied realworld problem taken 33 particular demonstrate deconvolution impulse response identification addressed jointly described section iiib3 goal quantify insulin secretion rate isr humans glucose stim ulus experimental setting related socalled intravenous glucose tolerance 22 ieee transactions pattern analysis machine intelligence arbitrary units f arbitrary units fig 6 simulated deconvolution problem comparison mcmc scheme maximum likelihood one see text true function dashed line mcmc estimate continuous line maximum likelihood estimate thick line test ivgtt impulse dose glucose administered order assess subject capability bringing blood glucose levels within normal ranges endogenous release insulin main glucoregolatory hormone isr cannot directly measured since insulin secreted pancreas portal vein accessible vivo possible measure effect secretion circulation plasma concentration insulin large liver extraction plasma insulin concentration reflects posthepatic delivery rate circulation problem circumvented measuring cpeptide cp concentration plasma cp cosecreted insulin equimolar basis extracted liver directly reflects pancreatic isr thus problem turns estimation isr basis noisy measurements cp plasma since cp kinetics described linear model obtain following magni et al bayesian function learning using mcmc methods 23 samples lambda2 samples lambda2 b fig 7 simulated deconvolution problem panel markov chain 2 parameter panel b frequency histogram proportional posterior distribution parameter 2 estimated derived maximum likelihood approach see text 000665 k cp plasma measurements pmolml fdelta isr pmolmin hdelta cp impulse response ml gamma1 v k measurement error pmolml cp impulse response 33 h x parameters ff estimated adhoc experiment particular intravenous bolus biosynthetic cp delivered patient number plasma measurements cp collected somatostatin infusion administered order avoid endogenous pancreatic secretion impulse response parameters depend single patient considered constant time specific patient apply mcmc scheme section iiib3 order jointly perform cp impulse response identification isr reconstruction following section ii problem written matrix obtained discretization deconvolution integral z noisy measurements cp plasma concentration impulse response identification experiment v ffl measurements errors sampling instants identification experiment l elements case prior knowledge signal available 33 isr known exhibit spike external glucose stimulus regular profile thereafter knowledge modelled considering two different regularization parameters two phases computational reasons nonuniform discretization f adopted ac cordingly regularization operator p chosen satisfy magni et al bayesian function learning using mcmc methods 25 n0 l x x n x x z nm 1 l e fig 8 probabilistic model reconstruction insulin secretion rate moreover variances oe 2 v oe 2 ffl measurements errors v ffl imprecisely known must estimated well complete model shown fig 8 derived sampling distributions mcmc scheme described section iiib3 26 ieee transactions pattern analysis machine intelligence formulas n 1 number point used regularization first region spike n 2 number point used second region vector w number measurements identification experiment diagonal matrix first n 1 elements equal 2 1 n 2 elements equal 2 2 finally psi psi ffl diagonal matrices elements 2 z 2 respectively way parameters oe v oe ffl represent cv coefficient variation measurements errors two experiments completely specify mcmc scheme following prior distributions must 3 025 0125 005 sigma diagonal matrix elements sigma magni et al bayesian function learning using mcmc methods 27 value 0 derived basis prior knowledge dynamics impulse response value hyperparameters oe v oe ffl assessed knowing measurements error cv ranges 4 6 priors reflect knowledge signal shape two response phases perform test data set described 34 33 data set used impulse response identification collected cp bolus 49650 pmol data set used isr reconstruction taken intravenous glucose bolus 05 gkg basal value isr estimated basis five cp measurements taken glucose bolus goal reconstruct isr correspondence sampling instants cp measurements taken case nn 33 take n corresponding first phase ranging 16 data two experiments shown fig 9 2500 samples run mcmc scheme convergence verified using method described 26 assumption q r previous reported results shown fig 10 obtained fig 10a shows estimate cp plasma levels ivgtt experiment estimated curve slightly smoother measurements fig 10b depicts isr curve estimated deconvolution mcmc scheme reconstructed isr reproduces expected physiological shape characterized two regions different regularities results obtained comparable obtained 33 deconvolution impulse response identification treated separately fig 10c shows estimated cp impulse response identification easy notice good quality fit fig 11 frequency histograms samples generated mcmc estimator six impulse response parameters reported proposed mcmc scheme able jointly perform identification impulse response deconvolution isr classical approach 33 two experi 28 ieee transactions pattern analysis machine intelligence time min plasma pmolml time min plasma pmolml fig 9 data set isr insulin secretion rate reconstruction panel impulse response identification experiment consisting 32 noisy samples cp cpeptide concentration plasma collected cp intravenous bolus time 0 min panel b intravenous glucose tolerance test consisting 27 noisy samples cp plasma collected around intravenous glucose bolus time 0 min ments treated separate fashion first step impulse response identified using measurements identification set second step data deconvolution set used reconstruct unknown function uncertainty impulse response possibly taken account deconvolution step contrary scheme combines together information coming two ex periments uses order provide optimal point estimates well posterior moments confidence intervals v conclusions mcmc methods constitute set emerging techniques successfully applied variety contexts statistical physics image analysis 23 magni et al bayesian function learning using mcmc methods 29 time min plasma pmolml b time min isr c time min plasma pmolml fig 10 solution joint deconvolution impulse response identification problem panel estimated level cp plasma ivgtt continuous line collected samples stars panel b isr reconstructed 95 confidence interval mcmcestimator panel c estimated impulse response hdelta continuous line samples stars collected identification experiment medical monitoring 25 genetics 35 archaeology36 powerfulness mcmc methodologies lies inherent generality enables analyst deal full complexity real world problems paper exploited generality propose unified bayesian framework reconstruction functions direct indirect measurements particular using conceptual scheme easily coped problems previously solved adhoc methods obtained results cases least samples samples alpha2 b samples alpha3 c value 1ml samples samples samples a3 fig 11 frequency histograms proportional posterior distribution estimated impulse response parameters good previously proposed solutions addition since approach able soundly estimate posterior probability distribution reconstructed function information provided end estimation procedure richer methods first second moments confidence intervals posterior distributions obtained byproduct finally framework exploited implement new strategy joint estimation deconvoluted signal impulse response previous approaches based twostep procedure able optimally combine information available data main limits mcmc approach time required converge posterior distribution difficulty choose best sampling scheme limitations force use mcmc methods offline reconstructon summary mcmc methods shown play crucial role offline magni et al bayesian function learning using mcmc methods 31 function learning problem since provide flexible relatively simple strategy able provide optimal results bayesian sense acknowledgements authors would like thank antonietta mira methodological support designing mcmc scheme claudio cobelli giovanni sparacino provided experimental data insulin secretion rate reconstruction problem thank also anonymous reviewers useful suggestions r practical approximate solutions linear operator equations data noisy deconvolution problem fast algorithms including preconditioned conjugategradient compute map estimator linear inverse problems illposed problems nonparametric input estimation physiological systems problems methods case studies blind deconvolution via sequential imputations bayesian learning neural networks parameter estimation engineering science technique numerical solution certain integral equations first kind numerical solution fredholm integral equations first kind inversion linear system produced quadrature solutions illposed problems networks approximation learning smoothing noisy data spline functions estimating correct degree smoothing method generalized crossvalidation spline models observational data bayesian interpolation gaussian processes regression automatic bayesian curve fitting nonparametric spline regression prior information markov chain monte carlo practice numerical tools analysis solution fredholm integral equation first kind time series approach numerical differenti ation equations state calculations fast computing machine monte carlo sampling methods using markov chain applications stochastic relaxation gibbs distributions bayesian restoration images likelihood analysis nongaussian measurment time series unified approach modeling longitudinal failure time data application medical monitoring implementing mcmc inference monitoring convergence probabilistic reasoning intelligent systems dynamic probabilistic networks modelling identifying dynamic systems mcmc approach adaptive rejection sampling gibbs sampling markov chains posterior distributions discussion inverse problem radiography stochastic deconvolution method reconstruct insulin secretion rate glucose stimulus peripheral insulin parallels changes insulin secretion closely cpeptide bolus intravenous glucose administration censored survival models genetic epidemi ology gibbs sampling approach archaeological example radiocarbon dating tr ctr gianluigi pillonetto claudio cobelli brief paper identifiability stochastic semiblind deconvolution problem class timeinvariant linear systems automatica journal ifac v43 n4 p647654 april 2007 xudong jiang wee ser online fingerprint template improvement ieee transactions pattern analysis machine intelligence v24 n8 p11211126 august 2002 ranjan k dash erkki somersalo marco e cabrera daniela calvetti efficient deconvolution algorithm estimating oxygen consumption muscle activities computer methods programs biomedicine v85 n3 p247256 march 2007