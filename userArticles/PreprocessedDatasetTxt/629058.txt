compiletime techniques data distribution distributed memory machines solution problem partitioning data distributed memory machines discussed solution uses matrix notation describe array accesses fully parallel loops allows derivation sufficient conditions communicationfree partitioning decomposition arrays series examples illustrate effectiveness technique linear references use loop transformations deriving necessary data decompositions formulation aids deriving heuristics minimizing communication communicationfree partitions feasible presented b introduction distributed memory machines intel ipsc2 ncube process address space processes must communicate explicitly sending receiving messages local memory accesses machines much faster involving interprocessor commu nication result programmer faces enormously difficult task orchestrating parallel execution programmer forced manually distribute code data addition managing communication among tasks explicitly task addition errorprone timeconsuming generally leads nonportable code hence parallelizing compilers machines active area research recently 3 4 5 9 14 19 20 22 24 25 enormity task extent relieved hypercube programmers paradigm 6 attention paid partitioning tasks alone assuming fixed data partition programmerspecified form annotations data partition 9 7 14 20 number efforts way develop parallelizing compilers multicomputers programmer specifies data decomposition compiler generates tasks appropriate message passing constructs 4 7 14 15 20 22 25 though rely intuition based domain knowledge programmer always possible verify annotations indeed result efficient execution recent paper programming multiprocessors alan karp 12 observes see data organization key parallel algorithms even shared memory systems importance data management also problem people writing automatic parallelization compilers todate compiler technology directed toward optimizing control flow today hierarchical distributed memories make program performance function data organization compiler existence changes data addresses specified programmer improve perfor mance compilers successful particularly messagepassing systems new kind analysis developed analysis match data structures executable code order minimize memory traffic paper attempt providing new kind analysis present matrix notation describe array accesses fully parallel loops lets us present sufficient conditions communicationfree partitioning decomposition arrays case commonly occurring class accesses present formulation fractional integer programming problem minimize communication costs communicationfree partitioning arrays possible rest paper organized follows section 2 present background assumptions make discuss related work section 3 illustrates examples importance difficulty finding good array decompositions section 4 present matrixbased formulation problem determining existence communicationfree partitions arrays present conditions case constant offset array access section 5 series examples presented illustrate effectiveness technique linear references addition show use loop transformations deriving necessary data decomposi tions section 6 generalizes formulation presented section 4 arbitrary linear references section 7 present formulation aids deriving heuristics minimizing communication communicationfree partitions feasible section 8 concludes summary discussion assumptions related work communication message passing machines could arise need synchronize nonlocality data impact absence globally shared memory compiler writer severe addition managing parallelism essential compiler writer appreciate significance data distribution decide data copied generated local memory focus distribution arrays commonly used scientific computation primary interest arrays accessed execution nested loops consider following model processor owns data element make updates exactly one copy even case fully parallel loops care must taken ensure appropriate distribution data next sections explore techniques compiler use determine data distributed communication incurred operations involving two operands require operands aligned corresponding operands stored memory processor executing operation model consider means operands used operation must communicated processor holds operand appears left hand side assignment statement alignment operands generally requires interprocessor communication current day machines interprocessor communication timeconsuming instruction execution insufficient attention paid data allocation problem amount time spent interprocessor communication might high seriously undermine benefits parallelism therefore worthwhile compiler analyze patterns data usage determine allocation order minimize interprocessor communication present machineindependent analysis communicationfree partitions make following assumptions 1 exactly one copy every array element processor whose local memory element stored said element 2 owner array element makes updates element ie instructions modify value element executed owner processor 3 fixed distribution array elements data reorganization costs architecturespecific 21 related work research problems related memory optimizations goes back studies organization data paged memory systems 1 balasundaram others 3 working interactive parallelization tools multicomputers provide user feedback interplay data decomposition task partitioning performance programs gallivan et al discuss problems associated automatically restructuring data moved local memories case shared memory machines complex memory hierarchies present series theorems enable one describe structure disjoint sublattices accessed different processors use information make correct copies data local mem ories write data back shared address space modifications complete gannon et al 8 discuss program transformations effective complexmemory management cedarlike architecture threelevel memory gupta banerjee 10 present constraintbased system automatically select data decompositions loop nests program hudak abraham 11 discuss generation rectangular hexagonal partitions arrays accessed sequentially iterated parallel loops knobe et al 13 discuss techniques automatic layout arrays compiler targeted simd architectures connection machine computer system li chen 17 5 addressed problem index domain alignment finding set suitable alignment functions map index domains arrays common index domain order minimize communication cost incurred due data movement class alignment functions consider primarily permutations embeddings kind alignment functions deal general mace 18 proves problem finding optimal data storage patterns parallel processing shapes problem npcomplete even limited one twodimensional arrays ad dition efficient algorithms derived shapes problem programs modeled directed acyclic graph dag derived seriesparallel combinations treelike subgraphs wang gannon 23 present heuristic statespace search method optimizing programs memory hierarchies addition several researchers developing compilers take sequential program augmented annotations specify data distribution generate necessary communication koelbel et al 14 15 address problem automatic process partitioning programs written functional language called blaze given userspecified data partition group led kennedy rice university 4 studying similar techniques compiling version fortran local memory machines includes annotations specifying data decomposition show existing transformations could used improve performance rogers pingali 20 present method given sequential program data partition performs task partitions enhance locality references zima et al 25 developed superb interactive system semiautomatic transformation fortran programs parallel programs suprenum machine looselycoupled hierarchical multiprocessor 3 deriving good partitions examples consider following loop example 1 allocate row array row array b local memory processor communication incurred allocate columns blocks interprocessor communication incurred data dependences example loops referred doall loops easy see allocation rows would result zero communication since offset access b along first dimension figure 1 shows partitions arrays b next example even though nonzero offset along dimension communication free partitioning possible example 2 case row column block allocation arrays b would result interprocessor communication case b partitioned family parallel lines whose equation communication result figure 2 shows partitions b kth line array ie line line array b must assigned processor figure 1 partitions arrays b example 1 figure 2 partitions arrays b example 2 loop structure array modified function array b communication partitioning case referred compatible partition consider following loop skeleton example 3 array modified loop l 1 function elements array b loop l 2 modifies array using elements array loops l 1 l 2 adjacent loops level nesting effect poor partition exacerbated since every iteration outer loop suffers interprocessor communication cases communicationfree partitioning possible extremely important communicationfree partitions arrays involved adjacent loops referred mutually compatible partitions preceding examples following array access pattern computing element ai j element bi 0 required c c j constants consider following example example 4 example allocation row array column array b processor would result communication see figure 3 partitions b example note 0 function j j 0 function presence arbitrary array access patterns existence communicationfree partitions determined connectivity data access graph described array element accessed either written read associate node graph k different arrays accessed graph k groups nodes nodes belonging given group elements array let node associated left hand side assignment statement referred writes set nodes associated array elements figure 3 array partitions example 4 right hand side assignment statement called readsets edge every member readsets data access graph graph connected communicationfree partition 19 referencebased data decomposition consider nested loop following form accesses arrays b example 5 linear functions j ie disjoint partitions array find corresponding required disjoint partitions array b order eliminate communication partition b required given partition elements partition b appear right hand side assignment statements body loop modify elements partition given partition required referred images maps discuss array partitioning context fully parallel loops though techniques presented 2dimensional arrays generalize easily higher dimensions particular interested partitions arrays defined family parallel hyper planes partitions beneficial point view restructuring compilers portion loop iterations executed processor generated relatively simple transformation loop thus question partitioning stated follows find partitions induced parallel hyperplanes b communication focus attention 2dimensional arrays hyperplane 2 dimensions line hence discuss techniques find partitions b parallel lines incur zero communication loops occur scientific computation functions f g linear j equation defines family parallel lines different values c given ff fi constants one zero example defines columns defines diagonals given family parallel lines array defined find corresponding family lines array b given communication among processors conditions solutions one ff fi zero similarly one ff 0 fi 0 zero otherwise equations define parallel lines solution satisfies conditions referred nontrivial solution corresponding partition called nontrivial partition since 0 given equations 1 2 implies 21 22 20 since family lines defined ffi 21 5 22 6 20 7 solution system equations would imply zero communication matrix notation 0 11 21 0 12 22 0 c 0c b ff cc set equations decouples 11 21 12 22 ff illustrate use sufficient condition following example example element ai j need two elements b consider element bi gamma 1 j communicationfree partitioning systemb c 0c b ff cc must solution similarly considering element solution must exist following system well 0 c 0c b ff cc given single allocation b two systems equations must admit solution reduces following system figure 4 partitions arrays b example 6 set equations reduce solution say implies b partitioned antidiagonals figure 4 shows partitions arrays zero communication relations c c 0 give corresponding lines b minor modification example 6 shown example 7 reduced system equations would solution 2 figure 5 shows lines arrays b incur zero communication next example shows nested loop arrays partitioned communication example 8 figure 5 lines arrays b example 7 system equations case reduces one solution ff 0 nontrivial solution thus communicationfree partitioning arrays b examples considered far involve constant offsets access array elements find compatible partitions next case considered one need find mutually compatible partitions consider nested loop example 9 case accesses due loop l 1 result system accesses due loop l 2 result system therefore communicationfree partitioning two systems equations written must admit solution thus get reduced system solution figure 6 shows partitions b diagonals 41 constant offsets reference discuss important special case array accesses constant offsets occur codes solution partial differential equations consider following loop k q j integer constants vectors referred offset vectors offset vectors one access pair ai j bi figure mutually compatible partition b example 9 cases system equations access indexed k 1 k mb c 0c b ff cc reduces following constraints therefore given collection offset vectors communicationfree partitioning possible consider offset vectors q k points 2dimensional space communication free partitioning possible points q collinear addition communication required rowwise partitions similarly q j partitioning arrays columns results zero communication zero communication nested loops involving kdimensional arrays means offset vectors treated points kdimensional space must lie dimensional hyperplane examples one solution set values ff ff 0 next section show example infinite number solutions loop transformations play role choice specific solution 5 partitioning linear references program transformations section discuss communicationfree partitioning arrays references constant offsets linear functions consider loop example 10 example 10 communication free partitioning possible system equationsb c 0c b ff cc solution one ff fi zero one ff 0 fi 0 zero set reduces set infinite number solutions present four discuss relative merits first two equations involve four variables fixing two leads values two example set array partitioned rows set equations get ff 0 array b partitioned columns since assign row k array column k array processor communication see figure 1 partitions second partition chosen setting 1 case array partitioned columns therefore ff 0 means b partitioned rows figure 7a partition set array partitioned antidiagonals set equations get ff 0 means array b also partitioned antidiagonals figure 7b shows third partition fourth partition chosen setting gamma1 case array partitioned diagonals therefore ff 0 means b also partitioned antidiagonals case kth subdiagonal diagonal corresponds kth superdiagonal diagonal array b figure 7c illustrates partition figure 7 decompositions arrays b example 10 point loop transformations 2 24 rewrite loop indicate processor executes portion loop iterations partitions 1 2 easy let us assume number processors p number rows columns n n multiple p case partition 1 partitioned rows rewritten processor k executes 1 k p n p rows r r mod assigned processor k columns r b r mod also assigned processor k case partition 2 partitioned columns loop rewritten processor k executes 1 k p n p columns r r mod rows r b r mod also assigned processor k since data dependences anyway loops interchanged written processor k executes 1 k p n p partitions 3 4 result complicated loop structures partition 3 1 steps perform transform loop use loop skewing loop interchange transformations 24 perform following steps 1 distribute iterations outer loop roundrobin manner case processor k system p processors executes iterations referred wrap distribution apply loop interchange wrapdistribute iterations interchanged outer loop apply following transformation index set ff fi 2 apply loop interchanging outer loop stripmined since loops involve flow antidependences loop interchanging always legal first transformation loop need rectangular therefore rules interchange trapezoidal loops 24 used performing loop interchange resulting loop transformation loop interchange following example 11 loadbalanced version loop processor k executes 1 k p 2n p reason distribute outer loop iterations wraparound manner distribution result load balanced execution n p partition 4 gamma1 resulting loop transformation loop interchange following processor k executes 1 k p next consider complicated example illustrate partitioning linear recurrences example 12 access bi results following systemb c 0c b ff cc second access results systemb c 0c b ff cc together give rise following set equations one solution 0 thus communicationfree partitioning shown impossible following loop communicationfree partitioning columns possible example 13 accesses give following set equations case solution 1 thus b partitioned columns 6 generalized linear references section discuss generalization problem formulation discussed section 4 example 14 linear functions j ie thus statement loop case family lines array b given lines array given thus families lines array rewritten array 11 ff 0 12 ff 0 22 fi 0 20 15 therefore communicationfree partitioning find solution following system equations constraint one ff fi zero one ff 0 zerob 11 21 0 12 22 0 c 0c b ff cc consider following example example 15 figure 8 partitions arrays b example 15 accesses result following system equationsb c 0c b ff cc leads following set equations solution 1 see figure 8 partitions complicated example example accesses result following system equationsb c 0c b ff cc leads following set equations solutions ff system following solution 1 loop transformation processor k executes 1 k p 2n p following section deals formulation problem communication minimization communicationfree partitions feasible 7 minimizing communication constant offsets section present formulation communication minimization problem used communicationfree partitioning impossible focus twodimensional arrays constant offsets accesses results generalized higher dimensional arrays consider following loop model array accesses loop give rise set offset vectors 2 theta matrix q whose columns offset vectors q referred offset matrix since ai j computed iteration j partition array defines partition iteration space viceversa constant offsets shape partitions two arrays b array boundaries depend offset vectors given offset vectors problem derive partitions processors equal amount work communication minimized assume n 2 iterations n 2 elements array computed number processors p also assume n 2 multiple p thus workload processors n 2 shapes partitions considered parallelograms rectangles special case parallelogram defined two vectors normal one side parallelogram let normal vectors matrix refers j array indices defines family lines given different values c 1 vector defines family lines given different values c 2 must nonsingular order define parallelogram blocks span entire array matrix defines linear transformation applied point j image point j consider parallelograms defined solutions following set linear inequalities r 1 l r 2 l lengths sides parallelograms number points discrete cartesian space enclosed region must workload processor n 2 dets 6 0 nonzero entries matrix q 0 sum absolute values entries ith row q ie communication volume incurred 2l thus problem finding blocks minimize interprocessor communication finding matrix value l aspect ratios r 1 r 2 communication volume minimized subject constraint processors amount workload ie elements matrix determine shape partitions values r size partitions summary current day distributed memory machines interprocessor communication timeconsuming instruction execution insufficient attention paid data allocation problem much time may spent interprocessor communication much benefit parallelism lost therefore worthwhile compiler analyze patterns data usage determine allocation order minimize interprocessor communication paper formulated problem determining communicationfree array partitions decompositions exist presented machineindependent sufficient conditions class parallel loops without flow anti dependences array references affine functions loop index variables addition communicationfree decomposition possible presented mathematical formulation aids minimizing communication acknowlegdment gratefully acknowledge helpful comments referees improving earlier draft paper r performance enhancement paging systems program analysis transformations automatic translation fortran programs vector form interactive environment data partitioning distribution compiling programs distributedmemory multiprocessors compiling parallel programs optimizing performance solving problems concurrent processors volume volume1volume general techniques regular problems problem optimizing data transfers complex memory systems strategies cache local memory management global program transformations array distribution superb automatic data partitioning distributed memory multipro cessors compiler techniques data partitioning sequentially iterated parallel loops programming parallelism data optimization allocation arrays reduce communication simd machines semiautomatic process partitioning parallel computation supporting shared data structures distributed memory machines compiling programs nonshared memory machines index domain alignment minimizing cost crossreferencing distributed arrays memory storage patterns parallel processing process decomposition locality reference compiling locality reference mapping data processors distributed memory computa tions applying ai techniques program optimization parallel computers optimizing supercompilers supercomputers superb tool semiautomatic mimdsimd parallelization tr programming parallelism automatic translation fortran programs vector form memory storage patterns parallel processing solving problems concurrent processors vol 1 general techniques regular problems strategies cache local memory management global program transformation problem optimizing data transfers complex memory systems process partitioning parallel computation process decomposition locality reference data optimization allocation arrays reduce communication simd machines supporting shared data structures distributed memory architectures compiletime techniques parallel execution loops distributed memory multiprocessors compiling programs nonshared memory machines compiler techniques data partitioning sequentially iterated parallel loops array distribution superb optimizing supercompilers supercomputers compiling locality reference ctr g n srinivasa prasanna agrawal b r musicus hierarchical compilation macro dataflow graphs multiprocessors local memory ieee transactions parallel distributed systems v5 n7 p720736 july 1994 jihwoei huang chihping chu efficient communication scheduling method processor mapping technique applied data redistribution journal supercomputing v37 n3 p297318 september 2006 wenglong chang jihwoei huang chihping chu using elementary linear algebra solve data alignment arrays linear quadratic references ieee transactions parallel distributed systems v15 n1 p2839 january 2004 array partitioning based smith normal form international journal parallel programming v33 n1 p3556 february 2005 ravi ponnusamy yuanshin hwang raja das joel h saltz alok choudhary geoffrey fox supporting irregular distributions using dataparallel languages ieee parallel distributed technology systems technology v3 n1 p1224 march 1995 anant agarwal david kranz venkat natarajan automatic partitioning parallel loops data arrays distributed sharedmemory multiprocessors ieee transactions parallel distributed systems v6 n9 p943962 september 1995 wenrui gong gang wang r kastner storage assignment highlevel synthesis configurable architectures proceedings 2005 ieeeacm international conference computeraided design p36 november 0610 2005 san jose ca kueiping shih jangping sheu chuahuang huang statementlevel communicationfree partitioning techniques parallelizing compilers journal supercomputing v15 n3 p243269 mar12000 kandemir choudhary n shenoy p banerjee j ramanujam hyperplane based approach optimizing spatial locality loop nests proceedings 12th international conference supercomputing p6976 july 1998 melbourne australia skewed data partition alignment techniques compiling programs distributed memory multicomputers journal supercomputing v21 n2 p191211 february 2002 manish gupta prithviraj banerjee paradigm compiler automatic data distribution multicomputers proceedings 7th international conference supercomputing p8796 july 1923 1993 tokyo japan minyi guo linear data distribution based index analysis high performance scientific engineering computing hardwaresoftware support kluwer academic publishers norwell 2004 catherine mongenet mappings communication minimization using distribution alignment proceedings ifip wg103 working conference parallel architectures compilation techniques p185193 june 2729 1995 limassol cyprus chen j p sheu communicationfree data allocation techniques parallelizing compilers multicomputers ieee transactions parallel distributed systems v5 n9 p924938 september 1994 paul feautrier toward automatic partitioning arrays distributed memory computers proceedings 7th international conference supercomputing p175184 july 1923 1993 tokyo japan esin onbasioglu linet zdamar optimization data distribution processor allocation problem using simulated annealing journal supercomputing v25 n3 p237253 july jordi garcia eduard ayguad jesus lebarta novel approach towards automatic data distribution proceedings 1995 acmieee conference supercomputing cdrom p78es december 0408 1995 san diego california united states thomas rauber gudula rnger deriving array distributions optimization techniques journal supercomputing v15 n3 p271293 mar12000 akimasa yoshida kenichi koshizuka hironori kasahara datalocalization fortran macrodataflow computation using partial static task assignment proceedings 10th international conference supercomputing p6168 may 2528 1996 philadelphia pennsylvania united states vincent loechner catherine mongenet communication optimization affine recurrence equations using broadcast locality international journal parallel programming v28 n1 p47102 february 2000 david garzasalazar wim bhm reducing communication honoring multiple alignments proceedings 9th international conference supercomputing p8796 july 0307 1995 barcelona spain wei li keshav pingali access normalization loop restructuring numa computers acm transactions computer systems tocs v11 n4 p353375 nov 1993 wei li keshav pingali access normalization loop restructuring numa compilers acm sigplan notices v27 n9 p285295 sept 1992 micha cierniak wei li unifying data control transformations distributed sharedmemory machines acm sigplan notices v30 n6 p205217 june 1995 james r larus compiling sharedmemory messagepassing computers acm letters programming languages systems loplas v2 n14 p165180 marchdec 1993 kandemir j ramanujam choudhary p banerjee layoutconscious iteration space transformation technique ieee transactions computers v50 n12 p13211336 december 2001 peizong lee efficient algorithms data distribution distributed memory parallel computers ieee transactions parallel distributed systems v8 n8 p825839 august 1997 mahmut kandemir alok choudhary nagaraj shenoy prithviraj banerjee j ramanujam linear algebra framework automatic determination optimal data layouts ieee transactions parallel distributed systems v10 n2 p115135 february 1999 ender zcan esin onbaioglu memetic algorithms parallel code optimization international journal parallel programming v35 n1 p3361 february 2007 mahmut taylan kandemir compiler technique improving wholeprogram locality acm sigplan notices v36 n3 p179192 march 2001 mahmut kandemir alok choudhary j ramanujam prith banerjee reducing false sharing improving spatial locality unified compilation framework ieee transactions parallel distributed systems v14 n4 p337354 april mahmut taylan kandemir improving wholeprogram locality using intraprocedural interprocedural transformations journal parallel distributed computing v65 n5 p564582 may 2005 peizong lee zvi meir kedem automatic data computation decomposition distributed memory parallel computers acm transactions programming languages systems toplas v24 n1 p150 january 2002