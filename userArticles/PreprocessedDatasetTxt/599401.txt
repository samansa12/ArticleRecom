annealed importance sampling simulated annealingmoving tractable distribution distribution interest via sequence intermediate distributionshas traditionally used inexact method handling isolated modes markov chain samplers shown one use markov chain transitions annealing sequence define importance sampler markov chain aspect allows method perform acceptably even highdimensional problems finding good importance sampling distributions would otherwise difficult use importance weights ensures estimates found converge correct values number annealing runs increases annealed importance sampling procedure resembles second half previouslystudied tempered transitions seen generalization recentlyproposed variant sequential importance sampling also related thermodynamic integration methods estimating ratios normalizing constants annealed importance sampling attractive isolated modes present estimates normalizing constants required may also generally useful since independent sampling allows one bypass problems assessing convergence autocorrelation markov chain samplers b introduction bayesian statistics statistical physics expectations various quantities respect complex distributions must often computed simple distributions estimate expectations sample averages based points drawn independently distribution interest simple monte carlo approach cannot used distribution complex allow easy generation independent points might instead generate independent points simpler approximating distribution use importance sampling estimate points weighted compensate use wrong distribution alternatively could use sample dependent points obtained simulating markov chain converges correct distribution show paper two approaches combined using importance sampling distribution defined series markov chains method inspired idea annealing way coping isolated modes leads call annealed importance sampling method especially suitable multimodality may problem may attractive even since allows one bypass problems convergence assessment annealed importance sampling also supplies estimate normalizing constant distribution sampled statistical physics minus log normalizing constant canonical distribution known free energy estimation longstanding problem independent work jarzynski 1997ab described method primarily aimed free energy estimation essentially annealed importance sampling method described focus instead statistical applications discuss use method estimating expectations functions state well normalizing constant importance sampling works follows see example geweke 1989 suppose interested distribution quantity x probabilities probability densities proportional function fx suppose also computing fx x feasible able directly sample distribution defines however able sample distribution approximates one defined fx whose probabilities probability densities proportional function gx also able evaluate base estimates sample n independent points x distribution defined gx x compute importance weight follows estimate expectation ax respect distribution defined fx n provided gx 6 0 whenever fx 6 0 easy see n gamma1 r fx dx z r gx dx normalizing constants fx gx one also see converge expectation ax respect distribution defined fx accuracy depends variability importance weights weights vary widely estimate effectively based points largest weights importance sampling work well distribution defined must therefore fairly good approximation defined fx ratio fxgx vary wildly x highdimensional fx complex perhaps multimodal finding good importance sampling distribution difficult limiting applicability method alternative obtain sample dependent points simulating markov chain converges distribution interest metropolishastings algorithm metropolis et al 1953 hastings 1970 markov chain methods long used statistical physics widely applied statistical problems illustrated papers book edited gilks richardson spiegelhalter 1996 markov chains used sample complex distributions must usually proceed making small changes state variables causes problems distribution contains several widelyseparated modes nearly isolated respect transitions chain move modes rarely take long time reach equilibrium exhibit high autocorrelations functions state variables long time lags method simulated annealing introduced kirkpatrick gelatt vecchi 1983 way handling multiple modes optimization context employs sequence distributions probabilities probability densities given p 0 x p j differs slightly p j1 distribution p 0 one interest distribution p n designed markov chain used sample allows movement regions state space traditional scheme set annealing run started initial state first simulate markov chain designed converge p n number iterations necessarily enough actually approach equilibrium next simulate number iterations markov chain designed converge p ngamma1 starting final state previous simulation continue fashion using final state simulation p j initial state simulation finally simulate chain designed converge p 0 hope distribution final state produced process close p 0 note p 0 contains isolated modes simply simulating markov chain designed converge p 0 starting arbitrary point could give poor results might become stuck whatever mode closest starting point even mode little total probability mass annealing process heuristic avoiding taking advantage freer movement possible distributions gradually approaching desired p 0 unfortunately reason think annealing give precisely correct result mode p 0 found exactly right probability little consequence optimization context final distribution degenerate maximum serious flaw many applications statistics statistical physics require sample nondegenerate distribution annealed importance sampling method present paper essentially way assigning weights states found multiple simulated annealing runs produce estimates converge correct value number runs increases done viewing annealing process defining importance sampling dis tribution explained section 2 discussing accuracy importance sampling general section 3 analyse efficiency annealed importance sampling section 4 find good results obtained using sufficient number interpolating distributions provided vary smoothly demonstrations simple distributions section 5 statistical problem section 6 confirm annealed importance sampling related tempered transitions neal 1996 another way modifying annealing procedure produce correct results discussed section 7 annealed importance sampling sometimes preferable using tempered transitions tempered transitions still used relationship annealed importance sampling allows one find estimates ratios normalizing constants previously unavailable section 8 shows one also view form sequential importance sampling due maceachern clyde liu 1998 instance annealed importance sampling finally section 9 discuss general utility annealed importance sampling way handling multimodal distributions way calculating normalizing constants way combining adaptivity markov chains advantages independent sampling 2 annealed importance sampling procedure suppose wish find expectation function x respect distribution probabilities probability densities given p 0 x available sequence distributions given p 1 x p n x hope assist us sampling p 0 satisfy p j x 6 0 wherever distribution must able compute function f j x proportional must also method sampling p n preferably one produces independent points finally 1 n gamma 1 must able simulate markov chain transition j leaves p j invariant sequence distributions used specially constructed suit problem following scheme may generally useful fix f 0 give distribution interest fix f n give simple distribution sample let note traditional simulated annealing scheme f j would usually less suitable since usually leads p n independent sampling easy applications bayesian statistics f n would prior density often easy sample f 0 would unnormalized posterior distribution product f n likelihood posterior expectations interest neither prior likelihood need normalized normalizing constant posterior marginal likelihood interest likelihood must properly normalized prior need discussed markov chain transitions represented functions giving probability probability density moving x 0 current state x necessary actually compute j x x 0 generate x 0 given x using j transitions may constructed usual ways eg metropolis gibbs sampling updates may involve several scans iterations annealed importance sampling scheme valid j must leave corresponding p j invari ant essential j produce ergodic markov chain though would usually desirable annealed importance sampling produces sample points x corresponding weights w estimate expectation function ax found equation 2 generate point x associated weight w first generate sequence points x generate x ngamma1 p n generate x ngamma2 x using generate x 1 x 2 using 2 generate x 0 x 1 using 1 let x f avoid overflow problems may best computations terms logw see annealed importance sampling valid consider extended state space points original state function original state considered function extended state looking component define distribution following function proportional joint probability probability density e j reversal transition defined j e invariance p j respect j ensures valid transition probabili ties r e 1 turn guarantees marginal distribution x 0 6 original distribution interest since joint probability product marginal probability x 0 conditional probabilities later components given earlier components use apply equation 7 rewrite function f follows e f e f look joint distribution defined annealed importance sampling procedure 4 proportional following function regard importance sampler distribution 6 extended state space appropriate importance weights found using equations 1 9 10 dropping superscript right side simplify notation f f weights equation 5 showing annealed importance sampling procedure valid procedure produces sample single independent points x use estimating expectations equation 2 practice better estimates often obtained use point initial state markov chain leaves p 0 invariant simulate predetermined number iterations estimate expectation ax weighted average using w simple average states markov chain valid expectation ax respect p 0 x expectation respect p 0 x average value along markov chain leaves p 0 invariant started state x since start state distribution p 0 later states also p 0 annealed importance sampling also provides estimate ratio normalizing constants f 0 f n normalizing constants important statistical physics statistical problems bayesian model comparison normalizing constant f defined equation 6 f 0 normalizing constant g equation 10 f n average importance weights converges ratio normalizing constants z 0 z n bayesian application f n proportional prior f 0 product f n likelihood ratio z 0 z n marginal likelihood model prior probability probability density observed data note prior need normalized since constant factors cancel ratio likelihood must include constant factors estimate marginal likelihood correct data collected annealed importance sampling runs p n p 0 also used estimate expectations respect intermediate distribu simply uses states x j found application 4 weights found omitting factors equation 5 pertain later states similarly one estimate ratio normalizing constants f j f n averaging weights finally although would usually prefer start annealing runs distribution p n generate independent points annealed importance sampling still valid even points x ngamma1 generated start run independent particular points could generated using markov chain samples annealed importance sampling estimates still converge correct values provided markov chain used sample p n ergodic 3 accuracy importance sampling estimates discussing annealed importance sampling necessary consider accuracy importance sampling estimates general results also needed demonstrations sections 5 6 reference importance sampling estimate e f based points x drawn independently density proportional gx n w importance weights accuracy importance sampling estimator discussed geweke 1989 estimator form also used regenerative markov chain methods mykland tierney yu 1995 ripley 1987 weights lengths tours regeneration points determining accuracy estimator assume without loss generality normalizing constant g e g w since multiplying w constant effect also assume e f since adding constant ax simply shifts amount without changing variance large n numerator denominator right side equation 12 converge expectations assumptions gives differences averages expectations n large discard first term e 1 judge accuracy variance assuming finite approximate hi return actual situation e g w may one e f may zero modifying equation 14 suitably hi geweke 1989 estimates data used compute follows h n equivalent estimate discussed ripley 1987 section 64 context regenerative simulation n small ripley recommends using jacknife estimate instead w ax independent g equation 15 simplifies w last step uses following equation 18 shows w ax independent cost using points drawn gx rather fx given one plus variance normalized importance weights estimate using sample variance gives us rough indication factor sample size effectively reduced without reference particular function whose expectation estimated note many applications expectations several functions estimated sample x variance w also intuitively attractive indicator accurate estimates since large points largest importance weights dominate estimates would imprudent trust estimate adjusted sample size small even equation 16 gives small estimate variance estimator one note however possible sample variance w small even estimates wildly inaccurate since sample variance could bad estimate true variance normalized importance weights could happen example important mode f almost never seen sampling g earlier suggested e f might estimated weighted average values states markov chain started x accuracy estimate estimated treating average values single data points treating dependent states along chain independently drawn g could lead overestimation effective sample size finally x independently drawn g instead generated markov chain sampler assessing accuracy estimates difficult depend variance normalized importance weights autocorrelations produced markov chain used one reason preferring p n generate points independently start annealed importance sampling run 4 efficiency annealed importance sampling efficiency annealed importance sampling depends normalized importance weights w large variance several sources variability importance weights first different annealing runs may end different modes assigned different weights variation weights due large important modes found rarely general guarantee happen one hope find effective scheme defining annealing distributions use radically different markov chain eliminates isolated modes altogether high variability importance weights also result using transitions distributions bring distribution close equilibrium extreme case j nothing case annealed importance sampling reduces simple importance sampling based p n inefficient p n close p 0 variability source reduced increasing number iterations basic markov chain update used example consists k metropolis updates variance importance weights might reduced increasing k j brings state closer equilibrium distribution least within local mode variability importance weights also come using finite number distributions interpolate p 0 p n analyse affects variance w sequence distributions used comes smoothly varying oneparameter family equation 3 analysis assume j produces state drawn p j independent previous state assumption course unrealistic especially isolated modes purpose understand effects unrelated markov chain convergence convenient look logw rather w discussed section 3 measure inefficiency estimation one plus variance normalized importance weights using fact ey q x gaussian mean variance oe 2 see logw gaussian mean variance oe 2 sample size effectively reduced factor equation 5 logw distributions used defined equation 3 logw logf assume fi j equally spaced 0 1 logw n logf assumption j produces state drawn independently p j provided logf 0 drawn central limit theorem applied conclude logw approximately gaussian distribution large n keeping f 0 f n fixed n increases variance logw asymptotically form oe 2 one plus variance normalized weights form expoe 2 assume transition j takes fixed amount time regardless n time required produce estimate given degree accuracy proportional n expoe 2 0 n minimized 0 point variance logs importance weights one variance normalized importance weights e gamma 1 behaviour occur fi j equally spaced long chosen scheme leads going approximately inverse proportion n range fi values p j close gaussian p 0 x approximately constant regions high density p j argument similar used tempered transitions neal 1996 section 42 shows best scheme uses uniform spacing logfi j ie geometric spacing fi j results also hold generally annealing schemes based families distributions density given x varies smoothly parameter analogous fi get idea efficiency annealed importance sampling affected dimensionality problem supposing p j k components x independent identically distributed assuming j produces independent state drawn p j quantities logf 0 logf n composed k identically distributed independent terms variance quantity increase proportion k variance logw asymptotically form koe 2 n optimal choice n koe 2 0 makes variance normalized importance weights e gamma 1 assuming behaviour similar interesting distributions components independent analysis shows increasing dimensionality problem slow annealed importance sampling however linear slowdown much less severe simple importance sampling whose efficiency goes exponentially k analysis assumes j generates state nearly independent previous state would presumably require many metropolis gibbs sampling iterations probably better practice however use transitions come close producing independent state hence take much less time increasing number interpolating distributions produce total computation time states generated would still come close equilibrium distributions since distributions change less one annealing step next increased number distributions may help reduce variance importance weights though perhaps much analysis since terms equation 24 longer independent therefore see variance importance weights reduced needed increasing number distributions used annealing scheme provided transitions distribution good enough establishing equilibrium isolated modes latter provision true global sense transitions sample well within local mode used whether performance annealed importance sampling adequate depend whether annealing heuristic fact capable finding modes distribution absence theoretical information pointing modes located reliance heuristic inevitable 5 demonstrations simple distributions illustrate behaviour annealed importance sampling show works simple distribution single mode using markov chain transitions sample well intermediate distributions distribution two modes isolated respect markov chain transitions distribution interest distributions r 6 unimodal distribution six components state x 1 x 6 independent distribution gaussian mean 1 standard deviation 01 distribution defined f 0 normalizing constant 201 2 sequence annealing distributions defined according scheme equation 3 distribution chosen components independent gaussian mean zero standard deviation 1 function f n used define distribution chosen corresponding gaussian probability density normalized therefore estimate normalizing constant f 0 average importance weights use annealed importance sampling must choose sequence fi j define intermediate distributions number spacing fi j must appropriate problem mentioned previous section gaussian p 0 diffuse p n expect geometric spacing appropriate fi j far one spaced fi j near zero arithmetically detail first test used 40 fi j spaced uniformly 0 001 followed 160 fi j spaced geometrically 001 1 total 200 distributions later tests annealing sequences twice many half many distributions also used spaced according scheme must also define markov chain transitions j distributions general one might use different schemes different distributions tests used metropolis updates proposal distributions j transition probabilities course different j since metropolis acceptance criterion changes detail used sequences three metropolis updates gaussian proposal distributions centred current state covariances 005 2 015 2 05 2 used together three proposal distributions lead adequate mixing intermediate distributions first test sequence three updates repeated 10 times give j one later test repeated 5 times test 1000 annealing runs done first test 200 states produced run result applying j succession starting point generated independently p 200 saved every twentieth state however applying etc 0 note 0 applied end run tests even though required occurs naturally program used state applying 0 used estimates even though valid use state 1 well figure 1 shows results first test upper graphs show variance log importance weights increases course run importance weights run defined equation 5 factors later distributions omitted transitions distributions expected mix well best strategy minimizing variance final weights space fi j variance log weights increases equal amount annealing step plot upper right shows spacing chosen test close optimal respect furthermore according analysis section 4 number intermediate distributions used close optimal since variance logs weights end annealing run close one lower two graphs figure 1 show distribution value first component state x 1 test seen lower left distribution narrows distribution p 0 fi approaches one plot lower right shows values first component importance weights states ends runs case values weights appear independent estimate expectation first component state first test 10064 standard error 00050 estimated using equation 16 compatible true value one case error estimate equation 16 close one would arrive estimated standard deviation 010038 adjusted sample size expected values weights independent average importance weights test 0000236 standard error 0000008 estimated simply sample variance weights divided n compatible true normalizing constant 0000248 two tests done run used half much computer time first test one annealing sequence identical first test number repetitions three metropolis updates j reduced 10 5 increased variance normalized importance weights 218 corresponding increase standard errors estimates test number distributions annealing sequence cut half spaced according scheme number metropolis repetitions kept 10 increased variance normalized importance weights 272 expected spreading given number updates many intermediate distributions appears better using many updates try produce nearly independent points fewer stages final test unimodal distribution used twice many intermediate distri butions spaced according scheme reduced variance normalized importance weights 0461 corresponding reduction standard errors benefit case worth factor two increase computer time however test confirm j mixes well variance importance weights reduced desired spacing fi j closely tests also done distribution two modes mixture two gaussians six components independent means standard deviations one gaussians mixing proportion 13 means 1 standard deviations 01 distribution used unimodal tests gaussian mixing proportion 23 means gamma1 standard deviations 005 mixture distribution defined following normalizing constant f 0 3 0000744 means components respect p 0 gamma13 f n used tests independent standard gaussian distributions component normalized transitions based metropolis updates used well along scheme spacing fi j first test number distributions used 200 first test unimodal distribution results shown figure 2 seen lower left figure distributions fi near zero cover modes fi increased two modes become separated metropolis updates able move modes fi near one even using larger proposals standard deviation 05 since probability proposing movement mode simultaneously six components small modes seen annealing mode gamma1 seen rarely 27 times 1000 runs despite fact twice probability mode final distribution average final states annealing runs would therefore give inaccurate results plot lower right figure shows importance weights compensate unrepresentative sampling runs ended rarelysampled mode received much higher weights ending wellsampled mode estimate expectation first component runs gamma0363 estimated standard error 0107 equation 16 compatible true value gamma13 standard error estimate less one might expect estimated standard deviation 092 adjusted sample size n 1 varw 350 difference arises values importance weights independent case average importance weights runs 0000766 estimated standard error 0000127 compatible true value 0000744 normalizing constant f 0 beta log weight index distribution variance log weights 200 150 100 50 002061014 beta first component state 11first component state importance weight figure 1 results first test unimodal distribution upper left logs importance weights ten values fi 1000 runs upper right variance log weights function index fi lower left distribution first component state ten fi values lower right joint distribution first component importance weight ends runs random jitter added fi values plots left improve presentation beta log weight index distribution variance log weights 200 150 100 50 002061014 beta first component state 11first component state importance weight figure 2 results first test distribution two modes four plots correspond figure 1 therefore see annealed importance sampling produces valid estimates example however procedure less efficient might hope runs end mode gamma1 another symptom problem variance normalized importance weights test 276 quite high compared variance 112 seen similar test unimodal distribution see comes upper plots figure 2 small values fi plots quite similar figure 1 presumably mode gamma1 almost influence distributions however mode becomes important fi approaches one producing high variance weights end one might hope reduce variance importance weights increasing number intermediate distributions ie spacing fi j closely ran tests twice many distributions four times many distributions cases using number metropolis updates distribution results differed little first test variance importance weights runs ending within mode reduced difference importance weights modes reduced number runs ending mode gamma1 increase therefore little difference standard errors estimates example annealing heuristic used marginally adequate one could expect obtain better results finding better initial distribution p n better scheme interpolating p n p 0 equation 3 example also illustrates dangers uncritical reliance empirical estimates accuracy 100 runs done probability none runs would found mode gamma1 would around 007 result simulated using first 100 runs ended mode 1 1000 runs actual test based 100 runs estimate expectation first component 0992 estimated standard error 0017 estimate normalizing constant f 0 0000228 estimated standard error 0000020 estimates differ true values many times estimated standard error unrecognized inaccuracies course also possible importance sampling markov chain method whenever theoreticallyderived guarantees accuracy available 6 demonstration linear regression problem illustrate use annealed importance sampling statistical problems briefly describe application two bayesian models linear regression problem based gaussian cauchy priors example previous section implemented using software flexible bayesian modeling version 19980901 data command files used included software available web page data consists 100 independent cases 10 realvalued predictor vari ables realvalued response variable modeled x residual ffl modeled gaussian mean zero unknown variance oe 2 100 cases synthetically generated model oe 10 predictor variables generated multivariate gaussian variance x one correlations 09 pair x two bayesian models tried prior reciprocal residual variance 1oe 2 gamma mean 101 2 shape parameter 05 models also hyperparameter 2 controlling width distribution fi k reciprocal given gamma prior mean 1005 2 shape parameter 025 model gaussian priors 2 variance fi k mean zero independent conditional 2 model based cauchy priors similar except width parameter cauchy distribution ie density fi k conditional 11 fi 2 suspect cauchy prior prove appropriate actual data since prior gives substantial probability situations many fi k close zero fi k much bigger seems quite possible posterior using cauchy prior could multimodal since x highly correlated one fi k extent substitute another cauchy prior favours situations fi k large could produce several posterior modes correspond different sets fi k regarded significant sampled models using combination gibbs sampling oe 2 hybrid monte carlo method fi k see neal 1996 sign problems isolated modes difficult sure basis modes exist annealed importance sampling applied order either find isolated modes provide evidence absence also compare two models calculating marginal likelihoods annealing schedule based equation 3 used experimentation adequate results obtained using schedule 1000 distributions 50 distributions geometrically spaced distributions geometrically spaced finally 500 distributions geometrically spaced updates used distribution single annealing run took approximately 8 seconds 194 mhz sgi machine 500 runs model annealing runs resulted much smaller weights others variance logs weights large hence useful judging whether annealing schedule good instead looked log one plus variance normalized importance weights distribution logs weights gaussian w would equal variance logs weights distribution gaussian w less affected extremely small weights plots w show models increases approximately linearly index distribution reaching final value around 06 bit less optimal value one models estimates posterior means fi k found using annealed importance differ significantly found using hybrid monte carlo without annealing therefore appears isolated modes present problem annealed importance sampling runs yielded estimates log marginal likelihood model gaussian priors 15868 model cauchy priors 15824 standard error 004 estimates difference 044 corresponds bayes factor 155 favour model cauchy priors 7 relationship tempered transitions several ways modifying simulated annealing procedure order produce asymptotically correct estimates developed past including simulated tempering marinari parisi 1992 geyer thompson 1995 metropolis coupled markov chains geyer 1991 method tempered transitions neal 1996 closely related annealed importance sampling method paper tempered transition method samples distribution interest p 0 using markov chain whose transitions defined terms elaborate proposal procedure involving sequence distributions p 1 p n proposed state found simulating sequence base transitions leave invariant distributions 1 p n followed second sequence base transitions n leave p n invariant reversals corresponding respect p j decision whether accept reject final state based product ratios probabilities various distributions proposed state rejected new state old state detail tempered transition operates follows starting state x generate x 1 using generate x 2 using generate x n x using generate x using generate x 1 using generate x 0 using state x 0 accepted next state markov chain probability min second half tempered transition procedure 26 identical annealed importance sampling procedure 4 provided n fact generates point p n independent x n also recognize annealed importance sampling weight given equation 5 essentially second half product defining tempered transition acceptance probability 27 due similarities characteristics annealed importance sampling quite similar corresponding tempered transitions particular comparison neal 1996 tempered transitions simulated tempering relevant annealed importance sampling well major difference annealed importance sampling tempered transitions tempered transition requires twice much computation corresponding annealing run since tempered transition involves upward sequence transitions p 1 p n well downward sequence p n p 1 present methods reason prefer annealed importance sampling easy generate independent points distribution p n easy tempered transitions might preferred though annealed importance sampling could still used conjunction markov chain sampler produces dependent points p n tempered transitions also possibility using one sequence annealing distributions sequence chosen randomly tempered transition fixed order potentially could lead good sampling even neither annealing sequence would adequate appears way employing multiple annealing sequences annealed importance sampling without adding equivalent upward sequence present tempered transitions tempered transitions used idea behind annealed importance sampling applied order estimate ratios normalizing constants previously unavailable using tempered transitions see note first half tempered transition generation x x ngamma2 using annealed importance sampling run sequence distributions reversed p 0 p n exchange roles first state run current state x 0 comes p 0 general x j 4 corresponds x ngamma1gammaj 26 importance weights backwards annealed importance sampling f average weights tempered transitions accepted rejected converge ratio normalizing constants f n f 0 similar estimate found imagining reversal markov chain defined tempered transitions chain states visited reverse order accepted transitions original chain become accepted transitions reversed chain reversed sequence states rejected transitions original chain remain unchanged importance sampling estimate ratio normalizing constants f n f 0 obtained using reversed chain manner importance weights accepted transitions follows terms original chain f f importance weights rejected transitions equation 28 two estimates averaged producing estimate uses states beginning end accepted transitions plus states beginning rejected transitions double weight estimate ratio normalizing constant f j f 0 found similar fashion intermediate distributions simply averaging weights obtained truncating products equations 28 29 appropriate point weights also used estimate expectations functions respect intermediate distributions note error assessment importance sampling estimates take account variance importance weights autocorrelations produced markov chain based tempered transitions cautionary note regarding estimates comes considering situation two distributions used prior posterior bayesian model estimate reciprocal marginal likelihood based equation 28 average points drawn posterior reciprocal likelihood estimator often infinite variance bad problem enough data posterior much affected prior since marginal likelihood affected prior compare annealed importance sampling estimate marginal likelihood using two distributions average likelihood points drawn prior good posterior much concentrated prior bad averaging reciprocal likelihood even many intermediate distributions used seems possible annealed importance sampling estimates may better corresponding backwards estimates using tempered transitions assuming p n diffuse p 0 8 relationship sequential importance sampling variant sequential importance sampling recently developed maceachern clyde liu 1998 viewed instance annealed importance sampling sequence distributions obtained looking successively data points method maceachern et al call sequential importance sampler s4 applies model joint distribution observable variables x along associated latent variables finite range able compute joint probabilities well marginal probabilities x k together k subset indexes wish estimate expectations respect conditional distribution known values x could apply gibbs sampling problem possible slow converge due isolated modes method maceachern et al viewed annealed importance sampling sequence distributions p 0 p n p j related distribution conditional observed variables p 0 distribution interest conditional x detail distributions probabilities proportional following apply annealed importance sampling sequence distributions using transitions defined follows j begins number gibbs sampling updates 1 ngammaj based p ignore ngammaj1 n generate values afterward conditional distribution independently previous values done forward simulation based conditional probabilities actually need generate values k k since values effect subsequent computations anyway easily seen equivalent sampling done procedure s4 maceachern et al importance weights equation 5 products factors following form product factors produces weights used maceachern et al sequential importance sampler s4 maceachern et al thus equivalent annealed importance sampling annealing distributions defined equation 30 unlike family distributions given equation 3 distributions form fixed discrete family consequently variance importance weights cannot decreased increasing number distributions could sometimes make method inefficient practical use however possible sequence distributions defined equation 30 could extended continuous family partially conditioning x k way eg adjusting variance gaussian likelihood forms annealed importance sampling eg based family equation 3 could also applied problem 9 discussion annealed importance sampling potentially useful way dealing isolated modes means calculating ratios normalizing constants general monte carlo method combines independent sampling adaptivity markov chain methods handling isolated modes original motivation annealing primary motivation developing methods related annealing produce asymptotically correct results annealed importance sampling another method whose characteristics similar tempered transitions discussed neal 1996 methods best may depend whether sequence annealing distributions deceptive certain ways therefore possible say annealed importance sampling always better methods simulated tempering probably easily implemented methods annealing methods closely related methods estimating ratios normalizing constants based simulations many distributions many discussed gelman meng 1998 therefore surprising methods simulated tempering marinari parisi 1992 geyer thompson 1995 metropolis coupled markov chains geyer 1991 easily yield estimates ratios normalizing constants byproduct tempered transitions previously seen deficient respect neal 1996 see estimates fact obtained using annealed importance sampling estimators conjunction tempered transitions one also estimate expectations respect intermediate distributions way also possible simulated tempering metropolis coupled markov chains ratios normalizing constants also obtained using annealed importance sampling perspective seen form thermodynamic integration see gelman meng 1998 one might expect thermodynamic integration estimate based finite number points suffer systematic error results paper show annealed importance sampling estimate ratio normalizing constants fact unbiased converge correct value number annealing runs increases note procedure one averages estimates multiple runs ratio normalizing constants log ratio might perhaps seem natural unlike simulated tempering related method umbrella sampling torrie preliminary estimates ratios normalizing constants required using annealed importance sampling metropolis coupled markov chains share advantage disadvantage require storage states intermediate distributions annealed importance sampling may therefore convenient general method estimating normalizing constants addition particular uses annealed importance sampling may sometimes attractive combines independent sampling ability markov chain sampler adapt characteristics distribution evans 1991 also devised adaptive importance sampling method makes use sequence intermediate distributions similar used annealing method requires class tractable importance sampling densities defined contains density appropriate distributions sequence annealed importance sampling instead uses sampling distribution implicitly defined operation markov chain transitions whose density generally tractable compute making use simple importance sampling infeasible perspective idea behind annealed importance sampling one nevertheless find appropriate importance weights use sampling distribution looking ratios densities along sequence intermediate distributions one annoyance markov chain monte carlo need estimate autocorrelations order assess accuracy estimates obtained provided points p n used start annealing runs generated independently need annealed importance sampling instead one must estimate variance normalized importance weights may perhaps easier though nightmare scenarios drastically wrong results obtained without indication problem possible using methods either sort annealed importance sampling occur distribution importance weights heavy upward tail apparent data collected another annoyance markov chain monte carlo need decide much run discard burnin ie coming close equilibrium distribution one long run simulated exact amount discarded burnin may crucial several shorter runs done instead desirable order diagnose possible nonconvergence decision may harder discarding little lead biased estimates discarding much waste data annealed importance sampling one must make analogous decision much computation time spend annealing runs determine importance weights much spend simulating chain samples p 0 starting final state annealing run usually desirable see section 2 however decision affects variance estimates results asymptotically correct regardless far annealing process reaching equilibrium regenerative methods mykland tierney yu 1995 also eliminate problems dealing sequential dependence also replace possible problems due heavytailed distributions use regenerative methods appropriate splitting scheme must devised markov chain sampler highdimensional problems may harder defining appropriate sequence intermediate distributions use annealed importance sampling discussed section 4 time required annealed importance sampling expected increase direct proportion dimensionality problem addition increase due markov chain samplers used slower higher dimensions one must also consider human computer time required select appropriate sequence intermediate distributions along appropriate markov chain transitions reasons annealed importance sampling probably useful allows one find needed ratios normalizing constants serves avoid problems isolated modes one note however potential problems multiple modes exists whenever theoretical guarantee distribution unimodal acknowledgements thank david mackay helpful comments research supported natural sciences engineering research council canada r chaining via annealing simulating normalizing constants importance sampling bridge sampling path sampling markov chain monte carlo maximum likelihood annealing markov chain monte carlo applications ancestral inference bayesian inference econometric models using monte carlo inte gration markov chain monte carlo practice monte carlo sampling methods using markov chains applications nonequilibrium equality free energy differences equilibrium freeenergy differences nonequilibrium measure ments masterequation approach optimization simulated annealing simulated tempering new monte carlo scheme sequential importance sampling nonparametric bayes models next generation equation state calculations fast computing machines regeneration markov chain samplers sampling multimodal distributions using tempered transi tions bayesian learning neural networks stochastic simulation nonphysical sampling distributions monte carlo freeenergy estimation umbrella sampling tr ctr jonathan deutscher ian reid articulated body motion capture stochastic search international journal computer vision v61 n2 p185205 february 2005 malte kuss carl edward rasmussen assessing approximate inference binary gaussian process classification journal machine learning research 6 p16791704 1212005 j sullivan blake isard j maccormick bayesian object localisation images international journal computer vision v44 n2 p111135 september 2001 jrgen gall jrgen potthoff christoph schnrr bodo rosenhahn hanspeter seidel interacting annealing particle filters mathematics recipe applications journal mathematical imaging vision v28 n1 p118 may 2007 cristian sminchisescu bill triggs building roadmaps minima transitions visual models international journal computer vision v61 n1 p81101 january 2005 tapani raiko harri valpola markus harva juha karhunen building blocks variational bayesian learning latent variable models journal machine learning research 8 p155201 512007 ajay jasra david stephens christopher c holmes populationbased simulation static inference statistics computing v17 n3 p263279 september 2007 david forsyth okan arikan leslie ikemoto james obrien deva ramanan computational studies human motion part 1 tracking motion synthesis foundations trends computer graphics vision v1 n2 p77254 july 2006