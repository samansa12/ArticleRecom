simple decomposition method support vector machines decomposition method currently one major methods solving support vector machines important issue method selection working sets paper design decomposition methods boundconstrained svm formulations demonstrate working set selection trivial task experimental analysis propose simple selection working set leads faster convergences difficult cases numerical experiments different types problems conducted demonstrate viability proposed method b introduction support vector machine svm new promising technique classication surveys svm example vapnik 1995 1998 scholkopf et al 1998 given training vectors x two classes vector 2 r l 1g support vector technique requires solution following optimization problem l training vectors x mapped higher maybe innite dimensional space function existing common method solve 11 dual nite quadratic programming problem e vector ones c upper bound variables q l l positive semidenite kernel diculty solving 12 density q q ij general zero case q becomes fully dense matrix prohibitive amount memory required store matrix thus traditional optimization algorithms newton quasi newton etc cannot directly applied several authors example osuna et al 1997 joachims 1998 platt 1998 saunders et al 1998 proposed decomposition methods conquer diculty reported good numerical results basically separate training set two sets b n b working set denote b n vectors containing corresponding elements objective value equal 1 n qnn n e n n iteration n xed following subproblem variable b solved qbb qbn qnb qnn permutation matrix q q size b strict decrease objective function holds theoretical convergence studied chang et al 1999 usually special storage using idea cache used store recently used q ij hence computational cost later iterations reduced however computational time still strongly related number iterations main thing aects number iterations selection working sets careful choice b dramatically reduce number iterations main topic paper instead 11 paper decide work dierent svm formulation l dual becomes boundconstrained problem formulation proposed studied mangasarian musicant 1999 friess et al 1998 think easier handle problem without general linear constraints importantly solving 12 numerical diculty deciding whether variable bound generally recommended compare oatingpoint number another one example calculate b 12 use following kkt condition therefore calculate b 16 element b however implementing 16 cannot directly compare 0 c sv light joachims 1998 small introduced consider free c otherwise wrong considered obtained b erroneous hand bounded formulation used appropriate solvers subproblem 29 used possible directly compare 0 c without needing example lin 1999 used method called project gradient implementation values bounds done direct assignments hence safe compare 0 c precise oatingpoint computation c assigned somewhere future oatingpoint comparison c c returns true internal representation later paper also show bounded formulations provide exibility selection working set course may worry 15 produces solution good 12 indeed 15 nding separating hyperplane passing origin maximal margin data x mangasarian musicant 1999 proposition 21 showed conditions 12 15 produce decision function however cristianini shawetaylor 2000 showed approach increase vcdimension error rate classifying test data may good using 12 numerical experiments issue paper conduct comparisons results indicate practically 15 acceptable formulation section 2 tests dierent selections working set show nding good strategy trivial task based observations propose simple selection leads faster convergences dicult cases section 3 implement proposed algorithm software bsvm compare sv light joachims 1998 problems dierent size obtaining classiers training data also compare error rates classifying test data using 12 15 finally section 4 present discussions conclusions preliminary version software bsvm available authors homepage z 2 selection working set among existing methods osuna et al 1997 saunders et al 1998 nd working set choosing elements violate kkt condition platt 1998 special heuristic algorithm mainly case 2 systematic way proposed joachims 1998 software sv light following problem solved represent solution kth iteration rf k gradient f k note jfd j 6 0gj means number components z bsvm available httpwwwcsientuedutwcjlinbsvm zero constraint 22 implies descent direction involving q variables obtained components k nonzero included working set b used construct subproblem 29 note used identifying b search direction joachims 1998 showed computational time solving 21 mainly nding q2 largest q2 smallest elements hence cost oql aordable implementation therefore following sv light solving 15 natural method choose working set b similar problem 24 c otherwise note f becomes 1 problem essentially except strictly enforce 0 note 21 0 possible k 0 c use like draw small circle nd best linear approximation f intersection disk feasible region since used deciding b instead 0 1 used 24 b obtained course consider another type constraints without restricting size consider good method nal iterations consideration small region note solving 23 easy calculating following vector contains indices smallest q elements v interestingly bound constrained formulation solving 23 nding maximal violated elements kkt condition note elements violate kkt condition hence q maximal violated elements smallest q elements v similar relation kkt condition 12 22 easy see kkt condition 12 involves number b appear 22 interpretation given considers possible intervals b select violated points end points intervals approaches reasonable intuitively think nding violated elements natural choice however unlike sv light selection working set perform well rest paper name implementation bsvm table 21 solving problem heart statlog collection michie spiegelhalter taylor 1994 see bsvm much worse sv light table 21 problem heart comparison early iterations light iter obj free c light takes 63 iterations bsvm takes 590 iterations experiment use kx methods use similar stopping criteria initial solution zero column c presents number elements k upper bound c columns number elements b two dierent classes note easy experiments use simple implementation sv light bsvm written matlab observe early iterations components working set class decrease objective values bsvm much slower light addition number variables upper bound seven sv light iterations many bsvm bsvm produces iterations free variables explain observation follows first make assumptions easy description 1 solutions current next iterations respectively 2 b current working set elements class words assume one iteration situation elements working class happens 3 show possible components selected next working set note also know early iterations elements zero nonzero elements v 26 small 23 essentially nding smallest elements rf next iteration since v included working set elements n therefore q small next iteration elements selected rfd becomes smaller example table 21 really observe sign rf changed every early iteration explanation similar note 27 28 hold rbf kernel another popular kernel polynomial kernel attributes data scaled holds hence 27 28 remain valid next explain situation happens early iterations variables reach upper bound since solving following subproblem like following primal problem solved components b class 210 problem separable data hence b general equal c thus algorithm diculties identify correct bounded variables addition decrease objective function becomes slow hand constraint 21 sv light provides excellent channel selecting working set two dierent classes precise zero selecting largest elements like selecting elements conversely smallest elements data seen columns number 21 another explanation rst second order approximations optimization current solution considered xed vector variable problem 13 equivalent solving since 13 like select best q elements linear part 211 minimized similarly 29 equivalent solving clearly main dierence 211 212 b involves term b 2 objective value 212 211 b appears one constraints therefore since using linear approximation selecting working set 212 b b quadratic term considered 23 thus 23 know 2 large hand remains constraint like 2 implicitly minimized one word 21 23 come linear approximation one contains information based observation try modify 23 selecting contains best q2 elements best q2 elements 1 new result table 22 substantial improvement obtained seven iterations algorithms reach solutions number components upper bound objective values also similar table 22 problem heart new comparison early iterations light iter obj free c iter obj free c iteration free figure 21 free variables line bsvm dashed sv light however table 22 bsvm still takes iterations sv light observe slow convergence nal iterations improve performance analyze algorithm detail figure 21 present number free variables iteration clearly seen bsvm goes points free variables since weakest part decomposition method cannot consider variables together iteration q selected larger number free variables causes diculty words component correctly identied c 0 problem numerical accuracy however general easy decide value free variable considered together comparing working set selection 21 sv light strategy selecting q2 elements natural therefore middle iterative process variables correctly identied upper bound number free variables becomes larger leads us conjecture keep number free variables small possible possible strategy achieve adding free variables previous iteration current working set table 23 working sets nal iterations bsvm iter b iter2 table 24 working set nal iterations sv light iter b iter2 second observation elements working set use 23 select b nal iterations components working set shown table 23 table 24 working sets running sv light presented last column table 23 seen working set kth iteration close iteration however table 24 situation serious example nal solutions using 15 25 12 bsvm second half iterations number free variables less 30 note nal iterations algorithm concentrates deciding value free variables since iteration select 10 variables working set subproblem 29 solved gradient 10 elements become zero hence solution 23 mainly comes free variables explains working sets kth iterations similar apparently selection like table 23 appropriate one mentioned earlier weakest part decomposition method cannot consider variables together situation like two groups variables never considered together convergence extermely slow based observations propose following scheme selection working set let r number free variables k r 0 select indices largest minq2 r elements v select q minq2 r smallest elements v b else select q2 smallest elements smallest elements algorithm 21 selection working set q minq2 r elements still problem like 23 minq2 r elements free components largest jrf k previous working set satisfy rf k many elements included next working set exceptional situations bounds happens choose q2 best elements best elements following discussion results table 22 motivation selection described follows consider minimizing f 2 e positive semidenite matrix constraints problem equivalent solving decomposition method used b k 1 working set k 1st iteration written ab k 1 therefore similar 23 let b k next working set contain smallest q elements rf k words elements violate kkt condition selected thus b k include elements holds kth iteration updated k1 equality fails hence like zigzaging process point view solving linear system think considering inequalities equalities together better method avoid zigzaging process addition previous obervations suggest reduction number free variables therefore basically select q2 violated elements kkt condition q2 satised elements free using algorithm 21 bsvm takes 50 iterations fewer light comparing 388 iterations presented table 22 improvement dramatic figure 22 number free variables methods presented clearly seen bsvm number free variables kept small early iterations time q elements considered move upper bound free variables algorithm 21 tends consider subsequent iterations bsvm opportunities push upper bound since feasible region like box say bsvm walks closer walls box think general good property decomposition method faces diculties handeling algorithm 21 belongs class working set selections discussed chang et al 1999 therefore bsvm theoretically converges optimal point 15 iteration free figure 22 free variables line bsvm dashed sv light computational experiments section describe implementation bsvm present comparison bsvm sv light version 32 results show bsvm converges faster light dicult cases computational experiments section done pentium iii500 using gcc compiler table 31 rbf kernel light without shrinking problem n svbsv mis obj iter time svbsv mis obj iter time australian 690 245 190 diabetes 768 447 434 168 41357 130 048 447 435 168 41356 105 039 german 1000 599 vehicle 846 439 414 210 41327 178 085 439 414 210 41302 332 094 letter 15000 569 538 104 45118 238 2285 564 531 103 44698 349 2438 shuttle 4350061906185 1117 528917 2948 4792061646152 1059 524141 1120 44927 dna 2000 696 537 50 42791 326 856 697 533 50 42738 253 614 segment fourclass 862 411 403 168 38387 190 073 408 401 167 38358 124 041 web4 7366 856 285 123 32612 787 1394 869 281 126 32607 876 1982 table 32 rbf kernel table 33 polynomial kernel light without shrinking problem n svbsv mis obj iter time svbsv mis obj iter time australian 690 283 242 95 22798 117 036 284 238 95 22793 188 046 diabetes 768 539 532 231 49941 105 047 538 532 230 49924 99 037 german 1000 624 527 202 51642 244 136 625 527 203 51624 236 103 vehicle 846 440 412 212 42269 90 072 443 403 212 42218 217 069 letter 15000 10341020 172 78391 199 3165 10401015 172 78305 223 2817 shuttle 435001769917698 7248 1556058 3706 1084861770017694 7248 1556017 3008 111819 dna 2000 1103 772 464 90609 222 1239 1103 772 464 90558 216 766 segment fourclass 862 485 480 195 44425 236 070 485 479 195 44376 125 041 table 34 polynomial kernel table 35 rbf kernel using sv light shrinking problem n svbsv mis obj iter time svbsv mis obj iter time australian 690 245 190 98 20164 504 075 223 55 28 8119977 119658 17644 diabetes 768 447 435 168 41356 105 038 377 273 128 30246370 75141 11526 german 1000 599 512 189 50277 311 117 509 vehicle letter 15000 564 531 103 44698 349 2036 152 52 6 5532574 27290 13539 shuttle 4350061646152 1059 524141 1120 4782814881467 137 118841466 24699 124339 dna 2000 697 533 50 42738 253 615 408 segment fourclass 862 408 401 167 38358 124 041 89 74 2 5388574 11866 1941 iteration sv light iteration b bsvm figure 31 problem fourclass number svline free svdashed table rate 10fold cross validation statlog classifying test data adult australian 8536 8623 8565 8551 8594 8406 8217 8029 7797 diabetes 7680 7719 7667 7653 7601 7497 7432 7302 7263 german 7540 7500 7590 7270 6970 6900 6880 6910 6870 heart 8185 8074 8037 7889 7741 7556 7518 7407 7407 segment 9965 9970 9970 9974 9978 9978 9974 9970 9983 vehicle 7495 7897 7956 8167 8239 8535 8523 8617 8534 adult1 8423 8396 8303 8054 8022 7959 7943 7943 7943 adult4 8440 8419 8383 8184 8100 7973 7942 7928 7927 table 37 bsvm accuracy rate 10fold cross validation statlog classifying test data adult australian 8536 8623 8565 8551 8594 8406 8217 8015 7797 diabetes 7693 7719 7693 7653 7601 7510 7432 7302 7263 german heart 8222 8074 7963 7852 7741 7593 7518 7407 7407 segment 9965 9970 9970 9974 9978 9978 9974 9974 9983 vehicle 7495 7897 7956 8179 8250 8523 8523 8629 8534 adult4 8440 8419 8383 8184 8100 7974 7942 7928 7927 light uses following conditions termination criteria fair comparison use similar criteria bsvm note b conditions sv light bsvm set solve subproblem 29 using software tron lin 1999 x tron designed large sparse boundconstrained problems subproblem small fully dense problem redundant operations plan write dense version tron near future pointed existing work decomposition methods expensive step iteration evaluation q columns matrix q words maintain vector q iteration calculate q k1 k involves q columns q avoid recomputation columns existing methods use idea cache recently used columns stored bsvm simple implement leastrecentlyused caching strategy future plan optimize performance using advanced implementation techniques experiments section use 160mb cache size bsvm sv light test problems dierent collections problems australian segment statlog collection michie et al 1994 problem fourclass ho kleinberg 1996 problems adult1 adult4 compiled platt 1998 uci adult data set murphy aha 1994 problems web1 web7 also platt note problems statlog except dna fourclass real numbers scale problems 2 classes treat data rst class second class problems dna adult web problems binary representation conduct scaling test problems using rbf polynomial kernels rbf kernel use problems fourclass use kx adult web problems following setting joachims 1998 polynomial kernel kx cases kernel test usually good initial guess dicult nd optimal c procedure try dierent cs compare error rates obtained cross validation saunders et al 1998 point plotting graph error rate dierent cs typically give bowl shape best value c somewhere middle therefore think may necessary solve problems large cs test cases addition default c sv light 1000 numerical results using presented tables 31 34 column svbsv represents number support vectors bounded support vectors column mis number misclassied training data obj iter columns objective values number iterations respectively note present objective value dual 12 15 also present computational time seconds last column sv light implements technique called shrinking x tron available httpwwwmcsanlgovmoretron table 38 iterations q bsvm australian 13592 3288 1395 682 diabetes 21190 3478 1201 461 german 16610 6530 3453 2126 vehicle 10594 1964 636 335 table 39 iterations q sv light australian 101103 71121 62120 34491 diabetes 75506 43160 37158 41241 german 43411 31143 28084 24529 heart 5772 3321 3068 1982 vehicle 177188 113357 107344 90370 drops variables upper bound iterative process therefore work smaller problem iterations right implemented similar techniques bsvm tables 3134 present results sv light without using shrinking technique except option use default options sv light note use default optimizer sv light version 32 solving 12 following suggestion joachims 2000 link sv light loqo van derbei 1994 achieve better stability give idea eects using shrinking table 35 present results sv light using technique seen shrinking useful technique large problems eectively incorporate shrinking bsvm issue future investigation tables 31 34 see results obtained bsvm matter number support vectors number misclassied data objective values similar sv light suggests using bsvm formula additional term b 2 2 objective function aect training results much another interesting property objective value bsvm always smaller sv light due properties 12 feasible region 12 subset 15 check eectiveness using 15 tables 36 37 present error rates testing small statlog problems 10fold cross validation test data available adult problems also present error rates classifying results suggest problems using 15 produces classier good using 12 light take number iterations however clearly seen methods take many iterations problems tested presented observe slow convergence decomposition methods c large several possible reasons cause diculty think one c increased number free variables iterative process increased addition number free variables nal solution also increased though numbers support bounded support vectors decreased c increased many cases bounded variables c increased separating hyperplane tries many training data possible hence points ie free two planes w decomposition method diculties handling free variables problem illconditioned iterations required selection working set always try push free variables bounded variables number free variables kept small therefore convergence seems faster clearly seen almost cases tables 32 34 bsvm takes fewer iterations sv light problem fourclass table 32 best example show characteristic bsvm problem nal solution number free variables small iterative process bsvm many free variables iterates fact bounded variables nal solution bsvm considers free variables subsequent iterations bounded variables quickly identied number free variables kept small slow local convergence happen however sv light goes iterative process free variables takes lot iterations use figure 31 illustrate observation detail seen figure 31a number free variables sv light increased 70 beginning dicult identify whether bounds especially nal iterations putting free variable bound take thousands iterations hand number free variables bsvm always small less 40 also note sometimes many free variables iterative process still free nal solution hence bsvm may pay much attention wrongly put bounded variables therefore iterations wasted gap bsvm light smaller example adult problems next study relation number iterations q size working set using rbf kernel results tables 38 39 nd using bsvm number iterations dramatically decreased q becomes larger hand using sv light number iterations decrease much since optimization solvers costs certain amount computational time iteration result shows sv light suitable using small q hand algorithm 21 provides potential using dierent q dierent situations conduct experiments large problems optimization solver currently ecient dense subproblems 4 discussions conclusions optimization point view decomposition methods like coordinate search methods alternating variables method fletcher 1987 chapter 22 slow convergences rst second order information used addition working set selection appropriate though strict decrease objective value holds algorithm may converge see example powell 1973 however even disadvantages decomposition method become one major methods svm think main reason decomposition method ecient svm following situations 1 c small support vectors upper bound iterative process 2 problem wellconditioned even though many free variables example think adult problems belong cases dicult problems decomposition methods applications need solutions problems belong situations current decomposition methods may good enough especially smo type platt 1998 algorithm advantage requiring optimization solver ever many cases need solve dicult problems example c large optimization knowledge techniques considered hope practical applications provide better understanding issue regarding svm formulation think 15 simpler 11 similar quality test problems addition paper experiment dierent implementation working set selection cost always oql selecting largest smallest rf k may case regular svm formulation 11 due linear constraint sv light implementation simple 21 special change constraints 21 0 solution procedure may complicated currently add b 2 2 objective function nding hyperplane passing origin separating data pointed cristianini shawetaylor 2000 number 1 added may best choice experimenting dierent numbers future issue improving performance bsvm section 2 demonstrate nding good working set easy task sometimes natural method turns bad choice also interesting note dierent formulations 12 15 similar selection strategies give totally dierent performance therefore new svm formulations careful existing selections working set may perform well finally summarize possible advantages bsvm 1 uses simpler formula boundconstrained optimization problem 2 keeps number free variables low possible general leads faster convergences dicult problems 3 algorithm 21 tends consider free variables current iteration subsequent iterations therefore corresponding columns elements naturally cached acknowledgments work supported part national science council taiwan via grant nsc 892213e002013 authors thank chihchung chang many helpful discussions comments part software implementation beneted help also thank thorsten joachims pavel laskov john platt helpful comments r analysis decomposition methods support vector machines introduction support vector machines practical methods optimization kernel adatron algorithm fast simple learning procedure support vector machines making largescale svm learning practical machine learning fast training support vector machines using sequential minimal optimization search directions minimization support vector machine reference manual tech loqo interior point code quadratic programming tech statisical learning theory tr ctr chihchung chang chihjen lin training vsupport vector regression theory algorithms neural computation v14 n8 p19591977 august 2002 daniela giorgetti fabrizio sebastiani multiclass text categorization automated survey coding proceedings acm symposium applied computing march 0912 2003 melbourne florida chihchung chang chihjen lin training support vector classifiers theory algorithms neural computation v13 n9 p21192147 september 2001 nikolas list hans ulrich simon general polynomial time decomposition algorithms journal machine learning research 8 p303321 512007 weichun kao kaimin chung chialiang sun chihjen lin decomposition methods linear support vector machines neural computation v16 n8 p16891704 august 2004 rameswar debnath masakazu muramatsu haruhisa takahashi efficient support vector machine learning method secondorder cone programming largescale problems applied intelligence v23 n3 p219239 december 2005 hush patrick kelly clint scovel ingo steinwart qp algorithms guaranteed accuracy run time support vector machines journal machine learning research 7 p733769 1212006 daniela giorgetti fabrizio sebastiani automating survey coding multiclass text categorization techniques journal american society information science technology v54 n14 p12691277 december tzuchao lin paota yu adaptive twopass median filter based support vector machines image restoration neural computation v16 n2 p332353 february 2004 simon hill arnaud doucet adapting twoclass support vector classification methods many class problems proceedings 22nd international conference machine learning p313320 august 0711 2005 bonn germany tatjana eitrich bruno lang efficient optimization support vector machine learning parameters unbalanced datasets journal computational applied mathematics v196 n2 p425436 15 november 2006 ningning guo libo zeng qiongshui wu method based multispectral imaging technique white blood cell segmentation computers biology medicine v37 n1 p7076 january 2007 luca zanni thomas serafini gaetano zanghirati parallel software training large scale support vector machines multiprocessor systems journal machine learning research 7 p14671492 1212006 chenglung huang muchen chen chiehjen wang credit scoring data mining approach based support vector machines expert systems applications international journal v33 n4 p847856 november 2007 tatjana eitrich bruno lang optimal working set size serial parallel support vector machine learning decomposition algorithm proceedings fifth australasian conference data mining analystics p121128 november 2930 2006 sydney australia