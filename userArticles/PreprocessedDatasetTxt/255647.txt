teaching strategy memorybased control combining different machine learning algorithms system produce benefits beyond either method could achieve alone paper demonstrates genetic algorithms used conjunction lazy learning solve examples difficult class delayed reinforcement learning problems better either method alone class class differential games includes numerous important control problems arise robotics planning game playing areas solutions differential games suggest solution strategies general class planning control problems conducted series experiments applying three learning approaches lazy qlearning knearest neighbor knn genetic algorithm particular differential game called pursuit game experiments demonstrate knn great difficulty solving problem lazy version qlearning performed moderately well genetic algorithm performed even better results motivated next step experiments hypothesized knn difficulty good examples common source difficulty lazy learning therefore used genetic algorithm bootstrapping method knn create system provide examples experiments demonstrate resulting joint system learned solve pursuit games high degree accuracy outperforming either method alone relatively small memory requirements b introduction two people learn task together benefit different skills brings table result learn better would likewise machine learning methods able work together learn solve difficult problems paper describes lazy learning algorithm genetic algorithm work together produce better solutions either method could produce explore hypothesis two learning algorithms work together outperform either individually focused particular problem agent must perform task task requires several steps accomplish limit feedback well agent performing end task several learning algorithms applied family problems called delayed reinforcement problems widrow 1987 atkeson 1990 watkins 1989 barto sutton watkins 1990 millan torras 1992 moore atkeson 1993 little done evaluate power combining different types learning algorithms problems one way characterizing delayed reinforcement problems learning solve markov decision problem van der wal 1981 markov decision problems agent develops mapping set states set actions possibly different ones state optimal strategy given state depends current state actions directed toward achieving goal performing task payoff penalty action awarded immediately delayed reinforcement problems apply zero payoff intermediate states apply actual reward end sequence one class problems frequently modeled markovian decision problem class differential games differential games require players make long sequence moves behaviors strategies modeled differential equations finding solution differential game consists computing value game terms expected payoff determining optimal strategies players yield value differential games difficult solve yet important solving wide variety multiagent tasks widespread application military entertainment industries recently systems intelligent highways air traffic control railroad monitoring ship routing using differential game theory assist agents optimizing often competing goals generally strategies solving games used planning intelligent agents thus making approach discussed applicable broader domain control problems study begin considering differential game involved one agent trying pursue capture another ie pursuit game earlier research showed least one implementation task known evasive maneuvers grefenstette ramsey schultz 1990 solved genetic algorithm ga developed lazy learning approach using knearest neighbor knn task hoping demonstrate lazy learning could perform well better ga made task substantially harder study limitations lazy learning methods class problems complicated task described section 32 also resembles complicated planning tasks agent satisfy several goals simultaneously chapman 1987 experiments show successful developing method solve difficult reinforcement learning task key idea behind success combined use lazy learning gas observed comparing two lazy methods knn adaptation qlearning genetic algorithms lazy methods learn solve task dependent good examples database later found best learning agent first used ga generate examples switched knn reaching certain performance threshold experiments demonstrate significant improvement performance lazy learning overall accuracy memory requirements result using techniques combined system also performed better ga alone demonstrating two learning algorithms working together outperform either method used alone previous work recently considerable work done applying learning algorithms markov decision problems date little done apply algorithms differential games one exception grefenstettes samuel system uses genetic algorithm addition evasive maneuvers task grefenstette 1991 applied samuel aerial dogfighting target tracking ramsey grefenstette 1994 used casebased method initializing samuel population solutions dependent current environment use ga jump start lazy learner ramsey grefenstette use lazy learner jump start ga suggests combined strategy lazy learner ga transmit information directions could powerful combination related research gordon subramanian 1993a 1993b use approach similar explanation based learning ebl incorporate advice genetic algorithm using samuel ga multistrategy apprach spatial knowledge base highlevel strategic guidance human teacher encoded using rule compilation operationalizes rules encoding form suitable samuel use samuel uses refines advice genetic algorithm idea using lazy learning methods delayed reinforcement tasks recently studied small number researchers atkeson 1990 employed lazy technique train robot arm follow prespecified trajectory moore 1990 took advantage improved efficiency provided storing examples kdtrees using lazy approach learn several robot control tasks recently moore atkeson 1993 developed prioritized sweeping algorithm interesting examples q table focus updating mccallum 1995 developed nearest sequence memory algorithm lazy algorithm solving control problems plagued hidden state hidden state artifact perceptual aliasing mapping states perceptions onetoone whitehead 1992 mccallum showed algorithm lazy methods reduce effects perceptual aliasing appending history information state information since approach stores complete sequences minimized effects hidden state another study aha salzberg 1993 used nearestneighbor techniques train simulated robot catch ball study provided agent knew correct behavior robot therefore provided corrected actions robot made mistake approach typical nearestneighbor applications rely determining good actions storing examples case idea examples good needed approach determine examples one popular approaches reinforcement learning using neural network learning algorithms often error backpropagation algorithm used simple multistep control problems widrow 1987 nguyen widrow 1989 using knowledge correct control action train network millan torras 1992 used reinforcement learning algorithm embedded neural net control variables permitted vary continuously addressed problem teaching robot navigate around obstacles considerable research performed using form reinforcement learning called temporal difference learning sutton 1988 temporal difference methods apply reinforcement throughout sequence actions predict future reinforcement appropriate actions performing task specifically predictions refined process identifying differences results temporally successive actions two popular temporal difference algorithms acease barto sutton anderson 1983 barto et al 1990 qlearning watkins 1989 original work barto et al 1983 demonstrated cart pole problem could solved using method clouse utgoff 1992 later used acease separate teacher cart pole prob lem applied qlearning navigating race track lin 1991 used qlearning teach robot navigate halls classroom building plug wall socket recharge batteries describe lazy variant qlearning show also capable learning complex control tasks addition dorigo colombetti 1994 colombetti dorigo 1994 describe approach using reinforcement learning classifier systems teach robot approach pursue target approach uses separate reinforcement program monitor performance robot provide feedback performance learning occurs genetic algorithm applied classifiers fitness determined reinforcement program recently michael littman 1994 observed reinforcement learning applied multiagent activities context markov games littman expanded watkins qlearning algorithm cover two players simplified game soccer embedded linear programming determine optimal strategy prior play applied modified accounted competitive goals players update estimates expected discounted reward player recent work learning strategies game playing begun deal issues colearning superficial level strategic games chess othello pell 1993 developed environment deriving strategies calls symmetric chess like games metagamer focused translating rules constraints game strategy using declarative formulation games characteristics metagamer applied chess checkers noughts crosses ie tictactoe go yielded performance intermediate level games smith gray applied call coadaptive genetic algorithm learn play othello coadaptive ga genetic algorithm fitness values members population dependent fitness members population found able control development niches population handle several different types opponents finally tesauro used temporal difference learning tesauro 1992 neural networks tesauro sejnowski 1989 train backgammon program called tdgammon backgammons stochastic component move determined part roll dice distinguishes deterministic games chess despite additional complexity tdgammon currently playing master level 3 problem reinforcement learning rl challenging part delay taking action receiving reward penalty typically agent takes long series actions reward hard decide actions responsible eventual payoff lazy eager approaches reinforcement learning found literature common eager approach use temporaldifference learning neural networks barto et al 1983 1990 clouse utgoff 1992 tesauro 1992 advantages lazy approach threefold first minimal computational time required training since training consists primarily storing examples traditional lazy approach knearest neighbor second lazy methods shown good functionapproximators continuous state action spaces atkeson 1992 see become important task learning play differential games third traditional eager approaches reinforcement learning assume tasks markov decision problems tasks nonmarkovian eg history significant information must appended state encapsulate prior state information order approximate markov decision problem since lazy approach stores complete sequences nonmarkovian problems treated similar fashion markovian problems class rl problems studied also studied field differential game theory differential game theory extension traditional game theory game follows sequence actions continuous state space achieve payoff isaacs 1963 sequence modeled set differential equations analyzed determine optimal play players also interpret differential games version optimal control theory players positions develop continuously time goal optimize competing control laws players friedman 1971 31 differential game theory originated early 1960s isaacs 1963 framework formal analysis competitive games differential game dynamics game ie behaviors players modeled system first order differential equations form dt set actions taken p players time k vector real euclidean nspace denoting position play ie state game h j history game jth dimension state space words differential equations model actions taken players game change state game time games initial state game k 0 given object analyzing differential game determine optimal strategies player game determine value game ie expected payoff player assuming players follow optimal strategies details see sheppard salzberg 1993 pursuit game special type differential game two players called pursuer p evader e evader attempts achieve objective frequently escape fixed playing arena pursuer attempts prevent evader achieving objective examples include simple games childrens game called tag popular video game pacman much complicated predatorprey interactions nature examples illustrate common feature pursuit gamesthe pursuer evader different abilities different speeds different defense mechanisms different sensing abilities one classic pursuit game studied differential game theory homicidal chauffeur game game think playing field open parking lot single pedestrian crossing parking lot single car driver car chauffeur trying run pedestrian although car much faster pedestrian pedestrian change direction much quickly car typical formulation car pedestrian traveling fixed speeds car fixed minimum radius curvature pedestrian able make arbitrarily sharp turns ie radius curvature zero basar olsder 1982 analyzing game turns solution relatively simple depends four parametersthe speed car speed pedestrian radius curvature car lethal envelope car ie distance car pedestrian considered close enough hit pedestrian isaacs 1963 shows assuming optimal play players ability p capture e conversely e escape depends ratio players speeds p radius curvature intuitively optimal play p turn randomly lined e turn sharply otherwise optimal strategy e head directly towards p inside radius curvature turn sharply since es strategy interesting focus learning evade similar game 32 evasive maneuvers task evasive maneuvers task differential game variation homicidal chauffeur game even though solution homicidal chauffeur game intuitive actual surface characterizing solution highly nonlinear thus reasonably expect surface extensions problem discussed paper difficult characterize grefenstette et al 1990 studied evasive maneuvers task demonstrate ability genetic algorithms solve complex sequential decision making problems twodimensional simulation single aircraft attempts evade single missile initially implemented pursuit game grefenstette et al later extended make substantially difficult game play occurs relative coordinate system centered evader e relative frame reference search space reduced games determined starting positions p uses fixed control law attempt capture e e must learn evade p even basic game difficult homicidal chauffeur game pursuer variable speed evader nonzero radius curvature extended version includes second pursuer makes problem much harder unlike singlepursuer problems twopursuer problem known optimal strategy imado ishihara 1993 initial states possibility escape second gave evader additional capabilities onepursuer game e controls turn angle time step thus basically zigzags back forth makes series sharp turns path p escape twopursuer game gave e ability change speed also gave bag smoke bombs limited time help hide e pursuers definition twopursuer task pursuers p1 p 2 identical maneuvering sensing abilities use control strategy anticipate future location e aim location capture fewest time steps begin game random locations fixedradius circle centered evader e initial speeds p1 p2 much greater speed e lose speed maneuver direct proportion sharpness turns make maximum speed reduction 70 scaled linearly turn reduction speed maximum turn angle allowed 135 regain speed traveling straight ahead limited fuel speed p1 p2 drops minimum threshold e escapes wins game e also wins successfully evading pursuers 20 times steps ie p1 p2 run fuel paths either p1 p2 ever pass within threshold range es path game e loses ie pursuer grab e figure 1 use term game include complete simulation run beginning initial placements players ending either wins loses 20 time steps later playing one pursuer capabilities e identical simulated aircraft used grefenstette et al one pursuer e controls turn angle sufficient play game well two pursuers p1 p2 game e additional information opponents information includes 13 features describing state game including es speed angle previous turn game clock angle defined p 1ep 2 range difference p1 p 2 also eight features measure p1 p2 individually speed bearing heading distance bearing measures position pursuer relative direction e facing eg e facing north p1 due east bearing would 3 oclock heading angle es direction pursuers direction fleeing two pursuers e adjust speed turn angle time step also periodically release evader pursuer pursuer evader caught1 12334 figure 1 game e caught smoke bomb introduces noise sensor readings p1 p 2 smoke released turn angle pursuer shifted random factor 50 current turn angle severity turn increases potential effect smoke 4 learning algorithms following sections discuss details experiments three learning algorithms motivate need learning strategy combining eager learning teacher lazy learning performer explored several algorithms determine applicability lazy learning control problems general pursuit games particular began examining ability qlearning learn play evasive maneuvers game adapt qlearning large continuous state space resulted lazy variant standard qlearning tried traditional lazy learning approach knearest neighbors finally experimented eager learning method genetic algorithms compare two lazy methods 41 lazy qlearning evasive maneuvers qlearning solves delayed reinformement learning problems using temporal difference learning rule watkins 1989 td methods usually assume feature space variables predicted discrete sutton 1988 tesauro 1992 qlearning represents problem using lookup table contains states naturally causes problems large continuous state spaces encountered differential games therefore develop method predicting rewards stateaction pairs without explicitly generating resulting algorithm lazy version qlearning rather constructing complete lookup table implementation qlearning stores examples similar set instances produced lazy method knn begins generating set actions random particular game actions result successful evasion instead algorithm applies payoff function defined determine reward sequence stateaction pairs initially stores actual payoff values pairs generating first set pairs learning proceeds follows first assuming neighboring states require similar actions specify two distance parameters one states one actions 1 respectively noting distances normalized purpose parameters guide search instance database system begins evasive maneuvering game initializing simulator simulator passes first state state matcher locates states database within 1 current state state matcher failed find nearby states action comparator selects action random otherwise action comparator examines expected rewards associated states selects action highest expected reward resulting action passed simulator game continues termination also probability 03 generating random action regardless finds table permits fill database ie exploring state space learning passes resulting action simulator game continues termination point simulator determines payoff q function updates database using complete game end game system examines stateaction pairs game stores database stateaction pair new along reward game pair already exists predicted reward updated follows qx predicted reward state x corresponding action j learning rate ae actual reward fl discount factor ey maximum q value actions associated state state state follows action applied state x reward determined using payoff function grefenstette et al 1990 namely evades pursuers captured time pairs game compared pairs database distance stored state action less 1 2 respectively stateaction pair game stored stateaction pairs q value updated 42 knn evasive maneuvers lazy learning classical approach machine learning pattern recognition commonly form knearest neighbor algorithm knn rarely used markov decision problems represent pursuit game format amenable algorithm successful lazy approach must database full correctly labeled examples knn expects example labeled class name difficulty determine correct action store state formulate markov decision problems classification problems letting state variables correspond features examples actions correspond classes typically classification tasks assume small set discrete classes assigned require quantization state space action space instead use interpolation action produced knn classifier order know correct action store state must least wait determined outcome game deciding label step one example added time step however even successful game e evades p cannot sure actions every time step correct ones general construct initial database instances simulator generated actions randomly evaded p complete game corresponding stateaction pairs engagement stored point knn used future games states passed simulator classifier searched database k nearest neighbors selected action averaging associated actions knn failed produce game ended successful evasion game replayed example generator randomly selecting actions play ended evasion evasion occurred corresponding sequence states actions ie complete game stored database evasion usually occurred 20 time steps since rare lazy learner pursuers speeds dropped threshold thus stored game typically consisted 20 stateaction pairs implementation uses euclidean distance find k nearest neighbors arithmetic mean control values determine appropriate actions distance computed follows instance attrib nearest neighbor determined simply 8instance e fails evade using stored instances reset game starting position generate actions randomly e succeeds also generate random actions probability 001 regardless performance resulting set examples added database initial experiments using knearest neighbors varied k 1 5 determined yielded best performance completely surprising averaging control values k 1 tended cancel values extreme example three instances indicated turns 90 degrees left 5 degrees right 85 degrees right selected action would turn course averaging cyclic values example 359 degrees close 1 degree improving averaging process might enable k 1 perform better examples consisted randomly generated games resulted success e thus could assume least es actions correct random games every action taken e random database checked nearby neighbors 43 ga evasive maneuvers grefenstette et al demonstrated genetic algorithms perform well solving single pursuer game typically gas use rules called classifiers simple structures terms antecedent consequent represented binary attributes booker goldberg holland 1989 holland 1975 knowledge evasive maneuvers problem requires rules terms numeric values therefore modified standard ga representation operators problem using formulation similar grefenstette et al 1990 call set rules plan ga plan consists 20 rules general low 1 state 1 high 1 low n state n high n clause antecedant compares state variable lower upper bound dont care conditions generated setting corresponding range maximally gen eral map rule form chromosome ga store attribute bounds followed action example suppose following rule single pursuer problem previous turn 90 chromosome corresponding rule would associated rule rule strength associated plan plan fitness population may contain fifty plans compete ga system strength fitness values described determine winners competition initially rules maximally general result rules match states one rule selected uniform probability following training game rules fired generalized specialized ga using hillclimbing modify upper lower limits tests state variable follows lb ub lower upper bounds respectively rule fired state fi learning rate current state within bounds predicate bounds shift closer state based learning rate 01 study hand state outside bounds nearer bound adjusted shifting toward value state following game strengths rules fired updated based payoff received game payoff used qlearning given payoff function strength rule fired game updated using profit sharing plan grefenstette 1988 follows c profit sharing rate experiments ae payoff received estimate mean strength rule oe estimate variance rule strength plan fitness calculated running plan set randomly generated games computing mean payoff set tests testing plan highest fitness used control e heart learning algorithm lies application two genetic operators mutation crossover rules within plan selected mutation using fitness proportional selection goldberg 1989 namely probability selection determined strength r 8s2rules strength rules set rules plan r rule interest probability selection plans determined similarly using plan fitness rather rule strength details implementation see sheppard salzberg 1993 44 results algorithms variations evasive maneuvers game ran ten experiments produce learning curves combined results ten experiments averaging algorithms performance regular intervals estimated accuracy algorithm testing results training 100 randomly generated games results qlearning experiments encouraging led next phase study applied traditional lazy learning method knearest neighbors knn evasive maneuvers task found knn work well considered eager learning algorithm genetic algorithm choice motivated previous work grefenstette et al indicated ga capable solving type task fact able replicate results onepursuer problem scale ga still worked quite well twopursuer game 441 performance lazy qlearning onepursuer task qlearning extremely well initially figure 2 reaching 80 evasion within first 5000 games performance flattened peak performance experiments stopped 90 apparent plateau 5000 games 30000 games performance remained range 8085 performance jumped another plateau 90 remainder experiment qlearnings performance twopursuer task also encouraging reached 60 evasion within 5000 games continued improve reaching plateau 80 plateau maintained throughout remainder experiment since implementation qlearning uses form lazylearning results led us believe might percent success stored games one pursuer two pursuers figure 2 performance qlearning one twoplayer pursuit games2060100 percent success stored games one pursuer two pursuers figure 3 performance knn one twoplayer pursuit games possible design traditional lazy method ie knn solve evasion task first however hypothesis supported see next section 442 performance knn figure 3 shows well knn performed two versions evasive maneuvers game number training examples games increased figure compares performance two problems respect number games stored game contains 20 stateaction pairs experiments indicate problem escaping single pursuer relatively easy solve knn developed set examples 95 successful storing approximately 1500 games eventually reached almost perfect performance distance p e start game guarantees escape always possible however results disappointing e given task learning escape two pursuers fact lazy learning approach difficulty achieving level performance 45 demonstrates two pursuer problem significantly difficult knn one possible reason knns poor performance twopursuer task presence irrelevant attributes known cause problems nearest neighbor algorithms aha 1992 salzberg 1991 experimented method similar stepwise forward selection devijver kittler 1982 determine set relevant attributes however determining relevant attributes dynamic environment difficult reason determining good examples difficult know attributes use many successful examples generated another possible reason poor performance knn two pursuer task size search space onepursuer problem state space contains 75 theta 10 15 points whereas twopursuer evasion state space 29 theta 10 33 points one pursuer game showed good performance 5700 games achieve similar coverage state space twopursuer game would require roughly 54 theta 10 22 games assuming similar distributions games training data likely reason knns troubles concluded generating bad examples early phases game stated lazy learner needs correct action something close stored almost every state memory strategy collecting examples play random games first store games e succeeded escaping however many actions taken random games incorrect e might escape one two particularly good actions game lasts 20 time steps 20 stateaction pairs stored since lazy learning approach way firstsee section 52 throw away examples collected many bad examples could get stuck forever low level performance 443 performance ga show results ga experiments figure 4 knn ga performs well faced one pursuer fact achieves near perfect performance 15000 games good performance 90 5000 games number games somewhat inflated ga evaluates 50 plans generation thus counted one generation 50 games fact simulation ran 500 generations ie 25000 games experiments striking difference performance knn genetic algorithm ga learned excellent strategies twopursuer problem nearest neighbor percent success games one pursuer two pursuers figure 4 performance genetic algorithm one twoplayer pursuit games table 1 comparing learning evasive maneuvers task convergence algorithm one pursuer two pursuers knn 969 423 qlearning 933 817 ga 996 945 qlearnings performance though much better knn still inferior ga indeed ga achieved 90 success 16000 games 320 generations success rate continued improve reached approximately 95 444 comparing one twopursuer evasion figure 5a shows sample game e evades single pursuer gives intuition strategy e learn essentially e keeps turning sharply p unable match changes direction although three algorithms well task closer examination results reveals interesting differences knn eventually reached successful evasion rate 9798 reached 93 evasion 10000 games superior qlearnings asymptotic performance knn performed better ga 5000 games course ga eventually achieved near perfect performance qlearning also learned rapidly beginning exceeding gas ability first 3000 games learning slowed consider ably fact point ga performing nearly perfectly qlearnings performance around 85 twice many games ga qlearning achieving 91 evasion still performing considerably poorer ga knn table 1 shows results comparing three algorithms two evasion tasks pursuer evader pursuer pursuer evader b two pursuers one evader one pursuer one evader figure 5 sample games e successfully evades convergence considered algorithms converged showed improvement 500 games knn qlearning 100 generations ga recognizing difficulty twopursuer task relative onepursuer task see profound differences performance three approaches see figure 5b sample game e evades two pursuers ga started slowly outperformed knn qlearning 3000 games ga began improve rapidly passing knn almost immediately catching qlearning additional 5000 games end results show ga surpassing qlearning margin 11 knn margin 52 striking result though poor performance knn twopursuer game next set improve figure 5 combining ga lazy learning initially surprised knns performance twopursuer task attempt improve performance considered provide good examples knn based hypothesis primary cause poor performance poor quality training experiences lazy learning work effectively control tasks stored examples must high probability good ones ie action associated stored state correct nearly correct credit assignment problem difficulty tasks designed initial training difficult algorithm gll init population run genetic algorithm run ga one generation select best plan determine performance ga perf evaluate performance experiments best plan ga evades evade store examples stores 20 examples evaluate lazy test 100 games figure lazy learner contrast ga initially searches wide variety solutions problems studied tends learn rapidly early stages observations suggested twophase approach adopted first trained ga used provide examplars bootstrap knn 51 bootstrapping nearest neighbor bootstrapping idea requires one algorithm train time communicate learned second algorithm point second algorithm takes later first algorithm adds additional examples alternation continues combined system reaches asymptotic limit ga learned much better twopursuer game selected first learner knn second details communication teaching phase given figure 6 using approach examples continue accumulate genetic algorithm learns task results training knn using ga teacher shown figure 7 call system gll first uses ga uses lazy learning algorithm ie knn points shown graph averages 10 trials first threshold set 0 meant ga provided examples knn beginning training second threshold set 50 permit ga achieve level success approximately equal best performance knn thus plans achieved least 50 evasion allowed transmit examples knn finally threshold set 90 limit examples knn games highly trained ga made decisions examples store almost immediately reaches level equal best performance percent success examples ga0 ga50 ga90 figure 7 results ga teaching knn knn around 45 improves somewhat erratically steadily reaches performance approximately 97 success figure shows performance plotted number examples stored number examples stored higher number examples stored knn alone halt learning 50000 examples consistent earlier knn experiments performance would 85 range still enormous improvement knns performance better ga starts performing high level 70 quickly exceeds 90 success 50000 examples gll obtained success rate 95 individual trials random sets 100 games achieving 100 success addition learning curve much smoother indicating knn probably storing many bad examples confirms part earlier hypothesis knns fundamental problem storage bad examples stores examples bad actions take bad actions later performance continue poor whenever new state similar one bad examples finally performance always superb exceeding gas 90 success rate first set examples gll converged nearperfect performance 10000 examples one striking observation gll performed better ga throughout learning example achieved 5080 success ga still achieving 210 success gll remained ahead ga throughout training even achieved 98100 evasion ga still achieving around 95 evasion neither ga knn able obtain high success rate number trials 52 reducing memory size bootstrapping algorithm gll performs well even small number examples provided ga even outperforms teacher ga training amount knowledge required ga perform well task quite smallonly 20 rules stored single plan number examples used though small comparison knn still requires significantly space time rules ga consequently decided take study one step attempted reduce size memory store lazy learning phase gll zhang 1992 skalak 1994 pattern recognition literature eg dasarathy 1991 algorithms reducing memory size known editing methods however lazy learning usually applied control tasks able find editing methods specifically tied type problem therefore modified known editing algorithm problem call resulting system gle ga plus lazy learning plus editing gll performs quite well described would like reduce memory requirements without significantly affecting performance early work wilson 1972 showed examples could removed set used classification suggested simply editing would frequently improve classification accuracy way pruning improves decision trees mingers 1989 wilsons algorithm classifies example data set k nearest neighbors points incorrectly classified deleted example set idea points probably represent noise tomek 1976 modified approach taking sample 1 data classifying sample remaining examples editing proceeds using wilsons approach ritter et al 1975 described another editing method differs wilson points correctly classified discarded ritter method similar harts 1968 basically keeps points near boundaries classes eliminates examples midst homogenous region editing approach took combined editing procedure ritter et al sampling idea tomek devijver 1986 began generating ten example sets set consisted single set examples ga selected set best performance 10000 test games case obtained nearly perfect accuracy 1700 examples next edited memory base classifying example using examples set phase used five nearest neighbors point correctly classified deleted probability 025 probability selected arbitrarily used show performance changed editing occurred prior editing pass data example set tested using 1nn percent evasion examples figure 8 results editing examples provided genetic algorithm knn 10000 random games one complication classifying points editing class actually threedimensional vector three different actions two realvalued turn angle speed one binary emitting smoke clear exact match would strict constraint therefore specified range around 3vector within system would consider two classes addition three values normalized equalize effect range measurement results running gle 1700 examples summarized figure 8 logarithmic scale used xaxis highlight fact accuracy decreased slowly almost examples edited read right left graph shows accuracy decreases number examples decreases 11 examples gle achieved better 80 evasion substantially better best ever achieved knn alone 21 examples comparable size plan ga gle achieved 86 evasion performance remained high level greater 90 success 66 examples thus clear small well chosen set examples yield excellent performance difficult task furthermore small memory base guarantees online performance knn quite fast 6 discussion conclusions study considered approaches developing strategies learning play differential games particular examined several methods learning evasion strategies pursuit games optimal strategies differential games determined solving system differential equations even simple games resulting strategies complex many games known closedform solutions illustrated complexity differential strategies pursuit game experiments demonstrated ability three algorithms ga knn lazy q learning perform well simple task one pursuer show increasing difficulty adding second pursuer adversely affects two algorithms knn lazy qlearning knn particular considerable difficulty scaling complex task thus left question whether even possible lazy learning techniques knearest neighbor perform well problems type motivated second phase study used gabased teacher train knn twopursuer task experiments reported show possible use genetic algorithms conjunction lazy learning produce agents perform well difficult delayed reinforcement learning problems experiments also demonstrate clearly power teacher source good examples lazy learning methods complex control tasks teacher probably necessary component lazy memorybased method experiments show genetic algorithm used learn plans control laws complex domains train lazy learner using learned rules generate good examples result hybrid system outperformed parent systems hybrid approach course applied many ways example standard qlearning notoriously slow converge approaches could used accelerate one surprising result performance gll outperformed ga point training hypothesize best examples given generation passed knn rather experiences ga generation fact gll outperformed ga right away indicates perhaps could used teach ga instead way around addition found editing example set produced relatively small set examples still play game extremely well makes sense since editing served identify strongest examples database given poor examples still likely included early stages learning might possible careful editing reduce size memory even question related theoretical work salzberg et al 1991 studies question find minimalsize training set use helpful teacher explicitly provides good examples helpful teacher similar oracle used clouse utgoff 1992 except provides theoretically minimal number examples required learning 7 next steps current implementation takes first step towards truly combined learning system two learners would assist learning task approach uses one algorithm start learning process hands results first algorithm second algorithm continue learning task envision general architecture different learning algorithms take turns learning depending one learning effectively given time architecture expand capabilities learning algorithms tackle increasingly difficult control problems one possible future direction use genetic operators methods directly examples lazy learning approach rather producing rules begin set examples mutate directly using genetic operators evolve database ie example set perform task one approach might examine frequency examples used successfully evade select frequently used examples examples converted plan ga specifying range attribute example results new set rules sufficient construct plan new plan seeded population ga use general problem determining optimal strategies differential games complex solving games involves solving system differential equations learning solutions games involves simultaneous learning players means players must learn highly dynamic environment rather player learning counter single constant strategy player must adapt strategy changing strategy opponent environment one must avoid prematurely converging fixed solution study problems building environment analyzing learning algorithms multiagent environments specifically wish explore effects sequential decision making several agents learning time exploring ability agent apply one approach learn evasion tactics another agent using perhaps different approach develop pursuit strategies also pit strategy study whether single learning algorithm develop multiple solutions reactive control task acknowledgments thanks david aha anonymous reviewers special issue many valuable comments earlier draft paper also thanks diana gordon john grefenstette simon kasif sk murthy helpful comments ideas formative stages work material based upon work supported part national science foundation grant nos iri9116843 iri9223591 r learning catch applying nearest neighbor algorithms dynamic control tasks tolerating noisy using local models control movement neuronlike adaptive elements solve difficult learning control problems learning sequential decision making gabriel dynamic noncooperative game theory classifier systems genetic algorithms planning conjunctive goals training agents perform sequential behavior editing rate multiedit algorithm pattern recognition statistical approach robot shaping developing autonomous agents learning differential games genetic algorithms search credit assignment rule discovery systems based genetic algo rithms lamarkian learning multiagent environments learning sequential decision rules using simulation models competition condensed nearest neighbor rule differential games mathematical theory applications warfare topics programming robots using reinforcement learning teaching markov games framework multiagent reinforcement learning reinforcement connectionist approach robot path finding nonmazelike environments empirical comparison pruning methods decision tree induction efficient memorybased learning robot control prioritized sweeping reinforcement learning less data less time truck backerupper example self learning neural networks strategy generation evaluation metagame playing case based reasoning papers distance metrics instancebased learning learning helpful teacher prototype feature selection sampling random mutation hill climbing algorithms learning predict methods temporal differences practical issues temporal difference learning parallel network learns play backgammon experiment edited nearestneighbor rule stochastic dynamic programming learning delayed rewards reinforcement learning adaptive control perception action asymptotic properties nearest neighbor rules using edited data selecting typical instances instancebased learning tr editing rate multidit algorithm planning conjunctive goals credit assignment rule discovery systems based genetic algorithms parallel network learns play backgammon classifier systems genetic algorithms using local models control movement adaptation natural artificial systems practical issues temporal difference learning reinforcement connectionist approach robot path finding nonmazelike environments teaching method reinforcement learning selecting typical instances instancebased learning tolerating noisy irrelevant novel attributes instancebased learning algorithms reinforcement learning adaptive control perception action prioritized sweeping training agents perform sequential behavior robot shaping genetic algorithms search optimization machine learning empirical comparison pruning methods decision tree induction learning sequential decision rules using simulation models competition learning predict methods temporal differences distance metrics instancebsed learning ctr john w sheppard colearning differential games machine learning v33 n23 p201233 novdec 1998