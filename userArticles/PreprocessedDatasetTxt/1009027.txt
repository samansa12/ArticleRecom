corpus structure language models ad hoc information retrieval previous work recently developed languagemodeling approach information retrieval focuses documentspecific characteristics therefore take account structure surrounding corpus propose novel algorithmic framework information provided documentbased language models enhanced incorporation information drawn clusters similar documents using framework develop suite new algorithms even simplest typically outperforms standard languagemodeling approach precision recall new interpolation algorithm posts statistically significant improvements metrics three corpora tested b introduction well known basic problem information retrieval determine relevant particular document query automatic ad hoc retrieval setting examples relevant documents supplied given absence explicit relevance evidence important consider information sources exploited methods patterned classic tfidf document vector approach text representation focus mostly permission make digital hard copies part work personal classroom use granted without fee provided copies made distributed profit commercial advantage copies bear notice full citation first page copy otherwise republish post servers redistribute lists requires prior specific permission andor fee july 25292004 sheffield south yorkshire uk utilizing withindocument features term frequen cies information drawn corpus whole generally consists aggregates statistics gathered document considered isolation example inverse document frequency based checking docu ment whether document contains particular term recent work demonstrated eectiveness alternative approach wherein probabilistic models text generation constructed documents induced language models lms used perform document ranking 15 5 like tfidf related techniques though lang uagemodeling methods typically use individualdocu ment features corpuswide aggregates cor pus term counts generally employed smoothing unseen text assigned nonzero probability neither aforementioned approaches typically makes use potentially powerful source information similarity structure corpus clusters convenient representation similarity whose potential improving retrieval performance long recognized 4 16 point view one key advantage provide smoothed representative statistics elements recognized statistical natural language processing time 3 example could infer document containing certain query term still relevant document belongs cluster whose component documents generally contain term however relying clusters alone potential draw backs clustering retrieval time expensive oline clustering seems denition queryindependent therefore may based factors irrelevant user information need also cluster statistics may overgeneralize respect specic member documents therefore propose framework incorporating corpusstructure information using precomputed overlapping clusters individualdocument information importantly although cluster formation queryindependent within framework choice clusters incorporate depend query consider several many possible algorithms arising specic instantiations framework include novel methods special cases standard noncluster based lm approach variant clusterbased aspect model 9 empirical evaluation consists experiments array settings created varying several parameters metaparameters include corpus information representation eg language models versus tfidfstyle vec tors applicable smoothing method selected nd even worstperforming novel algorithms competitive lm approach indeed always provides substantial improvement recall general algorithms provide good performance comparison number recently proposed methods thus demonstrating integration approach incorporating document corpusstructure information eective way improve ad hoc retrieval notational conventions use q c c denote document query cluster corpus respectively xed vocabulary assumed use notation pd language model assigns probabilities text strings xed vocabulary induced pre specied method pc language model induced c section 5 describes induction methods used experiments convenient use kronecker delta notation set denitions argument statement holds 0 otherwise 2 retrieval framework noted rank documents respect query desire perdocument scores rely information drawn particular documents contents document situated within similarity structure ambient corpus structure representation via overlapping clusters document clusters attractive choice representing corpus similarity structure see 16 chapter 3 extended dis cussion clusters thought facets corpus users might interested given particular document relevant user several reasons dierent users dierent reasons believe set overlapping clusters 1 forms better model similarity structure partitioning corpus furthermore employing intersecting clusters may reduce information loss due generalization clustering introduce 16 pg 44 information representation motivated empirical successes languagemodelingbased approaches 15 5 use language models induced documents clusters information representation thus pd q pcq specify initial knowledge relation query q particular document cluster c respectively however section 6 shows using tfidf representation also yields performance improvements respect appropriate baseline though degree using language models information integration assign ranking documents corpus c respect q want score q way incorporates information queryrelevant corpus facets belongs one could compute clusters specic q retrieval time eciency considerations compel us create clustersc include soft probabilistic clusters category set clusters advance hence queryindependent fashion compensate retrieval time base choice appropriate facets query might cluster information used discussion indicates clusters serve two roles insofar approximate true facets corpus aid selection relevant documents would want retrieve belong clusters corresponding facets interest user hand clusters also capacity smooth individualdocument language models since pool statistics multiple documents finally must remember overreliance pcq overgeneralize failing account documentspecic information encoded pd q observations motivate algorithm template shown figure 1 template fairly general standard languagemodeling approach 15 aspect model concrete instantiations template choice facetsq corresponds utilizing clusters selection role scoring step thought integrating pd q clusterbased language models smoothing role optional reranking step used way bias nal ranking towards documentspecic formation desired note reranking change average noninterpolated precision absolute precision recall retrieval results therefore use necessary enhance average precision section 6 reports experiments studying ecacy oine create given q n number documents retrieve 2 c choose cluster subset facetsq score weighted combination pd q pcqs c 2 facetsq set topdocsn rankordered list n topscoring documents optional rerank 2 topdocsn pd q return topdocsn figure 1 algorithm template next section describe number specic algorithms arising template concentrating degree dependence clusterinduced language models 3 retrieval algorithms table 1 summarizes algorithms consider represent choices many possible ways instantiate template figure 1 preference picking algorithms towards simpler methods focus impact using cluster information opposed impact tuning many weighting parameters first step cluster formation selection many algorithms used create clustersc set overlapping document clusters required figure 1s template experiments simply document form basis cluster cohortd consisting k 1 nearest neighbors k free parameter note two clusters dierent basis documents may facetsq score rerank pd q basisselect fcohortdg topclusters q pd q jfacetsq dj 0 redundant setselect topclusters q pd q jfacetsq dj 0 redundant bagselect topclusters q pd q jfacetsq dj yes topclusters q c2facetsq pcq yes topclusters q c2facetsq pcq pcd yes interpolation topclusters q pd q c2facetsq pcq pcd table 1 algorithm specications contain set documents interdocument distance measured kullbackleibler kl divergence corresponding smoothed language models 11 idea behind use cohorts documents nearest neighbors similarity space represent local frag ment tile overall similarity structure cor pus evaluation results show even relatively unsophisticated way approximate facets enables eective leveraging corpus structure least serves form nearestneighbor smoothing see rst retrievaltime action specied algorithm template choose facetsq querydependent subset clustersc algorithms described except baseline doesnt use cluster information documentselection aspect subset documents c 2 facetsq appear nal rankedlist output ideally would use clusters best approximating true facets corpus representative users interests expressed q therefore require facetsq subset topclusters q top clusters c respect pcq also want evaluate respect facets actually exhibits thus follows except baseline facetsq always dened subset topclusters q assume large enough produce desired number retrieved documents n baseline method baseline experiments denoted lm simply rank documents pd q cluster information used details particular implementation given section 5 selection methods class algorithms cluster induced language models play small role set facetsq selected essence standard language modeling approach ranking pd q invoked rank documents comprising clusters facetsq method scoring intended serve precisionenhancing mechanism downgrading documents happen members c 2 facetsq dint similarity respects pertinent q basisselect algorithm net eect definition given table 1 basis documents clusters topclusters q allowed appear nal output list thus algorithm uses pooling statistics documents cohortd simply decide whether worth ranking rank based solely pd q setselect algorithm diers documents clusters topclusters q may appear nal output list set referred name union clusters topclusters q idea document best cluster basis potentially relevant ranked ranking selected documents pd q 2 another natural variant idea documents appearing one cluster topclusters q get extra consideration given appear several approximations facets thought interest user idea gives rise bagselect algorithm named reference incorporation documents multiplicity bag formed multiset union clusters facetsq first selected document assigned score consisting product languagemodeling score pd q number top clusters belongs n topscoring documents reranked via pd q presented new sorted order aspectx methods turn algorithms making explicit use clusters smoothing mechanisms par ticular study term aspectx methods choice name reference work hofmann puzicha 9 conceives clusters explanatory latent variables underlying observed data x stands extended setting idea translates using pcq proxy pd q degree dependence particular pcq based strength association c aspectx algorithm measures association pcd uniformaspectx algorithm assumes every 2 c degree association c cases reranking pd q applied scoring function use aspectx algorithm motivated appealing probabilistic derivation aspect model 9 follows fact c aspect model assumes query conditionally independent document given cluster way using clusters smooth individualdocument statistics case c pqjcpcjd assume pd pc constant write implementation treats clusters component documents fo manner deviates slightly template let n 0 number documents 1 highestranked clusters n n 0 documents mth cluster closest kldivergence sense clusters basis allowed topdocsn c pqjcpdjc constant doesnt aect ranking aspectx algorithm arises replacing conditional probabilities corresponding language models summing clusters facetsq constraining clusters participate sum relatively high rank important experiments indicate using large number clusters could detrimen tal note however appears dicult within strictly probabilistic framework original aspect model incorporate constraint particular clusters rank depends clusters none terms basic aspectmodel equation explicitly conditions hybrid algorithm selectiononly algorithms emphasize pd q scoring document contrast aspectx algorithms rely pcq created interpolation algorithm combine advantages two approaches algorithm derived dropping original aspect models conditional independence assumption namely pqjd c pqjc instead setting pqjd c equation 1 pqjd indicates degree emphasis individualdocument information via algebra get c pqjcpcjd finally applying assumptions described discussion aspectx algorithm yields score function linear interpolation score standard lm approach score aspectx algorithm note reranking step occurs shall see interpolation algorithms incorporation documentspecic information yields higher precision 4 related work document clustering long history information retrieval 4 16 particular approximating topics via clusters recurring theme arguably work related dint employing clustering language modeling context ad hoc retrieval 3 latent variable models eg 9 8 12 2 classic aspect model one instantiation work takes strictly probabilistic approach problems discussed standard language modeling opposed algorithmic viewpoint also focus latentvariable work sophisticated cluster induction whereas nd simple clustering scheme works rather well practice interestingly hofmann 8 linearly interpolated probabilistic models score based soft clusters usual cosine metric quite close spirit interpolation algorithm implicit corpus structure also exploited laerty zhais expanded query language model 11 method uses interleaved documentterm markov chains thought tracing paths related documents enhance language models built queries similar conceptually frameworks use interdocument similarities enhance performance document language models although work notion similarity explicit 3 see eg 3 10 6 applications clustering related areas 5 experimental setup conducted experiments trec data used titles rather full descriptions queries resulting average length 25 terms characteristics three corpora summarized following table corpus docs queries previous work rst two data corpora ap89 ap8889 chosen served data previous research stateoftheart algorithms somewhat related considerably extending basic lm approach used stemming stopwordremoval policies previous experiments hence applied porter stemmer ap89 collection disk one ran krovetz stemmer ap8889 removed inquery stopwords 1 lengthone tokens lafr disk 5 4 respectively part trec8 corpus used trec8 ad hoc queries neither stemmed subjected stopword removal corpus heterogeneous two induction base language models unless otherwise specied use unigram dirichletsmoothed language models previously shown yield good performance short queries 19 following manner purposes discussion use term document notation refer either true document corpus c query let fx 2 number times word x occurs item text sequence wn dirichletsmoothed language model induced assigns following probability w free parameter controls degree documents statistics altered overall corpus statis tics ml indicates maximumlikelihood estimate two documents 0 set pd 0 exp normalizing appropriate kullbackleibler divergence formulation actually equivalent loglikelihood criterion certain assumptions 11 practice less sensitive p dir 0 variations length 0 given cluster c corresponding language model pc induced concatenating cs component documents applying documentlm induction method new document reference comparisons one goals demonstrate incorporating corpus structure retrieval framework provide improvements performance standard lm algorithm also wish detemine whether algorithms competitive stateoftheart languagemodelingbased algorithms one natural choice baseline lm basiss sets bags uniform aspectx interp pseudofeedback markov chains avg prec 2103 22 1 22 45 22 3 21 7 22 6 249 23 2 prec 0 574 585 585 58 1 572 58 2 558 534 recall 4867 54 86 56 15 62 77 57 31 62 16 6362 61 91 table 2 ap89 results 3261 relevant documents baseline lm basiss sets bags uniform aspectx interp relevance model avg prec 2437 26 58 28 11 26 65 24 92 27 5 3128 26 17 prec 0 6552 recall 6653 71 86 76 48 79 23 69 05 78 9 8083 77 69 table 3 ap8889 results 4805 relevant documents comparison laerty zhais pseudofeedback markov chains algorithm 11 extends expanded query language model described forcing chains pass topranked documents determined using standard lm approach another obvious candidate lavrenko crofts relevance model 13 rst method explicitly incorporate relevance language modeling framework demonstrated excellent per formance note algorithms contrast framework depend pseudofeedback mechanisms cope lack true user feedback implementation used lemur toolkit 14 run experiments implementations baseline used optimized smoothingparameter settings respect average noninterpolated precision 4 computed via line search novel algorithms optimized clustersize parameter k interpolation algorithms interpolation parameter parameters set default values suggested previous literature 19 thus baseline algorithm given extra advantage rather reimplement pseudofeedback markovchain relevancemodel algorithms described report results presented previous literature 11 13 realize minor dierences performance could stem specic implementation issues stated goal test competitiveness algorithms performance respect prominent algo rithms prove algorithms superiority 6 experimental results evaluation measures used average noninterpolated precision interpolated precision 0 recall selected documents main experimental results given tables 2 3 4 figure 2 tables evaluation metric strongest performance boldfaced results baseline lm italicized also wilcoxon twosided test employed signicance threshold statistically signicant performance improvements degradations algorithms relative baseline marked star clearly indicated settings given captions 4 optimization respect recall yielded results statistically indistinguishable respect performance metrics even worst algorithms always competitive baseline lm approach occasional exceptions mostly precision 0 generally better also observe aspectx interpolation algorithms competitive pseudofeedback markovchains algorithm see table 2 relevancemodel algorithm see table respect performance measures figure shows 11point precisionrecall curves algorithms baseline three corpora interpolation algorithm best overall ap88 ap8889 clusterbased algorithms whole generally perform demonstrably better baseline lafr however new algorithms exception interpolation algorithm seem dicult distinguish lm borne relative lack statisticalsignicance indications table 4 fact aspectx algorithm usually superior uniformaspectx indicates incorporating withincluster structure represented pcd important finally generally high performance aspectx interpolation algorithms seems support claims importance using corpusstructural information particular ways suggested specically two algorithms clusters play selection smoothing role documentspecic information intracluster structure incorporated well follows discuss results experimental studies space reasons present subset performance gures selection corpora parameter selection clustersize parameter k noticeable impact performance series preliminary experiments whose results omitted due space restrictions indicate small values k eg 5 10 yield better results baseline lm uniformaspectx method demonstrating usefulness even tiny document clusters however increasing k 40 resulted superior performance ap89 ap8889 datasets suggests rerank step algorithm template compensate degree extra irrelevant documents large clusters may bring consideration must also choose number clusters trieved recalling wish return xed number documents experimental results reported two dierent schemes used al avg prec 2216 2192 22 52 22 2173 22 45 2388 prec 0 5737 57 91 57 89 58 16 5728 5825 57 81 recall 4831 55 64 58 09 53 34 58 09 637 61 18 table 4 results lafr 1391 relevant documents precision recall 11pt precision curves corpus ap89 clustersize40 baseline basisselect setselect bagselect aspectx uniformaspectx precision recall 11pt precision curves corpus ap8889 clustersize40 baseline basisselect setselect bagselect aspectx uniformaspectx precision recall 11pt precision curves corpus lafr clustersize10 baseline basisselect setselect bagselect aspectx uniformaspectx figure 2 11point precisionrecall curves ap89 ap8889 gorithms using clusters solely selection set either 1000 minimum value needed 1000 documents receiving nonzero score 5 remaining algorithms aspectx uniformaspectx interpolation 10000 former group algorithms sensitive choice latter long exceed 10000 satisfactory improvements respect baseline algorithms observed however drawing upon clusters sense classic aspect model 9 clearly detrimental data corpora important regard interpolation algorithm diers methods introduced inclusion additional free parameter representing degree dependence pd q relative aspectx al gorithm figure 3 plots trajectories interpolation algorithm performance space increased gure makes visually clear interplay cluster document information small emphasizing clus ters result better recall relatively poor precision large emphasizing documents improve precision expense recall performance average values around 6 shows integrating document cluster level information provides better performance either produce alone note aspectx algorithm viewed version interpolation algorithm reranking added improve average precision return performance degradation relative interpolation algorithm oers advantage one fewer parameter tune fairly robust ms value well rerank step important reranking step topranked documents rescored documentspecic language models producing good pre cision ran several experiments explore issue first observed considerable degradation average 5 bagselect algorithm chose lower values would suced rerank yes yes yes avg prec 226 198 275 2695 2245 1601 prec 0 582 4698 659 6521 5825 4692 table 5 eect rerank step aspectx precision ap89 ap8889 k40 lafr k10 precision removed pd q term score functions basisselect setselect algorithms reranking redundant note version basis select algorithm corresponds applying basic lm approach document cohortd rather thought smoothing method wherein document language model created backing completely cluster language model next examined role optional reranking step algorithms explicitly incorporate aspectx uniformaspectx algorithms two cases scoring function incorporate pd q run without optional reranking phase low average precision precision 0 resulted implying reliance pc alone suers overregularization results aspectx algorithm shown table 5 furthermore reranking also required achieve reasonable precision bagselect algorithm even though scoring function incorporates pd q reranking applied average precision suers clusters small case interpolation algorithm however additional rerank phase needed long interpolation weight documentbased language model large enough seen figure 4 dier ence average precision without rerank dierent values reported observe 04 reranking degrades performance results suggest 1 best results important strike right balance documentspecic interdocument information 2 algorithms reranking creates balance others upset average precision recall n1000 effect increasing lambda recall average precision corpus ap89 baseline lm average precision recall n1000 effect increasing lambda recall average precision corpus ap8889 baseline lm figure 3 interpolation algorithms recall vs average precision grows increments 1 9 925 95 975 98 99 recall would yield baseline languagemodel scoring function similar patterns observed lafr corpus omit results clarity 006 002002006 avg prec difference without rerank lambda effect rerank average precision interp algorithm lafr figure 4 eect rerank step average precision interpolation algorithm varies ap89 ap8889 k40 lafr k10 smoothing sensitivity lm approach choice smoothing technique smoothing parameters prompted great deal research 19 7 17 however found algorithms simply setting dirichlet smoothing parameter suggested value 2000 19 turned randomlychosen values within neighborhood 2000 outperformed optimized baseline moreover experiments jelinekmercer absolute discounting two wellknown singleparameter smoothing methods 19 yielded outcome relative insensitivity choice parameter value underlying smoothing method employed feature selection another interesting observation eective incorporation cluster information somewhat obviates need feature selection particular table 6 shows one case using aspectx interpolation algorithms without stemmer stopword list outperforms baseline access porter stemmer respect average precision recall hand stemming led degradation precision zero results cluster sizes 40 dierent corpora consistent ndings due language modeling throughout paper used language models information representation interesting question whether representation eg pc source representation eg c matters therefore explored eect using alternative representation speci cally queries documents represented using logbased tfidf inner product distance measure clusters treated large documents formed concatenating contents altering selection algorithms basisselect setselect bagselect way led improved performance respect basic tfidf retrieval algorithm shown table 7 hand algorithms well original lmbased counterparts thus see algorithmic framework boost performance information representations structureblind alternative language models seem advantages least comparison tfidf 7 conclusions summary proposed general framework enables development variety algorithms integrating corpus similarity structure modeled via clusters documentspecic information although proposal motivated recent languagemodeling approach information retrieval specic algorithms presented use language models representation purposes good eect observed framework also used basic classic ir techniques tfidf interesting direction future work explore eect using alternative clustering algorithms would sbaseline ubaseline saspectx uaspectx sinterpolation avg prec 2103 1956 22 6 21 51 249 24 08 prec 0 574 601 58 2 609 558 589 recall 4867 4499 62 16 table stemming comparison ap89 stemmed version u unstemmed version signicant dierences reported respect corresponding baseline tfidf version lm version baseline basisselect setselect bagselect baseline basisselect setselect bagselect avg prec 1643 prec 0 4666 46 94 4729 46 92 5737 57 91 57 89 5816 recall 4745 table 7 simple similarity metric based tfidf vs lmbased similarity lafr cluster size also like study role overlapping plays framework performance gain due high degree overlap clusters way structure individualdocument information integrated another interesting direction examine whether algorithms lmbased pseudofeedback methods used reference comparisons 11 13 benet replace basic lm retrieval algorithm employ one importantly would like develop principled probabilistic interpretation framework pro posed done preliminary work based considering factorization c pqjd cpcjd components scoring functions considered rough approximations terms factorization creating rigorous probabilistic foundation work described one main future goals acknowledgments thank eric breck claire cardie shimon edelman thorsten joachims art munson bo pang ves stoyanov anonymous reviewers valuable comments thanks chengxiang zhai victor lavrenko answering questions work andres corra daemmanuel responding queries lemur paper based upon work supported part national science foundation grants itrim iis0081334 iis0329064 alfred p sloan research fellowship opinions ndings conclusions recommendations expressed authors necessarily ect views national science foundation sloan foundation 8 r inquery trec9 latent dirichlet allocation della pietra model cluster searching based classi reexamining cluster hypothesis scattergather retrieval results unsupervised learning probabilistic latent semantic analysis unsupervised learning dyadic data modeling long distance dependence language topic mixtures vs dynamic cache models optimal mixture models ir experiments using lemur toolkit language modeling approach information retrieval information retrieval bayesian extension language model ad hoc information retrieval chengxiang zhai john la tr classbased inigram models natural language reexamining cluster hypothesis language modeling approach information retrieval clusterbased language models distributed retrieval document language models query models risk minimization information retrieval relevance based language models study smoothing methods language models applied ad hoc information retrieval information retrieval smoothing language modeling approach information retrieval unsupervised learning probabilistic latent semantic analysis optimal mixture models ir bayesian extension language model ad hoc information retrieval language modeling information retrieval latent dirichlet allocation ctr xiaoyong liu w bruce croft representing clusters retrieval proceedings 29th annual international acm sigir conference research development information retrieval august 0611 2006 seattle washington usa tao tao xuanhui wang qiaozhu mei chengxiang zhai accurate language model estimation document expansion proceedings 14th acm international conference information knowledge management october 31november 05 2005 bremen germany tao tao xuanhui wang qiaozhu mei chengxiang zhai language model information retrieval document expansion proceedings main conference human language technology conference north american chapter association computational linguistics p407414 june 0409 2006 new york new york seunghoon na insu kang jonghyeok lee adaptive document clustering based querybased similarity information processing management international journal v43 n4 p887901 july 2007 seunghoon na insu kang jieun roh jonghyeok lee empirical study query expansion clusterbased retrieval language modeling approach information processing management international journal v43 n2 p302314 march 2007 fernando diaz regularizing ad hoc retrieval scores proceedings 14th acm international conference information knowledge management october 31november 05 2005 bremen germany seunghoon na insu kang jonghyeok lee parsimonious translation models information retrieval information processing management international journal v43 n1 p121145 january 2007 oren kurland lillian lee carmel domshlak better real thing iterative pseudoquery processing using clusterbased language models proceedings 28th annual international acm sigir conference research development information retrieval august 1519 2005 salvador brazil carlo curino yuanyuan jia bruce lambert patricia west clement yu mining officially unrecognized side effects drugs combining web search machine learning proceedings 14th acm international conference information knowledge management october 31november 05 2005 bremen germany niall rooney david patterson mykola galushka vladimir dobrynin relevance feedback mechanism clusterbased retrieval information processing management international journal v42 n5 p11761184 september 2006 oren kurland lillian lee pagerank without hyperlinks structural reranking using links induced language models proceedings 28th annual international acm sigir conference research development information retrieval august 1519 2005 salvador brazil oren kurland lillian lee respect authority hits without hyperlinks utilizing clusterbased language models proceedings 29th annual international acm sigir conference research development information retrieval august 0611 2006 seattle washington usa