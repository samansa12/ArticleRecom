simple algorithm nearest neighbor search high dimensions abstractthe problem finding closest point highdimensional spaces common pattern recognition unfortunately complexity existing search algorithms kd tree rtree grows exponentially dimension making impractical dimensionality 15 nearly applications closest point interest lies within userspecified distance epsilon present simple practical algorithm efficiently search nearest neighbor within euclidean distance epsilon use projection search combined novel data structure dramatically improves performance high dimensions complexity analysis presented helps automatically determine epsilon structured problems comprehensive set benchmarks clearly shows superiority proposed algorithm variety structured unstructured search problems object recognition demonstrated example application simplicity algorithm makes possible construct inexpensive hardware search engine 100 times faster software equivalent c implementation algorithm available upon request searchcscolumbiaeducave b introduction searching nearest neighbors continues prove important problem many fields science engineering nearest neighbor problem multiple dimensions stated follows given set n points novel query point q ddimensional space find point set distance q lesser equal distance q point set 21 variety search algorithms advanced since knuth first stated postoffice problem need new algorithm answer existing techniques perform poorly high dimensional spaces complexity techniques grows exponentially dimensionality high dimensional mean say 25 high dimensionality occurs commonly applications use eigenspace based appearance matching realtime object recognition tracking inspection 26 feature detection moreover techniques require nearest neighbor search performed using euclidean distance l 2 norm hard problem especially dimensionality high high dimensionality also observed visual correspondence problems motion estimation mpeg coding estimation binocular stereo d2581 optical flow computation structure motion also d2581 paper propose simple algorithm efficiently search nearest neighbor within distance ffl high dimensions shall see complexity proposed algorithm small ffl grows slowly algorithm successful tackle nearest neighbor problem originally stated finds points within distance ffl novel point property sufficient pattern recognition problems problems stated match declared high confidence novel point sufficiently close training point occasionally possible assume ffl known suggest method automatically choose ffl briefly outline proposed algorithm algorithm based projection search paradigm first used friedman friedmans simple technique works follows preprocessing step dimensional training points ordered different ways individually sorting coordi nates sorted coordinate arrays thought 1d axis entire dimensional space collapsed projected onto given novel point q nearest neighbor found follows small offset ffl subtracted added qs coordinates obtain two values two binary searches performed sorted arrays locate positions values axis minimum number points positions chosen finally points positions chosen axis exhaustively searched obtain closest point complexity technique roughly ondffl clearly inefficient high simple projection search improved upon yunck utilizes precomputed data structure maintains mapping sorted unsorted original coordinate arrays addition mapping indicator array n elements used element indicator array henceforth called indicator corresponds point beginning search indicators initialized number 1 small offset ffl subtracted added novel point qs coordinates obtain two values two binary searches performed sorted arrays locate positions values mapping sorted unsorted arrays used find points corresponding coordinates values indicators corresponding points binary shifted left one bit entire process repeated dimensions end points whose indicators value must lie within 2ffl hypercube exhaustive search performed hypercube points find nearest neighbor data structure yunck able find points within hypercube using primarily integer operations however total number machine operations required teger floating point find points within hypercube similar friedmans algorithm roughly ondffl due fact modern cpus significantly penalize floating point operations improvement slight benchmarked later section propose data structure significantly reduces total number machine operations required locate points within hypercube roughly onffl moreover data structure facilitates simple hardware implementation result increase performance two orders magnitude previous work search algorithms divided following broad categories exhaustive search b hashing indexing c static space partitioning dynamic space partitioning randomized algorithms algorithm described paper falls category algorithms categorized work vector spaces work metric spaces categories bd fall former category falls later metric space search techniques used possible somehow compute distance measure sample points pieces data space points reside lacks explicit coordinate structure paper focus vector space techniques detailed discussion searching metric spaces refer exhaustive search term implies involves computing distance novel point every point set finding point minimum distance approach clearly inefficient complexity ond hashing indexing fastest search techniques run constant time however space required store index table increases exponentially hence hybrid schemes hashing high dimensional space low 1 2 dimensional space indexing low dimensional space proposed dimensionality reduction called geometric hashing problem increasing dimensionality becomes difficult construct hash function distributes data uniformly across entire hash table index added drawback arises fact hashing inherently partitions space bins two points adjacent bins closer third point within bin search algorithm uses hash table index correctly find point adjacent bin hence hashing indexing really effective novel point exactly equal one database points space partitioning techniques led elegant solutions multidimensional search problems method particular theoretical significance divides search space polygons voronoi polygon geometrical construct obtained intersecting perpendicular bisectors adjacent points 2d search space voronoi polygons allow nearest neighbor found olog 2 n operations n number points database unfortunately cost constructing storing voronoi diagrams grows exponentially number dimensions details found 3 another algorithm interest 1d binary search generalized dimensions 11 runs olog 2 n time requires storage 4 makes impractical n 100 perhaps widely used algorithm searching multiple dimensions static space partitioning technique based k dimensional binary search tree called kd tree kd tree data structure partitions space using hyperplanes placed perpendicular coordinate axes partitions arranged hierarchically form tree simplest form kd tree constructed follows point database chosen root node points lying one side hyperplane passing root node added left child points side added right child process applied recursively left right children small number points remain resulting tree hierarchically arranged hyperplanes induces partition space hyperrectangular regions termed buckets containing small number points kd tree used search nearest neighbor follows k coordinates novel point used descend tree find bucket contains exhaustive search performed determine closest point within bucket size query hypersphere set distance closest point information stored parent nodes used determine hypersphere intersects buckets bucket exhaustively searched size hypersphere revised necessary fixed certain assumptions underlying data kd tree requires onlog 2 n operations construct olog 2 n operations search kd trees extremely versatile efficient use low dimensions however performance degrades exponentially 1 increasing dimensionality high dimensions query hypersphere tends intersect many adjacent buckets leading dramatic increase number points examined kd trees dynamic data structures means data added deleted small cost impact adding deleting data search performance however quite unpredictable related amount imbalance new data causes tree high imbalance 1 although appears contradictory previous statement claim olog 2 n complexity made assuming fixed varying n exact relationship complexity yet established observed us many others roughly exponential generally means slower searches number improvements basic algorithm suggested friedman recommends partitioning hyperplane chosen passes median point placed perpendicular coordinate axis along whose direction spread points maximum 15 sproull suggests using truncated distance computation increase efficiency high dimensions 36 variants kd tree used address specific search problems rtree also space partitioning structure unlike kd trees partitioning element hyperplane hyperrectangular region hierarchical rectangular structure useful applications searching image content needs locate closest manifold cluster novel manifold cluster rtree also addresses problems involved implementing kd trees large disk based databases rtree also dynamic data structure unlike kd tree search performance affected addition deletion data number variants rtrees improve basic technique packed rtrees 34 although rtrees useful implementing sophisticated queries managing large databases performance nearest neighbor point searches high dimensions similar kd trees complexity grows exponentially static space partitioning techniques proposed branch bound none significantly improve performance high dimensions clarkson describes randomized algorithm finds closest point dimensional space olog 2 n operations using rpo randomized post however time taken construct rpo tree dd2e1ffl space required store also dd2e1ffl makes impractical number points n large 3 3 algorithm 31 searching slicing illustrate proposed high dimensional search algorithm using simple example 3d space shown figure 1 call set points wish search closest point point set goal find point point set closest 2e 2e 2e 2e xe xe ze ze z figure 1 proposed algorithm efficiently finds points inside cube size 2ffl around novel query point q closest point found performing exhaustive search within cube using euclidean distance metric novel query point qx within distance ffl approach first find points lie inside cube see figure 1 side 2ffl centered q since ffl typically small number points inside cube also small closest point found performing exhaustive search points points inside cube know points within ffl points within cube found follows first find points sandwiched pair parallel planes x 1 x 2 see figure 1 add list call candidate list planes perpendicular first axis coordinate frame located either side point q distance ffl next trim candidate list discarding points also sandwiched parallel pair planes 1 2 perpendicular x 1 x 2 located either side q distance ffl procedure repeated planes z 1 z 2 end candidate list contains points within cube size 2ffl centered q since number points final trimmed list typically small cost exhaustive search negligible major computational cost technique therefore constructing trimming candidate list 32 data structure candidate list construction trimming done variety ways propose method uses simple preconstructed data structure along 1d binary searches efficiently find points sandwiched pair parallel hyperplanes data structure constructed raw point set depicted figure 2 assumed point set static hence given point set data structure needs constructed point set stored collection 1d arrays j th array contains j th coordinate points thus point set coordinates point lie along row illustrated dotted lines figure 2 suppose novel point q coordinates recall order construct candidate list need find points point set lie pair parallel hyperplanes separated distance 2ffl perpendicular first coordinate axis centered need locate points whose first coordinate lies limits ffl done help two binary searches one limit coordinate array sorted beforehand end sort coordinate arrays point set independently obtain ordered set unfortunately sorting raw coordinates leave us information regarding points arrays ordered set correspond given point point set vice versa purpose maintain two maps backward map maps coordinate ordered set corresponding coordinate point set conversely forward map maps point point set point ordered set notice maps simple integer arrays p dn point set odn ordered set f dn bdn forward backward maps respectively using backward map find corresponding points point set shown dark shaded areas add appropriate points candidate list construction candidate list complete next trim candidate list iterating point set ordered set dimensions points forward map forward map backward map input e e e e e e figure 2 data structures used constructing trimming candidate list point set corresponds raw list data points ordered set coordinate sorted forward backward maps enable efficient correspondence point ordered sets follows iteration k check every point candidate list using forward map see k th coordinate lies within limits limits also obtained binary search points k th coordinates lie outside range shown light grey discarded list end final iteration points remaining candidate list ones lie inside hypercube side 2ffl centered q discussion proposed constructing candidate list using first dimension performing list trimming using dimensions 2 3 order wish emphasize operations done order still yield desired result next section shall see possible determine optimal ordering cost constructing trimming list minimized important note operations used trimming list integer comparisons memory lookups moreover using proposed data structure limited use floating point operations binary searches needed find row indices corresponding hyperplanes feature critical efficiency proposed algorithm compared competing ones facilitates simple software implementation also permits implementation hardware search engine previously stated algorithm needs supplied appropriate ffl prior search possible large class problems pattern recognition instance match declared novel point q sufficiently close database point reasonable assume ffl given priori however choice ffl prove problematic case one solution set ffl large might seriously impact performance hand small ffl could result hypercube empty determine optimal ffl given problem exactly ffl affect performance algorithm seek answers questions following section 4 complexity section attempt analyze computational complexity data structure stor age construction nearest neighbor search saw previous section constructing data structure essentially sorting arrays size n done odn log 2 n time additional storage necessary hold forward bacward maps requires space ond nearest neighbor search major computational cost process candidate list construction trimming number points initially added candidate list depends ffl also distribution data point set location novel point q hence facilitate analysis structure problem assuming widely used distributions point set following notation used random variables denoted uppercase letters instance q vectors bold q suffixes used denote individual elements vectors instance q k k th element vector q probability density written f q q q continuous 2e axis c figure 3 projection point set novel point onto one dimensions search space number points inside bin b given binomial distribution figure 3 shows novel point q set n points 2d space drawn known distribution recall candidate list initialized points sandwiched hyperplane pair first dimension generally c th dimension corresponds points inside bin b figure 3 entire point set q projected c th coordinate axis boundaries bin b hyperplanes intersect axis c c number points bin b order determine average number points added candidate list must compute c distance q c point candidate list distribution z c may calculated distribution point set define p c probability projected point point set within distance ffl q c possible write expression density c terms p c irrespective distribution points c binomially distributed 2 equivalent elementary probability problem given success point within bin b occur probability p c number successes occur n independent trials points binomially distributed expression average number points bin b em c j q c easily determined note em c j q c random variable depends c location q distribution q known expected number points bin computed perform one lookup backward map every point hyperplane pair main computational effort equation 3 directly estimates cost candidate list construction next derive expression total number points remaining candidate list trim dimensions sequence c 1 recall iteration k perform forward map lookup every point candidate list see lies c k th hyperplane pair many points candidate list lie hyperplane pair equation 3 used time replacing n number points candidate list rather entire point set assume point set independently distributed hence n k total number points candidate list iteration k define n total cost constructing trimming candidate list trim need perform one forward map lookup two integer comparisons hence assign one cost unit operations expression n written aid equation 4 average equation 6 suggests distributions f q q f z z known compute average cost en terms ffl next section shall examine two cases particular interest z uniformly distributed b z normally distributed note left cost exhaustive search points within final hypercube reason cost exhaustive search dependant distance metric used cost however small neglected cases n ae needs considered added equation 6 end section making observation mentioned earlier advantage examine dimensions specific order order expanding summation product factoring terms equation 5 rewritten immediate value n minimum p c 1 words chosen numbers sandwiched points hyperplane pairs ascending order easily ensured simply sorting numbers sandwiched points note numbers obtained time od simply taking difference indices ordered set returned pair binarysearchs cost sorting numbers od log 2 heapsort costs negligible problem reasonable dimensionality 41 uniformly distributed point set look specific case point set uniformly distributed x point point set assume independent uniform distribution extent l coordinates 1l gammal2 x l2 15e0625e06cost e d5 e50000015e0625e06cost n50000 n150000 b figure 4 average cost algorithm independent grows linearly small ffl point set cases assumed uniformly distributed extent l 1 point set contains 100000 points 5d 10d 15d 20d 25d spaces b point set 15d contains 50000 75000 100000 125000 150000 points using equation 8 fact z expression density z c written f zc jqc p c written gammaffl f zc jqc zdz gammaffll dz l substituting equation 10 equation 6 considering upper bound worst case get l l l l l l l neglecting constants write d5 cost e n50000 cost b figure 5 average cost algorithm independent grows linearly small ffl point set cases assumed normally distributed variance oe 1 point set contains 100000 points 5d 10d 15d 20d 25d spaces b point set 15d contains 50000 75000 100000 125000 150000 points small ffl observe ffl 0 cost independent figure 4 equation 11 plotted ffl different figure 4a different figure 1 observe long ffl 25 cost varies little linearly proportional n also means keeping ffl small crucial performance algorithm shall see later ffl fact kept small many problems hence even though cost algorithm grows linearly n ffl small enough many real problems better pay price linearity rather exponential dependence 42 normally distributed point set next look case point set normally distributed x point point set assume independent normal distribution variance oe coordinates f xc x p 2oe exp using z expression density z c obtained get f zc jqc 2oe p c written gammaffl f zc jqc zdz oe oe p expression substituted equation 6 evaluated numerically estimate cost given q figure 5 shows cost function ffl uniform distribution observe ffl 1 cost nearly independent grows linearly n variety pattern classification problems data take form individual gaussian clusters mixtures gaussian clusters cases results serve basis complexity analysis 5 determining ffl apparent analysis preceding section cost proposed algorithm depends critically ffl setting ffl high results huge increase cost setting ffl small may result empty candidate list although freedom choose ffl may attractive applications may prove nonintuitive hard others cases automatically determine ffl closest point found high certainty distribution point set known first review well known facts l p norms figure 6 illustrates norms selected values p points surfaces equidistant sense respective norm central point formally l p distance two vectors b defined distance metrics also known minkowskip metrics relevant determining ffl l 2 norm occurs frequently pattern recognition problems unfortunately candidate list trimming algorithm find points within l 2 figure illustration various norms also known minkowski pmetrics points surfaces equidistant central point l1 metric bounds l p p within l1 ie hypercube since l1 bounds l 2 one naively perform exhaustive search inside l1 however seen figure 7a always correctly find closest point notice p 2 closer q p 1 although exhaustive search within cube incorrectly identify closest simple solution problem performing exhaustive search impose additional constraint points within l 2 radius ffl considered see figure 7b however increases possibility hypersphere empty example instance p 1 discarded would able find point clearly need consider fact automatic method determining ffl describe next propose two methods automatically determine ffl first computes radius smallest hypersphere contain least one point specified probability ffl set radius algorithm proceeds find points within circumscribing hypercube side 2ffl method however efficient high dimensions reason follows increase dimensionality difference hypersphere hypercube volumes becomes great hypercube corners contain far points inscribed hypersphere consequently extra effort necessary perform l 2 distance computations corner points eventually wasted rather find circumscribing hypercube second method simply find length side smallest hypercube contain least one point specified probability ffl set half length side leads problem described earlier searching points outside hypercube closer l 2 sense points inside shall describe methods detail see remedy e r 2e e b figure 7 exhaustive search within hypercube may yield incorrect result p 2 closer exhaustive search within cube incorrectly identify closest point b remedied imposing constraint exhaustive search consider points within l 2 distance ffl q given length side hypercube 2ffl problem 51 smallest hypersphere method let us see analytically compute minimum size hypersphere given want able guarantee non empty probability p let radius hypersphere ffl hs let total number points within hypersphere let q novel point define kzk l 2 distance q point point set binomially distributed density probability p least one point hypersphere simply e hs r b figure 8 ffl computed using two methods finding radius smallest hypersphere contain least one point high probability search performed setting ffl radius constraining exhaustive search within ffl b finding size smallest hypercube contain least one point high probability searching ffl set half length side additional searches performed areas marked bold equation suggests know q density f z jq z probability p solve ffl hs example consider case point set uniformly distributed density given equation 9 cumulative distribution function kzk uniform distribution integrated within hypersphere simply volume thus l dgammad2 substituting equation 19 solving ffl hs get l dgammad2 using equation 21 ffl hs plotted probability two cases figure 9a fixed different values 5 25 n fixed 100000 figure 9b n fixed different values 50000 150000 fixed 5 figures illustrate important property large changes probability p result small probability success02061epsilon d5 probability success005015 epsilon n150000 n50000 b figure 9 radius ffl necessary find point inside hypersphere varies little probability means ffl set knee probability close unity point set cases uniformly distributed extent l 1 point set contains 100000 points 5 10 15 20 25 dimensional space b point 5d contains 50000 75000 100000 125000 150000 points changes ffl hs suggests ffl hs set right hand knee curves probability close unity words easy guarantee least one point within hypersphere search performed setting length side circumscribing hypercube 2ffl hs imposing additional constraint exhaustive search points within l 2 distance ffl hs considered 52 smallest hypercube method attempt analytically compute size smallest hypercube given want able guarantee non empty probability p let number points within hypercube size 2ffl hc define z c distance c th coordinate point set point novel point q binomially distributed density k kc 22 probability p least one point hypercube simply probability success010305 epsilon d5 probability success005015025035 epsilon n150000 n50000 b figure 10 value ffl necessary find point inside hypercube varies little probability means ffl set knee probability close unity point set cases uniformly distributed extent l 1 point set contains 100000 points 5 10 15 20 25 dimensional space b point set 5d contains 50000 75000 100000 125000 150000 points n equation suggests know q density f zc jqc z probability p solve ffl hc specific case point set uniformly distributed expression ffl hc obtained closed form follows let density uniform distribution given equation 9 using equation 10 get l substituting equation 23 solving ffl hc get li using equation 25 ffl hc plotted probability two cases figure 10a fixed different values 5 25 n fixed 100000 figure 10b n fixed different values 50000 150000 fixed 5 similar graphs obtained case hypersphere ffl hc set right hand knee curves probability close unity notice value ffl hc required hypercube much smaller required hypersphere especially high precisely reason prefer second smallest hypercube method recall sufficient simply search closest point within hypercube point outside closer point inside remedy problem suggest following technique first exhaustive search performed compute distance closest point within hypercube call distance r figure 8b closest point within hypercube distance r q clearly closer point exists within hypersphere radius r since parts hypersphere lie outside original hypercube also search hyperrectangular regions shown bold performing additional list trimmings performing exhaustive search hyperrectangles impose constraint point considered less distance r q figure 8b p 2 present one hyperrectangular region happens closer q p 1 although method complicated gives excellent performance sparsely populated high dimensional spaces high dimensional uniform distribution conclude wish emphasize hypercube hypersphere methods used interchangeably guaranteed find closest point within ffl however choice one methods use depend dimensionality space local density points densely populated low dimensional spaces hypersphere method performs quite well searching hyperrectangular regions worth additional overhead sparsely populated high dimensional spaces effort needed exhaustively search huge circumscribing hypercube far overhead searching hyperrectangular regions however difficult analytically predict one methods suits particular class data hence encourage reader implement methods use one performs best finally although discussion relevant l 2 norm equivalent analysis easily performed norm 6 benchmarks performed extensive set benchmarks proposed algorithm looked two representative classes search problems may benefit algorithm first class data statistical structure case instance points uniformly normally distributed second class problems statistically un structured instance points lie high dimensional multivariate manifold difficult say anything distribution section present results benchmarks performed statistically structured data benchmarks statistically unstructured data refer reader section 7 tested two commonly occurring distributions normal uniform proposed algorithm compared kd tree exhaustive search algorithms algorithms included benchmark yield comparable performance first set benchmarks two normally distributed point sets containing 30000 100000 points variance 10 used test per search execution time another set points shall call test set constructed test set contained 10000 points also normally distributed variance 10 algorithm execution time calculated averaging total time required perform nearest neighbor search 10000 points test set determine ffl used smallest hypercube method described section 52 since point set normally distributed cannot use closed form solution ffl however numerically computed follows substituting equation 16 equation 23 get oe oe pn setting p probability least one point hypercube 99 oe variance 10 computed ffl search point q using fast simple bisection technqiue figures 11a 11b show average execution time per search point set contains 30000 100000 points respectively execution times include time taken search computation ffl using equation 26 time taken 1 additional 3 searches necessary point found within hypercube although 3 point found within hypercube incremented ffl 01 searched time secs dimensions proposed algorithm kd tree exhaustive time secs dimensions proposed algorithm kd tree exhaustive search b005015025035 time secs dimensions proposed algorithm kd tree exhaustive time secs dimensions proposed algorithm kd tree exhaustive search c figure 11 average execution time proposed algorithm benchmarked statistically structured problems point set normally distributed variance 10 contains 30000 points b point set normally distributed variance 10 contains 100000 points proposed algorithm clearly faster high c point set uniformly distributed extent 10 contains 30000 points point set uniformly distributed extent 10 contains 100000 points proposed algorithm perform well uniform distributions due extreme sparseness point set high ffl varies q values ffl sample points follows values ffl point corresponding respectively point values ffl 024 061 086 104 117 corresponding respectively values ffl point corresponding respectively point values ffl corresponding proposed algorithm faster kd tree algorithm figure 11a figure 11b proposed algorithm faster 12 also notice kd tree algorithm actually runs slower exhaustive search 15 reason observation follows high dimensions space sparsely populated radius query hypersphere large consequently hypersphere intersects almost buckets thus large number points examined along additional overhead traversing tree structure makes inefficient search sparse high dimensional space second set benchmarks used uniformly distributed point sets containing 30000 100000 points extent 10 test set contained 10000 points also uniformly distributed extent 10 execution time per search calculated averaging total time required perform closest point search 10000 points test set determine ffl smallest hypercube method described section 52 used recall uniformly distributed point sets ffl computed closed form using equation 25 figures 11c 11d show execution times point set contains 30000 100000 points respectively values ffl corresponding tively values ffl corresponding respectively uniform distribution proposed algorithm perform well although appear slightly faster kd tree exhaustive search algorithms reason high dimensional space sparsely populated hence requires ffl quite large result algorithm ends examining almost points thereby approaching exhaustive search process repeated till point found 7 example application appearance matching demonstrate two applications fast efficient high dimensional search technique desirable first real time object recognition requires closest point found among 36000 points 35d space second closest point required found points lying multivariate high dimensional manifold problems examples statistically unstructured data let us briefly review object recognition technique murase nayar 24 object recognition performed two phases 1 appearance learning phase 2 appearance recognition phase learning phase images hundred objects poses captured images used compute high dimensional subspace called eigenspace images projected eigenspace obtain discrete high dimensional points smooth curve interpolated points belong object way object get curve univariate manifold parameterized pose manifolds second phase object recognition easy image object projected eigenspace obtain single point manifold closest point identifies object closest point manifold identifies pose note manifold continuous order find closest point manifold need finely sample obtain discrete closely spaced points benchmark used columbia object image library along slam software package 28 compute 100 univariate manifolds 35d eigenspace manifolds correspond appearance models 100 objects 20 100 objects shown figure 12a 100 manifolds sampled 360 equally spaced points obtain 36000 discrete points 35d space impossible manually capture large number object images would needed large test set hence automatically generated test set 100000 points sampling manifolds random locations roughly equivalent capturing actual images without image sensor noise lens blurring perspective projection effects important simulate effects cause projected point shift away manifold hence substantially affect performance nearest neighbor search algorithms 4 4 instance kd tree large query hypersphere would result large increase number adjacent buckets may searched algorithm time secs proposed algorithm 0025 kd tree 0045 exhaustive search 1533 projection search 2924 b figure 12 proposed algorithm used recognize estimate pose hundred objects using columbia object image library twenty hundred objects shown point set consisted 36000 points 360 object 35d eigenspace b average execution time per search compared algorithms unfortunately difficult relate image noise perspective projection distortion effects location points eigenspace hence used simple model add uniformly distributed noise extent 5 01 coordinates points test set found approximates realworld data determined setting gave us good recognition accuracy figure 12b shows time taken per search different algorithms search time calculated averaging total time taken perform 100000 closest point searches using points test set seen proposed algorithm outperforms techniques ffl set predetermined value point found within hypersphere time object recognition useful search closest point within ffl provides us means reject points far manifold likely objects database next examine another case data statistically unstructured closest point required found points lying single smooth multivariate high dimensional manifold manifold appears frequently appearance matching problems visual tracking 26 visual inspection 26 parametric feature detection 25 object recognition manifold representation visual appearance given novel appearance point matching involves finding point manifold closest point given manifold continuous pose appearance matching nearest neighbor problem sample manifold densely obtain discrete closely spaced points trivariate manifold used benchmarks obtained visual tracking experiment conducted nayar et al 26 first benchmark manifold sampled obtain 31752 discrete points second benchmark sampled obtain 107163 points cases test set 10000 randomly sampled manifold points used explained previously noise extent 01 added coordinate test set execution time per search averaged test set 10000 points point set determined gave good recognition accuracy figure 13a shows algorithm two orders magnitude faster algorithms notice exponential behaviour rtree algorithm also notice yuncks algorithm 5 extent eigenspace 10 10 maximum noise amplitude hence 05 extent eigenspace time secs dimensions proposed algorithm rtree exhaustive search projection search yunck search a0005001500250035 time secs dimensions proposed algorithm kd tree0005001500250035 time secs dimensions proposed algorithm kd tree b c figure 13 average execution time proposed algorithm benchmarked unstructured problem point set constructed sampling high dimensional trivariate manifold manifold sampled obtain 31752 points proposed algorithm two orders magnitude faster algorithms b manifold sampled obtain 31752 points c manifold sampled obtain 107163 points kd tree algorithm slightly faster low dimension degrades rapidly increase dimension slightly faster friedmans difference due use integer operations could benchmark yuncks algorithm till due use 32bit word indicator array figure 13b seen proposed algorithm faster kd tree figure 13c proposed algorithm faster 21 8 hardware architecture major advantage algorithm simplicity recall main computations performed algorithm simple integer map lookups backward forward maps two integer comparisons see point lies within hyperplane boundaries conse quently possible implement algorithm hardware using offtheshelf inexpensive components hard envision case competitive techniques kd trees rtrees given difficulties involved constructing parallel stack machines proposed architecture shown figure 14 field programmable gate array acts algorithm state machine controller performs io cpu dynamic rams drams hold forward backward maps downloaded cpu initialization cpu initiates search performing binary search obtain hyperplane boundaries passed search engine held static rams srams fpga independently begins candidate list construction trimming candidate looked backward map forward maps integer comparator returns true candidate within range else discarded trimming candidate points going dimensions final point list form point set indices returned cpu exhaustive search andor processing note although described architecture single comparator number added run parallel near linear performance scaling number comparators search engine trimming candidate list cpu course free carry tasks parallel begun implementation proposed architecture result intended small lowcost scsi based module plugged standard workstation pc estimate module result 100 fold speedup optimized software implementation algorithm control unit backward map forward map cpu comparator within limit flag lower limit upper limit control bus fpgapld figure 14 architecture inexpensive hardware search engine based proposed algorithm 9 discussion 91 k nearest neighbor search section 5 saw possible determine minimum value ffl necessary ensure least one point found within hypercube hypersphere high probability possible extend notion ensure least k points found high certainty recall probability exists least one point hypersphere radius ffl given equation 19 define p k probability least k points within hypersphere write p k expression substituted equation 18 given p k numerically solved ffl hs similarly substituted equation 22 compute minimum value ffl hc hypercube 92 dynamic point insertion deletion currently algorithm uses floating point arrays store ordered set 2d integer arrays store backward forward maps result possible efficiently insert delete points search space limitation easily overcome ordered set stored array set binary search trees bst bst corresponds array ordered set similarly forward maps replaced single linked list backward maps done away completely indices made reside within node bst although bsts would allow efficient insertion deletion nearest neighbor searches would longer efficient integer arrays also order get maximum efficiency bsts would well balanced see 19 discussion balancing techniques 93 searching partial data many times required search nearest neighbor absence complete data instance consider application requires features extracted image matched features feature space possible extract features matching done partially trivial adapt algorithm situation trimming list need look dimensions data hard envision case kd trees example space partitioned hyperplanes particular dimensions traversing tree locate bucket contains query point possible choose traversal direction node data corresponding partitioning dimension node missing query point acknowledgements wish thank simon baker dinkar bhat detailed comments criticisms suggestions helped greatly improving paper research conducted center research intelligent systems department computer science columbia university supported parts arpa contract daca7692c007 dodonr muri grant n000149510601 nsf national young investigator award r design analysis computer algorithms nearest neighbor searching applications diagrams survey fundamental geometric data struc ture multidimensional binary search trees used associative searching multidimensional binary search trees database applications data structures range searching optimal expectedtime algorithms closest point problems multidimensional indexing recognizing visual shapes randomized algorithm closestpoint queries multidimensional searching problems algorithms combinatorial geometry fast nearestneighbor search dissimilarity spaces algorithm finding nearest neigh bors algorithm finding best matches logarithmic expected time branch bound algorithm computing knearest neighbors effective way represent quadtrees dynamic index structure spatial searching fundamentals data structures complexity ddimensional voronoi diagrams sorting searching new version nearestneighbor approximating eliminating search algorithm aesa linear preprocessing time memory requirements visual learning recognition 3d objects appear ance parametric feature detection slam software library appearance matching digital pictures similarity searching large image databases computational geometry introduction numerical recipes c direct spatial search pictorial databases using packed rtrees refinements nearestneighbor searching kdimensional trees reducing overhead aesa metricspace nearest neighbour searching algorithm data structures algorithms nearest neighbor search general metric spaces technique identify nearest neighbors tr ctr james mcnames fast nearestneighbor algorithm based principal axis search tree ieee transactions pattern analysis machine intelligence v23 n9 p964976 september 2001 david w patterson mykola galushka niall rooney characterisation novel indexing technique casebased reasoning artificial intelligence review v23 n4 p359393 june 2005 philip quick david capson subspace position measurement presence occlusion pattern recognition letters v23 n14 p17211733 december 2002 freeman thouis r jones egon c pasztor examplebased superresolution ieee computer graphics applications v22 n2 p5665 march 2002 bin zhang sargur n srihari fast knearest neighbor classification using clusterbased trees ieee transactions pattern analysis machine intelligence v26 n4 p525528 april 2004 jim z c lai yiching liaw julie liu fast knearestneighbor search based projection triangular inequality pattern recognition v40 n2 p351359 february 2007 lin liang ce liu yingqing xu baining guo heungyeung shum realtime texture synthesis patchbased sampling acm transactions graphics tog v20 n3 p127150 july 2001 fast texture synthesis using treestructured vector quantization proceedings 27th annual conference computer graphics interactive techniques p479488 july 2000 yongsheng chen yiping hung tingfang yen chioushann fuh fast versatile algorithm nearest neighbor search based lower bound tree pattern recognition v40 n2 p360375 february 2007 mineichi kudo naoto masuyama jun toyama masaru shimbo simple termination conditions knearest neighbor method pattern recognition letters v24 n910 p12031213 01 june john favata offline general handwritten word recognition using approximate beam matching algorithm ieee transactions pattern analysis machine intelligence v23 n9 p10091021 september 2001 edgar chvez gonzalo navarro compact space decomposition effective metric indexing pattern recognition letters v26 n9 p13631376 1 july 2005 aaron hertzmann steven seitz examplebased photometric stereo shape reconstruction general varying brdfs ieee transactions pattern analysis machine intelligence v27 n8 p12541264 august 2005 stahlhut extending natural textures multiscale synthesis graphical models v67 n6 p496517 november 2005 gonzalo navarro searching metric spaces spatial approximation vldb journal international journal large data bases v11 n1 p2846 august 2002 edgar chvez jos l marroqun gonzalo navarro fixed queries array fast economical data structure proximity searching multimedia tools applications v14 n2 p113135 june 2001 jinxiang chai jing xiao jessica hodgins visionbased control 3d facial animation proceedings acm siggrapheurographics symposium computer animation july 2627 2003 san diego california edgar chvez gonzalo navarro ricardo baezayates jos luis marroqun searching metric spaces acm computing surveys csur v33 n3 p273321 september 2001 huzefa neemuchwala alfred hero paul carson image matching using alphaentropy measures entropic graphs signal processing v85 n2 p277296 february 2005 gisli r hjaltason hanan samet indexdriven similarity search metric spaces acm transactions database systems tods v28 n4 p517580 december richard szeliski image alignment stitching tutorial foundations trends computer graphics vision v2 n1 p1104 january 2006