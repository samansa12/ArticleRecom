mathematical programming data mining mathematical programming approaches three fundamental problems described feature selection clustering robust representation feature selection problem considered discriminating two sets recognizing irrelevant redundant features suppressing creates lean model often generalizes better new unseen data computational results real data confirm improved generalization leaner models clustering exemplified unsupervised learning patterns clusters may exist given database useful tool knowledge discovery databases kdd mathematical programming formulation problem proposed theoretically justifiable computationally implementable finite number steps resulting kmedian algorithm utilized discover useful survival curves breast cancer patients medical database robust representation concerned minimizing trained model degradation applied new problems novel approach proposed purposely tolerates small error training process order avoid overfitting data may contain errors examples applications concepts given b introduction mathematical programming optimization subject constraints broad discipline applied great variety theoretical applied problems operations research 29 54 network problems 60 53 game theory economics 71 35 engineering mechanics 57 37 recently machine learning 3 61 23 48 46 paper describe three recent mathematicalprogrammingbased developments relevant data mining feature selection 45 10 clustering 11 robust representation 67 note outset plan survey either fields data mining mathematical programming rather highlight recent highly effective applications latter former however point approaches mostly based mathematical programming fundamental nonlinear programming problem 6 44 consists minimizing objective function subject inequality equality constraints typically written follows fx subject gx 0 x ndimensional vector real variables f realvalued function x g h finite dimensional vector functions x functions f g h linear problem simplifies linear program 15 52 69 classical problem mathematical mathematical programming technical report 9605 august 1996 revised november 1996 march 1997 material based research supported national science foundation grant ccr9322479 computer sciences department university wisconsin 1210 west dayton street madison wi 53706 email programming x twodimensional linear program thought problem finding lowest point necessarily unique tilted plane surrounded piecewiselinear fence extremely efficient algorithms exist solution linear programs thus reducing problem single finite sequence linear programs tantamount solving problem another reason emphasizing mathematical programming work broad applicability optimizationunderconstraints paradigm great variety problems many fields formulated effectively solved mathematical programs according great eighteenth century mathematician leonhard euler nothing happens universe sense either certain maximum minimum 68 p 1 point view applicability largescale data mining problems proposed algorithms employ either linear programming sections 2 3 polynomialtimesolvable 36 69 convex quadratic programming section 4 also polynomialtimesolvable 69 extremely fast linear quadratic programming codes 14 capable solving linear programs millions variables 8 40 large quadratic programs make proposed algorithms easily scalable effective solving wide range problems one limitation however problem features must real numbers easily mapped real numbers features discrete represented integers techniques integer programming 24 55 14 employed integer programming approaches applied example clustering problems 64 1 described principally combinatorial approach fundamentally different analytical approach optimization real variables stochastic optimization methods based simulated annealing also used problems inductive concept learning 50 problems considered paper 1 feature selection feature selection problem treated discriminating two finite point sets ndimensional feature space separating plane utilizes features possible problem formulated mathematical program parametric objective function linear constraints 10 step function appears objective function approximated concave exponential nonnegative real line instead conventional sigmoid function neural networks 28 leads fast iterative linearprogrammingbased algorithm solving problem terminates finite number steps wisconsin prognosis breast cancer wpbc 72 51 database proposed algorithm reduced crossvalidation error cancer prognosis database reducing problem features 4 2 clustering clustering problem considered paper assigning points ndimensional real space r n k clusters problem formulated determining k centers r n sum distances point nearest center minimized cluster centers determined training set new point assigned cluster nearest cluster center polyhedral distance 1norm distance used problem formulated minimizing piecewiselinear concave function polyhedral set shown equivalent bilinear program minimizing product two linear functions set determined satisfying system linear inequalities 11 although bilinear program nonconvex optimization problem ie minimizing function valleylike fast finite kmedian algorithm consisting solving linear programs closed form leads stationary point computational testing algorithm kdd tool 18 quite encouraging wisconsin prognosis breast cancer database wpbc distinct clinically important survival curves discovered database kmedian algorithm whereas traditional kmean algorithm 32 64 uses square 2norm distance thus emphasizing outliers failed obtain distinct survival curves database four publicly available databases kmedian kmean algorithms best two databases 3 robust representation problem deals modeling system relations within database manner preserves extent possible validity representation data model based changes problem closely related generalization problem machine learning train system given training set improve generalization new unseen testing set 39 63 73 use simple linear model 67 show sufficiently small error purposely tolerated constructing model broad class perturbations model accurate representation one obtained conventional zero error tolerance simple example demonstrates result 11 notation summarize notation background material used paper ffl vectors column vectors unless transposed row vector superscript ffl vector x ndimensional real space r n jxj denote vector absolute values components x x ffl base natural logarithm denoted gammay denote vector r component gammay ffl x 1 norm kxk p denote pnorm ffl notation 2 r mthetan signify real theta n matrix matrix denote transpose denote row denote rows 2 ae given subset ffl vector ones real space arbitrary dimension denoted e vector zeros real space arbitrary dimension denoted 0 ffl x vectors r n notation minfx yg denote vector r n componentwise minimum x ffl notation arg min fx denote set minimizers fx set similarly arg vertex min fx denote set vertex minimizers fx polyhedral set ffl polyhedral set r n intersection finite number closed halfspaces r n ffl vertex polyhedral set boundary point lies intersection n linearly independent planes constituting boundary typically vertex subset 2 r nthetan nonsingular ffl separating plane respect two given point sets b r n plane attempts separate r n two halfspaces open halfspace contains points mostly b ffl alternatively separating plane interpreted classical perceptron 62 43 threshold determined distance plane origin incoming arc weights perceptron determined components normal vector plane feature selection feature selection extraction attempts use simplest model describe essence phenomenon hence considered application occams law parsimony also known occams razor 65 9 states done fewer assumptions done vain statistical 22 machine learning 39 33 well mathematical programming 12 45 10 approaches feature selection problem work shall deal principally latter novelty approach effectiveness problem shall address binary classification problem ie problem discriminating two given point sets b ndimensional real space r n using n dimensions space possible example medical application described end section attempt discriminate features possible breast cancer patients recurrence disease within two years diagnosis model shall adopt perceptron linear threshold unit ltu employs features given problem possible extensions complex models leading compact accurate decision trees also made 12 geometrically approach corresponds constructing plane r n defined normal w 2 r n distance jflj origin suppressing many components w possible addition set must lie extent possible open halfspace set b open halfspace corresponds ltu threshold fl incoming arc weights w 2 r n many weights possible set zero represent set matrix 2 r mthetan set b matrix b 2 r kthetan problem find fl 2 r w 2 r n many components equal zero possible following inequalities satisfied best sense e vector ones linear programming cannot handle strict inequality constraints rescale 5 follows divide variables w fl positive quantity min call rescaled variables notational economy mind slight abuse notation w fl 5 equivalent e 6 since inequalities may solution general one resorts satisfying best approximate sense minimizing average sum violations leads following robust linear programming formulation 4 rlp min wflyz robustness refers fact useless null vector w 0 naturally excluded solution 7 case linear programming formulations problem 41 66 26 25 note constraints problem variables z satisfy following conditions hence minimizing e force satisfaction best sense 6 equivalently 5 minimizing average violations 6m zero 6 equivalently 5 exactly satisfied linear programming formulation 7 number natural theoretical properties including robustness also effective computationally 4 49 however address problem suppressing irrelevant features order suppress features objective function 7 merely measures average sum violations inequalities 6 modified also suppress many components weight vector w possible achieved weighting original objective 7 weighting exponential function approximation absolute value v weight vector w exponential function e approximates 1norm step function v leads following mathematical program concave objective function linear constraintsfeature selection concave problem degenerates robust w 6 linear program 7 obtains plane p 2 separates sets b optimal fashion without regard feature suppression 0 addition objective separating b attempt suppress many components w possible minimizing exponential smoothing step function nonnegative real line base natural logarithms v absolute value w applications value sufficient make exponential good approximation step function force suppression unnecessary components w described algorithm 21 parameter chosen give best crossvalidated error small values shown theoretically 47 minimization problem 8 picks solution robust linear program 7 minimizes exponential term 8 hence solves rlp 7 suppressing redundant components w objective function 8 concave function bounded zero nonempty polyhedral set 8 follows vertex solution feasible region contain lines extending infinity directions 59 corollaries 3233 3234 excluding lines readily accomplished simple transformation variables w fl nonnegative variables w 1 using standard transformation sake simplicity needed computationally shall forgo transformation fast finitelyterminating successive linear programming algorithm proposed solving problem 10 follows algorithm 21 successive linearization algorithm sla fsv 8 choose 2 0 1 start random w w determine next iterate solving linear program w stop v comment parameter ff set 5 parameter chosen set f0 005 desired one achieving best crossvalidated separation shown 45 theorem 42 algorithm terminates finite number steps typically five six global solution stationary point satisfying necessary optimality condition algorithm tested 32feature wisconsin prognostic breast cancer wpbc database 72 51 collected 28 patients cancer recurred within two years 119 patients cancer recur within two years thus terminology formulation 28 118 problem separating plane obtained successive linearization algorithm 21 used 4 features 32 increasing tenfold crossvalidation correctness 354 10 values parameter used successive linearization algorithm 21 number features determine separating plane vary 1 32 effect using different number features shown figure 1 shows plot tenfold crossvalidation correctness corresponding number features used indicated figure 1 shows best tenfold correctness occurs four features number nonzero elements w test correctness figure 1 feature selection prognosis problem tenfold crossvalidation correctness versus number features selected successive linearization algorithm 21 3 clustering via mathematical programming unsupervised assignment elements given set groups clusters like points objective cluster analysis many approaches problem including statistical 32 machine learning 20 integer mathematical programming approaches 64 1 58 11 shall describe recent approach 11 utilizes fast bilinear programming minimizing product two linear functions set defined linear inequalities principal motivation behind mathematical programming approach precise concise statement clustering problem concave minimization problem 11 given note concave minimization problem involves minimizing concave function mountain like function polyhedral set difficult minimizing convex function valleylike function former may many local minima vertices global minima whereas latter local minimum global minimum nevertheless clustering application concave formulation effective discovering well separated survival curves determining k cluster centers sum 1norm distances point given database nearest cluster center minimized new point assigned cluster center nearest point simple formulation restated bilinear program 12 leads fast kmedian algorithm 31 reformulation 11 using square 2norm instead 1norm leads kmean algorithm 32 64 although intention carry detailed comparative study two clustering algorithms kmedian algorithm described give well separated survival curves breast cancer patients whereas kmean algorithm four publicly available databases kmedian kmean algorithms best two databases indicates potential kmedian algorithm kdd tool describe mathematical programming approach given set points r n represented matrix 2 r mthetan number k desired clusters formulate clustering problem follows find cluster centers c sum minima 2 1norm distance point cluster centers c specifically need solve following mathematical program minimize cd min subject gammad 2 r n dummy variable bounds components difference point center c e n theta 1 vector ones r n hence e bounds 1norm distance c note case robust regression 3127 pp 8287 use 1norm measure error criterion leads insensitivity outliers resulting distributions pronounced tails also note since objective function 11 minimum k linear hence concave functions piecewiselinear concave function 44 corollary 4114 case 2norm pnorm p 6 1 although 11 nphard reformulated following bilinear program solved effectively using kmedian algorithm consists solving succession simple linear programs closed form state bilinear programming formulation kmedian algorithm solving clustering problem proposition 31 clustering bilinear program clustering problem 11 equivalent following bilinear program minimize subject gammad essentially obvious result 11 proposition 22 seen fact fixed setting components equal zero except one corresponding smallest e respect equal 1 leads objective function 11 12 note constraints 12 uncoupled variables c variable hence uncoupled bilinear program algorithm ubpa 5 algorithm 21 applicable simply stated algorithm alternates solving linear program variable linear program variables c algorithm terminates finite number iterations stationary point satisfying minimum principle necessary optimality condition problem 12 5 theorem 21 note however simple structure bilinear program 12 two linear programs solved explicitly closed form leads following algorithmic implementation algorithm 31 kmedian algorithm given cluster centers c j k iteration j compute k following two steps cluster assignment determine c j closest one norm b cluster center update choose c j1 median assigned stop c j1 assign point cluster whose center closest 1norm point although kmedian algorithm similar kmean algorithm wherein 2norm distance used 64 22 differs computationally theoretically fact underlying problem 12 kmedian algorithm concave minimization polyhedral set corresponding problem two pnorm p 6 1 minimize cd min subject gammad concave minimization polyhedral set minimum set convex functions general concave also note kmean algorithm finds stationary point problem 13 problem except kd k 2 replaced kd k 2 2 thus favoring outliers without squared distance term subproblem kmean algorithm becomes considerably harder weber problem 56 13 locates center r n closest sum euclidean distances squares finite set given points weber problem closed form solution however using mean cluster center points assigned cluster done kmean algorithm minimizes sum squares distances cluster center points guaranteed way ensure global optimality solution obtained either kmedian kmean algorithms different starting points used initiate algorithm random starting cluster centers heuristic used placing k initial centers along coordinate axes densest second densest intervals axes latter heuristic used computational results test effectiveness kmedian algorithm used kdd tool 18 mine breast cancer database wpbc order discover medical knowledge medical databases extracting wellseparated survival curves provides essential prognostic tool survival curves 34 38 give expected percent surviving patients function time kmedian algorithm applied wpbc extract curves survival curves constructed 194 patients using two clinically available features patient tumor size number cancerous lymph nodes excised using kmedian algorithm separated points 3 clusters survival curve cluster depicted figure 2a key observation make curves well separated hence clusters used prognostic indicators assign survival curve patient depending cluster patient falls contrast kmean algorithm obtained poorly separated survival curves shown figure 2b hence useful prognosis another comparison kmedian kmean algorithms performed databases known classes correctness measured ratio sum number majority points cluster total number points data set table 1 shows results averaged ten random starts four databases irvine repository databases 51 note two databases kmedian gave better correctness kmean two kmean better months percent diseasefree survival curves 3 clusters using tumorlymph kmedian kmedian percent diseasefree survival curves 3 clusters using tumorlymph kmean b kmean figure 2 survival curves 3 clusters 194 cancer patients obtained kmedian kmean algorithms algorithm database wdbc cleveland votes stargalaxybright unsupervised kmedian 932 806 846 876 unsupervised kmean 911 831 855 856 table 1 training set correctness using unsupervised kmedian kmean algorithms supervised robust lp four databases robust representation consider problem generate robust representation system model remains valid class data perturbation problem closely related generalization problem machine learning train system given training set improve generalization new unseen testing set 39 63 73 shall concentrate recent results 67 obtained simple linear model make essential use mathematical programming ideas ideas although rigorously established simple linear model likely extend complex systems somewhat related approach vapnik proposed quadratic program constructing plane obtain smallest probability error separating two point sets 70 section 54 bennett bredensteiner 2 formulated similar problem using linear programming vapnik 70 section 59 also makes use hubers robust regression ideas 30 extends latters robust regression loss function 70 p 152 adding ffl insensitive zone fflinsensitive zone similar tolerance zone see 17 18 wherein errors disregarded fall within band gamma another similarity vapniks work presence regularization term fflkxk 2 2 minimization 17 problem introduced order make solution unique rigorously derive generalization theorems vapniks bounding weight vector order control vc dimension 70 p 128 theorem 51 however approach provides novel precise deterministic conditions propositions 41 42 tolerating insensitivity zone give better generalization results conventional zerotolerance used ordinary least squares approach model shall consider consists training set fa ag given theta n real matrix given theta 1 real vector vector x r n learned linear system exact solution satisfied approximate fashion error satisfying unseen testing set c c 2 r kthetan theta r k minimized course disregard testing set error 15 problem becomes standard leastnorm problem min kdeltak norm r however eye possible perturbations given training set fa ag pose following motivational question vector training set known accuracy small positive number make sense attempt drive error zero done 16 better tolerate errors satisfaction magnitude words instead 14 try satisfy following system inequalities best sense solve following regularized quadratic program nonnegative small positive ffl minimize xyz2 2subject gammaz gamma e ax gamma e z errors satisfying inequalities 17 ffl small fixed positive regularization constant ensures uniqueness x component solution although ffl held fixed computational experiments possible optimize value crossvalidation tuning set order obtain better generalization note immediately degenerates regularized classical least squares problem min x2r key question ask conditions solution x 18 0 give smaller error x0 testing set able give answer question corroborate computationally 67 considering general testing set c c 2 r kthetan theta r k problem 15 well simpler testing set right side 14 perturbed first restrict latter simpler perturbation p arbitrary fixed perturbation r consider following associated error particular would like know f0 local minimum f set f j 0g fact interested interval 0 defined minimum value 18 approaches zero ffl approaches zero following proposition gives sufficient condition ensures solving 18 positive produces x generalizes better system 20 obtained solving plain regularized least squares problem 19 f proposition 41 robust representation 67 solution x 18 testing set error function f 21 strict local maximum 0 global minimum 0 defined 22 0 whenever 2 0 sufficiently small 410121416tolerance test figure 3 computational example proposition 41 bottom curve depicts decreasing test error f 21 perturbed system 23 top curve demonstrates increasing error effect violating 23 sufficiently small ffl computational results carried 67 corroborated improved generalization results proposition 41 depict figure 3 simple numerical example uses training model 14 vector x learned various error tolerances 18 tested perturbed system p learning situation depicted bottom curve figure makes acute angle neighborhood hence increases test error satisfying decreases upper curve show perturbation angle reversed testing model using perturbation p testing error goes increases away zero conclude section extending proposition 41 general testing model c 2 r kthetan c 2 r k chosen arbitrarily define corresponding error function g 21 measures error satisfying 24 x learned solving regularized quadratic program 18 thus error ck 2 general formulation able give following result tells us tolerant training lead improved generalization proposition 42 improved generalization positive tolerance testing model let x defined tolerant training ax regularized quadratic program 18 tolerance 0 let g denote error generated x testing model defined 25 zerotolerance error g0 local maximum g set f 0g whenever r residual vector defined furthermore testing set error function g 25 global minimum 0 defined 22 0 whenever condition 26 holds 5 conclusion number ideas based mathematical programming proposed solution fundamental problems feature selection clustering robust representation examples applications ideas given show effectiveness discuss issues associated approaches methods use real variables even though class problems falling category quite broad requirement imposes restriction type problems handled nevertheless proposed methods applied problems discrete variables one willing use techniques integer mixed integer programming 55 21 difficult fact one proposed algorithms kmedian algorithm whose finite termination established problems real variables directly applicable change problems ordered discrete variables integers well performs problems would interesting problem examine another important practical issue scalability mentioned earlier since linear programs millions variables solved present day stateoftheart methods largescale databases amenable proposed linearprogrammingbased methods addition large body literature parallel solution decomposition large scale mathematical programs 7 16 17 19 many algorithms part data loaded memory time parallel decomposition algorithms extend applicability proposed methods large scale databases numerical standpoint mathematical programming codes especially linear quadratic programming codes reliable robust codes constant state improvement last fifty years wellunderstood polynomialtime finite termination linear quadratic programming interior methods led powerful reliable commercial software cplex 14 easily reliably implement proposed algorithms finally point although linear model used feature selection robust representation models nonlinear models linear parameters eg quadratic surfaces easily transformed linear system done example 41 quadratic separation achieved linear programming however inherently nonlinear problems example parameters separating surface appear nonlinearly one may resort nonlinear models theory algorithms nonlinear programming 42 6 would promising problem pursue conclude hope problems solved demonstrate theoretical computational potential mathematical programming versatile effective tool solving important problems data mining knowledge discovery databases r tabu search approach clustering problem geometry learning neural network training via linear programming robust linear programming discrimination two linearly inseparable sets bilinear separation two sets nspace nonlinear programming parallel distributed computation largescale linear programming case study combining interior point simplex methods occams razor feature selection via mathematical pro gramming clustering via concave minimization feature minimization within decision trees fermatweber problem convex cost functionals cplex optimization inc linear programming extensions serial parallel solution large scale linear programs augmented lagrangian successive overrelaxation kdd process extracting useful knowledge volumes data parallel variable distribution knowledge acquisition via incremental conceptual clustering fundamentals applications topics chemical engineering statistical pattern recognition convergence properties backpropagation neural nets via theory stochastic gradient methods integer programming improved linear programming models discriminant analysis mathematical methods pattern classification fundamentals artificial neural networks introduction theory neural computation introduction operations research robust estimation location parameter robust statistics algorithms clustering data irrelevant features subset selection problem nonparametric estimation incomplete observations mathematical methods theory games new polynomial time algorithm linear programming mathematical programming contact problems survival analysis optimal brain damage linear nonlinear separation patterns linear programming nonlinear programming mathematical programming neural networks nonlinear programming machine learning via polyhedral concave minimization mathematical programming machine learning nonlinear perturbation linear programs serial parallel backpropagation convergence via nonmonotone perturbed minimization breast cancer diagnosis prognosis via linear programming combinatorial optimization inductive concept learning uci repository machine learning databases linear programming network programming operations research john wiley quadratically convergent method minimizing sum euclidean norms inequality problems mechanics applications analysis mathematical programming convex analysis network flows monotropic optimization polynomial time algorithm construction training class multilayer perceptrons parallel distributed processing overfitting avoidance bias readings machine learning pattern classifier design linear programming improved generalization via tolerant training foundations theory learning systems linear programming foundations extensions nature statistical learning theory theory games economic behavior wpbc wisconsin prognostic breast cancer database mathematics generalization tr ctr bernardete ribeiro learning adaptive kernels model diagnosis design application hybrid intelligent systems ios press amsterdam netherlands goberna v jeyakumar n dinh dual characterizations set containments strict convex inequalities journal global optimization v34 n1 p3354 january 2006 sanjeev arora prabhakar raghavan satish rao approximation schemes euclidean carlotta orsenigo carlo vercellis accurately learning examples polyhedral classifier computational optimization applications v38 n2 p235247 november 2007 v f demyanov bagirov rubinov method truncated codifferential application problems cluster analysis journal global optimization v23 n1 p6380 may 2002 p bradley l mangasarian j b rosen parsimonious least norm approximation computational optimization applications v11 n1 p521 oct 1998 scalable decision tree system application pattern recognition intrusion detection decision support systems v41 n1 p112130 november 2005 jongshi pang guest editorial computational optimization applications v13 n13 p512 april 1999 jongshi pang guest editorial computational optimization applications v12 n13 p512 jan 1999 tapas kanungo david mount nathan netanyahu christine piatko ruth silverman angela wu analysis simple gentile new approximate maximal margin classification algorithm journal machine learning research 2 312002 tapas kanungo david mount nathan netanyahu christine piatko ruth silverman angela wu efficient kmeans clustering algorithm analysis implementation ieee transactions pattern analysis machine intelligence v24 n7 p881892 july 2002 gunnar rtsch sebastian mika bernhard schlkopf klausrobert mller constructing boosting algorithms svms application oneclass classification ieee transactions pattern analysis machine intelligence v24 n9 p11841199 september 2002 balaji padmanabhan alexander tuzhilin use optimization data mining theoretical interactions ecrm opportunities management science v49 n10 p13271343 october sudipto guha adam meyerson nina mishra rajeev motwani liadan ocallaghan clustering data streams theory practice ieee transactions knowledge data engineering v15 n3 p515528 march