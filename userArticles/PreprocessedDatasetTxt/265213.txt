design evaluation windowconsistent replication service abstractrealtime applications typically operate strict timing dependability constraints although traditional data replication protocols provide fault tolerance realtime guarantees require bounded overhead managing redundancy paper presents design evaluation windowconsistent primarybackup replication service provides timely availability repository relaxing consistency replicated data service guarantees controlled inconsistency scheduling update transmissions primary backups ensures client applications interact windowconsistent repository backup must supplant failed primary experiments prototype implementation network intelbased pcs running rtmach show service handles range client loads maintaining bounds temporal inconsistency b introduction many embedded realtime applications automated manufacturing process control require timely access faulttolerant data repository faulttolerant systems typically employ form redundancy insulate applications failures time redundancy protects applications repeating computation communication operations space redundancy masks failures replicating physical resources timespace tradeoffs employed systems may prove inappropriate achieving fault tolerance realtime environment particular time scarce overhead managing redundancy high alternative approaches must balance tradeoff timing predictability fault tolerance example consider processcontrol system shown figure 1a digital controller supports monitoring control actuation plant external world controller software executes work reported paper supported part national science foundation grant mip9203895 opinions findings conclusions recommendations expressed paper authors necessarily reflect view nsf controller plant external world sensors actuators repository inmemory controller backup primary controller sensors actuators plant external world replicated inmemory repository digital controller interacting plant b primarybackup control system figure 1 computer control system tight loop sampling sensors calculating new values sending signals external devices control also maintains inmemory data repository updated frequently iteration control loop data repository must replicated backup controller meet strict timing constraint system recovery primary controller fails shown figure 1b event primary failure system must switch backup node within hundred milliseconds since hundreds updates data repository iteration control loop impractical perhaps impossible update backup synchronously time primary repository changes alternative solution exploits data semantics processcontrol system allowing backup maintain less current copy data resides primary application may distinct tolerances staleness different data objects sufficiently recent data backup safely supplant failed primary backup reconstruct consistent system state extrapolating previous values new sensor readings however system must ensure distance primary backup data bounded within predefined time window data objects may distinct tolerances far backup lag behind object state becomes stale challenge bound distance primary backup consistency compromised minimizing overhead exchanging messages primary backup paper presents design implementation data replication service combines faulttolerant protocols realtime scheduling temporal consistency semantics accommodate system requirements 24 29 client application registers data object service declaring consistency requirements data terms time window primary selectively transmits backup opposed sending update every time object changes bound resource utilization data inconsistency primary ensures backup site maintains version object valid primary within preceding time window scheduling update messages next section discusses related work faulttolerant protocols relaxed consistency se mantics emphasis supporting realtime applications section 3 describes proposed windowconsistent primarybackup architecture replication protocols maintaining controlled inconsistency within service replication model introduces number interesting issues scheduling fault detection system recovery section 4 considers realtime scheduling algorithms creating maintaining windowconsistent backup section 5 presents techniques fault detection recovery primary backup communication failures section 6 present evaluate implementation windowconsistent replication service network intelbased pcs running rtmach 32 section 7 concludes paper highlighting limitations work discussing future research directions related work 21 replication models common approach building faulttolerant distributed systems replicate servers fail independently active statemachine replication schemes 6 30 collection identical servers maintains copies system state client write operations applied atomically replicas detecting server failure remaining servers continue service passive primarybackup replication 2 9 hand distinguishes one replica primary server handles client requests write operation primary invokes transmission update message backup servers primary fails failover occurs one backups becomes new primary recent years several faulttolerant distributed systems employed statemachine 7 11 26 primarybackup 4 5 9 replication general passive replication schemes longer recovery delays since backup must invoke explicit recovery algorithm replace failed primary hand active replication typically incurs overhead responding client requests since service must execute agreement protocol ensure atomic ordered delivery messages replicas replication models client write operation generates communication within service maintain agreement amongst replicas artificially ties rate write operations communication capacity service limiting system throughput ensuring consistent data past work server replication focused cases improving throughput latency client requests example figure 2a shows basic primarybackup model client write operation primary p triggers synchronous update backup b 4 service improve c2a blocking b efficient blocking c nonblocking figure 2 primarybackup models response time allowing backup b acknowledge client c 2 shown figure 2b finally primary reduce write latency replying c immediately sending update message b without waiting acknowledgement 8 shown figure 2c similar performance optimizations apply statemachine replication model although techniques significantly improve average performance guarantee bounded worstcase delay since limit communication within service synchronization redundant servers poses additional challenges realtime environments applications operate strict timing dependability constraints server replication hard realtime systems investigation several recent experimental projects 15 16 33 synchronization overheads communication delay interaction external environment complicate design replication protocols realtime applications overheads must quantified precisely system satisfy realtime constraints 22 consistency semantics replication service bound overheads relaxing data consistency requirements repository large class realtime applications system recover server failure even though servers may maintained identical copies replicated state facilitates alternative approaches trade atomic causal consistency amongst replicas less expensive replication protocols enforcing weaker correctness criterion studied extensively different purposes application areas particular number researchers observed serializability strict correctness criterion realtime databases relaxed correctness criteria higher concurrency permitting limited amount inconsistency transaction views database state 12 17 18 20 28 similarly imprecise computation guarantees timely completion application relaxing accuracy requirements computation 22 particularly useful applications use discrete samples continuoustime variables since values approximated sufficient time compute exact value weak consistency also improve performance nonrealtime applications instance quasicopy model permits inconsistency central data cached copies remote sites 1 gives scheduler flexibility propagating updates cached copies spirit windowconsistent replication allows computations may otherwise disallowed existing active passive protocols require atomic updates collection replicas 3 windowconsistent replication windowconsistent replication service consists primary one backups data primary shadowed backup site servers store objects change time response client interaction primary absence failures primary satisfies client requests supplies dataconsistent repository however primary crashes window consistent backup performs failover become new primary hence service availability hinges existence windowconsistent backup supplant failed primary 31 system model unlike primarybackup protocols figure 2 windowconsistent replication model decouples client read write operations communication within service shown figure 3 primary object manager om handles client data requests sending messages backups behest update scheduler us since read write operations trigger transmissions backup sites client response time depends local operations primary allows primary handle high rate client requests independently sending update messages backup sites although update transmissions must accommodate temporal consistency requirements objects primary cannot compromise client applications processing demands hence primary must match update rate available processing network bandwidth selectively transmitting messages backups primary executes admission control algorithm part object creation ensure us schedule sufficient update transmissions new objects unlike client reads writes object creation deletion requires complete agreement primary backups replication service 32 consistency semantics primary us schedules transmissions backups ensure replica sufficiently recent version object timestamps p versions object primary backup sites respectively time primary p copy written client application time p backup b stores possibly older version originally written p time b may older version p copy b must recent scheduler update object manager scheduler update object manager primary backup update ack ack createdelete createdelete readwrite figure 3 windowconsistent primarybackup architecture enough window ffi windowconsistent backup must believe data valid p within last definition 1 time backup copy object windowinconsistency maximum time 0 object windowconsistent windowconsistent objects windowconsistent words b windowconsistent copy object time example figure 4 p performs several write operations behalf client requests selectively transmits update messages b time primary recent version object written client time backup copy first recorded primary time b primary stopped believing version time c thus p b windowconsistent version time backup object inconsistency less windowconsistency requirement ffi small value allows client operate recent copy object backup must supplant failed primary represents objects temporal inconsistency within replication service seen omniscient observer since backup site always uptodate knowledge client operations backup conversative view temporal consistency discussed section 52 client may also require bounds staleness backups object relative primarys copy construct valid system state failover occurs particular client reads time p receives version wrote units ago hand b supplants failed primary client would read version wrote time units ago version p primary figure 4 client view inconsistency gamma b btbackup view omniscient view client view client update message figure 4 windowconsistency semantics definition 2 time object recovery inconsistency p two components contribute recovery inconsistency client write patterns temporal inconsistency within service windowconsistent replication bounds latter allowing client bound recovery inconsistency based access patterns example suppose consecutive client writes occur w time units apart typically w smaller since primary sends selective updates backup sites windowconsistency bound gamma 0 ensures backups copy object written primary earlier time window consistency guarantees p 4 realtime update scheduling section describes primary use existing realtime task scheduling algorithms coordinate update transmissions backups absence link performance crash failures 10 assume bound endtoend communication latency within service example realtime channel 14 23 desired bound could established primary backups several approaches providing bounds communication latency discussed 3 client operation modifies primary must send update object within next otherwise backups may receive sufficiently recent version timewindow elapses order bound temporal inconsistency within service suffices primary send backups least every units bounding temporal inconsistency primary may send additional updates backups sufficient processing network capacity available extra transmissions increase services resilience lost update messages average goodness replicated data addition sending update transmissions backups primary must allow efficient integration new backups replication service limited processing network capacity necessitate tradeoff timely integration new backup keeping existing backups windowconsistent primary minimize time integrate new replica especially windowconsistent backups since subsequent primary crash would result server failure primary constructs schedule sends object backup exactly allows primary smoothly transition update transmission schedule several task models accommodate requirements windowconsistent scheduling backup integration initially consider periodic task model 19 21 41 periodic scheduling updates transmissions updates cast tasks run periodically deadlines derived objects windowconsistency requirements primary coordinates transmissions backups scheduling update task period p service time e object consistency permits maximum period 2 end period serves deadline one invocation task arrival time subsequent invocation scheduler always runs ready task highest priority preempting execution higherpriority task arrives example ratemonotonic scheduling statically assigns higher priority tasks shorter periods 19 21 earliestduedate scheduling favors tasks earlier deadlines 21 scheduling algorithm coupled object parameters e ffi determines schedulability criterion based total processor network utilization schedulability criterion governs object admission replication service primary rejects object registration request specifying e cannot schedule sufficient updates new object without jeopardizing window consistency existing objects ie sufficient processing network resources accommodate objects windowconsistency requirements scheduling algorithm maintains window consistency objects long collection tasks exceed certain bound resource utilization eg 069 ratemonotonic 1 earliestduedate 21 42 compressing periodic schedule periodic model guarantee sufficient updates object schedule updates per period p even computation network resources permit frequent transmissions restriction arises periodic model assumes task becomes ready run period boundaries however primary transmit current version object time scheduler capitalize readiness tasks improve resource utilization window consistency backups compressing periodic schedule consider two objects 1 1 shown figure 5 unshaded boxes denote transmission 1 shaded boxes signify transmission 2 scheduler must send update requiring 1 unit processing time every 3 time units 1 size determines time e required update transmission order accommodate preemptive scheduling objects various sizes primary send update message one fixedlength packets periodic schedule b compressed periodic schedule figure 5 compression p 1 unshaded box update requiring 2 units processing time every 5 time units shaded box schedule repeats major cycle length 15 time unit corresponds tick granularity resource allocation processing transmission packet example ratemonotonic earliestduedate algorithms generate schedule shown figure 5a update sent required major cycle length 15 schedule 4 units slack time replication service capitalize slack time improve average temporal consistency backup objects particular periodic schedule figure 5a provide order task executions without restricting time tasks become active tasks ready run scheduler advance earliest pending task activate task advancing logical time start next period object compressed schedule primary still transmits update least per period p send frequent update messages time allows shown figure 5b compressing slack time allows schedule start time 11 worst case compressed schedule degrades periodic schedule associated guarantees 43 integrating new backup minimize time service operates without windowconsistent backup primary p needs efficient mechanism integrate new invalid backup b p must send new backup b copy object transition normal periodic schedule shown figure 6 although b may windowconsistent objects execution integration schedule object must become consistent remain consistent first update normal periodic schedule result b must receive copy within period p periodic schedule begins ensures b afford wait next p interval start receiving periodic update messages order integrate new backup primary must execute integration schedule would allow transition periodic schedule maintaining window consistency referring figure 6 windowconsistent transition requires prior post prior j time elapsed last transmission j end integration schedule post j time start periodic schedule first transmission j j k prior post transition integration schedule periodic schedule figure integrating new backup repository ensures window consistency object even across schedule transition since periodic task model provides post suffices ensure post simple schedule integration send objects new backup using normal periodic schedule already used update transmissions existing replicas incurs worstcase delay 2 integrate new backup service however service windowconsistent backup sites primary minimize time required integrate new replica particular efficient integration schedule transmit object exactly transitioning normal periodic schedule primary may adapt normal periodic schedule efficient integration schedule removing duplicate object transmissions particular primary transmit objects order last update transmissions end major cycle normal schedule example schedule shown figure 5a integration schedule last transmission 1 time 10 12 transition integration schedule normal schedule sustains window consistency newly integrated backup since normal schedule guarantees window consistency across major cycles since integration schedule derived periodic schedule follows prior post normal schedule order determined objects created first major cycle normal schedule since schedule transmits object integration delay number registered objects although approach efficient static object sets dynamic creation deletion objects introduces complexity since transmission order normal schedule depends object set primary must recompute integration schedule whenever new object enters service cost constructing integration schedule especially dynamic object sets reduced sending objects b reverse period order objects larger periods sent smaller periods object j ensures objects smaller equivalent periods follow j integration schedule objects precede j periodic schedule guarantees integration schedule transmits j p units start periodic schedule ensuring windowconsistent transition example figure 6 p sends update b b receives p receives select object xmit last last last last send figure 7 update protocols periodic schedule objects transmitted least within time post exactly within time prior follows prior post object creations deletions primary construct new integration schedule sorting new set periods primary minimizes time operates without windowconsistent backup transmitting object exactly transitioning normal periodic schedule 5 fault detection recovery although realtime scheduling update messages maintain windowconsistent replicas processor communication failures potentially disrupt system operation assume servers may suffer crash failures communication subsystem may suffer omission performance failures site fails remaining replicas must recover timely manner continue datarepository service primary attempts minimize time operates without windowconsistent backup since subsequent primary crash would cause service failure similarly backup tries detect primary crash initiate failover backup objects become windowinconsistent although primary backup cannot complete knowledge global system state message exchange servers provides measure recent service activity 51 update protocols figure 7 shows primary backup sites exchange object data estimate global system state assume servers communicate exchanging messages since messages include temporal information p b cannot effectively reason unless server clocks synchronized within known maximum bound clock synchronization algorithm use transmit times update acknowledgement messages bound clock skew service using update protocols p b approximate global state maintaining recent information received site transmitting update message time primary records version timestamp xmit selected object since b information gives p optimistic view backups window consistency primarys message backup contains object data along version timestamp transmission time b uses transmission time detect outof order message arrivals maintaining xmit time recent transmission successfully received sites store monotonically nondecreasing version timestamps without requiring reliable inorder message delivery service upon receiving newer transmission backup updates objects data version timestamp b discussed section 52 backup uses xmit reason window consistency diagnose crashed primary b also maintains last transmission time last message received p regarding object last g similarly p tracks transmission times bs messages diagnose possible crash failures hence backups acknowledgement message p includes transmission time well b recent version timestamp b using information primary determines ack recent version b successfully acknowledged since b variable gives p pessimistic measure backups window consistency discussed section 53 primary uses ack select policies scheduling update transmissions backup 52 backup recovery primary failures backup site must estimate window consistency status primary successfully supplant crashed primary b may unaware recent client interaction p object b know time xmit object although p may continue believe version xmit even transmitting update message b conservatively estimates client wrote new version p transmitted object time xmit particular definition 3 time backup copy object estimated inconsistency backup knows windowconsistent figure 4 shows example backup view window consistency using consistency metric backup must balance possibility becoming window inconsistent likehood falsely diagnosing primary crash b believes objects still windowconsistent b need trigger failover delay would endanger consistency backup object particular backup conservatively estimates copy could become windowinconsistent time xmit absence update messages p however reduce likelihood false failure detection failover occur b received messages p minimum time fi adaptive failure detection mechanism b diagnoses primary crash time crash last failover new primary site invokes client application begins interacting external environment period time new p operates partially inconsistent data gradually constructs consistent system state old values new sensor readings new p later integrates fresh backup enhance future service availability diagnoses primary crash missed update messages lost delayed messages could still trigger false failure detection resulting multiple active primary sites system multiple backups replicas vote select single valid primary however service two sites communication failures cause site assume failed situation thirdparty witness 27 select primary site witness act primary backup server casts deciding vote failure diagnosis realtime control system actuator devices could implicitly serve witness new server starts issuing commands actuators devices could ignore subsequent instructions previous primary site 53 primary recovery backup failures service availability also depends timely recovery backup failures since datarepository service continues whenever valid primary exists primary temporarily tolerate backup crashes communication failures without endangering client application ultimately though p minimize portion time operates without windowconsistent backup since subsequent primary crash would cause service failure primary diagnose possible backup crashes efficiently integrate new backup sites p believes operational backup become window inconsistent due lost update messages transient overload conditions primary quickly refresh inconsistent objects section 52 timeout mechanisms detect possible server failures primary assumes backup crashed p received acknowledgement messages last ff time units ie last ff detecting backup crash p integrate fresh backup site system continuing satisfy client read write requests p mistakenly diagnoses backup crash system must operate one less replica primary integrates new backup new backup become windowconsistent integration schedule completes described section 43 however backup actually failed large timeout value increases failure diagnosis latency also increases time system operates without sufficient backup sites hence p must carefully select ff maximize backups chance recovering subsequent primary failure even backup site crash delayed lost update messages compromise window consistency backup objects making b ineligible replace crashed primary using ack xmit estimate consistency backup objects select appropriate policy scheduling update transmissions primary may choose reintegrate inconsistent backup even last ff rather wait later update message restore objects window probability message loss040120average maximum distance w300 msec compression w700 msec compression w300 msec compression w700 msec compression probability message loss002006010 probability backup inconsistent w300 msec compression w700 msec compression w300 msec compression w700 msec compression average maximum distance b probabilitybackup inconsistent figure 8 window consistency graphs show performance service function client write rate message loss schedule compression although object inconsistency increases message loss compressing periodic schedule reduces effects communication failures inconsistency increases client writes frequently since primary changes object soon transmitting update message backup consistency suppose primary thinks bs copy windowinconsistent periodic update scheduling p may send another update message object time 2p later object large window ffi primary reestablish backups window consistently quickly executing integration schedule requires time p service time object described section 41 still primary cannot accurately determine backup object inconsistent since lost delayed acknowledgement messages result overly pessimistic value ack primary overly aggressive diagnosing inconsistent backup objects since reintegration temporarily prohibits backup replacing failed primary instead p ideally retransmit offending object without violating window consistency objects service example p schedule special retransmission window transmitting objects received acknowledgement messages past updates retransmission object selected service p transmits update message one existing objects based values ack improves likelihood windowconsistent backup sites even presence communication failures 6 implementation evaluation 61 prototype implementation developed prototype implementation windowconsistent replication service demonstrate evaluate proposed service model implementation consists primary backup server client application running primary node shown figure 3 primary implements ratemonotonic scheduling update transmissions option enable schedule compression tick scheduling allocates processor different activities handling client requests sending update messages processing acknowledgements backup start tick primary transmits update message backup one objects determined scheduling algorithm client readwrite requests update acknowledgements processed next priority given client requests server currently intelbased pc running realtime mach 25 32 operating system 2 sites communicate ethernet udp datagrams using socket library 31 extensions unix select call prioritybased access active sockets initialization sockets registered appropriate priority socket receiving client requests higher priority receiving update acknowledgements backup tick period 100 ms chosen minimize intrusion runnable system processes 3 minimize interference experiments conducted lightlyloaded machines ethernet segment observe significant fluctuations network processor load experiments primary backup sites maintain inmemory logs events runtime efficiently collect performance data minimal intrusion estimates clock skew primary backup derived actual measurements roundtrip latency used adjust occurrence times events calculate distance objects primary backup sites prototype evaluation considers three main consistency metrics representing window consistency backup client views performability metrics influenced several parameters including client write rate communication failures schedule compression experiments vary client write rate changing time w successive client writes object inject communication failures randomly dropping update messages captures effect transient network load well lost update acknowledgements invariants evaluation tick period 100 ms objects window size ffi number objects given tick period ffi n determined schedulability criterion ratemonotonic scheduling algorithm objects update transmission time one tick object size chosen time process transmit object reasonably small earlier experiments sun workstations running solaris 11 show similar results 24 3 100 ms tick period granularity process scheduling quantum limit interference jobs running machine however smaller tick periods desirable order allow objects specify tighter windows window size expressed number ticks respond client requests timely manner compared tick size extra time within tick period used process client requests update acknowledgements experiments ran 45 minutes data point 62 omniscient view window consistency windowconsistency metric captures actual temporal inconsistency primary backup sites serves reference point performance replication service figure 8a shows average maximum distance primary backup function probability message loss three different client write periods without schedule compression measures inconsistency backup object receiving update averaged versions objects reflecting goodness replicated data figure 8b shows probability inconsistent backup function message loss faulttolerance metric measures likelihood backup one inconsistent objects experiments client writes object every tick w every 3 ticks w every 7 ticks w probability message loss varies 0 10 experiments higher message loss rates reveal similar trends message loss increases distance primary backup well likelihood inconsistent backup however influence message loss pronounced due conservative object admission current implementation occurs average periodic model schedules updates twice often necessary order guarantee required worstcase spacing update transmissions message loss influence scheduling models permit higher resource utilization discussed section 7 higher client write rates also tend increase backups inconsistency client writes frequently primarys copy object changes soon sending update message resulting staler data backup site schedule compression effective improving performance variables average maximum distance primary backup message loss yintercept reduces 30 high client rates figure 8a similar reductions seen message loss probabilities occurs schedule compression successfully utilizes idle ticks schedule generated ratemonotonic scheduling algorithm utilization thus increases 100 primary sends approximately 30 object updates backup compression plays relatively important role reducing likelihood inconsistent backup seen figure 8b also compression reduces impact communication failures since extra update transmissions effectively mask lost messages 63 backup view estimated consistency although figure 8 provides systemwide view window consistency backup site limited knowledge primary state backups view estimate actual window consistency shown figure 9 backup site uses metric evaluate probability message loss040120average maximum distance w300 msec compression w700 msec compression w300 msec compression w700 msec compression probability message loss002006010 probability backup inconsistent w300 msec compression w700 msec compression w300 msec compression w700 msec compression average maximum distance b probabilitybackup inconsistent figure 9 backup view plots show system performance backups conservative viewpoint function client write rate message loss schedule compression figure temporal consistency improves schedule compression worsens increasing message loss backups view impervious client write rate window consistency detect crashed primary effect failover figure 8 message loss increases average maximum distance figure 9a likelihood inconsistent backup figure 9b schedule compression also similar benefits backups estimate window consistency however unlike figure 8 client write rate influence backups view window consistency backup pessimistically assumes client writes object primary immediately primary transmits update message object backup reason backups estimate average maximum distance primary backup always worse derived omniscient view follows estimate accurate high client write rates seen comparing figures 8a 9a high client rates relative window virtually identical windowconsistent replication model designed operate high client write rates relative communication within service backup typically accurate view temporal consistency 64 client view recovery consistency client view p measures inconsistency primary backup versions object reads better recovery consistency provides accurate system state failover since client read arbitrary time figure 10 shows time average recovery inconsistency averaged across objects without compression attribute minor fluctuations graphs noise measurements distance metric sensitive client write rate since frequent client writes increase probability message loss020060100 average distance w300 msec compression w700 msec compression w300 msec compression w700 msec compression figure 10 client view p graph presents time average recovery inconsistency function client write rate message loss schedule compression compressing update schedule improves consistency generating frequent update transmission message loss worsens read consistency metric largely independent client write rate p client writes often primary copy changes frequently ie close backup also receives recent versions data ie b close moderate message loss significant influence read inconsistency especially schedule compression expected schedule compression improves read inconsistency seen client significantly 30 therefore effective technique improving goodness replicated data 7 conclusion future work window consistency offers framework designing replication protocols predictable timing behavior decoupling communication within service handling client requests replication protocol handle higher rate read write operations provide timely response clients scheduling selective communication within service provides bounds degree inconsistency servers prototype implementation successfully demonstrated utility windowconsistent replication model extensive evaluation needed validate ideas identified paper recently added support fault detection failover integration new backups experiments current platform ascertain usefulness processor capacity reserves 25 rtmach features implementing windowconsistent replication service present work extends several fruitful areas object admissionscheduling studying techniques maximize number admitted objects improve objects window consistency optimizing object admission update scheduling windowconsistent replication service periodic task model overly conservative accepting object registration requests may either limit number objects accepted may accept objects relatively large windows occurs average periodic model schedules updates twice often necessary order guarantee required worstcase spacing update transmissions exploring scheduling algorithms distanceconstrained task model 13 assigns task priorities based separation constraints terms implementation complexity ability accommodate dynamic creationdeletion objects also considering techniques maximize goodness replicated data one possible approach exploring ways incorporate client write rate object admission scheduling alternate approach optimize object window size proportionally shrinking object windows system remains schedulable improve objects worstcase temporal inconsistency selection object window sizes cast instance linear programming optimization problem schedule compression still used improve utilization remaining available resources interobject window consistency extending windowconsistent replication model incorporate temporal consistency constraints objects goal bound consistency replicated set related objects new algorithms may necessary realtime update scheduling object sets related problem ensuring temporally consistent objects realtime database system however goal bound consistency replicated set related objects alternative replication models although current prototype implements primarybackup architecture single backup site studying additional issues involved supporting multiple backups addition also exploring window consistency statemachine replication would enable us investigate applicability window consistency alternative replication models acknowledgements authors wish thank sreekanth brahmamdam hocksiong ang help running experiments postprocessing collected data reviewers helpful comments r data caching issues information retrieval system principle resilient sharing distributed resources realtime communication packetswitched networks nonstop kernel highly available network file server reliable communication presence failures process group approach reliable distributed computing tradeoffs implementing primarybackup protocols tradeoffs implementing primarybackup protocols understanding fault tolerant distributed systems faulttolerance advanced automation system partial computation realtime database systems scheduling distanceconstrained realtime tasks realtime communication multihop networks dis tributed faulttolerant realtime systems mars approach ttp protocol faulttolerant realtime systems triggered real time databases consistency constraints ssp semanticsbased protocol realtime data access rate monotonic scheduling algorithm exact characterization average case behavior model hard realtime transaction systems scheduling algorithms multiprogramming hard realtime environment imprecise computations structuring communication software qualityof service guarantees design evaluation windowconsistent replication service processor capacity reserves operating system support multimedia applications consul communication substrate faulttolerant distributed programs using volatile witnesses extend applicability available copy protocols replica control distributed systems asynchronous approach windowconsistent replication realtime applications implementing faulttolerant services using state machine approach tutorial realtime toward predictable realtime system extra performance architecture xpa tr ctr hengming zou farnam jahanian realtime primarybackup replication service ieee transactions parallel distributed systems v10 n6 p533548 june 1999