correlation clustering consider following clustering problem complete graph n vertices items edge u v labeled either depending whether u v deemed similar different goal produce partition vertices clustering agrees much possible edge labels want clustering maximizes number edges within clusters plus number edges clusters equivalently minimizes number disagreements number edges inside clusters plus number edges clusters formulation motivated document clustering problem one pairwise similarity function f learned past data goal partition current set documents way correlates f much possiblesemi also viewed kind agnostic learning probleman interesting feature clustering formulation one need specify number clusters k separate parameter measures kmedian minsum minmax clustering instead formulation optimal number clusters could value 1 n depending edge labels look approximation algorithms minimizing disagreements maximizing agreements minimizing disagreements give constant factor approximation maximizing agreements give ptas building ideas goldreich goldwasser ron 1998 de la veg 1996 also show extend results graphs edge labels 1 1 give results case random noise b introduction suppose given set n documents cluster topics unfortunately idea topic however disposal classifier fa b given two documents b outputs department computer science carnegie mellon university fnikhilavrimshuchigcscmuedu research supported part nsf grants ccr0085982 ccr0122581 ccr 0105488 ibm graduate fellowship whether believes b similar example perhaps f learned past training data case natural approach clustering apply f every pair documents set find clustering agrees much possible results specifically consider following problem given fullyconnected graph g edges labeled similar find partition vertices clusters agrees much possible edge labels particular look terms maximizing agreements number edges inside clusters plus number edges clusters terms minimizing disagreements number edges inside clusters plus number edges clusters two equivalent optimality usual differ point view approximation paper give constant factor approximation problem minimizing disagreements ptas maximizing agreements also extend results case realvalued edge weights problem formulation motivated part clustering problems whizbang labs learning algorithms trained help various clustering tasks 8 9 10 1 interesting clustering problem defined unlike clustering formulations need specify number clusters k separate pa rameter example kmedian 7 15 minsum clustering 20 minmax clustering 14 one always get perfect score putting node cluster question well one k clusters clustering formulation single objective example one problem clustering entity names problem items entries taken multiple databases eg think namesaffiliations researchers goal robust uniq collecting together entries correspond entity per son eg case researchers person might appear multiple times different affiliations might appear middle name without etc practice classifier f typically would output probability case natural edge label logprsameprdifferent 0 classifier unsure positive classifier believes items likely cluster negative classifier believes likely different clusters case f g labels corresponds setting classifier equal confidence decisions optimal clustering might many clusters depends edge labels get feel problem notice exists perfect clustering ie one gets edges correct optimal clustering easy find delete edges output connected components graph remaining called naive algorithm 10 thus interesting case clustering perfect also notice graph g trivial produce clustering agrees least half edge labels edges edges simply put vertices one big cluster otherwise put vertex cluster observation means maximizing agreements getting 2approximation easy note show ptas general finding optimal clustering nphard seen via tedious reduction x3c details found 5 another simple fact notice graph contains triangle two edges labeled one labeled clustering perfect generally number edgedisjoint triangles form gives lower bound number disagreements optimal clustering fact used constantfactor approximation algorithm maximizing agreements ptas quite similar ptas developed 12 maxcut dense graphs related ptass 4 3 notice since must exist clustering least nn 14 agreements means suffices approximate agreements within additive factor n 2 problem also closely related work testing graph properties 13 19 1 fact show use general partition property tester 13 subroutine get ptas running time one 1 unfortunately doubly exponential 1 also present alternative direct algorithm based closely approach 12 takes 2 e 1 relation agnostic learning one way view clustering problem edges examples labeled positive negative trying represent target function f using hypothesis class vertex clusters hypothesis class limited representational power want say u v v w positive language say u w positive might able represent f perfectly sort problem trying find nearly best representation arbitrary target f given limited hypothesis language sometimes called agnostic learning 17 6 observation one trivially agree least half edge labels equivalent standard machine learning fact one always achieve error 12 using either positive negative hypothesis ptas approximating number agreements means optimal clustering error rate find one error rate running time exponential 1 means achieve constant error gap polynomial time makes interesting point view agnostic learning nontrivial problems agnostic learning done polynomial time even simple classes conjunctions disjunctions polynomialtime algorithms known give even error gap 12 notation definitions e complete graph n vertices let eu v denote label edge u v let denote positive negative neighbors respectively let opt denote optimal clustering graph general clustering c let cv set vertices cluster v use denote clustering produced algorithms clustering c call edge u v mistake either call mistake positive mistake otherwise called negative mistake denote total number mistakes made clustering use mopt denote number mistakes made opt positive real numbers x z use x 2 yz finally let x x v denote complement v n x 3 constant factor approximation minimizing disagreements describe main algorithm constantfactor approximation minimizing number disagreements highlevel idea algorithm follows first show lemma 1 cluster portion graph using clusters look sufficiently clean definition 1 charge mistakes made within portion erroneous triangles triangles two edges one edge furthermore way triangles charge nearly edgedisjoint allowing us bound number mistakes constant factor opt second show lemma 2 must exist nearly optimal clustering opt 0 nonsingleton clusters clean finally show theorem 3 lemma 7 algorithmically produce clustering entire graph containing clean clusters singleton clusters mistakes endpoint singleton clusters bounded opt 0 mistakes endpoints clean clusters bounded using lemma 1 begin definition clean cluster good vertex vertex v called good respect c c v satisfies following vertex v good respect wrt c called bad wrt c finally set c clean v 2 c good wrt c present two key lemmas given clustering v clusters clean 14 number mistakes made clustering 8mopt proof let clustering v bound number mistakes made clustering 8 times number edgedisjoint erroneous triangles graph erroneous triangle triangle two edges one edge use fact opt must make least one mistake triangle first consider negative mistakes pick negative edge considered far pick w 2 c u w v w positive associate u v erroneous triangle u v w show u v w always picked negative edges u v 0 ie ones sharing u v also pick w since c clean neither u v jc negative neighbors inside c thus u v least vertices w u w v w positive moreover 2jc j 2 could already chosen negative edges v least 1 4s choices w satisfy required condition since 14 u v always able pick w note positive edge v w chosen times scheme negative mistakes v possibly negative mistakes w thus account least fourth positive edges double counted negative mistakes using edge disjoint erroneous triangles consider positive mistakes associate mistakes erroneous triangles start afresh without taking account labelings previous part consider positive edge j pick w 2 c u w positive v w negative least jc vertices jc jjc j j already taken moreover positive edge u w chosen twice u w w u repeating argument see account least half hence least quarter positive mistakes using edge disjoint triangles depending whether negative mistakes positive mistakes choose triangles appropriately hence account least 18 total mistakes clustering exists clustering opt 0 nonsingleton cluster clean mopt 0 9 1mopt proof consider following procedure applied clustering opt call resulting clustering opt 0 procedure cleanup let c opt k clusters opt 1 let 2 number bad vertices c opt jc opt call dissolving cluster b else let b denote bad vertices c opt 3 output clustering opt prove mopt mopt 0 closely related first show c 0 clean clearly holds nonempty know jc opt 3 point similarly counting positive neighbors v c opt outside c opt thus c 0 clean account number mistakes dissolve c opt clearly mistakes associated vertices original c opt least 3 2 jc opt mistakes added due dissolving clusters c opt dissolved original mistakes least 3jc opt mistakes added procedure jb jjc opt j noting 6 lemma follows clustering opt 0 given lemma use c 0 denote nonsingleton clusters denote set singleton clusters describe algorithm cautious tries find clusters similar opt 0 throughout rest section assume 1 pick arbitrary vertex v following let b vertex removal step 9x 2 av x 3bad wrt av fxg c vertex addition step let 7good wrt avg let 2 delete av set vertices repeat vertices left produced sets av empty latter case output remaining vertices singleton nodes call clusters output algorithm cautious z set singleton vertices created final step main goal show clusters output algorithm satisfy property stated theorem 3 8j 9i c 0 moreover 11clean order prove theorem need following two lemmas clean cluster opt 0 vertex w 2 c 0 proof v w j also 1 jc 0 j thus get following two conditions thus w 3good wrt n v 2 observe vertex addition step vertices added one step opposed vertex removal step lemma 5 given arbitrary set x cannot 3good wrt x proof firstly v 3good wrt arbitrary set x suppose v 1 v 2 3good respect implies jn also since v 1 lies clean cluster c 0 opt 0 follows jn notice jc 0 j j holds c 0 however since 19 contradiction thus result follows gives us following important corollary corollary 6 remove phase algorithm two vertices distinct c 0 j present av go prove theorem 3 proof theorem 3 first show either subset contains exactly one clusters c 0 first part theorem follow cluster let 0 set produced vertex removal phase cluster obtained applying vertex addition phase 0 two cases first consider case 0 vertex addition step vertex enter 0 j follows since c 0 j clean disjoint 0 u enter need jc 0 1 jc 0 two conditions cannot satisfied simultaneously thus second case u 2 c 0 j present 0 ever case observe corollary 6 vertices c 0 k present 0 k 6 j also reasoning case 0 vertex c 0 enter 0 vertex addition phase remains show c 0 since u removed 0 follows many vertices c 0 present 0 particular jn j implies jc 0 j also ja 0 j ja 0 j show remaining vertices c 0 enter vertex addition phase j together imply ja 0 j holds ja 0 added vertex addition step thus shown av contain c 0 j one j fact contain set entirely next show every j 9i st c 0 let v chosen step 1 algorithm v 2 show vertex removal step vertex j removed proof follows easy induction number vertices removed far r vertex removal step base case lemma 4 since every vertex c 0 j 3good respect v induction step observe since vertex j removed thus far every vertex c 0 still 3good wrt intermediate av mimicking proof lemma 4 n replaced av thus contains least 1 jc 0 vertices c 0 j end vertex removal phase hence second case vertex addition phase finally show every nonsingleton cluster 11clean know end vertex removal phase j total number positive edges leaving 0 3ja 0 since vertex addition step add vertices 7good wrt 0 j since vertices v least 7good wrt 0 similarly gives us result ready bound mistakes terms opt opt 0 call mistakes end points clusters j internal mistakes end point z external mistakes similarly opt 0 call mistakes among sets c 0 internal mistakes mistakes one end point external mistakes bound mistakes cautious two steps following lemma bounds external mistakes lemma 7 total number external mistakes made cautious less external mistakes made opt 0 proof theorem 3 follows z cannot contain vertex v c 0 thus z external mistakes made cautious positive edges adjacent vertices z edges also mistakes opt 0 since incident singleton vertices hence lemma follows consider internal mistakes notice could many internal mistakes opt 0 however point apply lemma 1 graph induced v particular bound internal mistakes follows easily observing 11 14 mistakes optimal clustering graph induced v 0 mopt thus lemma 8 total number internal mistakes cautious 8mopt summing results lemmas 7 8 using lemma 2 get following theorem theorem 9 cautious 9 1 4 ptas maximizing agreements section give ptas maximizing agree ments total number positive edges inside clusters negative edges clusters let opt denote optimal clustering denote clustering abuse notation also use opt denote number agreements optimal solution noticed introduction opt nn 14 suffices produce clustering least opt n 2 agreements goal algorithm let number positive edges sets similarly let number negative edges two let opt denote optimal clustering nonsingleton clusters size greater n proof consider clusters opt size less equal n break apart clusters size 1 breaking cluster size reduces objective function viewed s2 per node cluster since n nodes clusters clusters size n total loss n 2the lemma means suffices produce good approximation opt note number nonsingleton clusters opt less 1 let c opt k denote nonsingleton clusters opt let c opt k1 denote set points correspond singleton clusters 41 ptas doublyexponential 1 willing run time doubly exponential 1 reducing problem general partitioning problem 13 idea follows denote graph edges g notice express quality opt terms sizes clusters number edges inside c opt k1 par ticular number agreements opt e ii e k1k1 general partitioning property tester 13 allows us specify values e ij partition g exists satisfying constraints produce partition satisfies approximately obtain partition least opt n 2 agreements property tester runs time exponential 1 polynomial n thus guess values sizes number edges accurately would done suffices fact guess values additive 2 n additive 3 n 2 e ij introduces additional error calls property tester need made algorithm proceeds finding partition possible value e ij returns partition maximum number agreements get following result theorem 11 general partitioning algorithm returns clustering graph g opt agreements probability least 1 runs time exponential 1 polynomial n 1 42 singlyexponential ptas describe algorithm based basic idea random sampling used general partitioning algorithm idea behind algorithm follows notice knew density positive edges vertex clusters could put v cluster positive edges ever trying possible values densities requires much time instead adopt following select small random subset w vertices cluster correctly fw g w 8i enumerating possible clusterings w since subset picked ran domly high probability vertices v density positive edges v w approximately equal density positive edges v decide cluster put v based information however sufficient account edges two vertices belong w consider subsets u size time try possible clusterings fu ij g picking one maximizes agreements respect fw g gives us ptas firstly note jc opt consider agreements graph gnc opt affects solution n 2 assume jc opt present algorithm analysis based assumption later discuss changes required deal case following algorithm performance parameter specified later let density positive edges inside cluster c opt ij density negative edges clusters c opt begin defining measure goodness clustering set u respect fw g enable us pick right clustering set u satisfies following least 1 0 n vertices x 8 j algorithm follows algorithm dividechoose 1 pick random subset w v size 2 partitions w w b let 1 consider random partition 8i ju c 1partitions u let u partition partition choose arbitrarily let number agreements clustering 3 let fw g partition w maximizes return clusters fa corresponding partition w concentrate right partition w given show number agreements clustering corresponding partition fw g least opt 2n 2 since pick best clustering gives us ptas begin showing high probability values partition u corresponding optimal partition good respect fw g thus algorithm find least one partition next show algorithm finds good partitions achieves least opt 2 agreements need following results probability ory please refer 2 proof fact 1 let hn l hypergeometric distribution parameters n l choosing l samples n points without replacement random variable taking value 1 exactly n points let lm fact 2 let mutually independent rvs jx ex j let 2 also need following lemma lemma 12 let arbitrary disjoint sets z set picked random following sum jzj random variables v 2 z bounded jy j expected thus applying fact 2 get 2e 02 jzj2 notice since picked w uniformly random v high probability sizes w proportion jc opt j following lemma formalizes probability least 1 2ke 02 m2 8i proof given using fact 1 since 2e 02 mjc opt taking union bound k values get result using lemma 13 show computed values close true values p n ij respectively gives us following two lemmas 3 lemma 14 w c opt w j c opt probability least 1 4e proof apply lemma 12 two steps first bound w terms considering process picking w c opt second bound terms fixing w considering process picking w j using lemma 13 combine two get lemma lemma 15 probability least 1 8 proof note cannot use argument similar previous lemma directly since dealing edges inside set use following trick consider partition c opt subsets size 0 n 0 j idea bound number positive edges every pair subsets c opt using argument previous lemma adding get result following lemma shows high probability u ij 0 good wrt g able find 0 good partitions lemma given let u probability least 1 g proof sketch first second conditions definition 2 obtained applying argument similar lemmas 15 14 respectively order obtain third condition consider sum f0 1g random variables corre sponding picking u v 1 iff picked vertex lies c opt j adjacent x application chernoff bound followed union bound gives us condition bound total number agreements terms opt 3 please refer 5 full proofs lemmas theorem 17 jc opt probability least 1 proof lemma 16 probability able find 0 good partition u wrt k m4 choice probability least 1 2 2 u 0 good partition following calculation number agree ments assume able find good partitions u need subtract n 2 2 value obtain actual number agreements since u effect number agreements mn start calculating number positive edges inside cluster j given x using fact u aj good wrt condition 3 last follows fact u bj good wrt fw g condition 1 lemma 13 thus bound using lemma 15 total number agreements least hence similarly consider negative edges using lemma 14 estimate u ai u bj get ab summing j get total number negative agreements least opt 12 opt 44 however since lose n 2 2 finding 0 good partitions every u argued n 2 due c opt using obtain opt 3n 2 algorithm fail four situations 2 0 good partition probability 2 lemma 13 hold w probability 2ke 02 m2 lemma 15 hold probability 8k m4 lemma 14 hold pair probability latter three quantities 2 choice algorithm succeeds probability greater 1 need argue case jc opt notice case using argument similar lemma 13 show jw k1 j mwith high prob ability good high probability u ik1 also 0 good wrt w k1 values count number negative edges vertices incorporate proof theorem 17 k clusters case modify algorithm dividechoose consider goodness k1th partitions well gives us guarantee theorem 17 thus strategy run algorithm dividechoose assuming assuming jc opt picking better two outputs one two cases correspond reality give us desired approximation opt u ok different partitions iteration takes onm time nm u partition w algorithm takes time k different partitions w total running time algorithm 2 k 2m gives us following theorem theorem using dividechoose runs time 2 e 1 probability least 1 produces clustering number agreements least opt n 2 minimizing disagreements weighted graphs section 3 developed algorithm minimizing disagreements graph 1 1 weighted edges consider situation edge weights lie interval address setting need define cost model penalty placing edge inside clusters one natural model linear cost function specifically given clustering assign cost 1 xif edge weight x within cluster cost 1xif placed two clusters example edge weighing 05 incurs cost 025 lies inside cluster 075 oth erwise 0 weight edge hand incurs cost 12 matter turns algorithm finds good clustering f1 1ggraph also works well case linear cost function theorem 19 let algorithm produces clustering f1 1ggraph approximation ratio construct algorithm 0 achieves approximation ratio linear cost function proof let g 1ggraph obtained assign weight 1 positive edges g 1 negative edges 0 cost edges weighted arbitrarily let opt optimal clustering g opt 0 optimal clustering g 0 also let 0 measure cost g 0 f1 1g penalty model new opt 2mopt latter opt incurs greater penalty 1 0 compared positive edge clusters negative edge inside cluster situations opt incurs cost least 12 1 0 gives us equation algorithm 0 simply runs graph g 0 outputs resulting clustering 0 need bound terms 0 notice positive edge lies two clusters negative edge lies inside cluster cost incurred edges 0 1 1 total cost due mistakes 0 hand consider cost due positive edges inside clus ters negative edges clusters opt also incurs least cost edges cost due edges increase clustered differ ently cost due mistakes mopt mopt 2mopt mopt another natural cost model one edge weight x incurs cost jxj clustered improperly inside cluster x 0 clusters x 0 cost 0 correct know good approximation case see section 7 6 random noise going back original motivation imagine true correct clustering opt n items reason clustering appear perfect function fa b used label edges error natural consider case errors random constant noise rate 12 edge independently mislabeled respect opt probability machine learning context called problem learning random noise expected much easier handle worstcase problem fact simple algorithms one whp produce clustering quite close opt much closer number disagreements opt f analysis fairly standard much like generic transformation kearns 16 machine learning context even closer analysis condon karp graph partitioning 11 fact problem nearly matches special case planted partition problem mcsherry 18 present analysis anyway since algorithms simple onesided noise easier special case let us consider onesided noise true edge flipped probability case u v different clusters opt jn certain u v cluster every node cluster independently probability neighbor cluster large n nonempty intersection high probability consider clustering greedily pick arbitrary node v produce cluster repeat probability correctly cluster nodes whose clusters opt size log n remaining nodes might placed clusters small overall number edgemistakes twosided noise twosided case technically easier consider symmetric difference n u v u v cluster opt every node w 62 fu vg probability exactly 21 belonging symmetric difference u v different clusters nodes w optuoptv probability 1 belonging symmetric difference w 62 optu optv probability remains 21 since 21 constant less 12 means confidently detect u v belong different clusters long log n furthermore using vj approximately sort vertices cluster sizes combining two facts whp correctly cluster vertices large clusters place others cluster making total 32 edge mistakes 7 open problems concluding remarks paper presented constantfactor approximation minimizing disagreements ptas maximizing agreements problem clustering vertices fullyconnected graph g f g edge labels section 5 extended results case realvalued labels linear cost metric one interesting open question find good approximations case edge weights f1 0 1g equivalently edges labeled g necessarily fullyconnected without considering 0edges half mistake context still easy cluster perfect clustering exists simple strategy works removing edges producing connected component resulting graph cluster random case also easy defined appropriately however approximation techniques appear go know achieve constantfactor even logarithmic factor approximation minimizing disagree ments note still use divide choose algorithm achieve additive approximation n 2 number agreements however imply ptas maximizing agreements opt might generalization problem consider unbounded edge weights lying 11 example edge weights might correspond log odds two documents belonging cluster number disagreements could defined total weight positive edges clusters negative edges inside clusters agreements defined analogously know good algorithm approximating number disagreements case believe problem maximizing agreements apxhard generalization able prove show however ptas would give n approximation algorithm kcoloring constant k 4 incomplete model seems hard problem original problem fully connected f g graph another question whether one approximate correlation number agreements minus number disagreements easy show opt must n measure know good approx imation would also good improve currently fairly large constant approximating disagreements r efficient testing large graphs probabilistic method new rounding procedure assignment problem applications dense graph arrangements polynomial time approximation schemes dense instances nphard prob lems correlation clustering httpwww agnostic boost ing improved combinatorial algorithms facility location kmedian problems personal communication learning match cluster entity names learning match cluster large highdimensional data sets data integration algorithms graph partitioning planted partition model property testing connection learning approximation unified approach approximation algorithms bottleneck problems efficient noisetolerant learning statistical queries toward efficient agnostic learning spectral partitioning random graphs testing diameter graphs clustering edgecost minimization tr unified approach approximation algorithms bottleneck problems toward efficient agnostic learning maxcut randomized approximation scheme dense graphs myampersandotildeitalicnitalicsupscrpt314supscrptcoloring algorithm 3colorable graphs property testing connection learning approximation efficient noisetolerant learning statistical queries polynomial time approximation schemes dense instances inlineequation f scnpscf inlineequationhard problems clustering edgecost minimization extended abstract algorithms graph partitioning planted partition model approximation algorithms metric facility location italickitalicmedian problems using primaldual schema lagrangian relaxation computers intractability guide theory npcompleteness testing diameter graphs improved algorithms random cluster graph model agnostic boosting learning match cluster large highdimensional data sets data integration improved combinatorial algorithms facility location kmedian problems spectral partitioning random graphs clustering qualitative information correlation clustering ctr moses charikar venkatesan guruswami anthony wirth clustering qualitative information journal computer system sciences v71 n3 p360383 october 2005 anders dessmark jesper jansson andrzej lingas evamarta lundell mia persson approximability maximum minimum edge clique partition problems proceedings 12th computing australasian theroy symposium p101105 january 1619 2006 hobart australia thorsten joachims john hopcroft error bounds correlation clustering proceedings 22nd international conference machine learning p385392 august 0711 2005 bonn germany thomas finley thorsten joachims supervised clustering support vector machines proceedings 22nd international conference machine learning p217224 august 0711 2005 bonn germany aristides gionis heikki mannila panayiotis tsaparas clustering aggregation acm transactions knowledge discovery data tkdd v1 n1 p4es march 2007 shai avidan yael moses yoram moses centralized distributed multiview correspondence international journal computer vision v71 n1 p4969 january 2007 surajit chaudhuri anish das sarma venkatesh ganti raghav kaushik leveraging aggregate constraints deduplication proceedings 2007 acm sigmod international conference management data june 1114 2007 beijing china ioannis giotis venkatesan guruswami correlation clustering fixed number clusters proceedings seventeenth annual acmsiam symposium discrete algorithm p11671176 january 2226 2006 miami florida nir ailon moses charikar alantha newman aggregating inconsistent information ranking clustering proceedings thirtyseventh annual acm symposium theory computing may 2224 2005 baltimore md usa yigal bejerano mark smith joseph naor nicole immorlica efficient location area planning personal communication systems ieeeacm transactions networking ton v14 n2 p438450 april 2006