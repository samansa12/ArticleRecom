communication lower bounds distributedmemory matrix multiplication present lower bounds amount communication matrix multiplication algorithms must perform distributedmemory parallel computer denote number processors ipi dimension square matrices ini show widely used class algorithms socalled twodimensional 2d algorithms optimal sense algorithm uses ioiinisup2supipi words memory per processor least one processor must send receive inisup2supipisup12sup words also show algorithms another class socalled threedimensional 3d algorithms also optimal algorithms use replication reduce communication show algorithm uses ioiinisup2supipisup23sup words memory per processor least one processor must send receive inisup2supipisup23sup words furthermore show continuous tradeoff size local memories amount communication must performed 2d 3d bounds essentially instantiations tradeoff also show input distributed across local memories multiple nodes without replication inisup2sup words must cross bisection cut machine bounds apply conventional inisup3sup algorithms apply strassens algorithm ioiinisup3sup algorithms b introduction although communication bottleneck many computations running distributedmemory parallel computers clusters workstations servers communication lower bounds proved know great deal amount communication specic algorithms perform know little much communication must perform present lower bounds amount communication required multiply matrices conventional algorithm distributedmemory parallel computer analysis uses unied framework also applies analysis capacity cache misses sequential matrixmultiplication algorithm use simple yet realistic computational model prove lower bounds model parallel computer collection p processormemory nodes connected communication network memory distributed among nodes communicate across network analysis bounds number words must sent received least one nodes bounds apply even processormemory node includes several processors fairly common today machines ranging clusters dualprocessor workstations sgi origin 2000 ibm sp aggarwal chandra snir 5 presented lower bounds amount communication matrix multiplication number computations bounds however assume sharedmemory computational model model well existing computers lpram model assumes p processors private cache connected large shared memory furthermore assumed computation begins caches empty computation ends output must returned main memory analysis bounds amount data must transferred shared main memory private caches bound matrix multiplication essentially quanties number compulsory capacity cache misses sharedmemory multiprocessor lpram model model well systems memory physically distributed among processing nodes eg clusters worksta tions parallel computers sgi origin 2000 ibm sp distinction lpram model model highly relevant matrix multiplication lower bounds matrix multiplication nearly always subroutine larger computation distributed memory machine mul tiplicands already distributed manner matrix multiplication subroutine called product must left distributed processors local memories returns thus lpram bound essentially shows processor must access irrelevant distributedmemory machines since elements may already reside processors mem ory lpram lower bound depend amount local memory data allowed stored perhaps replicated local memories computation begins ends communication may necessary contrast lower bounds allow initial data distribution input trices including data distributions replicate input elements 3d algorithms lower bounds even count communication necessary perform replication input elements except section 6 explicitly forbid replication analyze communication across bisection machine lower bounds hold even data allowed replicated prior invocation algorithm bounds also allow distribution use standard asymptotic notation paper specically use denitions 10 gn set functions fn exist positive constants c 1 c 2 0 c 1 gn fn c 2 gn n n 0 ogn dened similarly using weaker condition 0 fn c 2 gn gn dened condition 0 c 1 gn fn set ogn consists functions fn c 2 0 exists constant n 0 0 0 fn c 2 gn n n 0 communication lower bounds matrix multiplication 5 output matrix c constraint place algorithm upon completion every element c must reside processors local memory possibly several state prove lower bounds paper using concrete constants rather asymptotic notation since bounds amount communication depends three parameters size matrices n number processors p size local memory asymptotic notation makes less clear parameters must grow order function show asymptotic behavior also use concrete constants claries dependence amount communication three parameters constants appear statement lemmas theorems however chosen make proofs simple possible chosen tight possible bounds assume matrices involved square whereas others apply matrices shape restrict matrices square shape feel bounds rectangular matrices would complicate statement results proofs unnecessarily analysis applies conventional matrix multiplication algorithms apply strassens algorithm 32 3 algorithms lower bounds communication complexity strassens nonconventional algorithms beyond scope paper rest paper organized follows section 2 presents technical tool underlies unied approach communication cachetrac lower bounds section 3 presents basic memorycommunication tradeo shows lack memory increases communication section 4 uses provable tradeo analyze socalled twodimensional 2d matrixmultiplication algorithms 1 9 12 13 37 algorithms use n 2 p words memory per processor constant factor required store input output algorithms use 3 words per processor 6 dror irony sivan toledo alexander tiskin minimal amount required store input output without compres sion show amount communication perform per processor asymptotically optimal amount memory section 5 uses sophisticated argument show socalled threedimensional 3d algorithms also optimal 3d algorithms 1 5 6 12 14 19 replicate input matrices need n words memory per processor allows reduce amount communication n 2 p 23 per processor show amount communication optimal amount memory used argument case somewhat complex since continuous tradeo prove section 3 apply amount local memory per processor n 2 2p 23 section 6 prove replication input elements allowed must cross bisection machine finally section 7 use basic lemma underlies results prove wellknown lower bound number cache misses sometimes referred page faults ios lowerbounds literature main point section 7 show kinds lower bounds derived using unied approach io communication lower bounds related 2 basic lemma section presents technical lemma underlies lower bounds paper lemma shows processor accesses n elements n elements b contributes computation n elements product perform 32 useful arithmetic operations hong kung 17 proved weaker form lemma lemma considers access elements b contributions elements c weak used proofs distributedmemory lower bounds communication lower bounds matrix multiplication 7 also hong kung stated result using asymptotic notation whereas state prove using concrete constants formally proved following lemma dene formal matrixmultiplication model hong kung used similar directedacyclicgraph model lemma 21 hong kung 17 consider conventional matrix multiplication c r processor local memory cache size words must read write least words secondary memory compute product state prove lemma must dene precisely kind algorithms applies informally want deal algorithms use element addition multiplication compute element c ik explicit sum products ij b jk thus rule eg strassens algorithm 32 saves computation using element subtraction furthermore must assume computation involving values ij b jk c ik taking place thus rule eg boolean matrix multiplication algorithm 33 compute explicit sums still saves communication memory using intermediate compact representation matrices definition 21 conventional matrix multiplication b n r c r dened directed acyclic graph dag mn nr input nodes representing elements ij b jk matrices b mr output nodes representing elements c ik matrix c mnr computation nodes representing elementary multiplications v arcs represent data dependencies particular input nodes unbounded outdegree corresponding replication inputs output nodes unbounded indegree corresponding combining partial sums outputs thus denition covers whole class algorithms may perform replication combining dierent order ready prove basic lemma note holds matrices shape long shapes allow multiplication lemma 22 consider conventional matrix multiplication mn b n r c r processor contributes nc elements c accesses na elements nb elements b perform multiplications proof lemma immediate corollary discrete loomiswhitney inequality 26 15 8 relates cardinality nite set z cardinalities ddimensional orthogonal projections 1 application discrete loomiswhitney inequality matrixmultiplication lower bounds suggested paterson 29 see also 34 2 let v nite set points z 3 let va vb vc orthogonal projections v onto coordinate planes discrete loomiswhitney inequality states notation number elementary multiplications performed processor therefore nanbnc 12 lemma 22 main tool proving communication lower bounds fact important would like restate dierent form new communication lower bounds matrix multiplication 9 version slightly weaker lemma 22 allows direct proof feel may benet reader lemma 23 assumptions lemma 22 processor perform min elementary multiplications proof statement follows lemma 22 arithmeticgeometric mean inequality give alternative direct proof based idea similar 17 denote sa set elements index pairs processor accesses sb set elements b processor accesses sc set elements c processor contributes rst show nb nc n 12 bounds number elementary multipli cations partition rows two sets set contains rows least n 12 elements sa fa contains rest rows n 12 rows since row c product corresponding row b nbn 12 elementary multiplications involving rows since element c product row column b since row fa less n 12 elements sa ncn 12 elementary multiplications involving rows fa similar argument shows na nc n 12 b bounds number elementary multiplications partition columns b set mb consisting columns least n 12 b elements sb set fb containing rest columns since column c product corresponding column b nan 12 elementary multiplications involving columns mb since element c product row column b ncn 12 elementary multiplications involving rows fb finally show na nb n 12 c bounds number elementary mul tiplications partition rows c sets mc fc row c product row b c elementary multiplications involving rows mc element used computation elements one row c row c contains less n 12 c elements sc element us used less n 12 cations hence number elementary multiplications involving rows c fc less nan 12 c 3 memorycommunication tradeoff section prove tradeo memory communication matrix multiplication algorithms analysis shows reducing amount memory forces algorithm perform communication shall use provable tradeo next section prove 2d matrixmultiplication algorithms asymptotically optimal amount memory use algorithms use little extra memory beyond storage required store matri ces hence lie extreme end memorycommunication tradeo section 5 shall extend tradeo deal larger memory sizes enable us prove 3d algorithms also asymptotically optimal amount memory use lemma 31 consider conventional matrix multiplication mn b n r c r p processor distributedmemory parallel communication lower bounds matrix multiplication 11 computer consider processor words local memory performs elementary multiplications processor must send receive least wp words proof decompose schedule computation processor phases phase begins total number words sent received far processor exactly thus phase except perhaps last phase processor send receives exactly words number na elements processor may access phase 2m since one elements must reside processors memory phase begins else must received another processor phase argument shows nb 2m dene element c ik product c live phase 1 processor computes ij b jk j phase 2 partial sum containing ij b jk either resides processors memory end phase sent another processor phase number nc live elements c phase 2m since live element either uses one word memory end phase sent another processor lemma 22 shows number elementary multiplications phase nanbnc 12 2 total number elementary multiplications algorithm w therefore number full phases phases exactly words sent received least wp wp total amount communication least wp wp concludes proof note proof allows input values ij b jk pre distributed multiple copies dierent processors memories free input replica tion also ignores need collect add together individual processors contributions output value c ik free output combining also note lower bound degenerates zero w 23 2 inevitable since communication indeed zero eg inputs outputs local memory generally communication required input replication output com bining accounted proof however latter case 3w 23 therefore threshold constant factor away best achievable lemma proved concentrates single processor next theorem takes global view running time typically determined heavily loaded processor therefore showing least one processor must perform lot communication derive lower bound amount time whole algorithm must spend communication theorem 31 memorycommunication tradeoff consider conventional matrix multiplication c r p processor distributedmemory parallel computer words local memory per processor total number words sent received least one processor least proof least one processors must perform mnrp multiplications result follows applying lemma 31 communication lower bounds matrix multiplication 13 lower bound degenerates zero eg 4 communication lower bound almostinplace algorithms among many existing parallel matrix multiplication algorithms recent algorithm mccoll tiskin 27 knowledge one whose performance matches asymptotically lower bound memorycommunication tradeo value however two classes earlier algorithms match lower bounds specic values one end spectrum socalled 2d algorithms use little extra memory beyond required store matrices end spectrum socalled 3d algorithms use extra memory replicate input matrices reduce com munication specialize theorem 31 2d algorithms section 5 analyzes algorithms use extra memory rst distributedmemory parallel matrixmultiplication algorithm probably one due cannon 9 cannon originally proposed algorithm parallel computer connected twodimensional mesh generalization cannons algorithm larger blockdistributed matrices due dekel nassimi sahni 12 algorithm also generalized hypercubes interconnection topologies fox otto hey 13 describe dierent algorithm unlike cannons algorithm uses broadcasts another 2d algorithm proposed agarwal gustavson zubair 3 independently almost simultaneously van de geijn watts 37 algorithm called summa uses many broadcasts relatively small matrix pieces allows broadcasts pipelined occur concurrently computation storage required 2d algorithms proportional size matrices square matrices amount memory per processor proportional n 2 p clearly 3n 2 p words per processor necessary store 14 dror irony sivan toledo alexander tiskin multiplicands product 2d algorithms eg summa need storage beyond storage required matrices simpler 2d algorithms blocked implementations cannons algorithm require additional words per processor order store two blocks two blocks b reduced 2 p breaking communication phase many small messages possibly resulting increased communication overhead next theorem shows 2d algorithms asymptotically optimal amount communication per processor algorithm uses words memory per processor must perform per processor order keep theorem simple state prove square matrices theorem 41 2d communication lower bound consider conventional multiplication two n n matrices p processor distributedmemory parallel computer processor n 2 p words local memory least one processors must send receive least words proof theorem 31 amount communication least one processors bounded communication lower bounds matrix multiplication 15 inequality relies fact p 12 4 5 extending tradeoff n 2 2p 23 tradeo section 3 fails provide meaningful bounds n 2 2p 23 regime may even single full phase sense proof lemma 31 since proof lemma 31 take account either amount communication last phase communication necessary input replication output combining provides useful bound case section analyzes amount communication must performed including communication necessary output combining still input replication show regime amount communication per processor bound matches asymptotically upper bounds achieved 3d algorithms using input replication algorithms reduce communication traditional 2d algorithms although dekel nassimi sahni 12 perhaps rst propose 3d algorithms algorithms communicationecient total number words communicated n 3 communicationecient 3d algorithms rst proposed berntsen 6 aggarwal chandra snir 5 time berntsens paper submitted publication 1988 paper aggarwal chandra snir presented conference 1988 essentially algorithm 5 proposed later independently gupta kumar 14 johnsson 19 berntsens algorithm somewhat complex rest aggarwal chandra snir also prove amount communication per processor matrix multiplication algorithms must perform sharedmemory parallel computer explained introduction bound apply matrix multiplication distributed memory machine bound relies private caches processors empty computation begins contrast distributedmemory machine matrices typically already distributed matrix multiplication subroutine invoked main result section hinges following lemma lemma 51 consider conventional multiplication two n n matrices p processor distributedmemory parallel computer consider processor words local memory performs least n 3 p elementary multipli cations let processor must send receive least words proof na theorem statement holds since elements reside processors local memory computation begins rest must received processors argument holds nb theorem statement holds since processor must send contributions least n 2 p 23 elements c processors na nb nc less claim three quantities must greater communication lower bounds matrix multiplication 17 let w number multiplications processor performs using lemma 22 notation 1 1 fact na nb less therefore identical argument shows expression 2 also bounds na nb number elements c processor compute without contributions processors small every c ik processor must access entire ith row entire kth column b na nb less processor compute n n elements c suppose processor participates computation c ik compute c ik resides processor end computation processor must received contribution c ik least one processor c ik resides another processor processor must sent contribution processor either way one word data must either received sent c ik therefore processor must send receive least words participate computation elements c subtracting 3 2 hypotheses p rst two inequalities true since third holds since p 128 implies p 23 turn implies shows number words must sent received least claimed state main result section proof essentially theorem 31 omitted theorem 51 3d communication lower bound consider conventional multiplication two n n matrices p processor distributedmemory parallel computer processor n 2 p 23 words local memory least one processor must send receive least min words communication lower bounds matrix multiplication 19 6 bisectionbandwidth bounds section analyzes amount data must cross bisection distributedmemory parallel computer partition memoryprocessor nodes two subsets establish lower bound amount data must cross cut separates subsets communication network computer assume element input matrices stored exactly distributed memory machine input evenly split subsets indeed allow replication input matrices output computed without communication across bisection example 3d algorithms perform communication across bisection cuts network following initial data replication phase another way derive lower bounds communication across cuts apply lower bounds section 4 5 groups processors bounds also bound amount communication group p processors must perform p p processors machine communication must transmitted edge cut communication network two processor groups hence bound amount communication must traverse cuts network technique however unlikely provide useful bounds large p say general memorycommunication tradeo degenerates specialized bounds theorems 41 51 hold large p therefore need specic bound communication across bisection cuts theorem assumes matrices b evenly distributed proof easily modied show asymptotic bounds also apply two processor subsets initially stores n 2 elements n 2 elements b constant 1 2 1 2 complex proof would required theorem 61 consider conventional multiplication two n n matrices p processor distributedmemory parallel computer input element initially stored local memory exactly one processor least words must transferred across cut splits input distribution multiplicands b evenly proof let consider cut communication network splits nodes two subsets holding exactly n 2 2 elements n 2 2 elements b n 2 elements n 2 elements b transferred across cut theorem statement holds otherwise claim subset machine compute elements c summing products ij b jk computed locally computing element requires access entire row entire column b n 2 elements b cross cut subset access n rows columns b compute elements c hence communication lower bounds matrix multiplication 21 elements c must computed two subsets machine together means least many words must cross cut since 13 34 theorem statement holds 7 io lower bound section use lemma 22 bound number compulsory capacity cache misses matrix multiplication establish lower bound number words must transferred slow memory fast cache arithmetic instructions access data cache results new show proof technique use parallel communication bounds applied analysis cache misses specically bounds prove asymptotically proved hong kung 17 toledo 36 bounds however specify constants unlike hong kungs result stated using asymptotic notation constants slightly stronger given 36 proof technique similar eect using toledos proof technique use lemma 22 simplies structure proof lower bounds use lax memorysystem model equivalent hong kungs redblue pebble game therefore apply cache organiza tion lower bounds also match asymptotically performance recursive matrix multiplication blocked matrix multiplication assuming block size chosen appropriately cache fully associative uses lru replacement policy matrix multiplication algorithms whose asymptotic performance matches lower bound old computers rutledge rubinstein 31 30 described library blocked matrix subroutines designed together herbert f mitchell implemented univac rstgeneration computer 22 dror irony sivan toledo alexander tiskin became operational 1952 mckeller coman 28 provided rst rigorous analysis data reuse matrix computations including matrix multipli cations showed blocked algorithms transferred fewer words fast slow memory algorithms operated row column highquality implementations ioecient matrixmultiplication algorithms widely available frequently used 4 2 7 11 16 20 23 21 22 24 25 38 proof next theorem similar proof lemma 31 theorem 71 consider conventional multiplication two n n matrices computer large slow memory fast cache contain words arithmetic operations performed words cache number words moved slow memory fast cache least proof decompose schedule computation phases phase begins total number words moved memory cache exactly thus phase except perhaps last phase exactly words transferred memory cache number na elements processor may operate upon phase 2m since one elements must either reside cache phase begins must read cache phase argument shows nb 2m dene element c ik product c live phase processor computes ij b jk k phase partial sum containing ij b jk either resides cache end phase written slow memory phase number nc live elements c phase communication lower bounds matrix multiplication 23 2m since live element either uses one word cache end phase written slow memory lemma 22 shows number elementary multiplications phase nanbnc 12 2 total number elementary multiplications algorithm n 3 therefore number full phases phases exactly words transferred least total number words transferred least concludes proof next corollary shows matrices several times larger cache number cache misses proportional n 3 12 constants corollary essentially example picking stronger bound size matrices relative cache could proved stronger bound number cache misses corollary 71 io lower bound 17 conditions theorem 71 assuming n number words must transferred cache least proof n 32 number words must transferred least n 2pd r f february 2 2004 8 conclusions presented rigorous lower bounds amount communication distributedmemory parallel matrix multiplication algorithms must perform bounds hold directedacyclicgraph dag model conventional matrix multiplication nary summation dag allows summation order bounds hold 3 matrix multiplication algorithms strassens hold cases arbitrary computation matrix elements allowed even elementary products computed explicitly main signicance results rigorously validate algorithmdesigners long suspected 2d algorithms asymptotically optimal little replication allowed 3d algorithms asymptotically optimal replication allowed therefore search ecient algorithms must concentrate improving constants involved eorts design nonconventional algorithms specic domains use compression nondag techniques another important conclusion research 3d algorithms somewhat unlikely become competitive bounds show asymptotic reduction communication relative 2d algorithms cannot exceed factor p 16 typical machine sizes value unlikely high enough make replication protable note although authors found 3d algorithms faster 2d algorithms practice 1 nearly widely used existing communication lower bounds matrix multiplication 25 libraries implement 2d algorithms also note eort rst two authors design 3d factorization algorithm suggested constants 3d algorithm high constants higher triangular factorization matrix multiplication 18 acknowledgments thanks two anonymous referees helpful comments suggestions sivan toledo supported part ibm faculty partnership award grants 57200 906099 israel science foundation founded israel academy sciences humanities r threedimensional approach parallel matrix multiplication exploiting functional parallelism power2 design highperformance numerical algorithms improving performance linear algebra algorithms dense matrices using algorithmic prefetch communication complexity prams communication ecient matrix multiplication hypercubes optimizing matrix multiply using phipac portable geometric inequalities cellular computer implement kalman parallel matrix graph algorithms matrix algorithms hypercube scalability matrix multiplication algorithms parallel computers performance intel tflops supercomputer io complexity redblue pebble game trading replication communication parallel distributedmemory dense solvers minimizing communication time matrix multiplication mul tiprocessors local basic linear algebra subroutines lblas connection machine system cm200 local basic linear algebra subroutines lblas cm55e inequality related isoperimetric inequality private communication high order matrix computation univac matrix algebra programs univac gaussian elimination optimal bulksynchronous parallel random access machine survey outofcore algorithms numerical linear algebra robert van de geijn jerrell watts automatically tuned linear algebra software tr communication complexity prams minimizing communication time matrix multiplication multiprocessors improving performance linear algebra algorithms dense matrices using algorithmic prefetch exploiting functional parallelism power2 design highperformance numerical algorithms highperformance matrixmultiplication algorithm distributedmemory parallel computer using overlapped communication threedimensional approach parallel matrix multiplication optimizing matrix multiply using phipac bulksynchronous parallel random access machine survey outofcore algorithms numerical linear algebra organizing matrices matrix operations paged memory systems introduction algorithms high order matrix computations univac bulksynchronous parallel multiplication boolean matrices io complexity automatically tuned linear algebra software cellular computer implement kalman filter algorithm