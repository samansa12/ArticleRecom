bayesian mars bayesian approach multivariate adaptive regression spline mars fitting friedman 1991 proposed takes form probability distribution space possible mars models explored using reversible jump markov chain monte carlo methods green 1995 generated sample mars models produced shown good predictive power averaged allows easy interpretation relative importance predictors overall fit b introduction common problem statistics disciplines approximate adequately function several variables statistical setting known multiple regression task performed either parametrically global modelling eg linear regression nonparametrically see examples methods aim model dependence response variable one predictor variables data given data assumed come relationship described f unknown regression function wish estimate ffl zeromean error distribution commonly assumed gaus sian domain interest usually taken convex hull defined predictor variables regression function f gives predictive relationship x ie conditional expectation given x thus may use f predict future values previously unseen points domain aim regression analysis use data construct estimate b f x true regression function serve reasonable approximation domain interest many methods exist model function interest f eg additive models hastie tibshirani 1990 cart breiman et al 1984 projection pursuit regression friedman stuetzle 1981 alternating conditional expectation breiman friedman 1985 however concentrate multivariate adaptive regression spline mars methodology proposed friedman 1991 method seems highly flexible easily interpretable motivated recursive partitioning approach regression morgan sonquist 1963 breiman et al 1984 produces continuous model made continuous derivatives greater flexibility model relationships nearly additive involve variables model represented way additive contributions predictor variable interactions variables easily identified helps identify variables important model highlight progression recursive partition regression mars start giving partition regression model suitably chosen coefficients basis functions b k number basis functions model basis functions b indicator function one argument true zero elsewhere r partition usual mars model given 1 except basis functions different instead b given 2 degree interaction basis b ji shall call sign indicators equal sigma1 vj give index predictor variable split ji known knot points give position splits vj delta j constrained distinct predictor appears interaction term see section 3 friedman 1991 comprehensive illustration model illustrate initially confusing notation present example suppose mars model contains basis function b given immediately see two factors interaction term 2 sign indictors knot points given gamma37 labels predictors split mars model continuous made continuous first derivatives replacing truncated linear basis functions b truncated cubic basis functions effect rounding basis function split points mars algorithm proceeds follows forward stepwise search basis functions takes place constant basis function one present initially step split minimises lackoffit criterion possible splits basis function chosen splits permissable marginal predictor values split predictor x corresponds two new basis func tions henceforth referred note unlike recursive partitioning algorithm basis function split made removed continues model reaches predetermined maximum number basis functions twice number expected model aid subsequent backwards stepwise deletion basis functions involves removing basis functions one time lackoffit criterion minimum basis improves fit degrades least removed step finally resulting model made continuous first derivative rounding split points mentioned lackoffit measure used friedman 1991 generalised crossvalidation criterion originally proposed craven wahba 1979 aim paper provide bayesian algorithm mimics mars procedure done considering number basis functions along type see section 21 coefficients form positions split points sign indicators random treat additional parameters problem make inference using data problem routine calculation posterior distribution models addressed designing suitable markov chain monte carlo mcmc reversible jump simulation algorithm set green 1995 simulated sample contains many different mars models corresponding posterior weights estimate f high predictive power required pointwise averaging models sample suggested work extension bayesian approach curve fitting one dimension given denison et al 1998b related bayesian cart algorithms proposed denison et al 1998a chipman et al 1998 section 2 outline bayesian mars method show examples using method simulated data section 3 real data section 4 section 5 contains discussion proposed methodology 2 bayesian mars method 21 basic ideas must first define mean type basis function using notation 1 2 consider basis functions type identical permutation hence predictor variables n different types basis function n given note sum include constant basis function term would equal 0 3 basis b 1 always sole constant basis function model cannot chosen candidate basis frequently maximum order interaction assigned model j case sum 3 runs 1 let type basis function b thus effect tells us predictor variables splitting ie values v1 example suppose problem two predictors 2 types basis functions could model including constant one sigmax say type 2 sigmax say type 3 basis functions split predictor x 2 types equal 2 propose model used set probability distribution space possible mars structures mars model uniquely defined number basis functions present coefficients types basis functions together knot points sign indicators associated interaction term means make k random j uniquely defined via change dimension model change k bayesian computation use reversible jump mcmc approach green 1995 richardson green 1997 considering changes number basis functions model inference carried assuming true model unknown comes class models denotes model exactly k basis functions overall parameter space theta written countable union subspaces subspace euclidean space r nk r nk denotes dimensional parameter space corresponding model k b 21j dimensional vector corresponds basis function b natural hierarchical structure setup denoting generic element theta k k data vector formalise modelling joint distribution k product model probability parameter prior likelihood bayesian inference k k based joint posterior shall explore summarise regarding target distribution tailored mcmc computations often useful consider factorised form generate samples joint posterior k k using class reversible jump metropolishastings algorithms green 1995 full details method found reference cited focus essence methodology particular forms algorithms current context 22 bayesian model assume ffl 1 follows n0 oe 2 distribution oe 2 unknown result extend parameter vector include new unknown leads loglikelihood model l k jy given l k gamman log oe gamma2oe 2 f form given 1 2 use vague proper prior variance error distribution ie oe gamma2 assumed uniformly distributed ng sign indicators ji knot points ji also assumed uniform sets fgamma1 1g ng respectively use another vague proper prior coefficients basis functions assume n0 2 take variance priors may chosen differently formulation used let data dictate form model leads proper posterior distribution priors proper particular prior type basis functions could chosen carefully priori main effects favoured interaction terms could useful interpretation model rather prediction important refinement used mallick et al 1997 1998 concentrate situations interpretability model great interest poisson distribution parameter used specify prior probabilities number basis functions giving practice poisson distribution truncated k k max suitable choice k max adopted however help sampler mix better limit prior influence put gamma hyperprior parameters equal 10 reflects knowledge expect basis functions fit data well controls overfitting 23 computational strategy aim simulate samples joint posterior distribution p since analytic numerical analyses totally intractable situation purpose design reversible jump algorithm general type discussed green 1995 reader referred details context problem multiple parameter subspaces different dimensionality necessary devise different types moves subspaces combined form tierney 1994 calls hybrid sampler making random choice available moves transition order traverse freely around combined parameter space use following move types change knot location b addition basis function c deletion basis function note steps b c changing dimension model add basis functions pairs standard mars forwardstepwise procedure fact depart completely sort recursive partitioning approach found adding basis functions singly makes procedure flexible reversibility condition easier satisfy change mars structures described coefficients basis functions new model little relationship current model inference difficult lead labelling problems richardson green 1997 instead choose integrate coefficients error variance oe 2 parameter vector k contains model param eters straightforward chose use conjugate priors coefficients error variance however make predictions using generated sample models draw coefficients model sample full conditional distributions given model parameters given current model step straightforward first pick basis uniformly random pick one factors alter current knot location ji probability 1reverse sign indicator choose new knot location marginal predictor values variable x vji set new ji move type undertaken using metropolis step metropolis et al 1953 hastings 1970 accept reject proposed new state addition basis function birth step b carried choosing uniformly type basis function say add model uniformly choose knot location sign indicator factors new basis step c death deletion basis function constructed way make jump step reversible easily done choosing basis function uniformly present except constant basis removing end iteration move step performed use gibbs steps gelfand smith 1990 generate new straightforward full conditional simple calculate given full cycle algorithm obtain sample k k 24 algorithm reversible jump algorithm use three move types described write set moves refers changing knot location refers increasing number terminal nodes m1 decreasing m1 independent move types randomly chosen probabilities ae k k k problem took b constant c parameter sampler taken 04 find marginalising coefficients error variance acceptance probability given green 1995 simplifies denotes current model parameters 0 proposed model pa rameters probability proposing move 0 data thus acceptance probability bayes factor kass multiplied prior proposal odds terms approach common many fixed dimensional parameter inference problems recently used contexts unknown number parameters chipman et al 1998 holmes mallick 1997 use conjugate prior distributions allows simple evaluation integrals bayes factor term demonstrated ohagan 1994 leads bayes factor given 0 refers proposed model identity matrix note x refer usual design data matrices current regression b bayesian least squares regression estimates coefficients constants parameters gamma prior distribution oe gamma2 section 22 taken 001 note ordinary least squares approach estimating regression coefficients undertaken denison et al 1998ab equivalent method using reference priors coefficients error variance show prior proposal odds terms calculated using birth step example birth step b adds basis b k1 currently k basis functions model prior odds given prior functions k1 prior k basis functions k terms numerator 6 given prior number basis functions prior type basis functions priors knot positions sign indicators similarly denominator prior probability set bases k model thought probability choosing items set n ordering matter constant basis function fixed need chosen prior model parameters product probability factor certain sign indicator ie2 certain knot point power number factor terms model corresponding proposal odds given ppropose death ppropose birth k1 k propose death randomly choosing one basis functions including constant one birth randomly choosing type basis function probability 1n together sign indicator knot point factor new basis fn2ng gammaj k1 hence using equations 45 78 find acceptance probability birth step acceptance probability death step worked similarly algorithm use straightforward works quickly birth step described detail appendix steps similar algorithm 1 start constant basis function present 2 set k equal number basis function current structure 3 generate u uniformly 01 4 goto move type determined u ffl else b ffl else goto change step 5 draw using gibbs steps 6 repeat 2 suitable number iterations evidence convergence 3 simulated examples 31 bivariate predictors first test methodology examples given hwang et al 1994 studied roosen hastie 1994 following approaches generate 225 pairs predictors uniformly unit square response fx true value test function ffl drawn n0 025 2 distribution test functions ffl simple interaction function ffl radial function f 2 ffl harmonic function ffl additive function ffl complicated interaction function functions scaled translated standard deviation one nonnegative range use fraction variance unexplained test fits models data given fx true value function b fitted value f mean true values use fvu helpful comparing fits model differently generated datasets evaluate fvu fit replace expectations averages test set 10000 points bivariate examples simply 100 100 grid unit square ie 1200 199200 higher dimensional examples follows test set composed 10000 random uniform values domain interest training set calculate fvu training sample treating observed values fx took results last 30000 iterations sampler initial burnin period long enough convergence occurred end convergence assumed mean squared error fit settled time similar curve fitting algorithm denison et al 1998b table 1 display fvu training set data test set standard mars algorithm together number basis functions found bmars model give average fvu posterior mean model obtained pointwise averaging 10 runs algorithm display average number basis functions samples produced 10 repetitions algorithm standard mars models referred lmars piecewiselinear mars cmars piecewise cubic mars give results mars models table 1 shows bmars model gives comparable often better results lmars cmars wide variety examples also average number basis functions found bmars commonly less number found using standard mars true surfaces examples shown fig 1 corresponding bmars estimates given fig 2 figs 1 2 table 32 high dimensional predictors take example friedman et al 1983 basic function following friedman et al generate 200 random uniform predictors sixdimensional unit hypercube take response fx ffl independent identically distributed n0 1 random variables extra predictor spurious affect response friedman 1991 also uses function paper tendimensional predictor used five spurious sample size reduced 100 commonly case friedman 1991 allow twoway interactions mars models use higherorder interaction terms generally improve fit make model unnecessarily complex even though could incorporated required table 2 display fvu training test set examples using standard mars bmars methods took results last 30000 iterations 10 runs algorithm initial burnin period results shown demonstrate bmars model parsimoniously fits data compared standard mars fits table 4 real data example illustrate methodolgy using real dataset use data study bruntz et al 1974 dependence ozone meteorological variables 111 days may september 1973 sites new york metropolitan area cleveland et al 1988 work cube root ozone dataset known air available splus becker et al 1988 three predictor variables radiation temperature wind speed vastly differing ranges response predictors standardised data beforehand ie linearly transformed variable zero mean unit variance allow maineffect twoway interaction terms mars models mars fit 6 basis functions residual sum squares rss given 1841 linear approximation 2019 cubic one final model form fwindftemperaturefwind temperature fradiation temperature one basis function terms except final one two basis functions existed 5 runs algorithm using priors previous section average rss given bmars model 1832 406 basis functions lower rsss given mars model fewer basis functions shown fig 3 bmars estimate temperature wind plane smooth whereas piecewiselinear estimate using mars fig 4 bmars estimate also smaller rss less degrees freedom table 3 display estimated posterior probabilities possible terms models rss average number basis functions 5 runs immediately seen produced similar results suggests convergence occurred end burnin period also appears important basis functions fit main effect terms radiation temperature windtemperature interaction term borne fact model included terms referred easily largest posterior probability made well 50 generated samples use bmars tool performing stochastic search variable selection comes free performing analysis prediction variable selection difficult problem one received much attention literature bmars method could used similar way gibbs samplingbased method outlined george mcculloch 1993 shown identify good models using stochastic search procedure figs 3 4 table 3 presented bayesian approach finding regression surfaces uses truncated linear basis functions build surface use data find knot points main effect andor interaction terms number basis functions required adequately approximate required surface simulate random sample models using reversible jump mcmc approach green 1995 bayesian approach multiple regression produces model high predictive power due posterior averaging models leads good overall fit data also help combat overfitting problems however choose single model estimate picking single models sample various criteria models less predictive power interpretability easily produce anova decompositions produce expected numbers posterior probabilities basis functions posterior mean estimate used identify variables important fit applications may wish draw coefficients use mean sampling distributions instead identifying good models prediction main aim drawing coefficients slightly quicker produces models smaller squared error may useful related bmars algorithms analysing financial time series denison 1997 failure time data mallick et al 1997 approach denison et al 1998a undertake find good cart models via stochastic search example prediction using posterior mean helpful structure algorithm allows sampler move fluidly probability space means bayesian mars inherit problems bayesian cart algorithms denison et al 1998a chipman et al 1998 searching restricted portions entire space greatly helped splitting current basis functions thus inducing hierarchical structure form bases point demonstrated full chapter 5 denison 1997 space predictor variables substantial 50 say convergence sampler would seem unlikely still bmars approach yield good models bmars algorithm used paper written ansi c available together datasets world wide wed address httpmaicacukdgtd algorithm takes around 5 minutes run dec alpha workstation 200 datapoints acknowledgements work first author supported epsrc research stu dentship acknowledge helpful comments made anonymous referees associate editor mr cc holmes appendix move types change birth death given algorithm section 23 undertaken similarly describe birth step pseudocodethe notation follows section 2 1 uniformly choose type basis function add n possible ones 2 uniformly choose knot positions predictors split sign indicators new basis remembering predictor may occur basis function 3 generate u uniformly 01 4 work acceptance probability ff 5 accept proposed model else keep current model 6 return main algorithm r new language estimating optimal transformations multiple regression correlation discussion classification regression trees dependence ambient ozone solar radiation bayesian cart model search discussion smoothing noisy data spline func tions simulation based bayesian nonparametric regression methods multivariate adaptive regression splines dis cussion multidimensional additive spline approximation projection pursuit regression variable selection via gibbs sam pling reversible jump markov chain monte carlo computation bayesian model determination generalized additive models monte carlo sampling methods using markov chains applications bayesian wavelet networks nonparametric regression regression modeling backpropagation projection pursuit learn ing bayes factors generalized linear models bayesian perpective equations state calculations fast computing machines problems analysis survey data proposal bayesian analysis mixtures unknown number components discussion automatic smoothing spline projection pursuit markov chains exploring posterior distributions discussion figure 2 hwang figure 4 linearpiecewise left cubicpiecewise right mars estimates temperature tr ctr c c holmes b k mallick bayesian radial basis functions variable dimension neural computation v10 n5 p12171233 july 1 1998 david j nott anthony kuk hiep duc efficient sampling schemes bayesian mars models many predictors statistics computing v15 n2 p93101 april 2005 christophe andrieu nando de freitas arnaud doucet robust full bayesian learning radial basis networks neural computation v13 n10 p23592407 october 2001