pacbayesian stochastic model selection pacbayesian learning methods combine informative priors bayesian methods distributionfree pac guarantees stochastic model selection predicts class label stochastically sampling classifier according posterior distribution classifiers paper gives pacbayesian performance guarantee stochastic model selection superior analogous guarantees deterministic model selection guarantee stated terms training error stochastic classifier kldivergence posterior prior shown posterior optimizing performance guarantee gibbs distribution simpler posterior distributions also derived nearly optimal performance guarantees b introduction pacbayesian approach machine learning attempts combine advantages pac bayesian approaches 20 15 bayesian approach advantage using arbitrary domain knowledge form bayesian prior pac approach advantage one prove guarantees generalization error without assuming truth prior pacbayesian approach bases bias learning algorithm arbitrary prior distribution thus allowing incorporation domain knowledge yet provides guarantee generalization error independent truth prior pacbayesian approaches related structural risk minimization srm 11 interpret broadly describing learning algorithm optimizing tradeoff complexity structure prior probability concept model goodness fit description length likelihood training data interpretation srm bayesian algorithms select concept maximum posterior probability map algorithms viewed kind srm algorithm various approaches srm compared theoretically experimentally kearns et al 11 give experimental evidence bayesian mdl algorithms tend fit experimental settings bayesian assumptions fail pacbayesian approach uses prior distribution analogous used map mdl provides theoretical guarantee fitting independent truth prior perhaps simplest example pacbayesian theorem noted 15 consider countable class concepts f 1 f 2 f 3 concept f mapping set x twovalued set f0 1g let p arbitrary prior probability distribution functions let probability distribution pairs hx yi x 2 x 2 f0 1g assume relation p define fflf error rate f ie probability selecting hx yi according f x 6 let sample pairs drawn independently according define fraction pairs hx yi f x 6 fflf measure well f fits training data log 1 viewed description length concept f noted 15 simple combination chernoff union bounds yields probability least choice sample following f 2m 1 inequality justifies concept selection algorithm selects f f minimizing descriptionlength vs goodnessoffit tradeoff right hand side happens lowdescriptionlength concept fits well algorithm perform well however simple concepts fit poorly performance guarantee poor practice probabilities arranged concepts apriori viewed likely fit well given high probability domain specific knowledge used selecting distribution p precisely sense p analogous bayesian prior concept f likely fit well given high prior probability p f note however inequality 1 holds independent assumption relation distributions p formula 1 model selection algorithms select single model concept however model selection inferior model averaging certain applications example statistical language modeling speech recognition one smoothes trigram model bigram model smoothes bigram model unigram model smoothing essential minimizing cross entropy say model test corpus newspaper sentences turns smoothing statistical language modeling naturally formulated model averaging model selection smoothed language model large contains full trigram model full bigram model full unigram model parts one uses mdl select structure language model selecting model parameters maximum likelihood resulting structure much smaller smoothed trigram model furthermore mdl model performs quite badly smoothed trigram model theoretically derived compact representation bayesian mixture exponential number smaller suffix tree models 18 model averaging also applied decision trees produce probabilities leaves rather hard classifications common method constructing decision trees first build overly large tree fits training data prune tree way get smaller tree fit data 19 10 trees probabilities leaves alternative construct weighted mixture subtrees original fit tree possible construct concise representation weighting exponentially many different subtrees 3 17 9 paper stochastic model selection algorithms stochastically select model according posterior distribution models stochastic model selection seems intermediate model selection model averaging like model averaging based posterior distribution models uses distribution differently model averaging deterministically picks value favored majority models weighted posterior stochastic model selection stochastically picks single model according posterior distribution first main result paper bound performance stochastic model selection improves 1 stochastic model selection given better guarantees deterministic model selection intuitively model averaging perform even better stochastic model selection proving pac guarantee model averaging superior pac guarantees given stochastic model selection remains open problem paper also investigates nature posterior distribution providing best performance guarantee stochastic model selection shown optimal posterior gibbs distribution however also shown simpler posterior distributions nearly optimal section gives statements main results paper section 3 relates results previous work remaining sections present proofs 2 summary main results formula 1 applies countable class concepts turns guarantees stochastic model selection hold continuous classes well eg concepts realvalued parameters assume prior probability measure p possibly uncountable continuous concept class c sampling distribution possibly uncountable set instances x also assume measurable loss function l concept c instance x lc x 2 0 1 example might concepts predicates instances target concept c lc x define lc expectation sampling instance x lc x ie e xd lc x let range samples instances drawn independently according distribution define lc 1 x2s lc x q probability measure concepts lq denotes e cq lc lq denotes notation 8 signifies probability generation sample phis least 1 gamma ffi countable concept classes formula 1 generalizes follows loss function l lemma 1 mcallester98 probability distribution p countable rule class c following 8 discussed introduction leads learning algorithm selects concept c minimizing srm tradeoff right hand side inequality first main result paper generalization 1 uniform statement distributions arbitrary concept class new bound involves kullbackleibler divergence denoted dqjjp distribution q distribution p quantity dqjjp defined dp c following first main result paper proved section 4 theorem 1 probability distribution measure possibly uncountable set c measurable loss function l following q ranges distributions measures c 8 note definition lq namely e cq lc average loss stochastic model selection algorithm makes prediction first selecting c according distribution q interpret theorem 1 bound loss stochastic model selection algorithm using posterior q case countable concept class q concentrated single concept c quantity dqjjp equals c large theorem 1 essentially lemma 1 theorem 1 considerably stronger lemma 1 handles case uncountable continuous concept classes even countable classes theorem 1 lead better guarantee lemma 1 posterior q spread exponentially many different models similar empirical error rates might occur example mixtures decision trees constructed 3 17 9 second main result paper posterior distribution minimizing error rate bound given theorem 1 gibbs distribution value fi 0 define q fi posterior distribution defined follows z normalizing constant z posterior distribution q define bq follows second main result paper following theorem 2 c finite exists fi 0 q fi optimal ie bq fi bq q fi satisfies following unfortunately multiple local minima bq fi function fi even multiple local minima satisfying 2 fortunately simpler posterior distributions achieve nearly optimal performance simplify discussion consider parameterized concept classes concept specified parameter vector theta 2 r n let ltheta x loss concept named parameter vector theta data point x discussed simplify analysis assume given x ltheta x continuous function theta example might take theta coefficients nth order polynomial p theta take ltheta x max1 ffjp theta x gamma fxj fx fixed target function ff fixed parameter loss function note two valued loss function continuous function theta unless prediction independent theta consider sample consisting data points data points define empirical loss ltheta parameter vector theta empirical loss average finite number expressions form ltheta x hence ltheta must continuous function theta assuming prior theta given continuous density get exists continuous density p l empirical errors satisfying following p u denotes measure subset u concepts according prior measure concepts x second main result paper summarized following approximate equation bq denotes inf q bq approximate inequality justified two theorems stated stating formal theorems however interesting compare 3 lemma 1 countable concept class define c concept minimizing bound lemma 1 large lemma 1 interpreted follows clearly structural similarity 4 3 however two formulas fundamentally different 3 applies continuous concept densities 4 applies countable concept classes another contribution paper theorems giving upper lower bounds bq justifying 3 first give simple posterior distribution nearly achieves performance 3 define l follows define posterior distribution q l follows z normalizing constant following theorem theorem 3 prior probability measure concept class concept named vector theta 2 r n sample instances loss function ltheta x always interval 0 1 continuous theta prior theta continuous probability density r n density p l nondecreasing interval following assumptions used theorem 3 quite mild final assumption density p l nondecreasing interval defining q l justified fact definition l implies differentiable density function p l must density p l increasing finally show q l nearly optimal posterior theorem 4 prior probability measure concept class concept named vector theta 2 r n sample instances loss function ltheta x always interval 0 1 continuous theta prior theta continuous probability density r n following posterior q 3 related work model selection guarantee similar 1 given barron 1 assume concepts f 1 f 2 f 3 true empirical error rates fflf fflf 1 let f defined follows case error rates also known 01 loss barrons theorem reduces following several differences 1 5 discussing 1 take f concept f minimizing right hand side 1 nearly definition f 5 formula 1 implies following 8 note 5 bounds expectation fflf 1 large deviation result gives bound fflf function desired confidence level ffi also note 1 provides bound fflf terms information available sample 5 provides bound expectation fflf terms unknown quantities fflf means learning algorithm based 1 output performance guarantee along selected concept true even concept selected incomplete search concept space hence different f guarantee computed 5 bound terms unknown quantities fflf desired proof method used prove 1 yields following 8 also note 5 like 1 unlike theorem 1 vacuous continuous concept classes various model selection results similar 1 appeared literature guarantee involving index concept arbitrary given sequence concepts given 12 bound based index concept class sequence classes increasing vc dimension given 14 neither bounds handle arbitrary prior distribution concepts however give pac srm performance guarantees involving form prior knowledge learning bias guarantees model selection algorithms density estimation given yamanishi 21 barron cover 2 guarantees bound measures distance selected model distribution true data source distribution cases model assumed selected optimize srm tradeoff model complexity goodness fit training data bounds hold without assumption relating prior distribution data distribution however performance guarantee better exist simple models fit well precise statement bounds somewhat involved perhaps less interesting elegant guarantee given formula 6 discussed guarantees model averaging also proved first consider model averaging density estimation let f 1 f 2 f 3 infinite sequence models defines probability distribution set x let p prior probability densities f assume unknown distribution g x need equal f let sample elements x sampled iid according distribution g let h natural posterior density x defined follows z normalizing constant note posterior density h function sample hence random variable catoni 5 yang 23 prove somewhat different general theorems special case statement independent g selected expectation drawing sample according g kullbackleibler divergent dgjjh bounded follows 6 holds without assumed relation g prior p happens low complexity simple model f dgjjf small posterior density h small divergence g simple model small divergence g dgjjh large also 6 unlike theorem 1 vacuous continuous model classes observations also apply general forms appearing 23 5 catoni 4 also gives performance guarantees model averaging density estimation continuous model spaces using gibbs posterior however statements guarantees quite involved relationship bounds paper unclear yang 22 considers model averaging prediction consider fixed distribution pairs hx yi x 2 x 2 f0 1g consider countable class conditional probability rules f 1 f 2 f 3 f function x 0 1 f x interpreted p yjx f consider arbitrary prior models f construct posterior given sample qf posterior models induces posterior h given x defined follows let gx true conditional probability p yjx defined distribution function g 0 x 0 1 define loss lg 0 follows x denotes selecting x marginal x finally define ffi follows 2 following corollary yangs theorem ia formula bounds loss bayesian model average without making assumption relationship data distributions prior distribution p however seems weaker 5 6 imply even finite model class large samples loss posterior converges loss best model 6 guarantee vacuous continuous model classes observations apply general statement 22 weighted model mixtures also widely used constructing algorithms online guarantees particular weighted majority algorithm variants proved compete well best expert arbitrary sequence labeled data 13 6 8 7 posterior weighting used online algorithms gibbs posterior q fi defined statement theorem 2 one difference online guarantees theorem 1 algorithms one must know appropriate value fi seeing training data since aprior knowledge fi required online algorithm guaranteed perform well optimal performing well optimal srm tradeoff requires tuning fi response training data another difference online guarantees either formula 1 theorem 1 1 theorem 1 provides guarantee even cases incomplete searches concept space feasible online guarantees require algorithm find concepts perform well training data finding single simple concept fits well insufficient closely related earlier result theorem 15 bounding error rate stochastic model selection case model selected stochastically set u models probability measure simply renormalization prior u theorem 1 generalization result case arbitrary posterior distributions 4 proof theorem 1 departure point proof theorem 1 following sample size deltac abbreviates lemma 2 prior distribution probability measure p possibly uncountable concept space c following 8 4m proof suffices prove following 4m 7 lemma 2 follows 7 application markovs inequality prove suffices prove following individual given concept 4m 8 given concept c probability distribution sample induces probability distribution deltac chernoff bound distribution delta satisfies following suffices show distribution satisfying must satisfy 8 distribution delta satisfying 9 maximizing ee continuous density f delta satisfying implies following z 1e theorem 1 consider selecting sample lemma 2 implies probability least 1 gamma ffi selection sample following 4m prove theorem 1 suffices show constraint 10 function deltac implies body theorem 1 interested computing upper bound quantity note deltac prove following lemma lemma 3 fi 0 k 0 q proving lemma 3 note lemmas 3 2 together imply theorem 1 see consider sample satisfying 10 arbitrary posterior probability measure q concepts possible define three infinite sequences vectors conditions lemma 3 satisfying following taking limit conclusion lemma 3 get e cq deltac prove lemma 3 suffices consider values dropping indices change value enlarging feasible set weakening constraint 10 furthermore point theorem immediate assume without loss generality jensens inequality suffices prove consequence following lemma 1 1 original version paper 16 proved bound approximately form maximizing subject constraint 10 lemma 4 fi 0 k 0 q n prove lemma 4 take p q given use kuhntucker conditions find vector maximizing subject constraint 11 functions r n r maximum cy set satisfying f 1 c f continuous differentiable either exists f f exists nonempty subset constraints f 1 positive coefficients 1 note lemma 4 allows negative first step proving lemma 4 show without loss generality work closed compact feasible set k 0 difficult show exists feasible point ie vector let c 0 denote arbitrary feasible value ie point without loss generality need consider points satisfying 1 constrained optimization problem objective function set defined following constraints version theorem 1 form lq proved bound application jensens inequality idea maximizing achieving theorem 1 directly due robert schapire constraint 12 implies upper bound constraint 13 implies lower bound hence feasible set closed compact note continuous objective function closed compact feasible set must bounded must achieve maximum value point set constraint form fy 0 called active objective function whose gradient nonzero everywhere least one constraint must active maximum since c 0 feasible value objective function constraint 13 active maximum kuhntucker lemma point achieving maximum value must satisfy following implies following since constraint 12 must active maximum following get following since maximum value lemma proved 5 proof theorem 2 wish find distribution q minimizing bq defined follows distribution p empirical error lc given fixed letting k ln1ffiln m2 letting fl objective function rewritten follows k fl fixed positive quantities independent q simplify analysis consider finite concept classes let p prior probability ith concept let l empirical error rate ith concept problem becomes finding values q satisfying minimizing following p zero q nonzero dqjjp infinite minimizing bq assume q zero p zero assume without loss generality p nonzero p nonzero objective function continuous function compact feasible set hence realizes minimum point feasible set consider following partial derivative note q zero p nonzero dqjjp q means transfer infinitesimal quantity probability mass q reduces bound minimum must occur boundary point satisfying assume without loss generality nonzero p nonzero two distributions support kuhntucker conditions imply rb direction gradient one constraints cases must exist single value bq yields following hence minimal distribution following form r distribution q fi theorem 2 6 proof theorems 3 4 posterior distribution theorem 3 first note following dp c assumed p l nondecreasing interval 1m implies following also theorem 3 follows definition bq prove theorem 4 first define concept distribution u u induces uniform distribution error rates l let w subset values l 2 0 1 p l 0 let ff denote size w measured uniform measure 0 1 note ff 1 define concept distribution u follows total measure u written follows z du dp dp z hence u probability measure concepts let q arbitrary posterior distribution concepts following dp dp du implies following third line follows jensens inequality min 7 conclusion pacbayesian learning algorithms combine flexibility prior distribution models performance guarantees pac algorithms pac bayesian stochastic model selection given performance guarantees superior analogous guarantees deterministic pacbayesian model se lection performance guarantees stochastic model selection naturally handle continuous concept classes lead natural notion optimal posterior distribution use stochastically selecting model although optimal posterior gibbs distribution shown mild assumptions simpler posterior distributions perform nearly well open question whether better guarantees given model averaging rather stochastic model selection acknowledgments would like give special thanks manfred warmuth inspiring paper emphasizing analogy pac online settings would also like give special thanks robert schapire simplifying strengthening theorem 1 avrim blum yoav freund michael kearns john langford yishay mansour yoram singer also provided useful comments suggestions r complexity regularization application artificial neural networks minimum complexity density estimation learning classification trees gibbs estimators universal aggregation rules sharp oracle inequali ties warmuth use expert advice adaptive game playing using multiplicative weights predicting nearly well best pruning decision tree experimental theoretical comparison model selection methods results learnability vapnikchervonenkis dimension weighted majority algo rithm concept learning using complexity regulariza tion pacbayesian theorems pruning averaging decision trees efficient extension mixture techniques prediction decision trees pac analysis bayesian estimator learning nonparametric densities tyerms finitedimensional parametric hypotheses adaptive estimation pattern recognition combining different procedures mixing strategies density estimation tr ctr franois laviolette mario marchand pacbayes risk bounds samplecompressed gibbs classifiers proceedings 22nd international conference machine learning p481488 august 0711 2005 bonn germany matti kriinen john langford comparison tight generalization error bounds proceedings 22nd international conference machine learning p409416 august 0711 2005 bonn germany avrim blum john lafferty mugizi robert rwebangira rajashekar reddy semisupervised learning using randomized mincuts proceedings twentyfirst international conference machine learning p13 july 0408 2004 banff alberta canada arindam banerjee bayesian bounds proceedings 23rd international conference machine learning p8188 june 2529 2006 pittsburgh pennsylvania ron meir tong zhang generalization error bounds bayesian mixture algorithms journal machine learning research 4 1212003 matthias seeger pacbayesian generalisation error bounds gaussian process classification journal machine learning research 3 p233269 312003