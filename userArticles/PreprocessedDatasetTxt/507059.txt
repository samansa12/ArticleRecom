impulse memory controller abstractimpulse memory system architecture adds optional level address indirection memory controller applications use level indirection remap data structures memory result control data accessed cached improve cache bus utilization impulse design require modification processor cache bus designs since functionality resides memory controller result impulse adopted conventional systems without major system changes describe design impulse architecture impulse memory system used variety ways improve performance memorybound applications impulse used dynamically create superpages cheaply dynamically recolor physical pages perform strided fetches perform gathers scatters indirection vectors performance results demonstrate effectiveness optimizations variety scenarios using impulse speed range applications 20 percent factor 5 alternatively impulse used os dynamic superpage creation best policy creating superpages using impulse outperforms previously known superpage creation policies b introduction since 1987 microprocessor performance improved rate 55 per year contrast latencies improved 7 per year dram bandwidths 1520 per year 17 result relative performance impact memory accesses continues grow addition instruction issue rates increase demand memory bandwidth grows least proportionately possibly even superlinearly 8 19 many important applications eg sparse database signal processing multimedia cad applications exhibit sufficient contact information prof john carter school computing 50 central campus drive room 3190 university utah slc ut 841129205 retraccsutahedu voice 8015855474 fax 8015815843 locality reference make effective use onchip cache hierarchy applications growing processormemory performance gap makes difficult effectively exploit tremendous processing power modern microprocessors impulse project attacking problem designing building memory controller powerful conventional ones impulse introduces optional level address translation memory controller key insight feature exploits unused physical addresses translated real physical addresses memory controller unused physical address legitimate address backed dram example conventional system 4gb physical address space 1gb installed dram 3gb physical address space remains unused call unused addresses shadow addresses constitute shadow address space impulse controller maps physical memory giving applications control mediated os use shadow addresses impulse supports applicationspecific optimizations restructure data using impulse requires software modifications applications compilers operating systems requires hardware modifications processors caches buses simple example impulses memory remapping used consider program accesses diagonal elements large dense matrix physical layout part data structure shown righthand side figure 1 conventional memory system time processor accesses new diagonal element aii requests full cache line contiguous physical memory typically 32128 bytes data modern systems program accesses single word cache lines access shown top half figure 1 using impulse application configure memory controller export dense shadow space alias contains diagonal elements os map new set virtual addresses shadow memory application access diagonal elements via new virtual alias access shown bottom half figure 1 remapping array diagonal dense alias yields several performance benefits first program enjoys higher cache hit rate several diagonal elements loaded caches second program consumes less bus bandwidth nondiagonal elements sent bus third program makes effective use cache space diagonal elements contiguous shadow addresses general impulses flexibility allows applications customize addressing fit needs section 2 describes impulse architecture describes organization memory controller well system call interface applications use control operating system must mediate use memory controller prevent applications accessing others physical memory section 3 describes types optimizations impulse supports many optimizations describe new impulse first system provides hardware support generalpurpose computer systems optimizations include transposing matrices memory without copying creating superpages without copying scattergather indirection vector section 4 presents results simulation study impulse shows optimizations benefit wide range applications various applications see speedups ranging 20 factor 5 os policies dynamic superpage creation using impulse around 20 better speedup prior work section 5 describes related work great deal work done compiler operating systems communities related optimizations contribution impulse provides hardware support many optimizations previously performed purely software result tradeoffs performing optimizations different section 6 summarizes conclusions describes future work impulse architecture impulse expands traditional virtual memory hierarchy adding address translation hardware memory controller optional extra level remapping enabled fact physical addresses traditional virtual memory system typically map valid memory locations unused physical addresses constitute shadow address space technology trend putting bits physical addresses example 64bit systems coming one result trend shadow space getting larger larger impulse allows software configure memory controller interpret shadow addresses virtualizing unused physical addresses way improve efficiency onchip caches tlbs since hot data dynamically segregated cold data data items whose physical dram addresses contiguous mapped contiguous shadow addresses response cache line fetch shadow address memory controller fetches compacts sparse data dense cache lines returning data proces sor determine data associated compacted shadow cache lines reside physical memory impulse first recovers offsets within original data structure call pseudovirtual addresses translates pseudovirtual addresses physical dram addresses pseudovirtual address space page layout mirrors virtual address space allowing impulse remap data structures lie across noncontiguous physical pages shadowpseudovirtualphysical mappings take place within memory controller operating system manages resources expanded memory hierarchy provides interface application specify optimizations particular data structures 21 software interface os support exploit impulse appropriate system calls must inserted application code configure memory controller architecture language implementation group university massachusetts developing compiler technology impulse response impulse system call os allocates range contiguous virtual addresses large enough map elements new synthetic data structure os maps new data structure shadow memory corresponding physical data elements allocating contiguous range shadow addresses downloading two pieces information mmc function mmc use perform mapping shadow pseudovirtual space ii set page table entries used translate pseudovirtual physical dram addresses example consider remapping diagonal n n matrix figure 2 depicts memory translations matrix remapped image diagonal upon seeing access shadow address synthetic diagonal data structure memory controller gathers corresponding diagonal elements original array packs dense cache line returns cache line processor os interface allows alignment offset characteristics remapped data structure specified gives application control l1 cache behavior current impulse design coherence maintained software os application programmer must keep aliased data consistent explicitly flushing cache 22 hardware organization organization impulse controller architecture depicted figure 3 critical component impulse mmc shadow engine processes shadow accesses shadow engine contains small sram assembly buffer used scattergather cache lines shadow address space shadow descriptors store remapping configuration information wasted bus bandwidth conventional memory system impulse memory system impulse controller cache physical pages figure 1 using impulse remap diagonal dense matrix dense cache line black boxes represent data diagonal whereas gray boxes represent nondiagonal data diagonal virtual memory shadow memory pseudovirtual memory physical memory page table conventional memory system impulse memory system remapping function figure 2 accessing sparse diagonal elements array via dense diagonal variable impulse cpu mmu system memory bus shadow address dram interface normal address shadow engine addrcalc assembly buffer descriptors impulse mmc figure 3 impulse memory controller organization alu unit addrcalc translate shadow addresses pseudovirtual addresses memory controller translation lookaside buffer mtlb cache recently used translations pseudo virtual addresses physical addresses shadow engine contains eight shadow descriptors capable saving configuration settings one remapping shadow descriptors share alu unit mtlb since extra level address translation optional addresses appearing memory bus may physical backed dram shadow memory spaces valid physical addresses pass untranslated dram interface shadow addresses must converted physical addresses presented dram shadow engine first determines shadow descriptor use passes contents addrcalc unit output addrcalc series offsets individual sparse elements need fetched offsets passed mtlb compute physical addresses need fetched hide latency fetching remapped data shadow descriptor configured prefetch remapped cache line following currently accessed one depending impulse used access particular data structure shadow address translations take three forms direct strided scattergather direct mapping translates shadow address directly physical dram address mapping used recolor physical pages without copying construct superpages dynamically strided mapping creates dense cache lines array elements contiguous mapping function maps address soffset shadow space pseudovirtual address pvaddr stride soffset pvaddr starting address data structures pseudovirtual image pvaddr assigned os upon configuration scattergather mapping uses indirection vector vec translate address soffset shadow space pseudovirtual address pvaddr 3 impulse optimizations impulse remappings used enable wide variety optimizations first describe impulses ability pack data cache lines either using stride scattergather remapping used examine two scientific application kernelssparse matrixvector multiply smvp dense matrixmatrix product dmmpand three image processing algorithmsimage filter ing image rotation ray tracing show impulses ability remap pages used automatically improve tlb behavior dynamic superpage creation results published prior conference papers 9 13 44 49 31 sparse matrixvector product sparse matrixvector product smvp irregular computational kernel critical many large scientific algorithms example time conjugate gradient 3 simulations 33 spent performing smvp avoid wasting memory sparse matrices generally compacted nonzero elements corresponding index arrays stored example class input matrix nas conjugate gradient kernel cga 14000 14000 contains 185 million nonzeroes although sparse encodings save tremendous amounts memory sparse matrix codes tend suffer poor memory performance data must accessed indirection vectors cga sgi origin 2000 processor 2way 32k l1 cache 2way exhibits l1 l2 cache hit rates 63 92 respectively inner loop sparse matrixvector product cg roughly n sum 0 j rowsi rowsi11 sum code data structures smvp illustrated figure 4 iteration multiplies row sparse matrix dense vector x accesses x indirect via column index vector sparse making code perform poorly conventional memory systems whenever x accessed conventional memory system fetches cache line data one element used large sizes x column data sparse nature accesses x inhibit data reuse l1 cache element column data used almost every access x results l1 cache miss large l2 cache enable reuse x physical data layouts managed prevent l2 cache conflicts x unfortunately conventional systems typically provide mechanisms managing physical layout impulse memory controller supports scattergather physical addresses indirection vectors vector machines cdc star100 18 provided scattergather capabilities hardware within processor impulse allows conventional cpus take advantage scat tergather functionality implementing operations memory reduces memory traffic bus exploit impulse cgs smvp code modified follows impulse remapx x n column indirect n sum 0 j rowsi rowsi11 sum dataj xj impulse remap operation asks operating system 1 allocate new region shadow space 2 map x shadow region 3 instruct memory controller map elements shadow region xk physical memory xcolumnk remapped array set code accesses remapped version gathered structure rather original structure x optimization improves performance smvp two ways first spatial locality improved l1 cache since memory controller packs gathered elements cache lines cache line contain 100 useful data rather one useful element second sum 0 j rowsi rowsi11 sum dataj xcolumnj n f2 c u column rows figure 4 conjugate gradients sparse matrixvector product matrix encoded using three dense arrays data rows column contents data rowsi indicates th row begins data columni indicates column element stored datai comes processor issues fewer memory instructions since read indirection vector column occurs memory controller note use scattergather memory controller reduces temporal locality l2 cache remapped elements x cannot reused since elements different addresses alternative scattergather dynamic physical page recoloring direct remapping physical pages physical page recoloring changes physical addresses pages reusable data mapped different part physicallyaddressed cache nonreused data performing page recoloring conflict misses eliminated conventional machine physical page recoloring expensive way change physical address data copy data physical pages impulse allows physical pages recolored without copying virtual page recoloring explored authors 6 smvp x vector reused within iteration elements data row column vectors used iteration alternative scattergather x memory controller impulse used physically recolor pages x conflict data structures l2 cache example cga benchmark x 100k bytes would fit l1 caches would fit many l2 caches impulse remap x pages occupy physicallyindexed l2 cache remap data rows columns small number pages conflict x experiments color vectors x data column conflict l2 cache multiplicand vector x heavily reused color occupy first half l2 cache keep large data column structures conflicting divide second half l2 cache two quarters color data column occupy one quarter cache effect use pieces l2 cache set virtual stream buffers 29 data rows columns 32 tiled matrix algorithms dense matrix algorithms form important class scientific kernels example lu decomposition dense cholesky factorization dense matrix computational kernels algorithms tiled blocked increase efficiency iterations tiled algorithms reordered improve memory performance difficulty using tiled algorithms lies choosing appropriate tile size 27 tiles noncontiguous virtual address space difficult keep conflicting cache avoid conflicts either tile sizes must kept small makes inefficient use cache tiles must copied nonconflicting regions memory expensive impulse provides alternative method removing cache conflicts tiles use simplest tiled algorithm dense matrixmatrix product dmmp example impulse improve behavior tiled matrix algorithms assume computing want keep current tile c matrix l1 cache compute addition since row matrix used multiple times compute row c matrix would like keep active row l2 cache impulse allows basestride remapping tiles noncontiguous portions memory contiguous tiles shadow space result impulse makes easy os virtually remap tiles since physical footprint tile match size use os remap virtual address matrix tile new shadow alias eliminate interference virtuallyindexed l1 cache first divide l1 cache three segments segment keep tile current output tile c input tiles b finish one tile use impulse remap virtual tile next physical tile maintain cache consistency must purge b tiles flush c tiles caches whenever remapped section 412 shows costs minor 33 image filtering image filtering applies numerical filter function image modify appearance image filtering may used attenuate highfrequency components caused noise sampled image adjust image different geometry detect enhance edges within image create various special effects box bartlett gaussian binomial filters common practice modifies input image different way share similar computational characteristics concentrate representative class filters binomial filters 15 pixel output image computed applying twodimensional mask input image binomial filtering computationally similar single step successive overrelaxation algorithm solving differential equations filtered pixel value calculated linear function neighboring pixel values original image corresponding mask values example order5 binomial filter value pixel j output image be256 avoid edge effects original image boundaries must extended applying masking function figure 5 illustrates black andwhite sample image application small binomial filter practice many filter functions including binomial separable meaning symmetric decomposed pair orthogonal linear filters example twodimensional mask decomposed two onedimensional linear masks twodimensional mask simply outer product onedimensional mask transpose process applying mask input image performed sweeping first along rows columns calculating partial sum step pixel original image used short time makes filtering pure streaming application impulse transpose input output image arrays without copying gives column sweep much better cache behavior 34 image rotation image warping refers algorithm performs imagetoimage transformation separable image warps decomposed multiple onedimensional transformations 10 separable warps impulse used improve cache tlb performance onedimensional traversals orthogonal image layout memory threeshear image rotation algorithm example separable image warp algorithm rotates 2dimensional image around center three stages performs shear operation image figure 5 example binomial image filtering original image left filtered image right illustrated figure 6 algorithm simpler write faster run fewer visual artifacts direct rotation underlying math straightforward rotation angle expressed matrix multiplication 0a cos sin sin cos x rotation matrix broken three shears follows cos sin sin cos 1 0 tan 1a 1 sin tan 1a none shears requires scaling since determinant matrix 1 involves shift rows columns algorithm simple understand implement robust defined rotation values 0 90 twoshear rotations fail angles near 90 assume simple image representation array pixel values second shear operation along axis walks along column image matrix gives rise poor memory performance large images impulse improves cache tlb performance transposing matrix without copying walking along columns image replaced walking along rows transposed matrix isosurface rendering using ray tracing isosurface rendering benchmark based technique demonstrated parker et al 37 benchmark generates image isosurface volume specific point view figure threeshear rotation image counterclockwise one radian original image upper left first sheared horizontally upper right image sheared upwards lower right final rotated image lower left generated via one final horizontal shift contrast volume visualization methods method generate explicit representation isosurface render zbuffer instead uses bruteforce ray tracing perform interactive isosurfacing ray first isosurface intersected determines value corresponding pixel approach high intrinsic computational cost simplicity scalability make ideal large data sets current highend systems traditionally ray tracing used volume visualization suffers poor memory behavior rays travel along direction data stored ray must traced potentially large fraction volume giving rise two problems first many memory pages may need touched results high tlb pressure second ray high angle incidence may visit one volume element voxel per cache line case bus bandwidth wasted loading unnecessary data pollutes cache carefully handoptimizing ray tracers memory access patterns parker et al achieve acceptable performance interactive rendering 10 frames per second improve data locality organizing data set multilevel spatial hierarchy tiles composed smaller cells smaller cells provide good cacheline utilization macro cells created cache minimum maximum data values cells tile macro cells enable simple minmax comparison detect whether ray intersects isosurface within tile empty macro cells need traversed careful handtiling volume data set yield much better memory performance choosing optimal number levels spatial hierarchy sizes tiles level difficult resulting code hard understand maintain impulse deliver better performance handtiling lower programming cost need preprocess volume data set good memory performance impulse memory controller remap dynamically screen volume figure 7 isosurface rendering using ray tracing picture left shows rays perpendicular viewing screen traced volume one right illustrates ray visits sequence voxels volume impulse optimizes voxel fetches memory via indirection vectors representing voxel sequences ray addition source code retains readability modifiability like many realworld visualization systems benchmark uses orthographic tracer whose rays intersect screen surface right angles producing images lack perspective appear far away relatively simple compute use impulse extract voxels ray potentially intersects traversing vol ume righthand side figure 7 illustrates ray visits certain sequence voxels volume instead fetching cache lines full unnecessary voxels impulse remap ray voxels requires useful voxels fetched 36 online superpage promotion impulse used improve tlb performance automatically operating system automatically create superpages dynamically superpages supported translation lookaside buffers tlbs almost modern processors groups contiguous virtual memory pages mapped single tlb entry 12 30 43 using superpages makes efficient use tlb physical pages back superpage must contiguous properly aligned dynamically coalescing smaller pages superpage thus requires pages coincidentally adjacent aligned unlikely copied become overhead promoting superpages copying includes direct indirect costs direct costs come copying pages changing mappings indi 0physical addresses 0x06155000 0x40138000 0x20285000 0x80243000 0x00005000 0x00004000 virtual addresses shadow addresses 0x00007000 00004 004physical size processor tlb virtual memory controller figure 8 example creating superpages using shadow space rect costs include increased number instructions executed tlb miss due new decisionmaking code miss handler increased contention cache hierarchy due code data used promotion process deciding whether create superpages costs must balanced improvements tlb performance romer et al 40 study several different policies dynamically creating superpages tracedriven simulations analysis show policy balances potential performance benefits promotion overheads improve performance tlbbound applications 50 work extends romer et al showing impulse changes design dynamic superpage promotion policy impulse memory controller maintains page tables shadow memory mappings building superpages base pages physically contiguous entails simply remapping virtual pages properly aligned shadow pages memory controller maps shadow pages original physical pages processors tlb affected extra level translation takes place controller figure 8 illustrates superpage mapping works impulse example os mapped contiguous 16kb virtual address range single shadow superpage physical page frame 0x80240 address shadow physical range placed system memory bus memory controller detects physical address needs retranslated using local shadowtophysical translation tables example figure 8 processor translates access virtual address 0x00004080 shadow physical address 0x80240080 controller turn translates real physical address 0x40138080 performance performed series detailed simulations evaluate performance impact optimizations described section 3 studies use ursim 48 executiondriven simulator derived rsim 35 ursim models microarchitecture close mips r10000 microprocessor 30 64entry instruction window configured issue four instructions per cycle model 64kilobyte l1 data cache nonblocking writeback virtually dexed physically tagged directmapped 32byte lines 512kilobyte l2 data cache nonblocking writeback physically indexed physically tagged twoway associative 128byte lines l1 cache hits take one cycle l2 cache hits take eight cycles ursim models splittransaction mips r10000 cluster bus snoopy coherence pro tocol bus multiplexes addresses data eight bytes wide threecycle arbitration delay onecycle turnaround time model two memory controllers conventional highperformance mmc based one sgi o200 server impulse mmc system bus memory controller drams clock rate one third cpu clocks memory system supports critical word first ie stalled memory instruction resumes execution first quadword returns load latency first quadword cycles unified tlb singlecycle fully associative softwaremanaged combined instruction data employs leastrecentlyused replacement policy base page size 4096 bytes superpages built poweroftwo multiples base page size biggest superpage tlb map contains 2048 base pages model 128entry tlb remainder section examine simulated performance impulse examples given section 3 calculation l2 cache hit ratio mem memory hit ratio uses total number loads executed total number l2 cache accesses divisor ratios formulation makes easier compare effects l1 l2 caches memory accesses sum l1 cache l2 cache memory hit ratios equals 100 41 finegrained remapping first set experiments exploit impulses finegrained remapping capabilities create synthetic data structures better locality original programs 411 sparse matrixvector product table 411 shows impulse used improve performance nas class conjugate gradient cga benchmark first column gives results running cga nonimpulse system second third columns give results running cga impulse system second column numbers come using impulse memory controller perform scattergather third column numbers come using perform physical page coloring conventional memory system cga suffers many cache misses nearly 17 accesses go memory inner loop cga small generate cache misses quickly leads large number cache misses outstanding given time large number outstanding memory operations causes heavy contention system bus memory controller drams baseline version cga bus utilization reaches 885 result average latency memory operation reaches 163 cycles baseline version cga behavior combined high cache miss rates causes average load cga take 476 cycles compared 1 cycle l1 cache hits scattergather remapping cga improves performance factor 3 largely due increase l1 cache hit ratio decrease number loadsstores go memory main memory access remapped vector x loads cache several useful elements original vector x increases l1 cache hit rate words retrieving elements remapped array x improves spatial locality cga scattergather remapping reduces total number loads executed program 493 million 353 million original program two loads issued compute xcolumnj scattergather version program one load issued processor load indirection vector occurs memory controller reduction compensates scattergathers increase average cost load accounts almost onethird cycles saved provide another example useful impulse use recolor pages major data structures cga page recoloring consistently reduces cost memory accesses eliminating conflict misses l2 cache increasing l2 cache hit ratio 197 220 result fewer loads go memory performance improved 17 although page recoloring improves performance cga nearly effective scat tergather difference primarily page recoloring achieve two major improvements scattergather provides improving locality cga reducing number loads executed comparison mean page recoloring useful optimiza tion although speedup page recoloring cga substantially less scattergather page recoloring broadly applicable 412 dense matrixmatrix product section examines performance benefits tile remapping dmmp compares results software tile copying impulses alignment restrictions require remapped tiles aligned l2 cache line boundaries adds following constraints matrices tile sizes must multiple cache line experiments size 128 bytes constraint overly limiting especially since makes efficient use cache space arrays must padded tiles aligned 128 bytes compilers easily support constraint similar padding techniques explored context vector processors 7 table 412 illustrates results tiling experiments baseline conventional nocopy tiling software tile copying outperforms baseline code almost 10 impulse tile remapping outperforms 20 improvement performance primarily due difference cache behavior copying remapping double l1 cache hit rate reduce average number cycles load less two impulse higher l1 cache hit ratio software copying since copying tiles incur cache misses number loads go memory reduced twothirds addition cost copying tiles greater overhead using impulse remap tiles result using impulse provides twice much speedup comparison conventional impulse copying schemes conservative several reasons copying works particularly well dmmp number operations performed tile size 2 3 means overhead copying relatively low algorithms reuse data lower relative overhead copying greater likewise caches therefore tiles grow larger cost copying grows whereas low cost impulses tile remapping remains fixed finally authors found performance copying vary greatly matrix size tile size cache size 45 impulse insensitive crossinterference tiles conventional scattergather page coloring time 548b 177b 467b l2 hit ratio 197 159 220 mem hit ratio 169 63 143 avg load time 476 232 387 loads 493m 353m 493m speedup 310 117 table 1 simulated results nas class conjugate gradient benchmark using two different opti mizations times processor cycles conventional software copying impulse time 664m 610m 547m l1 hit ratio 496 986 995 l2 hit ratio 487 11 04 mem hit ratio 17 03 01 avg load time 668 171 146 table 2 simulated results tiled matrixmatrix product times millions cycles matrices 512 512 32 tiles tiled impulse time 459m 237m l1 hit ratio 9895 997 mem hit ratio 024 005 avg load time 157 116 issued instructions total 725m 290m graduated instructions total 435m 280m issued instructions tlb 256m 78m graduated instructions tlb 134m 33m table 3 simulated results image filtering various memory system configurations times processor cycles tlb misses number user data misses 413 image filtering table 3 presents results order129 binomial filter 321024 color image impulse version code pads pixel four bytes performance differences hand tiled impulse versions algorithm arise vertical pass data tiled version suffers 35 times many l1 cache misses 40 times many tlb faults executed 134 million instructions tlb miss handlers indirect impact high tlb miss rate even dramatic baseline filtering program almost 300 million instructions issued graduated contrast impulse version algorithm executes 33 million instructions handling tlb misses 10 million instructions issued graduated compared dramatic performance improvements less 1 million cycles spent setting impulse remapping negligible overhead although versions algorithm touch data element number times impulse improves memory behavior image filtering code two ways original algorithm performs vertical filtering pass touches pages per iteration processor tlb hold yielding high kernel overhead observed runs image cache lines conflicting within l1 cache degrade performance since impulse version code appear processor contiguous addresses suffers tlb faults nearperfect temporal spatial locality l1 cache 414 threeshear image rotation table 4 illustrates performance results rotating color image clockwise one radian image contains 24 bits color information ppm file measure three versions benchmark original version adapted wolberg 47 handtiled version code vertical shears traversal blocked version adapted impulse matrices transposed memory controller impulse version requires pixel padded four bytes since impulse operates poweroftwo object sizes quantify performance effect padding measure results nonimpulse versions code using threebyte fourbyte pixels performance differences among different versions entirely due cycles saved vertical shear horizontal shears exhibit good memory behavior rowmajor layout performance bottleneck impulse increases cache hit rate roughly 95 985 reduces number tlb misses two orders magnitude reduction original original tiled tiled impulse padded padded time 572m 576m 284m 278m 215m l1 hit ratio 950 948 981 976 985 l2 hit ratio 15 16 11 15 11 mem hit ratio 35 36 08 09 04 avg load time 385 385 181 219 150 issued instructions total 476m 477m 300m 294m 232m graduated instructions total 346m 346m 262m 262m 229m issued instructions tlb 212m 215m 52m 51m 081m graduated instructions tlb 103m 104m 24m 24m 042m table 4 simulation results performing 3shear rotation 1kby1k 24bit color image times processor cycles tlb misses user data misses tlb miss rate eliminates 99 million tlb miss handling instructions reduces number issued graduated instructions 100 million two effects constitute impulses benefit tiled version walks columns 32 pixels time yields hit rate higher original programs lower impulses tiles source matrix sheared destination matrix even though cache performance source nearly perfect suffers destination reason decrease tlb misses tiled code great impulse code impulse code requires 33 memory store 24bit color image also measured performance impact using padded 32bit pixels nonimpulse codes original program padding causes cache line fetch load useless pad bytes degrades performance program already memorybound contrast tiled program increase memory traffic balanced reduction load shift mask operations manipulating wordaligned pixels faster manipulating bytealigned pixels padded tiled version rotation code still slower impulse tiled version shear uses cycles recomputing saving restoring columns displacement traversing tiles input image displacement computed 1024 32 times since column length 1024 tile height 32 contrast impulse code tiled computes columns displacement since column completely traversed visited isosurface rendering using ray tracing simplicity benchmark assumes screen plane parallel volumes z axis result compute entire planes worth indirection vector need remap addresses every ray assumption large restriction assumes use volume rendering algorithm like lacroutes 26 transforms arbitrary viewing angles angles better memory performance isosurface volume one edge surface parallel xz plane measurements present two particular viewing angles table 5a shows results screen parallel yz plane rays exactly follow layout voxels memory assume xyz layout order table 5b shows results screen parallel xz plane rays exhibit worst possible cache tlb behavior traversing xy planes two sets data points represent extremes memory performance ray tracer data measurements labeled original ray tracer uses macrocells reduce number voxels traversed tile volume macrocells 444 voxels size results labeled indirection use macrocells address voxels indirection vector indirection vector stores precomputed voxel offsets xy plane finally results labeled impulse use impulse perform indirection lookup memory controller table 5a rays parallel array layout impulse delivers substantial performance gain precomputing voxel offsets reduces execution time approximately 9 million cycles experiment reported indirection column exchanges computation voxels offsets accesses indirection vector although increases number memory loads still achieves positive speedup accesses cache hits impulse accesses indirection vector performed within memory controller hides access latencies consequently impulse obtained higher speedup compared original version impulse saves computation voxels offsets table 5b rays perpendicular voxel array layout impulse yields much larger performance gain speedup 549 reducing number tlb misses saves approximately million graduated instructions reducing number issued graduated instructions approximately 120 million increasing cache hit ratio loading useless voxels cache saves remaining quarterbillion cycles indirection version executes 3 slower original one rays perpendicular voxel array accessing voxels original indirection impulse time 742m 650m 614m l1 hit ratio 951 908 918 l2 hit ratio 37 73 63 mem hit ratio 12 19 19 avg load time 18 28 25 loads 216m 172m 13m issued instructions total 131m 714m 577m graduated instructions total 128m 693m 555m issued instructions tlb 068m 114m 018m graduated instructions tlb 035m 050m 015m speedup 114 121 original indirection impulse time 383m 397m 697m l2 hit ratio 06 22 51 mem hit ratio 123 152 16 avg load time 82 103 24 loads 32m 27m 16m issued instructions total 348m 318m 76m graduated instructions total 218m 148m 68m issued instructions tlb 126m 156m 018m graduated instructions tlb 59m 60m 015m table 5 results isosurface rendering times processor cycles tlb misses user data misses rays follow memory layout image b perpendicular memory layout generates lots cache misses frequently loads new data cache loads evict indirection vector cache bring cache hit ratio indirection vector accesses result overhead accessing indirection vector outweighs benefit saving computation voxel offsets slows execution 42 online superpage promotion evaluate performance impulses support inexpensive superpage promotion reevaluated romer et als work dynamic superpage promotion algorithms 40 context im pulse system model differs several significant ways employ form tracedriven simulation atom 42 binary rewriting tool rewrite applications using atom monitor memory references modified applications used onthefly simulation tlb behavior simulated system two 32entry fullyassociative tlbs one instructions one data uses lru replacement tlb entries base page size 4096 bytes better understand tlb size may affect perfor mance model two tlb sizes 64 128 entries romer et al combine results tracedriven simulation measured baseline performance results calculate effective speedup benchmarks execute benchmarks dec alpha 3000700 running dec osf1 21 processor system dualissue inorder 225 mhz alpha 21064 system two megabytes offchip cache 160 megabytes main memory simulations assume following fixed costs take cache effects account 1kbyte copied assigned 3000cycle cost asap policy charged cycles tlb miss approxonline policy charged 130 cycles tlb miss performance results presented obtained complete simulation bench marks measure kernel application time direct overhead implementing superpage promotion algorithms resulting effects system including expanded tlb miss handlers cache effects due accessing page tables maintaining prefetch coun ters overhead associated promoting using superpages impulse present comparative performance results application benchmark suite 421 application results evaluate different superpage promotion approaches larger problems use eight programs mix sources benchmark suite includes three spec95 benchmarks com press gcc vortex three image processing benchmarks described earlier iso rotate filter one scientific benchmark adi one benchmark dis benchmark suite dm 28 benchmarks compiled sun cc workshop compiler 42 optimization level xo4 compress spec95 data compression program run input ten million characters avoid overestimating efficacy superpages compression algorithm run instead default 25 times gcc cc1 pass version 253 gcc compiler sparc architectures used compile 306kilobyte file 1cpdec1c vortex objectoriented database program measured spec95 test input isosurf interactive isosurfacing volume renderer described section 415 filter performs order129 binomial filter 321024 color image rotate turns 10241024 color image clockwise one radian adi implements algorithm alternative direction integration dm data management program using input file dm07in two benchmarks gcc compress also included romer et als benchmark suite although use spec95 versions whereas used spec92 versions use spec92 applications study due benchmarks obsolescence several romer et als remaining benchmarks based tools used research environment university washington readily available us table 6 lists characteristics baseline run benchmark fourway issue superscalar processor superpage promotion occurs tlb miss time total time spent data tlb miss handler benchmarks demonstrate varying sensitivity tlb performance system smaller tlb 92 351 execution time lost due tlb miss costs percentage time spent handling tlb misses falls less 1 334 system 128entry tlb figures show normalized speedups different combinations promotion policies asap approxonline mechanisms remapping copying compared baseline instance benchmark experiments found best approxonline threshold twopage superpage 16 conventional system 4 impulse system also thresholds used fullapplication tests figure 9 gives results 64 entry results 128entry tlb online superpage promotion improve total cache tlb tlb benchmark cycles misses misses miss 64entry tlb compress 632 3455 4845 279 gcc 628 1555 2103 103 vortex filter 425 241 4798 351 rotate 547 3570 3807 179 dm 233 129 771 92 128entry tlb compress 426 3619 36 06 gcc 533 1526 332 20 vortex 423 763 1047 81 isosurf 93 989 548 174 filter 417 240 4544 334 rotate 545 3569 3702 169 dm 211 143 250 33 table characteristics baseline run speedup impulseasap impulseapproxonline copyingasap copyingapproxonline compress gcc vortex 107 106093 106 085 filter 176 174 170 168 rotate dm 113 109 111 105 figure 9 normalized speedups two promotion policies 4issue system 64entry tlb performance factor two adi remapping asap also decrease performance similar factor using copying version asap isosurf make two orthogonal comparisons figures remapping versus copying asap versus approxonline 422 asap vs approxonline first compare two promotion algorithms asap approxonline using results figures 9 10 relative performance two algorithms strongly influenced choice promotion mechanism remapping copying using remapping asap slightly outperforms approxonline average case exceeds performance approxonline 14 16 experiments trails performance approxonline one case vortex 64entry tlb differences performance range asapremap outperforming aolremap 32 adi 64entry tlb aolremap outperforming asapremap 6 vortex 64entry tlb general however performance differences two policies small asap average 7 better 64entry tlb 6 better 128entry tlb results change noticeably employ copying promotion mechanism approxonline outperforms asap nine 16 experiments policies perform almost identically three seven cases magnitude disparity approxonline asap results also dramatically larger differences performance range asap outperforming approxonline 20 vortex 64entry tlb approxonline outperforming asap 45 isosurf 64entry tlb overall results confirm romer et al best promotion policy use creating superpages via copying approxonline taking arithmetic mean performance differences reveals asap average 6 better 64entry tlb 4 better 128entry tlb speedup impulseasap impulseapproxonline copyingasap copyingapproxonline compress gcc vortex 107 103 084 082 106 106092 filter 173 169 167 165 rotate dm figure 10 normalized speedups two promotion policies 4issue system 128entry tlb relative performance asap approxonline promotion policies changes employ different promotion mechanisms asap tends create superpages aggressively approxonline design assumption underlying approxonline algorithm reason performs better asap copying used superpages created cost tlb misses equals cost creating superpages given remapping much lower cost creating superpages copying surprising aggressive asap policy performs relatively better approxonline 423 remapping vs copying compare two superpage creation mechanisms remapping clear winner highly varying margins differences performance best overall remappingbased algorithm asapremap best copyingbased algorithm aonlinecopying large 97 case adi 64entry 128entry tlb overall asapremap outperforms aonlinecopying 10 eleven sixteen experiments averaging 33 better 64entry tlb 22 better 128entry tlb 424 discussion romer et al show approxonline generally superior asap copying used remapping used build superpages though find reverse true using impulsestyle remapping results larger speedups consumes much less physical memory since superpage promotion cheaper impulse also afford promote pages aggressively romer et als tracebased simulation model cache interference application tlb miss handler instead study assumes superpage promotion costs total 3000 cycles per kilobyte copied 40 table 7 shows measured perkilobyte cost cpu cycles promote pages copying four representative benchmarks note cycles per 1k average baseline bytes promoted cache hit ratio cache hit ratio filter 5966 9980 9980 raytrace 10352 9650 8720 dm 6534 9980 9986 table 7 average copy costs cycles approxonline policy also assume relatively faster processor measure bound subtracting execution time aolremap aolcopy dividing number kilobytes copied simulation platform benchmark suite copying least twice expensive romer et al assumed gcc raytrace superpage promotion costs three times cost charged tracedriven study part differences due cache effects copying incurs find copying used promote pages approxonline performs better lower aggressive threshold used romer et al specifically best thresholds experiments revealed varied four 16 study used fixed threshold 100 difference thresholds significant impact performance example run adi benchmark using threshold 32 approxonline copying slows performance 10 128entry tlb contrast run approxonline copying using best threshold 16 performance improves 9 general find even copyingbased promotion algorithms need aggressive creating superpages suggested romer et al given cost promoting pages much higher 3000 cycles estimated study one might expect best thresholds would higher romers however cost tlb miss far outweighs greater copying costs tlb miss costs order magnitude greater assumed study 5 related work number projects proposed modifications conventional cpu dram designs improve memory system performance including supporting massive multithreading 2 moving processing power dram chips 25 developing configurable architectures 50 projects show promise almost impossible prototype nontraditional cpu cache designs perform well commodity processors addition performance processorinmemory approaches handicapped optimization dram processes capacity increase bit density rather speed morph architecture 50 almost entirely configurable programmable logic embedded virtually every datapath system enabling optimizations similar described primary difference impulse morph impulse simpler design used current systems radram project uc davis building memory system lets memory perform computation 34 radram pim processorinmemory project similar iram 25 raw project mit 46 even radical idea iram element almost entirely reconfigurable contrast projects impulse seek put entire processor memory since dram processes substantially slower logic processes many others investigated memory hierarchies incorporate stream buffers focus nonprogrammable buffers perform hardware prefetching consecutive cache lines prefetch buffers introduced jouppi 23 even though stream buffers intended transparent programmer careful coding required ensure good memory performance palacharla kessler 36 investigate use similar stream buffers replace l2 cache farkas et al 14 identify performance trends relationships among various components memory hierarchy including stream buffers dynamically scheduled processor studies find dynamically reactive stream buffers yield significant performance increases imagine media processor streambased architecture bandwidthefficient stream register file 38 streaming model computation exposes parallelism locality applica tions makes systems attractive domain intelligent dram scheduling competitive algorithms perform online costbenefit analyses make decisions guarantee performance within constant factor optimal offline algorithm romer et al 40 adapt approach tlb management employ competitive strategy decide perform dynamic superpage promotion also investigate online software policies dynamically remapping pages improve cache performance 6 39 competitive algorithms used help increase efficiency operating system functions resources including paging synchronization file cache management chen et al 11 report performance effects various tlb organizations sizes results indicate important factor minimizing overhead induced tlb misses reach amount address space tlb map instant time even though spec benchmarks study relatively small memory requirements find tlb misses increase effective cpi cycles per instruction factor five jacob mudge 22 compare five virtual memory designs including combinations hierarchical inverted page tables hardwaremanaged softwaremanaged tlbs find large tlbs necessary good performance tlb miss handling accounts much memorymanagement overhead also project individual costs tlb miss traps increase future microprocessors proposed solutions growing tlb performance bottleneck range changing tlb structure retain working set eg multilevel tlb hierarchies 1 16 implementing better management policies software 21 hardware 20 masking tlb miss latency prefetching entries software 4 hardware 41 approaches improved exploiting superpages commercial tlbs support superpages several years 30 43 research needed best make general use khalidi 24 mogul 31 discuss benefits systems support superpages advocate static allocation via compiler programmer hints talluri et al 32 report many difficulties attendant upon general utilization superpages result requirement superpages map physical memory regions contiguous aligned 6 conclusions impulse project attacks memory bottleneck designing building smarter memory controller impulse requires modifications cpu caches drams one special form smarts controller supports applicationspecific physical address remapping paper demonstrates several simple remapping functions used different ways improve performance two important scientific application kernels flexible remapping support impulse controller used implement variety optimizations experimental results show impulses finegrained remappings result substantial program speedups using scattergather indirection vector remapping mechanism improves nas conjugate gradient benchmark performance 210 volume rendering benchmark performance 449 using strided remapping improves performance image filtering image rotation dense matrixmatrix product applications 94 166 21 respectively impulses direct remappings also effective range programs used dynamically build superpages without copying thereby reduce frequency tlb faults simulations show optimization speeds eight programs variety sources factor 203 25 better prior work pagelevel remapping perform cache coloring improves performance conjugate gradient 17 optimizations describe applicable across variety memorybound ap plications particular impulse useful improving systemwide performance ex ample impulse speed messaging interprocess communication ipc impulses support scattergather remove software overhead gathering ipc message data multiple user buffers protocol headers ability use impulse construct contiguous shadow pages noncontiguous pages means network interfaces need perform complex expensive address translations finally fast local ipc mechanisms like lrpc 5 use shared memory map buffers sender receiver address spaces impulse could used support fast nocopy scattergather shared shadow address spaces acknowledgments thank al davis bharat chandramouli krishna mohan bob devine mark swanson arjun dutt ali ibrahim shuhuan yu michael abbott sean cardwell yeshwant kolla contributions impulse project r amd athlon processor technical brief tera computer system nas parallel benchmarks software prefetching caching translation buffers lightweight remote procedure call avoiding conflict misses dynamically large directmapped caches organization use parallel memories memory bandwidth limitations future microprocessors simulation based study tlb performance compaq computer corporation revisiting superpage promotion hardware support image processing computer graphics hal computer systems inc computer architecture quantitative approach control data star100 processor design intrinsic bandwidth requirements ordinary programs pentium pro family developers manual look several memory management units improving directmapped cache performance addition small fully associative cache prefetch buffers virtual memory support multiple page sizes scalable processors billiontransistor era iram fast volume cache performance optimizations blocked algorithms access ordering memoryconscious cache utilization big memories desktop surpassing tlb performance superpages less operating system support sparse matrix kernels shared memory message passing systems active pages model computation intelligent memory rsim reference manual evaluating stream buffers secondary cache replacement interactive ray tracing isosurface rendering bandwidthefficient architecture media processing using virtual memory improve cache tlb performance reducing tlb memory overhead using online superpage promotion atom system building customized program analysis tools increasing tlb reach using superpages backed shadow memory copy copy compiletime technique assessing data copying used eliminate cache conflicts digital image warping ursim reference manual memory system support image processing architectural adaptation applicationspecific locality optimizations tr lightweight remote procedure call cache performance optimizations blocked algorithms simulation based study tlb performance copy copy atom evaluating stream buffers secondary cache replacement avoiding conflict misses dynamically large directmapped caches surpassing tlb performance superpages less operating system support reducing tlb memory overhead using online superpage promotion memory bandwidth limitations future microprocessors intrinsic bandwidth requirements ordinary programs fast volume rendering using shearwarp factorization viewing transformation tera computer system memorysystem design considerations dynamicallyscheduled processors active pages increasing tlb reach using superpages backed shadow memory computer architecture 2nd ed interactive ray tracing isosurface rendering bandwidthefficient architecture media processing look several memory management units tlbrefill mechanisms page table organizations improving directmapped cache performance addition small fullyassociative cache prefetch buffers recencybased tlb preloading digital image warping image processing computer graphics scalable processors billiontransistor era baring software 3d transformations images scanline order access ordering memoryconscious cache utilization softwaremanaged address translation impulse memory system support image processing architectural adaptation applicationspecific locality optimizations reevaluating online superpage promotion hardware support using virtual memory improve cache tlb performance ctr jun miyazaki hardware supported memory access high performance main memory databases proceedings 1st international workshop data management new hardware june 1212 2005 baltimore maryland mccorkle programmable busmemory controllers modern computer architecture proceedings 43rd annual southeast regional conference march 1820 2005 kennesaw georgia hur calvin lin memory prefetching using adaptive stream detection proceedings 39th annual ieeeacm international symposium microarchitecture p397408 december 0913 2006 lixin zhang mike parker john carter efficient address remapping distributed sharedmemory systems acm transactions architecture code optimization taco v3 n2 p209229 june 2006 daehyun kim mainak chaudhuri mark heinrich evan speight architectural support uniprocessor multiprocessor active memory systems ieee transactions computers v53 n3 p288307 march 2004 xipeng shen yutao zhong chen ding locality phase prediction acm sigops operating systems review v38 n5 december 2004 eero aho jarno vanne timo hmlinen configurable data memory multimedia processing journal signal processing systems v50 n2 p231249 february 2008 xiaodong li zhenmin li yuanyuan zhou sarita adve performance directed energy management main memory disks acm transactions storage tos v1 n3 p346380 august 2005 xiaodong li zhenmin li francis david pin zhou yuanyuan zhou sarita adve sanjeev kumar performance directed energy management main memory disks acm sigarch computer architecture news v32 n5 december 2004 zhen fang lixin zhang john b carter ali ibrahim michael parker active memory operations proceedings 21st annual international conference supercomputing june 1721 2007 seattle washington bruno diniz dorgival guedes wagner meira jr ricardo bianchini limiting power consumption main memory acm sigarch computer architecture news v35 n2 may 2007 enric gibert jesus sanchez antonio gonzalez distributed data cache designs clustered vliw processors ieee transactions computers v54 n10 p12271241 october 2005 xipeng shen yutao zhong chen ding predicting locality phases dynamic memory optimization journal parallel distributed computing v67 n7 p783796 july 2007 ruchira sasanka manlap li sarita v adve yenkuang chen eric debes alp efficient support levels parallelism complex media applications acm transactions architecture code optimization taco v4 n1 p3es march 2007 martin hirzel data layouts objectoriented programs acm sigmetrics performance evaluation review v35 n1 june 2007