parallel solver largescale markov chains consider parallel computation stationary probability distribution vector ergodic markov chains large state spaces preconditioned krylov subspace methods parallel preconditioner obtained explicit approximation factorized form particular generalized inverse generator matrix markov process graph partitioning used parallelize whole algorithm resulting twolevel methodconditions guarantee existence preconditioner given results parallel implementation presented results indicate method well suited problems generator matrix explicitly formed stored b introduction discrete markov chains large state spaces arise many applications including instance reliability modeling queueing network analysis large scale economic modeling computer system performance evaluation stationary probability distribution vector ergodic markov process n theta n transition probability matrix p unique 1 theta n vector satisfies letting computation stationary vector reduces finding nontrivial solution homogeneous linear system 0 ergodicity assumption means p therefore irreducible perronfrobenius theory 9 guarantees rank n gamma 1 onedimensional null space n spanned vector x positive entries upon normalization 1 norm stationary distribution vector markov process coefficient matrix singular mmatrix called generator markov process 3 matrix nonsymmetric although sometimes structurally symmetric see 26 good introduction markov chains numerical solution due large number n states typical many realworld applica tions increasing interest recent years developing parallel algorithms markov chain computations see 2 5 10 17 19 24 attention far focused linear stationary iterative meth ods including block versions jacobi gaussseidel 10 19 24 iterative aggregationdisaggregation schemes specifically tailored stochastic matrices 10 17 contrast little work done parallel preconditioned krylov subspace methods partial exceptions 5 symmetrizable stationary iteration cimminos method accelerated using conjugate gradients cray t3d 19 outofcore parallel implementation conjugate gradient squared precondi tioning used solve large markov models 50 million states suitability preconditioned krylov subspace methods solving markov models demonstrated eg 25 although discussion parallelization issues given paper investigate use parallel preconditioned iterative method large sparse linear systems context markov chain com strictly speaking generator matrix work instead q conform familiar notation numerical linear algebra putations preconditioning strategy twolevel method based sparse approximate inverses first introduced 3 however due singularity generator matrix applicability approximate inverse techniques context obvious indeed possible consequence fact singular mmatrix paper organized follows section 2 discuss problem preconditioning singular equations general establish link standard preconditioners generalized inverses sections 35 devoted ainv preconditioning markov chain problems including discussion parallel implementation theoretical analysis existence preconditioner numerical tests reported section 6 conclusions presented section 7 preconditioning markov chain problems markov chain context preconditioning typically amounts finding easily invertible nonsingular matrix preconditioner good approximation krylov subspace method used solve preconditioning notice even singular preconditioner must nonsingular change solution set ie null space preconditioners generated means splittings n used stationary iterative methods including jacobi gauss seidel sor block versions schemes see 26 also class popular incomplete lu ilu factorization preconditioners ilutype methods successfully applied markov chain problems saad 25 sequential environment existence incomplete factorizations nonsingular mmatrices already proved 20 investigation existence ilu factorizations singular mmatrices found 11 incomplete factorization methods work quite well wide range problems easily implemented parallel computers reasons much effort put recent years developing alternative preconditioning strategies natural parallelism comparable ilu methods terms robustness convergence rates work resulted several new techniques known sparse approximate inverse preconditioners see 7 recent survey extensive references sparse approximate inverse preconditioners based directly approximating inverse coefficient matrix sparse matrix g gamma1 application preconditioner requires matrixvector products easily parallelized techniques applied almost exclusively nonsingular systems equations b exception seems 13 spai preconditioner 16 used connection fast wavelet transform techniques singular systems stemming discretizations neumann problem poissons equation application approximate inverse techniques singular case raises several interesting theoretical practical questions inverse exist clear matrix g approximation presumably generalized inverse one note question asked gamma1 preconditioner 26 page 143 stated gamma1 approximation group generalized inverse ilu factorization u implicitly yields approximation see interpretation entirely correct somewhat misleading group inverse see 12 one many possible generalized inverses well known 21 group inverse plays important role modern theory finite markov chains however seldom used computational tool part computation requires knowledge stationary distribution vector turns different preconditioners result implicitly explicitly approximations gamma1 different generalized inverses typically group inverse let us consider ilu preconditioning first ntheta irreducible singular mmatrix ldu factorization l u unit lower upper triangular matrices respectively diagonal matrix rank see 26 notice l u nonsingular mmatrices particular l gamma1 u gamma1 nonnegative entries define matrix easily verified gamma satisfies first two penroses four conditions 12 first identity states gamma inner inverse second gamma outer inverse generalized inverse satisfying two conditions called 1 2inverse innerouter inverse another term found literature reflexive inverse see 12 gamma necessarily satisfy third fourth penrose conditions moorepenrose pseudoinverse general obviously 1 2inverse kind generalized inverse nonunique indeed infinitely many 1 2inverses general pair r n subspaces ir n complements null space range respectively uniquely determines 1 2inverse gnr null space n gnr range rgnr see 12 case gamma readily verified denotes ith unit basis vector ir n easy see r complementary n n complementary ra pseudoinverse corresponds 1 2inverse gamma also different group inverse general seen fact general aa gamma 6 gamma whereas group inverse always satisfies aa also notice singular irreducible mmatrix 1 2inverse nonnegative trix true general either group moorepenrose let u incomplete ldu factorization unit lower triangular u u unit upper triangular diagonal matrix positive entries main diagonal clearly hence ilu factorization yields implicit approximation gamma rather also seen fact always nonnegative true straightfoward check gamma oblique projector onto along n aa gamma oblique projector onto along g therefore gamma eigenvalues 0 multiplicity 1 1 multiplicity likewise aa gamma hence makes good sense construct preconditioners based approximating gamma either implicitly explicitly since case eigenvalues preconditioned matrix clustered around 1 next consider approximate inverse preconditioner ainv see 4 6 method based observation z w matrices whose columns abiorthogonal w diagonal matrix leading principal minors except possibly last one nonzeros z w obtained applying generalized gramschmidt process unit basis vectors e 1 case z w unit upper triangular follows uniqueness ldu factorization u gamma1 ldu ldu factorization diagonal matrix factorizations approximate inverse factorized form w obtained dropping small entries course generalized gramschmidt process similar ilu incomplete inverse factorization guaranteed exist nonsingular matrices 4 see next section singular mmatrix case either case w nonnegative matrix hence preconditioner interpreted direct explicit approximation 1 2inverse gamma lastly take look sparse approximate inverse techniques based frobenius norm minimization see eg 16 14 class meth ods approximate inverse g computed minimizing functional subject sparsity constraints jj delta jj f denotes frobenius matrix norm sparsity constraints could imposed priori dynamically course algorithm either case natural ask kind generalized inverse approximated g singular matrix shown moorepenrose pseudoinverse matrix smallest frobenius norm minimizes jji gamma axjj f see instance 23 page 428 hence singular case spai preconditioner seen sparse approximate moorepenrose inverse generally different approximate 1 2inverses obtained either ilu ainv instance spai produce nonnegative preconditioner general next section restrict attention ainv preconditioner application markov chain problems 3 ainv method singular matrices ainv preconditioner 4 6 based abiorthogonalization generalized gramschmidt process applied unit basis vectors e n generalization standard inner product replaced bilinear form hx ay process well defined exact arith metic leading principal minors nonzero otherwise form pivoting row andor column interchanges may needed nonsingular mmatrix leading principal minors positive process well defined need pivoting perfectly analogous lu factorization indeed exact arithmetic abiorthogonalization process computes inverses triangular factors singular irreducible mmatrix leading principal minors except nth one determinant positive process still completed order obtain sparse preconditioner entries fillins inverse factors z w less given drop tolerance magnitude dropped course computation resulting incomplete process stability incomplete process mmatrices analyzed 4 particular denotes ith pivot ie ith diagonal entry incomplete process proposition 31 4 mmatrix pivot breakdown occur exactly argument applies case irreducible singular mmatrix case breakdown first steps incomplete abiorthogonalization process since first leading principal minors positive pivots incomplete process cannot smaller exact ones even nth pivot happened zero could simply replaced positive number order nonsingular preconditioner argument 4 shows n must nonnegative number extremely unlikely exactly zero incomplete process another way guarantee nonsingularity preconditioner perturb matrix adding small positive quantity last diagonal entry makes matrix nonsingular mmatrix incomplete abiorthogonalization process applied slightly perturbed matrix yield well defined nonsingular preconditioner practice ever perturbation necessary since dropping factors typically equivalent effect see theorem 3 ainv preconditioner extensively tested variety symmetric nonsymmetric problems conjunction standard krylov subspace methods like conjugate gradients symmetric positive definite matrices gmres bicgstab tfqmr unsymmetric problems preconditioner found comparable ilu methods terms robustness rates convergence ilu methods somewhat faster average sequential computers main advantage ainv ilutype methods application within iterative process requires matrixvector multiplies much easier vectorize parallelize triangular solves unfortunately computation preconditioner using incomplete biorthogonalization process inherently sequential one possible solution problem adopted 8 compute preconditioner sequentially one processor distribute approximate inverse factors among processors way minimizes communication costs achieving good load balancing approach justified applications like considered 8 matrices small enough fit local memory one processor preconditioner reused number times case time computing preconditioner negligible relative overall costs markov chain setting however preconditioner cannot reused general imperative setup costs minimized furthermore markov chain problems large desirable able compute preconditioner parallel 4 parallel preconditioner present section describe achieve fully parallel precon ditioner strategy used parallelize preconditioner construction based use graph partitioning approach first proposed 3 context solving nonsingular linear systems arising discretization partial differential equations idea illustrated follows p processors available graph partitioning used decompose adjacency graph associated sparse matrix structurally symmetric p subgraphs roughly equal size way number edge cuts approximately minimized nodes connected cut edges removed subgraphs put separator set numbering nodes separator set last symmetric permutation q aq obtained permuted matrix following structure diagonal blocks correspond interior nodes graph decomposition approximately order offdiagonal blocks connections subgraphs diagonal block connections nodes separator set order equal cardinality separator set kept small possible note irreducibility assumption block nonsingular mmatrix ldu factorizations ldu ldu factorization q aq easy see inverse unit lower triangular factor schur complement matrix next section show singular irreducible mmatrix hence well defined ldu factorization likewise u inverse unit upper triangular factor important observe l gamma1 u gamma1 preserve good deal sparsity since fillin occur within nonzero blocks matrix simply defined note diagonal entries positive except last one zero 1 2inverse gamma defined obvious way hence write generally dense generalized inverse q aq gamma product sparse matrices l practice however inverse factors l gamma1 u gamma1 contain many nonzeros since interested computing preconditioner need compute sparse approximations l gamma1 u gamma1 accomplished follows graph partitioning matrix distributed processor p holds one processors marked p also hold processor computes sparse approximate inverse factors w using ainv algorithm done processor computes product point computation proceeds parallel communication next step accumulation approximate schur complement accumulation done steps fanin across processors next section show although exact schur complement singular approximate schur complement nonsingular mmatrix rather mild conditions soon computed processor p computes factorized sparse approximate using ainv algorithm sequential bottleneck explains size separator set must kept small approximate inverse factors computed broadcast remaining processors actually preconditioner application implemented way needs broadcast notice matrixvector products required application preconditioner need explicitly way factorized sparse approximate 1 2inverse q aq obtained twolevel preconditioner sense computation preconditioner involves two phases first phase sparse approximate inverses diagonal blocks computed second phase sparse approximate inverse approximate schur complement computed second step preconditioner would reduce block jacobi method inexact block solves terminology domain decomposition methods additive schwarz inexact solves overlap well known fixed problem size rate convergence preconditioner tends deteriorate number blocks subdomains grows hence assuming block assigned processor parallel computer method would scalable however approximate schur complement phase provides global exchange information across processors acting coarse grid correction coarse grid nodes interface nodes ie correspond vertices separator set see prevents number iterations growing number processors grows long cardinality separator set small compared cardinality subdomains subgraphs algorithm scalable terms parallel efficiency indeed case application preconditioner step krylov subspace method like gmres bicgstab easily implemented parallel relativeley little communication needed 5 approximate schur complement section investigate existence approximate 1 2inverse generator matrix key role played approximate schur complement first briefly review situation case nonsingular mmatrix assume partitioned 21 22 wellknown schur complement also nonsingular mmatrix see eg 1 moreover true approximate schur complement provided x 11 gamma1 inequalities hold componentwise see 1 page 264 singular case situation slightly complicated following examine basic properties exact schur complement singular irreducible mmatrix corresponding ergodic markov chain recall irreducible rowstochastic transition probability matrix irreducible rowstochastic matrix partitioned assume 21 22 schur complement 11 singular irreducible mmatrix onedimensional null space proof consider stochastic complement 22 sigma p 22 note gamma p 11 invertible since p irreducible theory developed 22 know sigma row stochastic irreducible since p consider schur complement 11 22 clearly irreducible singular mmatrix follows perronfrobenius theorem onedimensional null space 2 previous lemma especially useful cases exact schur complement used context preconditioning often important know properties approximate schur complements shown previous section graph partitioning induces reordering block partitioning matrix form 1 interested properties approximate schur complement obtained approximating inverses diagonal blocks ainv particular interested conditions guarantee nonsingular mmatrix case ainv algorithm safely applied resulting well defined preconditioner begin lemma recall zmatrix matrix nonpositive offdiagonal entries 9 lemma 2 let singular irreducible mmatrix let c c 6 nonsingular mmatrix proof since singular mmatrix aeb denotes spectral radius b let zmatrix b c gamma therefore write modified distinguish two following simple cases assume first nonsingular mmatrix since written aebi gamma b gamma c c last inequality follows irreducibility b properties nonnegative matrices see 9 page 27 cor 15 b assume hand b note assumption least one diagonal entries ii must positive let denote largest positive diagonal entry since b ffii irreducible similar previous case follows nonsingular mmatrix finally 6 c 6 result follows combining two previous arguments 2 context parallel preconditioner lemma says inexactness approximate inverses diagonal blocks results approximate schur complement still zmatrix furthermore nonnegative nonzero nonsingular mmatrix following theorem states sufficient conditions nonsingularity approximate schur complement b b lambdaj denote ith row jth column matrix b respectively theorem 3 let 21 22 singular irreducible mmatrix 11 2 ir mthetam assume 11 approximation gamma1 11 11 furthermore assume exist indices following three conditions satisfied approximate schur complement 12 nonsingular mmatrix proof lemma 1 know exact schur complement 11 singular irreducible mmatrix note approximate schur complement induced approximation 11 block gamma1 11 exact schur complement related follows gamma1a c since 21 12 gamma1 zmatrix definition assumption gamma16 gamma1and exists least one nonzero entry ff ij 11 equal fact strictly less corresponding entry gamma1 11 see corresponding row 21 column j 12 nonzero nonzero entry c result follows lemma 2 2 let us apply results preconditioner described previous section case 11 block diagonal ainv algorithm used approximate inverse diagonal block separately parallel approximate schur complement 2 result subtracting p terms refer terms schur complement updates one updates nonnegative approximates exact update c entrywise nonnegative ordering since see 6 theorem 3 says long least one updates entry strictly less corresponding entry c approximate schur complement nonsingular mmatrix practice conditions satisfied result dropping approximate inversion diagonal blocks nevertheless desirable rigorous conditions ensure nonsingularity following theorem gives sufficient condition nonsingular approximate schur complement consequence dropping ainv namely specifies conditions dropping forces nonsingular note conditions theorem apply global matrix 1 since 11 block diagonal therefore reducible however applied individual schur complement update corresponding diagonal block irreducible making result fairly realistic theorem 4 let 11 2 ir mthetam singular mmatrix 21 22 irreducible let lu factorization 11 assume column l 11 except last one row u 11 except last one least one nonzero entry addition diagonal one denote z 11 11 factorized sparse approximate inverse 11 obtained ainv algorithm approximate schur complement nonsingular mmatrix provided z 11 6 z 11 proof first note two conditions nonzero entries l 11 u 11 implied similar conditions entries lower upper triangular parts 11 namely easy see barring fortuitous cancellation 0 trilb triub denote lower upper triangular part matrix b respectively conditions easier check weaker ones triangular factors 11 conditions 3 4 imply path graph l 11 path graph u 11 matrix irreducible 12 6 21 6 means exist indices rs 6 0 existence previously mentioned paths implies w 11 z 11 exact inverse factors 11 approximate inverse factors ainv algorithm satisfy 6 z 11 hence conditions theorem 3 satisfied result proved 2 instructive consider two extreme cases 11 diagonal approximate schur complement necessarily equal exact one therefore singular case course conditions last theorem violated hand 11 irreducible tridiagonal inverse factors completely dense last theorem enough drop single entry inverse factor obtain nonsingular approximate schur complement purpose theory developed shed light observed robustness proposed preconditioner rather serve practical tool words seem necessary check conditions advance indeed thanks dropping approximate schur complement always found nonsingular mmatrix actual computations 6 numerical experiments section report results obtained parallel implementation preconditioner several markov chain problems underlying krylov subspace method bicgstab 27 found perform well markov chains 15 fortran implementation uses mpi dynamic memory allocation package metis 18 used graph partitioning working graph whenever structurally symmetric test problems arise real markov chain applications provided dayar matrices used 15 compare different methods sequential environment description test problems provided table 1 n problem size nnz number nonzeros matrix test problems structurally nonsymmetric except ncd mutex matrices unstructured tables 211 contain test results runs performed sgi origin 2000 los alamos national laboratory using 64 processors except matrices leaky ncd 2d performed origin 2000 helsinki university technology using 8 processors cases initial guess constant nonzero vector similar results obtained randomly generated initial guess tables ptime denotes time compute preconditioner pdensity ratio number nonzeros preconditioner number nonzeros matrix denotes number iterations needed reduce 2 norm initial residual eight orders magnitude ittime time perform iterations tottime sum ptime ittime timings seconds furthermore sepsize cardinality information test problems matrix n nnz application hard 20301 140504 complete buffer sharing atm networks multiplexing model leaky bucket 2d 16641 66049 twodimensional markov chain model telecom 20491 101041 telecommunication model ncd 23426 156026 ncd queueing network mutex 39203 563491 resource sharing model qn 104625 593115 queueing network separator set ie order schur complement matrix avg dom average number vertices subdomain subgraph graph partitioning problem drop tolerance ainv algorithm levels preconditioner approximate inversion approximate inversion approximate schur complement except mutex problem see tables present results matrix hard using three different values drop tolerance ainv algorithm seen changing value changes density preconditioner number itera tions however total timings scarcely affected especially least 8 processors used see 8 similar observation different con text also clear runs good speedups obtained long size separator set small compared average subdomain size soon separator set comparable size average subdomain larger sequential bottleneck represented schur complement part computation begins dominate performance deteriorates number iterations remains roughly constant slight downward trend number processors grows due influence approximate schur complement problem also solved using bicgstab diagonal precondi tioning required approximately 700 iterations 164 seconds one processor implemented parallel method would probably give results slightly worse obtained ainv similar observation applies matrices qn mutex hand diagonally preconditioned bicgstab converge telecom problem hence ainv robust approach furthermore ability reduce number iterations therefore total number inner products advantage distributed memory machines inner products incur additional penalty due need global communication matrix hard ptime 235 119 065 040 034 pdensity 621 590 567 514 452 ittime 943 291 142 099 088 tottime 118 410 207 139 122 sepsize 156 321 540 900 1346 avgdom 10073 5155 2470 1213 592 table matrix hard ptime 119 060 035 022 021 pdensity 310 302 295 278 252 106 109 99 98 97 ittime 685 290 159 105 119 tottime 804 350 194 127 140 table matrix hard ptime 073 038 022 015 014 pdensity 139 136 134 128 119 170 167 159 151 153 ittime 603 252 150 130 164 tottime 676 290 172 145 178 results matrices leaky 2d reported tables 5 6 two matrices rather small 8 processors used note speedups better 2d leaky also notice preconditioner sparse leaky rather dense 2d tables 7 8 refer telecom test problem found small values consequently dense preconditioners necessary order achieve convergence reasonable number iterations problem completely different matrices arising solution elliptic partial differential equations notice fairly small size sep matrix leaky ptime 026 017 009 pdensity 134 134 132 ittime 139 084 062 tottime 165 101 071 sepsize 48 144 335 avgdom 4105 2028 990 table matrix 2d ptime 038 016 009 pdensity 860 712 654 33 36 37 ittime 146 056 038 tottime 184 072 047 sepsize 129 308 491 avgdom 8256 4083 2018 table matrix telecom ptime 249 100 335 121 105 137 pdensity 167 116 70 44 28 11 12 14 13 12 12 ittime 365 140 604 096 038 043 tottime 614 240 939 217 143 180 sepsize 34 97 220 471 989 1603 avgdom 10229 5099 2534 1251 609 295 arator set causes density preconditioner decrease fast number processors corresponding subdomains grows result speedups quite good even superlinear 32 processors sufficiently high number processors density preconditioner matrix telecom ptime 704 378 137 061 049 056 pdensity ittime 594 270 128 218 136 171 tottime 664 308 142 279 186 227 table matrix ptime 142 069 031 pdensity 413 265 190 292 288 285 ittime 170 845 638 tottime 184 914 669 sepsize 3911 6521 12932 avgdom 9758 4226 1379 table matrix ptime 119 040 010 pdensity 014 014 014 ittime 164 119 151 tottime 283 159 161 sepsize 13476 17749 20654 avgdom 12864 5363 2319 comes acceptable convergence rate comparable obtained dense preconditioner small number processors tables results matrices ncd mutex respectively first matrix see separator set larger average subdomain already nevertheless possible use effectively 8 processors matrix mutex exhibits behavior radically different matrix qn ptime 412 237 226 282 pdensity 127 123 118 114 ittime 134 633 458 568 tottime 175 870 684 850 sepsize 2879 6579 13316 20261 avgdom 50873 24511 11414 5273 matrices arising pdes two three space dimensions separator set huge already 2 due fact problem state space graph high dimensionality leading unfavorable surfacetovolume ratio graph partitioning order solve problem use two different values two levels ainv subdomain level used forming approximate schur complement dropped everything outside main diagonal resulting diagonal spite convergence rapid nevertheless pay use processors table 11 report results largest example data set qn model consists network three queues analogous threedimensional problem fairly rapid growth separator set pay use processors test problems considered far although realistic relatively small hence difficult make efficient use 16 processors partial exceptions matrices hard telecom test scalability proposed solver larger problems generated simple reliability problems analogous used 2 5 see also 26 page 135 problems closed form solution table 12 show timing results running 100 preconditioned bicgstab iterations reliability problem entries problem sufficiently large show good scalability algorithm processors conclude section numerical experiments noting virtually runs preconditioner construction time quite modest total solution time dominated cost iterative phase reliability model ptime 953 486 249 131 090 090 pdensity 405 402 397 390 380 370 ittime 1388 705 372 168 947 796 tottime 1483 754 397 181 104 886 sepsize 542 1229 2186 3268 4919 7336 avgdom 124729 62193 30977 15421 7659 3792 conclusions investigated use parallel preconditioner krylov subspace methods context markov chain problems preconditioner direct approximation factorized form 1 2inverse generator matrix based abiorthogonalization process parallelization achieved graph partitioning although approaches also possible existence preconditioner justified theoretically numerical experiments parallel computer carried order assess effectiveness scalability proposed technique numerical tests indicate preconditioner construction costs modest good scalability possible provided amount work per processor sufficiently large compared size separator set method appears well suited problems generator explicitly formed stored parallelization based graph partitioning usually effective possible exception problems state space high dimensionality ie large state descriptor set problems different parallelization strategy needed order achieve scalability implementation acknowledgements would like thank professors gutknecht w schonauer kind invitation take part commemoration friend colleague rudiger weiss indebted tugrul dayar providing test matrices used numerical experiments useful information problems well comments early version paper thanks also carl meyer valuable input generalized inverses r arithmetic mean method finding stationary vector markov chains sparse approximate inverse preconditioner conjugate gradient method parallel block projection method cimmino type finite markov chains sparse approximate inverse preconditioner nonsymmetric linear systems comparative study sparse approximate inverse preconditioners approximate inverse preconditioning parallel solution sparse eigenproblems nonnegative matrices mathematical sciences academic press distributed steady state analysis using kronecker algebra incomplete factorization singular mmatrices generalized inverses linear transformations pitman publishing ltd fast wavelet iterative solvers applied neumann problem priory sparsity patterns parallel sparse approximate inverse preconditioners comparison partitioning techniques twolevel iterative solvers large parallel preconditioning sparse approximate inverses asynchronous iterations solution markov systems fast high quality multilevel scheme partitioning irregular graphs distributed diskbased solution techniques large markov models iterative solution method linear systems coefficient matrix symmetric mmatrix role group generalized inverse theory finite markov chains theory nearly reducible systems matrix analysis applied linear algebra siam experimental studies parallel iterative solutions markov chains block partitions preconditioned krylov subspace methods numerical solution markov chains introduction numerical solution markov chains princeton university press bicgstab fast smoothly converging variant bicg solution nonsymmetric linear systems tr incomplete factorization singular mmatrices stochastic complementation uncoupling markov chains theory nearly reducible systems bicgstab fast smoothly converging variant bicg solution nonsymmetric linear systems iterative solution methods sparse approximate inverse preconditioner conjugate gradient method parallel preconditioning sparse approximate inverses sparse approximate inverse preconditioner nonsymmetric linear systems fast high quality multilevel scheme partitioning irregular graphs comparative study sparse approximate inverse preconditioners matrix analysis applied linear algebra comparison partitioning techniques twolevel iterative solvers large sparse markov chains priori sparsity patterns parallel sparse approximate inverse preconditioners ctr aliakbar montazer haghighi dimitar p mishev parallel priority queueing system finite buffers journal parallel distributed computing v66 n3 p379392 march 2006 ilias g maglogiannis elias p zafiropoulos agapios n platis george gravvanis computing success factors consistent acquisition recognition objects color digital images explicit preconditioning journal supercomputing v30 n2 p179198 november 2004 nicholas j dingle peter g harrison william j knottenbelt uniformization hypergraph partitioning distributed computation response time densities large markov models journal parallel distributed computing v64 n8 p908920 august 2004 michele benzi preconditioning techniques large linear systems survey journal computational physics v182 n2 p418477 november 2002