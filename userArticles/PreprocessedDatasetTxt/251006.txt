accuracy metalearning scalable data mining paper describe general approach scaling data mining applications come call metalearning metalearning refers general strategy seeks learn combine number separate learning processes intelligent fashion desire metalearning architecture exhibits two key behaviors first metalearning strategy must produce accurate final classification system means metalearning architecture must produce final outcome least accurate conventional learning algorithm applied available data second must fast relative individual sequential learning algorithm applied massive databases examples operate reasonable amount time paper focussed primarily issues related accuracy efficacy metalearning general strategy number empirical results presented demonstrating metalearning technically feasible widearea network computing environments b introduction many believe poised radical shift way learn work amount new knowledge acquire coming age high performance network computing widely available data highways transform information age knowledge age providing new opportunities defense commerce education science sharing utilizing information however new technological capability comes along number hard technical problems many centered issue scale perhaps obvious massive amounts data information available anywhere anytime enables many new opportunities acquire new knowledge field data mining studies precisely achieved efficient transparent fashion one means acquiring new knowledge databases apply various machine learning algorithms compute descriptive representations data well patterns may exhibited data field machine learning made substantial progress years number algorithms popularized applied host applications diverse fields thus may simply apply current generation learning algorithms large databases wait response however question long might wait indeed current generation machine learning algorithms scale tasks common today include thousands data items new learning tasks encompassing much two orders magnitude data physically distributed furthermore many existing learning algorithms require data resident main memory clearly untenable many realistic databases certain cases data inherently distributed cannot localized one machine variety practical reasons situations infeasible inspect data one processing site compute one primary global classifier call problem learning useful new knowledge large inherently distributed databases scaling problem machine learning approach solve scaling problem execute number learning processes implemented distinct serial program number data subsets data reduction technique parallel eg network separate processing sites integrate collective results process call metalearning 6 without integration discuss later individual results generated data subsets far desired metalearning serves means gluing multiple knowledge sources together note interest general metalearning approach independent underlying learning algorithms may employed furthermore independent computing platform used thus metalearning approach intended scalable well portable extensible however may able guarantee accuracy final result good individual learning algorithm applied entire data set since considerable amount information may accessible separate learning processes primary issue study paper 2 related work relational database context typical data mining task explain predict value attribute data given collection tuples known attribute values existing relation attribute values drawn domain thus treated training data learning algorithm computes logical expression concept description classifier later used predict value desired attribute test datum whose desired attribute value unknown details approach discussed first summarize closely related work others improving accuracy learning algorithms applied large amounts data machine learning researchers clearly desire accurate learning algorithms one recent approach focussed integrating means multiple strategies multiple algorithms research concentrated methods improve existing algorithm using algorithm generate purposely biased distributions training data notable work area due schapire 17 schapire proves theoretical pac probabilistic approximately correct learning model 20 boosting technique improve weak learner achieve arbitrary high accuracy researchers proposed implementing learning systems integrating fashion number different algorithms boost overall accuracy basic notion behind integration complement different underlying learning strategies embodied different learning algorithms effectively reducing space incorrect classifications learned concept mainly two strategies may consider integrating different learning strategies one strategy increase amount knowledge learning system example work reported integrating inductive explanationbased learning 12 explanationbased techniques integrated provide appropriate domain knowledge complements inductive learning knowledge poor approach requires complicated new algorithm implements strategies learning single system another strategy loosely integrate number different inductive learning algorithms integrating collective output concepts fashion techniques described later evaluated empirical results many simpler techniques aim combine multiple evidence singular prediction based voting first scheme examine simple voting based predictions different base classifiers final prediction chosen classification plurality votes variation simple voting weighted voting classifier associated weight determined accurate classifier performs validation set validation set set examples randomly selected available data since classifier trained one subset examples subsets contribute validation set provide measure predictiveness prediction weighted classifiers assigned weight weights classification summed final prediction classification weight littlestone warmuth 14 propose several weighted majority algorithms integrating different classifiers work classifiers different prediction algorithms necessarily learned training data used calculating weights integrating algorithms similar weighted voting method described main difference weights obtained basic algorithm called wm associates learned classifier initial weight example training set processed classifiers final prediction example generated weighted voting final prediction wrong weights classifiers whose predictions incorrect multiplied fixed discount fi 0 fi 1 decreases contribution final predictions variation basic wm algorithm called wml allow weights discounted beyond predefined limit xu et al 22 developed method integrating predictions multiple classifiers based bayesian formalism belief function derived equation simplified belclass classifiers x instance classifier k x classification instance x predicted classifier k final prediction class j belclass largest among classes experiments reported estimate conditional probabilities frequencies generated validation set interesting approach loosely combine learning programs learn combine independently learned concepts stolfo et al 18 propose learning rules training weighted voting schemes merging different phoneme output representations multiple trained speech recognizers wolpert 21 presents theory stacked generalization combine several classifiers indeed work closest mean metalearning describe later several level classifiers first learned training set predictions made classifiers training set correct classifications form training set next level level 1 classifier instance classified level classifiers first make predictions instance predictions presented level 1 classifier makes final prediction zhang et als 23 work utilizes similar approach learn combiner based predictions made three different classifiers latter ideas suggest general approach may exhibit favorable scaling characteristics discuss later researchers investigate different characteristics successful integration multiple classifiers ali pazzani 1 empirically show classifiers fewer uncorrelated errors reduce error rate integrated model krogh vedelsby 13 prove overall error rate reduced classifiers generating highly independent predictions next going detail metalearning approach 3 metalearning metalearning loosely defined learning metaknowledge learned knowl edge work concentrate learning output concept learning systems case metalearning means learning predictions classifiers common training data thus interested output classifiers internal structure strategies learning algorithms selves moreover several schemes define training data presented learning algorithms initially also available metalearner certain circumstances 31 computing initial base classifiers consider two distinct phases metalearning data reduction applied two different fashions first phase base level classifiers computed 5from initial input database thus initial input database n j j divided random unbiased subsets training data roughly size ns subsets input learning algorithms executed concurrently second phase metalearning number computed base classifiers may similarly partition metadata across subsets classifiers integrated smaller groups however may compose distributions metalevel training data purposefully biased classifications underlying base classifiers ie filter data according predictions precomputed classifiers however several important considerations must concerned bias introduced particular distribution formed data reduction method example data partitioned class attribute ie target concept inductive learning resultant classifiers would specific single class others may poor strategy least two important reasons first scheme important information distinguishes two classes available learning algorithm thus nearmisses counterfactuals available learning algorithm may lead overly general inductively inferred descriptions data putting heavier burden metalearning correct mistakes base classifiers indeed many discrimination based learning algorithms require negative training examples compute useful results secondly independent subsets training data may still large process efficiently example large n relatively small number classes c quantity nc may large num ber implies attributes data must participate data reduction scheme distribute computation must concerned choosing good distributions minimize potential severe bias skew may lead faulty misleading classifiers importance choosing right attributes resultant impact learning cannot understated random selection partitioned data sets uniform distribution classes perhaps sensible solution may attempt maintain frequency distribution class attribute partition represents good smaller model entire training set otherwise totally random selection strategy may result absence classes wish discriminate among training subsets several experiments conducted reported explore issues 32 integrating base classifiers since different learning algorithms employ different knowledge representations search heuristics different search spaces may explored hence potentially diverse results obtained mitchell 15 refers phenomenon inductive bias outcome running algorithm biased towards certain outcome furthermore different partitions data set different statistical characteristics performance single learning algorithm might differ substantially partitions observations imply great care must taken designing appropriate distributed metalearning architecture number issues explored paper precisely integrate number separately learned classifiers bayesian statistics theory provides one possible approach combining several learned classifiers based upon statistics behavior classifiers training set given set classifiers c feature vector x seek compute class label x bayes theorem suggests optimal strategy follows probability c predicts correctly ie probability true probability x class given c course makes sense probabilities indeed known classifiers probabilistic categorical best estimate calculate appropriate statistics observing behavior classifier training set approximation actual probabilities may quite inaccurate furthermore bayes theorem would optimal knew possible classifiers happen compute information however provides statistics classifiers behavior respect training set information classifiers related example learning two classifiers rarely agree predicting class label meaning one classifier predicts might much predictive value eg combined third classifier merely knowing two classifiers predict equal probability view purely bayesian approach baseline use methods derived approach bayes 9 bayesianbelief 22 comparative purposes experiments reported later many approaches might imagine based upon learning relationships classifiers manner learn relationship classifiers learn new classifier metalevel classifier whose input set predictions two classifiers common data latter view call metalearning following sections detail metalearning arbitration combining cases variety inductive learning algorithms employed generate appropriate metaclassifiers strategy treated great detail including variety training data distributions generated scheme number important questions poorly understood substantial experimental evidence suggests directions future exploration ffl metalearning data partitions maintain boost accuracy single global classifier ffl voting bayesian techniques compare metalearning accuracy ffl arbiters compare combiners accuracy ffl metalearned classifier may treated base classifier thus might hierarchically metalearned classifiers perform better single layered meta learned architecture ffl much training data distribution arbiter combiner provided order produce accurate results substantial number exploratory evaluations completed reported number papers suggesting answers questions several results repeated completeness exposition discovered experimentation three interesting behaviors exhibited various metalearning strategies warrant elaboration demonstrate certain circumstances metalearning architecture learn effectively fraction total available information one site accuracy boosted global classifier trained available data maximal parallelism effectively exploited metalearning disjoint data partitions without substantial loss accuracy 8 results suggest strongly field test techniques real world network computing environment eg database server sites web technically feasible also important next step development ideas next section present metalearning arbitration combining following present hierarchical metalearning devote considerable depth topics demonstrate range issues involved attempting scale machine learning systems 4 metalearning arbitration combining distinguish base classifiers arbiterscombiners follows base classifier outcome applying learning algorithm directly raw training data base classifier program given test datum provides prediction unknown class arbiter combiner detailed program generated learning algorithm trained predictions produced set base classifiers raw training data arbitercombiner also classifier hence arbiters combiners computed set predictions arbiterscombiners 41 arbiter strategies arbiter 7 learned learning algorithm arbitrate among predictions generated different base classifiers arbiter together arbitration rule decides final classification outcome based upon base predictions figure 1 instance final prediction arbiter arbitration rule arbiters prediction combiner instance final prediction figure 1 arbiter combiner two classifiers depicts final prediction made predictions input two base classifiers single arbiter let x instance whose classification seek c 1 x c 2 x c k x predicted classifications x k base classifiers c 1 c 2 c k ax classification x predicted arbiter one arbitration rule studied reported follows ffl return class plurality occurrences c 1 x c 2 x c k x ax preference given arbiters choice case tie detail arbiter learned training set arbiter generated picking examples validation set e validation set e randomly selected available data prior onset arbiter training choice examples selected e dictated selection rule purposefully biases arbiter training data one version selection rule studied follows ffl instance e selected none classes k base predictions gathers majority classification k2 votes ie majorityc 1 purpose rule choose data sense confusing ie majority classifiers agree data classified figure 2 provides abstract example three base classifiers trained data three classes b c later discussion refer set arbiter training data disagreements training set formed arbiter generated learning algorithm used train base classifiers together arbitration rule learned arbiter resolves conflicts among classifiers necessary class attribute vector example base classifiers predictions classx attrvec 1 x 1 b c c attrvec 3 x 3 c b training set arbiter scheme instance class attribute vector training set classcombiner scheme instance class attribute vector 1 figure 2 sample training sets generated combiner arbiter strategies 42 combiner strategies combiner 5 strategy predictions learned base classifiers training set form basis metalearners training set composition rule varies different schemes determines content training examples metalearner examples metalearner generates meta classifier call combiner classifying instance base classifiers first generate predictions based composition rule new instance generated predictions classified combiner see figure 1 aim strategy coalesce predictions base classifiers learning relationship predictions correct prediction combiner computes prediction may entirely different proposed base classifier whereas arbiter chooses one predictions base classifiers arbiter experimented two schemes composition rule first predic tions c 1 x c 2 x c k x example x validation set exam ples e generated k base classifiers predicted classifications used form new set metalevel training instances used input learning algorithm computes combiner manner computed varies defined following definitions classx attribute vectorx denote correct classification attribute vector example x specified validation set e 1 return metalevel training instances correct classification pre dictions ie scheme also used wolpert 21 reference scheme denoted classcombiner 2 return metalevel training instances classcombiner addition attribute vectors ie attribute scheme denoted classattribute combiner note difference training data arbiters computed form distinguished biased subset data selected input database used train base classifiers combiners however trained predicted classifications data generated base classifiers well data 43 issues several issues arise metalearning strategies detailed follows number size training subsets number initially partitioned training data subsets largely depends number processors available inherent distribution data across multiple platforms possibly mobile periodically disconnected total size available training set complexity learning algorithms available resources processing sites naturally defines upper bound size subset number subsets exceeds number processors available processor simulate work multiple ones serially executing task processor another consideration desired accuracy wish achieve see experimental results may tradeoff number subsets final accuracy metalearning system moreover size subset cannot small sufficient data must available learning process produce effective base classifier initial stage training distribution examples disjoint replicated since totally random distribution examples may result absence one classes partitioned data subsets classifiers formed subsets ignorant classes disagreements may occur classifiers leads larger arbiter training sets maintaining class distribution subset total available training set may alleviate problem classifiers generated subsets may closer behavior global classifier produced entire training set trained random class distributions addition disjoint data subsets promote maximum amount parallelism hence desirable yet partial replication 8 may mitigate problem extreme bias potentially introduced disjoint data strategies indeed many strategies arbitration combining detailed impacting size training data required implement effectively several experiments run determine relative effectiveness strategies vary type information biased distributions 12 34 14 classifiers training data subsets arbiters figure 3 sample arbiter tree training data arbiter allowed see thus far metalearning strategies discussed applied solely single collection base classifiers called onelevel metalearners also studied building hierarchical structures recursive fashion ie metalearning arbiters combiners collection lower level arbiters combiners hierarchical classifiers attempt improve prediction accuracy may achieved onelevel metalearned classifiers 5 hierarchical metalearning onelevel metalearning learning techniques may produce highly accurate classifiers explore hierarchical techniques applying metalearning strategies recursively 51 arbiter trees arbiter tree hierarchical structure composed arbiters computed bottomup binarytree fashion choice binary tree simplify discussion higher order trees also studied arbiter initially learned output pair base classifiers recursively arbiter learned output two arbiters k subsets k classifiers log 2 generated instance classified arbiter tree predictions flow leaves root first leaf classifiers produces initial classification test instance pair predictions parent arbiters prediction another prediction produced arbitration rule process applied level final prediction produced root tree proceed describe build arbiter tree detail suppose initially four training data subsets processed learning algorithm l first four classifiers c generated four instances l applied union subsets 1 2 classified c 1 c 2 generates two sets predictions p 1 selection rule detailed earlier generates training set 12 arbiter predictions p 1 subset u 12 arbiter trained set 12 algorithm l similarly arbiter 34 generated 3 hence firstlevel arbiters produced u 14 formed union subsets 1 4 classified arbiter trees rooted 12 34 similarly 14 14 root arbiter generated arbiter tree complete resultant tree depicted figure 3 process generalized arbiter trees higher order higher order shallower tree becomes parallel environment translates faster execution however logically increase number disagreements hence data items selected training higher communication overhead level tree due arbitration many predictions single arbitration site note interest distributed computing environment union sets need formed one processing site rather classify subset transmitting learned classifier site used scan local data set labeled classifiers predictions classifier computational object far smaller size training sets derived example network computing environment classifier may encapsulated agent communicated among sites experimented several different arbiter strategies besides one described section 41 entire set results obtained various strategies reported 7 next discuss computational efficiency various strategies explored 511 discussion since arbiter training set constructed results arbiters two subtrees node arbiter tree synchronization point arbitrary subtrees run asynchronously communication pair subtrees join parent time learn arbiter tree proportional longest path tree bounded path training data reduce complexity learning arbiter trees size training sets arbiters purposefully restricted larger training sets used compute base classifiers thus parallel processing time level tree relatively equal throughout tree however several experiments restriction allowable size training sets arbiters removed explore two key issues whether higher accuracy could achieved providing information arbiter might number disagreements generated hence size training data would naturally formed selection rules notice maximum training set size doubles one moves one level tree equal size entire training set root reached obviously desire forming training set root large original training set indeed metalearning case use great expense therefore desire means control size arbiter training sets move tree without significant reduction accuracy final result since training sets selected arbiter node depends classification results two descendant subtrees run time configuration arbiter tree cannot optimized compile time size sets ie number disagreements known base classifiers first computed however may optimize configuration tree run time clever pairing classifiers configuration resulting tree depends upon manner classifiers arbiters paired ordered level goal devise pairing strategy favors smaller training sets near root one strategy may consider pair classifiers arbiters level would produce fewest disagreements hence smallest arbiter training sets denoted minsize another possible strategy pair classifiers produce highest number disagreements maxsize first glance first strategy would seem attractive however disagreements classifiers resolved bottom tree data commonly classified surface near root tree also fewer choices pairings classifiers control growth training sets hence may advantageous resolve disagreements near leaves producing fewer disagreements near root may desirable pair classifiers arbiters produce largest sets lower tree perhaps counterintuitive sophisticated pairing schemes might decrease arbiter training set size might also increase communication overhead distributed computing environment also create synchronization points level instead node special pairings performed compromise strategy might perform pairing leaf level indirectly affects subsequent training sets level synchronization occurs node level 52 combiner trees way combiner trees learned used similar arbiter trees combiner tree trained bottomup combiner instead arbiter computed nonleaf node combiner tree simplify discussion describe binary combiner tree used trained experiments reported later included higher order trees well classify instance leaf classifiers produces initial prediction pair predictions composition rule used generate metalevel instance classified parent combiner process applied level final prediction produced root tree another significant departure arbiter trees combiner trees random set examples validation set selected level learning generating combiner tree instead choosing set union underlying data subsets learning commences random set examples picked underlying subsets level combiner tree ensure efficient processing size random training sets limited size initial subsets used train base classifiers base classifiers learned leaf level disjoint training data pair base classifiers produce predictions random training set first level following composition rule metalevel training set generated predictions training examples combiner learned metalevel training set applying learning algorithm process repeated level root combiner created network computing environment classifiers may represented remote agent processes distribute metalearning process arbiter combiner tree strategies different impact efficiency arbiter tree approach implemented requires classification possibly entire data set root level significant speed might easily obtained combiner tree approach however always classifies set data bounded size relatively small validation set therefore combiner trees generated efficiently arbiter trees however remains seen impact accuracy either scheme may exhibit large number experiments conducted evaluate several metalearning strategies varying particular learning algorithms distribution schemes three learning tasks results reported next 6 experimental results evaluation one common techniques used evaluating accuracy learning program crossvalidation 2 technique entire data set divided training set disjoint test set classifiers computed training set evaluated test set process repeated n times case using entirely different training test sets accuracies n different classifiers measured n different test sets averaged final prediction accuracy learning algorithm employed learning algorithms learning tasks evaluate cross validation detailed following pages experimental results reported average 10fold cross validation runs plotted represents hundreds experimental runs various metalearning strategies intoto also statistical significance difference averages measured using onesided ttest 90 confidence value 61 learning algorithms four inductive learning algorithms used experiments obtained id3 16 cart 2 part ind package 3 nasa ames research cen algorithms compute decision trees wpebls weighted version pebls 10 nearestneighbor learning algorithm bayes bayesian classifier based computing conditional probabilities described 9 latter two algorithms reimplemented c 62 learning tasks two molecular biology sequence analysis data sets obtained uci machine learning database used studies dna splice junctions sj data set 19 courtesy towell shavlik ordewier contains 3190 sequences nucleotides type splice junction center sequence three classes sequence 60 nucleotides eight different values four base ones plus four combinations protein coding regions pcr data set 11 courtesy craven shavlik contains 20000 dna nucleotide sequences binary classifications coding noncoding sequence 15 nucleotides four different values two data sets chosen experiments represent two different kinds data sets one difficult learn pcr 70 easy learn sj 90 although large data sets provide us idea strategies behave practice since data sets sufficiently small able generate base line statistics accuracy learning algorithm chosen use study otherwise using massive database would imply unbounded resources time order compute baseline statistics noted well 4 might take many years computing furthermore scaling studies possible smaller sets simply varying number size subsets formed initial data reduction schemes extrapolating however larger data sets sought use study focus work exhausted experiments possible smaller test cases stated another way cannot display useful interesting results metalearning small test cases would much point writing paper first place 63 voting metalearning first consider whether metalearning performs well common voting bayesian techniques reported literature experiments varied number equisized subsets training data 2 64 ensuring disjoint proportional distribution examples class size validation accuracy number subsets splice junctions id3 voting weightedvoting wml wmr avgbase accuracy number subsets splice junctions id3 voting arbiter classcombiner classattcombiner8595 accuracy number subsets splice junctions cart voting weightedvoting wml wmr avgbase maxbase accuracy number subsets splice junctions cart voting arbiter classcombiner classattcombiner figure 4 accuracy onelevel integrating techniques splice junctions domain set used generating integrating structures weightsprobabilitiesarbiterscombiners twice size underlying training set base classifier prediction accuracy separate test set primary comparison measure different strategies run two data sets two learning algorithms results splice junctions data set plotted figures4 protein coding regions data set figure 5 figure first row graphs depicts results different integrating techniques using id3 second row using cart accuracy global classifier plotted one subset meaning learning algorithms applied entire training set produce baseline accuracy results comparison average accuracy base classifiers number subsets also plotted labeled avgbase way com parison average accuracy accurate base classifiers plotted maxbase plotted accuracy average 10fold crossvalidation runs accuracy number subsets protein coding regions id3 voting weightedvoting wml wmr avgbase accuracy number subsets protein coding regions id3 voting arbiter classcombiner classattcombiner6575 accuracy number subsets protein coding regions cart voting weightedvoting wml wmr avgbase accuracy number subsets protein coding regions cart voting arbiter classcombiner classattcombiner figure 5 accuracy onelevel integrating techniques protein coding regions domain experiments run splice junctions data set indicate methods sustain drop accuracy number subsets increases ie size distinct subset training data decreases either algorithm class combiner classattributecombiner schemes exhibit higher accuracy techniques difference statistically significant id3 subset sizes cart subset sizes 64 subsets 45 examples methods sustain significantly 10 accuracy degradation combiner methods incur around 10 less decrease accuracy weightedmajorityrandom method performs worst significantly worse others protein coding regions data set arbiter scheme maintain sometimes exceeds original accuracy level techniques suffer significant drop accuracy 2 subsets climb back original accuracy level number subsets increases weightedmajorityrandom method performs much worse others general methods except weightedmajorityrandom scheme considerably outperform average base classifier avgbase gap statistically significant furthermore outperform average accurate base classifier except cart splice junction domain random sampling training data definitely sufficient generate accurate classifiers two data sets studied hence combining techniques necessary results experiments indicate metalearning strategies dominate weighted voting techniques across domains learners used study however metalearning techniques always outperform weighted voting schemes sj domain combiner techniques favorable pcr domain arbiter technique clear circumstances particular metalearning strategy perform better additional studies underway attempt gain understanding circumstances observe sj domain none schemes maintain baseline accuracy number subsets increases techniques presented far characterized onelevel methods perform one level processing generate integrating structures next consider behavior hierarchical metalearning structures 64 arbiter trees first examine results bounded arbiter training sets arbiter trees different orders binary trees 8ary trees followed results achieved case arbiter training sets unbounded different pairing strategies 641 order arbiter trees training set size limit performed experiments splice junctions protein coding regions data evaluate arbiter tree approach varied number subsets 2 64 measured prediction accuracy disjoint test set plotted results figure 6 averages 10fold crossvalidation runs varied order arbiter trees two eight sj data set plots display drop accuracy number subsets increases also higher order trees generally less accurate lower ones however pcr data set experiments accuracy maintained exceeded circumstances regardless order trees recall tree level size arbiter training set fixed size data subset used training base classifiers relax restriction size data set training arbiter might expect improvement accuracy expense processing time test hypothesis set accuracy number subsets splice junctions id3 binary 4ary 8ary binary max x2 4ary max x2 8ary max x28595 accuracy number subsets splice junctions cart binary 4ary 8ary binary max x2 4ary max x2 8ary max x26575 accuracy number subsets protein coding regions id3 binary 4ary 8ary binary max x2 4ary max x2 8ary max x26575 accuracy number subsets protein coding regions cart binary 4ary 8ary binary max x2 4ary max x2 8ary max x2 figure 6 accuracy arbiter tree techniques experiments performed double maximumtraining set size arbiters observe figure 6 doubling arbiter training set size original accuracy roughly maintained binary trees sj domain regardless learner 4ary 8ary trees accuracy results show significant improvement however multilevel arbiter tree approach demonstrate accuracy improvement onelevel techniques generally cannot maintain accuracy obtained whole data set experiments 642 largest arbiter training set size classifier pairing observe figure 7 restriction size training set arbiter lifted level accuracy achieved sj data set empirical results show single largest arbiter training set exhibited largest arbiter training set size number subsets id3 sj random dist random dist 2 random dist 2 maxsize pairing random dist 2 minsize pairing uniform dist2060100 largest arbiter training set size number subsets random dist random dist 2 random dist 2 maxsize pairing random dist 2 minsize pairing uniform dist2060100 largest arbiter training set size number subsets wpebls sj random dist random dist 2 random dist 2 maxsize pairing random dist 2 minsize pairing uniform dist2060100 largest arbiter training set size number subsets bayes sj random dist random dist 2 random dist 2 maxsize pairing random dist 2 minsize pairing uniform dist figure 7 arbiter training set size different class distributions pairing strategies tree 30 entire training set significant result less time memory serial version needed reach accuracy arbitration demonstrating significant scaling properties percentage total training data exhibited tree dependent several factors prediction accuracy algorithm given data set distribution data subsets pairing learned classifiers arbiters level mentioned section 43 pairing classifiers arbiters affects arbiter training set sizes several experiments performed two pairing strategies maxsize minsize applied leaf level results displayed figure 7 experiments conducted sj data set initial random class distribution uniform class distribution second random class distribution used training base classifiers second random distribution composed way ensure half learned arbiter tree ignorant one classes case initial random distribution different pairing strategies used uniform distribution second random distribution shown figure 7 uniform distribution achieved smaller training sets two random distributions largest training set size case approximately 10 total available data number subsets larger eight except bayes 64 subsets bayes seemed able gather enough statistics small subsets note number subsets eight fewer training sets leaf classifiers larger 10 original data set become largest arbiter tree two pairing strategies affect sizes uniform distribution shown figure one possible explanation uniform distribution produced smallest training sets possible pairing strategies matter however maxsize pairing strategy generally reduce sizes training subsets second random distribution minsize pairing strategy hand affect sometimes even increased sizes generated subsets summary uniform class distribution tends produce smallest training sets maxsize pairing strategy reduce size subsets random class distributions 65 combiner trees consider accuracy combiner trees experiments varied number equisized subsets training data 2 64 ensuring disjoint proportional distribution examples class also varied order combiner trees two eight results experiments combiner trees two different training strategies displayed figure 8 9 baseline accuracy comparative evaluation plotted one subset meaning learning algorithms applied entire training set intoto produce global classifier plots derived average 10fold crossvalidation runs results classcombiner tree strategy displayed figure 8 show drop accuracy data sets cases compared global classifier number subsets increases drop varies 3 15 percentage decrease amount data training subset far larger binary combiner trees seems less accurate higher order trees case might due lack information finding correlations among two sets predictions experiments arbiter trees doubled size metalevel training sets statistically significant improvements observed sj data set cart learner another experiment using classattributecombiner tree strategy figure 9 suggests binary trees appear maintain accuracy global classifier except splice junctions data set cart learner higherorder trees generally less accurate note interest doubling size training sets combiners improved accuracy significantly protein coding regions data set accu accuracy number subsets splice junctions id3classcombiner binary 4ary 8ary binary max x2 4ary max x2 8ary max x28595 accuracy number subsets splice junctions cartclasscombiner binary 4ary 8ary binary max x2 4ary max x2 8ary max x26575 accuracy number subsets protein coding regions id3classcombiner binary 4ary 8ary binary max x2 4ary max x2 8ary max x2 accuracy number subsets protein coding regions cartclasscombiner binary 4ary 8ary binary max x2 4ary max x2 8ary max x2 figure 8 accuracy classcombiner tree techniques racy binary trees consistently higher global classifier ie metalearning strategy demonstrated means boosting accuracy single classifier trained entire data set improvement statistically significant particularly interesting finding since information loss due data partitioning recovered combiner tree thus scheme demonstrates means integrating collective knowledge distributed among individual base classifiers summary experimental results suggest increasing size metalevel training sets improves accuracy learned trees likely result simple observation data available training leading better information correlation among base classifiers experimental data convincingly demonstrate doubling training set size metalevel partitions relative underlying subsets used training base classifiers sufficient accuracy number subsets splice junctions id3classattributecombiner binary 4ary 8ary binary max x2 4ary max x2 8ary max x28595 accuracy number subsets splice junctions cartclassattributecombiner binary 4ary 8ary binary max x2 4ary max x2 8ary max x26575 accuracy number subsets protein coding regions id3classattributecombiner binary 4ary 8ary binary max x2 4ary max x2 8ary max x2 accuracy number subsets protein coding regions cartclassattributecombiner binary 4ary 8ary binary max x2 4ary max x2 8ary max x2 figure 9 accuracy classattributecombiner tree techniques maintain level accuracy global classifier indeed may boost accuracy well 7 concluding remarks way summary demonstrated several interesting behaviors various metalearning architectures studied date results based many empirical experiments using different combinations small number data sets learning algorithms ffl metalearning strategies show consistent improvement classification accuracy base classifiers trained subsets available training data studies show classifiers trained individually random subsets large data set accurate integrating collection separately learned classifiers ffl metalearning strategies outperform common onelevel votingbased bayesian techniques learning tasks domains studied onelevel metalearning schemes consistently maintain high accuracy number subsets increases amount available data thus decreases however results show hierarchical metalearning approach able sustain level accuracy global classifier trained entire data set distributed among number sites ffl arbiter tree strategy allowing unbounded metalevel training sets determined variety algorithms employed 30 certain cases 10 entire training data required one processing site maintain equivalent predictive accuracy single global classifier computed available data words arbiter tree strategy site process larger learning task least 3 times domain studied without increasing memory resources ffl unbounded metalevel training sets necessary achieve good results limiting metalevel training set size twice size data subsets used compute base classifiers usually yielded system able maintain level accuracy achieved global classifier important complexity perspective ffl combiner arbiter trees lower order perform better ones higher order seems mainly attributed increase number opportunities correcting base classifiers since levels lower order trees filter compose good training data ffl finally combiner tree strategy demonstrated consistently boost predictive accuracy global classifier certain circumstances suggests properly configured metalearning strategy combining multiple knowledge sources provides accurate view available data one learning algorithm alone achieve believe concepts embodied term metalearning proposed provide important first step understanding developing systems learn massive widely dispersed databases scale metalearning architectures may provide means using large numbers low cost networked computers collectively learn massive databases useful important new knowl edge would otherwise prohibitively expensive cost time achieve believe metalearning systems important contributing technology future infrastructures envisioned intelligent integration information systems realized acknowledgments work performed columbia university partially supported grants nsf iri9413847 cda9024735 new york state science technology foundation citicorp thank david wolpert many useful discussions work r reduction learning multiple descriptions introduction ind recursive partitioning test flight experiments multistrategy learning metalearning toward parallel distributed learning metalearning scaling learning metalearning disjoint partially replicated data cn2 induction algorithm weighted nearest neighbor algorithm learning symbolic features learning represent codons challenge problem constructive induction study explanationbased mehtods inductive learning neural network ensembles weighted majority algorithm need biases learning generalizaions induction decision trees strength weak learnability speech recognition parallel refinement approximate domain theories knowledgebased neural networks theory learnable stacked generalization methods combining multiple classifires applications handwriting recognition hybrid system protein secondary structure prediction tr theory learnable strength weak learnability stacked generalization weighted nearest neighbor algorithm learning symbolic features experiments multistrategy learning metalearning reduction learning multiple descriptions study explanationbased methods inductive learning cn2 induction algorithm induction decision trees weighted majority algorithm supersedes 8916 ctr rueyhsia li geneva g belford instability decision tree classification algorithms proceedings eighth acm sigkdd international conference knowledge discovery data mining july 2326 2002 edmonton alberta canada vincent cho beat wthrich distributed mining classification rules knowledge information systems v4 n1 p130 january 2002 aleksandar lazarevic zoran obradovic boosting algorithms parallel distributed learning distributed parallel databases v11 n2 p203229 march 2002 philip k chan salvatore j stolfo david wolpert guest editors introduction machine learning v36 n12 p57 julyaugust 1999 leo breiman pasting small votes classification large databases online machine learning v36 n12 p85103 julyaugust 1999 ljupo todorovski sao deroski combining classifiers meta decision trees machine learning v50 n3 p223249 march jaideep vaidya chris clifton privacy preserving association rule mining vertically partitioned data proceedings eighth acm sigkdd international conference knowledge discovery data mining july 2326 2002 edmonton alberta canada w nick street yongseog kim streaming ensemble algorithm sea largescale classification proceedings seventh acm sigkdd international conference knowledge discovery data mining p377382 august 2629 2001 san francisco california puuronen alexey tsymbal local feature selection dynamic integration classifiers fundamenta informaticae v47 n12 p91117 january 2001 guido heumer heni ben amor bernhard jung grasp recognition uncalibrated data gloves machine learning approach presence teleoperators virtual environments v17 n2 p121142 april 2008 christophe giraudcarrier ricardo vilalta pavel brazdil introduction special issue metalearning machine learning v54 n3 p187193 march 2004 jaideep vaidya chris clifton secure set intersection cardinality application association rule mining journal computer security v13 n4 p593622 july 2005 chihfong tsai ken mcgarry john tait claire modular support vector image indexing classification system acm transactions information systems tois v24 n3 p353379 july 2006 ron kohavi llew mason rajesh parekh zijian zheng lessons challenges mining retail ecommerce data machine learning v57 n12 p83113 octobernovember 2004 foster provost venkateswarlu kolluri data mining tasks methods scalability handbook data mining knowledge discovery oxford university press inc new york ny 2002 yifeng zhang siddhartha bhattacharyya genetic programming classifying largescale data ensemble method information sciences international journal v163 n13 p85101 14 june 2004 perspective view survey metalearning artificial intelligence review v18 n2 p7795 october 2002 foster provost venkateswarlu kolluri survey methods scaling inductive algorithms data mining knowledge discovery v3 n2 p131169 june 1999