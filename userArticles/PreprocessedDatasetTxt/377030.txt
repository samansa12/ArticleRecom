framework integrating data alignment distribution redistribution distributed memory multiprocessors abstractparallel architectures physically distributed memory provide costeffective scalability solve many large scale scientific problems however systems difficult program tune systems choice good data mapping parallelization strategy dramatically improve efficiency resulting program paper present framework automatic data mapping context distributed memory multiprocessor systems framework based new approach allows alignment distribution redistribution problems solved together using single graph representation communication parallelism graph cpg structure holds symbolic information potential data movement parallelism inherent whole program cpg particularized given problem size target system used find minimal cost path graph using general purpose linear 01 integer programming solver data layout strategy generated optimal according current cost compilation models b introduction increasing availability massively parallel computers composed large number processing nodes far matched availability programming models software tools enable users get high levels performance systems proliferation small mediumsize parallel systems desktop multiprocessor pcs workstations processors systems modest number processors shared memory broadened use parallel computing scientific engineering environments user need consider underlying system characteristics get reasonable performance case ease programming portability main aspects consider favored popularity sharedmemory programming models thorough understanding complexities target parallel system required scale applications run large systems composed hundreds processors numa interconnects although availability programming models high performance fortran 18 offers significant step towards making systems truly usable programmer forced design parallelization data mapping strategies heavily dependent underlying system characteristics combination hpf sharedmemory paradigms common targeting multiple levels parallelism offered current systems composed smp nodes necessarily reduce complexity problem strategies designed find balance minimization data movement maximization exploitable parallelism using strategies compiler generates single program multiple data spmd program 16 execution target machine softwarecoherent numa architecture compiler translates global references hpf local nonlocal references satisfied appropriate messagepassing statements usually respecting ownercomputes rule ie processor owning piece data responsible performing computations update best choice data mapping depends program structure characteristics underlying system number processors optimizations performed supporting compilation system problem size crucial aspects data movement parallelism load balancing taken consideration unified way efficiently solve data mapping problem automatic data distribution tools may play important role making massively parallel systems truly usable usually offered sourcetosource tools annotate original program data mapping directives executable statements offered dataparallel extensions current sharedmemory programming models automatic data distribution maps arrays onto distributedmemory nodes system according array access patterns parallel computation done application applications considered automatic optimization solve regular problems ie use dense arrays main data structures problems allow computation communication requirements derived compile time 11 traditional methods automatic data mapping methods split static data mapping problem two main independent steps alignment distribution alignment step attempts relate dimensions different arrays minimizing overall overhead interprocessor data movement 24 authors prove alignment problem npcomplete distribution step decides aligned dimensions distributed number processors assigned distribution pattern usually arrays distributed either block cyclic fashion although tools also consider blockcyclic distribution assigning blocks consecutive elements processor roundrobin fashion good distribution maximizes potential parallelism application balances computational load offers possibility reducing data movement serializing however two steps independent tradeoff minimizing data movement exploiting parallelism single layout whole program mapping said static however complex problems remapping actions code blocks improve efficiency solution case mapping said dynamic note dynamic data mapping requires data movement reorganize data layout code blocks order solve dynamic data mapping problem approaches consider set reasonably good solutions alignment distribution code block additional step one solution selected code block maximizes global behavior note approach may discard solutions phase could lead global optimal solution kremer demonstrates 19 optimal selection mapping phase npcomplete anderson lam show 2 dynamic data mapping problem presence control flow statements phases nphard 12 proposal paper propose new framework automatically determine data mapping parallelization strategy fortran 77 program context parallelizing environment dmm systems main interest develop approach find optimal solution data mapping problem given characteristics target architecture assuming predetermined compilation model compared previous approaches algorithm combines data distribution dynamic redistribution parallelism information single graph representation communicationparallelism graph cpg cpg contains information data movement parallelism within phases possible data movement due remapping information weighted time units representing data movement computation costs allows alignment distribution redistribution problems solved together use cpg model data mapping problem minimal cost path problem set additional constraints ensure correctness solution general purpose linear 01 integer programming techniques guarantee optimality solution provided used solve minimal cost path problem techniques proven effective solving variety compilation problems 21 data mapping strategies considered static dynamic one twodimensional block cyclic distribution patterns effects control flow statements phases considered framework cost model based profiling information obtained previous serial execution addition parameters target system number processors parallel thread creation overhead communication latency bandwidth provided generated data mapping strategy used annotate original fortran program using hpf data mapping loop parallelization directives note considered strategies based hpf model therefore optimality solution conditioned capabilities target hpf compiler accuracy cost model current implementation handle communication optimization pipelined computation therefore assume hpf compiler generates spmd code according ownercomputes rule loop transformation communication optimization performed compiler rest paper organized follows section 2 use motivating example describe related work order simplify presentation cpg section 3 describes information required model onedimensional data mappings model extended section 4 support twodimensional data mappings section 5 describes formulation minimal cost path problem used find optimal data mapping several experiments performed validate accuracy feasibility model presented section 6 finally concluding remarks summarized section 7 related work motivation large number researchers addressed problem automatic data distribution context regular applications due complexity problem related work splits global problem several independent steps usually alignment distribution remapping addition also perform second level simplification using heuristic algorithms solve step finally another common difference proposed methods cost model adopted several steps although data mapping specified means three attributes alignment distribution remapping automatic data mapping framework solve independently let us first motivate coupling alignment distribution sub problems solved independently li chen 24 gupta et al 15 kremer kennedy 20 chatterjee et al 8 ayguade et al 3 alignment step may impose constraints terms parallelism exploitation distribution step instance assume simple example shown figure 1 loop includes two statements body assignment matrix b matrix assignment matrix b transposed matrix c alignment algorithm easily determine perfect alignment arrays b transposition array c solution communication free however alignment forces distribution step partition different loop nest statement body following ownercomputes rule lead loop parallelization unless loop distribution applied may possible statements cause conflict involved data dependence cycle result alignment distribution steps previously referenced proposals end set candidate static mappings phase candidate mappings input final independent step evaluates usefulness dy figure 1 simple example alignment distribution namic mappings 27 20 9 3 instance figure 2 shows excerpt adi integration kernel code consists two phases sequence sweeps along rows first phase followed sweeps columns second phase phases analyzed isolation row layout best performance first phase column layout best performance second phase note solution one phase leads sequentialization phase sequentialization parallelizable phase far considered best strategy phase isolation therefore remapping step propose dynamic transposition arrays two phases requires data reorganization proposals export set solutions phase rest phases 20 case remapping step would consider possibility applying static row column layout whole sequence phases depending characteristics target system instance low bandwidth could better global solution even one phase sequentialized however proposals consider solutions part initial set candidate solutions phase may lead situation instance static twodimensional distribution rows columns phases may skipped performance isolated phase best global solution 5 fact later related work also claims simultaneous alignment distribution step 11 7 order preserve parallelism minimizing data movements ci ci figure 2 simple example dynamic mapping however still propose additional step solve dynamic data mapping problem algorithms alignment problem proven npcomplete reason li chen 24 authors working modifications initial work 15 3 propose heuristic algorithm solve problem researchers propose use algorithms based dynamic programming 8 however 20 find optimal solution alignment problem using 01 integer programming techniques order solve distribution problem exhaustive search usually performed 25 authors describe model exhaustively explores distribution options based pattern matching reference pattern assignment statement predefined set communication primitives 15 use constraintbased approach assuming default distribution furthermore authors 1 combine mathematical graphbased problem representations find communicationfree alignment use heuristic eliminate largest potential communication costs still maintaining sufficient parallelism dynamic data mapping problem npcomplete solved 9 using divideandconquer approach 27 authors use dynamicprogramming techniques starting static solution recursively decompose employing cost model dynamicprogramming algorithm also used 23 determine best combination candidate layouts sequence phases 3 controlled exhaustive search considered find solution problem finally authors 20 formulate dynamic data partitioning problem another 01 integer programming problem selects single candidate data layout predetermined set layout candidates cost model approaches need cost model make decisions step performance estimates precise enough able distinguish considered search space possible data mappings communicationno communication 24 cheapexpensive 1 cost model quite simple obtain reliable solutions complex programs another option estimate performance symbolic analysis code 25 15 3 ever array data sizes known compile time well number loop iterations probabilities conditional statements information provided user obtained profiling contrast training sets 6 obtain good performance estimations set reported programs although difficult build training set general enough guarantee possible source programs considered general difficult optimizing tool make best guesses compile time incomplete information thus running program serially first obtaining profiling information common strategy adopted many commercial optimizers especially quality solution depends heavily characteristics source program although attempts made add interaction three steps solution proposed paper improves related work two main aspects present unified representation allows compiler explore solutions would obtained isolated analysis ii use linear 01 integer programming techniques find optimal solution whole problem obviously improvements trade computation time required get optimal solution however expensive technique important tool compiler applied selectively cases optimal solution expected result significant performance gain 3 onedimensional data mapping valid data mapping strategy onedimensional data mapping case distributes one dimension array onedimensional grid processors either block cyclic fashion distribution derived may static dynamic number processors n target architecture assumed known compile time sequential execution original fortran 77 program must profiled order obtain problemspecific parameters array sizes loop bounds execution times probabilities conditional statements 31 communicationparallelism graph framework define single data structure represents effects data mapping strategy allowed model name data structure communicationparallelism graph cpg core approach cpg undirected graph gv e h contains information data movement parallelism program analysis created analysis assignment statements within loops reference least one array set v nodes represents distributable array dimensions set e edges represents data movement constraints set h hyperedges 1 represents parallelism constraints edges hyperedges labeled symbolic information later used obtain weights following particular cost model programs initially decomposed computationally intensive code blocks named phases phase static data mapping strategy realignment redistribution actions occur phases approach adopted following definition phase made 17 phase loop nest induction variable occurring subscript position array reference loop body phase contains surrounding loop defines induction variable operational definition allow overlapping nesting phases 311 nodes nodes g organized columns column v g associated array phase one array used several phases column phase array appears column contains many nodes maximal dimensionality arrays program v j denotes j 2 f1dg th dimension array thus node represents distributable array dimension one array dimensionality 0 array embedded onto ddimensional template case additional nodes gamma dimensions used represent data mappings array distributed 312 edges edges g reflect possible alignment choices pairs array dimensions edges connect dimensions different arrays edge connecting dimension j 1 array 1 hyperedge generalization edge connect two nodes dimension j 2 array 2 say edge represents effects terms data movement aligning distributing dimensions phase p program data movement information obtained performing analysis reference patterns pairs arrays within scope p reference patterns defined 24 represent collection dependences arrays sides assignment statement reference pattern theta edges added connecting node v 1 node v 2 set edges represents behavior alignment alternatives dimensions arrays selfreference pattern selfedges added v 1 one node edges labeled data movement primitives representing type data movement performed corresponding array dimensions distributed data movement primitives considered framework include 1to1 1ton nto1 nton 1to1 primitive defined data movement one processor another processor shift copy similarly 1ton primitive defined data movement one processor several processors broadcast nto1 primitive defined data movement several processors one processor reduction finally nton primitive defined data movement several processors several processors multicast remapping information impact data movement included g terms data movement edges phases data flow analysis detects whether array phase p 1 named used later phase say p case theta edges added connecting node column associated array phase p 1 one associated array phase p 2 label assigned edge represents data movement performed remapping corresponding dimensions distributed dynamic model described 14 control flow statements phases considered performing data flow analysis entry exit points conditional iterative statements modify execution flow program therefore cause sequencing phases program different lexicographic order control flow analysis used weight costs explained cost model remapping edges set analysis described 13 313 hyperedges hyperedges g reflect opportunities parallelism candidate parallel loop k hyperedge h k g connecting array dimensions distributed loop parallelized distributed memory machines loop fully parallelized carry data flow dependence 28 datadependence analysis detects set loops candidates parallelization according ownercomputes rule processor owns datum responsible performing computations update therefore candidate parallel loop parallelized lefthand side array dimensions inside loop subscripted loop control variable distributed means hyperedge h k links nodes v j 1 array updated loop body enclosed loop k ie appears lefthand side assignment statement 2 induction variable loop k used subscript expression dimension j case hyperedge h k labeled information associated corresponding candidate parallel loop 314 cyclic information cyclic distributions useful balancing computational load triangular iteration spaces however neighbor communication patterns appear code cyclic distribution incurs excessive data movement framework assume block distribution default meaning edge hyperedge labels cpg assigned assuming block distribution however code contains triangular loops contain neighbor communication cyclic distribution assumed meaning labels cpg assigned assuming cyclic distribution event conflict alternatives considered duplicating cpg labels first cpg copy assigned assuming block dis tribution labels second cpg copy assigned assuming cyclic distribution note cost model described section 33 different according whether distribution block cyclic case data movement edges connecting cpg copies added order allow arrays change distribution pattern phases details model fully described 12 315 example figure 3 shows simple code used working example throughout paper code consists two loop nests following assumed definition phase identified phases ai figure 3 sample code maximum dimensionality arrays code 2 therefore column remap phase 1 phase 2 loop j loop remap figure 4 cpg sample code g two nodes first phase four columns say corresponding arrays b c similarly second phase three columns say corresponding arrays c e seen figure 4 note although array e onedimensional corresponding column v 7 two nodes first assignment statement first phase one reference pattern arrays b identified reference pattern indicates first dimension arrays distributed 1to1 data movement needed processor send last row following processor however second dimension arrays distributed array accesses require data movement addition first dimension array second dimension array b vice versa aligned distributed nton data movement necessary transposition ie processors send block array processors similar analysis performed reference pattern phase program information shown figure 4 dotted edges represent data movement remapping edges connect uses array different phases instance columns 3 5 figure 4 represent array c different phases edges connecting dimension columns mean distribution holds phases therefore data movement required edges connecting different dimensions columns represent effects changing distribution array ie remapping finally data dependence analysis detects j loop first phase loop second phase candidates parallelization j loop parallelized second dimension arrays b distributed first phase therefore hyperedge connecting nodes inserted g labeled information loop similarly hyperedge connecting first dimension arrays c second phase inserted case hyperedge labeled information loop seen figure 4 32 data mapping cpg cpg contains information required model estimate performance effects program different mapping strategies valid data mapping strategy onedimensional case includes one node v j column v g set nodes determines array dimension j array distributed phase note selecting set nodes distributed alignment implicitly specified performance effects selected data mapping strategy estimated set edges hyperedges remain inside selected set nodes edges represent data movement actions hyperedges represent loops effectively parallelized instance figure 5 shows valid data mapping strategy second dimension arrays b c aligned distributed first phase first dimension arrays c e aligned distributed second phase effects data mapping strategy first phase nton data movement array c j loop parallelized similarly second phase nton data movement array e loop parallelized addition arrays c remapped two phases remap phase phase i1 loop j loop remap figure 5 valid dynamic mapping cpg sample code 33 cost model order select appropriate data mapping strategy cost functions labeling edges hyperedges cpg replaced constant weights note accuracy cost model orthogonal issue respect framework presented paper cpg could weighted either simple binary cost functions cheap expensive performing complex performance prediction analysis performance estimation machine dependent therefore aimed specific architecture framework configuration file parameters target system number processors data movement latency bandwidth parallel thread creation overhead addition cost model based profiling information provides array data sizes sequential computation time loop cost assigned data movement edge computed function number bytes interchanged remote memory accesses machine specific latency bandwidth reference pattern matched set predefined data movement routines defined 25 routines considered framework introduced previous section 1to1 1ton nto1 nton according ownercomputes rule processor owns data lefthand side assignment statement responsible computing statement therefore data moved nonlocal data righthand side statement given data movement routine number processors distribution pattern block cyclic estimate block size data move therefore data movement time hyperedge associated candidate parallel loop weighted computation time saved loop effectively parallelized given sequential computation time loop shape iteration space number processors parallel thread creation overhead given distribution pattern time easily estimated instance order estimate execution time first phase sample code data mapping strategy illustrated figure 5 distribute second dimension array block fashion assume number elements array dimension size 32 bit floating point per element sequential computation time j loop time seq addition consider target system np processors data movement latency lt seconds bandwidth bw bytessecond parallel thread creation overhead pt seconds nton data movement array c therefore block size bs data move computed bytes size theta according features target system data movement time estimated seconds addition j loop phase parallelized therefore computation time estimated total estimated time thus move time plus comp time note cost weights cpg expressed time units uniform cost representation allows estimation tradeoffs data movement parallelization gains estimation cpg used main data structure either performance estimation tool automatic data mapping tool details cost model found 12 4 twodimensional data mapping section describe extend cpg order support twodimensional distributions believe scientific programs restricting number distributed dimensions single array two lead loss effective parallelism even higherdimensional arrays show parallelism dimension restricting number distributed dimensions necessarily limit amount parallelism exploited 15 number available processors n known compile time assumed number power 2 ie twodimensional processor topology defined grid n 1 theta n 2 processors data mapping strategy twodimensional data mapping model distributes one two dimensions arrays twodimensional grid processors distribution may static dynamic processor topology may change according preferences program simplicity explanation initially assume n 1 theta n 2 processor topology fixed known compilation time single topology although realistic case allows us introduce general case multiple processor topologies considered 41 single topology first case assume processor topology twodimensional static known compilation time therefore valid data mapping strategy twodimensional data mapping case single topology distributes two dimensions arrays fixed n 1 theta n 2 grid processors end cpg made two undirected graph copies identical except weights first copy named g 1 weights computed assuming processors likewise second copy named g 2 weights computed assuming processors one g b copy b 2 f1 2g therefore represents effects distributing array dimensions b th dimension grid processors order distribute dimension j array across first dimension grid processors g 1 node selected g 1 say v 1 j alternatively distribute dimension j array across second dimension grid processors g 2 node v j selected g 2 say v 2 j allows array dimension mapped dimension grid processors according model valid data mapping strategy twodimensional distribution single topology problem contains one node v b j column v b copy additional restriction one dimension j 1 selected v 1 different dimension j 2 selected v 2 data movement parallelization effects selected twodimensional data mapping strategy estimated set edges hyperedges remain inside selected set nodes figure 6 example valid data mapping first dimension arrays distributed along first dimension n 1 theta n 2 grid processors second dimension arrays distributed along second dimension grid processors according data mapping note j loop first phase parallelized n 2 processors loop second phase parallelized n 1 processors note also onedimensional array e distributed along first dimension grid processors reason replication considered framework 1ton data movement satisfy assignment statement array e array second phase loop loop j figure solution twodimensional cpg n 1 theta n 2 topology cost functions twodimensional model modified respect onedimensional case data movement costs cpg copy estimated assuming two array dimensions distributed order estimate computation time nested loops edges connecting cpg copies inserted modifications fully described 12 42 multiple topologies order consider twodimensional topology model idea build cpg many twodimensional g copies topologies may considered symbolic information contained copy identical weights computed according number processors assumed corresponding topology regularity onedimensional data mapping modeled twodimensional n theta 1 grid processors assuming g ab graph copy corresponding b th dimension th topology valid data mapping strategy general twodimensional distribution problem select one node v ab j column v ab g ab copy within single twodimensional topology previous model dimension j 1 selected v 1 different dimension j 2 selected v 2 j 2 topology selected one phase arrays phase however topology may change phases necessary one change distribution topology array requires redistribution therefore additional data movement edges inserted cpg allowing kind remapping estimating effects corresponding data movement primitive current implementation limited two different topologies onedimensional n theta 1 topology squared twodimensional n 1 theta n 2 topology 2 odd number n 1 set 2 theta n 2 extension two topologies straightforward details found 12 instance figure 7 contains valid general twodimensional data mapping strategy case second dimension arrays phase p 1 aligned distributed onedimensional grid processors n processors arrays c redistributed first dimension arrays phase p 2 aligned distributed first dimension n 1 theta n 2 grid processors second dimension arrays aligned distributed second dimension twodimensional grid processors note nodes selected phase p 1 belong single topology copy onedimensional topology nodes selected phase p 2 belong another topology copy twodimensional one means distribution first phase onedimensional distribution second phase twodimensional loop j g 11 procs loop g 21 g 22 phase figure 7 valid solution general twodimensional cpg 5 minimal cost path problem formulation given valid data mapping strategy summation weights edges remain inside selected set nodes data movement time estimation summation weights hyperedges remain inside selected set nodes estimation computation time saved due parallelization total execution time parallelized program estimated sequential execution time plus data movement time minus computation time saved due parallelization optimal data mapping strategy problem minimizes estimated parallel execution time order find optimal data mapping strategy according model translate data mapping problem minimal cost path problem constraints added guarantee validity solution section describe formulation data mapping problem minimal cost path problem set additional constraints guarantees validity solution linear programming lp provides set techniques study optimization problems objective function constraints linear functions optimization involves maximizing minimizing function usually many variables subject set inequality equality constraints 26 linear pure integer programming problem lp variables subject integrality restrictions addition several models integer variables used represent binary choices therefore constrained equal 0 1 case model said linear programming problem framework model whole data mapping problem linear 01 integer programming problem 01 integer variable associated edge hyperedge final value binary variable indicates whether corresponding edge hyperedge belongs optimal solution objective function minimize specified estimated execution time parallelized version original program problem purely minimal cost path problem several additional restrictions added path selection 01 variables assuming g ab graph copy corresponding b th dimension th topol ogy let e ab pq denote set variables g ab associated edges connecting nodes column p nodes column q note according current implementation 2g set e ab pq contains theta elements let e ab pq j variable g ab associated edge connecting node column p node j column q value one corresponding edge belongs path zero otherwise note graph undirected e ab pq j equivalent e ab qp j redistribution edges behave like regular data movement edges however connect different g ab copies sets 01 integer variables associated redistribution edges called r simplicity notation subscripts therefore let r ab pq j variable associated redistribution edge connecting node column p g ab node j column q g 0 b 0 denotes alternate topology finally index k assigned hyperedge h ab k denote 01 integer variable g ab associated k th hyperedge similarly value one nodes links belong path zero otherwise model assume ddimensional data mapping problem different topologies dimensionality valid solution problem includes nodes array one column restriction nodes selected within phase belong single topology points noted g going details linear 01 integer programming model ffl pairs edges connecting two nodes replaced single edge weight equal addition weights original ones ffl path pair columns g set columns connected set analyzed independently assigned different data distribution strategy order guarantee validity solution minimal cost path problem formulation constraints specified constraints organized following sets solution set paths nodes selected path distinct path within single phase selects nodes different dimension 1 single topology path visits one node column connecting selected nodes included solution connecting selected nodes included solution set constraints c1 guarantees path g copy connected thus column q connected one column p r one edge leading node q selected set e ab pq set r 0 b pq exists one edge leaving node must selected set e ab qr set r ab qr exists terms variables values stated g ab copy node column q connected one column p r sum values variables associated edges connect node column p must equal sum values variables associated edges connect node column r e ab e ab set constraints c2 guarantees paths nodes common words array dimension distributed achieved ensuring number selected edges connecting node g ab copies column lower equal one terms variables values node column p connected another column q e ab r ab pq summation values variables associated edges connect node column q g ab copy lower equal onex e ab set constraints c3 forces selected path belong different dimensions topology modeled topology ensuring number selected edges one dimension b topology equals number selected edges alternate dimension b 0 topology terms variables values set edges e ab pq topology summation values variables associated edges g ab must equal summation values variables associated edges g ab 0 e ab e ab 0 sets constraints c4 c5 specified together modeled forcing one edge selected dimension single topology stated terms variables values assuming summation set edges e ab pq r ab pq equal one dimension b topologies ax e ab finally set constraints c6 ensures one hyperedge selected nodes connected selected according model node column p selected g ab one edge e ab pq j r ab pq j connects column q selected assume hyperedge h ab connects n nodes g ab say nodes columns respectively stated terms variables values must accomplished hyperedge k g ab copy example instance assume first phase onedimensional cpg shown figure 4 four columns say figure b c respectively three sets connecting columns addition one hyperedge say onedimensional case b equal 1 therefore redistribution edges cpg copies required set constraints c1 guarantees path cpg connected columns c connected one column one constraint added column sets constraints c2 c3 guarantee compatibility different paths multidimensional cpg therefore necessary example set constraints c4 c5 specified together force selection one edge set edges set constraints c6 ensures correct selection hyperedge note graph undirected third constraint could also specified 6 experimental results several experiments performed order validate different aspects framework first show complexity terms computational time spent finding optimal solution set programs different benchmark suites secondly accuracy predictions illustrated demonstrate validity model 61 complexity approach programs selected evaluate complexity model alternating direction implicit adi integration kernel erlebacher program developed thomas eidson icase programs shallow tomcatv x42 xhpf benchmark set 2 routine rhs appsp nas benchmark set purpose evaluation programs erlebacher shallow baro inlined ie call replaced actual code routine rhs transformed single program table includes information number code lines total number loops number loops candidates parallelization number phases program number different arrays dimensionality number different reference patterns arrays characteristics parameters determine complexity final optimization problem table 2 shows number 01 integer variables number constraints required according model described section 5 formulate minimal cost path problem onedimensional data mappings last column shows total cpu time seconds required find optimal solution cpu times obtained using sun ultrasparc model built assuming multiprocessor system 8 2 xhpf available anonymous ftp ftpinfomallorg directory tenantsapribenchmarks program lines loops parall phases arrays dims patts rhs 535 37 37 4 4 4 24 tomcatv 178 baro 1153 98 86 24 38 2 428 shallow table 1 characteristics selected programs processors bandwidth 2 mbytes per second program edges hyper constr time erlebacher 1359 68 804 34 rhs 336 37 176 05 baro 1484 83 1608 104 shallow 936 38 1004 39 table 2 characteristics onedimensional model timeconsuming application baro 104 seconds shallow er lebacher x42 need 39 34 31 seconds respectively programs need half second optimized twodimensional data mapping problem assuming single topology number cpg copies duplicated well number 01 integer variables however number constraints required model problem double additional constraints added relate two cpg copies table 3 shows number edges hyperedges constraints total computation time spent find optimal solution programs baro erlebacher require two minutes reach solution shallow x42 rhs require 32 46 seconds tomcatv adi need two one seconds respectively program edges hyper constr time erlebacher 2718 136 2014 1176 rhs 672 74 543 327 tomcatv 496 20 583 20 baro 2968 166 3703 1257 shallow 1872 76 2335 461 table 3 characteristics twodimensional model constant topology finally table 4 number edges hyperedges constraints general twodimensional model shown together computation times required find optimal solution program edges hyper constr time erlebacher 8892 272 4065 31566 rhs 2048 148 922 9856 x42 4304 116 3481 24555 baro 8944 332 7315 63545 shallow 6160 152 4726 16367 table 4 characteristics general twodimensional model selected programs model structure minimal cost path problem harder solve program baro requires almost two hours programs erlebacher x42 shallow need half hour one hour program rhs needs 16 minutes programs require seconds discussion according experiments seconds required solve onedimensional data mapping problem however twodimensional case computation time required greater note decide alignment distribution parallelization loops dynamic changes strategy phases routines program together step number data mapping candidates considered becomes 2 210 baro number candidates erlebacher becomes 3 109 although longest computation time required find optimal data mapping observed two hours must considered tool provides optimal solution therefore computation time investment considered paid within program run order reduce computation times note longest times usually programs inlined ie programs baro shallow erlebacher complexity inlined program becomes greater routines considered together analyzed routine programs isolation one routine program baro requires two minutes routines need less half minute analysis routine program shallow requires seconds routines program erlebacher need less one half minutes results encourage us consider interprocedural analysis way reduce current complexity finally also observed linear 01 integer programming solvers tend find optimal solution least nearoptimal solutions beginning search although requires many iterations explore whole search space number iterations performed solver provided user parameter limit search space obtained suboptimal solutions baro erlebacher x42 shallow less 10 minutes estimated performance solutions higher 85 optimal estimated performance 62 accuracy predictions order test accuracy predictions given model solutions predicted compared actual execution parallelized program silicon graphics origin 2000 32 processors origin 2000 cachecoherent nonuniform memory access multiprocessor physically distributed memory high capacity 4 mbyte cache memory processor distributed arrays across caches caches might act first level distributed memory case cache memory accesses higher latency programs selected experiments adi integration kernel erlebacher program shallow benchmark routine rhs purpose evaluation programs erlebacher shallow inlined routine rhs transformed program profiling information obtained executing sequential code single processor origin 2000 system predictions assumed bandwidth 100 mbytes per second performed several experiments trying different data mapping strategies changing number processors framework implemented part another automatic data distribution tool ddt 3 generates file linear 01 integer programming problem input general purpose solver output solver manually generate parallel code order control scheduling loop iterations according ownercomputes rule stripmined distributed loops details loop transformations found 12 parallel code compiled using native mipspro f77 compiler compiler parallel optimizations disabled avoid change parallelization strategy order generate model data movement costs estimated assuming caching effects ie data transferred cache lines first experiment compare optimal solution suggested tool set selected programs actual execution origin 2000 system trying different data mapping strategies different numbers processors program experiment intend show proposed solution actually yields best result among executed strategies predictions close actual measured executions adi program defines twodimensional data space consists sequence initialization loops followed iterative loop 6 phases performs com putation loop iteration forward backward sweeps along rows columns done sequence solution suggested tool dynamic onedimensional data mapping distributing arrays rows first computation flow columns second computation flow resulting predicted parallel times optimal solution using 2 4 8 16 32 processors seen dotted line figure 8 solid lines show measured execution times static onedimensional row column distributions dynamic onedimensional strategy predicted parallel times performed using profiled sequential execution time 13793 seconds times figure expressed seconds note predictions within 10 actual measured execution times dynamic strategy except execution processors code falls false sharing arrays distributed rows number processors515 execution time measured 1st dim measured 2nd dim measured dynamic predicted figure 8 predicted measured execution times adi erlebacher program 3d tridiagonal solver based adi integration kernel inlined program consists 38 phases perform symmetric forward backward computations along dimension four main threedimensional arrays 10 authors point best performance achieved program obtained static twodimensional distributions pipelining computations however pipelining computations considered model therefore parallelization strategy suggested tool distribute third dimension arrays first second computation flows redistribute third computation flow leaving second dimension arrays distributed dotted line figure 9 shows predicted parallel times using 2 4 8 16 processors predicted parallel times problem size 128 theta 128 theta 128 performed using profiled sequential execution time 5855 seconds solid lines show measured parallel times static distribution first second third dimensions dynamic parallelization strategy note actual measured execution times dynamic strategy within 10 predicted times number processors26 execution time secs measured 1st dim measured 2nd dim measured 3rd dim measured dynamic predicted figure 9 predicted measured execution times erlebacher shallow water equations model defines set 512 theta 512 sized arrays inlined program consists 27 phases within iterative loop ncycles iterations optimal data mapping strategy suggested tool static onedimensional column distribution arrays resulting predicted parallel times optimal data distribution strategy seen figure 10 together measured parallel times static row column data distributions predicted parallel times computed using profiled sequential execution time 45152 seconds note predicted times example within 5 actual measured execution times although executions obtain similar performance number processors103050 execution time measured 1st dim measured 2nd dim predicted figure 10 predicted measured execution times shallow rhs routine defines set 5 theta 64 theta 64 theta 64 fourdimensional arrays consists four phases 36 loops performing flux differences second third fourth di rections solution suggested tool dynamic onedimensional data mapping arrays distributed fourth dimension first three phases third dimension fourth phase predicted parallel times optimal solution together measured times static distribution second third fourth dimensions dynamic strategy shown figure 11 predicted parallel times computed using profiled sequential execution time 165413 seconds last experiment forced tool generate fixed strategy adi code order analyze performance predictions different data distribution strategies table 5 predicted measured execution times seconds several strategies listed row col correspond static onedimensional row column number processors50150 execution time secs measured 2nd dim measured 3rd dim measured 4th dim measured dynamic predicted figure 11 predicted measured execution times rhs distributions respectively dyn dynamic onedimensional strategy 2d squared static twodimensional data distribution strategy implementations predicted executed different number processors ranging 2 32 predicted 1082 932 858 820 802 row measured 990 887 875 822 1511 predicted 988 791 693 643 619 col measured 997 790 693 670 664 predicted 768 403 207 105 052 dyn measured 685 400 226 118 185 predicted 988 689 394 348 188 measured 997 663 413 373 219 table 5 comparison measured predicted execution times row column dynamic twodimensional data mappings adi code predictions performed using profiled sequential time adi code 13793 seconds note predictions data mapping strategy within 10 actual measured parallel execution time except codes fall false sharing 3 3 false sharing occurs executions processors arrays distributed rows one conclusions automatic data distribution tools context distributed memory multiprocessor systems usually decompose parallelization problem three independent steps alignment distribution remapping however steps really independent addition algorithms used solve steps based heuristics work presented paper represents first automatic data mapping parallelization prototype provides optimal solution according cost compilation models contributions proposal respect previous work ffl definition model represents whole data mapping problem allows alignment distribution remapping problems solved within single step ffl formulation minimal cost path problem provides solution model use linear 01 integer programming techniques guarantee solution provided optimal framework based definition single data structure named communicationparallelism graph cpg integrates data movement parallelism related information inherent phase program plus additional information denoting remapping possibilities data mappings considered framework one twodimensional static dynamic block cyclic take account effects control flow statements phases cost model based profiling information obtained previous serial execution experiments show cost model fairly accurate usually within 10 predicting performance different data mapping strategies addition shown complexity approach terms computation time spent find optimal dimensional row distribution dynamic distribution phases arrays distributed rows solution although onedimensional case time required find optimal solution benchmark set matter seconds general twodimensional case time increase two hours trading quality solution computation time analysis however shown times dramatically reduced nearoptimal solutions accepted case model succumb problem previous work since data mappings would missed search space summary integrating sufficient information solve automatic data mapping single graph ambitious however expensive technique important tool applied selectively large number additional aspects considered model definition order extend capabilities framework consequently quality solutions generated part future work plan include model information reflects data movement optimizations detection elimination redundant communication overlapping communication computation information estimates cache effects data distributions addition development interprocedural analysis module may reduce computation time required find optimal solution set solutions considered model currently limited generate either parallel sequential loops shown 4 22 10 better solutions obtained handling pipelining computations feature could modeled framework appropriately weighted hyperedges r automatic computation data decomposition multiprocessors global optimizations parallelism locality scalable parallel machines data distribution loop parallelization sharedmemory multiprocessors tools techniques automatic data layout case study static performance estimator guide data partitioning decisions alignmentdistribution graph array distribution dataparallel programs towards compiler support scalable parallelism using multipartitioning automatic data decomposition messagepassing machines automatic data distribution massively parallel processors dynamic data distribution control flow analysis framework automatic dynamic data mapping automatic data partitioning distributed memory multicomputers programming parallelism automatic data layout high performance fortran high performance fortran handbook automatic data layout distributed memory machines optimal nearoptimal solutions hard compilation problems fortran red retargetable environment automatic data layout efficient algorithms data distribution distributed memory parallel computers index domain alignment minimizing cost crossreferencing distributed arrays compiling communicationefficient programs massively parallel machines john wiley automatic selection dynamic partitioning schemes distributedmemory multicomputers optimizing fortran compiler distributedmemory machines tr ctr minyi guo yi pan zhen liu symbolic communication set generation irregular parallel applications journal supercomputing v25 n3 p199214 july skewed data partition alignment techniques compiling programs distributed memory multicomputers journal supercomputing v21 n2 p191211 february 2002 bjorn franke michael f p oboyle complete compiler approach autoparallelizing c programs multidsp systems ieee transactions parallel distributed systems v16 n3 p234245 march 2005