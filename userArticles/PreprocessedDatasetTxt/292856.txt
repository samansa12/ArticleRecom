faulttolerant dynamic scheduling algorithm multiprocessor realtime systems analysis abstractmany timecritical applications require dynamic scheduling predictable performance tasks corresponding applications deadlines met despite presence faults paper propose algorithm dynamically schedule arriving realtime tasks resource faulttolerant requirements multiprocessor systems tasks assumed nonpreemptable task two copies versions mutually excluded space well time schedule handle permanent processor failures obtain better performance respectively algorithm tolerate one fault time employs performance improving techniques 1 distance concept decides relative position two copies task task queue 2 flexible backup overloading introduces tradeoff degree fault tolerance performance resource reclaiming reclaims resources deallocated backups early completing tasks quantify simulation studies effectiveness techniques improving guarantee ratio defined percentage total tasks arrived system whose deadlines met also compare simulation studies performance algorithm best known algorithm problem show analytically importance distance parameter faulttolerant dynamic scheduling multiprocessor realtime systems b introduction realtime systems defined systems correctness system depends logical result computation also time results produced 22 real work supported indian national science academy department science technology time systems broadly classified three categories namely hard realtime systems consequences executing task deadline may catastrophic ii firm realtime systems result produced corresponding task ceases useful soon deadline expires consequences meeting deadline severe iii soft realtime systems utility results produced task soft deadline decreases time deadline expires 25 examples hard realtime systems avionic control nuclear plant control online transaction processing applications airline reservation banking examples firm realtime systems telephone switching system image processing applications examples soft realtime systems problem scheduling realtime tasks multiprocessor systems determine processor given task executes 22 25 done either statically dynamically static algorithms assignment tasks processors time tasks start execution determined priori static algorithms often used schedule periodic tasks hard deadlines however approach applicable aperiodic tasks whose characteristics known priori scheduling tasks require dynamic scheduling algorithm dynamic scheduling new set tasks correspond plan arrive system scheduler dynamically determines feasibility scheduling new tasks without jeopardizing guarantees provided previously scheduled tasks plan typically set actions either done fully action could correspond task tasks may resource requirements possibly may precedence constraints thus predictable executions schedulability analysis must done tasks execution begun schedulability analysis tasks worst case computation times must taken account feasible schedule generated timing constraints resource faulttolerant requirements tasks new set satisfied ie schedulability analysis successful feasible schedule cannot found new set tasks plan rejected previous schedule remains intact case plan getting rejected application might invoke exception task must run depending nature plan planning allows admission control results reservationbased system tasks dispatched according feasible schedule type scheduling approach called dynamic planning based scheduling 22 spring kernel 27 example paper use dynamic planning based scheduling approach scheduling tasks hard deadlines demand complex realtime applications require high computational needs timing constraints faulttolerant requirements led choice multiprocessor systems natural candidate supporting realtime applications due potential high performance reliability due critical nature tasks hard realtime system essential every task admitted system completes execution even presence failures therefore faulttolerance important issue systems realtime multiprocessor systems faulttolerance provided scheduling multiple versions tasks different processors four different models techniques evolved faulttolerant scheduling realtime tasks namely triple modular redundancy tmr model 12 25 ii primary backup pb model 3 iii imprecise computational ic model 11 iv kfirm deadline model 23 tmr approach three versions task executed concurrently results versions voted pb approach two versions executed serially two different processors acceptance test used check result backup version executed undoing effects primary version output primary version fails acceptance test either due processor failure due software failure ic model task divided mandatory optional parts mandatory part must completed tasks deadline acceptable quality result optional part refines result characteristics realtime tasks better characterised kfirm deadlines k consecutive tasks must meet deadlines ic model kfirm task model provide scheduling flexibility trading result quality meet task deadlines applications automatic flight control industrial process control require dynamic scheduling faulttolerant requirements flight control system controllers often activate tasks depending appears monitor similarly industrial control system robot monitors controls various processes may perform path planning dynamically results activation aperiodic tasks another example taken 3 system monitors condition several patients intensive care unit icu hospital arrival patients icu dynamic new patient plan arrives system performs admission test determine whether new patient plan admitted alternate action like employing nurse carried life criticality application demands desired action performed even presence faults paper address scheduling dynamically arriving realtime tasks pb faulttolerant requirements set processors resources way versions tasks feasible schedule objective dynamic realtime scheduling algorithm improve guarantee ratio 24 defined percentage tasks arrived system whose deadlines met rest paper structured follows section 2 discusses system model section 3 related work motivations work presented section 4 propose algorithm faulttolerant scheduling realtime tasks also propose enhancements section 5 performance proposed algorithm together enhancements studied simulation also compared algorithm proposed recently 3 finally section 6 make concluding remarks system model section first present task model followed scheduler model definitions necessary explain scheduling algorithm 21 task model 1 tasks aperiodic ie task arrivals known priori every task attributes arrival time ready time r worst case computation time 2 actual computation time task denoted c may less worst case computation time due presence data dependent loops conditional statements task code due architectural features system cache hits dynamic branch prediction worst case execution time task obtained based static code analysis average execution times possible worst cases might cases actual computation time task may worst case computation time techniques handle situations one technique task pair scheme 28 worst case computation time task added worst case computation time exception task actual computation time exceeds original worst case computation time exception task invoked 3 resource constraints task might need resources data structures variables communication buffers execution resource may multiple instances every task two types accesses resource exclusive access case task use resource b shared access case share resource another task task also willing share resource resource conflict exists two tasks j require resource one accesses exclusive 4 task two versions namely primary copy backup copy worst case computation time primary copy may backup attributes resource requirements copies identical 5 task encounters one failure either due processor failure due software failure ie primary fails backup always succeeds 6 tasks nonpreemptable ie task starts execution processor finishes completion 7 tasks parallelizable means task executed one processor necessitates sum worst case computation times primary backup copies less equal copies task schedulable within interval 8 system multiple identical processors connected shared medium 9 faults transient permanent independent ie correlated failures considered 10 exists faultdetection mechanism acceptance tests detect processor failures software failures complex realtime applications periodic aperiodic tasks dynamic planning based scheduling approach used paper also applicable realtime applications described system resources including processors partitioned two sets one periodic tasks aperiodic tasks periodic tasks scheduled static tabledriven scheduling approach 22 onto resource partition corresponding periodic tasks aperiodic tasks scheduled dynamic planning based scheduling approach 21 22 13 onto resource partition corresponding aperiodic tasks tasks may precedence constraints ready times deadlines tasks modified comply precedence constraints among dealing precedence constraints equivalent working modified ready times deadlines 11 therefore proposed algorithm also applied tasks precedence constraints among 22 scheduler model dynamic multiprocessor scheduling tasks arrive central processor called scheduler distributed processors system execution communication scheduler processors dispatch queues processor dispatch queue organization shown fig1 ensures processors always find tasks enough tasks system dispatch queues finish execution current tasks scheduler running parallel processors scheduling newly arriving tasks periodically updating dispatch queues scheduler ensure dispatch queues always filled minimum capacity tasks left parallel operation minimum capacity depends worst case time required scheduler reschedule tasks upon arrival new task 24 scheduler arrives feasible schedule based worst case computation times tasks satisfying timing resource faulttolerant constraints use one scheduler whole system makes scheduler single point failure scheduler made faulttolerant employing modular redundancy technique backup scheduler runs parallel primary scheduler schedulers perform acceptance test dispatch queues updated one schedulers passes acceptance test simple acceptance test check whether task schedule finishes deadline satisfying requirements tasks task queue current schedule dispatch queues dispatch queues feasible schedule processors scheduler min length p2fig1 parallel execution scheduler processors 221 resource reclaiming resource reclaiming 24 refers problem utilizing resources processors resources left unused task version executes less worst case computation time ii deleted current schedule deletion task version takes place extra versions initially scheduled account fault tolerance ie pb faulttolerant approach primary version task completes execution successfully need temporally redundant backup version executed hence deleted processor invokes resource reclaiming algorithm completion currently executing task resource reclaiming used processors execute tasks strictly based scheduled start times per feasible schedule results making resources remain unused thus reducing guarantee ratio scheduler informed time reclaimed reclaiming algorithm scheduler schedule newly arriving tasks correctly effectively protocol achieving suggested 24 therefore dynamic scheduling scheme scheduler associated resource reclaiming 3 background section first discuss existing work faulttolerant scheduling highlight limitations works form motivation work 31 related work many practical instances scheduling problems found npcomplete 2 ie believed optimal polynomialtime algorithm shown 1 exist algorithm optimally scheduling dynamically arriving tasks without mutual exclusion constraints multiprocessor system negative results motivated need heuristic approaches solving scheduling problem recently many heuristic scheduling algorithms 21 13 proposed dynamically schedule set tasks whose computation times deadlines resource requirements known arrival multiprocessor systems resource constrained tasks heuristic search algorithm called myopic scheduling algorithm proposed 21 authors 21 shown integrated heuristic used function deadline earliest start time task performs better simple heuristics earliest deadline first least laxity first minimum processing time first 10 pb scheme proposed preemptively scheduling periodic tasks uniprocessor system approach guarantees primary copy meets deadline failure ii backup copy run deadline failure achieve precomputes tree schedules tree encoded within tabledriven scheduler considering possible failure scenarios tasks scheme applicable simple periodic tasks periods tasks multiples smallest period objective approach increase number primary task executions another pb scheme proposed 19 scheduling periodic tasks multiprocessor system strategy backup schedule created set tasks primary schedule tasks rotated primary backup schedules different processors overlap approach tolerates one failure worst case using double number processors used corresponding nonfaulttolerant schedule 7 processor failures handled maintaining contingency backup schedules schedules used event failure backup schedules generated assuming optimal schedule exists schedule enhanced addition ghost tasks function primarily standby tasks addition tasks may possible schedules pb based algorithm backup overloading backup deallocation proposed recently 3 faulttolerant dynamic scheduling realtime tasks multiprocessor systems call backup overloading algorithm backup overloading algorithm allocates single backup time interval time interval task interval scheduled start time scheduled finish time task deallocates resources unused backup copies case faultfree operation two backups overlap schedule overloading processor primaries backups scheduled different processors concept backup overloading valid assumption one fault instant time entire system 3 shown backup deallocation effective backup overloading paper also provides mechanism determine number processors required provide faulttolerance dynamic realtime system discussion related work faulttolerant realtime scheduling found 3 32 motivations work algorithms discussed 7 19 static algorithms cannot applied dynamic scheduling considered paper due high complexities algorithm discussed 10 scheduling periodic tasks uniprocessor systems cannot extended dynamic scheduling expects tasks periodic though algorithm proposed 3 dynamic scheduling consider resource constraints among tasks practical requirement complex realtime system assumes one failure instant time pessimistic algorithm 3 able deallocate backup primary successful uses reclaimed processor time schedule tasks greedy manner resource reclaiming systems simple said workconserving means never leaves processor idle dispatchable task resource reclaiming multiprocessor systems resource constrained tasks complicated due potential parallelism provided multiprocessor potential resource conflicts among tasks actual computation time task differs worst case computation time nonpreemptive multiprocessor schedule resource constraints runtime anomalies may occur 4 workconserving reclaiming scheme used anomalies may cause already guaranteed tasks miss deadlines particular one cannot use workconserving scheme resource reclaiming resource constrained tasks moreover algorithm proposed 3 reclaim resources actual computation times tasks less worst case computation times true many tasks resource reclaiming cases effective improving guarantee ratio 24 spring scheduling approach 27 schedules dynamically arriving tasks resource requirements reclaims resources early completing tasks address faulttolerant requirements explicitly algorithm works within spring scheduling approach builds faulttolerant solutions around support pb based faulttolerance best knowledge first work addresses faulttolerant scheduling problem practical model means algorithm handles resource constraints among tasks reclaims resources early completing tasks deallocated backups performance algorithm compared backup overloading algorithm section 55 4 faulttolerant scheduling algorithm section first define terms present faulttolerant scheduling algorithm uses terms 41 terminology 1 scheduler fixes feasible schedule feasible schedule uses worst case computation time task scheduling ensures timing resource faulttolerant constraints tasks met partial schedule one contain tasks definition 2 stt scheduled start time task satisfies r scheduled finish time task satisfies r definition 3 eat k earliest time resource r k becomes available shared exclusive usage 21 definition 4 let p set processors r set resources requested task earliest start time task denoted estt earliest time execution started defined est denotes time processor p j available executing task third term denotes maximum among available time resources requested task shared mode exclusive mode definition 5 proct processor task scheduled processor task get scheduled denoted exclude proct definition scheduled start time f tpr scheduled finish time primary copy task similarly stbk f tbk denote backup copy definition 7 primary backup copies task said mutually exclusive time denoted time exclusiont definition 8 primary backup copies task said mutually exclusive space denoted space exclusiont task said feasible faulttolerant schedule satisfies following conditions ffl primary backup copies task satisfy r stpr copies task must satisfy timing constraints assumed backup executed failure primary detected time exclusion failure detection done acceptance test means completion every primary copy time exclusion primary backup copies task relaxed backup allowed execute parallel 5 30 overlap primary preferable dynamic scheduling discussed ffl primary backup copies task mutually exclude space schedule necessary tolerate permanent processor failures mutual exclusion time useful resource reclaiming point view primary successful backup need executed time interval allocated backup reclaimed fully primary backup satisfy time exclusion thereby improving schedulability 15 words primary backup task overlap execution backup unnecessarily executes part full even primary successful would result poor resource utilization thereby reducing schedulability moreover overlapping primary backup task introduces resource conflicts access exclusive among since resource requirements forces exclude time one instance requested resource available time 42 distance myopic algorithm spring scheduling 27 approach uses heuristic search algorithm called myopic algorithm 21 nonfaulttolerant scheduling resource constrained realtime tasks multiprocessor system uses basic early start algorithms resource reclaiming one objectives work propose faulttolerant enhancements spring scheduling approach make following enhancements spring scheduling support pb based faulttolerance ffl notion distance introduced decides relative difference position primary backup copies task task queue ffl flexible level backup overloading introduces tradeoff number faults system system performance ffl use restriction vector rv 15 based algorithm reclaim resources deallocated backups early completing tasks 421 notion distance since task model every task two copies place task queue relative difference distancepr positions primary copy task always precedes backup copy task queue let n number currently active tasks whose characteristics known algorithm know characteristics new tasks may arrive scheduling currently active tasks distance input parameter scheduling algorithm determines relative positions copies task task queue following way distance first n gamma n mod distance tasks mod distance last n mod distance tasks following example task queue assuming deadlines tasks nondecreasing order positioning backup copies task queue relative primaries easily achieved minimal cost two queues one primary copies n entries backup copies n entries ii merging queues invoking scheduler based distance value get task queue 2n entries cost involved due merging 2n 422 myopic scheduling algorithm myopic algorithm 21 heuristic search algorithm schedules dynamically arriving realtime tasks resource constraints works follows scheduling set tasks vertex search tree represents partial schedule schedule vertex extended vertex strongly feasible vertex strongly feasible feasible schedule generated extending current partial schedule task feasibility check window feasibility check window subset first k unscheduled tasks larger size feasibility check window higher scheduling cost look ahead nature current vertex strongly feasible algorithm computes heuristic function task within feasibility check window based deadline earliest start time task extends schedule task best smallest heuristic value otherwise backtracks previous vertex schedule extended using task next best heuristic value 423 distance based faulttolerant myopic algorithm make faulttolerant extensions original myopic algorithm using distance concept scheduling set tasks assume task plan algorithm attempts generate feasible schedule task set minimum number rejections distance myopic 1 order tasks primary copies nondecreasing order deadlines task queue insert backup copies appropriate distance primary copies 2 compute earliest start time est first k tasks k size feasibility check window 3 check strong feasibility check whether est true k tasks 4 strongly feasible backtracking possible compute heuristic function first k tasks w input parameter ffl bk task considered h function evaluation pr yet scheduled set est bk b choose task best smallest h value extend schedule c best task meets deadline extend schedule best task best task accepted schedule ffl best task primary copy pr task achieve time exclusion task achieve space exclusion task else reject best task move feasibility check window one task right e rejected task backup copy delete primary copy schedule 5 else backtrack previous search level try extending schedule task next best h value 6 repeat steps 25 termination condition met termination condition either tasks scheduled ii tasks considered scheduling backtrack possible complexity algorithm original myopic algorithm okn noted distance myopic algorithm tolerate one fault point time number faults limited assumption one copies task fail processor fault detected recovery inherent schedule meaning backups primaries scheduled failed processors always succeed addition whether failed processors considered scheduling depends type fault transient processor fault processor failure occurred considered scheduling hand permanent processor fault processor failure occurred considered scheduling till gets repaired failure due task error software fault treated like transient processor fault 424 flexible backup overloading distance myopic discuss incorporate flexible level backup overloading distance myopic algorithm introduces tradeoff number faults system guarantee ratio defining flexible backup overloading state 3 condition backups overloaded pr pr j scheduled two different processors backups bk bk j overlap execution processor backup overloading depicted fig2 fig2 bk 1 bk 3 scheduled processor execution whose primaries pr 1 pr 3 scheduled different processors p 1 p 3 respectively backup overloading valid assumption one failure system instant time pessimistic assumption especially number processors system large processor 1 processor 2 processor 3 primary 1 primary 2 primary 4 primary 3 time backups 1 3 fig2 backup overloading introduce flexibility overloading hence number faults forming processors different groups let groupp denote group processor p member number processors system rules flexible backup overloading every processor member exactly one group ffl group least three processors backup overloading take place group ffl size group gsize except one group mgsize integer ffl backup overloading take place among processors group ffl primary backup copies task scheduled processors group flexible overloading scheme permits dmgsizee number faults instant time restriction one fault group flexible overloading scheme number faults permitted increased flexibility backup overloading limited hence guarantee ratio might drop mechanism gives flexibility system designer choose desired degree faulttolerance section 525 study tradeoff number faults performance system 425 restriction vector based resource reclaiming dynamic faulttolerant scheduling approach used restriction vector rv algorithm resource reclaiming rv algorithm uses data structure called restriction vector captures resource precedence faulttolerant constraints among tasks unified way task associated mcomponent vector rv 1m called restriction vector number processors system rv j task contains last task j j set tasks scheduled finish begins updating dispatch queues scheduler computes restriction vector tasks feasible schedule computing rv task scheduler checks k tasks order latest finish time first scheduled finish processors starts execution latest task processor j resource conflict precedence relation task becomes rv j task exists kth task rv j rv algorithm 15 says start executing task processor scheduled idle tasks restriction vector successfully finished execution performance studies section first present simulation studies various algorithms present analytical study based markov chains highlights importance distance parameter faulttolerant dynamic scheduling simulation experiments conducted two parts first part highlights importance distance parameter second part highlights importance guarantee ratio improving techniques namely distance concept backup deallocation backup overloading point performance curves figs415 total number tasks arrived system 20000 parameters used simulation studies given fig3 tasks simulation generated follows 1 worst case computation times primary copies chosen uniformly min c max c 2 deadline task primary copy uniformly chosen r 2 3 inter arrival time tasks primary copies exponentially distributed mean 1 min c max c2 4 actual computation time primary copy chosen uniformly min c worst case computation time awratio random rand otherwise awratio times worst case computation time 5 backup copies assumed identical characteristics primary copies parameter explanation value taken varied fixed min c minimum computation time tasks 40 max c maximum computation time tasks 80 task arrival rate 020307 05 04 r laxity parameter 237 4 usep probability task uses 010205 04 resource sharep probability task accesses 04 resource shared mode k size feasibility check window 13 11 3 w weight estt h function 1 awratio ratio actual worst case 050610 rand computation times faultp probability primary fails 010205 02 distance relative difference positions primary 15913 5 backup copies task queue number processors 5610 8 numres number resources 2 numinst number instances per resource 2 fig3 simulation parameters 51 experiments highlighting distance parameter section present simulation results obtained different values distance parameter varying k usep faultp parameters study value taken 05 fixed algorithms studied reclaims resources early completing tasks deallocated backups actual computation time task chosen uniformly min c worst case computation time 511 effect feasibility check window fig4 shows effect varying distance different values k note larger values k number h function evaluations est computations also means higher scheduling cost interplay distance size feasibility check window described ffl distance small position backup copies task queue close respective primary copies hence possibility scheduling backup copies may get postponed call backup postponement due time space exclusions makes unscheduled backup copies getting accumulated number exceeds k scheduler forced choose best task say b among backup copies results creation hole 1 schedule since estt b greater available time avail time idle processors hole creation avoided moving feasibility check window till primary task falls however consider approach since increases scheduling cost ffl distance large position backup copies task queue far apart respective primary copies ie tasks backup copies lower deadlines may placed tasks primary copies higher deadlines may lead backtracks hence rejection backtrack possible feasibility check window reaches backup copies call forced backtrack guarantee ratio increases increasing k given distance time growing phase starts decreasing higher values k shrinking phase fig4 shrinking phase starts k values 755 7 distance values 159 13 respectively reason backup postponement high beginning growing phase decreases along reaches lowest value end equivalently beginning shrinking phase number forced rejections low beginning shrinking phase increases along reveals two facts increased value k increased look ahead nature necessarily increase guarantee ratio b optimal k distance different right combination k distance offers best guarantee ratio fig4 best guarantee ratio obtained 9 suppose two distance values give best guarantee ratio one lower k preferable lower scheduling cost 512 effect resource usage task load fault probability fig5 probability task uses resource usep varied fixed value sharep 04 higher usep means resource conflicts among tasks fig5 seen guarantee ratio decreases usep increases applicable values distance fig5 values usep better guarantee ratio obtained distance 9 task arrival rate varied fig6 higher means lower inter arrival time hence higher task load fig6 seen increasing decreases guarantee ratio values unusable processor time scheduling distance fig6 values better guarantee ratio obtained distance 5 9 compared values fig7 probability primary copy encounters failure varied faultp increases guarantee ratio decreases applicable values distance fig7 distance 5 9 better guarantee ratio obtained compared values distance50602 4 guarantee ratio size feasibility check window fig4 effect feasibility check window5060708090 guarantee ratio resource usage probability fig5 effect resource usage probability507090 guarantee ratio task arrival rate fig6 effect task load52566064 guarantee ratio primary fault probability fig7 effect primary fault probability 513 choice distance based observations simulation studies simple heuristic select good k distance value based number processors number resources usage resources high usep low sharep exists resource conflicts among tasks cases est task mostly decided resource available time rather processor available time task ready time therefore large value k might help situations value distance may approximately equal value range m2 since consecutive primaries backups scheduled worst case value k may less distance since larger k means higher scheduling cost might nullify reduce gain obtained 52 experiments highlighting gr improving techniques section show simulation importance guarantee ratio gr improving techniques namely distance concept backup deallocation backup overloading experiments taken value 04 fixed actual computation time task chosen uniformly min c worst case computation time plots figs813 correspond following algorithms myopic faulttolerant version myopic algorithm distance 1 algorithm reclaims resources early completing tasks distance concept algorithm a0 except distance 5 value distance chosen based previous experiments discussions deallocation algorithm a1 together resource reclaiming deallocated backups well overloading algorithm a2 together backup overloading full overloading considered ie gsize algorithm permits one failure whereas algorithms tolerate one failure difference guarantee ratio algorithms a0 a1 due distance concept ii a1 a2 due backup deallocation iii a2 a3 due backup overloading figs813 see guarantee ratio improving techniques improves guarantee ratio system minimal increase scheduling cost algorithms a0 a1 a2 a3 offer nondecreasing order guarantee ratio distance concept backup deallocation effective compared backup overloading 521 effect task laxity resource usage task load effect task laxity r studied fig8 laxity increases guarantee ratio also increases lower laxities difference guarantee ratio algorithms less increases increasing laxity lower values laxity deadlines tasks tight due guarantee improving techniques less flexibility effective fig9 probability task uses resource usep varied increase usep fixed sharep increases resource conflicts among tasks hence guarantee ratio decreases true algorithms effect task load studied fig10 load increases guarantee ratio decreases algorithms lower values task loads 03 guarantee ratio four algorithms less low load system enough processors resources feasibly schedule tasks load increases difference guarantee algorithms also increases means proposed techniques effective higher loads 522 effect number processors effect varying number processors studied fig11 task load fixed load 8 processors increase number processors increases guarantee ratio four algorithms difference guarantee ratio two successive values ie 1 high small decreases increases limited availability resources ie bottleneck resources processors means increased beyond 10 cannot much improvement guarantee ratio6670747882 guarantee ratio task laxity a3 fig8 effect task laxity65758595 guarantee ratio resource usage probability a3 fig9 effect resource usage probability guarantee ratio task arrival rate a3 fig10 effect task load50607080 guarantee ratio number processors a3 fig11 effect number processors7072747678 guarantee ratio actual worst case computation ratio a3 fig12 effect actual worst case computation guarantee ratio primary fault probability fig13 effect primary fault probability 523 effect actual worst case computation time ratio ratio actual worst case computation time awratio tasks varied fig12 experiment actual computation time task taken awratio times worst case computation task fig12 increase awratio decreases guarantee ratio algorithms awratio10 reclaiming due backup deallocation wherever applicable example algorithms a0 a1 awratio10 resource reclaiming possible awratio10 difference guarantee ratio a0 a1 purely due distance concept a1 a2 purely due backup deallocation a2 a3 purely due backup overloading 524 effect fault probability probability primary encounters fault faultp varied fig13 three algorithms a0 a1 a2 plotted number faults given faultp generated studying a3 different less one fault time compared algorithms fault system means every backup deallocated guarantee ratio algorithms a0 a1 flat values faultp since deallocate backups a2 increase faultp decreases guarantee ratio 525 performance flexible overloading performance flexible backup overloading studied various parameters present sample results experiments taken 8 different algorithms studied overloading algorithm a2 ii half overloading gsize say a4 iii full overloading algorithm a3 tradeoff performance faulttolerance studied experiment point time algorithm a2 tolerate one fault algorithm a4 tolerate two faults restriction one fault within group algorithm a3 tolerate one fault task load laxity varied fig14 15 respectively figures guarantee ratio offered full overloading better two half overloading better loading gain guarantee ratio obtained trading reducing number faults full overloading around 2 3 experiments lower task loads gain less 1 higher task loads reveals backup overloading less effective improving guarantee ratio compared techniques distance concept backup deallocation reclaiming early completing tasks thus flexible overloading provides tradeoff performance degree faulttolerance guarantee ratio task arrival rate a3 fig14 effect task load70747882 guarantee ratio task laxity a3 fig15 effect task laxity 53 analytical study section show analytically using markov chains distance important parameter faulttolerant dynamic scheduling realtime tasks using markov chains possible states system probabilities transitions among determined used evaluate different dependability metrics system analysis presented similar one given 3 17 except backup preallocation strategy make analysis tractable make following assumptions 1 tasks unit worst case computation time ie c 2 backup slots preallocated schedule based distance parameter 3 fifo scheduling strategy used 4 size feasibility check window k 1 5 task deadlines uniformly distributed interval w min w max relative ready times call deadline window 6 task arrivals uniformly distributed mean av 7 backup overloading resource reclaiming considered 531 backup preallocation strategy since tasks unit length reserve slots schedule backup copies based distance parameter let number processors distance let backup preallocation strategy time available number primary slots 1 odd 2 even similarly available number backup slots 2 odd 1 even words backups reserved time slot primaries time slot fig16 shows backup preallocation 2 note backup preallocation processors distance gamma processors distance backup preallocation strategy equal gamma 1 primary scheduled slot even backup slot already reserved processor time slot violation space exclusion also meaning preallocation processors time backup slot fig16 distance based backup preallocation 54 analysis p ar k probability k tasks arriving given time p ar pwin k probability arriving task relative deadline w arriving tasks primary copies appended task queue q scheduled fifo order given 1 2 tasks scheduled given time slot depending whether odd even respectively position tasks q indicates scheduled start times beginning time slot task kth task q scheduled execute time k time task execute whose position q k defined equation 3 arrives time schedulability depends length q relative deadline w task appended position q q w g q primary copy pr guaranteed execute time task schedulable since miss deadline moreover w guaranteed execute w note backup preallocation strategy backup task scheduled immediate next slot primary dynamics system modelled using markov chain state represents number tasks q transition represents change length q one unit time probabilities different transitions may calculated rate task arrival simplicity average number tasks executed time m2 u represents state q contains u tasks u m2 probability transition u ugammam2k p ar k since time k tasks arrive m2 tasks get executed u tasks executed state transition u k probability ar k k arriving tasks finite deadlines tasks may rejected let p qk probability one k tasks rejected queue size q value p qk probability relative deadline task smaller extra one time unit needed schedule backup pwin w 4 hence queue size q probability p rej r k q r k tasks rejected r number possible ways select r k elements objective find guarantee ratio rejection ratio different values distance need compute number tasks rejected state done splitting state u onedimensional markov chain 2a av av maximum number task arrivals possibly rejected unit time twodimensional markov chain state ur represents queue size u r tasks rejected transition made ur twodimensional markov chain contains m2w columns number arrivals unit time 1 transition probabilities become computing steady state probabilities rejection states possible compute expected value number rejected tasks rej per unit time p ss u v steady state probability state uv vp ss u v 6 rate task rejection given reja av note p ss u 0 included equation 6 since states corresponding rejection 541 results figs17 show rejection ratio varying distance different values av w max spectively values fixed parameters also given figures since preallocation backups distance gamma identical corresponding rejection ratios also plots observed rejection ratio varies varying distance lower values distance rejection ratio true higher values distance lowest rejection ratio best guarantee ratio corresponds medium value distance figs17 19 optimal value distance m2 therefore distance parameter plays crucial role effectiveness dynamic faulttolerant scheduling algorithms rejection ratio distance fig17 effect task load rejection ratio distance fig18 effect laxity rejection ratio distance fig19 effect distance 55 comparison existing algorithm section compare distance myopic algorithm recently proposed algorithm ghosh melhem mosse call gmm algorithm 3 faulttolerant scheduling dynamic realtime tasks gmm algorithm uses full backup overloading gsize backup deallocation permits one failure point time gmm algorithm address resource constraints among tasks reclaims resources due backup deallocation limitations algorithm discussed section 32 gmm algorithm primary backup copies task scheduled succession words distance always 1 algorithm informally stated gmm algorithm begin 1 order tasks nondecreasing order deadline task queue 2 choose first primary second backup tasks scheduling ffl schedule primary copy early possible end fitting middle fitting middle adjusting ffl schedule backup copy late possible backup overloading end fitting middle fitting middle adjusting 3 primary backup copies meet deadline accept schedule 4 else reject end fitting schedule current task last task schedule processor b middle fitting schedule current task middle schedule processor c b middle adjusting schedule current task middle schedule processor changing start finish times adjacent tasks backup overloading schedule current task backup time interval primary copies corresponding backup copies scheduled two different processors steps bd search fitting adjusting overlapping begins end schedule proceeds towards start schedule every processor depth search limited input parameter k since steps bd takes time km worst case time taken schedule primary copy 2km whereas 3km backup copy performance distance myopic algorithms compared gmm algorithm sake comparison gmm algorithm resource constraints among tasks considered make comparison fair resource reclaiming due backup deallocation considered since gmm reclaim resources early completing tasks plots figs20 21 correspond four algorithms distance myopic dm ii distance myopic full backup overloading dm algorithm without backup overloading gmm overload iv gmm algorithm scheduling cost algorithms made equal appropriately setting k 4 k 1 parameters distance myopic gmm algorithms respectively experiments values r usep faultp awratio distance values taken 5 0 02 1 5 respectively present sample results task load varied fig20 figure different algorithms ordered decreasing order guarantee ratio offered dm overloading fig21 number processors varied fixing task load equal load 8 processors lower number processors even dm algorithm better gmm simulation experiments shown proposed algorithm dm overloading better gmm algorithm even restricted task model proposed50709005 06 07 guarantee ratio task arrival rate gmm dm fig20 effect task load305070903 4 5 6 7 8 guarantee ratio number processors gmm dm fig21 effect number processors 6 conclusions paper proposed algorithm scheduling dynamically arriving realtime tasks resource primarybackup based faulttolerant requirements multiprocessor system algorithm tolerate one fault time employs techniques distance concept flexible backup overloading resource reclaiming improve guarantee ratio system simulation studies also analytically shown distance crucial parameter decides performance faulttolerant dynamic scheduling realtime multiprocessor systems simulation studies distance parameter show increasing size feasibility check window hence look ahead nature necessarily increase guarantee ratio right combination k distance offers best guarantee ratio also discussed choose combination quantified effectiveness proposed guarantee ratio improving techniques simulation studies wide range task system parameters simulation studies show distance concept resource reclaiming due backup deallocation early completion tasks effective improving guarantee ratio compared backup overloading flexible backup overloading introduces tradeoff number faults guarantee ratio studies flexible backup overloading gain guarantee ratio obtained favouring performance ie reducing number faults significant indicates backup overloading less effective compared techniques also compared algorithm recently proposed 3 faulttolerant dynamic scheduling algorithm although algorithm takes account resource constraints among tasks tolerates one fault time sake comparison restricted studies independent tasks one failure simulation results show algorithm used backup overloading offers better guarantee ratio algorithm task system parameters currently investigating integrate different faulttolerant approaches namely triple modular redundancy primarybackup approach imprecise computation single scheduling framework r multiprocessor online scheduling hard realtime tasks computers intractability guide theory np completeness faulttolerance scheduling aperiodic tasks hard realtime multiprocessor systems bounds multiprocessing timing anomalies approaches implementation reparable distributed recovery block scheme distributed faulttolerant realtime systems scheduling tasks quick recovery failure realtime systems architectural principles safetycritical realtime applications fault tolerant scheduling problem imprecise computations modular redundancy message passing system efficient dynamic scheduling algorithm multiprocessor realtime systems new study faulttolerant realtime dynamic scheduling algorithms new algorithms resource reclaiming precedence constrained tasks multiprocessor realtime systems realtime system scenarios analysis faulttolerant multiprocessor scheduling al gorithm adaptive software fault tolerance policies dynamic realtime guarantees multiprocessor support realtime faulttolerant scheduling environment developing faulttolerant software efficient scheduling algorithms realtime multiprocessor systems scheduling algorithms operating systems support realtime systems graceful degradation realtime control applications using mkfirm guarantee resource reclaiming multiprocessor realtime systems realtime computing new discipline computer science engineering understanding faulttolerance reliability spring kernel new paradigm realtime operating systems taskpairscheduling approach dynamic realtime systems low overhead multiprocessor allocation strategies exploiting system spare capacity fault detection location faulttolerant scheduling algorithm distributed realtime systems determining redundancy levels fault tolerant realtime systems multiprocessor scheduling processes release times deadlines precedence exclusion constraints scheduling tasks resource requirements hard realtime systems tr ctr wei sun chen yu xavier defago yuanyuan zhang yasushi inoguchi realtime task scheduling using extended overloading technique multiprocessor systems proceedings 11th ieee international symposium distributed simulation realtime applications p95102 october 2226 2007 r alomari k somani g manimaran adaptive scheme faulttolerant scheduling soft realtime tasks multiprocessor systems journal parallel distributed computing v65 n5 p595608 may 2005 r alomari arun k somani g manimaran efficient overloading techniques primarybackup scheduling realtime systems journal parallel distributed computing v64 n5 p629648 may 2004 xiao qin hong jiang novel faulttolerant scheduling algorithm precedence constrained tasks realtime heterogeneous systems parallel computing v32 n5 p331356 june 2006 wenjing rao alex orailoglu ramesh karri towards nanoelectronics processor architectures journal electronic testing theory applications v23 n23 p235254 june 2007