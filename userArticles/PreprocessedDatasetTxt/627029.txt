distributed memory compiler design sparse problems abstractthis paper addresses issue compiling concurrent loop nests presence complicated array references irregularly distributed arrays arrays accessed within loops may contain accesses make impossible precisely determine reference pattern compile time paper proposes run time support mechanism used effectively compiler generate efficient code situations compiler accepts input fortran 77 program enhanced specifications distributing data outputs message passing program runs nodes distributed memory machine runtime support compiler consists library primitives designed support irregular patterns distributed array accesses irregularly distributed array partitions variety performance results intel ipsc860 presented b introduction modern scalable multicomputers widely recognized addition detecting exploiting available parallelism reducing communication costs crucial achieving good performance existing systems dino 34 fortran 12 superb 44 communication optimizations presence regular array reference patterns within loops message blocking collective communications utilization message coalescing aggregation parallel loop nests however often contain array references cannot analyzed compile time array references classified irregular methods described deal parallel loops loops contain reduction type output dependencies methods work loops contain cross processor loopcarried dependencies crossprocessor loopindependent dependencies crossprocessor dependence one whose end points cross processors loopcarried dependence involves write location one iteration followed read location later iteration loopindependent dependence involves write location followed read location loop iteration data parallelism achieved partitioning arrays across nodes machine processor performs computations part array parallelism achieved partitioning loop iterations processors cross processor loop independent dependences occur runtime optimization techniques developed designed reduce communication costs irregular references following ways judicious partitioning data computational work ffl combining element messages larger message thereby reducing number messages transmitted eliminating redundant communication array elements demonstrate optimizations performed automatically compiler prototype compiler called arf arguably fortran developed arf accepts simplified fortran 77 program enhanced specifications distributing data outputs program executes directly nodes distributed memory machine case intel ipsc860 compiler partitions computations analyzes array references classify regular irregular irregular references performs runtime optimizations reduce communication costs since development arf significant amount work done standardizing extensions fortran language high performance fortran forum hpff joint effort academic community industry agreed preliminary set data parallel programming language extensions 16 20 heavily influenced experimental languages fortran 12 vienna fortran 45 crystal 7 24 23 25 kali 22 dino 32 cm fortran 9 hpff decided defer consideration language extensions targeting irregular problems next years hpff plans consider possible irregular problem language extensions 11 overview parti arf runtime optimizations implemented using parti parallel automated runtime toolkit icase runtime preprocessing procedures embedded com piler procedures 1 support shared name space 2 provide infrastructure needed implement nonuniform data mappings efficiently 3 coordinate interprocessor data movement 4 manage storage access copies offprocessor data compiler consists two distinct layers bottom layer library parti runtime procedures designed support irregular patterns distributed array accesses efficiently top layer compiler carries program transformations embedding calls parti primitives original program parti procedures support variety operations globally named distributed array indices distributed arrays partitioned nonuniform manner distributed array element assigned arbitrary processor operations include offprocessor data fetches data stores accumulations offprocessor memory locations multicomputer program generated distributed memory accesses carried using embedded procedures emphasize goal project develop production quality compiler demonstrate run time optimizations generated automatically efficiently compiler complexity system parti procedures parti procedures developed transformations needed embed appropriate primitives implemented relative ease distributed memory compilers paper begins description language accepted relates fortran outline compiler phases described section 3 section 4 describes parti run time primitives implemented incorporated runtime systems employed compiler section 5 provides details code generation optimizations performed compiler compiler described context two example code kernels kernels written arf translated compiler message passing code section 6 reports experimental performance measurements codes compiled arf section 7 describes relationship research related projects arf compiler developed demonstrate feasibility approach irregular problem compilation consequently arf language extensions limited scope sake clarity better understanding show language extensions related real dataparallel language like fortran first describe syntax fortran language extensions provide functionality arf extensions go describe corresponding arf language extensions 21 fortran language fortran version fortran 77 enhanced data decomposition specifications section present brief description features fortran used support irregular problems align decomposition two key fortran data decomposition constructs decomposition abstract problem domain align used map arrays respect decomposition align guarantees elements different arrays mapped element decomposition reside processor simplest alignment occurs array exactly mapped onto decomposition distribute statement specifies mapping decomposition physical machine distributions regular example consider block distribution nproc processors n elements decomposition nproc divides n block distribution divides decomposition contiguous chunks size nnproc assigning one block processor fortran also allows user specified irregular distributions use mapping array typically dis tributed mapping array contains processor numbers used specify processor owns individual decomposition element example specifies irregular partitioning fortran s5 set values map array using mapping method s6 align x irreg example arrays x data arrays map mapping array array map mapped onto decomposition reg statement s4 decomposition reg turn distributed blocks across processors statement s5 arrays x aligned decomposition irreg statement s6 finally decomposition irreg irregularly partitioned across processors using distributed mapping array map statement s7 result statements array elements xi yi assigned processor mapi sometimes convenient ignore certain array dimensions mapping array decomposition array elements unassigned dimensions collapsed mapped index decomposition instance align zij mapj means second dimension z aligned map example means map column j z processor mapj fortran also provides directive clause 21 specify processor execute iteration loop instance nproc processors i1n modinproc assigns loop iterations processors roundrobin fashion 22 arf extensions distributed arrays declared arf either partitioned processors regular manner eg equal sized blocks contiguous array elements assigned processor irregular manner arf extensions explicitly specify array partitioned processors arf make use decomposition statements like ones found fortran arf arrays irregularly partitioned across processors using distributed mapping array arf code fragment effect first code fragment presented previous section irregular distribution specified follows distributed regular using block integer map1000 set values map array using mapping method distributed irregular using map real x1000y1000 statement s1 declares integer array map states array distributed blockwise across processors statement s2 declares real arrays x assigns array elements yi processor mapi arf arrays distributed single dimension distributed dimension must last declared array dimension instance arf statement distributed irregular using map real x101000 would assign column x processor mapi arf also contains clause example distributed i1n partition means work associated iteration carried processor partitioni 3 compiler support irregular computations compile time analysis make possible generate highly efficient code compiler gather enough information analysis source code order generate efficient code compiler needs tractable representations array subscript func tions also needs tractable representations data computational work partitioned 18 15 19 35 instance consider fortran example align xy blocks s5 i1750 assume processor responsible computing values data owns ie owner computes rule 18 4 processors processor contiguous chunks 250 elements arrays x since subscript function xi identity owner computes rule implies three processors execute 250 iterations loop s5 example clear inspection nonlocal accesses occur processors last two loop iterations addition easy determine nonlocal data must obtained instance processor responsible loop iterations 1 250 need first two values stored processor responsible loop iterations 251 500 variety researchers 18 15 implemented techniques generate optimized calls message passing routines given compiletime information array subscript functions array distribution distribution loop iterations paper deals situations compile time analysis fails crucial information available program executes variety applications array subscript functions cannot known compile time many cases subscript functions given integer arrays consider reference yiai code fragment align xyia blocks s5 ia gets assigned values runtime s6 i1750 s8 end compile time analysis difficult irregular array distributions irregular partitions loop iterations example impossible predict compile time data needs communicated distribution x known runtime s5 set values map array using mapping method s6 align x irreg s8 i1750 arf compiler able handle parallel loops marked distributed array references subscripts given functions 1 loop index 2 scalars redefined loop body 3 arrays indexed loop index examples index functions assuming index distributed ia could distributed regular irregular manner arf compiler cannot general handle loops reference patterns simple form instance compiler presented could deal following loop distribute 1100 partition s5 end s6 end one difficulty arises reference xcolj statement s4 values subscript array colj computed statement s3 statement s3 turn lies within loop s2 whose upper bound determined values taken array num das et al 11 describes program slicing techniques used extend methods described broader set constructs except one special case arf compiler unable handle loops loop carried dependencies special case involves accumulation type dependencies decision include special case greatly expands number irregular application codes methods apply arf compiler able recognize accumulations indirectly addressed array shown following example distribute partition commutative associative property operator allows arf compiler postpone accumulations distributed array x end loop computation 31 inspectorsexecutors inspectors executors perform optimizations reduce communication costs nonlocal accesses arising irregular array references processor precomputes data send receive communication volume reduced prefetching single copy offprocessor datum even referenced several times number messages reduced prefetching large quantities offprocessor data single message 32 inspector inspector loop carries preprocessing needed reduce volume communication number messages transmitted figure 1 illustrates inspector generated arf compiler parallel loop hash tables called hashedcaches used temporary storage run time primitives initialize hashed caches store retrieve data flush hashed caches appropriate program execution hash table records offprocessor fetches stores enabling user recognize one reference made offprocessor distributed array element way one copy element must fetched stored inspector phase carry set interprocessor communications allows us anticipate exactly send receive communication calls processor must execute executing loop carry inspector loop described must able find owner distributed array element regular distributions comprise require simple functions compute processor local offset particular array element example one dimensional array distributed block manner simple function used compute processor local offset particular array element hand irregular distributions attempt partition way balances following two objectives 1 processor perform approximately amount work 2 minimize communication overhead foreach processor p ffl generate clone partitioned loop nest ffl insert code perform following foreach rhs irregular array references generate list offprocessor data fetched foreach lhs irregular array reference generate list data stored offprocessor exchange messages processors determine copies nonlocal data sent received executor phase figure 1 simplified inspector single loop nest typically possible express resulting array partitions simple way allowing arbitrary assignment distributed array elements processors take additional burden maintaining data structure describes partitioning size data structure must size irregularly distributed array call data structure distributed translation table distributed translation tables partitioned processors simple manner described section 43 distributed translation tables accessed inspector phase determine data element resides preprocessing completed every processor knows exactly nonlocal data elements needs send receive processors finished position carry necessary communication computation 33 executor loop transformed executor loop figure 2 outlines steps involved nature distributed array distribution affect executor initial data exchange phase follows plan established inspector processor obtains copies nonlocal distributed array elements copies written processors hashed cache communication phase processor carries computation processor uses locally stored portions distributed arrays along nonlocal distributed array elements stored hashed cache insert code loop ffl communicate local data referenced processor ffl receive non local data referenced locally insert code inside loop ffl obtain non local data hashed cache ffl store non local writes hashed cache insert code loop ffl update offprocessor stores figure 2 executor single loop nest computational phase finished distributed array elements stored offprocessor obtained hashed cache sent appropriate offprocessor locations next section describe details parti run time primitives may invoked inspector executor phases 4 parti primitives parti run time primitives divided three categories primitives may invoked inspector phase executor phase inspector executor phase scheduler primitive invoked inspector phase determines send receive calls needed executor phase calls may scatter data gather data perform reduction operations executor phase distributed translation table mentioned earlier used inspector phase hashed cache primitives used inspector executor phases next section describes details scheduler distributed translation table scatter gather reduction hashed cached primitives 41 scheduler primitive use simple example illustrate preprocessing carried scheduler assume distributed array partitioned among three processors irregular fashion depicted figure 3 loop computation access local array 0 offsets 2142global array processors p3 figure 3 mapping global array processors pattern array shown figure 4 processor stores elements distributed array local array thus processor p 1 needs fetch array element a3 element 0 2 local array processor p 2 processors p 2 p 3 need fetch a4 element 0 2 local array p 1 recall task scheduler anticipate exactly send receive communications must carried processor scheduler first determines many messages processor send receive data exchange takes place executor phase gather information processor needs know total number processors executing code defined processor p array nmsgs processor sets value nmsgs j 1 needs data processor j 0 scheduler updates nmsgs processor elementbyelement sum nmsgs j j operation uses fanin tree find sums end fanin processors entries nmsgs identical value nmsgsj equal number messages processor p j must send exchange phase example scenario see end fan value nmsgs processor 210 figure 5 thus p 1 able determine needs send data two yet unspecified processors needs send data one processor p 3 need send data point processor transmits appropriate processor list required array elements list contains local offsets global array elements irregular access pattern array a21 local array 0 global array processors p3 figure 4 irregular access pattern example sends message p 2 requesting element 2 local array 0 p 3 send message p 1 requesting element 2 local array 0 processor information required set send receive messages needed carry scheduled communications figure 6 schedule generated scheduler reused schedule also used carry identical patterns data exchange several different identically distributed arrays schedule reused carry particular pattern data exchange single distributed array data exchange primitives make use given schedule 42 data exchange primitives data exchangers called processor ffl gather data processors ffl scatter data processors ffl perform global reduction operations exchangers use state information stored scheduler described previous section scheduler determines send receive calls needed carry data processors distributed sum tree output tree input sum data needs data needs data needs figure 5 computing number send messages exchanges scheduler given information memory locations involves processors local indices processor p calls data exchanger passes exchanger routine starting address first local array element memory call address p exchanger routines use p base address read write distributed array elements 43 translation table allow users assign globally numbered distributed array elements processors irregular pattern using distributed translation table recall scheduler data exchangers deal indices arrays local processor translation primitives however assume distributed array elements assigned global indices procedure buildtranslationtable constructs distributed translation table processor passes buildtranslationtable set globally numbered indices responsible distributed translation table may striped blocked across processors striped translation table translation table entry global data sent processors local array messages sent processors2send receiving processors p3 processors p3 figure final message pattern index stored processor mod p p represents number processors blocked translation table translation table entries partitioned number equal sized ranges contiguous integers ranges placed consecutively numbered processors dereference accesses distributed translation table constructed buildtranslationtable given distributed array dereference passed set global indices need accessed distributed memory dereference returns processors memory locations specified global indices stored illustrate primitives using simple two processor example processor assigned indices 1 4 processor p 2 assigned indices 2 3 example assume translation table partitioned two processors blocks depict translation table data structure table 1 entry translation table assigns processor local array index globally indexed distributed array element example translation table information global indices 1 2 stored processor 1 information global indices 3 4 stored processor 2 continue example assume processors use dereference primitive find assigned processors local indices corresponding particular global distributed table 1 translation table entries global assigned local index processor index processor 1 processor 2 table 2 results obtained dereference processor global assigned local number index processor index array indices table 2 depict results obtained processor 1 dereferences global indices 1 3 processor 2 dereferences global indices 2 3 4 44 hashed cache usefulness parti primitives described section 4 enhanced coupling primitives hash tables hash table records numerical value associated distributed array element hash table also records processor local index associated element dereference uses hash table reduce volume interprocessor communication recall dereference returns processor assignments memory locations correspond given list distributed array indices distributed array index may appear several times lists passed dereference hash table used remove duplicates lists offprocessor distributed array elements passed scheduler may contain multiple references element scheduler uses hash table identify unique offprocessor data references data exchange procedures use hash tables store copies offprocessor distributed array elements gatherexchanger fetches copies offprocessor distributed array elements places values hash table similarly scatterexchanger obtains copies offprocessor distributed array elements hash table writes values obtained specified local array element designated processor primitives support accumulations nonlocal memory use hash tables way scatterexchanger parti supplies number primitives support reading well writing accumulating hash tables offprocessor accumulations must performed first carry possible accumulations copies distributed array elements hash table perform accumulation data exchange use hash function hashed cache size 2 k masks lower k bits key key formed concatenating processorlocal index pair corresponds distributed array reference 45 summary parti primitives section summarize parti primitives described present example used consider following parti procedure calls ttable build translation tabledistributionmappingnum elements call dereferencettable idglobal indices processorslocal indicesnum indices call setup hashedcachehashedcache processors local indices call scheduleridnhashedcachelocal indicesprocessors call gatherexchangeridhashedcachelocalarray example processor p arranges obtain copies specified offprocessor data elements copies placed hash table hashedcache processors call build translation table function data mapping function returns pointer structure stores data layout p calls dereference function find local addresses corresponding global indices requires dereference call returns processor number local address corresponding global indices p calls function setup hashedcache information returned dereference allocate hashed table p passes scheduler list offprocessor local array indices scheduler build schedule make possible p obtain data elements p obtain data element 1 n processor processorsi local index local indicesi previously allocated hash table hashedcache used eliminate duplicate offprocessor indices irregular problems data access pattern loops data point referenced multiple times partitioning loops cause duplicate offprocessor references scheduler returns integer id used subsequent call gatherexchanger processor calls gatherexchanger processor gatherexchanger primitive passed pointer schedule id generated previous call scheduler pointer allocated hash table hashedcache base address portion array localarray execution gatherexchanger call copies offprocessor elements array localarray reside hash table hashedcache 5 arf compiler arf compiler transforms source program single program multiple data spmd form data distribution specifications used partition program generate appropriate communication compiler incorporates parti primitives carry computations processor efficiently kernels presented coded fortran 77 enhanced arf data distribution statements compiled run ipsc860 section 6 presents performance data obtained kernels describe compilation algorithm slightly general algorithm actually used arf compiler two algorithms produce equivalent code test data sets 51 code generation arf compiler compiler uses distribution specifications generate code set distributed translation tables calls build translation table embedded sequential code one call generated distribution translation table pointer array stored symbol table array distributed regular manner translation table contains function evaluated runtime find processor local index particular datum array irregularly distributed index processor local index stored explicitly distributed translation table order describe algorithm used generate inspector executor figure 7 simple irregular loop loop descriptor must defined descriptor descriptor tuple gives complete description subscript consists following components descriptor sd name array indexed subscript identifies array distributed block cyclic irregular etc type type reference subscript expression used one exchanger types gather scatter accumulation list subscript expression expressions used determine array dex implementation assume single dimension accessed using type index functions shown section 3 figure 7 arrays x ia ib distributed arrays ia ib used index arrays x respectively compile time possible figure indices x accessed dependent values stored arrays ia ib data access pattern becomes available runtime algorithm assumed loops crossprocessor loop carried dependencies later section describe loops contain reductions handled first basic algorithm produce inspector executor given loop presented loop l ffl find array references loop figure 7 array references xiai yib ffl using references subscript expressions form list descriptors oe sd loop shown figure 7 two descriptors generated one reference xiai yibi generating list oe sd ready generate inspector executor code sd 2 oe sd ffl generate declaration statement temporary array temp store values assigned subscript corresponding sd ie sd4 inside l note two descriptors generated example loop storing reference trace temporary array skipped arrays ia ib used directly dereferencing ffl generate clone loop l loop l 0 l ffl body loop l 0 consists statement records temp value taken subscript expression sd4 ffl generate call dereference passing array temp translation table pointer associated array sd1 example loop dereferencing done arrays ia ib ffl next generate call scheduler using arrays pa la returned dereference form schedule ffl gather call gatherexchanger generated using schedule runtime obtains offprocessor data puts data hash table example loop offprocessor values gathered call scatterexchanger generated using schedule call scatter exchanger runtime takes data hash table h sends processors example loop data values array x scattered accumulation call scatterop exchanger generated using schedule call scatterop exchanger runtime takes data hash table h accumulates processors figure 8 irregular loop staged indirect indexing ffl replace subscript expression indexes array sd1 inside loop l temporary array temp arf compiler tailored recognize idiom used index distributed arrays many irregular codes see example figure 8 programmer assigns expression would otherwise used subscript array reference scalar used array subscript type indexing pattern scalar defined inside loop used index distributed arrays precisely ffl scalar defined iteration loop definition may function loop index b scalars defined loop body c arrays indexed loop index ffl used index distributed dimension distributed arrays loop body one carries forward substitution subscript expressions loops written using idiom properties outlined section 3 note forward substitution transforms example figure 8 example figure 7 52 optimizations two main optimizations performed first optimization reduces scheduling overhead identifying sets distributed array references make use optimization array distribution subscript type name expression common schedule elimination dont match match dont care care common exchanger match match match match elimination table 3 optimization patterns schedule second optimization reduces data transfer costs identifying distributed array references make use precisely exchanger invocation optimizations carried sorting descriptors equivalence classes several distributed array references share schedule long arrays question 1 identically distributed 2 matching subscript expressions set distributed array references share exchanger call references identical descriptors table 3 summarizes conditions 53 arf compiler examples section present two examples used demonstrate arf compiler works section 531 presents arf used program distributed memory block sparse matrix vector multiply kernel section 532 presents example computational fluid dynamics 531 sparse block matrix vector multiply figure presents arf program carries block sparse matrix vector multiply kernel iterative solver produced program designed calculate fluid flow geometries defined unstructured mesh 40 matrix assumed size 4 4 blocks nonzero entries statements s4 s5 loops sweep nonzero entries block assumed array partition passed sparse matrix vector multiply kernel generated elsewhere figure presents specification data decomposition sparse block matrix vector multiplication example written fortran fortran used write example change figure 10 replacement statements s1 s2 statements s1 s10 figure 11 array map figure 11 specifies mapping data arrays data arrays single dimension distributed rest compressed figure 10 integer array partition local processor enumerates list indices assigned processor mentioned earlier current implementation partitions one dimension last dimension array parti primitives however support broader class array mappings 6 thus partition describes partitioning last dimension arrays declared statements s1 s2 arf compiler uses information partition make calls primitives initialize distributed translation tables distributed translation tables used describe mapping x cols ncols f statements s1 s2 partitioning computational work specified statement s3 using clause example distributed array partition used specify loop iterations carried processor reference xmcolsji s6 may require processor references arf consequently generate inspector produce schedule hash table handle accesses distributed array x reference irregularly distributed array f occurs statement s6 note distributed array f irregularly distributed using array partition partition also used clause partition loop iterations s3 therefore deduced reference f statement s6 onprocessor partition specifies distributed array elements loop iterations distributed processors separate partitioning routine generates partition arf compiler generates inspector executor run processor code executed processor generate inspector shown figure 9 statement s1 shows generation translation table using partition array statement s2 shows dereference call made figure address various data elements next two statements inspector code generates data communication schedule hash table structure executor generated arf processor p depicted figure 12 fortran 90 notation used appropriate enhance readability offprocessor elements x gathered placed hash table h step figure 12 values x obtained h local memory step iia figure 12 arrays pa la used distinguish build translation table using mapping defined array partition call dereference find processor assignments pa local indices la consecutive references xm colsj employing partition call setup hashedcachehashedcache pa la call scheduleridnhashedcachelapa figure 9 inspector generated arf sparse block matrix vector multiply local offprocessor array accesses step iib accumulate note declarations s1 s3 figure 10 allow compiler determine accumulations local 532 fluxroe kernel kernel taken program computes convective fluxes using method based roes approximate riemann solver 41 42 referred fluxroe kernel paper fluxroe computes flux across edge unstructured mesh fluxroe accesses elements array yold carries flux calculations accumulates results array case sparse block matrix vector multiply kernel four sections array distributed accessed identical manner figure 13 depicts outline fluxroe kernel indices two vertices comprise edge noted compute fluxes f luxk across ith edge access yoldk n1 yoldk n2 1 k 4 part figure 13 fluxes computed add newly computed flux values f luxk yk n1 subtract f luxk yk n2 part iii figure 13 note arrays yold irregularly distributed using ypartition distributed array node irregularly distributed using edgepartition since clause distributed statement also uses edgepartition specify loop iterations partitioned offprocessor references made node part figure 13 inspector compute schedule n1 offprocessor additions yk n1 part iiia figure 13 different schedule n2 offprocessor subtractions distributed irregular using partition real8 x4n y4nf44maxcolsn distributed irregular using partition integer cols9n ncolsn initialization local variables distributed partition s5 distributed enddo figure 10 arf sparse block matrix vector multiply s5 align map reg s7 align fijkl mapl s8 align ncolsi mapi figure 11 fortran data distribution statements sparse block matrix vector mul call gatherexchanger using schedule obtain offprocessor elements x gatherexchanger places gathered data hash table h ii rows assigned processor p k 14 iia pacount p else use pacount lacount get vx14 hashtable h endif m14 iib figure 12 executor generated arf sparse block matrix vector multiply distributed irregular using ypartition real8 yold4numbernodes y4number nodes distributed irregular using edgepartition integer node2numberedges initialization local variables distributed 1numberedges edgepartition k14 ia ib ii calculate flux using vak vbk iii k14 iiia iiib distributed enddo figure 13 arf kernel riemann solver build translation table using mapping defined array ypartition call dereference find processor assignments pa n1 local indices la n1 consecutive references yk n1 employing ygammapartition call dereference find processor assignments pa local indices la consecutive references yk n2 employing ygammapartition call setup hashedcachehashed gamma cache n1 s5 call setup hashedcachehashed gamma cache s7 call scheduleridnhashed gamma cache figure 14 inspector generated arf fluxroe kernel figure 13 parallelized fluxroe reads well accumulates offprocessor distributed array locations data exchange primitives use schedule schedule n1 gather offprocessor references yoldk n1 part ia figure used schedule used gather offprocessor references yoldk n2 part ib figure 13 inspector code generated arf compiler fluxroe kernel shown figure 14 statement s1 shows call build translation table function store information array partitioned statements s2 s3 calls dereference function find addresses various references array dereference calls use translation table setup statement s1 statements s4 s5 generates hash table structure last two statements code fragment shows building communication schedules figure 15 outlines executor produced arf processor p figure 15 fortran 90 notation used appropriate enhance readability step ia ib two sets offprocessor elements yold gathered using schedules n1 n2 step ii appropriate elements yold accessed either local memory appropriate hash table step iii yold values used calculate fluxes newly computed fluxes accumulated local element distributed array appropriate addition subtraction carried steps iva ivc figure 15 flux must accumulated offprocessor element accumulate flux copy stored hash table steps ivb ivd figure 15 fluxes calculated local accumulations completed call scatteradd scattersubtract exchangers exchangers carry needed offprocessor accumulations current version arf compiler attempts minimize number schedules computed single schedule offprocessor yold data accesses could produced computing single schedule references yold lead efficient executor cost expensive inspector 54 memory utilization section overview memory requirements exacted methods described given suggestions made ways requirements reduced many sparse unstructured programs use large integer arrays determine reference patterns respect kernels depicted typical figure 10 9n element integer array cols used purpose figure 13 size array node employed executors depicted figure 12 figure replace cols node local arrays store processor assignments local indices references irregularly distributed arrays kernels figure sum number elements used processors store processor assignments local indices larger 18n figure 13 parallelized code uses total 4 number gamma edges elements amount additional storage needed parallelized code reduced following simple manner iterations loop divided two disjoint sets first set iterations local memory references locally stored array elements second set offgammaprocessor iteration contains processor distributed array reference case listing processor assignments loop iterations offgammaprocessor necessary since frequently possible map problems memory references local processor substantial memory savings results schemes described thus far would use large quantities extra memory attempting handle loop small number distributed array elements accessed many times instance consider following loop f function defined 1 fi 2 ia call gatherexchanger using schedule sn1 obtain first set offprocessor elements yold gatherexchanger places data hash table h n1 ib call gatherexchanger using schedule sn2 obtain second set offprocessor elements yold gatherexchanger places data hash table h ii edges assigned processor p i1number edges assigned p pan1 count p else get va14 hash table h n1 endif pan2 count p else get vb14 hash table h endif iii calculate fluxes flux14 using va14 vb14 iv pan1 count p iva yold14la n1 else ivb accumulate flux14 hash table h n1 endif pan2 count p ivc yold14la else ivd accumulate flux14 hash table h endif va call scatteradd exchanger using schedule n1 hash table h n1 vb call scattersubtract exchanger using schedule hash table h figure 15 executor generated arf fluxroe kernel distributed irregular partition yfi reference pattern distributed array determined f two distinct elements referenced loop loops sort handled using hash table store processor local index assignments distinct memory reference example processor would store processor local index assignments two references distributed array performance penalty using hash table find processor local index assignments distributed array elements examining variety sparse unstructured codes decided implement method described section arf compiler see analysis 30 time space tradeoffs outlined section 6 experimental results section presents range performance data summarizes effects preprocessing measures overall efficiency also discussed performance effects problem irregularity partitioning computational experiments employed fluxroe kernel block sparse matrix vector multiply kernel kernels coded arf parallelized benchmark numbers obtained programs generated arf compiler note syntax accepted arf compiler differs minor ways presented previous sections experiments described paper used either 32 processor ipsc860 machine located icase nasa langley research center 128 processor ipsc860 machine located oak ridge national laboratories processor 8 megabytes memory greenhill 185 beta version c compiler used generate code 80860 processors 61 unstructured mesh data input data variety unstructured meshes used including actual unstructured meshes obtained aerodynamic simulations synthetically generated meshes unstructured meshes aerodynamics two unstructured meshes generated aerodynamic simulations used mesh 21672 element mesh generated carry aerodynamic simulation involving multielement airfoil landing configuration 28 mesh 11143 points mesh b 37741 element mesh generated simulate 42 circular arc airfoil channel 14 mesh 19155 points mesh point associated x coordinate physical domain domain information used partition mesh three different ways strips orthogonal binary dissection algorithm 5 13 another mesh partitioning algorithm jagged partitioning 38 partitioning meshes done sequentially mapping arrays generated distribution data structures synthetic mesh templates finite difference template links k points square two dimensional mesh connectivity pattern distorted incrementally random edges introduced subject constraints new mesh point still requires information k mesh points mesh generator makes following assumptions 1 problem domain consists 2dimensional square mesh n points 2 point initially connected k neighbors determined finite difference template 3 probability q mesh link replaced link randomly chosen mesh point note q equal 00 mesh links modified changes introduced q equal 10 random graph generated two templates used one template connects point four nearest neighbors k4 template connects point four nearest neighbors four diagonal neighbors k8 referred five point template k8 template nine point template experiments described section 256 256 point mesh employed 62 overall performance data presented give overview performance obtained ipsc860 arf compiler output block distributed translation table used table 4 presents inspector time time required carry inspector preprocessing phase b computation time time required perform computations iterative portion program c communication time time required exchange messages within iterative portion program inspector time includes time required set needed distributed translation table well time required access distributed translation table carrying preprocessing unstructured meshes b partitioned using orthogonal binary dissection experiments ratio time required carry inspector time required single iteration communication time ranged factor 07 factor 36 preprocessing time represents set use distributed translation table instance consider block matrix vector multiply 64 processors using 21672 element mesh total preprocessing cost 122 milliseconds milliseconds represent work related translation table parallel efficiency given number processors p defined sequential time divided product execution time p processors times p sequential time measured using separate sequential version kernel run single node ipsc860 algorithm sequential code parallel code table 4 column single sweep efficiency depicts parallel efficiencies would obtained requirement preprocess kernel time calculations carried reality preprocessing time amortized multiple mesh sweeps time required preprocess problem computing parallel efficiencies neglected second set parallel efficiency measurements obtained executor efficiency presented table 4 executor efficiencies 64 processors ranged 048 059 single sweep efficiencies ranged 010 017 experiments depicted table 4 computing time least factor 2 greater communication time executor efficiencies effected fact computations parallelized codes carried less efficiently sequential program parallel code spends time accessing hashed cache also needs perform indirections sequential program table 4 performance different number processors nprocs inspector comp comm single sweep executor timems timems timems efficiency efficiency sparse block matrix vector multiply mesh sparse block matrix vector multiply mesh b table 5 summarizes performance fluxroe kernel meshes varying degrees regularity varying mesh mappings experiment conducted using processors table 5 depicts synthetic meshes derived 5 9 point stencils probability edge move q equal either 00 04 meshes mapped 1d strips 2d blocks one might expect synthetic meshes communications costs increase dramatically increasing q dramatic increases present volume communication required number messages sent per node much higher large q preprocessing costs also increased q communications costs went least factor 16 preprocessing costs went factor 18 table 5 summarizes results meshes b partitioned three ways strips orthogonal binary dissection algorithm jagged partitioning binary dissection jagged partitioning algorithm break domain two dimensional rectangular regions two methods produce similar performance results table 5 performance 32 processors different meshes nprocs inspector comp comm single sweep executor timems timems timems efficiency efficiency 5 point template synthetic mesh partitioned strips q04 310 293 361 025 037 5 point template synthetic mesh partitioned 2d block q04 463 291 319 023 040 9 point template synthetic mesh partitioned strips q04 385 620 530 031 042 9 point template synthetic mesh partitioned 2d block q04 595 624 527 028 042 mesh binary 134 80 22 024 057 jagged 135 81 22 024 056 strips 148 83 26 022 053 binary 191 136 23 028 061 jagged 186 137 21 028 062 strips 219 149 31 024 054 63 breakdown inspector overhead table 6 summarizes cost dereferencing scheduling fluxroe kernel different numbers processors using blocked translation table five point template used mesh partitioned either 1d strips 2d blocks mesh partitioned strips dereference involves mostly local data accesses since domain data translation table identically partitioned strip partitioning used translation table initialization involve communication measurements presented table 6 defined following manner ffl executor time computation communication time required execute include time required preprocessing ffl table initialization time time needed initialize distributed translation table ffl dereference time time taken dereference parti primitive ffl scheduler time time required produce communications schedule required processor locations local indices found dereference majority costs incurred inspector due translation table initialization dereference see table 6 instance consider case 64 processors used carry sweep 2d block partitioned mesh 5 point template translation table initialization dereference together require 183 executor time generation schedule requires 12 executor time problems communication costs comprise small fraction executor time consequently method used partition domain make significant performance impact executor time table 6 costs translation table initialization dereference strongly dependent domain partitioned 2d block partitioning leads higher translation table related costs almost certainly due increased communication requirements needed translation table initialization dereference strip partitioning per se necessarily lead low translation table related costs table 5 noted strip partitioning actually leads higher inspector costs mesh mesh b translation table partitioned blocks contiguously numbered indices assigned processor however mesh mesh b mesh points numbered regular fashion indices corresponding domain strip contiguously numbered table cost dereferencing scheduling different number processors nprocs executor table init dereference schedule time 5 point template synthetic mesh partitioned strips 5 point template synthetic mesh partitioned 2d blocks 64 cost translation table section 43 discussed two straightforward ways map distributed translation table onto processors consider question distribute translation table minimize costs associated translation table access table 7 compares time required carry dereference blocked striped translation tables depicting ffl time required carry particular call dereference ffl average number nonlocal accesses table entries required dereference ffl average number nonlocal processors accessed call dereference results unstructured meshes b examined consistent performance difference cost required dereference blocked striped translation table seen similar numbers offprocessor table entries need accessed either translation table distribution blocked translation tables lead superior performance synthetic meshes used reasons described section 63 particularly good results obtained striped partition blocked translation table used interest blocked translation table also proved superior synthetic meshes partitioned 2d blocks used table 7 cost dereference processors problem indirect blocked indirect striped time nonlocal nonlocal time nonlocal nonlocal ms data proc ms data proc synthetic 5 point template strip partition q02 157 1045 17 365 2862 31 q04 218 1825 17 368 3350 31 synthetic 5 point template 2d block partition q04 mesh binary 97 768 21 96 743 31 jagged strips binary jagged 139 1293 24 130 1263 31 strips 159 1519 31 172 1513 31 65 scheduler data exchanger performance quantify communications costs incurred parti scheduler data exchange primitives time required carry scheduler gatherexchanger scatterexchanger procedure calls measured compared handcoded version ipsc860 supplied sends receives sends receives communicated amount data parti procedures experiment conducted two processors repeatedly exchanged w single precision words information exchange carried using gatherexchangers scatterexchangers ipsc860 supplied send receive calls table 8 summarizes results experiments presented time milliseconds required carry requisite data exchange using send receive messages ratio time taken scheduler gatherexchanger parti primitive calls time taken equivalent send receive calls scatter exchanger calls also timed results virtually identical corresponding gatherexchanger call gatherexchanger exceeded 20 explicitly coded sendreceive pairs moving w words information two processors additional overhead required scheduler carry data exchange factor 21 10 times table 8 overheads parti scheduler gatherexchanger primitives number send gather scheduler data receive exchanger elements timems ratio ratio 400 10 11 14 900 18 11 13 1600 29 12 13 43 12 11 cost using explicitly coded sendreceive pairs move w words 7 relation work programs designed carry range irregular computations 2 26 4 43 13 including sparse direct iterative methods require many optimizations described paper several researchers developed programming environments target particular classes irregular adaptive problems williams 43 describes programming environment dime calculations unstructured triangular meshes using distributed memory machines baden 3 developed programming environment targeting particle computations provides facilities support dynamic load balancing one key distinctions present work baden williams parti runtime support designed used compilers handle parallel loops irregular array references addition used programmers wide range applications contrast programming environments described baden williams highly customized use specific application areas variety compilers targeting distributed memory multiprocessors 44 8 33 31 1 39 exception kali project 22 parti work described 36 29 37 compilers attempt deal loops irregular references efficiently work described paper also related schemes carry distributed memory runtime parallelization 29 27 schemes ambitious described paper include mechanisms carry runtime partitioning parallelization chen 27 suggests optimization similar one described proposed reducing scheduling overheads identifying distributed array references one employ identical schedules point hand coding based timing experiments carried study schemes proposed 29 27 prototype compiler described able generate code capable efficiently handling kernels parallel loops containing irregular array references procedures carry runtime optimizations coupled distributed memory compiler via set compiler transformations compiler described tested paper qualitatively different efforts cited number important respects mechanisms developed demonstrated support irregularly distributed arrays making possible map data computational work arbitrary manner irregularly distributed arrays supported possible compare performance effects different problem mappings support arbitrary distributions proposed 29 37 first implementation compilerbased distributed translation table mechanism irregular scientific problems many unstructured nasa codes must carry data accumulations offprocessor memory locations one demonstration kernels addressed primitives compiler designed handle situation compiler effort unique ability carry irregular patterns offprocessor data accumulations efficiently primitives augmented hash table designed eliminate duplicate data accesses addition hash table manages copies offprocessor array elements researchers used different data structures management offprocessor data copies 22 8 conclusion paper described experimentally characterized compiler runtime support procedures embody methods capable handling important class irregular problems arise scientific computing examining number complete nasa codes two kernels extracted demonstrate methods kernels involved computations unstructured meshes kernels coded arf dialect fortran generated code run nodes ipsc860 detailed timings carried kernels using unstructured meshes aerodynamics along meshes generated using random numbers incrementally distort matrices obtained fixed finite difference template benchmarking suite stressed communications capabilities ipsc860 parti primitives variety ways experiments reported section 62 ratio time required carry preprocessing time required single iteration either kernel ranged factor 07 factor 36 section 63 majority preprocessing costs arose need support irregularly distributed arrays section 65 performance scheduler data exchanger parti primitives quantified dataexchangers demonstrated maximum increase 20 analogous send receive calls provided intel one virtues layered approach distributed compiler design capture set critical optimizations runtime support primitives primitives hence optimizations migrated variety compilers targeting distributed memory multiprocessors intended implement primitives parascope parallel programming environment 17 addition parti primitives used directly programmers applications codes 6 10 applications described 10 particularly noteworthy applications explicit multigrid unstructured euler solvers employed compute flows full aircraft configura tions explicit unstructured euler solver achieved computational rate 15 gflops 512 processors intel touchstone delta multigrid unstructured euler solver achieved computational rate 12 gflops 512 delta processors cases cost inspectors preprocessing approximately equal cost single iteration euler solver amounting less 3 total time complexity system parti procedures parti procedures developed transformations needed embed appropriate primitives implemented relative ease distributed memory compilers primitives used implement runtime support include communications procedures designed support irregular patterns distributed array access procedures find location irregularly mapped distributed array data using distributed translation tables primitives also support maintenance hash tables store copies offprocessor data 9 acknowledgements would like thank harry jordan bob voigt donna meisel careful editing manuscript would also like thank advanced computing laboratory oak ridge national laboratories nas nasa ames providing access 128 node intel ipsc860 hypercubes wish thank dimitri mavriplis david whitaker supplying unstructured meshes david whitaker p venkatkrishnan access codes r pandore system manage data distribution programming abstractions dynamically partitioning coordinating localized scientific calculations running multiprocessors experimental study methods parallel preconditioned krylov methods partitioning strategy pdes across multi processors execution time support adaptive scientific algorithms distributed memory architectures design methodology synthesizing parallel algorithms architec tures paragon multicomputer environment first implementation cm fortran reference manual design implementation parallel unstructured euler solver using software primitives slicing analysis indirect access distributed arrays fortran language specification solving problems concurrent computers numerical methods computation inviscid transonic flows shock waves gamm workshop updating distributed variables local computations high performance fortran forum compiler support machineindependent parallel programming fortran compiler optimizations fortran mimd distributedmemory machines compiling programs nonshared memory machines compiling global namespace programs distributed execution supporting shared data structures distributed memory architectures generating explicit communication sharedmemory program references computational models task scheduling parallel sparse cholesky factorization parallelizing loops indirect array references pointers multigrid solution twodimensional euler equations unstructured triangular meshes principles runtime support parallel processors scheme supporting automatic data migration multicomputers process decomposition locality reference overview dino new language numerical computation distributed memory multiprocessors expressing complex parallel algorithms dino massive parallelism process contraction dino dino parallel programming language crystal runtime system performance effects irregular communications patterns massively parallel multiprocessors parallelizing compiler distributed memory parallel computers parallel preconditioned iterative methods compressible navier stokes equations solution algorithms twodimensional euler equations unstructured meshes distributed irregular finite elements superb tool semiautomatic mimdsimd parallelization vienna fortran language specification tr ctr manuel ujaldon emilio l zapata efficient resolution sparse indirections dataparallel compilers proceedings 9th international conference supercomputing p117126 july 0307 1995 barcelona spain ayon basumallik rudolf eigenmann optimizing irregular sharedmemory applications distributedmemory systems proceedings eleventh acm sigplan symposium principles practice parallel programming march 2931 2006 new york new york usa rongguey chang tyngruey chuang jenq kuen lee efficient support parallel sparse computation array intrinsic functions fortran 90 proceedings 12th international conference supercomputing p4552 july 1998 melbourne australia roxana e diaconescu distributed component architecture scientific applications proceedings fortieth international conference tools pacific objects internet mobile embedded applications february 01 2002 sydney australia vladimir kotlyar keshav pingali paul stodghill compiling parallel code sparse matrix applications proceedings 1997 acmieee conference supercomputing cdrom p118 november 1521 1997 san jose ca kevin b theobald gagan agrawal rishi kumar gerd heber guang r gao paul stodghill keshav pingali landing cg earth case study finegrained multithreading evolutionary path proceedings 2000 acmieee conference supercomputing cdrom p4es november 0410 2000 dallas texas united states renato ferreira gagan agrawal joel saltz data parallel language compiler support data intensive applications parallel computing v28 n5 p725748 may 2002 gagan agrawal joel saltz interprocedural compilation irregular applications distributed memory machines proceedings 1995 acmieee conference supercomputing cdrom p48es december 0408 1995 san diego california united states peizong lee zvi meir kedem automatic data computation decomposition distributed memory parallel computers acm transactions programming languages systems toplas v24 n1 p150 january 2002