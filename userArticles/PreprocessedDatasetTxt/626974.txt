memory latency effects decoupled architectures decoupled computer architectures partition memory access execute functions computer program achieve highperformance exploiting finegrain parallelism two architectures make use access processor perform data fetch ahead demand execute process hence often less sensitive memory access delays conventional architectures past performance studies decoupled computers used memory systems interleaved pipelined studies latency effects partially hidden due interleaving detailed simulation study latency effects decoupled computers undertaken paper decoupled architecture performance compared single processors caches memory latency sensitivity cache based uniprocessors decoupled systems studied simulations performed determine significance data caches decoupled architecture observed decoupled architectures reduce peak memory bandwidth requirement total bandwidth whereas data caches reduce total bandwidth capturing locality may concluded despite capability partially mask effects memory latency decoupled architectures still need data cache b introduction execution computer program involves two interrelated processes accessing data elements memory true computations large amount parallelism exists two tasks concurrent execution tasks result high performance principle decoupled access execute architectures many early high performance computers ibm 360370 cdc 6600 cray1 incorporated techniques exploit parallelism access execute tasks several architectures past years like map200 4 dae 19 24 pipe 8 23 sma 15 sdp 17 fom 2 zs1 20 25 26 wm 28 partition access operations computation functions program distinctly almost aforementioned architectures consist two processors one perform address calculations load store operations operate data produce results two processors often referred access processor execute processor respectively essence job part performed execute processor execute processor cannot perform role without information furnished access processor decoupled architectures fifo buffers queues provided access execute processors maximize overlap independence two processors independence two processes allows access processor fetch data elements ahead demand execute processor phenomenon called slip previous researchers 9 24 27 slip actually refers extent decoupling access execute processes memory latency effects decoupled architectures studied 24 8 26 28 etc smith et al 24 compared performance pipelined decoupled architecture scalar cray1 particular research effort also included studies effect memory latency varying access time main memory since comparison made cray1 interleaved memory configuration cray1 assumed memory bank conflicts also ignored memory access time varied 5 cycles 32 cycles since memory system 16 way interleaved memory latency effects may hidden first twelve lawrence livermore loops used simulation workload goodman et al evaluated performance pipe vlsi decoupled architecture using lawrence livermore loops 8 9 work includes effects memory speed performance conducting studies fast slow memory module fast memory access time one clock cycle slow one access time four clock cycles four way interleaved systems included memory controller overheads incurred controller effective delay seen case fast memory 3 cycles 6 cycles slow memory 9 also ignore memory module conflicts study smith abraham davidson 26 reported results effects memory latency fine grain parallelism astronautics zs1 connected pipelined memory system observed slip limits high computer system almost insensitive memory latency study also based lawrence livermore loops wulf 28 presented preliminary results performance wm architecture though specific memory latency studies performed mentioned data fifos would partially mask effects memory latency also comment probable achilles heel architecture would build memory system capable supplying bandwidth processor absorb 11 objective performance study presented paper three goals first compare performance decoupled computers uniprocessor systems caches second study memory latency sensitivity decoupled computers noninterleaved nonpipelined memory third determine significance data cache decoupled architecture performance decoupled computers compared single processors without caches previous studies conceptually decoupled systems using processor access task eliminate delay data operands cache based systems utilize cache capture locality eliminate long delays required access main memory would interesting find two schemes compare one another paper investigate whether access processor hides memory latency computer system better data cache hence perform comparison performance decoupled mode uniprocessors caches another objective paper study sensitivity decoupled architecture memory latency memory noninterleaved nonpipelined previous studies report decoupled computers less sensitivity memory path length conventional systems 24 8 also reported speed single processor configuration becomes greater memory becomes slower mentioned studies used interleaved pipelined memory systems want study whether system behavior exhibit similar pattern even memory noninterleaved suggesting decoupled architectures need interleaved memories aim isolate memory latency insensitivity contributed decoupling third objective paper study significance data caches decoupled architectures memory system interleaved decoupled architectures generally data caches architectures described zs1 data cache reduced sensitivity memory access time observed previous studies tend suggest improvement possible data cache would minor interleaving memory pipelining obscure memory latency suspect one reason insensitivity longer memory cycle times studies past investigate whether data cache would result serious performance advantage decoupled architectures noninterleaved memory units 12 overview section 2 briefly describe decoupled architecture used conduct performance studies description illustrates architecture similar decoupled architectures hence results obtained system apply least qualitatively decoupled architectures well section 3 analyze mechanism decoupled computers alleviate memory latency problem section 4 explain simulation tools describe benchmarks used section 5 detail simulation results obtained compare cache based uniprocessors decoupled systems study sensitivity decoupled systems memory latency examine limitations caches decoupled architectures eliminating memory latency study significance data cache decoupled architecture conclude paper section 6 2 deap architecture section describe decoupled execute access processor deap used conduct simulation studies deap architecture uses two processors execute processor ep access processor ap shown fig1 standard decoupled access execute architectures 19 ep executes program instructions performs required data computations accesses data memory done ap two processors communicate architectural queues avoid memory contentions ep ap processor equipped separate memory unit access related instructions fetched ap computation instructions fetched ep respective instruction caches data operands needed ep fetched data memory unit ap passed ep via architectural queues results ep computations deposited queues transferred data memory ap ep access path data memory ap instructions stored instruction memory private ap avoid bus contentions data fetch since pin limitations ap vlsi implementation might pose problem ap instructions could also stored global data memory unit degrade performance problems loops since ap instruction cache performance results paper based deap architecture ap instructions stored memory unit data architecture exists simulation form time ap ep see entirely different instruction streams code split compile time way computation instructions put ep section code address calculation access instructions put ap section execute time two different instruction streams enter two processors two respective instruction memories ap makes address calculations performs data memory accesses furnishes ep data requires computations ep thus free perform data computations ap waiting access requests satisfied memory two instruction streams run speed queues absorbing excess data unlike pipe two queues deap read queue write queue see fig 1 ep reads data read queue stores results write queue may noticed queues named reference ep ap wherever coordination required ap ep use tokens ensure correct operation accessing multipleelement data structures ap uses endofdata eod tokens separate batches data array column matrix entire matrix ap use read queue pass eod token ep ep uses tokens control iterations potential problem due sending control information intermingled data queues explicit instructions used deposit tokens also access correctness program ensured long eod token deposited completion issued load instruction addition keeping system simple two queue implementation efficient system separate data control information queues since use tokens denote endofdata would necessitate using one bit called eod bit element queue problems static data bounds ap prior knowledge insert eod tokens problems dynamic bounds c library string copy strcpy string compare strcmp end string known actually encountered scenario different exploit advantage decoupled architecture application ap fetch string elements one one without checking delimiter ep perform comparison find end string due slip ap would fetched beyond end string time ep finds end string case ep sends eod token ap whereupon ap stops fetch operation also flushes unnecessary data fetched ep meanwhile continue operations read data ap flushed queue accomplished ap sends token ep mark flushed queue ep waits token reading data operands queue creates certain amount busy waiting seems inevitable parallelism exploited problem 3 memory latency access processor access processor ap eliminates latency main memory performing access process ahead demand execute processor program looping access instruction stream often precedes execution stream least one iteration read queue buffers prefetched data execute processor wait obtain data computations access processor would already loaded data read queue hence sufficient slip present ep obtains operands delay similarly case memory writes ep put data write queue proceed ap would store back main memory later assumed queue like registers accessed single cycle hence ap run ahead load queue data ep reaches section code reference data write queue long enough ep dump result proceed ep never experiences delay accessing main memory ap thus hides memory latency ep length queues critical factor decoupled architecture since often distance access process run ahead limited queues slow memory access path compensated using longer queues 24 sufficiently long queues high average transfer rate achieved even memory relatively low peak transfer rate capacity queues enable system utilize given memory bandwidth efficiently memory latency insensitivity achieved decoupled architecture depends slip attained case fast memories address calculation instructions consume significant fraction total execution time limit slip attained slower memories environment permits slip memory poses bottleneck slip limited memory speed decreases beyond certain point considered optimum memory speed optimum constant depend characteristics program execution fraction load store instructions relative ap ep workload etc 4 simulation methodology performance simulators developed uniprocessor actually mips r2000 11 deap system access execute processors mips instruction set deap modifications necessary r2000 queue operations epap interface assumed simulators written c run dec 3100 station unix operating system results mips r2000 system form baseline comparison ap ep instructions pipelined fetchdecodealuwriteback stages fashion similar mips r2000 11 hardware interlocks r2000 tries achieve single cycle execution instructions delayed load delayed branch techniques uniprocessor forms baseline system comparison well ep decoupled implementation incorporate techniques identical manners effect decoupling could easily identified decoupled mode length queues kept 20 simulations reported 24 29 8 short queues sufficient achieve performance close maximum available unbounded queues also performed experiments queue lengths observations confirm past results except loop unrolling slightly increases queue length requirements performed simulations lawrence livermore loops llls two signal processing algorithms convolution correlation saxpy routine linpack benchmark c library string copy strcpy llls chosen since used research past 24 8 9 since important wide range scientific algorithms applications signal processing algorithms used contain addressing patterns sequential 10 exhibit good locality properties also since studies involve cache based systems algorithms relevant studies saxpy routine linpack benchmark run loop increments equal one also unequal increments strcpy routine operates character data 8 bits wide benchmarks use data 32 bits wide benchmarks compiled dec 3100 workstation compilers highest level optimization assembly output compiler machine coded required trace format uniprocessor cases decoupled version assembly code manually split access execute streams coded required trace format could include results large benchmarks spec due difficulty generating two streams traces without compiler decoupled system dec compiler performs loop unrolling loops used uniprocessor trace unrolled degree unrolling retained traces decoupled system also 5 discussion simulation results section present simulation results first compare performance decoupled systems single processors caches order relate work previous research also simulate uniprocessors without caches simulation runs vary main memory access time find sensitivity system performance memory path length simulation results analyzed identify limitations decoupled architectures cache based systems finally relevance data cache decoupled architecture studied simulating system cache ap stage comprehensive comparison uniprocessors without caches decoupled systems without caches presented similar studies handcoded traces presented 13 51 comparison uniprocessors caches performance decoupled system comparison uniprocessors without caches shown fig 2 memory cycle time denoted mm expressed terms processor cycles since block sizes cache sizes critical effect performance cache based systems performed simulations different cache parameters order clutter figures much data plot one typical organization cache size 1024 bytes cache size might seem unrealistically small remembered benchmarks used small cache assumed single cycle access time fig 2 seen 5 cycle case decoupled architecture executes code faster uniprocessor without cache 15 cycles uniprocessor cache performs better decoupled system several traces behavior similar traces except strcpy strcpy even cycles uniprocessor cache superior decoupled system strcpy trace unique data element size smaller buswidth multiple data elements fetched one access 52 sensitivity performance memory access time variation execution time increase memory cycle time illustrated fig 3 table three types behavior observed fig 3 memory latency sensitivity first graph corresponds strcpy trace able make use cache due spatial locality uniprocessor cache exhibits less sensitivity memory access time decoupled system second graph illustrates typical behavior benchmarks make use locality uniprocessors caches decoupled architectures exhibit range variation execution time third graph illustrates convolution correlation traces contain true temporal locality cached uniprocessors exhibit significant insensitivity memory access time whereas decoupled architectures sensitive access time table illustrates convolution correlation strcpy increase execution time decoupled architecture significantly higher cached uniprocessors five traces cached uniprocessors decoupled architectures exhibit range variation benchmark uniprocessor cache decoupled architecture lll1 12022 10422 lll3 20000 20000 lll11 19524 20065 convolution 1938 31560 correlation 1070 30580 saxpyun 24497 25435 strcpy table increase execution time cycles tripling memory cycle time fig 4 illustrates change speed decoupled system memory access time varied results reported 24 8 indicate speedup decoupled organization improves memory speed decreases loops observe effect reported also notice beyond certain memory speed speedup declines loops showed effect reported 24 8 used handcoded traces 13 8 though mentioned performance advantage significant slower memory actually 5 12 lawrence livermore loops used show smaller speedup slower memory module since considered interleaved memories decrease speedup certain latency evident loops ranges memory speed used 53 limitations decoupled architectures decoupling smooth burst bandwidth requirements total bandwidth requirement higher time execute processor would take complete section code memory becomes bottleneck unless technique alleviate bottleneck incorporated quantify memory bandwidth problem decoupled architectures comparing total access time requirement pure computation time time ep takes complete section code assuming always finds requested data load queue without waiting able deposit result store queue without waiting ep execution time perfect memory perfect ap let us denote time ep standalone execution time total memory access time analogous bandwidth requirement less ep standalone execution time sufficient memory bandwidth available fig 5 shows memory access takes 18 36 54 times time ep takes complete computations memory speeds 5 10 15 cycles respectively memory system capable furnishing required bandwidth complete entire access time period less ep standalone execution time decoupled system would performed better decoupled architectures eliminate memory latency long total time required data fetch accommodated within time execute processor would consume complete section code without wait operands beyond point effect memory latency evident total execution time explains sensitivity decoupled systems exhibited memory access speed simulations might noted increasing queue length cannot hide latency memory become bottleneck like decoupled architectures reduce peak bandwidth requirement total bandwidth caches reduce total bandwidth requirement capturing locality limitations addressed section 55 load unbalance access execute processors might also limit speed achieved decoupled configuration typical general purpose instruction streams contain access related instructions true computations access processor often execute instructions execute processor illustrated instruction counts table ii severe load unbalance exists saxpyunequal traces used reported results generated highest level compiler optimization o4 address calculation instructions appear within loop except saxpyunequal code less optimization exhibits apep unbalance due presence address calculation instructions ratio ap load ep load less 21 saxpyequal optimizer successful removing address calculation saxpyunequal detailed address calculation iteration corresponding ratio greater 51 average ratio ap instruction count ep instruction count 2151 performed extensive studies memory bandwidth apep unbalance bottlenecks decoupled architectures due space con straints cannot present results interested readers may refer 14 present results benchmark ap instr count ep instr count ap countep count convolution 5000 3975 126 correlation 11083 5926 187 saxpyunequal 12253 2251 544 saxpyequal strcpy 10012 6001 167 table ii ap ep instruction counts ratio another limitation decoupled processors relates overhead incurred process exploiting access execute parallelism computing process slight code expansion results apep code generated branch instructions must appear processors execute processor support operations operands queue read queue one operands moved queue register also contributes code expansion problem alleviated two read queues two data elements loaded alternate queues case two register addresses used queues certain problems transposition matrix array copy etc whole problem access nature role execute processor play effort parallelize problems result serious overhead may increase execution time uniprocessor mode compiler recognizes problems dae system slower uniprocessor 54 significance data cache decoupled architecture section 51 observed uniprocessors data caches performed considerably better decoupled architectures slow memories see t15 cycle case fig 2 observation naturally leads question whether decoupled architecture could also benefit caches performed simulations investigate results appear fig 6 total execution time benchmark plotted decoupled systems caches uniprocessors without caches ordinary decoupled architectures results characteristic memory referencing behavior benchmark convolution correlation algorithms decoupled system caches performs better systems attributed strong temporal locality present data references benchmarks strcpy program benefits spatial locality benchmark also decoupled system cache exhibits superior speedup systems llls saxpy benefit caches benchmarks cache similar effect uniprocessors decoupled architectures t15cycles benchmark uniproc cache decoupled decoupled cache saxpy 096 115 110 mean 096 115 114 correlation 335 222 629 convolution 395 219 515 strcpy 297 106 363 mean 342 182 502 t5cycles benchmark uniproc cache decoupled decoupled cache saxpy 096 143 135 mean 093 141 127 correlation 159 239 308 convolution 189 247 247 strcpy 145 119 191 mean 164 202 249 table iii speedup comparison speedup figures memory access times equal 5 cycles 15 cycles presented table iii speedup calculated reference uniprocessors without caches mean values speedup figures shown separately programs different locality characteristics lawrence loops saxpy benefit caches form one group convolution correlation strcpy exhibit locality characteristics form another group limitations associated caches caches hide latency main memory capturing temporal spatial locality data references locality reference enables caches reduce bandwidth requirements programs illustrated previous section data caches improve performance decoupled architectures also several limitations associated caches lack temporal locality limits capability problems exploit cache fig 2 one note caches cause increase execution time benchmarks consider lawrence livermore loop 3 program steps arrays hence spatial locality study elements array representation data bus width bits well main memory organized word size 32 bits access furnishes one word 32 bits block size 4 bytes used array element demand loaded time miss since array element used words temporal locality cache 4 byte blocks improve performance larger block size would exploit spatial locality since fetching larger block requires proportionately larger number cycles cache decrease total execution time cache yield advantage fetch cycles could run parallel computation cycles cpu busy use bus otherwise cache slows system adding cache access time reference phenomenon observed llls fig 2 execution time cache based system higher system without cache could avoided looking cache main memory time case would deterioration still would improvement thus notice problems spatial locality sometimes benefit caches data size smaller word words one data element could fetched expense one fetch performance improvement could obtained among benchmarks used strcpy trace exhibits characteristic benchmark achieve strong insensitivity memory latency success cache organization depends minimizing miss ratio delay due miss penalty updating main memory number fetches required load block given size depends bus width larger block sizes capture spatial locality may decrease miss ratio increases delay due miss also updating penalty increase hit ratio obtained larger block sizes may mislead designers architects cautioned several cases bus traffic memory bandwidth requirements increase dramatically increase block size overall effect reduction performance despite increase hit ratio typical example shown fig 7 saxpy unequal increments execution time increases threefold block size increased 4 bytes 16 bytes similar behavior observed several traces spatial locality another fact observed figure limitations associated caches affect uniprocessors decoupled systems identically sensitivity performance block size also illustrated correlation algorithm fig 7 observed benchmark strong temporal locality exhibit sensitivity cache block size also execution time system cache always less system without cache general observation temporal locality often stronger spatial locality may concluded precise tuning cache parameters essential success cache organizations also observed 12 1 21 unless carefully designed implemented caches may result minimal performance improvement may even constitute burden 6 conclusion presented simulation results memory latency effects decoupled access execute architectures since caches timetested mechanism solve memory access problem also compared decoupled architecture performance uniprocessors caches see caches decoupled systems achieve best performance different domains since mechanisms alleviate memory bottleneck depend different characteristics computer program cases might result performance advantage problems one scheme might contribute significantly performance caches potential efficiently hiding main memory latency programs exhibit strong locality properties may also slow system carefully designed one fixed organization cache going used applications heavy risk cache affecting system adversely conditions another observation temporal locality often produces stronger effects spatial locality case decoupled systems note scope improvement decoupling slip main memory fast enough provide bandwidth processor demands slow memory effect memory latency clearly evident total execution time memory poses bottleneck decoupled computers cannot lower execution time total memory access time even region programs might benefit caches strong temporal locality present spite memory posing bottleneck cases speedup decoupled system relative noncached uniprocessor significant also performed simulations determine whether decoupled architectures obtain performance advantage data caches contribution caches minor main memory fast cases strong temporal locality decoupled architectures caches achieve level memory path insensitivity superior configurations concluded caches relevant decoupled computers uniprocessors though used noninterleaved memory studies paper major conclusions paper hold systems interleaved memories also memory latency high interleaving hide latency effects certain latency bandwidth likely become bottleneck beyond certain latency caches would significant decoupled architectures interleaved memory also memory bandwidth bottleneck using noninterleaved memory simply enabled us see effects low latency r performance tradeoffs microprocessor cache mem ories organization architecture tradeoffs fom queue based instruction cache memory functionally parallel architectures array processors performance evaluation onchip register cache organizations improving performance small onchip instruction caches implementation pipe processor pipe vlsi decoupled architecture performance evaluation pipe computer architecture performance analysis address generation coprocessor classification performance evaluation instruction buffering techniques memory latency effects decoupled architectures single data memory module bottlenecks decoupled architecture performance structured memory access architecture features structured memory access sma architecture architecture programmable digital signal processor cache memories decoupled accessexecute computer architecture zs1 central processor line block size choice cpu cache memories dynamic instruction scheduling astronautics zs1 pipe high performance vlsi architecture simulation study decoupled architecture computers performance comparison ibm rs6000 astronautics zs1 effects memory latency finegrain parallelism astronautics zs1 performance performance structured memory access archi tecture evaluation wm architecture simulation study architectural data queues preparetobranch instruction tr simulation study decoupled architecture computers line block size choice cpu cache memories zs1 central processor performance evaluation onchip register cache organizations mips risc architecture dynamic instruction scheduling astronautics zs1 improving performance small onchip instruction caches implementation pipe processor performance comparison ibm rs6000 astronautics zs1 classification performance evaluation instruction buffering techniques memory latency effects decoupled architectures single data memory module evaluation wm architecture cache memories decoupled accessexecute computer architectures performance tradeoffs microprocessor cache memories ctr milidonis n alachiotis v porpodas h michail p kakarountas c e goutis interactive presentation decoupled architecture processors scratchpad memory hierarchy proceedings conference design automation test europe april 1620 2007 nice france roger espasa mateo valero simulation study decoupled vector architectures journal supercomputing v14 n2 p124152 sept 1999 joanmanuel parcerisa antonio gonzalez improving latency tolerance multithreading decoupling ieee transactions computers v50 n10 p10841094 october 2001 w ro stephen p crago alvin despain jeanluc gaudiot design evaluation hierarchical decoupled architecture journal supercomputing v38 n3 p237259 december 2006