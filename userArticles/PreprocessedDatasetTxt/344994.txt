inexact preconditioned conjugate gradient method innerouter iteration important variation preconditioned conjugate gradient algorithms inexact preconditioner implemented innerouter iterations g h golub l overton numerical analysis lecture notes math 912 springer berlin new york 1982 preconditioner solved inner iteration prescribed precision paper formulate inexact preconditioned conjugate gradient algorithm symmetric positive definite system analyze convergence property establish linear convergence result using local relation residual norms also analyze algorithm using global equation show algorithm may superlinear convergence property inner iteration solved high accuracy analysis agreement observed numerical behavior algorithm particular suggests heuristic choice stopping threshold inner iteration numerical examples given show effectiveness choice compare convergence bound b introduction iterative methods solving linear systems usually combined preconditioner easily solved practical problems however natural efficient choice preconditioner may one solved easily direct method thus may require iterative method called inner iteration solve preconditioned equations also exist cases matrix operator contains inverses matrices explicit form available matrixvector products obtained approximately inner iteration linear systems arising saddle point problems 3 one example types problems original iterative method called outer iteration iterative method used solving preconditioner forming matrixvector products called inner iteration critical question use innerouter iterations precision preconditioner solved ie stopping threshold used inner iteration clearly high precision render outer iteration close exact case low one hand could make outer iteration irrelevant optimal one allow stopping address computing computational mathematics program department computer science stanford university stanford ca 94305 usa email golubsccmstanfordedu research supported part national science foundation grant dms9403899 department applied mathematics university manitoba winnipeg manitoba canada r3t 2n2 email yegaussamathumanitobaca research supported natural sciences engineering research council canada threshold large possible order reduce cost inner iteration maintaining convergence characteristic outer iteration words wish combine inner outer iterations total number operations minimized answer question requires understanding accuracy inner iteration affects convergence outer iteration studied golub overton 5 6 chebyshev iteration richardson iteration munthekaas 8 preconditioned steepest descent algorithms elman golub 3 uzawa algorithm saddle point problems giladi golub keller 4 chebyshev iteration varying threshold golub overton also observed interesting phenomenon preconditioned conjugate gradient algorithm outer iteration convergence cg could maintained large stopping threshold inner iteration yet convergence rate may extremely sensitive change threshold certain point given known convergence properties cg depends strongly global minimization property phenomenon found 5 6 seems surprising makes inexact preconditioned conjugate gradient attractive option implementing preconditioners however theoretical analysis explain interesting phenomena extreme sensitivity threshold makes hard implement practice present paper effort direction shall formulate analyze inexact preconditioned conjugate gradient method symmetric positive definite system establishing local relation consecutive residual norms prove linear convergence property bound rate illustrate convergence rate relatively insensitive change threshold certain point par ticular result used arrive heuristic choice stopping threshold also show using global relation 9 algorithm may superlinear convergence property global orthogonality nearly preserved usually occurs smaller thresholds shorter iterations paper organized follows section 2 present inexact preconditioned conjugate gradient algorithm properties together two numerical examples illustrating numerical behaviour give section 3 local analysis showing linear convergence property section 4 global analysis showing superlinear convergence property finally give numerical examples section 5 illustrate results shall use standard notation numerical analysis mnorm k delta k inner product defined kvk respectively conda denotes spectral condition number matrix denotes moorepenrose generalized inverse 2 pcg innerouter iterations consider preconditioned conjugate gradient pcg algorithm solving preconditioner symmetric positive definite step pcg iteration search direction p n found first solving preconditioned system mz direct method available solving iterative method possibly though necessarily cg used solve case find z n inner iteration stopping criterion stopping threshhold inner iteration used gamma1 norm theoretical convenience practical computations one replace res following criterion terms 2norm z n used define search direction p n pcg direction significance therefore also propose following direction based stopping criterion ie acute angle mz n r n gamma1 inner product acute angle z n gamma1 r n minner product remark 1 easily proved using triangular relation gamma1 inner product res satisfied ie ang satisfied sin however converse implication necessarily true therefore criterion ang less restrictive res namely ang may satisfied res inner itera tion however numerical tests suggest usually little difference two introduction ang use gamma1 inner product primarily theoretical convenience remark 2 inner iteration carried cg residual e n orthogonal mz n gamma1 inner product triangular relation case res ang equivalent also proved cg iteration minimizes residual well angle ang krylov subspace concerned formulate inexact preconditioned conjugate gradient algorithm ipcg follows inexact preconditioned conjugate gradient algorithm ipcg z equivalently ff end clearly j algorithm usual pcg j 0 allow preconditioner mz solved approximately remark 3 wellknown several different formulations ff n fi n algo rithm equivalent exact pcg inexact case may longer true particular formulation implies local orthogonality properties maintained even inexact z n given next lemma numerical tests also show indeed leads stable algorithm example fi n computed old form z algorithm may converge larger j lemma 1 sequences generated algorithm ipcg satisfies following local orthogonality proof first z thus z z supposing p thus lemma follows p induction argument eliminating p n recurrence ipcg written two consecutive steps second order recurrence unified form includes chebyshev second order richardson iterations cf 2 local orthogonality obtain following local minimization property proposition 1 proof orthogonality kr gamma1 easy check z n r n 6 0 either res ang ff n 6 0 thus strict inequality second inequality need show r n1 gamma1 az n r follows r recall steepest descent method constructs r sd ar n r n kr sd bound kr sd obtained kantrovich inequality inexact preconditioned version steepest descent method 8 construct r sd n az n kr sd z second inequality proposition shows kr n1 k choosing 21 numerical example motivate discussion next two sections present two numerical examples illustrate typical convergence behaviour ipcg simulate ipcg artificially perturbing applying pcg z pseudo random vector entries uniformly distributed gamma05 05 j perturbation parameter following convergence curves two diagonal matrices 10000 theta 10000 1000 1000 theta 1000 random constant term first note gamma1 norm residuals decreases monotonically cf prop 21 convergence occurs quite large j observe second example smaller superlinear convergence property exact cg recovered fact follow unperturbed case closely j larger convergence tends linear rate depending j interestingly convergence rate insensitive change magnitude j certain point 01 first 04 second around becomes extremely sensitive relatively small change j explanation analysis observation given two categories although clear boundary two smaller j global properties eg orthogonality among r n expected preserved leads global near minimization property thus superlinear convergence larger j global minimization property lost however show local relation r n r n1 destroyed turns preserve linear convergence property consider two categories separately next two sections 3 linear convergence ipcg section present local relation consecutive residual norms lead linear convergence bound ipcg basic idea relate reduction factor ipcg figure 1 convergence curves various perturbations j solid bottom number iterations ainversenorm residuals number iterations ainversenorm residuals reduction factor steepest descent method along inexact preconditioned gradient direction z n see 3 section 2 rrn gammataz n proposition 21 oe n theorem 1 let oe n fl n defined 4 5 ipcg oe z b r r gammag n gammag g k defined proof r r substituting ff apn obtain r thus r using z noting 2 ff gamma2 z z oe n r n used ap substituting 5 8 obtain part part b let oe oe first step ipcg amounts one step steepest descend method definition b follows r oe exact case j orthogonality equations simplified inexact case using local orthogonality lemma 21 j n bounded shall consider stopping criterion ang section since res implies ang sin results apply res case well simply replacing sin j first need following lemma geometrically clear acute angle 2 0 2 two vectors u v inner product delta delta defined cos resp acute angle u u acute angle u 1 v 1 least 2 proof assume u u unit vectors write vector orthogonal u v resp let note u orthogonal v v orthogonal u norm associated cos 1 sin cos 1 sin note second last expression increasing function 2 0 1 therefore bounded value 1 lemma 3 z n satisfies ang 4 z z proof first mz applying lemma inner product defined gamma1 pairs mz satisfy ang obtain ie jz hand jz furthermore easy check vector v min max denotes respectively minimal maximal eigenvalues thus completes proof lemma present bound fl n derived munthekaas 8 shall use one variations bounds 8 theorem 5 repeat arguments 8 first following lemma generalization 1 corollary 4 lemma 4 8 lemma 2 suppose p q 2 r n kpk kqk p q w symmetric positive definite matrix kpk kqk applying lemma p q w obtain r p q 1gammasin r present linear convergence bound ipcg theorem 2 converges even n proof consider two consecutive oe n ipcg apply theorem 1 oe fl defined lemma 5 gammag n1 oe n oe n1 oek 4 therefore even n bound follows kr n1 k shows ipcg converges finally expanding oek terms using terms stopping criterion res result holds sin replaced j see remark 1 therefore converge rate depending oe standard pcg convergence rate particular bound indicates ipcg convergence rate relatively insensitive magnitude j smaller j increases sharply certain point see rate curves fig 2 however bound convergence rate tends pessimistic recover classical bound case seem reflect trend rate changes j changes compare bound actual numerical results consider example similar one section 2 namely consider diagonal matrix whose eigenvalues linearly distributed 1 apply pcg kind random perturbation carry ipcg compute actual convergence rate kr ranging 10 gamma6 1 figure 2 graphs bound actual computed rate plotted case 100 comparing actual convergence rate bound observe bound worst case bound follows trend actual convergence rate curve quite closely bound reaches 1 particular j 0 seems good estimate point actual rate starts increase significantly ie slope greater 1 note slope less 1 increase figure 2 actual convergence rate bound verses j actual rate bound 0050709stopping threshold etasintheta convergence rate actual rate bound 0075085095stopping threshold etasintheta convergence rate rate may compensated comparable increase j therefore advocate value around j 0 heuristic choice stopping threshold inner iterations numerical examples section 5 confirm indeed reasonable strategy balancing numbers inner outer iterations 4 superlinear convergence ipcg bound previous section demonstrates linear convergence ipcg observed section 2 smaller j ipcg may actually enjoy superlinear convergence property exact cg explain phenomenon method 9 ie considering global equation approximately satisfied ipcg remark global property necessary examining superlinear convergence let r respectively obtain following matrix equations ipcg r u n b combining equations 11 obtain following equation r note u n tridiagonal matrix e therefore inexact case satisfies equation similar exact case error term rewrite 13 scaled form 9 eq 8 let 13 r n1 stopping criteria applying argument proof 9 theorem 35 14 obtain following theorem details omitted theorem 3 assume r linearly independent let v ie matrix consisting first n rows r interpret result note kr r n kkr n1 k gamma1 measure orthogonality among residual vectors residuals pcg orthogonal respect gamma1 hence k large loss orthogonality among residuals may gradual take modest length run n magnitude j therefore regime kr n1 k note ffl decreases superlinearly annihilation extreme spectrum see 10 see 9 artificially perturbed numerical examples summary global gamma1 orthogonality among residual vectors nearly maintained certain step residual ipcg close exact pcg point thus may display superlinear convergence property 5 numerical examples section present numerical examples innerouter iterations testing various choices stopping threshold compared 10 purpose shall consider ranging 001 5 well consider homogeneous dirichlet boundary condition using uniform fivepoint finite difference step size n1 obtain n theta n discretization gammarax yr n theta n block tridiagonal matrix consider solving equation using block jacobi preconditioner ie block diagonal part using discrete laplacian l ie l discretization gammadelta preconditioner purpose testing innerouter iterations iterative method ie sor cg preconditioned cg used preconditioners denote cgsor cgcg cgpcg respectively compare number outer n outer total inner n inner iterations required reduce gamma1 norm residual 10 gamma8 used tests first test ax discrete laplacian preconditioner l used sor optimal parameter 11 cg pcg modified incomplete cholesky factorization used inner iteration solve r results listed tables 1 2 3 used table 1 iteration counts cgsor discrete laplacian preconditioner l first rows tables also list results 1 case one step inner iteration carried outer iteration way closely related cases close 1 interestingly however extreme case usually equivalent applying cg directly original matrix preconditioner example inner iteration cg one step inner cg produces inner solution z n direction r n thus outer iteration exactly cg applied see appendix detailed discussion inner solvers convergence still occurs extreme cases analysis based mz would include case however z n chosen satisfy 10 136 136 10 232 232 10 478 478 table 2 iteration counts cgcg discrete laplacian preconditioner l table 3 iteration counts cgpcg discrete laplacian preconditioner l particular way convergence expected relatively small iteration counts due fact original system illconditioned comparing performance different j respect outer iteration counts appears lies right around point outer iteration count starts increase significantly confirms convergence analysis outer iteration total inner iteration counts performance larger j seems irregular among different inner solvers attributed different convergence characteristic extreme case different inner solvers see appendix particular observe larger j cgcg cgpcg performs better cgsor phenomenon also observed 3 overall j 0 seems reasonable choice balancing numbers inner outer iterations example thus j 0 remains nearly constant different n second test use block jacobi preconditioner ax 100 respectively sor cg used inner iteration results listed tables 4 5 appendix similar behaviour observed 6 conclusion formulated analyzed inexact preconditioned conjugate gradient method method proved convergent fairly large thresholds inner iterations linear convergence bound though pessimistic obtained leads heuristic choice stopping threshold inner iteration numerical tests demonstrate efficiency choice still remains unsolved problem choose optimal j minimizes total amount work see 4 although j 0 provides first approximation solving problem demands sharper bound outer iteration analysis near extreme threshold cases clear whether better bound could obtained approach present paper seems properties ipcg awaiting discovery example better bounds steepest descent reduction factor fl n may exist ipcg turn would lead improvement results r inequalities involving euclidean condition matrix generalized conjugate gradient method numerical solution elliptic partial differential equations inexact preconditioned uzawa algorithms saddle point problems inner outer iterations chebyshev algorithm stanford sccm technical report 9512 convergence twostage richardson iterative procedure solving systems linear equations convergence inexact chebyshev richardson iterative methods solving linear systems matrix computations convergence rate inexact preconditioned steepest descent algorithm solving linear systems analysis finite precision biconjugate gradient algorithm nonsymmetric linear systems rate convergence conjugate gradients iterative solution large linear systems academic press tr ctr carsten burstedde angela kunoth fast iterative solution elliptic control problems wavelet discretization journal computational applied mathematics v196 n1 p299319 1 november 2006 angela kunoth fast iterative solution saddle point problems optimal control based wavelets computational optimization applications v22 n2 p225259 july 2002 michele benzi preconditioning techniques large linear systems survey journal computational physics v182 n2 p418477 november 2002