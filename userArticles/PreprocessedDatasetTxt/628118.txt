simple strategies encode tree automata sigmoid recursive neural networks abstractrecently number authors explored use recursive neural nets rnn adaptive processing trees treelike structures one important languagetheoretical formalizations processing treestructured data deterministic finitestate tree automata dfsta may easily realized rnn using discretestate units threshold linear unit recent result sima neural network world7 1997 pp 679686 shows threshold linear unit operating binary inputs implemented analog unit using continuous activation function bounded real inputs constructive proof finds scaling factor weights reestimates bias accordingly paper explore application result simulate dfsta sigmoid rnn analog rnn using monotonically growing activation functions also present alternative scheme onehot encoding input yields smaller weight values therefore works lower saturation level b introduction last decade number authors explored use analog recursive neural nets rnn adaptive processing data laid trees treelike structures directed acyclic graphs arena frasconi gori sperduti 5 recently established rather general formulation adaptive processing structured data focuses directed ordered acyclic graphs includes trees sperduti starita 18 studied classication structures directed ordered graphs including cyclic graphs sperduti 18 studied computational power recursive neural nets structure processors one important languagetheoretical formalizations processing treestructured data deterministic nitestate tree automata dfsta also called deterministic frontiertoroot ascending tree automata19 7 may easily realized rnn using discretestate units threshold linear unit tlu sperduti fact 17 recently shown elmanstyle 3 rnn using tlu may simulate dfsta provides intuitive explanation similar expressed kremer 10 special case deterministic nite automata also work sigmoid networks incrementing gain sigmoid function lead arbitrarily precise simulation step function however unaware attempt establish nite value gain exact simulation may indeed performed analog rnn recent result sma 16 shows tlu operating binary inputs simulated analog unit using continuous activation function bounded real inputs tlu neuron computes output applying threshold step activation function biased linear combination binary inputs corresponding analog neuron works activation function gx two dierent nite limits b x 1 given input output tolerances constructive proof nds scaling factor weights basically value gain analog activation function uses scaling factor shifted value bias paper dene three possible ways encoding dfsta discretestate rnn using tlu explore application smas result turn discretestate rnn sigmoid rnn simulating original sigmoid meaning analog activation function monotonically growing addition present alternative scheme analog simulation yields smaller weight values smas scheme discretestate cases therefore works lower saturation level goal nd smallest possible scaling factor guaranteeing correct behavior last approach assumes onehot encoding inputs generalization approach used 2 stable encoding family nite state machines fsm variety sigmoid discretetime recurrent neural networks similar spirit previous work omlin giles 13 14 deterministic nite automata class fsm particular discretetime recurrent neural network dtrnn architecture secondorder dtrnn used giles et al 6 following section tree automata recursive networks intro duced section 3 describes three dierent schemes encode recursive neural networks discretestate rnn using tlu main result sma16 presented section 4 together similar construction case exclusive also called onehot encoding input section 5 describes conversion discretestate rnn sigmoid counterparts dierent schemes evaluated comparing magnitude resulting weight values finally present conclusions last section tree automata recursive neural network explore neural networks simulate tree automata need specify notation trees describe architecture recursive neural networks 21 trees nitestate machines denote ranked alphabet nite set symbols associated function giving rank symbol 1 subset symbols rank denoted set trees dened set strings made symbols augmented parenthesis comma representing ordered labeled trees recursively 1 0 symbol rank 0 singlenode tree 2 root node label f rank children valid trees belongs 1 rank may dened generally relation r n formulations equivalent symbols one possible rank split deterministic nitestate tree automaton dfsta vetuple nite set states alphabet labels ranked function r f q subset accepting states nite collection transition functions form maximum rank valence dfsta trees 2 result 2 q operation dfsta tree 2 dened undened otherwise 2 words state associated given tree depends label root node f also states ndfsta associates children 1 convention undened transitions lead unaccepted trees maximum number children node tree la usual language la recognized dfsta subset dened one may generalize denition dfsta produces output label node visited acts like structurepreserving nitestate tree transducer two generalizations possible correspond classes nitestate string transducers known mealy moore machines9 15 mealy tree transducers obtained replacing subset accepting states f denition dfsta collection output functions g one possible rank moore tree transducers obtained replacing f single output function whose argument new state conversely dfsta regarded particular case mealy moore machine operating trees whose output functions return two values 22 neural architectures dene two recursive neural architectures similar used related work frasconi gori sperduti 5 sperduti 17 sperduti starita 18 nd convenient talk mealy moore neural networks dene way networks compute output using analogy corresponding nitestate tree transducers rst architecture highorder mealy rnn second one rstorder moore rnn 2 221 highorder mealy recursive neural network highorder mealy recursive neural network consists two sets singlelayer neural networks rst one compute next state playing role collection transition functions nitestate tree transducer second one compute output playing role collection output functions mealy nitestate tree transducer nextstate function realized collection singlelayer networks one possible rank nx neurons m1 input ports input subtree state vectors 2 remaining two combinations highorder moore rnn may easily shown computational power mealy counterparts rstorder mealy machines need extra layer compute arbitrary output functions see dimensionality nx one input node labels represented vector dimensionality n u node label input port takes input vectors equal dimensionality number input symbols n particular node tree label l u input vector associated node component u k equal 1 input symbol node k 0 input symbols onehot exclusive encoding node label next state x computed corresponding 1th order singlelayer neural net follows represents bias network rank leaf ie l 2 0 expression component x reduces ik set jj weights type w 0 nxk play role initial state recurrent networks 4 output function realized collection networks n units input structure output function node rank evaluated 222 rstorder moore recursive neural network rstorder moore recursive neural network collection nextstate functions one rank form ik structure input ports highorder counterpart single output function form taking nx inputs producing n outputs 3 encoding tree automata discretestate recursive neural networks present three dierent ways encode nitestate recursive transducers discretestate rnn using tlu activation functions rst two use discretestate version highorder mealy rnn described section 221 third one uses discretestate version rst order moore rnn described section 222 rst two encodings straightforward third one explained detail encodings based exclusive onehot encoding states nitestate transducers n rnn said state component x nx dimensional state vector x takes high value components take low value addition exclusive encoding inputs n outputs n used one discretestate rnn encodings converted section 5 sigmoid rnn encodings using two strategies described section 4 31 highorder mealy encoding using biases assume mealy nitestate tree transducer discretestate highorder mealy rnn weights weights bias v behaves stable simulator nitestate tree transducer note uppercase letters used designate weights discretestate neural nets would also case biases set value 0 1 value 12 happens best value conversion sigmoid rnn 32 highorder mealy encoding using biases second possible encoding treetransducing counterpart string transducer encoding described 12 2 uses bias nextstate weights biases w output weights biases v case biased construction encoding also works biases set value 1 1 0 optimal value conversion sigmoid rnn 33 rstorder moore encoding consider moore nitestate tree transducer form encoding dfsta rstorder rnn need split states rst statesplitting found necessary implement arbitrary nite state machines rstorder discretetime recurrent neural networks 8 1 2 also recently described sperduti 17 encoding rnn easily shown equivalent easily constructed using method described sperduti 17 follows new set states q 0 subset dened nextstate functions new set dened follows shorthand notation used finally new output function dened follows split encoded discretestate rnn eqs 7 8 choosing parameters follows 3 exist q 0 jm 0 zero otherwise exist q 0 l zero otherwise zero otherwise dicult show operation discretestate rnn equivalent corresponding therefore case previous constructions dierent values biases also possible ones shown happen optimal conversion sigmoid rnn stable simulation discretestate units analog units 41 using smas theorem following restatement theorem sma 16 included convenience notation slightly changed order adapt present study 3 remember uppercase letters used denote weights discretestate rnn threshold linear unit tlu neuron computing output g h threshold activation function w j j realvalued weights binary input vector consider analog neuron weights w activation function g two dierent limits lim x1 gx magnitude max called maximum input tolerance finally let also mapping dened undened otherwise mapping classies output analog neuron three categories low 0 high 1 forbidden undened let r mapping r kg smas theorem states input tolerance 0 max output tolerance 0 exists analog neuron activation function g weights w r x 2 f0 1g n according constructive proof theorem 16 set sucient conditions equation hold b b jgx aj x jgx bj x jj jj small possible smas prescription simply scales weights tlu get analog network bias shifting conveniently avoid zero value argument activation function note inputs analog unit allowed within 0 1 whereas outputs allowed within b constructing recursive network outputs one analog unit normally used inputs another analog unit therefore natural choice choice compatible instance use logistic function g l whose limits exactly 0 activation functions hyperbolic tangent tanhx particular case eqs 23 24 reduce 25 becomes following rederive simple sucient conditions stable simulation tlu analog unit suitable strictly growing activation function restricted exclusive encoding input whereas smas construction valid binary input vector simplicity prescriptions allows alternate straightforward worstcase analysis leads weights common situations smaller obtained direct application smas theorem 42 using simple scheme exclusive encoding input conditions stable simulation nitestate machines fsm dtrnn studied following approach related sma16 carrasco et al 2 see also 12 11 conditions assume special usual case onehot exclusive encoding input strictly growing activation functions assumptions together worstcase analysis allow one obtain prescription choice suitable weights stable simulation works lower saturation levels general scheme eqs 2325 usually prescription realized singleparameter scaling weights tlu including bias scaling equivalent nding nite value gain sigmoid function ensures correct behavior note case exclusive encoding ones used section 3 n possible inputs binary vectors b vector whose ith component one rest zero therefore argument may take n dierent values w 0 w binary input however analog neuron input tolerance may receive input vectors r b ig property exclusive encoding makes possible formulate condition 22 holds possible inputs two cases distinguished 1 case 22 holds g strictly growing may also write w obviously minimum value w jg therefore sucient condition analog neuron simulate corresponding tlu input b 2 similar argument leads sucient condition instance choose w eqs 29 30 fullled either order compare eqs 2325 last two conditions may written single restrictive pair conditions simple choice w adequate case w 0 case appear encodings proposed section 3 5 encoding tree automata sigmoid recursive neural networks mentioned theorem section 4 leads natural choice addition applying neurons recursive neural networks due widespread use consider compare section various possible encodings using logistic function g although results dierent activation functions may also obtained indeed monotonic growth function along real line enough following derivation case eqs 29 30 case want simulate sigmoid rnn consider rst highorder mealy rnn architecture input x j 22 case rnn described 4 product outputs x jm one range 0 1 product always range 0 1 words forbidden region 1 dicult show equality holding therefore conditions max suce purposes 1 want use scaling factor weights possible ranks use consider rstorder moore rnn architecture case products conditions max sucient previous section describes two dierent schemes simulate discretestate neurons taking exclusive input vectors sigmoid neurons section describes application two schemes three recursive neural network architectures described section 22 51 using simas prescription smas construction section 41 gives biased highorder mealy rnn section 31 following therefore w condition 35 applied 4 together 1 condition ensures positive value h shown 37 minimum value allowed h depends nx given architecture nx maximum value xed changed exists least one value allows one choose minimum value h needed stable simulation sense minimization h choosing appropriate performed 2 leads values shown table 1 minimum required h function nx weights obtained grow slower logmn x seen inordinately large lead therefore saturated analog rnn applying smas construction biasless highorder mealy rnn sec tion 32 get 4 used equations 4 6 nu nx terms due exclusive encoding inputs nu 1n terms identically zero uncertainty together 1 positive values h weights obtained searching minimum h satisfying conditions shown table 2 seen weights show asymptotic behavior ones previous construction smaller still large avoid saturation finally applying smas construction 5 rstorder moore rnn section 33 nextstate function rank accordingly weights exist q 0 jm 0 zero otherwise exist q 0 l zero otherwise 36 used together 1 output function accordingly weights zero otherwise smas construction applied provided consider possible w ik u k nextstate function dierent bias w 0 u 2 f0 1g n choose safest prescription valid possible values bias present case bias always value 1 number additive terms provided 1 used want single value h assign weights nextstate functions output function use weights obtained searching minimum h satisfying conditions shown table 3 grow slower logmnx seen equal smaller ones biased highorder construction larger biased construction however fact state splitting leads larger values nx automata transition function taken account 52 using encoding exclusive inputs choose w including biases obtain biased highorder mealy encoding section 31 substituting eqs 31 32 together 1 happens expression one obtained previous section using smas construction biasless encoding section 32 results shown table 2 results obviously identical reported secondorder discretetime recurrent neural networks using biased construction 2 instead apply alternate encoding biasless highorder mealy construction section 32 get together 1 suitable minimization h leads best possible weights encodings weights grow nx slower logmn results shown table 4 previous case results obviously identical reported secondorder discretetime recurrent neural networks using biases 2 finally apply alternate encoding scheme rstorder moore construction section 33 particular form 33 case valid combinations w 0 take value w take value minimum value jw 0 exclusive values state vectors equal 1 therefore together 1 suitable minimization leads weights grow nx slower logmnx values shown table 5 values smaller ones obtained smas construction rstorder network still large especially one considers splitting leads large values nx 6 conclusion studied four strategies encode deterministic nitestate tree automata dfsta highorder sigmoid recursive neural networks rnn two strategies encode rstorder sigmoid rnn six strategies derived three dierent strategies encode dfsta discretestate rnn rnn using threshold linear units applying two dierent weight mapping schemes convert one sigmoid rnn rst mapping scheme one described sma16 second one alternate scheme devised us strategies yield analog rnn simple weight alphabet containing three weights proportional single parameter h best results ie smallest possible value h would desired derivativebased learning setting obtained appling alternate scheme biasless discretestate highorder rnn mentioned smas mapping yields larger weights cases general would also work distributed encodings allow construction smaller rnn constructions values h suggest even though principle rnn nite weights able simulate exactly behavior dfsta practice dicult learn exact nitestate behavior examples small gradients present weights reach adequately large values smaller weights obtained cost enlarging size rnn due exclusive encoding states inputs smas result also works distributed encodings acknowledgements work supported spanish comision interministerial de ciencia tecnologa grant tic970941 r finding structure time learning initial state secondorder recurrent neural network regularlanguage inference general framework adaptive data structures processing learning extracted syntactical pattern recognition introduction automata theory computational power elmanstyle recurrent net works constructing deterministic stable encoding large formal languages computational power neural networks struc tures supervised neural networks classi tree automata informal survey tr ctr barbara hammer alessio micheli alessandro sperduti marc strickert recursive selforganizing network models neural networks v17 n89 p10611085 octobernovember 2004 barbara hammer peter tio recurrent neural networks small weights implement definite memory machines neural computation v15 n8 p18971929 august henrik jacobsson rule extraction recurrent neural networks taxonomy review neural computation v17 n6 p12231263 june 2005