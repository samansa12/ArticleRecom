cache sensitive modulo scheduling paper focuses interaction software prefetching binding nonbinding software pipelining vliw machines first shown evaluating software pipelined schedules without considering memory effects rather inaccurate due stalls caused dependences memory instructions even lockupfree cache considered also shown penalty stalls general higher effect spill code second show general binding schemes powerful nonbinding ones software pipelined schedules finally main contribution paper heuristic scheme schedules memory operations according locality estimated compile time attributes dependence graph proposed scheme shown outperform heuristic approaches since achieves better tradeoff compute stall time others b introduction software pipelining wellknown loop scheduling technique tries exploit instruction level parallelism overlapping several consecutive iterations loop executing parallel 14 different algorithms found literature generating software pipelined sched ules popular scheme called modulo scheduling main idea scheme find fixed pattern operations called kernel steady state consists operations distinct iterations finding optimal scheduling resource constrained scenario npcomplete problem practical proposals based different heuristic strategies key goal schemes achieve high throughput eg 14112018 minimize register pressure eg 96 eg 1015716 none evaluated effect memory schemes assume fixed latency memory operations usually corresponds cachehit latency lockupfree caches allows processor stall cache miss however vliw architecture processor often stalls afterwards due true dependences previous memory operations alternative scheduling loads using cachemiss latency requires considerable instruction level parallelism increases register pressure 1 software prefetching effective technique tolerate memory latency 4 software prefetching performed two alternative schemes binding nonbinding prefetch ing first alternative also known early scheduling memory operations moves memory instructions away instructions depend second alternative introduces code special instructions called prefetch instructions nonfaulting instructions perform cache lookup modify register alternative prefetching schemes different drawbacks binding scheme increases register pressure lifetime value produced memory operation stretched may also increase initiation interval due memory operations belong recurrences nonbinding scheme increases memory pressure since increases number memory requests may produce increase initiation interval besides may produce increase register pressure since lifetime value used compute effective address stretched higher register pressure may require additional spill code results additional memory pressure paper investigate interaction software prefetching software pipelining vliw machine first show previous schemes consider effect memory penalties produce schedules far optimal evaluated taking account realistic cache memory evaluate several heuristics schedule memory operations insert prefetch instructions software pipelined schedule contributions stalls spill code quantified case showing stall penalties much higher impact performance spill code propose heuristic tries trade initiation interval stall time order minimize execution time software pipelined loop finally show schemes based binding prefetch effective based nonbinding prefetch software pipelined schedules use binding nonbinding prefetching previously studied 121 4813173 respectively among others however knowledge previous work analyzing interactions prefetching schemes software pipelining techniques selective scheduling 1 schedules operations cachehit latency others cachemiss latency like scheme proposed paper however selective scheduling based profiling information whereas method based static analysis performed compiletime addition selective scheduling consider interactions software pipelining rest paper organized follows section 2 motivates impact memory latency may software pipelined loop section 3 evaluates performance simple schemes scheduling load stores instructions section 4 describes new algorithm proposed paper section 5 explains experimental methodology presents performance results finally main conclusions summarized section 6 2 motivation software pipelined loop via modulo scheduling characterized basically two terms initiation stage counter former indicates number cycles initiation successive iterations latter shows many iterations lapped way execution time loop calculated given architecture given scheduler first term sum called compute time rest paper fixed determined compile time stall time mainly due dependences previous memory instructions depends runtime behavior program eg miss ratio outstanding misses etc order minimize execution time classical methods tried minimize initiation interval goal reduce fixed part exec minimum initiation interval bounded resources recurrences lower bound due resource constraints architecture assuming functional units pipelined calculated indicates number operations type loop body indicates number functional units type architecture lower bound due recurrences graph computed represents sum node latencies recurrence represents sum edge distances recurrence particular data flow dependence graph given architecture resulting ii dependent latency scheduler assigns operation latency operations usually known compiler except memory operations variable latency ii also depends affected spill code introduced scheduler parameters fixed conventional modulo scheduling proposals use fixed latency usually cachehit time schedule memory instructions scheduling instructions minimum latency minimize register pressure thus reduces spill code hand minimum latency scheduling increase stall time data dependences particular operation needs data loaded previous instruction memory access finished yet processor stalls data available figure 1 shows sample scheduling data dependence graph given architecture case memory instructions scheduled cachehit latency stall time ignored ii sc res ii rec ii res ii res max op arch nops op nfus op ii rec ii rec max rec graph lat rec dist rec usual studies dealing software pipeline techniques expected optimistic execution time suppose huge obviously optimistic estimation actual execution time rather inaccurate instance suppose miss ratio n1 load operation 025 eg stride 1 4 elements per cache line every cache miss processor stalls cycles called penalty penalty particular memory instruction depends hit latency miss latency distance scheduling memory operation first instruction uses data produced memory instruction dependence n1 n2 penalty 9 cycles stall time assuming remaining dependences produce penalty therefore case actual execution time near twice optimistic execution time assume miss ratio 1 instead 025 discrepancy optimistic actual execution time even higher case stall time therefore load mult load add store1357n1 alu mem b data flow dependence graph b code scheduling c kernel instruction latencies loadstore original code enddo figure 1 sample scheduling memory references considered effect stall time could greater discrepancy optimistic estimation usually utilized evaluate performance software pipelined schedulers actual performance could much higher also conclude scheduling schemes try minimize stall time may provide significant advantage paper proposed scheduler evaluated compared others using exec metric requires consider runtime behavior individual memory references requires simulation memory system 3 basic schemes schedule memory operations section evaluate performance basic schemes schedule memory operations point drawbacks motivates new approach proposed next section already mentioned previous section modulo scheduling schemes usually schedule memory operations using cachehit latency scheme called cachehit latency chl scheme expected produce significant amount processor stalls suggested previous section approach reduce processor stall insert prefetch instruction every memory operation instructions scheduled distance equal cachemiss latency actual memory references scheme called insert prefetch always ipa however scheme may result increase number operations due prefetch instructions also additional spill code therefore may require ii higher previous approaches finally alternative approach schedule memory operations using cachemiss latency scheme called early scheduling always esa scheme prefetches data without requiring additional instructions may result increase ii memory instructions recurrences besides may also require additional spill code figure 2 compares performance three schemes specfp95 benchmarks two different architectures details evaluation methodology architecture given section 5 column split compute stall time figure also shown lower bound execution time opt lower bound corresponds execution programs memory operations scheduled using cachehit latency minimizes spill code assuming always hit cache results null stall time lower bound defined optimistic execution time section 2 main conclusion drawn figure 2 performance three realistic schemes far away lower bound general chl scheme results significant percentage stall time aggressive architecture stall time represents 50 execution time programs ipa scheme reduces significantly stall time completely due fact programs especially tomcatv swim cache interfering instructions short distance therefore prefetches always effective may collide replace data used besides ipa scheme results significant increase compute time programs eg hydro2d turb3d among others esa scheme practically eliminates stall time remaining stall time basically due lack entries outstanding miss table used implement lockupfree cache however scheme increases significantly compute time programs like turb3d factor 3 aggressive architecture mgrid hydro2d due memory references recurrences limit ii 4 csms algorithm section propose new algorithm called cache sensitive modulo scheduling csms tries minimize compute time stall time terms independent reducing one may result increase shown previous section proposed algorithm tries find best tradeoff two terms csms algorithm based early scheduling selectively chosen memory operations scheduling memory operation using cachemiss latency hide almost memory latency shown previous section without increasing much number instructions opposed use prefetch instructions however increase execution time three ways may increase register pressure therefore may increase due spill code performance loop bounded memory operations may increase latency memory operations augmented may increase length individual loop iterations may increased augments cost prolog epilog figure 2 basic schemes performance chl ipa esa opt chl ipa esa opt chl ipa esa opt chl ipa esa opt chl ipa esa opt chl ipa esa opt020610 normalized loop execution time tomcatv swim su2cor hydro2d mgrid turb3d specfp95chl ipa esa opt chl ipa esa opt chl ipa esa opt chl ipa esa opt chl ipa esa opt chl ipa esa opt020610 normalized loop execution time tomcatv swim su2cor hydro2d mgrid turb3d 1257 3084 simple architecture b aggressive architecture ii ii rec two main issues csms algorithm reduction impact recurrences ii minimization stall time problem cost prolog epilog handled computing two alternative schedules focus minimizing stall time ii however one reduces impact prolog epilog expense increase stall time whereas care prolog epilog cost depending number iterations loop effective one chosen core csms algorithm shown figure 3 algorithm makes use static locality analysis addition issues order determine latency considered scheduling individual instruction locality analysis based analysis presented 19 divided three steps reuse analysis computes intrinsic reuse property memory instruction proposed 21 goal determine kind reuse exploited reference loop five types reuse determined none selftemporal selfspatial group temporal groupspatial interference analysis using initial address reference previous reuse analysis determines whether two static instructions always conflict cache besides selfinterferences also taken account considering stride exhibited static instruction references interfere refer figure 3 csms algorithm function csmsinnerloop il return scheduling recurrencesingraph else endif niter upperbound return sch1 else return sch2 endif endfunction function computeschedminreceffectgraph g return scheduling res foreach recurrence r g ii rec r ii endif endforeach return computeschedulingg endfunction function minimizerecurrenceeffectrec r int ii return integer orderinstructionsbylocalityr ii rec r ii endwhile return endfunction figure 4 scheduling loop recurrences ences considered type locality even exhibit type reuse volume analysis determines references cannot exploit reuse displaced cache based computing amount data used reference loop analysis concludes reference expected exhibit locality reuse interfere including volume data consecutive reuses lower cache size initially two data dependence graphs nodes edges generated difference latency assigned node grph1 memory node tagged according locality analysis tagged cachehit latency exhibits type locality cachemiss latency otherwise grph2 memory nodes tagged cachemiss latency schedule minimizes impact recurrences ii computed graph using function computeschedminreceffect shown figure 4 first step function change latency memory operations inside recurrences limit ii cachemiss cachehit ii limited resources constraining recurrence nodes modified chosen according locality priority order starting ones exhibit locality second step compute actual scheduling using modified graph step performed software pipelined schedulers proposed literature finally minimum number iterations upperbound ensures sch2 better sch1 computed main difference two schedules cost prolog epilog parts lower sch1 bound depends computed schedules results locality analysis calculated estimation execution time schedule sch1 chosen execution time given schedule estimated stall time estimated penalty calculated explained section 2 missratio estimated locality analysis way sch1 preferred sch2 use scheduling according locality chl achieves minimum sc order take account possible poor locality loops 5 performance evaluation csms section present performance evaluation csms algorithm compare performance basic schemes evaluated section 3 also compared alter est est stall est niter penalty op op mem penalty latmiss cycleuse cycleprod op mem native binding early scheduling nonbinding inserting prefetch instructions prefetch schemes 51 architecture model vliw machine considered evaluate performance different scheduling algorithms modeled two architectures order evaluate different aspects produced schedulings execution time stall time spill code etc first architecture called simple composed four functional units integer floating point branch memory cachemiss latency first level cache 10 cycles second architecture called aggressive two functional units type cachemiss latency 20 cycles functional units fully pipelined except divide square root operations models first memory level corresponds 8kb lockupfree directmapped cache lines 32 bytes 8 outstanding misses features modeled architectures depicted table 1 modeled architectures two reasons processor stall instruction requires operand available yet eg read second level cache b memory instruction produces cache miss already 8 outstanding misses 52 experimental framework locality analysis scheduling task performed using ictineo toolset 2 ictineo source source translator produces code sentence semantics similar current machine instructions translating code lowlevel representation applying classical optimizations dependence graph innermost loop constructed according particular prefetching approach instructions scheduled instructions latency machine model simple aggressive integer branch fus 1 2 floating point memory cache size 8 kb div sqrt pow 12 line size outstanding misses 8 control memory latency 110 120 branch 2 number registers 32 call return 4 table 1 modeled architectures using software pipelining algorithm particular software pipelining algorithm used experiments reported hrms 15 shown effective minimize ii register pressure resulting code instrumented generate trace feeds simulator architec ture program run first 100 million memory references performance figures shown section refer innermost loops contained part program measured memory references inside innermost loops represent 95 memory instructions considered benchmark statistics innermost loops quite representative whole section program different prefetching algorithms evaluated following specfp95 benchmarks tomcatv swim su2cor hydro2d mgrid turb3d restricted evaluation fortran programs since currently ictineo tool process fortran codes 53 early scheduling section compare csms algorithm schemes based early scheduling memory operations schemes use always cachehit latency chl ii use always cachemiss latency esa iii schedule instructions type locality using cachehit latency schedule remaining ones using cachemiss latency later scheme called early scheduling according locality esl different algorithms evaluated terms execution time split compute stall time stall time due dependences lack entries outstanding miss table figure 5 see results simple aggressive architectures benchmark columns normalized chl execution time seen csms algorithm achieves compute time close chl scheme whereas chl esa esl csms chl esa esl csms chl esa esl csms chl esa esl csms chl esa esl csms chl esa esl csms020610 normalized loop execution time tomcatv swim su2cor hydro2d mgrid turb3d 1593 1371 chl esa esl csms chl esa esl csms chl esa esl csms chl esa esl csms chl esa esl csms chl esa esl csms020610 normalized loop execution time tomcatv swim su2cor hydro2d mgrid turb3d simple architecture b aggressive architecture figure 5 csms algorithm compared early scheduling stall time close esa scheme results best tradeoff compute stall time programs recurrences limit initiation interval therefore esa scheme increases compute time instance hydro2d turb3d benchmarks csms method minimize effect expense slight increase stall time table 2 shows relative speedup different schedulers respect chl scheme average alternative schedulers outperform chl scheme usually one used software pipelining schedulers however programs mainly turb3d esa esl schedulers perform worse chl due increase ii caused recurrences csms algorithm achieves best performance benchmarks simple architecture average speedup 161 aggressive architecture 247 table 3 compares csms algorithm optimistic execution time opt defined section 3 used lower bound execution time also shows percentage execution time processor stalled seen simple architecture csms algorithm close optimistic bound cause almost stall aggressive architecture performance csms worse opt stall time represents 10 total execution time notice however optimistic bound could quite actual minimum execution time table 4 compares different schemes using chl algorithm reference point schemes shows increase compute time decrease stall time seen scheduling memory operations using cachemiss latency affect initiation interval stage counter results increase compute time column denoted dcompute represents increment compute time compared chl scheduling scheme calculated stall time due dependences eliminated scheduling memory instructions using cachemiss latency default spill code scheduled using cachehit latency therefore may cause stalls although unlikely spill code usually store followed load address since usually close otherwise spill code hardly reduces register pressure load cause stall interferes memory esa esl csms esa esl csms tomcatv 234 228 257 392 341 556 su2cor hydro2d 113 100 145 113 100 278 mgrid 115 100 117 112 100 119 turb3d 062 073 118 027 033 142 geometric mean 136 122 161 148 115 247 table 2 relative speedup dcompute stall reference store column denoted stall represents percentage stall time caused chl algorithm avoided scheme calculated see table 4 csms algorithm achieves best tradeoff compute time stall time reason outperforming others esa scheme best one reduce stall time expense large increment compute time 54 inserting prefetch instructions order reduce penalties caused memory operations alternative early scheduling memory instructions inserting prefetch instructions provided many current instruction set architectures eg touch instruction powerpc 5 new scheme introduce additional spill code since increases register pressure particular lifetime values used compute effective address increased since used optcsms stall optcsms stall su2cor 0972 192 0873 1117 hydro2d 0978 018 0962 184 mgrid 0998 turb3d 0951 254 0709 1954 geometric table 3 csms compared opt scheduling esa esl csms esa esl csms dcompute stall dcompute stall dcompute stall dcompute stall dcompute stall dcompute stall su2cor 1048 10000 1015 254 1009 9590 1215 9767 1060 409 1018 9327 hydro2d 1308 9999 1008 379 1021 9962 2532 9985 1067 484 1020 9898 mgrid 1023 9989 0999 359 1001 9968 1469 8758 1030 519 1337 8757 turb3d 1184 9435 1055 8575 1184 9435 7222 9821 5948 8759 1134 7248 geometric table 4 increment compute time decrement stall time relation chl stall chl stall prefetch ordinary memory instructions also increase initiation interval due additional memory instructions evaluated three alternative schemes introduce prefetch instructions insert prefetch always ipa ii insert prefetch references without temporal locality even exhibit spatial locality according static locality analysis ipt iii insert prefetch instructions without type locality ipl first scheme expected result stalls requires many additional instructions may increase ii ipt scheme selective adding prefetch instruction however adds unnecessary prefetch instructions references spatial locality instructions spatial locality cause cache miss new cache line accessed cache ipl scheme conservative sense adds less number prefetch instructions figure 6 compared total execution time csms scheduling abovementioned prefetching schemes figures normalized chl scheduling csms scheme always performs better schemes based inserting prefetch instructions except mgrid benchmark aggressive architecture latter case ipa scheme best one performance csms close among schemes insert prefetch instructions none outperforms others general depending particular program architecture best one among different one prefetch schemes outperform chl scheme general ie performance figures figure 6 general lower 1 cases may even worse chl general worse schemes based early scheduling comparing binding figure 5 nonbinding figure schemes observed binding prefetch always better three first benchmarks schemes similar per simple architecture b aggressive architecture figure 6 csms algorithm compared inserting prefetch instructions ipa ipt ipl csms ipa ipt ipl csms ipa ipt ipl csms ipa ipt ipl csms ipa ipt ipl csms ipa ipt ipl csms020610 normalized loop execution time tomcatv swim su2cor hydro2d mgrid turb3d ipa ipt ipl csms ipa ipt ipl csms ipa ipt ipl csms ipa ipt ipl csms ipa ipt ipl csms ipa ipt ipl csms020610 normalized loop execution time tomcatv swim su2cor hydro2d mgrid turb3d formance next two benchmarks last one nonbinding prefetch outperforms binding schemes understand reasons behavior prefetch schemes present additional statistics aggressive architecture table 5 shows percentage additional memory instructions executed csms algorithm schemes based inserting prefetch instructions csms algorithm additional instructions due spill code whereas schemes due spill code prefetch instructions see table except ipl scheme mgrid benchmark prefetch schemes require much higher number additional memory instructions expected increase number memory instructions ipa scheme highest followed ipt ipl finally csms table 6 shows increase compute time decrease stall time schemes based inserting prefetch instructions relation chl scheme negative numbers indicate stall time increased instead decreased see table 6 compute time increased prefetching schemes since large number additional instructions may imply significant increase ii loops memory bound stall time general reduced reduction less csms scheme see table 4 program mgrid one prefetch based scheme ipa outperforms csms algorithm however difference slight remaining programs performance csms scheme overwhelmingly better ipa scheme table 7 shows miss ratio different prefetching schemes compared miss ratio nonprefetching scheme chl see general schemes insert memory prefetches produce highest reductions miss ratio however inserting prefetch instructions remove cache misses even scheme inserts prefetch every memory instruction ipa due cache interferences prefetch instructions 1there spill code simulated part program csms inserting prefetch instr ipa ipt ipl su2cor hydro2d 212 5549 3994 285 mgrid 4990 5926 5657 750 table 5 percentage additional memory references prefetched data used quite common programs tomcatv swim instance two memory references interfere cache close code likely two prefetches corresponding scheduled memory references case least one two memory references miss spite prefetch besides prefetches memory instructions scheduled reverse order ie instruction scheduled b prefetch b scheduled prefetch memory instructions miss summarize two main reasons bad performance schemes based inserting prefetch instructions compared csms scheme increase compute time due additional prefetch instructions spill code always effective removing stalls caused cache misses due interferences prefetch instructions ipa ipt ipl dcompute stall dcompute stall dcompute stall tomcatv 1396 2328 1060 6714 1073 1942 su2cor 1454 7448 1269 8220 1090 325 hydro2d 1608 8487 1086 8681 1003 416 mgrid 1311 8832 1267 3544 1030 537 turb3d 1874 6891 1787 7390 1497 8260 geometric table 6 increment compute time decrement stall time schemes based inserting prefetch instructions chl ipa ipt ipl su2cor 2543 235 568 2155 hydro2d 1957 133 504 1880 mgrid 646 057 291 535 turb3d 1068 211 239 264 geometric table 7 miss ratio chl different prefetching schemes 6 conclusions interaction software prefetching software pipelining techniques vliw architectures studied shown modulo scheduling schemes using cachehit latency produce many stalls due dependences memory instructions simple architecture stall time represents 32 execution time 63 aggressive architec ture thus ignoring memory effects evaluating software pipelined scheduler may rather inaccurate compared performance different prefetching approaches based either early scheduling memory instructions binding prefetch inserting prefetch instructions nonbinding prefetch seen provide significant improvement general however methods based early scheduling outperform based inserting prefetches main reasons worse performance latter methods increase memory pressure due prefetch instructions additional spill code limitation remove shortdistance conflict misses proposed heuristic scheduling algorithm csms based early scheduling tries minimize compute stall time algorithm makes use static locality analysis schedule instructions recurrences shown outperforms rest strategies instance compared approach based scheduling memory instructions using cachehit latency produced code 16 times faster simple architecture 25 times faster aggressive architecture former case also shown execution time close optimistic lower bound r tr software pipelining effective scheduling technique vliw machines software prefetching data locality optimizing algorithm circular scheduling architecture softwarecontrolled data prefetching design evaluation compiler algorithm prefetching lifetimesensitive modulo scheduling balanced scheduling evolution powerpc architecture iterative modulo scheduling minimizing register requirements resourceconstrained rateoptimal software pipelining optimum modulo schedules minimum register requirements compiler techniques data prefetching powerpc stage scheduling hypernode reduction modulo scheduling predictability loadstore instruction latencies decomposed software pipelining static locality analysis cache management swing modulo scheduling ctr jess snchez antonio gonzlez instruction scheduling clustered vliw architectures proceedings 13th international symposium system synthesis september 2022 2000 madrid spain enric gibert jess snchez antonio gonzlez effective instruction scheduling techniques interleaved cache clustered vliw processor proceedings 35th annual acmieee international symposium microarchitecture november 1822 2002 istanbul turkey jess snchez antonio gonzlez modulo scheduling fullydistributed clustered vliw architecture proceedings 33rd annual acmieee international symposium microarchitecture p124133 december 2000 monterey california united states enric gibert jess snchez antonio gonzlez local scheduling techniques memory coherence clustered vliw processor distributed data cache proceedings international symposium code generation optimization feedbackdirected runtime optimization march 2326 2003 san francisco california javier zalamea josep llosa eduard ayguad mateo valero twolevel hierarchical register file organization vliw processors proceedings 33rd annual acmieee international symposium microarchitecture p137146 december 2000 monterey california united states enric gibert jess snchez antonio gonzlez interleaved cache clustered vliw processor proceedings 16th international conference supercomputing june 2226 2002 new york new york usa javier zalamea josep llosa eduard ayguad mateo valero modulo scheduling integrated register spilling clustered vliw architectures proceedings 34th annual acmieee international symposium microarchitecture december 0105 2001 austin texas alex alet josep codina jess snchez antonio gonzlez graphpartitioning based instruction scheduling clustered processors proceedings 34th annual acmieee international symposium microarchitecture december 0105 2001 austin texas enric gibert jesus sanchez antonio gonzalez distributed data cache designs clustered vliw processors ieee transactions computers v54 n10 p12271241 october 2005 javier zalamea josep llosa eduard ayguad mateo valero software hardware techniques optimize register file utilization vliw architectures international journal parallel programming v32 n6 p447474 december 2004