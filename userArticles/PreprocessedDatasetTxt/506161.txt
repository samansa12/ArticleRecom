mesh partitioning efficient use distributed systems mesh partitioning homogeneous systems studied extensively however mesh partitioning distributed systems relatively new area research ensure efficient execution distributed system heterogeneities processor network performance must taken consideration partitioning process equal size subdomains small cut set size results conventional mesh partitioning longer primary goals paper address various issues related mesh partitioning distributed systems issues include metric used compare different partitions efficiency application executing distributed system advantage exploiting heterogeneity network performance present tool called part automatic mesh partitioning distributed systems novel feature part considers heterogeneities application distributed system simulated annealing used part perform backtracking search desired partitions wellknown simulated annealing computationally intensive describe parallel version simulated annealing used part results parallelization exhibit superlinear speedup cases nearly perfect speedup remaining cases experimental results also presented partitioning regular irregular finite element meshes explicit nonlinear finite element application called whams2d executing distributed system consisting two ibmsps different processors results regular problems indicate 33 46 percent increase efficiency processor performance considered compared conventional even partitioning results indicate 5 15 percent increase efficiency network performance considered compared considering processor performance significant given optimal improvement 15 percent application results irregular problem indicate 36percent increase efficiency processor network performance considered compared even partitioning b introduction distributed computing regarded future high performance computing nationwide high speed networks vbns 25 becoming widely available interconnect highspeed computers virtual environments scientific instruments large data sets projects globus 15 legion 20 developing software infrastructure integrates distributed computational informational resources paper present mesh partitioning tool distributed systems tool called part takes consideration heterogeneity processors networks found distributed systems well heterogeneities found applications mesh partitioning required efficient parallel execution finite element finite difference applications widely used many disciplines biomedical engineering structural mechanics fluid dynamics applications distinguished use meshing procedure discretize problem domain execution meshbased application parallel distributed system involves partitioning mesh subdomains assigned individual processors parallel distributed system mesh partitioning homogeneous systems studied extensively 2 4 14 31 36 37 41 however mesh partitioning distributed systems relatively new area research brought recent availability systems ensure efficient execution distributed system heterogeneities processor network performance must taken consideration partitioning process equal size subdomains small cut set size results conventional mesh partitioning longer desirable part takes advantage following heterogeneous system features 1 processor speed 2 number processors 3 local network performance wide area network performance different finite element applications consideration may different computational complexity different communication patterns different element types also must taken consideration partitioning paper discuss major issues mesh partitioning distributed systems particular identify good metric used compare different partitioning results present measure efficiency distributed system discuss optimal number cut sets remote communication metric used part identify good efficiency estimated execution time also present parallel version part significantly improves performance partitioning process simulated annealing used part perform backtracking search desired partitions however well known simulated annealing computationally intensive parallel part use asynchronous multiple markov chain approach parallel simulated annealing 21 part used partition six irregular meshes 8 16 100 subdomains using 64 client processors ibm sp2 machine results show superlinear speedup cases nearly perfect speedup rest results also indicate parallel version part produces partitions consistent sequential version part using partitions part ran explicit 2d finite element code two geographically distributed ibm sp machines used globus software communication two sps compared partitions part generated using widelyused partitioning tool metis 26 considers processor performance results regular problems indicate increase efficiency processor performance considered compared conventional even partitioning results indicate 5 gamma 15 increase efficiency network performance considered compared considering processor performance significant given optimal 15 application result irregular problem indicate 36 increase efficiency processor network performance considered compared even partitioning remainder paper organized follows section 2 provides background section 3 discusses issues section 4 describes part detail section 5 experimental results section 6 gives previous work finally conclusion background 21 meshbased applications finite element method fundamental numerical analysis technique solve partial differential equations engineering community past three decades 24 3 three basic procedures finite element method problem first formulated variational weighted residual form second step problem domain discretized complex shapes called elements last major step solve resulting system equations procedure discretizing problem domain called meshing applications involve meshing procedure referred meshbased applications meshbased applications naturally suitable parallel distributed systems implementing finite element method parallel involves partitioning global domain elements connected subdomains distributed among p processors processor executes numerical technique assigned subdomain communication among processors dictated types integration method solver method explicit integration finite element problems require use solver since lumped matrix diagonal matrix used therefore communication occurs among neighboring processors common data relatively simple implicit integration finite element problems however communication determined type solver used application application used paper explicit nonlinear finite code called whams2d 6 used analyze elastic plastic materials focus whams2d code concept generalized implicit well meshbased applications 22 distributed system distributed computing consists platform network resources resources may clusters workstations cluster personal computers parallel machines resources maybe located one site distributed among different sites figure 1 shows example distributed system distributed systems provide economical alternative costly massively parallel computers researchers longer limited computing resources individual sites distributed computing environment also provides researchers opportunities collaborate share ideas use collaboration technologies distributed system define group set processors share one interconnection network performance group smp parallel computer cluster workstations personal computers communication occurs within group groups refer communication within group local communication processors different groups remote communication number groups distributed system represented term supercomputer smps supercomputer figure 1 distributed system 23 problem formulation mesh partitioning homogeneous systems viewed graph partitioning problem goal graph partitioning problem find small vertex separator equal sized subsets mesh partitioning distributed system however variation graph partitioning problem goal differs regular graph partitioning problem equal sized subsets may desirable distributed system partitioning problem stated follows given graph e jv maximum cost function f v minimized min paper cost function f estimate execution time given application distributed system function discussed section 4 graph partitioning proven npcomplete mesh partitioning problem distributed system also npcomplete proven appendix 1 therefore focus heuristics solve problem 3 major issues section discuss following major issues related mesh partitioning problem distributed systems comparison metric efficiency number cuts groups 31 comparison metric de facto metric comparing quality different partitions homogeneous parallel systems equal subdomains minimum interface cut set size although objections counter examples 14 metric used extensively comparing quality different partitions obvious equal subdomain size minimum interface valid comparing partitions distributed systems one may consider obvious metric distributed system unequal subdomains pro portional processor performance small cut set size problem metric heterogeneity network performance considered given local wide area networks used distributed system case big difference local remote communication especially terms latency argue use estimate execution time application target heterogeneous system always lead valid comparison different partitions estimate used relative comparison different partition methods hence coarse approximation execution appropriate comparison metric important make estimate representative application system estimate include parameters correspond system heterogeneities processor performance local remote communication also reflect application computational complexity 32 efficiency efficiency distributed system equal ratio relative speedup effective number processors v ratio given 1 e1 sequential execution time one processor e execution time distributed system term v equal summation processors performance relative performance processor used sequential execution term follows 2 k processor used sequential execution example two processors processor performance f 2 efficiency would processor 1 used sequential execution efficiency processor 2 instead used sequential execution 33 network heterogeneity wellknown heterogeneity processor performance must considered distributed systems section identify conditions heterogeneity network performance must considered distributed system recall define group collection processors performance share local interconnection network remote communication corresponds communication two groups given processors require remote local communication others require local communication disparity execution times processors corresponding difference remote local communications assuming equal computational loads 331 ideal reduction execution time retrofit step used part tool reduce computational load processor local remote communication equalize execution time among processors group step described detail section 62 reduction execution time occurs retrofit demonstrated considering simple case stripe partitioning communication occurs two neighboring processors assume exists two groups processor local network performance groups located geographically distributed sites requiring wan interconnection figure 2 illustrates one case g processors local communication local communication remote communication figure 2 communication pattern stripe partitioning processor well processor local remote communication difference two communication times x percentage difference cr cl total execution time e assume e represents execution time taking consideration processor performance since assumed processors performance entails even partition mesh time written consider case partitioning take consideration heterogeneity network performance achieved decreasing load assigned processor increasing loads processors group 1 applies processor j group 2 amount load redistributed cr gamma cl xe amount distributed g processors illustrated figure 6 discussed retrofit step part execution time g difference e e 0 x g g therefore taking network performance consideration partitioning percentage reduction execution time approximately xedenoted delta1 g includes following 1 percentage communication application 2 difference remote local communication factors determined application partitioning maximum number processors among groups remote communication reduction execution time follows g delta example whams2d application experiments calculated ideal reduction 15 regular meshes 8 partitions one processor group local remote communication therefore relatively easy calculate ideal performance improvement 332 number processors group local remote communication major issue addressed reduction partition domain assigned group maximize reduction particular issue entails tradeoff following two scenarios 1 many processors group local remote communication resulting small message sizes execution time without retrofit step smaller case 2 however given many processors group remote local communication fewer processors available redistribution additional load illustrated figure 3 mesh size n theta 2n partitioned 2p blocks block oe oe figure 3 size n theta 2n mesh partitioned 2p blocks n theta n assuming processors equal performance mesh partitioned two groups group p processors processors group boundary incur remote communication well local communication part computational load processors need moved processors local communication compensate longer communication times assume overlap communication messages message aggregation used communication one node diagonal processor communication time processor group boundary approximately localremote processor local communication communication time approximately message aggregation overlapping assumed local therefore communication time difference processor local remote communication processor local communication approximately local total p number processors local remote communication fore using equation 1 ideal reduction execution time group 1 group 2 n figure 4 size n theta 2n mesh partitioned 2p stripes blocks 2 one processor group local remote communication resulting large message sizes result execution time without retrofit step larger case 1 however processors available redistribution additional load illustrated figure 4 mesh partitioned stripes one processor group local remote communication following similar analysis figure 3 communication time difference processor local remote communication processor local communication approximately local one processor remote communication group hence using equation 1 ideal reduction execution time stripes comm stripes 15 therefore total execution time stripe block partitioning stripes reduction blocks reduction difference total execution time block stripe partition deltat blocksgammastripes ff l therefore difference total execution time block stripe partitioning determined term c positive since p 1 term b negative 4 block partition higher execution time ie stripe partitioning advantageous p 4 however block partitioning still higher execution time unless n large absolute value term b larger sum absolute values c note ff l ff r one two orders magnitude larger fi l experiments calculated block partitioning lower execution time n 127kb meshes used however largest n 10kb 4 description part part considers heterogeneities application system particular part takes consideration different mesh based applications may different computational complexities mesh may consist different element types distributed systems part takes consideration heterogeneities processor network performance figure 5 shows flow diagram part part consists interface program simulated annealing program finite element mesh fed interface program produces proposed communication graph fed simulated annealing program final partitioning computed partitioned graph translated required input file format application section describes initial interface program steps required partition graph problem domain groups processors per group computational models communication models partitioned data finite element mesh interface program graph simulated annealing partitioned graph interface program input processors figure 5 part flowchart 41 mesh representation use weighted communication graph represent finite element mesh natural extension communication graph communication graph vertices represent elements original mesh weight added vertex represent number nodes within element communication graph edges represent connectivity elements weighted communication graph weight also added edge represent number nodes information need exchanged two neighboring elements 42 partition method part entails three steps partition mesh distributed systems steps 1 partition mesh subdomains groups taking consideration heterogeneity processor performance element types 2 partition subdomain g parts g processors group taking consideration heterogeneity network performance element types 3 necessary globally retrofit partitions among groups taking consideration heterogeneity local networks among different groups steps described detail following subsections subsection includes description objective function used simulated annealing key good partitioning simulated annealing cost function cost function used part estimate execution time one particular supercomputer let e execution time ith processor 1 p goal minimized variance execution time processors running simulated annealing program found best cost function instead sum 2 20 actual cost function used simulated annealing program cost function ecomm includes communication cost partitions elements need communicate elements remote processor therefore execution time balanced parameter needs tuned according application problem size partition first step generates coarse partitioning distributed systems group gets subdomain proportional number processors performance processors computational complexity application hence computational cost balanced across groups cost function given number groups system 422 step 2 retrofit second step subdomain assigned group step 1 partitioned among processors within group simulated annealing used balance execution time step variance network performance considered processors entails inter group communication reduced computational load compensate longer communication time step illustrated figure 6 two supercomputers sc1 sc2 sc1 four processors used two processors used sc2 computational load reduced p3 since communicates remote processor amount reduced computational load represented ffi amount equally distributed three processors assuming cut size remains unchanged communication time change hence execution time balanced shifting computational load comm comp comm p3 p3 comp retrofit d4 figure illustration retrofit step two supercomputers assuming two nearest neighbor communication step entails generating imbalanced partitions group take consideration processors communicate locally remotely processors communicate locally imbalance represented term delta term added processors require local remote communication adding term results decrease ecomm compared processors requiring local communication cost function given following equation p number processors given group delta difference estimation local remote communication time processors communicate locally 423 step 3 global retrofit third step addresses global optimization taking consideration differences local interconnect performance various groups goal minimize variance execution time across processors step elements boundaries partitions moved according execution time variance neighboring processors step executed large difference performance different local interconnects case significant number elements moved groups step 3 second step executed equalize execution time group given new computational load step 2 processors group balanced execution time however execution time different groups may balanced may occur large difference communication time different groups balance execution among groups take weighted average execution times groups weight group equals computing power group versus total computing power computing power particular group multiplication ratio processor performance respect slowest one among groups number processors used group denote weighted average e assumption communication time change much ie separators step 1 incur large change size e optimal execution time achieved balance execution time group execution time e first compute difference e e gamma added e comp cost function communication cost ecomm remote communication cost group cost function therefore given number groups system groups whose domain increase groups whose domain decrease step 3 necessary step 2 performed partition within group 5 parallel simulated annealing part uses simulated annealing partition mesh figure 7 shows serial version simulated annealing algorithm algorithm uses metropolis criteria line 8 13 figure 7 accept reject moves moves reduce cost function accepted moves increase cost function may accepted probability e gamma deltae avoiding trapped local minima probability decreases temperature lowered simulated annealing computationally intensive therefore parallel version simulated annealing used parallel version part three major classes parallel simulated annealing 19 serial like 32 39 parallel moves 1 multiple markov chains 5 21 34 serial like algorithms essentially break move subtasks parallelize subtasks parallelizing line 6 7 figure 7 parallel moves algorithms processor generates evaluates moves cost function calculation may inaccurate since processors aware moves processors periodic updates normally used address effect cost function error parallel moves algorithms essentially parallelize loop figure 7 line 5 14 multiple markov chains algorithm multiple simulated annealing processes started various processors different random seeds processors periodically exchange solutions best selected given processors continue annealing processes 5 multiple markov chain approach shown effective vlsi cell placement reason parallel version part uses multiple markov chain approach given p processors straightforward implementation multiple markov chain approach would initiating simulated annealing p processors different seed processor performs moves independently finally best solution computed 1 get initial solution 2 get initial temperature 0 3 stopping criteria met f 4 number moves per temperature 5 6 generate random move 7 evaluate changes cost function deltae 8 deltae 9 accept move update solution 10 g else f 11 accept probability 12 update solution accepted 13 g 14 g end loop 15 16g end loop figure 7 simulated annealing processors selected approach however simulated annealing essentially performed p times may result better solution speedup achieve speedup p processors perform independent simulated annealing different seed processor performs mp moves number moves performed simulated annealing temperature processors exchange solutions end temperature exchange data occurs synchronously asynchronously synchronous multiple markov chain approach processors periodically exchange solutions asynchronous approach client processors exchange solutions server processor reported synchronous approach easily trapped local optima asynchronous 21 therefore parallel version part uses asynchronous approach solution exchange client solution better server processor updated better solution server solution better client gets updated better solution continues processor exchanges solution server processor end temperature ensure subdomain connected check disconnected components end part subdomain disconnected components parallel simulated annealing repeated different random seed process continues disconnected subdomains number trials exceed three times warning message given output disconnected subdomains 6 experiments section present results two different experiments first experiment focuses speedup parallel version part second experiment focuses quality partitions generated part 61 speedup results table 1 parallel part execution time seconds 8 partitions proc barth4 barth5 inviscid labarre spiral viscous 4 444 547 466 324 412 537 21 22 24 25 15 29 part used partition six 2d irregular meshes triangular elements barth4 11451 labarre 14971 elem spiral 1992 elem viscous 18369 elem running time partitioning six irregular meshes 8 100 parallel part speedup 8 partitions2060100140 barth4 barth5 inviscid labarre spiral viscous figure 8 parallel part speedup 8 partitions subdomains given tables 1 2 respectively assumed subdomains executed distributed system consisting two ibm sps equal number processors different processor performance machines interconnected via vbns performance network given table 4 discussed section 62 table column 1 number client processors used part columns 2 6 running time part seconds different meshes solution quality using two client processors within 5 using one client processor case solution quality estimate execution time whams2d figures 8 9 graphical representations speedup parallel version part relative one client processor figures show meshes partitioned 8 subdomains superlinear speedup occurs cases meshes partitioned 100 subdomains superlinear speedup occurs cases two smallest meshes spiral inviscid cases show slightly less perfect speedup superlinear speedup attributed use multiple client processors conducting search processors benefit results good solution found one clients information given clients quickly thereby reducing effort continuing search solution superlinear table 2 parallel part execution time seconds 100 partitions proc barth4 barth5 inviscid labarre spiral viscous 4 39822 46664 30823 42735 13044 39740 2883 4263 1928 2912 627 3917 speedup results consistent reported 33 62 quality partition 621 regular meshes part applied explicit nonlinear finite code called whams2d 6 used analyze elastic plastic materials code uses mpi built top nexus interprocessor communication within supercomputer supercomputers nexus runtime system allows multiple protocols within application computational complexity linear size problem code executed ibm sp machines located argonne national laboratory cornell theory center two machines connected internet macro benchmarks used determine network processor performance results network performance analysis given table 3 experiments conducted determine cornell nodes 16 times faster argonne nodes problem mesh consists 3 regular meshes execution time given 100 time steps corresponding 0005 seconds application time generally application may execute 10 000 100 000 time steps recorded execution time represents 100 runs taking data runs standard deviation less 3 regular problems executed parallel part speedup 100 partitions1030507090barth4 barth5 inviscid labarre spiral viscous figure 9 parallel part speedup 100 partitions table 3 values ff fi different networks argonne sp vulcan switch ff machine configuration 8 processors 4 anl ibm sp 4 ctc ibm sp table 4 presents results regular problems column 1 mesh configuration column 2 execution time resulting conventional equal partitioning particular used chacos spectral bisection column 3 result partitioning taken end first step variance processor performance computational complexity considered column 4 execution time resulting partitioning taken end second step variance network performance considered results table 4 shows approximately increase efficiency achieved balancing computational cost another 5 gamma 16 efficiency increase achieved considering variance network performance small increase efficiency considering network performance table 4 execution time using internet 8 processors 4 anl 4 ctc case chaco proc perf local retrofit 9 theta 1152 mesh 10299 7802 6881 efficiency 046 061 071 efficiency 047 061 068 36 theta 288 mesh 10388 7321 7022 efficiency 046 067 070 due communication small component whams2d application however recall optimal increase performance 15 regular problem described earlier global optimization step last step part balances execution time across supercomputers give significant increase efficiency included table 4 expected since two supercomputers used argonne ibm sp cornell ibm sp interconnection networks similar performance indicated table 3 results indicate performance gains achievable step comparison conventional methods evenly partition mesh given obvious considering processor performance results significant gains following section irregular meshes considers performance gains resulting considering network performance 622 irregular meshes experiments irregular meshes performed gusto testbed available experimented regular meshes testbed includes two ibm sp machines one located argonne national laboratory anl located san diego supercomputing center sdsc two machines connected vbns high speed backbone network service used globus 15 16 software allow multimodal communication within application macro benchmarks used determine network processor performance results network performance analysis given table 5 experiments conducted determine sdsc sp processors nodes 16 times fast anl ones table 5 values ff fi different networks anl sp vulcan switch ff sdsc sp vulcan switch ff part used partition five 2d irregular meshes triangular elements barth4 11451 labarre 14971 elem viscous 18369 elem inviscid 6928 called part without restriction sightly modified version part called part restriction used partition meshes one processor remote communication group metis 30 26 used generate partitions take consideration processor performance processors compute power used one inputs three partitioners used identify performance impact considering heterogeneity networks addition processors three partitioners highlight difference forcing remote communication occur one processor group versus multiple processors remote communication group consider 6 configurations two machines 4 anl 4 sdsc 8 anl 8 sdsc 20 anl 20 sdsc two groups correspond two ibm sps anl sdsc used 20 processors sp due limitations coscheduling computing resources execution time given 100 time steps recorded execution time represents average 10 runs standard deviation less 3 tables 6 table 8 show experimental results 3 configurations column one identifies irregular meshes number elements mesh included parenthesis column two execution time resulting partitions part restriction one processor per group entails remote communication columns 2 4 number indicates number processors remote communication group column three similar column two except partition restriction remote communication one processor column four execution time resulting metis takes computing power consideration processors compute power used one table execution time using vbns 8 processors 4 anl 4 sdsc mesh part w restriction part wo restriction proc perf metis efficiency viscous 18369 elem 1500s 1 1690s 3 1700s 3 efficiency 086 075 075 labarre 14971 elem 1330s 1 1420s 2 1460s 3 efficiency 079 073 071 efficiency 079 068 068 inviscid 6928 elem 732s 1 855s 3 885s3 efficiency 066 056 055 inputs metis program results show using part without restrictions slight decrease 13 execution time achieved compared metis forcing remote communication one processor retrofit step achieve significant reduction execution time results tables 6 table 8 show efficiency increased 36 compared metis execution time reduced 30 compared metis reduction comes fact even high speed network vbns difference message start cost remote local communication large table 5 see difference two orders magnitude message start compared approximately one order magnitude bandwidth restricting remote communication one processor allows part redistribute load among processors thereby achieving close ideal reduction execution time 7 previous work problem domain partitioning finite element meshes equivalent partitioning graph associated finite element mesh graph partitioning proven table 7 execution time using vbns processors 8 anl 8 sdsc mesh part w restriction part wo restriction proc perf metis efficiency 072 062 059 viscous 18369 elem 829s1 1008s4 1060s5 efficiency 077 064 061 labarre 14971 elem 758s1 837s3 886s3 efficiency 069 062 059 efficiency 074 050 048 inviscid 6928 elem 422s1 628s3 672s4 efficiency 057 039 036 npcomplete problem 17 many good heuristic static partitioning methods proposed kernighanlin 31 proposed locally optimized partitioning method farhat 13 14 proposed automatic domain decomposer based greedy algorithm berger bokhari 4 proposed recursive coordinate bisection rcb utilizes spatial nodal coordinate information nouromid et al 35 40 proposed recursive inertial bisection rib simon 37 proposed recursive spectral bisection rsb computes fiedler vector graph using lanczos algorithm sorts vertices according size entries fiedler vector recursive graph bisection rgb proposed george liu 18 uses sparspak rcm algorithm compute level structure sort vertices according rcm level structure barnard et al 2 proposed multilevel version rsb faster hendrickson leland 23 22 also reported similar multilevel partitioning method karypis kumar 27 28 30 proposed new coarsening heuristic improve multilevel method aforementioned decomposition methods available one three automated tools chaco 22 metis 26 29 topdomdec 38 chaco versatile implements inertial spectral kernighanlin multilevel algorithms algorithms used table 8 execution time using vbns 40 processors 20 anl 20 sdsc mesh part w restriction part wo restriction proc perf metis efficiency 069 054 045 viscous 18369 elem 387s1 586s5 649s7 efficiency 067 044 040 labarre 14971 elem 338s1 512s3 535s6 efficiency 062 041 040 efficiency 039 034 032 inviscid 6928 elem 3351 347s4 468s5 efficiency recursively bisect problem equal sized subproblems metis uses method fast partitioning sparse matrices using coarsening heuristic provide speed topdomdec interactive mesh partitioning tool tools produce equal size partitions tools applicable systems processors one interconnection network tools metis produce partitions unequal weights however none tools take network performance consideration partitioning process reason tools applicable distributed systems crandall quinn 7 8 9 10 11 12 developed partitioning advisory system network workstations advisory system three builtin partitioning methods contiguous row contiguous point block given information problem space machine speed network advisory system provides ranking three partitioning methods advisory system takes consideration variance processor performance among workstations problem however linear computational complexity assumed application case implicit finite element problems widely used variance network performance considered 8 conclusion paper addressed issues mesh partitioning problem distributed systems issues include comparison metric efficiency cut sets present tool part automatic mesh partitioning distributed systems novel feature part considers heterogeneities application distributed system heterogeneities distributed system include processor network performance heterogeneities application include computational complexity also demonstrate use parallel version part distributed systems novel part parallel part uses asynchronous multiple markov chain approach parallel simulated annealing mesh partitioning parallel part used partition 6 irregular meshes 8 16 100 subdomains using 64 client processors ibm sp2 machine results show superlinear speedup cases nearly perfect speedup rest used globus software run explicit 2d finite element code using mesh partitions parallel part testbed includes two geographically distributed ibm sp machines experimental results presented 3 regular meshes 4 irregular finite element meshes whams2d application executing distributed system consisting two ibm sps results regular problems indicate increase efficiency processor performance considered compared even partitioning results also indicate additional 5 gamma 16 increase efficiency network performance considered result irregular problem indicate 38 increase efficiency processor network performance considered compared even partitioning experimental results irregular problems also indicate 36 increase efficiency compared using partitions take processor performance consideration improvement comes fact even high speed network vbns message start cost remote local communication still large difference appendix 1 proof npcomplete mesh partitioning problem distributed systems partitioning problem distributed systems npcomplete proof 1 transform proven npcomplete problem minimum sum squares 17 partition problem distributed systems let set 2 arbitrary instance minimum sum squares shall construct graph desired partition exists g sum squares basic units minimum sum squares instance n local replacement substitute 2 collection e 3 edges shown figure 10 therefore e defined following 2 1 figure 10 local replacement 2 transforming minimum sum squares partition problem distributed systems easy see instance partition problem distributed systems constructed polynomial time minimum sum squares instance disjoint k partitions sum squares minimized corresponding k disjoint partitions v given taking fa 1 2 g every subset also restrict cost function f minimum sum squares 2 ensures partition sum squares cost function conversely disjoint k partition g minimum sum squares cost function corresponding disjoint k partition set given choosing vertices fa 1 2 hence minimum sum squares cost function k disjoint partitions ensures sum squares sa k disjoint set also minimized conclude partition problem distributed systems npcomplete appendix 2 nomenclature estimated execution time processor estimated computational time processor estimated communication time processor performance processor measured computation kernel ff l per message cost local communication message cost remote communication cost local communication cost remote communication size message cl local communication time processor cr remote communication time processor difference cr cl one processor difference cr cl processor maximum number processors group local remote communication coefficient computational complexity parameter used equalize contribution computation communication execution time number elements partition number processors system number processors group g number processors particular group p number groups system ith group system ratio speed processors relative slowest processor system r parallel simulated annealing algorithms cell placement hypercube multiprocessors fast multilevel implementation recursive spectral bisection partitioning unstructured problems finite element procedures engineering analysis partitioning strategy nonuniform problems multiproces sors evaluation parallel simulated annealing strategies application standard cell placement whams3d project progress report pr2 data partitioning networked parallel processing problem decomposition parallel networks block data partitioning partialhomogeneous parallel networks evaluating decomposition techniques highspeed cluster computing partitioning advisory system networked dataparallel processing simple efficient automatic fem domain decomposer automatic partitioning unstructured meshes parallel solution problems computational mechanics managing multiple communication methods highperformance networked computing systems software infrastructure iway metacomputing experiment computers intractability guide theory np completeness computer solution large sparse positive definite systems parallel simulated annealing techniques legion team simulated annealing based parallel state assignment finite state machines chaco users guide multilevel algorithm partitioning graphs finite element method internet fast lane research education fast high quality multilevel scheme partitioning irregular graphs fast high quality multilevel scheme partitioning irregular graphs multilevel kway partitioning scheme irregular graphs multilevel kway partitioning scheme irregular graphs parallel multilevel kway partitioning scheme irregular graphs efficient heuristic procedure partitioning graphs placement simulated annealing multiprocessor introduction parallel computing design analysis algorithms asynchronous communication multiple markov chain parallel simulated annealing solving finite element equations concurrent computers partitioning sparse matrices eigenvectors graphs partitioning unstructured problems parallel processing topdomdec software tool mesh partitioning parallel processing parallel nary speculative computation simulated annealing study factorization fillin parallel implementation finite element method retrofit based methodology fast generation optimization largescale mesh partitions beyond minimum interface size criterion tr partitioning strategy nonuniform problems multiprocessors partitioning sparse matrices eigenvectors graphs parallel simulated annealing techniques introduction parallel computing threedimensional grid partitioning network parallel processing legion vision worldwide virtual computer managing multiple communication methods highperformance networked computing systems simulated annealing based parallel state assignment finite state machines computer solution large sparse positive definite computers intractability parallel simulated annealing algorithms cell placement hypercube multiprocessors parallel nary speculative computation simulated annealing mesh partitioning distributed systems problem decomposition parallel networks ctr kyungmin lee dongman lee scalable dynamic load distribution scheme multiserver distributed virtual environment systems highlyskewed user distribution proceedings acm symposium virtual reality software technology october 0103 2003 osaka japan zhiling lan valerie e taylor greg bryan dynamic load balancing samr applications distributed systems scientific programming v10 n4 p319328 december 2002 zhiling lan valerie e taylor greg bryan dynamic load balancing samr applications distributed systems proceedings 2001 acmieee conference supercomputing cdrom p3636 november 1016 2001 denver colorado