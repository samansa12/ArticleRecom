checkpointing distributed shared memory distributed shared memory dsm promising programming model exploiting parallelism distributed memory systems provides higher level abstraction simple message passing although nodes standard distributed systems exhibit high crash rates dsm environments kind support faulttolerancein article present checkpointing mechanism dsm system efficient portable offers portability built top mpi uses services offered mpi posix compliant local file systemas far know first real implementation scheme dsm along description algorithm present experimental results obtained cluster workstations hope research shows efficient transparent portable checkpointing viable dsm systems b introduction distributed shared memory dsm systems provide shared memory programming model top distributed memory systems ie distributed memory multiprocessors networks workstations dsm appealing combines performance scalability distributed memory systems ease programming sharedmemory machines distributed shared memory received much attention past decade several dsm systems presented literature eskicioglu95raina92nitzberg91 however existing prominent implementations dsm systems provide support faulttolerance carter91 keleher94 li89 johnson95 limitation wanted overcome dsm system using parallel machines andor workstation clusters user aware likelihood processor failure increases number processors failure one processor lead crash hangup whole application distributed systems represent costeffective solution running scientific computations time vulnerable occurrence failures study presented long95 shows expect average failure every 8 hours typical distributed system composed 40 workstations machine exhibits mtbf 13 days program runs system takes 8 hours execute would difficult finish execution unless fault tolerance support assure continuity application parallel machines considerably stable even present relatively low mtbf instance mtbf large parallel machine like intel paragon xps 150 1024 processors oak ridge national laboratory order 20 hours ornl95 case longrunning scientific applications essential checkpointing mechanism would assure continuity application despite occurrence failures without mechanism application would restarted scratch costly applications paper present checkpointing scheme dsm systems despite transparent application quite general portable efficient scheme quite general depend specific feature dsm system fact system implements several protocols consistency models consistency checkpointing scheme made independent protocols scheme also quite portable since implemented top mpi nothing changed inside mpi layer call dsmpi silva97 scheme requires posix compliant file system makes use likckpt tool plank95 taking local checkpoints processes tool works standard unix machines finally dsmpi efficient implementation checkpointing usually efficiency depends write latency stable storage also characteristics checkpointing protocol first feature mainly depends underlying system second one control implemented nonblocking coordinated checkpointing algorithm freeze whole application checkpoint operation done blocking algorithms problem nonblocking algorithms blocking ones need record stable storage intransit messages cross checkpoint line however exploited semantics dsm messages achieved important optimization intransit message logged stable storage results taken using distributed stable storage observed maximum overhead 6 extremely short interval checkpoints 2 minutes realistic interval order tens minutes even hours overhead would fall insignificant values rest paper organized follows section 2 describes general organization dsmpi protocols section 3 presents transparent scheme based nonblocking coordinated checkpointing algorithm section 4 compares algorithm schemes section 5 presents performance results finally section 6 concludes paper 2 overview dsmpi section gives brief description dsmpi silva97 21 main features dsmpi parallel library implemented top mpi mpi94 provides abstraction globally accessed shared memory user specify datastructures variables shared shared data read andor written process mpi application important guidelines took account design dsmpi 1 assure full portability dsmpi programs 2 provide easytouse flexible programming interface 3 support heterogeneous computing platforms 4 optimize dsm implementation allow execution efficiency 5 provide support checkpointing sake portability dsmpi use memorymanagement facility operating system neither requires use special compiler linker shared data readwrite operations declared explicitly application programmer sharing unit program variable data structure dsmpi classified structurebased dsm opposed pagebased dsm systems like ivy li89 incur problem false sharing unit shared data completely related existing objects data structures application allows use heterogeneous computing platforms since library knows exact format shared data object dsm systems limited homogeneous platforms dsmpi allows coexistence programming models message passing shared data within mpi application considered promising solution parallel programming kranz93 concerning absolute performance expect applications use dsm perform worse message passing counterparts however always true really depends memoryaccess pattern application way dsm system manages consistency replicated data tried optimize accesses shared data introducing three different protocols data replication three different models consistency adapted particular application order exploit semantics facilities expect dsm programs competitive mpi programs terms performance performance results collected far corroborate expectation silva97 22 internal structure dsmpi two kinds processes application processes daemon processes latter ones responsible management replicated data protocols consistency since current implementations mpi threadsafe implement dsmpi daemons separate processes limitation current version dsmpi relaxed soon threadsafe implementation mpi communication daemons application processes done message passing application process access local cache located address space keeps copies replicated data objects daemon processes maintain master copies shared objects dsmpi maintains twolevel memory hierarchy local cache remote shared memory located managed daemons ownership data objects implemented static distributed scheme 23 dsm protocols provided flexibility accesses shared data introducing three different protocols data replication three different models consistency adapted particular application order exploit semantics shared objects classified two main classes singlecopy multicopy multicopy class replicates object among processes perform read request order assure consistency replicated data system use writeinvalidate protocol writeupdate protocol stumm90a parameter tuned application programmer order exploit execution efficiency also implemented three different models consistency 1 sequential consistency sc proposed ivy system li89 2 release consistency rc implements protocol dash multiprocessor 3 lazy release consistency lrc implements similar protocol proposed treadmarks system keleher94 shown lrc protocol able introduce significant improvements two models flexibility provided dsmpi terms different models protocols important contribution overall performance dsmpi 24 programming interface library provides c interface programmer calls dsmpi functions way calls mpi routine complete interface composed routines includes routines initialization clean termination object creation declaration read write operations routines synchronization like semaphores locks barriers full interface described silva97 3 transparent checkpointing algorithm devising transparent checkpointing algorithm dsm system tried achieve objectives like transparency portability low performance overhead low memory overhead ability tolerate partial total failures system satisfying previous guidelines easy task possible proposals existing checkpointing schemes dsm mainly concerned transparency transparent recovery implemented operating system level dsm layer attractive idea however schemes involve significant modifications system make difficult port systems sake portability checkpointing made independent underlying system much possible 31 motivations checkpointing scheme meant quite general depend specific feature dsm system system implements several protocols consistency models consistency checkpointing scheme made independent protocols scheme offers degree portability implemented top mpi per se already provides highlevel portability since accepted standard messagepassing nothing changed inside mpi layer scheme requires posix compliant file system makes use libckpt tool plank95 taking local checkpoints processes tool works standard unix machines taking transparent checkpoints means possible assure checkpoint migration machines different architectures however checkpoint mechanism ported dsm environments 32 coordinated checkpointing guideline adapt checkpointing techniques used message passing systems since checkpointing mechanisms widely studied messagepassing environments two methods taking checkpoints commonly used coordinated checkpointing independent checkpointing first method processes coordinate ensure local checkpoints form consistent system state independent checkpointing requires coordination processes result rollback propagation avoid domino effect reduce rollback propagation message logging used together independent checkpointing independent checkpointing message logging encouraging option dsm systems generate messages messagepassing programs thus chosen coordinated checkpointing strategy dsmpi reasons manifold minimizes overhead failurefree operation since need log messages limits rollback previous checkpoint avoids dominoeffect uses less space stable storage require complex garbagecollection algorithm discard obsolete checkpoints finally suitable solution support jobswapping shown elnozahi92plank94 coordinated checkpointing effective solution messagepassing systems experimental results shown overhead synchronizing local checkpoints negligible compared overhead writing checkpoints disk implementing checkpointing algorithm underneath dsm layer transparent way layer possible alternative provide faulttolerance suggested carter93 however would simplistic approach since dsm system exchanges several messages related application approach would result extra overhead would exploit characteristics dsm system 33 system model assume dsm system uses message passing implement protocols notion global time guarantee processor clocks synchronized way processors assumed failstop provision detect tolerate kind malicious failures processor fails stops sending messages respond parts system processor failures detected underlying communication system mpi use timeouts one processes fails mpi layer sends sigkill processes application daemons terminate complete application mpi makes extensive use static groups fact whole set processes belong beginning mpiworldcomm group process fails collective operations involve participation processes certainly hangup application thus makes sense failure one process result rollback entire application communication failures also dealt communication system mpi underlying messagepassing system provides reliable fifo pointtopoint communication service stable storage implemented shared disk assumed reliable disk attached central file server checkpoints become available working processors system stable storage implemented local disk every processor assuming host disk application recover failed host also able recover 34 checkpoint contents checkpoint dsm system include dsm data eg pagesobjects dsm directories private data application schemes save private data processes thus able recover whole state computation stumm90b leave application programmer responsibility saving computation state assure continuity application schemes assume sake simplicity private data shared data allocated dsm kermarrec95 depart assumption consider private data allocated dsm system besides private data like processor registers program counter process stack certainly part dsm data scheme checkpoint application processes well dsm daemons since maintain dsm relevant data dsm daemons save shared objects associated dsm directories saved directories reflect location default current owners shared objects optimizations made system readonly objects checkpointed replicated shared objects checkpointed one daemon one maintains ownership process application daemon saves checkpoint separate file global checkpoint composed n different checkpoint files statusfile keeps status checkpoint protocol execution file maintained checkpointing coordinator used phase recovery determine last committed checkpoint well ensure atomicity operation writing global checkpoint checkpointing algorithm since checkpointing schemes message passing systems well stabilized decided adopt one used techniques implement nonblocking global checkpointing elnozahy92silva92 main difficulty implementing nonblocking coordinated checkpoint guarantee global saved state consistent messages intransit time global checkpoint main concern saving consistent snapshot distributed application instance messages sent checkpoint process sender received checkpoint receiver called orphan messages silva92 sort messages violates consistency global checkpoint thus avoided algorithm messages sent checkpoint sender received checkpoint destination process called missing messages usually algorithm keep track occurrence replay recovery operation however considered features dsm system important implementation algorithm namely interaction processes done explicit messages object invocations rpclike interactions shared replicated data dsm system exchanges additional messages related dsm protocols affect application directly finally dsm directory maintained throughout system features taken account exploited introduce optimizations resulting scheme presents novel important feature nonblocking algorithms oriented messagepassing need record cross checkpoint message stable storage operation checkpointing triggered periodically timer mechanism one daemons acts like coordinator master daemon responsible initiating global checkpoint coordinating steps protocol one process given right initiate checkpointing session order avoid multiple sessions uncontrolled highfrequency checkpoint operations since always one dsmpi daemon elected master startup phase guarantee daemon checkpoint coordinator checkpointing adequately spaced time global checkpoint identified monotonically increasing number checkpoint number cn first phase protocol coordinator daemon increments cn broadcasts takechkp message daemons application processes upon receiving message processes takes tentative checkpoint increments local cn sends message tookchkp coordinator taking tentative checkpoint every process allowed continue computation application need frozen execution checkpointing protocol important feature avoid interference application reduce checkpointing overhead second phase protocol daemon broadcasts commit message receiving responses ie tookchkp participants upon receiving commit message tentative checkpoints transformed permanent checkpoints previous checkpoint files deleted phases recorded statusfile master daemon usually broadcast message takechkp received processes order preserves causality however due asynchrony system possible situations may violate causal consistency important aspect every shared object tagged local cn value every message sent dsm system tagged cn sender cn value piggybacked dsm messages prevents occurrence orphan messages daemon receives message higher cn local one take tentative checkpoint consuming message changing internal state later receives takechkp message tagged equal cn discards message since corresponding tentative checkpoint already taken cn value also helpful identifying missing messages incoming message carries cn value lower current local one means sent previous checkpoint interval potential missing message checkpointing algorithm distinguish messages used readwrite operations messages used dsm protocols using semantics dsm protocols avoid unnecessary logging potential missing messages daemon processes run cycle receive takechkp message take local snapshot internal state including dsm directories application processes get takechkp orders execute dsmpi routines whenever read local cache perform remote object invocation access synchronization variable invocations shared objects synchronization variables involve twoway interaction invocation response period process waiting response remains blocked thus change internal state interactions fit structure messages related dsm protocols usually messages originated daemon processes require rpclike interaction sake clarity let us distinguish three different cases case 1 messages processtodaemon interactions started application process wants perform readwrite operation shared object gain access synchronization variable process maintains directory location owners objects locks semaphores barriers wants access sends invocation message respective daemon blocks waiting response let us consider two different scenarios handled algorithm 11 process p running checkpoint interval n daemon k still running previous checkpoint interval ie 12 process p cn equal n1 daemon already taken n th tentative checkpoint cnn example first scenario illustrated figure 1 process p already took n th checkpoint performs read access remote object owned daemon k yet received corresponding takechkp message read message carries cn equal n daemon checks takes local checkpoint consuming message instead read operation write lock unlock wait signal barrier invocation operations associated acknowledge reply message figure 1 forcing checkpoint dsm daemon figure 2 represents second scenario daemon already taken n th checkpoint process started read transaction previous checkpoint interval receives readreply process realizes take local checkpoint increment local cn consuming message continue computation figure 2 forcing checkpoint application process however operation enough since checkpoint cpin record read invocation message recovery process repeat read transaction checkpoint routine record read invocation message contents checkpoint sense say logical checkpoint immediately sending read message represented figure 3 time checkpoint invocation message belongs address space process therefore already included checkpoint concern redirect starting point recovery point code immediately sending message purpose label included every dsmpi routine returning force checkpoint restart operation control flow jumps label invocation message resent thus assure read transaction repeated beginning figure 3 notion logical checkpoint case 2 messages daemontoprocess messages started daemon processes related dsm protocols usually invalidate update messages depending replication protocol messages follow rpclike structure thus block sending daemon application processes consume sort messages execute cache refresh procedure also identify two different scenarios 21 protocol message potential orphan message 22 protocol message potential missing message first situation represented figure 4 process p receives message carries higher cn local one forced take checkpoint proceeding figure 4 potential orphan message second scenario illustrated figure 5 invalidate update message sent previous checkpoint interval received application process taking local checkpoint theoretically example missing message normal case would recorded stable storage order replayed case recovery however need log missing invalidateupdate messages absolutely problem message replayed case rollback reason simple recovery procedure every application process clean local cache logical checkpoint process accesses shared object perform remote operation owner daemon gets uptodate version object figure 5 example missing message case 3 messages daemontodaemon using static distributed ownership scheme daemons communicate startup phase creation new objects also happens startup phase dynamic ownership scheme messages daemons sent time since ownership object move daemon daemon current version dsmpi follows static distributed scheme next version provide dynamic distributed scheme well consider dynamic distributed ownership scheme similar one presented li89 shared data object associated owner changing data migrates throughout system process wants write migratory object system changes location daemon associated process object location change frequently program execution every process daemon maintain guess probable owner shared object process needs copy data sends request probable owner daemon data object returns data otherwise forwards request new owner forwarding go current owner found chain probable owners current owner send reply asking process receives data updates value probable owner reply received expected daemon sometimes scheme inefficient since request may forwarded many times reaching current owner inefficiency reduced daemons involved forwarding request given identity current owner consider optimization involves sending ownership update messages current owner daemons belonging forward chain let us see implications forwardbased scheme checkpointing algorithm messages exchanged daemons divided three different classes 31 forward messages sent behalf readwrite transactions 32 ownerupdate messages announce current owner object 33 transfer object current owner next owner first kind messages follows rule stated case 1 ie transactions started process rule explained help figures 1 2 3 difference daemon processes involved forwarding chain apply rule case ownerupdate messages treated similar way case 2 ownerupdate message carries cn higher cn destination daemon message potential orphan message force checkpoint destination proceeding rule case 21 explained figure 4 applies case ownerupdate message carries lower cn destination daemon corresponds potential missing message like figure 5 normal case logged replayed case recovery however realized problem log messages system still able ensure consistent recovery application recovery procedure distributed directory reconstructed use broadcast update current ownership objects means ownerupdate messages lost recovery object directory updated case transfer data one daemon new owner included forward protocol reply sent current owner original process copy data also sent first sent daemon associated process daemon new owner rules stated previously also used case since object ownership change frequently execution checkpoint protocol necessary take care avoid shared object checkpointed solve problem simple way current owner responsible checkpointing shared object transfers object another daemon taking checkpoint object checkpointed cn value tagged object used prevent migratory object checkpointed summarize checkpointing algorithm follows nonblocking coordinated strategy avoids occurrence orphan messages detects potential missing messages log missing message stable storage system still ensure consistent recovery application achieve optimization exploited characteristics dsm protocols scheme works sequential consistency relaxed consistency models also independent replication protocol writeupdate writeinvalidate fact allows wide applicability algorithm different dsm systems 36 recovery procedure case application recovery involves roll back processes previous checkpoint see drawback rather imposition underlying communication system mpi nevertheless suits well goals use checkpointing jobswapping well tolerate number failures thus recovery procedure quite simple processes roll back previously committed checkpoint determination last committed checkpointed obtained statusfile restoring local checkpoints process still perform actions restarting execution object location directory constructed updated processes case static distribution operation bypassed ii every shared object defined multicopy owner daemon resets associated copyset list iii application process cleans local private cache updates object location directory necessary steps processes allowed resume computation cleaning private cache processes recovery introduce visible overhead allows simpler operation checkpoint operation since potential missing messages exchanged behalf dsm protocols need logged 4 comparison schemes coordinated checkpointing algorithms proposed literature algorithm presented janakiraman94 extends checkpointrollback operations processes communicate directly indirectly process initiator algorithm uses 2phase commit protocol processes participating checkpoint session suspend computations messages intransit flushed destinations algorithm waits completion ongoing readwrite operations proceeding checkpointing protocol pending readwrite operations terminated processors begin sending checkpoints stable storage may result higher checkpoint latency performance overhead since use blocking strategy cabillic95 presented implementation consistent checkpointing dsm system approach relies integration global checkpoints synchronization barriers application scheme implemented top intel paragon several optimizations included like incremental nonblocking preflushing checkpointing techniques shown copyonwrite checkpointing important optimization reduce checkpointing overhead recovery operation scheme processes forced roll back last checkpoint case limitation scheme work applications barrier within application system never able checkpoint costa96 also presents similar checkpointing scheme relies garbage collector mechanism achieve global consistent state system based fullblocking checkpointing approach kaashoek92 presented global consistent checkpointing mechanism orca parallel language easy implement dsm implementation based totalorder broadcast communication processes receive broadcast messages order assure consistency updates replicated objects checkpointing messages also broadcasted inserted total order messages ensures consistency global checkpoint unfortunately mpi characteristic choy95 presented definition consistent global states sequentially consistent shared memory systems also presented lazy checkpoint protocol assures global consistency however lazy checkpointing schemes may result high checkpoint latency desirable job swapping purposes different recovery schemes based coordinated checkpointing also presented literature wu89janssens93 based communication induced checkpointing every process allowed take checkpoints independently communicating another one forced checkpoint order avoid rollback propagation inconsistencies communicationinduced checkpointing sensitive frequency interprocess communication synchronization application may introduce high performance overhead uncontrolled checkpoint frequency another solution recovery based independent checkpointing message logging richard93 however find option encouraging dsm systems generate messages message passing programs even considering possible optimizations suri95 message logging would incur significant additional performance memory overhead considerable set proposals wilkinson93neves94stumm90bbrown94 kermarrec95 able tolerate single processor failures system goal meaningful distributed systems expect machine failures uncorrelated true parallel machines total multiple failures likely partial failures require checkpointing mechanism able tolerate number failures although different approaches could interesting systems find suitable system decided adopt coordinated checkpointing strategy 5 performance results section present results performance memory overhead transparent checkpointing scheme results collected distributed system composed 4 sun sparc4 workstations connected 10 mbs ethernet 51 parallel applications conduct evaluation algorithm used following six typical parallel applications tsp solves traveling salesman problem using branchandbound algorithm nqueens solves placement problem nqueens nsize chessboard sor solves laplaces equation regular grid using iterative method gauss solves system linear equations using method gausselimination asp solves allpairs shortest paths problem using floyds algorithm nbody program simulates evolution system many bodies influence gravitational forces 1 lack space refer interested reader silva97 details applications 52 performance overhead made experiments transparent checkpointing algorithm dedicated network sun sparc workstations every processor local disk access central file server ethernet network take local checkpoint process used libckpt tool fully transparent mode plank95 none optimizations tool used two levels stable storage used first level used local disks processors second level used central server accessible processors nfs protocol writing checkpoints local disks expected much faster writing remote central disk however first scheme stable storage able recover transient processor failures processor fails permanent way able restart checkpoint accessed processor network recovery becomes impossible central disk problem assuming disk reliable considering stable storage implemented central file server table 1 shows time commit corresponding overhead per checkpoint applications usually time takes commit global checkpoint higher overhead produced algorithm follows nonblocking approach application processes need wait completion protocol algorithm based blocking approach overhead per checkpoint would roughly equal whole time takes commit table observe overall nonblocking nature algorithm allows reduction checkpoint overhead application size chkp kbytes time commit gauss 1024 8500 130763 128832 table 1 time commit overhead per checkpoint using central disk time take checkpoint depends basically four factors size checkpoint ii access time stable storage iii synchronization structure application iv granularity tasks checkpoint operations performed inside dsmpi routines means application asynchronous coarsegrain takes time perform global checkpoint compared synchronous application factors important practice dominant factor actually operation writing checkpoint files stable storage reducing size checkpoints promising solution attenuate performance overhead another way use stable storage faster access table 2 shows overhead per checkpoint considering two different levels stable storage seen difference figures considerable cases one order magnitude using ethernet nfs central file server really bottleneck checkpointing operation nevertheless ensures global accessible stable storage device checkpoints made available even occurrence permanent failure processor application size chkp kbytes sec local sec central gauss 1024 8500 1186 128832 gauss 2048 35495 4284 1127654 table 2 overhead per checkpoint local vs central disk table 3 shows difference overall performance overhead considering two levels stable storage different intervals checkpoints present results sor application executed average time 4 hours application interval chkp table 3 total performance overhead local vs central disk average overhead checkpointing tuned changing checkpoint interval table 3 see maximum overhead observed using local disk 64 corresponding overhead central file server 332 shows consider distributed stable storage scheme performance become interesting nevertheless two minutes conservative interval checkpoints longrunning applications need checkpointed often 20 minutes acceptable interval case performance overhead using local disks 06 small value interval central disk stable storage presented overhead 307 interesting strategy would integration stable storage levels application checkpointed periodically central server meantime also checkpointed local disks processors application fails due transient perturbation processors able restart recover checkpoints saved local disk one correspond last committed checkpoint processors affected permanent outage application restarted last checkpoint located central disk possible solution make distributed stable storage scheme resilient permanent failure one processor implement sort logical ring processor copy local checkpoint file next processors disk done global checkpoint committed concurrent way lazy update scheme would introduce delay commit operation additional traffic network regulated use tokenbased policy perform remote checkpoint file copy sequential way obviously want tolerate n permanent processor failures replicate checkpoint file n1 locals disks network measured performance overhead using levels stable storage results presented figure 6 checkpoint central disk performed k checkpoints local disks factor k changed 0 10 figure 6 shows overhead reduction sor application 512x512 grid points515253545 factor k figure twolevel stable storage sor 512 instance user wants overhead lower 5 factor k 9 3 1 0 using checkpoint interval 2 5 10 20 minutes respectively permanent failure occurs one processors system worst case application loose approximately 20 minutes computation four previous cases advantage still goes interval 2 minutes k equal 9 since occurrence transient failure lose less computation figure 7 shows corresponding values sor application 1024x1024 factor k figure 7 twolevel stable storage sor 1024 analysis done considering watermark 10 performance overhead checkpointing application interval 2 5 10 20 minutes factor k 11 4 1 0 respectively user requires overhead lower 5 k 8 4 1 interval 510 20 minutes respectively 6 conclusions far know first implementation nonblocking coordinated algorithm real dsm system dsmpi provides different protocols models consistency algorithm works checkpointing scheme generalpurpose adapted dsm systems use protocol replication model consistency results taken considering distributed stable storage scheme observed maximum overhead 6 interval checkpoints 2 minutes checkpoint interval 20 minutes performance overhead 06 interval stable storage implemented central nfsfile server presented overhead 307 algorithm herein presented offers interesting level portability efficiency though plan enhance features dsmpi next release implemented mpi2 look forward threadsafe version mpi order redesign dsmpi daemons implement optimization techniques proposed cabillic95 hope line research would give contribution standard flexible checkpointing tool used real production codes acknowledgments work herein presented conducted first author visitor epcc edinburgh parallel computing centre visit made possible due tracs programme first author supported jnict behalf programa cincia bd208392ia 7 r dynamic snooping faulttolerant distributed shared memory performance consistent checkpointing distributed shared memory systems implementation performance munin network multicomputer using recoverable distributed shared memory distributed object checkpointing recovery lightweight logging lazy release consistency consistent distributed shared memory performance consistent checkpointing comprehensive bibliography distributed shared memory coordinated checkpointingrollback error recovery distributed shared memory multicomputers relaxing consistency recoverable distributed shared memory crl highperformance allsoftware distributed shared memory transparent faulttolerance parallel orca programs treadmarks distributed shared memory standard workstations operating systems recoverable distributed shared memory integrating coherence recoverability integrating messagepassing shared memory early experience directorybased cache coherence protocol dash multiprocessor memory coherence shared virtual memory systems longitudinal survey internet host reliability message passing interface standard checkpoint protocol entry consistent shared memory system distributed shared memory survey issues algorithms data available httpwww performance results ickp consistent checkpointer ipsc860 libckpt transparent checkpointing unix virtual shared memory survey techniques systems using logging asynchronous checkpointing implement recoverable distributed shared memory global checkpoints distributed programs implementation performance dsmpi algorithms implementing distributed shared memory faulttolerant distributed shared memory algorithms reduced overhead logging rollback recovery distributed shared memory implementing faulttolerance 64bit distributed operating system recoverable distributed shared virtual memory memory coherence storage structures tr memory coherence shared virtual memory systems algorithms implementing distributed shared memory distributed shared memory implementation performance munin transparent faulttolerance parallel orca programs integrating messagepassing sharedmemory checkpoint protocol entry consistent shared memory system crl distributed object checkpointing recovery lightweight logging lazy release consistent distributed shared memory directorybased cache coherence protocol dash multiprocessor performance consistent checkpointing distributed shared memory systems longitudinal survey internet host reliability recoverable distributed shared memory integrating coherence recoverability reduced overhead logging rollback recovery distributed shared memory virtual shared memory survey techniques systems