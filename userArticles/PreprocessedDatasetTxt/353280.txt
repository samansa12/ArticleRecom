merging splitting eigenspace models abstractwe present new deterministic methods given two eigenspace modelseach representing set ndimensional observationswill 1 merge models yield representation union sets 2 split one model another represent difference sets done accurately keep track mean give theoretical derivation methods empirical results relating efficiency accuracy techniques three general applications including construction gaussian mixture models dynamically updateable b introduction contributions paper 1 method merging eigenspace models 2 method splitting eigenspace models represent advance previous methods incremental computation eigenspace models considered addition subtraction single observation eigenspace model 1 2 3 4 5 6 method also allows origin updated unlike methods thus methods allow large eigenspace authors department computer science university wales cardiff po box 916 cardiff cf2 3xf wales uk petercscfacuk models updated quickly accurately using previous methods second advantage large eigenspace models may reliably constructed using divideand conquer approach limitations existing techniques mentioned 7 eigenspace models wide variety applica tions example classification recognition systems 8 characterising normal modes vibration dynamic models heart 9 motion sequence analysis 10 temporal tracking signals 4 clearly least appli cations merging splitting eigenspace models useful data likely presented processed batches example common paradigm machine learning systems identify clusters 11 methods allow clusters split merged dynamically updated new data arrive another example image database university students may require much one quarter records replaced year methods permit without need recompute eigenspace model ab initio paper solely concerned deriving theoretical framework merging splitting eigenspaces empirical evaluation new techniques rather particular application area eigenspace model statistical description set n observations ndimensional space model may regarded multidimensional gaussian distribution geometric point view eigenspace model thought hyperellipsoid characterises set observations centre mean observations axes point directions along spread observations maximised subject orthogonal surface hyperellipsoid contour lies one standard deviation mean often hyperellipsoid almost flat along certain directions thus modelled lower lower dimension space embedded eigenspace models computed using either eigenvalue decomposition also called principal component analysis singularvalue decomposition wish distinguish batch incremental computation batch computation observations used simultaneously compute eigenspace model incremental computation existing eigenspace model updated using new observations previous research incremental computation eigenspace models considered adding exactly one new observation time eigenspace model 1 2 3 4 5 6 common theme methods none require original observations retained rather description hyperellipsoid sufficient information incremental computation new eigenspace model previous approaches allows change dimensionality hyperellipsoid single additional axis added necessary previous work allows shift centre hyperellipsoid 6 methods keep fixed origin proves crucial eigenspace model used classification explained 6 set observations whose mean far origin clearly well modelled hyperellipsoid centred origin using incremental methods previous observations need kept thus reducing storage requirements making large problems computationally feasible incremental methods must used observations available simultaneously example computer may lack memory resources required store observations true even lowdimensional methods used compute eigenspace 5 12 mention lowdimensional methods later give advantage number observations less dimensionality space n n often true observations images even observations available usually faster compute new eigenspace model incrementally updating existing one rather using batch computation 3 incremental methods typically compute p eigenvectors p minn n disadvantage incremental methods accuracy compared batch methods incremental updates made inaccuracy small probably acceptable great majority applications 6 many thousands updates made eigenspace models incremented single observation time inaccuracies build although methods exist circumvent problem 4 contrast methods allow whole new set observations added single step thus reducing total number updates existing model section 2 defines eigenspace models detail standard methods computing used representing classifying observations section 3 discusses merging eigenspace models section 4 treats splitting section 5 presents empirical results section 6 gives conclusions eigenspace models section describe mean eigenspace models briefly discuss standard methods batch computation observations represented using firstly establish notation rest paper vectors columns denoted single un derline matrices denoted double line size vector matrix often im portant wish emphasise size denoted subscripts particular column vectors within matrix denoted superscript superscript vector denotes particular observation set observations treat observations column vectors matrix example mn ith column vector theta n matrix denote matrices formed concatenation using square brackets thus mn b theta n1 matrix vector b appended mn last column 21 theoretical background consider n observations column vector x compute eigenspace model follows mean observations covariance note c nn real symmetric axes hyperellipsoid spread observations axis eigenvectors eigenvalues eigenproblem equivalently eigenvalue decomposition evd c nn nn nn columns u nn eigenvectors nn diagonal matrix eigenvalues eigenvectors orthonormal u nn u nn nn n theta n identity matrix ith eigenvector u ith eigenvalue ii nn associated eigenvalue length eigen vector ith axis hyperellipsoid typically p minn n eigenvectors significant eigenvalues hence p eigenvectors need retained observations correlated covariance matrix good approximation rankdegenerate small eigenvalues presumed negligible thus eigenspace model often spans pdimensional subspace ndimensional space embedded different criteria discarding eigenvectors eigenvalues exist suit different applications different methods computation three common methods 1 stipulate p fixed inte ger keep p largest eigenvectors 5 2 p eigenvectors whose size larger absolute threshold 3 3 keep p eigenvectors specified fraction energy eigenspectrum computed sum eigenvalues retained chosen discard certain eigenvectors eigenvalues recast equation 4 using block form matrices vectors without loss general ity permute eigenvectors eigenvalues u np eigenvectors kept pp eigenvalues nd dd discarded may rewrite equation 4 nn u nd dd u np u nd nd 5 hence error u nd dd u nd small dd 0 nd thus define eigenspace model omegagamma mean reduced set eigenvectors eigen values number observations 22 lowdimensional computation eigenspace models lowdimensional batch methods often used compute eigenspace models especially important dimensionality observations large compared number thus may used compute eigenspace models would otherwise infeasible incremental methods also use low dimensional approach principle computing eigenspace model requires construct n theta n matrix n dimension observation practice model computed using n theta n trix n number observations advantage applications like image processing typically n n show done first considering relationship eigenvalue decomposition evd singular value decomposition svd leads simple derivation lowdimensional batch method computing eigenspace model results obtained greater length 5 see also 12 let nn set observations shifted mean x svd nn u nn left singular vectors identical eigenvectors previously given sigma nn matrix singular values leading diagonal nn right singular vectors u nn v nn orthonormal matrices used compute eigenspace models lowdimensional way follows nn n theta n eigenproblem nn nn n except presence extra trailing zeros main diagonal nn discard small singular values singular vectors following remaining eigenvectors vectors u np pp result formed basis incremental technique developed 5 allow change origin approach readily generalise merging splitting others 3 observe solution based matrix product likely lead inaccurate results conditioning problems develop method incrementally updating svd solutions single observation although svd method proven accurate see 6 difficult generalise especially change origin taken account svd methods actually proposed quite early development incremental eigenproblem analysis 2 early work included proposal delete single observations extend merging splitting svd also formed basis proposal incrementally update eigenspace several observations one step 10 however contrary method possible change dimension solution eigenspace considered none methods considered change origin incremental method based matrix product c nn specifically approximation equation 6 generalisation earlier work 6 appears naturally special case adding single observation 23 representing classifying observation highdimensional observations may approximated lowdimensional vector using eigenspace model eigenspace models may also used clas sification briefly discuss ideas prior using results section ndimensional observation x n represented using eigenspace n pdimensional vector shifts observation mean represents components along eigenvec tor also called karhunenloeve transform 13 ndimensional residue vector defined h n orthogonal every vector u np thus h n error representation x n respect likelihood associated observation given clearly definition cannot used directly cases n n c nn rank degenerate cases use alternative definition due moghaddam pentland 8 appropriate full explanation beyond scope paper merging eigenspace models turn attention one two main contributions paper merging eigenspace models derive solution following problem let nm two sets observations let eigenspace models respectively problem compute eigenspace model phi z w nr z nnm nm using onlyomega psi clearly total number new observations combined mean z n combined covariance matrix c nn nn covariance matrices nm respectively wish compute eigenvectors eigenvalues satisfy ns ss ns eigenvalues subsequently discarded give r nonnegligible eigenvectors eigenvalues problem solved size necessarily bounded explain perhaps surprising additional 1 upper limit latersection 311 briefly needed allow vector difference means 31 method solution problem may solved three steps 1 construct orthonormal basis set upsilon ns spans eigenspace models xgammay basis differs required eigenvectors w ns rotation r ss ns upsilon ns r ss 2 use upsilon ns derive intermediate eigenproblem solution problem provides eigen values pi ss needed merged eigenmodel eigenvectors r ss comprise linear transform rotates basis set upsilon ns 3 compute eigenvectors w ns discard eigenvectors eigenvalues using chosen criteria discussed yield pi rr give details step 311 construct orthonormal basis set construct orthonormal basis combined eigenmodels must chose set orthonormal vectors span three subspaces 1 subspace spanned eigenvectors u np 2 subspace spanned eigenvectors v nq 3 subspace spanned x gamma last single vec tor necessary vector joining centre two eigenspace models need belong either eigenspace accounts additional 1 upper limit bounds equation 17 example consider case eigenspaces 2d ellipse 3d space ellipses parallel separated vector perpendicular clearly merged model 3d ellipse vector origins sufficient spanning set ns nt nt orthonormal basis set component eigenspace psi orthogonal eigenspace omegagamma addition accounts component x gamma orthogonal eigenspaces construct nt start computing residues eigenvectors v nq respect eigenspace omegagamma h nq orthogonal u np sense general however h nq zero vectors vectors represent intersection two eigenspaces also compute residue h x respect eigenspace omegagamma using equation 12 nt computed finding orthonormal basis h nq h sufficient ensure upsilon ns orthonormal grammschmidt orthonor malisation may used nt 312 forming intermediate eigenproblem form new eigenproblem substituting equation 19 equation result together equation 15 equation 16 obtain nt ss pi ss r ss nt multiplying sides left u nt right u np nt using fact u np nt left inverse u np nt obtain nt nt ss pi ss r ss new eigenproblem whose solution eigenvectors constitute r ss seek whose eigenvalues provide eigenvalues combined eigenspace model know covariance matrices c nn nn eliminated follows first term equation 24 proportional nt nt nt c nn u np nt c nn nt equation 6 u u np also u nt pt construction using equation 6 conclude u np nt u np nt second term equation 24 proportional nt nt np nn u np u np nn nt nt nt nn nt nn substitution gives u np nt nt nt nt equation 20 g pq nt v nq obtain nt nt consider final term equation 24 nt nt nt nt nt nt setting nt becomes new eigenproblem solved may approximated g pq g pq ss ss r ss matrix size thus eliminated need original covariance matrices note also reduces size central matrix left hand side crucial computational importance makes eigenproblem tractable 313 computing eigenvectors matrix pi ss eigenvalue matrix set compute eigenvectors r ss comprise rotation upsilon ns hence use equation compute eigenvectors pi ss however eigenvectors eigenvalues need kept may discarded using criterion previously discussed section 2 discarding eigenvectors eigenvalues usually carried time pair eigenspace models merged 32 discussion form lution briefly justify solution obtained correct form considering several special cases first suppose eigenspace models null specified 0 0 0 0 system clearly degenerate null eigenvectors zero eigenvalues computed exactly one eigenspace model null nonnull eigenspace model computed returned process see suppose psi null second third matrices lefthand side equation 31 disappear first matrix reduces pp exactly hence eigenvalues remain unchanged case rotation r ss identity matrix eigenvectors also unchanged insteadomega null model second matrix remain nt v nq related rotation else identical solution eigenproblem computes inverse rotation eigenspace model remain unchanged suppose psi exactly one observation specified 0 0 1 hence middle term left equation 31 disappears nt unit vector direction x hence scalar eigenproblem becomes exactly form obtained one observation explicitly added proven elsewhere 6 special case interesting properties new observation lies within subspace spanned u np change eigenvectors eigenvalues explained rotation scaling caused g p g furthermore unlikely event right matrix disappears altogether case eigenvalues scaled nn 1 eigenvectors un changed finally indicating stable model limit theomega exactly one observation specified x 0 0 1 thus first matrix left equation 31 disappears g pq zero matrix nt v nq h h component orthogonal eigenspace psi hence eigenproblem given case gamma tq form delta qq row column zeros appended also fl substitution terms shows case solution reduces special case adding single new observation equation 33 form equation 32 readily shown theomega psi models identical case third term left equation 31 disappears furthermore gamma tq zero matrix g pq u np identity matrix hence first second matrices left equation 31 identical reduce matrices eigenvalues hence adding two identical eigenmodels yields identical third finally notice fixed n 1 solution tends theomega model fixed n reverse true n tend 1 simultaneously final term loses significance 33 algorithm completeness express mathematical results obtained merging models form algorithm direct computer im plementation see figure 1 34 complexity computing eigenspace model size n single batch incurs computational cost 3 examination merging algorithm shows also requires intermediate eigenvalue problem solved well steps overall giving cubic computational requirements nevertheless let us suppose n observations represented p eigenvectors psi function merge returns z w pi p begin column vector h discard column small magnitude endfor delta number basis vectors construct lhs equation 31 eigenvalues eigenvectors discard small eigensolutions appropriate end figure 1 algorithm merging two eigenspace models observations represented q eigenvectors typically p q much less n respectively compute overall model batch method requires 3 operations assuming models merged already known merging method requires op problem solved becomes smaller greater amount overlap eigenspaces ofomega psi fact number operations required os 3 see end section 311 one models merged unknown initially incur extra cost 3 reduces advan tage nevertheless one typical scenario might expectomega known existing large database n observations psi relatively small batch observations added case extra penalty om 3 little significance compared 3 exact analysis complicated indeed data dependent expect efficiency gains time memory resources practice furthermore computer memory limited subdivision initial set may unavoidable order manage eigenspace model computation provided tractable solution problem splitting eigenspace models show split two eigenspace models given eigenspace model remove give third use pi rr ss available general although splitting essentially opposite merging completely true impossible regenerate information discarded overall model created whether batch methods otherwise thus split one eigenspace model larger one eigenvectors remnant must still form subspace larger state results splitting without proof clearly new mean case merging new eigenvalues eigenvectors computed via intermediate eigenprob lem case g rp rp r rr x eigenvalues seek q nonzero elements diagonal rr thus permute r rr rr write without loss generality r rr rr r rt r rp r rt hence need identify eigenvectors r rr nonzero eigenvalues compute u np u np terms complexity splitting must always involve solution eigenproblem size r algorithm splitting may readily derived using similar approach merging section describes various experiments carried compare computational efficiency batch method new methods merging splitting eigenspace models produced compared models terms euclidean distance means mean angular deviation corresponding eigenvectors mean rela tiveabsolute difference corresponding eigen values took care models number dimensions well simple measures performance measures may relevant eigenspace models used particular ap plications thus tests also per formed eigenspace models may used approximating highdimensional observations lowdimensional vector error size residue vector sizes residue vectors readily compared batch incremental meth ods eigenspace models may also used classifying observations giving likelihood observation belongs cluster different eigenspace models may compared relative differences likelihoods average differences corresponding observations used database 400 face images 10304 pixels available online 1 tests reported similar results obtained tests randomly generated data gray levels images scaled range 0 1 division preprocessing done implemented functions using commercially available software matlab computer standard configuration sun sparc ultra 10 300 hz 64 mb ram results present used 300 images physical resources computer meant heavy paging started occur beyond limit batch method although paging affect incremental method tests experimental procedure used compute eigenspace models using batch method 12 compare models produced merging splitting models also produced batch method case largest three data sets contained 300 images partitioned two data sets containing multiple 50 images included degenerate cases one model contained zero images note tested smaller models merged larger ones viceversa number eigenvectors retained model including merged model set 100 maximum ease comparing results initial tests using strategies indicate resulting eigenspace model little effected 51 timing measuring cpu time ran code several times chose smallest value minimise effect concurrently running process initially measured time taken compute model using batch methods data sets different sizes results presented figure 2 show cubic complexity predicted 1 olivetti database faces httpwwwcamorlcoukfacedatabasehtml number input images time seconds observed data cubic fit figure 2 time compute eigenspace model batch method versus number images n time approximated cubic 53 theta 10 gamma4 n 511 merging measured time taken merge two previously constructed models results shown figure 3 shows time complexity approximately symmetric point half number input images result may surprising algorithm given merging symmetric respect inputs despite fact mathematical solution independent order approximate symmetry timecomplexity explained assuming independent eigenspaces fixed upperbound number eigenvectors suppose numbers eigenvectors models n complexities main steps approximately fol lows computing new spanning set om 3 solving eigenproblem 3 3 rotating new eigenvectors 3 3 thus time com plexity stated conditions approximately 3 3 symmetric next times taken compute eigenspace model 300 images total using batch method merging method compared number images first model time seconds incremental time joint figure 3 time merge two eigenspace models images versus number im ages n omegagamma number images phi 300 gamma n hence total number different images used compute phi constant 300 figure 4 incremental time time needed compute eigenspace model merged merge precomputed existing one joint time time compute smaller eigenmodels merge might expected incremental time falls additional number images required falls joint time approximately con stant similar total batch time incremental method offers time saving cases use much less memory could clearly seen model computed using 400 images paging effects set batch method used time taken rose 800 seconds time produce equivalent model merging two submodels size 200 however took less half 512 splitting time complexity splitting eigenspaces depend principally size large eigenspace smaller space removed size smaller eigenspace number images first model time seconds incremental time joint batch time figure 4 time make complete eigenspace model database 300 images incremental time addition time construct eigenspace added joint time time compute eigenspace models merge little effect size intermediate eigenproblem solved depends size larger space therefore dominates complexity expectations borne experi mentally computed large eigenmodel using 300 images removed smaller models sizes 50 250 images inclusive steps 50 images 100 eigenvectors kept modelthe average time taken approximately constant ranged 9 12 seconds mean time 114 seconds figures much smaller observed merging large eigenspace contains 100 eigenvectors thus matrices involved computation size 100 theta 100 whereas merging size least 150 theta 150 computations involved computing orthonormal basis 52 similarity performance measures used assessing similarity performance batch incremental methods described 521 merging first compared means models produced method using euclidean distance distance greatest models merged number input images 150 case fall smoothly zero either models merged empty value maximum typically small measured 35 theta 10 gamma14 units gray level compares favourably working precision matlab 22 theta 10 gamma16 next compared directions eigenvectors produced method error eigenvector direction measured mean angular deviation shown figure 5 ignoring degenerate cases one models empty see angular deviation single minimum eigenspace models built number images may small model added large model information tends swamped results show angular deviation small average number input images eigenvector mean angular deviation degrees figure 5 angular deviation eigenvectors produced batch incremental methods versus number images first eigenspace model sizes eigenvalues methods compared next general observed smaller eigenvalues larger errors might expected contain relatively little information susceptible noise figure 6 given mean absolute difference eigenvalue rises single peak number input images models even maximal value small 7 theta 10 gamma3 units gray level largest eigenvalue typically 100 number input images eigenvalue mean absolute deviation figure difference eigenvalues produced batch incremental methods versus number images first eigenspace model turn performance measures merged eigenspaces represent image data little loss accuracy measured mean difference residue error figure 7 performance measure typically small 10 gamma6 units gray level per pixel clearly noticeable effect finally compared differences likelihood values equation produced two methods difference small typically order 8 shows compared mean likelihood observations differences classifications would made models would small number input images difference mean residue figure 7 difference reconstruction errors per pixel produced batch incremental methods versus number images first eigenspace model number input images mean class difference figure 8 difference likelihoods produced batch incremental methods versus number images first eigenspace model 522 splitting similar measures splitting computed using exactly conditions described testing timing splitting exactly characteristics described merging case model subtracted computed batch method removed overall model splitting pro cedure also batch model made purposes comparison residual data set follows phrase size removed eigenspace means number images used construct eigenspace removed eigenspace built 300 images euclidean distance means models produced method grows monotonically size removed eigenspace falls never exceeds 15 theta 10 gamma13 graylevel units splitting slightly less accurate respect merging mean angular deviation corresponding eigenvector directions rises similar fashion 06 degrees size removed eigenspace 250 11 removed eigenmodel size 100 represented maximum deviation error error degree obtained removed model size 50 angular deviations somewhat larger merging mean difference eigenvalues shows general trend maximum 05 units gray level size removed eigenspace 50 much larger error case merging still relatively small compared maximum eigenvalue 100 case merging deviation eigenvalue grows larger size importance eigenvalue falls difference reconstruction error rises size removed eigenspace falls size units gray level per pixel negligible difference likelihoods significant relative difference cases factors 10 conducting experiments found relative difference sensitive errors introduced eigenvectors eigenvalues dis carded surprise given likelihood differences magnified exponentially found changing criteria discarding eigenvectors much reduced relative difference likelihood order 10 gamma14 achieved cases conclude application require splitting also require classification eigenvectors eigenvalues must discarded care suggest keeping eigenvectors exceed significance threshold kept overall trend clear accuracy performance grew worse measure used size eigenmodel removed falls 6 conclusion shown merging splitting eigenspace models possible allowing batch new observations processed whole theoretical result novel experimental results show methods wholly practical computation times feasible eigenspaces similar performance characteristics differ little enough matter applications time advantage obtained batch methods whenever one eigenspace models exists already typical scenario addition set observations new years take student faces say existing large database merging method even advantageous eigenspace models exist already typical scenario dynamic clustering classification two eigenspace models merged perhaps create hierarchy eigenspace models r eigenspace update algorithm image analysis kumar natural basis functions topographic memory face recognition probabilistic visual learning object representa tion 3rd edition generalised karhunenloeve expansion tr ctr luis carlos altamirano leopoldo altamirano matas alvarado nonuniform sampling improved appearancebased models pattern recognition letters v24 n13 p521535 january ko nishino shree k nayar tony jebara clustered blockwise pca representing visual data ieee transactions pattern analysis machine intelligence v27 n10 p16751679 october 2005 jieping ye qi li hui xiong haesun park ravi janardan vipin kumar idrqr incremental dimension reduction algorithm via qr decomposition proceedings tenth acm sigkdd international conference knowledge discovery data mining august 2225 2004 seattle wa usa hoi chan thomas kwok autonomic problem determination remediation agent ambiguous situations based singular value decomposition technique proceedings ieeewicacm international conference intelligent agent technology p270275 december 1822 2006 taekyun kim ognjen arandjelovi roberto cipolla boosted manifold principal angles image setbased recognition pattern recognition v40 n9 p24752484 september 2007 jieping ye qi li hui xiong haesun park ravi janardan vipin kumar idrqr incremental dimension reduction algorithm via qr decomposition ieee transactions knowledge data engineering v17 n9 p12081222 september 2005 xiang sean zhou dorin comaniciu alok gupta information fusion framework robust shape tracking ieee transactions pattern analysis machine intelligence v27 n1 p115129 january 2005 john p collomosse peter hall cubist style rendering photographs ieee transactions visualization computer graphics v9 n4 p443453 october