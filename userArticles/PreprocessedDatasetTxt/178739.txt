extracting refined rules knowledgebased neural networks neural networks despite empirically proven abilities little used refinement existing knowledge task requires threestep process first knowledge must inserted neural network second network must refined third refined knowledge must extracted network previously described method first step process standard neural learning techniques accomplish second step article propose empirically evaluate method final possibly difficult step method efficiently extracts symbolic rules trained neural networks four major results empirical tests method extracted rules 1 closely reproduce accuracy network extractedsemi 2 superior rules produced methods directly refine symbolic rulessemi superior produced previous techniques extracting rules trained neural networkssemi human comprehensible thus method demonstrates neural networks used effectively refine symbolic knowledge moreover ruleextraction technique developed herein contributes understanding symbolic connectionist approaches artificial intelligence profitably integrated b introduction artificial neural networks anns proven powerful general technique machine learning fisher mckusick 1989 shavlik et al 1991 however anns several wellknown shortcomings perhaps significant trained ann essentially black box determining exactly ann makes particular decision daunting task significant shortcoming without ability produce understandable decisions hard confident reliability networks address realworld problems also fruits training neural networks difficult transfer neural networks pratt et al 1991 ameliorate problem impossible directly transfer nonneural learning systems hence trained neural network analogous pierre de fermats comment last theorem like fermat network tells discovered something wonderful tell discovered paper sheds light neuralnetwork black box combining symbolic rulebased reasoning neural learning approach form threelink chain illustrated figure 1 symbolic knowledge revised corrected using neural networks thus approach present makes possible use neural networks empirical learning algorithm underlying rulerefinement system figure approximately first link threelink chain insert knowledge need neither complete correct neural network using kbann towell et al 1990 networks created using kbann referred knowledgebased neural networks knns step changes representation rules symbolic neurallybased thereby making rules refinable standard neural learning methods second link chain train knn using set classified training examples standard neural learning algorithm backpropagation rumelhart et al 1986 method weight optimization feedforward neural networks rules upon knn based corrected consistent training examples final link extract rules trained knns extremely difficult task arbitrarilyconfigured networks somewhat less daunting knns due properties stem initial comprehensibility taking advantage properties developed method efficiently extract intelligible rules networks evaluated two realworld test problems terms ability correctly classify examples seen training method produces sets rules closely approximate networks came moreover extracted rules equal superior rules resulting rulerefinement methods act directly rules rather rerepresentation neural network also show method superior previous algorithm extraction rules general neural networks eg saito nakano 1988 fu 1991 next section contains brief overview method inserting rules neural networks subsequent section describes method best reported method extraction rules trained knns sections 4 5 present series empirical tests ruleextraction method section 4 use two realworld learning problems taken molecular biology determine strengths ruleextraction method section 5 use artificial domain characterize closely possible using realworld domains abilities ruleextraction method final sections paper relate approach others extract rules trained neural networks discuss future research plans 2 kbann rulestonetwork translator kbann translates symbolic knowledge neural networks defining topology connection weights networks creates uses knowledge base domainspecific inference rules represented propositional horn clauses define initially known topic detailed explanation procedure used kbann translate rules knn given towell et al 1990 towell 1991 provide brief summary procedure following example consider sample knowledge base figure 2a defines membership category figure 2b represents hierarchical structure rules solid dotted lines represent necessary prohibitory dependencies respectively figure 2c represents knn results translating knowledge base neural network units 1 x figure 2c introduced knn represent disjunction 1 units basic processing elements neural networks receive realnumbered signals units weighted connections referred links mathematically transform signals realnumbered output sent units see table 1 generally units divided three categories input receive signals environment output send signals environment hidden connection environment rule set otherwise unit knn corresponds consequent antecedent knowledge base thick lines figure 2c represent heavilyweighted links knn correspond dependencies knowledge base thin lines represent links added network allow additional refinement knowledge base figure approximately example illustrates using procedure initialize knns two principal benefits first algorithm indicates input features believed important examples classification second specifies important derived features eg b c figure 2 thereby guiding choice number connectivity hidden units knn 3 rule extraction section presents detailed explanations approach previously reported approach extracting rules trained knns first algorithm presented comparison second algorithm introduced section 34 focus work 31 assumptions rule extraction methods present make two assumptions trained networks first assumption underlies methods extracting rules trained neural networks units either maximally active ie activation near one inactive ie activation near zero 2 assumption particularly restrictive commonlyused logistic activation function slightly modified ensure units approximate step functions making assumption noninput unit trained knn interpreted step function boolean rule therefore rule extraction problem determine situations rule true second assumption training significantly alter meaning units making assumption methods able attach labels extracted 2 exceptions generalization include method proposed bochereau bourgine 1990 methods extracting fuzzy rules eg berenji 1991 w ij 2 activation unit w ij weight link unit j unit bias unit parameter affecting slope sigmoid except explicitly stated table 1 logistic activation function used backpropagation rules correspond consequents symbolic knowledge upon knn based thereby enhancing comprehensibility rules examination trained networks indicates meanings usually quite stable although sutton 1986 suggests assumption may generally true 32 commonalities method introduce extracting rules neural networks well previously described eg saito nakano 1988 fu 1991 trying find combinations input values unit result activation near one hence section describe mathematics underlying neural networks use units backpropagationtrained neural networks normally activations defined equations 1 2 table 1 rephrase equation 1 activation unit function sum weighted inputs unit less bias broadly speaking result equation 2 summed weighted inputs exceed bias activation unit near one otherwise activation near zero hence ruleextraction methods search constraints input settings weighted sum guaranteed exceed bias first assumptions units trained networks activations near zero one simplifies search ensuring links carry signal equal weight signal result equation 1 simplified equation 3 states ruleextraction methods need concerned weight links entering unit reduces rule extraction search unit sets incoming links whose summed weights exceeds guarantees units bias exceeded regardless activation carried incoming links rule extraction simplified equation 2 guarantees noninput units always nonnegative activations problems investigate input units always activations f0 1g therefore negativelyweighted links give rise negated antecedents positivelyweighted links give rise nonnegated antecedents considerably reduces size search space fu 1991 finally note short exactly copying unit links way limit kinds differences extracted rules network reasonable complexity extracted rules may make errors omission commission respect network errors generally result cumulative effects lowweight links appear extracted rules one chief problems rule extraction minimize errors without simply rewriting network 33 subset algorithms first method rule extraction describe fundamentally similar approach described saito nakano 1988 well fu 1991 however particular implementation call general approach subset method explicitly searches subsets incoming weights exceed bias unit paper uses subset algorithm straw man show effectiveness new method rule extraction presented next section subset algorithms represent state art published literature simple breadthfirst subset algorithm starts determining whether sets containing single link sufficient guarantee bias exceeded yes sets rewritten rules search proceeds increasing size subsets possible subsets explored finally algorithm removes subsumed overlygeneral rules example given link weights bias shown figure 3a subset algorithm would initially find five rules eliminating one subsumed rule algorithm returns four rules listed figure 3b assuming units activations near zero one table 2 specification subset algorithm hidden output unit extract fi p subsets positivelyweighted incoming links whose summed weight greater bias unit b subset p fi p subsets found step 1 extract fi n minimal subsets negativelyweighted links whose summed weights greater sum p less bias unit 2 let z new predicate used nowhere else 3 subset n fi n subsets found step b1 form rule n z 4 form rule p z name unit figure 3 approximately major problem subset algorithms cost finding subsets grows size power set links unit hence approach expected exactly reproduce behavior simple networks working small problems avoid otherwise prohibitive combinatorics implementations subset algorithms use heuristics instance saito nakano 1988 establish ceiling number antecedents extracted rules establishing priori ceiling number antecedents several shortcom mings instance ceiling one domain sets acceptable tradeoff number extracted rules difference extracted rules network may acceptable another domain serious problem occurs properties problem studied suggest rules require large number antecedents instance initial rules one realworld domains study indicate smallest bound number antecedents safely set could require considering 10 5 sets therefore rather setting bound number antecedents implementation uses branchandbound algorithm limited terms number rules may find subset algorithm presented pseudocode table 2 see fu 1991 complete description subset algorithm briefly algorithm iterates every noninput unit trained knn unit uses branchandbound algorithm positivelyweighted links find fi p positive subsets whose summed weights exceed units bias positive subset makes new branchandbound search negativelyweighted links find minimal negative subsets whose summed weight greater magnitude difference summed weight positive subset units bias finally algorithm forms rules result consequent satisfied antecedents positive subset true none negative subsets associated positive subset contain antecedents true negative subsets sufficient prevent node active hence must ensure minimal subsets contains least one unsatisfied antecedent algorithm may discover fi p 1 rules noninput unit network translated described tradeoff number extracted rules accurate reproduction networks behavior hence set fi p fi n find reasonablysized sets rules approximate network came settings use result finding 300 rules realworld domains studied general accurate reproduction knns require many rules subset extracts large set rules smaller many handcrafted expert systems eg mcdermott 1982 hence subset delivers sets rules least potentially tractable however rules tend hide significant structures trained networks instance figure 3a links b c weight link e negative weight looking problem way suggests rules figure 3b could rewritten following rule provides much clearer statement conditions 3 f b c noteg recognizing structure shared among several conjunctive rules led development algorithm described next focus paper 34 mofn method algorithm called mofn addresses combinatorial presentation problems inherent subset algorithms differs subset algorithms explicitly searches rules form following n antecedents true suggested previously method arose noticed rule sets discov 9table 3 mofn approach rule extraction 1 hidden output unit form groups similarlyweighted links 2 set link weights group members average group 3 eliminate groups significantly affect whether unit active inactive 4 holding link weights constant optimize biases hidden output units using backpropagation algorithm 5 form single rule hidden output unit rule consists threshold given bias weighted antecedents specified remaining links 6 possible simplify rules eliminate superfluous weights thresholds ered subset often contain mofn style concepts support method comes experiments indicate neural networks good learning mofn concepts 1989 well experiments variant id3 show bias towards mofn style concepts useful murphy pazzani 1991 finally note purely conjunctive rules result set disjunctive rules results using mofn rules restrict generality idea underlying mofn abstracted version appears table 3 individual antecedents links unique importance rather groups antecedents form equivalence classes antecedent importance interchangeable members class equivalence class idea key mofn algorithm allows algorithm consider groups links without worrying particular links within group 341 mofn algorithm section contains detailed description ruleextraction algorithm illustrate algorithms example following section step 1 clustering backpropagation training tends group links knns loose clusters rather equivalence classes assumed mofn algorithm 3 hence first step mofn creates equivalence classes clustering links using standard clustering method join algorithm hartigan 1975 method operates successively combining two closest clusters starts cluster holding single link clustering stops pair clusters closer set distance mofn uses 025 step 2 averaging groups formed second step algorithm sets weight links group average groups weight thus first two steps force links network equivalence classes required rest algorithm step 3 eliminating equivalence classes place procedure next attempts identify eliminate groups unlikely bearing calculation consequent groups generally low link weights members elimination proceeds via two paths one algorithmic one heuristic first elimination procedure algorithmically attempts find clusters links cannot effect whether total incoming activation exceeds bias done calculating total possible activation cluster send total possible activation compared levels activation reachable given link weights network clusters cannot change whether net input exceeds bias eliminated note procedure similar subset however clustering link weights considerably reduces combinatorics problem heuristic elimination procedure based explicitly upon whether net input received cluster ever necessary correct classification training example procedure operates presenting training example clustered network sequentially zeroing input cluster results qualitative change activation unit receiving activation cluster cluster marked necessary clusters marked necessary eliminated 3 nowlan hintons 1991 neuralnetwork training algorithm draws link weights clusters reduces eliminate problem one future research plans combine training algorithm knowledgeinsertion ruleextraction methods step 4 optimizing unimportant groups eliminated fourth step mofn optimize bias unit step necessary first three steps change times unit active result altered networks may error rates significantly higher end training optimization done freezing weights links groups stay intact retraining biases network using backpropagation reflect rulelike nature network activation function slightly modified moreclosely resembles step function step 5 extracting step mofn algorithm forms rules exactly reexpress network rules created directly translating bias incoming weights unit rule weighted antecedents rule true sum weighted antecedents exceeds bias note equivalence classes elimination groups rules considerably simpler original trained network fewer antecedents antecedents tend weight classes step 6 simplifying sixth final step rules simplified whenever possible eliminate weights thresholds mofn simplifies rule scanning determine possible combinations antecedents exceed rules threshold ie bias scan may result one rule hence tradeoff simplification procedure complexity individual rule complexity resulting number syntactically simpler rules example consider rule 4 simplification rewrites rule following three rules 2 b c e 1 b c e 2 x z x z elimination weight biases requires rewriting single rule five rules rule left original state 4 function numbertrue returns number antecedents following set true 342 example mofn figure 4 illustrates process single unit seven incoming links transformed mofn procedure rule requires two three antecedents true first two steps group links two clusters one four links weight 11 one three links weight 61 third step algorithmically eliminates cluster weight 11 situation links affect unit fourth step bias optimization unnecessary example averaging elimination procedures significantly change properties unit fifth sixth steps simple single cluster remaining hence final mofn written inspection figure approximately 343 complexity analysis mofn complexity mofn cannot precisely analyzed biasoptimization phase uses backpropagation note problem addressed bias optimization considerably simpler initial training network usually networks order magnitude fewer links bias optimization initial training moreover biases allowed change result step takes reasonably short time 5 initial clustering requires ou theta l 2 time u number units l average number links received unit cluster elimination requires theta u theta l time n number training examples steps uses ou theta l time 35 subset mofn summary ruleextraction algorithms strengths respect example individual rules returned subset often easily understood returned mofn however subset generates many rules repetitive mofn rule sets returned mofn usually easier understand subset moreover measuring time respect number links unit subset exponential algorithm whereas mofn training simple classes neural networks proven npcomplete judd 1988 hinton 1989 suggests practice backpropagation usually runs ou theta l 3 time approximately cubic finally results presented later paper indicate rule sets derived mofn approximately equal accuracy networks extracted rules extracted subset significantly worse next sections contain series empirical tests demonstrate differences two algorithms 4 experiments realworld datasets 41 datasets test efficacy ruleextraction procedure realworld problems studied two problems domain molecular biology use datasets members small collection realworld problems exist approximatelycorrect domain theory set classified examples also datasets previously studied theory revision literature eg towell et al 1990 ourston mooney 1990 thompson et al 1991 noordewier et al 1991 making possible comparisons perform section 411 notation datasets special notation used simplify specifying locations dna sequence dna sequences linear strings nucleotides nucleotide drawn fa g cg idea number locations respect fixed biologicallymeaningful reference point negative numbers indicate sites preceding left reference point positive numbers indicate sites following reference point hence rules antecedents refer input features first state location relative reference point sequence vector dna nucleotide sequence nucleotides must occur eg 3 biological convention position numbers zero used rule specifications indicates nucleotide suffice example first rule conformation table 5 says must 45 nucleotides reference location another must position 44 two nucleotides appear finally must location 41 finally use standard ambiguity codes given table 4 refer acceptable alternatives particular location table 4 ambiguity codes dna nucleotides code meaning code meaning code meaning c r g w c g c k g v c g h c g b c g x g c problem 1 promoter recognition first problem investigate prokaryotic promoter recognition originally described towell et al 1990 promoters short dna sequences initiate expression gene basically promoter site protein rna polymerase binds dna according rules table 5 two sites binding must occur minus 10 minus 35 regions addition conformation rules attempt capture twist dna helix thereby ensuring binding sites spatially aligned set rules derived straightforward manner biological literature harley reynolds 1987 hawley mcclure 1983 koudelka et al 1987 consensus sequences promoters consensus sequences describe probable nucleotides given location however nucleotide appears every promoter instance minus10 rules probability nucleotide 085 also rules derived multiple sources prune subsumed rules extraction rules strictly less general others figure 5 approximately rule set translated kbann neural network topology shown figure 5 recall kbann adds additional lowweighted links shown additional sequence information relevant algorithm capture information training input features promoter recognition 57 sequential dna nucleotides location dna sequence translated 4 inputs units one nucleotide hence promoter network 228 input units problem reference point site gene transcription would begin example contained promoter point located 50 nucleotides preceding seven following table 5 initial rules promoter recognition promoter contact conformation contact minus35 minus10 conformation 45 aaa conformation 45 aa 28 ttaat04 conformation 49 27 tattg 01 conformation 47 caattac 22 gtc 08 gcgcccc reference point thus positive examples contain first seven nucleotides transcribed gene set examples contains 53 sample promoters 53 nonpromoter sequences 53 sample promoters obtained compilation produced hawley mcclure 1983 derived negative training examples randomly selecting contiguous substrings 15 kilobase sequence provided prof record university wisconsins chemistry department sequence fragment e coli bacteriophage isolated restriction enzyme haeiii virtue fact fragment bind rna polymerase believed contain promoter sites record personal communication prior training rules table 5 classify 106 examples promoters result rules useful say 42 problem 2 splicejunction determination second problem examine primate splicejunction determination originally described learning problem noordewier et al 1991 splice junctions points dna sequence superfluous dna removed process protein creation higher organisms problem posed dataset recognize given sequence dna boundaries exons ie parts dna sequence retained splicing introns ie parts dna sequence spliced problem consists two subtasks recognizing exonintron boundaries recognizing intronexon boundaries dataset problem contains 3190 examples approximately 25 exonintron boundaries 25 intronexon boundaries remaining 50 neither example consists 60nucleotide long dna sequence categorized according type boundary center sequence center sequence reference location used numbering nucleotides experiments presented paper randomly selected 1000 examples 3190 examples available addition examples information splicejunctions includes set 21 rules derived standard biological textbooks rule set shown appears noordewier et al 1991 towell 1991 promoter recognition success rate splice junction rules due largely tendency say rules correctly classify 40 ie 3 ei examples 43 experiments section presents set experiments designed determine relative strengths weaknesses two ruleextraction methods described section 3 compare ruleextraction techniques using two measures quality measured accuracy rules fidelity network extracted comprehensibility measured characterizing whole sets rules looking individual rules final part section examines meanings individual rules includes samples rules extracted trained knns subset mofn 431 systems methodology addition comparing mofn algorithm subset compare mofn three symbolic algorithms either ourston mooney 1990 c45 quinlan 1987 linus dzeroski lavrac 1991 first algorithms either ourston mooney 1990 method empirically adapting set propositional rules correct training examples c45 quinlan 1987 distinct algorithms compare builds decision trees without using background knowledge however extracts rules trees like systems consider final result c45 set rules final algorithm linus augments set input features boolean features represent truth value consequent rule set hence promoter domain linus 244 features available 228 original plus 16 rules use c45 inductive component linus hence result linus set rule may include consequents original rules training testing methodology neural networks train networks either training examples correct within 020 desired output activation b every example presented 500 times c improvement classification occurs five presentations every training example last termination criterion based upon fahlman lebieres 1989 patience metric following hintons 1989 suggestion improved network interpretability weights subject gentle decay training 6 addition networks trained using crossentropy error function hinton 1989 experience indicates better able handle type errors occur knns standard error function rumelhart et al 1986 testing examples considered correctly classified outputs within 05 correct training testing methodology linus c45 c45 trained using defaults code provided r quinlan specifically use ten trials windowing procedure training set build ten decision trees trees transformed ten rule sets finally optimal set rules derived ten rule sets optimal rule set used assess generalization training testing methodology either neither trained tested either rather numbers report derived ourstons thesis 1991 either unable work domain theories include negation report results either splicejunction domain 432 quality issue overriding importance work quality extracted rules measure quality least two dimensional first rules must accurately categorize examples seen training rules lose advantage accuracy kbann provides symbolic learning algorithms towell et al 1990 little value rule extraction would simpler use symbolic method 6 standard weight change function term oe added 0 1 thus weight standard weight adjustment rumelhart et al 1986 weights change example presentation use gentle develop rules using kbann develop highlyaccurate understand able classifiers second extracted rules must capture information contained knn necessary extracted rules useful gaining understanding exactly knn learned training measures show mofn superior subset also show accuracy rules extracted mofn equal superior symbolic algorithms importantly show mofn method extracts rules equivalent networks classifying testing examples accuracy assess accuracy system use ten repetitions 10fold crossvalidation weiss kulikowski 1990 figure 6 plots average tenfold crossvalidation runs comparison figure 6 includes accuracy trained knns prior rule extraction bars labeled network figure 6 approximately recall initial rule sets promoter recognition splicejunction determination correctly categorized 50 61 respectively examples hence systems plotted figure 6 superior initial rules promoter problem c45 linus either subset approximately equivalent testset accuracy methods lag significantly behind mofn method somewhat surprisingly outperforms linus although difference statistically significant accuracy rules extracted either subset approximately equal method achieving accuracy quite different whereas either perfect training set subset marginally better training testing set finally rules extracted mofn significantly superior least 99 confidence rules extracted methods investigated moreover difference accuracy rules extracted mofn networks came statistically significant pattern results somewhat different splice junction problem largely result relative improvement c45 linus domain results c45 linus mofn original networks statistically indistinguishable rules generated subset method much worse perhaps interesting result dataset mofn rules outperform networks came terms training set accuracy earlier tests towell shavlik 1991 showed error table fidelity extracted rules trained knns training examples rule overall probability agreement extraction percent knn extracted rules given method agreement knn correct knn incorrect splice junction subset 86 087 038 mofn 93 095 031 promoter subset 87 090 024 mofn 99 099 010 rate extracted rules promoter problem also networks rules extracted however alterations training method reduce overfitting improved accuracy networks without significantly affecting accuracy extracted rules section 44 analyze result error rate subset rules testing examples statistically worse rules learning methods fidelity fidelity mean ability extracted rules mimic behavior network extracted fidelity important two circumstances 1 extracted rules tool understanding behavior network 2 extracted rules used method transferring knowledge contained network table 6 indicates mofn superior subset reproducing behavior network examples seen training results obtained trials used previous section instance splice junction data rules extracted using mofn give answers trained knns 93 time rules extracted subset match 86 time addition table 6 shows methods rule extraction much better mimicking behavior network network correct incorrect 433 comprehensibility useful extracted rules must accurate also must understand able illdefined concept several ways understandability might measured one approach look statistics describing whole sets rules alternative examine whether individual rules meaningful following sections present results measures global comprehensibility first dimension along characterize comprehensibility sets rules ruleset size size concern sets large number rules difficult impossible understand figure 7 addresses issue ruleset size plotting ruleextraction method space spanned number extracted rules total number antecedents rules data figure 7 represents average crossvalidation study reported figure 6 unfortunately counting rules antecedents mofn networks straightforward mofn simplification phase may increase number rules require reuse antecedents data figures 7 8 blithely ignore complications reflecting rule antecedent counts unsimplified rule sets trained networks count antecedents includes link whose weight within two orders magnitude bias unit receiving link weights lesser size counted little effect finally rule antecedent counts linus include antecedents derived features linus used rules derived features counted single antecedent rule sets extracted linus would size extracted c45 figure 7 approximately taking initial rule sets domain standard interpretability rules refined symbolic methods likely easy understand recall however rules significantly less accurate promoter domain rule sets extracted mofn also likely easily understood contain fewer rules approximately number antecedents initial rule sets hand rules subset extracts much less likely comprehensible second important global statistic number antecedents per rule value large individual rules unlikely understandable bruner et al 1956 furthermore negated antecedents add difficulty evaluating rules nessier weene 1962 however effects negated antecedents difficult quantify weighted antecedents appearing mofn rules implicitly appear trained networks cloud picture rather arbitrarily assigning difficulty ranking type antecedent figure 8 simply show number negative positive antecedents average rule figure approximately measure ruleset size symbolic methods clear winners mofn rules slightly larger subset rules contain negative tecedents still methods return rules well within limits human comprehensibility miller 1956 recall mofn extracts many fewer rules individual comprehensibility global statistics discussed sufficient indicate whether whole rule set likely comprehensible determined rule set potentially comprehensible necessary look individual rules assess meaning make assessment examine rule sets extracted mofn subset methods training knn entire set examples problem domain tables 7 8 present rules mofn subset extract network promoter recognition rules extracted splicejunction domain paper much character promoter domain appear towell 1991 rule sets extracted subset large set fi p fi n values small deliver reasonably accurate rule sets result rules table 8 25 error rate training set rules extracted mofn table 7 somewhat murky vastly comprehensible network 3000 links induced moreover rules rewritten form similar one used biological community namely weight matrices stormo 1990 one major pattern apparent rules extracted mofn subset specifically knn learned disregard conformation conformation rules also dropped either often used linus suggests dropping rules artifact method rather dna nucleotides outside minus35 minus10 regions less important conformation hypothesis koudelka et al 1987 suggests hence demonstrate machine learning methods provide valuable evidence confirming refuting biological theories general rules mofn extracts confirm importance bases identified initial rules however whereas initial rules required matching every base extracted rules allow less perfect match addition extracted rules point table 7 promoter rules extracted mofn promoter minus35 minus10 12 cat 1 12 rbs 31 31 x nt35 ac minus3536 ctgac 10 x nt12 ta minus3535 ttdca 30 x nt12 table 4 meanings letters g c nt returns number named antecedents match given sequence nt14 c g would return 1 matched sequence 14 aaacaaaaa places changes sequence important instance final minus10 rule position 8 strong indicator rule true however replacing g prevents rule ever satisfied difficult get clear picture promoter subset rules table 8 three minus35 rules left column encode simple 2of3 concept rules approximate 3of7 concept note patterns support idea implemented mofn method bias towards mofn style concepts useful 44 discussion results presented section indicate mofn method able extract good set rules trained networks particular data supports contentions 1 mofn method able extract comprehensible rules trained knns reproduce behavior knn extracted 2 extracted rules equivalent superior rules obtained neurallybased symbolic rulerefinement methods rule sets produced mofn algorithm table 8 promoter rules extracted subset promoter minus35 minus10 minus35 minus35b minus35d minus35 37 tta minus35 minus35a minus35b minus35 37 tta minus35 minus35a minus35d minus35 37 gca minus35 37 tc minus35b 37 aca minus35d 37 ttc minus35d 37 tgc minus35d 37 tac minus35d 37 gac minus35d 37 tac abbreviated set rules extracted subset test set accuracy 75 sets whose statistics reported previously section contain 300 rules slightly larger produced symbolic approaches mofns rule sets small enough comprehensible domain experts much ac curate hence although weighing tradeoff accuracy understandability problem userspecific mofn algorithm networktorules translation offers appealing mixture two results section deserve additional consideration superiority mofns rules obtained symbolic rulerefinement techniques occasional superiority mofns rules networks extracted first glance might seem power mofn algorithm derives use expressive language rulerefinement systems true antecedents consequents mofns rules boolean hence mofns rules written disjunction conjunctive rules great increase number rules hypothesis must rejected still two hypotheses explain superiority mofn rulerefinement methods directly modify rules first rerepresentation rule set neural network allows finegrained refinement cast neural network rules modified small steps may make possible closely fit target concept taking large steps required direct rule refinement second hypothesis explain relative superiority mofn symbolic methods rule refinement mofn may better fit nature problem hand instance promoter problem several potential sites hydrogen bonds form dna protein enough bonds form promoter activity occur symbolic methods investigated easily express mofn rules hence advantage mofn method may result output language better fitting natural language dna sequenceanalysis prob lems general viewpoint problem may natural language parsimoniously solved language used learning system similar natural language learning system able effectively learn solve problem hypothesis correct expect problems closely fit inductive bias symbolic rulerefinement systems outperform mofn algorithm observation mofn rules superior networks extracted cannot attributed language bias mofn style rules subset languages expressible using neural networks instead advantage mofn rules likely occurs ruleextraction process reduces overfitting training examples several pieces evidence support hypothesis first noted previously revising network training procedures reduce overfitting promoter domain eliminated advantage mofn rules networks extracted reducing degree network fit training data eliminated advantage mofn rules networks extracted second difference ability correctly categorize testing training examples smaller mofn rules trained knns words rules mofn method extracts slightly better classifying training examples classifying testing examples differences training testing set performance smaller statistically significant 995 confidence using onetailed pairedsample ttest rules subset extracts also property much worse training set mofn rules trained knns testing set aside expect subset algorithms able exceed performance mofn two problems investigated however subset algorithms might expected equal accuracy mofn problems whose solution matches bias towards sets conjunctive rules third piece evidence support overfitting hypothesis comes work pruning neural networks eliminate superfluous parts mozer smolensky 1988 le cun et al 1989 7 rule extraction extreme form pruning links units pruned actions taken remaining network transform network set rules pruning efforts also led gains test set performance generally researchers attribute gains reduced overfitting training examples 5 tests artificial problems previous section explored utility mofn method context two realworld problems advantages realworld problems selfevident disadvantages realworld problems particular difficult closely control study investigates abilities algorithm instance tests previous section left unaddressed questions concerning robustness mofn method flaws initial domain theory know ways domain theories incorrect therefore section step away real world look two artificial problems monks problems thrun et al 1991 gain control distance correct theory theory provided learning system control allows us make additional predictions ability ruleextraction algorithm 51 monks problems chose monks problems testing domain widely tested thoroughly documented problem affords easy replication monks problems originally described thrun et al 1991 report authors describe tests machine learning algorithms monks problems consist population 432 examples defined six 7 also large body literature symbolic machine learning suggests methods reduce overfitting training set improve generalization eg quinlan 1987 features appearing top table 9 first monks problem learn classify examples according domain theory appearing middle table 9 domain theory simply disjunction four conjunctions theory easily expressed rules extracted knn using either subset mofn second monks problem given rule near bottom table 9 considerably complex express concept disjunction conjunctions requires 15 rules six antecedents alternately concept expressed single conjunctive rule two clauses expresses mofn concept rule appears bottom table 9 additional complexity second problem significant effect ability 25 algorithms tested thrun et al 1991 learn concept first problem tested algorithms supplied 124 training examples nine systems tested problem correctly classified whole population overall systems averaged 888 correct despite supplied examples 169 second problem four 25 systems tested correctly classified population overall systems averaged 763 correct system better second problem first 52 experiments question addressed experiments section difficult recover correct domain theory provided theory quite correct define difficulty average size training set needed correctly identify entire population definition difficulty similar teaching dimension goldman kearns 1991 except teaching dimension learningsystem specific experiments compare ability knn learn concept ability mofn subset extract rules express concept modify domain theories three ways first delete antecedents rules tests robustness missing antecedents second add negated unnegated antecedents rules tests robustness rules containing unnecessary antecedents third swap unnecessary antecedents correct antecedents tests ability simultaneously add delete antecedents modified every rule theory instance adding one antecedent rules first monks problem see table creates rule set three rules three antecedents one rule two antecedents addition three types changes modified domain theory accor feature name values smiling 2 fyes nog holding 2 fsword balloon flagg jacketcolor 2 fred yellow green blueg features values monks problems headshape round bodyshape round monk headshape square bodyshape square monk headshape octagon bodyshape octagon monk jacketcolor red monk correct theory first monks problem exactly two headshape round bodyshape round smiling yes holding sword jacketcolor red hastie yes monk correct theory second monks problem two headshape round bodyshape round smiling yes holding sword jacketcolor red hastie yes three headshape round bodyshape round smiling yes holding sword jacketcolor red hastie yes monk reformulation second monks problem table 9 domain correct theories monks problems dance representation first domain theory negated antecedents case systems must learn antecedent effect exactly opposite effect indicated provided domain theory second domain theory altered requirements number present absent antecedents instance rather requiring exactly two antecedents provided domain theory required exactly antecedents n 2 f1 3 4 5g also provided domain theory required n antecedents n 2 f1 2 3 4 5g ie matching 3 4 5 6 antecedents would satisfy rule examined five variants first monks problem fifteen variants second monks problem figures results results obtained creating knn based corrupted domain theory copy knn trained using small percentage population learning correctly classify whole training set rules extracted knn using mofn subset knn two sets extracted rules tested entire population three correctly classify population trial finished otherwise procedure repeated using new copy knn slightly larger training set chosen includes member previously training set size training set increased three systems correctly identify entire population entire population used training result procedure size smallest training set required knn two sets extracted rules correctly classify entire population figure 9 plots average 25 repetitions procedure five corruptions first monk theory eight corruptions second monk theory figure 10 uses data trials figure 9 plots average size terms number antecedents rule sets extracted subset mofn note figures short bars preferred knns unable completely learn training data second monks problem several noise conditions described particular knns failed even single required antecedent swapped unnecessary antecedent knns also failed learn second problem told exact number matched antecedents three told n sufficient total seven fifteen corruptions second domain theory resulted knns unable learn training data seven cases plotted figures 9 10 figure 9 approximately figure approximately 53 discussion significant trend figure 9 one case mofn learns concept fewer examples knn requires result supports suggestion made section 4 effectiveness mofn attributed action postpruning method network make suggestion cases mofn learned concept knn knn perfect training examples knns problem classifying full population spurious correlations among lowlyweighted links impacted categorization examples seen training ruleextraction process used mofn acts eliminate links cause knn problems hand subset explicitly prune network merely attempts reproduce behavior network using boolean logic hence unsurprising problems subset requires population correctly identify concept either mofn knn yet subset occasionally quite well several problems requires fewer training examples knns one problem fewer mofn although occasionally successful often spectacularly bad one case complete unable learn first problem despite knn reliably acquiring concept seeing 50 population looking size concept identified plotted figure 10 clear cases mofn exactly identifies concept first problem mofn extracts original rules four five trials fifth trial mofn includes several useless antecedents extracted rules interestingly one problems subset failed every case mofn able learn second problem extracted correct rules finally moderately surprising knn unable learn second problem corrupting rule set none corruptions particularly severe hoped network would simply resort knowledgefree start point would expected perform along lines standard neural network fullyconnected randomlyinitialized neural network single layer hidden units trained using backpropagation requires 39 dataset reliably learn first problem 47 percent data reliably learn second problem instead problems knn unable learn fell local minima unable escape observation prior knowledge hinder learning significant observation previously made pazzani 1992 focus paper surprisingly knn unable learn concept mofn subset rarely extracted rules expressed concept conclusion experiments show knn capable learning rules underlie concept mofn method able extract set rules close approximation thereof knn 6 related work two distinct sets work closely related ruleextraction methods present paper first set contains symbolic methods learning rules examples eg ourston mooney 1990 thompson et al 1991 dzeroski lavrac 1991 methods avoid extract rules learning performing manipulations directly rules methods appealing require shift symbolic neural representations results presented figure 6 indicate accurate problems investigated mofn method presented second set related research attempts extract rules randomlyweighted anns single layer hidden units saito nakano 1988 fu 1991 report method similar subset algorithm saito nakano looked inputoutput behavior trained networks form rules map directly inputs outputs domain theory provide names hidden units limit combinatorics inherent algorithms like subset saito nakano limited rules four antecedents however even limitation extracted 400 rules one output unit ann 23 outputs thus method potentially useful understanding networks may drown researchers sea rules several groups reported attempts extract rules networks like knns welldefined architecture sestito dillon 1990 avoid combinatorial problems approach rule extraction transforming network j inputs k outputs network j inputs training look links original j inputs similar weight pattern one k additional inputs method discover hierarchical rule sets discover relationship robin isa bird isa animal network must outputs robin bird animal another method extracting rules speciallyconfigured networks mcmillan mozer smolenskys 1991 connectionist scientist game uses neural networks iteratively learn sets propositional rules method similar mofn algorithm except assumes two groups one large positive weight one zero weight many problems including monks problem studied section 5 assumption quite useful significantly constrains search space however realworld problems assumption may prove overly restrictive finally recently attempts extract fuzzy rules neural networks instance berenji 1991 describes method much like kbann except begins ends fuzzy rules several techniques extracting fuzzy rules neural networks also reported hayashi 1990 masuoka et al 1990 bochereau bourgine 1990 7 limitations future work results presented section 4 indicate mofn method able effectively extract rules trained knns thereby making kbann excellent method refining existing rules however method also several limitations heretofore briefly mentioned significant limitations along brief discussion plans addressing limitations ffl mofn method requires network knowledge based networks must initially comprehensible investigating different methods training networks knowledgebased yield networks upon mofn method effective instance nowlan hintons 1991 training method groups links loose clusters may result networks amenable mofn ffl large shifts meanings units result training make extracted rules difficult comprehend minimum system flag rules review better solution would give kbann way analyzing extracted rules ffl domain theories may provide sufficientlyrich vocabulary allow knn accurately learn concept knn missing terms required express concept modify existing terms cover vocabulary shortfall often leads large shifts meaning terms discussed previous point cases necessary augment knn additional hidden units however adding hidden units opens many issues raised section 6 extracting rules networks initialized domain theory ffl system yet tested broad range problems discussed sequence analysis problems often contain aspects mofnlike mofn method may ideally suited sequenceanalysis problems two problems also share aspects related sequence analysis example initial domain theory problems overly specific empirical tests suggest kbann slightly effective domain theory overly specific towell 1991 hence tests broader range datasets needed prove generality method addition extending kbann mofn method address limitations currently working ruleextraction algorithm operates training knn rather allowing link weights freely take arbitrary values algorithm periodically clusters links weights training thus consists alternating cycles standard weight adjustment b rounding quite similar approach rule extraction taken mcmillan mozer smolensky 1991 form rules produced method similar mofn rules however search paths weight space taken algorithm quite different paths taken mofn algorithm training instead undergoing transitions interpretable set rules blackboxish knn back interpretable set rules online clustering algorithm preserve comprehensibility knowledge base training also expect insight training process gained able inspect knns knowledge base various points training lastly investigating ways enhancing comprehensibility rules returned mofn rules extracted networks mofn much comprehensible networks extracted completely satisfied comprehensibility hope alternate forms presentation enhancements mofn algorithm different network training methods improve comprehensibility resulting rules conclusions paper presents empirically validates mofn method extraction rules knowledgebased neural networks shown mofn method part kbann system forms rulerefinement algorithm results rules generalize better examples seen training rules produced allsymbolic rule refinement algorithms attribute generalization ability shift representation initial domain theory symbolic neurallybased shift alters bias system ways enhance ability accurately refine initial rules particular neural representations naturally admit mofn style concepts addition neural representations allow gradations importance antecedents difficult achieve symbolic learning systems representation shifts one used kbann system difficult effect pay handsomely improved performance easiest representation shifts involve training examples relative ease shift resulted plethora comparisons among empirical learning systems domainspecific knowledge much difficult shift representations initial work kbann provided method shifting domainspecific knowledge form propositional horn clauses neural networks towell et al 1990 shift made knowledge available neural learning algorithms backpropagation result initial efforts classifiers accurate obtained using training examples towell et al 1990 noordewier et al 1991 one sense earlier results dead end refined knowledge trained networks inaccessible previous papers provide method shifting knowledge symbolic neural provide method shifting back symbolic hence paper present methods completing symbolic circle open dead end making highlyaccurate neural classifiers accessible humaninspection learning symbolically oriented systems experimental results indicate mofn method extract concise intelligible rules trained knn without serious loss accuracy fact extracted rules accurate classifying testing examples networks came moreover extracted rules show utility machine learning techniques method validating refining realworld case biological theories finally extra work required system comparison symbolic rulerefinement methods shown worthwhile method provides superior rules small cost comprehensibility hence work shows value shifting finegrained representation detailed corrections back communication corrections 9 acknowledgments work partially supported office naval research grant n0001490j1941 national science foundation grant iri9002413 department energy grant de fg0291er61129 wish thank michiel noordewier construction two biological rule data sets general comments work comments richard maclin mark craven denis kibler three anonymous reviewers also gratefully acknowledged finally thank ross quinlan providing code c45 promoter recognition splicejunction determination datasets part collection datasets ucirvine available via anonymous ftp icsuciedu directory pubmachinelearningdatabases r refinement approximate reasoningbased controllers reinforcement learning extraction semantic features logical rules multilayer neural network study thinking learning relations noisy examples empirical comparison linus foil cascadecorrelation learning architecture empirical comparison id3 back propagation rule learning searching adapted nets complexity teaching analysis e clustering algorithms compilation analysis escherichia coli promoter dna sequences neural expert system automated extraction fuzzy ifthen rules advances neural information processing systems vol connectionist learning procedures complexity loading shallow neural networks effect noncontacted bases affinity 434 operator 434 repressor cro neurofuzzy system fuzzy inference using structured neural network r1 rulebased configurer computer systems connectionist scientist game rule extraction refinement neural network magical number seven skeletonization technique trimming fat network via relevance assessment hierarchies concept attainment training knowledgebased neural networks recognize genes dna sequences simplifying neural networks soft weightsharing advances neural information processing systems vol using explanationbased empirical methods theory revision phd thesis changing rules comprehensive approach theory refinement prior knowledge hinders learning direct transfer learned information among neural networks simplifying decision trees medical diagnostic expert system based pdp model using multilayered neural networks learning symbolic knowledge world scientific symbolic neural net learning algorithms empirical comparison consensus patterns dna two problems backpropagation steepest descent learning procedures networks using background knowledge concept formation monks problem performance comparison different learning algorithms symbolic knowledge neural networks insertion interpretation artificial neural networks mapping knowledgebased neural networks rules refinement approximately correct domain theories knowledgebased neural networks computer systems learn tr ctr koji fujimoto sampei nakabayashi applying gmdh algorithm extract rules examples systems analysis modelling simulation v43 n10 p13111319 october milan zorman peter kokol bruno stiglic transforming backpropagation neural networks decision trees using nndt cascade method second international workshop intelligent systems design application p7580 august 0708 2002 atlanta georgia khosrow kaikhah sandesh doddameti discovering trends large datasets using neural networks applied intelligence v24 n1 p5160 february 2006 rudy setiono huan liu symbolic representation neural networks computer v29 n3 p7177 march 1996 petra povalej mitja leni peter kokol combining multiple specialists opinions cellular automata improve medical decision making proceedings 4th wseas international conference applied informatics communications p15 december 1719 2004 tenerife canary islands spain rudy setiono wee kheng leow fernn algorithm fast extraction rules fromneural networks applied intelligence v12 n12 p1525 januaryapril 2000 marco muselli diego liberati binary rule generation via hamming clustering ieee transactions knowledge data engineering v14 n6 p12581268 november 2002 steffen hlldobler yvonne kalinke hanspeter strr approximating semantics logic programs recurrent neural networks applied intelligence v11 n1 p4558 julyaugust 1999 gary w flake eric j glover steve lawrence c lee giles extracting query modifications nonlinear svms proceedings 11th international conference world wide web may 0711 2002 honolulu hawaii usa witold pedrycz neural networks handbook data mining knowledge discovery oxford university press inc new york ny 2002 l mangasarian set containment characterization journal global optimization v24 n4 p473480 december 2002 yaochu jin bernhard sendhoff extracting interpretable fuzzy rules rbf networks neural processing letters v17 n2 p149164 april ron sun knowledge extraction reinforcement learning new learning paradigms soft computing physicaverlag gmbh heidelberg germany 2002 rudy setiono extracting rules neural networks pruning hiddenunit splitting neural computation v9 n1 p205225 jan 1 1997 johan huysmans bart baesens jan vanthienen new approach measuring rule set consistency data knowledge engineering v63 n1 p167182 october 2007 sankar k pal sushmita mitra pabitra mitra roughfuzzy mlp modular evolution rule generation evaluation ieee transactions knowledge data engineering v15 n1 p1425 january tony plate joel bert john grace pierre band visualizing function computed feedforward neural network neural computation v12 n6 p13371353 june 2000 judy goldsmith robert h sloan theory revision queries extended abstract proceedings thirtysecond annual acm symposium theory computing p441448 may 2123 2000 portland oregon united states amit gupta sang park siuwa lam generalized analytic rule extraction feedforward neural networks ieee transactions knowledge data engineering v11 n6 p985992 november 1999 glenn fung sathyakama sandilya r bharat rao rule extraction linear support vector machines proceeding eleventh acm sigkdd international conference knowledge discovery data mining august 2124 2005 chicago illinois usa shlomo argamonengelson moshe koppel hillel walters maximizing theory accuracy selective reinterpretation machine learning v41 n2 p123152 november 2000 peter haddawy vu ha angelo restificar benjamin geisler john miyamoto preference elicitation via theory refinement journal machine learning research 4 1212003 zhihua zhou rule extraction using neural networks neural networks journal computer science technology v19 n2 p249253 march 2004 hongjun lu rudy setiono huan liu neurorule connectionist approach data mining proceedings 21th international conference large data bases p478489 september 1115 1995 robert h sloan gyrgy turn theory revision queries proceedings twelfth annual conference computational learning theory p4152 july 0709 1999 santa cruz california united states warodom geamsakul tetsuya yoshida kouzou ohara hiroshi motoda hideto yokoi katsuhiko takabayashi constructing decision tree graphstructured data applications fundamenta informaticae v66 n12 p131160 january 2005 judy goldsmith robert h sloan new horn revision algorithms journal machine learning research 6 p19191938 1212005 judy goldsmith robert h sloan gyrgy turn theory revision queries dnf formulas machine learning v47 n23 p257295 mayjune 2002 kazumi saito ryohei nakano extracting regression rules neural networks neural networks v15 n10 p12791288 december 2002 boonserm kijsirikul boonserm kijsirikul lerdlamnaochai firstorder logical neural networks international journal hybrid intelligent systems v2 n4 p253267 december 2005 anna l buczak wojciech ziarko neural rough set based data mining methods engineering handbook data mining knowledge discovery oxford university press inc new york ny 2002 artur avila garcez gerson zaverucha connectionist inductive learning logic programming system applied intelligence v11 n1 p5977 julyaugust 1999 samuel h huang hao xing extract intelligible concise fuzzy rules neural networks fuzzy sets systems v132 n2 p233243 december 2002 c tan c p lim v c rao hybrid neural network model rule generation application process fault detection diagnosis engineering applications artificial intelligence v20 n2 p203213 march 2007 volker tresp jrgen hollatz subutai ahmad representing probabilistic rules networks gaussianbasis functions machine learning v27 n2 p173200 may 1997 ioannis hatzilygeroudis jim prentzas neurosymbolic approaches knowledge representation expert systems international journal hybrid intelligent systems v1 n34 p111126 december 2004 zhihua zhou yuan jiang shifu chen extracting symbolic rules trained neural network ensembles ai communications v16 n1 p315 january zhihua zhou yuan jiang shifu chen extracting symbolic rules trained neural network ensembles ai communications v16 n1 p315 may ismail taha joydeep ghosh symbolic interpretation artificial neural networks ieee transactions knowledge data engineering v11 n3 p448463 may 1999 andreas nrnberger witold pedrycz rudolf kruse data mining tasks methods classification neural network approaches handbook data mining knowledge discovery oxford university press inc new york ny 2002 zijian zheng constructing xofn attributes decision tree learning machine learning v40 n1 p3575 july 2000 martin holea piecewiselinear neural networks relationship rule extraction data neural computation v18 n11 p28132853 november 2006 ron sun todd peterson edward merrill hybrid architecture situated learning reactive sequential decision making applied intelligence v11 n1 p109127 julyaugust 1999 henrik jacobsson rule extraction recurrent neural networks taxonomy review neural computation v17 n6 p12231263 june 2005 russell greiner christian darken n iwan santoso efficient reasoning acm computing surveys csur v33 n1 p130 march 2001