prior learning gibbs reactiondiffusion abstractthis article addresses two important themes early visual computation first presents novel theory learning universal statistics natural imagesa prior model typical cluttered scenes worldfrom set natural images second proposes general framework designing reactiondiffusion equations image processing start studying statistics natural images including scale invariant properties generic prior models learned duplicate observed statistics based minimax entropy theory studied two previous papers resulting gibbs distributions potentials form uleft schmibf ilambda rightsumnolimitsalpha 1k sumnolimits left xy right lambda left alpha rightleft left fleft alpha rightschmibf rightleft xy right right set filters potential functions learned gibbs distributions confirm improve form existing prior models lineprocess contrast previous models inverted potentials ie x decreasing function x found necessary find partial differential equations given gradient descent ui essentially reactiondiffusion equations usual energy terms produce anisotropic diffusion inverted energy terms produce reaction associated pattern formation enhancing preferred image features illustrate models used texture pattern rendering denoising image enhancement clutter removal careful choice prior data models type incorporating appropriate features b texture pattern rendering denoising image enhancement clutter removal careful choice prior data models type incorporating appropriate features song chun zhu computer science department stanford university stanford ca 94305 david mumford division applied mathematics brown university providence ri 02912 work started authors harvard university 1 introduction motivation computer vision many generic prior models proposed capture universal low level statistics natural images models presume surfaces objects smooth adjacent pixels images similar intensity values unless separated edges applied vision algorithms ranging image restoration motion analysis 3d surface reconstruction example image restoration general smoothness models expressed probability distributions 9 4 20 11 image z normalization factor r x ix r ix differential operators three typical forms potential function displayed figure 1 functions figure 1b 1c flat tails preserve edges object boundaries thus said advantages quadratic function figure 1a figure three existing forms quadratic 2 b line process prior models motivated regularization theory 26 18 1 phys 1 smoothness term explained stabilizer solving illposed problems 32 ical modeling 31 4 2 bayesian theory 9 20 robust statistics 19 13 3 connections interpretations also observed 12 13 based effective energy statistics mechanics prior models kind either generalized traditional physical models 37 chosen mathematical convenience however little rigorous theoretical empirical justification applying prior models generic images little theory guide construction selection prior models one may ask following questions 1 differential operators good choices capturing image features 2 best form pi 3 relevant fact real world scenes observed less arbitrary scales thus good prior model remain image features multiple scales however none existing prior models scaleinvariance property 2d image lattice ie renormalizable terms renormalization group theory 36 previous work modeling textures proposed new class gibbs distributions following form 40 41 e gammau 2 equation set linear filters set potential functions features extracted central property class models reproduce marginal distributions f ff estimated set training images maximum entropy best set features ff 1 f 2 f k g quadratic variational solutions minimizing potential splines flexible membrane thin plate models selected minimizing entropy pi 41 conclusion earlier papers appropriate choice small set filters random samples models duplicate general classes textures far normal human perception concerned recently found similar ideas model inference using maximum entropy also used natural language modeling1 paper want study extent probability distributions type used model generic natural images try answer three questions raised start studying statistics database 44 real world images describe experiments gibbs distributions form equation 2 constructed duplicate observed statistics learned potential functions classified two categories diffusion terms similar figure 1c reaction terms contrast previous models inverted potentials ie x decreasing function jxj find partial differential equations given gradient descent essentially reactiondiffusion equations call gibbs reaction diffusion equations grade grade diffusion components produce denoising effects similar anisotropic diffusion 25 reaction components form patterns enhance preferred image features learned prior models applied following applications first run grade starting white noise images demonstrate grade easily generate canonical texture patterns leopard blobs zebra stripe turing reactiondiffusion equations do34 38 thus theory provides new method designing pdes pattern synthesis second illustrate learned models used denoising image enhancement clutter removal careful choice prior noise models type incorporating appropriate features extracted various scales orientations computation simulates stochastic process langevin equations sampling posterior distribution paper arranged follows section 2 presents general theory prior learning section 3 demonstrates experiments statistics natural images prior learning section 4 studies reactiondiffusion equations section 5 demonstrates experiments denoising image enhancement clutter removal finally section 6 concludes discussion 2 theory prior learning 21 goal prior learning two extreme cases define image n theta n lattice l function pixel x ix 2 l l either interval r l ae z assume underlying probability distribution fi image space l n 2 general natural images arbitrary views world let ni 2 mg set observed images independent samples fi objective learning generic prior model look common features statistics observed natural images features statistics incorporated probability distribution pi estimation fi pi prior model bias vision algorithms image features typical natural images noise distortion blurring objective reasonable assume image features equal chance occur location fi translation invariant respect x discuss limits assumption section 6 study properties images fi obs start exploring set linear filters characteristic observed images statistics extracted empirical marginal distributions histograms filter responses given probability distribution fi marginal distribution fi respect f ff z z f ff lambdaixyz 8z 2 r ffi dirac function point mass concentrated 0 given linear filter f ff image empirical marginal distribution histogram filtered image f ff ix compute histogram averaged images ni obs observed statistics obs z h ff z obs make good choice database may assume ff obs z unbiased estimate f ff z 1 ff obs z converges f ff learn prior model observed images fi obs immediately two simple solutions first one obs observed average histogram image intensities ie filter used taking 1 obs z rewrite equation 4 second solution let ki obs potts model 37 two solutions stand two typical mechanisms constructing probability models literature first often used image coding 35 second special case learning scheme using radial basis functions rbf 27 3 although philosophies learning two prior models differ ent observe share two common properties 1 potentials 1 2 built responses linear filters equation 7 obs used linear filters size n theta n pixels denote f n 2 filter f ff chosen pi cases duplicates observed marginal distributions trivial prove e p h ff z obs z thus increases second property general satisfied existing prior models equation 1 pi cases meets objective prior learning intuitively good choices equation 5 ffi filter capture spatial structures larger one pixels equation 7 filters f obsn specific predict features unobserved images fact filters used lie two extremes spectrum linear filters discussed gabor 7 ffi filter localized space extended uniformly frequency contrast filters like sine waves well 3 rbf basis functions presumed smooth gaussian function using ffi loyal observed data localized frequency extended space filter f obsn includes specific combination components space frequency quantitative analysis goodness filters given table 1 section 32 22 learning prior models minimax entropy generalize two extreme examples desirable compute probability distribution duplicates observed marginal distributions arbitrary set filters linear nonlinear goal achieved minimax entropy theory studied modeling textures previous papers 40 41 given set filters ff ff observed statistics f ff obs kg maximum entropy distribution derived following gibbs form equation consider linear filters set potential functions features extracted practice image intensities discretized finite gray levels filter responses divided finite number bins therefore ff approximated piecewisely constant functions ie vector denote ff ff computed nonparametric way learned pi reproduces observed statistics therefore far selected features statistics concerned cannot distinguish pi true distribution fi unfortunately simple way express ff terms ff obs two extreme examples compute ff adopted gibbs sampler 9 simulates inhomogeneous markov chain image space l jn 2 j monte carlo method iteratively samples distribution pi followed computing histogram filter responses sample updating ff bring histograms closer observed ones detailed account computation ff readers referred 40 41 previous papers following two propositions observed proposition 1 given filter set observed statistics f ff unique solution kg proposition 2 fi determined marginal distributions thus reproduces marginal distributions linear filters computational reasons often desirable choose small set filters efficiently capture image structures given set filters distribution pi goodness pi often measured kullbackleibler information distance pi ideal distribution z z fi log fi fixed model complexity k best feature set selected following criterion chosen general filter bank b gabor filters multiple scales orientations enumerating possible sets features filter bank comparing entropies computational expensive instead 41 propose stepwise greedy procedure minimizing kldistance start uniform distribution introduce one filter time added filter chosen maximally decrease kldistance keep decrease smaller certain value experiments paper used simpler measure infor mation gain achieved adding one new filter feature set roughly l 1 distance vs l 2 measure implicit kullbackleibler distance readers referred 42 detailed account pi defined information criterion ic filter f fi 2 bs step k kh fi z obs kh fi z obs obs zk call first term average information gain aig choosing f fi second term average information fluctuation aif intuitively aig measures average error filter responses database marginal distribution current model pi practice need sample pi thus synthesize images fi syn estimate e pi h fi z fi n filter f fi bigger aig information f fi captures reports error current model observations aif measure disagreement observed images bigger aif less responses f ff common 3 experiments natural images section presents experiments learning prior models start exploring statistical properties natural images figure 6 44 collected natural images 31 statistic natural images well known natural images statistical properties distinguishing random noise images 28 6 24 experiments collected set 44 natural images six shown figure 2 images various sources digitized books postcards corel image database database includes indoor outdoor pictures country urban scenes images normalized intensity 0 31 stated proposition 2 marginal distributions linear filters alone capable characterizing fi rest paper shall study following bank b linear filters 1 intensity filter ffi 2 isotropic centersurround filters ie laplacian gaussian filters const 2oe stands scale filter denote filters lgs special filter lg 3 theta 3 window 0 denote delta 3 gabor filters sine cosine components models frequency orientation sensitive simple cells const sine wave frequency 2 modulated elongated gaussian function rotated angle denote real image parts gx gsins two special gsins filters gradients r x r 4 approximate large scale filters filters small window sizes high level image pyramid image one level blowndown version ie averaged 2 theta 2 blocks image observed three important aspects statistics natural images first features statistics natural images vary widely image image look ffi filter section 21 average intensity histogram 44 images obs plotted figure 3a figure 3b intensity histogram individual image temple image figure 2 appears obs z close uniform distribution figure 3c difference figure 3a figure 3b big thus ic filter ffi small see table 1 second many filters histograms responses amazingly consistent across 44 natural images different histograms noise images example look filter r x figure 4a average histogram 44 filtered natural images figure 4b histogram individual filtered image image figure 3b figure 4c histogram filtered uniform noise image average histogram figure 4a different gaussian distribution figure 3 intensity histograms domain 0 31 averaged 44 natural images b individual natural image c uniform noise image figure 4 histograms r x plotted domain 30 30 averaged 44 natural images b individual natural image c uniform noise image figure 5 histogram r x plotted gaussian curve dashed mean variance domain gamma15 15 b logarithm two curves see figure 5a plots gaussian curve dashed mean variance histogram natural images higher kurtosis heavier tails similar results reported 6 see difference tails figure 5b plots logarithm two curves third statistics natural images essentially scale invariant respect features example look filters r x r image obs build pyramid n image sth layer set 0 n let s1 size n n2 theta n2 22 b c figure 2 b log xs c histograms filtered uniform noise image scales curve dashed curve filter r x let xs z average histogram r x figure 6a plots xs z almost identi cal see tails clearly display log xs z figure 6c difference still small similar results observed ys z average histograms r obs n contrast figure 6b plots histograms r x uniform noise image scales 2 combining second third aspects conclude histograms r x consistent across observed natural images across scales 2 scale invariant property natural images largely caused following facts 1 natural images contains objects sizes 2 natural scenes viewed made images arbitrary distances 32 empirical prior models section learn prior models according theory proposed section 2 analyze efficiency filters quantitatively experiment b c figure 7 three learned potential functions filters delta b r x c r dashed curves fitting functions 1 c 3 start compute aif aig ic filters filter bank list results small number filters table 1 filter delta biggest ic 0642 thus chosen f 1 distribution p 1 learned information criterion filter shown column headed p 1 table 1 notice ic filter delta drops near 0 ic also drops filters filters general independent delta small filters like lg1 smaller ics others due higher correlations delta figure 8 typical sample p 3 256 theta 256 pixels big filters larger ic investigated experiment ii experi ment choose r x r f 2 f 3 prior models therefore prior model p 3 learned potential plotted figure 7 since 1 plot 1 z z 2 gamma95 95 2 z 3 z z 2 gamma22 22 three curves fitted functions synthesized image sampled p 3 displayed figure 8 far used three filters characterize statistics natural images synthesized image figure 8 still far natural ones especially even though learned potential functions ff z tails 4 fact 1 obs obs n theta n size synthesized image filter filter size aif aig ic aig ic aig ic aig ic table 1 information criterion filter selection preserve intensity breaks generate small speckles instead big regions long edges one may expect based synthesized image compute aig ic filters results list table 1 column p 3 experiment ii clear need largescale filters better rather using large scale gabor filters chose use r x r 4 different scales impose explicitly scale invariant property find natural images given image defined n theta n lattice l build pyramid way let 3 four layers pyramid let h xs z x denote histogram r x x h ys z x histogram r x ask probability model pi satisfies 3 3 l image lattice level z average observed histograms r x r 44 natural images scales results maximum entropy distribution p energy following form u x figure 3 beginning learning process xs form displayed figure 7 low values around zero encourage smoothness learning proceeds gradually x3 turns side smaller values two tails x2 x1 turn upside one one similar results observed ys 3 figure 11 typical sample image p demonstrate scale invariant properties figure 10 show histograms h xs log h xs synthesized image 3 learning process iterates 10 000 sweeps verify learned restarted homogeneous markov chain noise image using learned model found markov chain goes perceptually similar image 6000 sweeps remark 1 figure 9 notice xs inverted ie decreasing functions j z j distinguishing model prior models computer vision first image intensity finite range 0 31 r x defined gamma31 31 therefore may define xs still welldefined second inverted potentials significant meaning visual computation image restoration high intensity difference r x x present likely noise however true 3 additive noise hardly pass high layers pyramid layer 2 theta 2 averaging operator reduces variance noise 4 times r x x large likely true b 5 c figure 9 learned potential functions xs 3 dashed curves fitting functions 22 b figure histograms synthesized image 4 scalesalmost indistinguishable b logarithm histograms figure 11 typical sample p 384 theta 384 pixels edge object boundary p x0 suppresses noise first layer xs encourages sharp edges form thus enhances blurred boundaries notice regions various scales emerge figure 11 intensity contrasts also higher boundary appearances missing figure 8 remark 2 based image figure 11 computed ic aig filters list column p table 1 also compare two extreme cases discussed section 21 ffi filter aif big aig slightly bigger aif since prior models learned preference image intensity domain image intensity uniform distribution limit inside 0 31 thus first row table 1 value ic aig filter obsi ie biggest among filters aig 1 cases ics two smallest 4 gibbs reactiondiffusion equations 41 gibbs distribution reactiondiffusion equations empirical results previous section suggest forms potentials learned images real world scenes divided two classes upright curves z even function increasing jzj increases inverted curves opposite happens similar phenomenon observed learned texture models 40 figure 9 xs z fit family functions see dashed curves respectively translation scaling constants kak weights contribution filter general gibbs distribution learned images given application potential function following form oe ff ffn 1 note filter set divided two parts kg cases consists filters r x r delta capture general smoothness images r contains filters characterize prominent features class images eg gabor filters various orientations scales respond larger edges bars instead defining whole distribution u one use u set variational problem particular one attempt minimize u gradient descent leads nonlinear parabolic partial differential equation f ff ffn 1 f ff f ff gammay thus starting input image ix first term diffuses image reducing gradients second term forms patterns reaction term call equation 14 gibbs diffusion reaction equation grade since computation equation 14 involves convolving twice selected filters conventional way efficient computation build image pyramid filters large scales low frequencies scaled small ones higher level image pyramid appropriate especially filters selected bank multiple scales gabor filters wavelet transforms adopt representation experiments image let image level pyramid potential function becomes ff ff x derive diffusion equations similarly pyramidal representation 42 anisotropic diffusion gibbs reactiondiffusion section compares grades previous diffusion equations vision 25 23 anisotropic diffusion equations generating image scale spaces introduced following form div divergence operator ie div malik defined heat conductivity cx functions local gradients example x 16 equation 16 minimizes energy function continuous form z z plotted figure 12 similar forms energy functions widely used prior distributions 9 4 20 11 also equivalently interpreted sense robust statistics 13 3 x 04 figure following address three important properties gibbs reactiondiffusion equations first note equation 14 extension equation 15 discrete lattice defining vector field divergence operator thus equation 14 written compared equation 15 transfers heat among adjacent pixels equation transfers heat many directions graph conductivities defined functions local patterns local gradients second figure 13 oe round tip fl 1 cusp occurs 0 value gamma1 1 shown dotted curves figure 13d interesting fact potential function learned real world images cusp shown figure 9a best fit 07 cusp forms large part objects real world images flat intensity appearances oe produce piecewise constant regions much stronger forces fl 1 continuity oe 0 assigned value range gamma 2 gammaffl ffl numerical simulations 2 gamma take gammaoe oe 2 gamma 04 04 x figure 13 potential function ac oe summation terms differential equation whose values well defined intuitively 0 forms attractive basin neighborhood n ff x specified filter window f ff pixel u v 2 n ff x depth attractive basin kf ff pixel involved multiple zero filter responses accumulate depth attractive basin generated filter thus unless absolute value driving force welldefined terms larger total depth attractive basin u v iu v stay unchanged image restoration experiments later sections performance forming piecewise constant regions third learned potential functions confirmed improved existing prior models diffusion equations interestingly reaction terms first dis covered produce patterns enhance preferred features demonstrate property experiments 43 gibbs reactiondiffusion pattern formation literature many nonlinear pdes pattern formation following two examples interesting turing reactiondiffusion equation models chemical mechanism animal coats 33 21 two canonical patterns turing equations synthesize leopard blobs zebra stripes 34 38 equations also applied image processing image halftoning 29 theoretical analysis found 15 ii swindale equation simulates development ocular dominance stripes visual cortex cats monkey 30 simulated patterns similar zebra stripes section show patterns easily generated 2 3 filters using grade run equation 14 starting ix uniform noise image grade converges local minimum synthesized texture patterns displayed figure 14 six patterns figure 14 choose f 1 laplacian gaussian filter level 0 image pyramid diffusion filter fix three patterns figure 14 abc choose isotropic centersurround filter lg widow size 7 theta 7 pixels reaction filter f 2 1 level 1 image pyramid set gamma60 differences three patterns caused forms patterns symmetric appearances black white part shown figure 14a negative black blobs begin form shown figure 14b positive blobs black background shown figure 14c 6 general smoothness appearance images attributed diffusion filter figure 14d generated two reaction filters lg 2 level 1 level 2 respectively b c figure 14 leopard blobs zebra stripes synthesized grades therefore grade creates blobs mixed sizes similarly selected one cosine gabor filter gcos4 pixels oriented 1 reaction filter f 2 figure 14f generated two reaction filters gcos4 seems leopard blobs zebra stripes among canonical patterns generated easy choices filters parameters shown 40 gibbs distribution capable modeling large variety texture patterns filters different forms learned given texture pattern 5 image enhancement clutter removal far studied use single energy function ui either log likelihood probability distribution function minimized gradient descent image processing often need model underlying images distortions maximize posterior distribution suppose distortions additive ie input image c many applications distortion images c often iid gaussian noise clutter structures trees front building military target clutter hard handle edge detection image segmentation algorithms propose model clutter extra gibbs distribution learned training images minimax entropy theory underlying image thus extra pyramidal representation gamma needed gibbs distribution form shown figure 15 resulting posterior distributions still gibbs form potential function u uc potential clutter distribution thus map estimate minimum u experiments use langevin equation minimization variant simulated annealing wx standard brownian motion process ie wx temperature controls magnitude random fluctuation mild conditions u equation 19 approaches global minimum u target features image pyramid clutters image pyramid targets clutter features observed image target image clutter image figure 15 computational scheme removing noise clutter low temperature analyses convergence equations found 14 10 8 computational load annealing process notorious applications like denoising fast decrease temperature may affect final result much experiment first experiment take uc quadratic ie c iid gaussian noise image first compare performance three prior models potential functions respectively u l 4scale energy equation 12 22 l lineprocess tfunction displayed figure 1b 1c respectively figure demonstrates results original image lobster boat displayed figure 2 normalized intensity 0 31 gaussian noise n0 25 added distorted image displayed figure 16a keep image boundary noisefree convenience boundary condition restored images using p l p p shown figure 16b 16c 16d respectively p model reaction term appears best effect recovering boat especially top boat also enhances water experiment ii many applications iid gaussian models distortions sufficient example figure 17a tree branches foreground make image segmentation object recognition extremely difficult cause strong edges across image modeling clutter challenging problem many applications paper consider clutter two dimensional pattern despite geometry 3d structure collected set images buildings set images trees clean background sky tree images translate image intensities sky case since trees always darker build thus negative intensity approximately take care occlusion effects learn gibbs distributions set respectively pyramid models respectively adopted prior distribution likelihood equation 18 recovered underlying images maximizing posteriori distribution using stochastic process example figure 17b computed using 6 filters 2 filters fr 4 filters c ie potential c equation oe fit potential functions learned set tree images c figure noise distorted image b c respectively restored images prior models p l figure 17 observed image b restored image using 6 filters energy term oe ix forces zero intensity clutter image allowing large negative intensities dark tree branches figure 18b computed using 8 filters 4 filters 4 filters c 13 filters used figure 19 clutter much heavier comparison run anisotropic diffusion process 25 figure 19a images iterations displayed figure 20 see becomes flat image robust anisotropic diffusion equation recently reported 2 6 conclusion paper studied statistics natural images based novel theory proposed learning generic prior model universal statistics real world scenes argue strategy developed paper used applications example learning probability models mri figure observed image b restored image using 8 filters b figure 19 observed image b restored image using 13 filters figure 20 images anisotropic diffusion iteration images 3d depth maps learned prior models demonstrate important properties inverted potentials terms patterns formation image enhancement expressive power learned gibbs distributions allow us model structured noiseclutter natural scenes furthermore prior learning method provides novel framework designing reactiondiffusion equations based observed images given application without modeling physical chemical processes people 33 although synthesized images bear important features natural images still far realistic ones words generic prior models little beyond image restoration mainly due fact generic prior models assumed translation invariant homogeneity assumption unrealistic call generic prior models studied paper first level prior sophisticated prior model incorporate concepts like object geometry call prior models second level priors diffusion equations derived second level priors studied image segmentation 39 scale space shapes 16 discussion typical diffusion equations given 22 hope article stimulate investigations building realistic prior models well sophisticated pdes visual computation r maximum entropy approach natural language processing robust anisotropic diffusion unification line processes outlier jection robust statistics applications early vision visual reconstruction relations statistics natural images response properties cortical cells theory communication sampling methods annealing algorithms stochastic relaxation gibbs distributions bayesian restoration images diffusion global optimization constrained restoration recover discontinu ities parallel deterministic algorithms mrfs surface reconstruction common framework image segmentation renormalization group approach image processing problems theory applications reactiondiffusion equations shapes shocks deformations components twodimensional shape reactiondiffusion space information sufficiency probabilistic solution illposed problems computational vision robust regression methods computer vision review optimal approximations piecewise smooth functions associated variational problems prepattern formation mechanism mammalian coat markings general framework geometrydriven evolution equations nonlinear image filtering edge corner enhance ment natural image statistics efficient coding scalespace edge detection using anisotropic diffusion computational vision regularization theory networks approximation learning statistics natural images scaling woods mlattice morphogenesis image processing model formation ocular dominance stripes multilevel computational processes visual surface reconstruc tion solutions illposed problems chemical basis morphogenesis generating textures arbitrary surfaces using reactiondiffusion efficiency model human image code renormalization group critical phenonmena knodo prob lem image analysis reactiondiffusion textures region competition unifying snakes region grow ing bayesmdl multiband image segmentation filters random fields minimax entropy towards unified theory texture modeling minimax entropy principle application texture modeling learning generic prior models visual computa tion tr ctr katy streso francesco lagona hidden markov random field frame modelling tca image analysis proceedings 24th iasted international conference signal processing pattern recognition applications p310315 february 1517 2006 innsbruck austria ulf grenander anuj srivastava probability models clutter natural images ieee transactions pattern analysis machine intelligence v23 n4 p424429 april 2001 yufang bao hamid krim smart nonlinear diffusion probabilistic approach ieee transactions pattern analysis machine intelligence v26 n1 p6372 january 2004 dmitry datsenko michael elad examplebased single document image superresolution global map approach outlier rejection multidimensional systems signal processing v18 n23 p103121 september 2007 giuseppe boccignone mario ferraro terry caelli generalized spatiochromatic diffusion ieee transactions pattern analysis machine intelligence v24 n10 p12981309 october 2002 thang v pham arnold w smeulders object recognition uncertain geometry uncertain part detection computer vision image understanding v99 n2 p241258 august 2005 songchun zhu stochastic jumpdiffusion process computing medial axes markov random fields ieee transactions pattern analysis machine intelligence v21 n11 p11581169 november 1999 alan l yuille james coughlan fundamental limits bayesian inference order parameters phase transitions road tracking ieee transactions pattern analysis machine intelligence v22 n2 p160173 february 2000 ying nian wu song chun zhu xiuwen liu equivalence julesz ensembles frame models international journal computer vision v38 n3 p247265 julyaugust 2000 marc sigelle cumulant expansion technique simultaneous markov random field image restoration hyperparameter estimation international journal computer vision v37 n3 p275293 june 2000 songchun zhu embedding gestalt laws markov random fields ieee transactions pattern analysis machine intelligence v21 n11 p11701187 november 1999 ann b lee david mumford jinggang huang occlusion models natural images statistical study scaleinvariant dead leaves model international journal computer vision v41 n12 p3559 januaryfebruary 2001 daniel cremers florian tischhuser joachim weickert christoph schnrr diffusion snakes introducing statistical shape knowledge mumfordshah functional international journal computer vision v50 n3 p295313 december 2002 song chun zhu xiu wen liu ying nian wu exploring texture ensembles efficient markov chain monte carlotoward trichromacy theory texture ieee transactions pattern analysis machine intelligence v22 n6 p554569 june 2000 scott konishi alan l yuille james coughlan song chun zhu statistical edge detection learning evaluating edge cues ieee transactions pattern analysis machine intelligence v25 n1 p5774 january j sullivan blake isard j maccormick bayesian object localisation images international journal computer vision v44 n2 p111135 september 2001 norberto grzywacz rosario balboa bayesian framework sensory adaptation neural computation v14 n3 p543559 march 2002 hedvig sidenbladh michael j black learning statistics people images video international journal computer vision v54 n13 p181207 augustseptember matthias heiler christoph schnrr natural image statistics natural image segmentation international journal computer vision v63 n1 p519 june 2005 kwang kim matthias franz bernhard scholkopf iterative kernel principal component analysis image modeling ieee transactions pattern analysis machine intelligence v27 n9 p13511366 september 2005 freeman egon c pasztor owen carmichael learning lowlevel vision international journal computer vision v40 n1 p2547 oct 2000 stefan roth michael j black spatial statistics optical flow international journal computer vision v74 n1 p3350 august 2007 charles kervrann mark hoebeke alain trubuil isophotes selection reactiondiffusion model object boundaries estimation international journal computer vision v50 n1 p6394 october 2002 jens keuchel christoph schnrr christian schellewald daniel cremers binary partitioning perceptual grouping restoration semidefinite programming ieee transactions pattern analysis machine intelligence v25 n11 p13641379 november rosario balboa norberto grzywacz minimal localasperity hypothesis early retinal lateral inhibition neural computation v12 n7 p14851517 july 2000 srivastava b lee e p simoncelli sc zhu advances statistical modeling natural images journal mathematical imaging vision v18 n1 p1733 january