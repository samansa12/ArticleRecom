computationally efficient feasible sequential quadratic programming algorithm sequential quadratic programming sqp algorithm generating feasible iterates described analyzed distinguishes algorithm previous feasible sqp algorithms proposed various authors reduction amount computation required generate new iterate proposed scheme still enjoys global fast local convergence properties preliminary implementation tested promising numerical results reported b introduction consider inequalityconstrained nonlinear programming problem min fx st continuously differentiable sequential quadratic programming sqp algorithms widely acknowledged among successful algorithms available solving p excellent recent survey sqp algorithms theory behind see 2 denote feasible set p 17 8 14 15 1 variations standard sqp iteration solving proposed generate iterates lying within x methods sometimes referred feasible sqp fsqp algorithms observed requiring feasible iterates algorithmic applicationoriented advantages algorithmically feasible iterates desirable ffl qp subproblems always consistent ie feasible solution always exists ffl objective function may used directly merit function line search engineering context feasible iterates important ffl often fx undefined outside feasible region x ffl tradeoffs design alternatives requiring hard constraints satisfied may meaningfully explored ffl optimization process may stopped iterations yielding feasible point last feature critical realtime applications feasible point may required algorithm time converge solution important function associated problem p lagrangian defined given feasible estimate x solution p symmetric matrix h approximates hessian lagrangian lx vector nonnegative lagrange multiplier estimates standard sqp search direction denoted 0 x h 0 short solves quadratic program qp st positive definiteness h often assumes ensures existence uniqueness solution appropriate merit function line search pro cedure hessian approximation rule necessary maratos effect 13 avoidance scheme sqp iteration known globally locally superlinearly convergent see eg 2 feasible direction point x 2 x defined vector r n x td belongs x 0 positive note sqp direction 0 direction descent f may feasible direction x though worst tangent active constraint surface thus order generate feasible iterates sqp framework necessary tilt 0 feasible set number approaches considered literature generating feasible directions specifically tilting sqp direction early feasible direction algorithms see eg 27 17 firstorder methods ie first derivatives used attempt made accumulate use secondorder information furthermore search directions often computed via linear programs instead qps conse quence algorithms converged linearly best polak proposed several extensions algorithms see 17 section 44 took secondorder information account computing search direction search directions proposed polak could viewed tilted sqp directions proper choice matrices encapsulating secondorder information defining equations even secondorder information though possible guarantee superlinear convergence algorithms mechanism included controlling amount tilting straightforward way tilt sqp direction course perturb righthand side constraints qp 0 x h building obser vation herskovits carvalho 8 panier tits 14 independently developed similar feasible sqp algorithms size perturbation function norm 0 x h current feasible point x thus algorithms required solution qp 0 x h order define perturbed qp algorithms shown superlinearly convergent hand byproduct tilting scheme global convergence proved elusive fact algorithm 8 globally convergent algorithm 14 resort firstorder search direction far solution order guarantee global convergence hybrid scheme could give slow convergence poor initial point chosen algorithm developed panier tits 15 analyzed weaker assumptions qi wei 20 enjoyed great deal success practice implemented ffsqpcfsqp 26 12 software packages refer algorithm throughout paper fsqp 15 instead directly perturbing qp 0 x h tilting accomplished replacing 0 convex combination essentially arbitrary feasible descent direction preserve local convergence properties sqp iteration ae selected function aed 0 0 way approaches 0 fast enough particular solution approached finally order avoid maratos effect guarantee superlinear rate convergence second order correction c denoted 15 used bend search direction armijotype search performed along arc xtdt 2 c tilted direction 15 directions 1 c computed via qps pointed c could instead taken solution linear least squares problem without affecting asymptotic convergence properties point view computational cost main drawback algorithm fsqp need solve three qps two qps linear least squares problem iteration clearly many problems would desirable reduce number qps iteration preserving generation feasible iterates well global local convergence properties especially critical context largescale nonlinear programs time spent solving qps dominates used evaluate functions goal mind consider following perturbation qp 0 x h given point x x symmetric positive definite matrix h nonnegative scalar j let dx h j flx h j solve qp st fl additional scalar variable idea away kkt points p flx h j negative thus dx h j descent direction f due first constraint well j strictly positive feasible direction due constraints note j set one search direction special case computed polaks secondorder feasible direction algorithms see section 44 book 17 difficult show j set zero recover sqp direc tion ie dx h values parameter j call tilting parameter emphasize feasibility small values j emphasize descent 1 birge qi wei propose feasible sqp algorithm based qp x h j motivation introducing righthandside constraint perturbation tilting parameters use vector parameters one constraint like obtain feasible search direction specifically motivated high cost function evaluations application problems targeting goal ensure full step one accepted line search early possible costly line searches avoided iterations end tilting parameters start positive anything increase step one accepted sideeffect updating scheme algorithm cannot achieve superlinear rate convergence authors point remark 51 1 present paper goal compute feasible descent direction approaches true sqp direction fast enough ensure superlinear convergence furthermore would like little computation per iteration possible computationally rather expen sive algorithm fsqp 15 convergence properties practical performance seek thus start reviewing key features x x define index set active constraints x fsqp order linesearch objective function f used directly merit function welldefined order preserve global fast local convergence sequence search directions fd k g generated algorithm fsqp constructed following properties hold kkt point p kkt point kkt point show section 3 given symmetric positive definite matrix h k nonnegative scalar automatically satisfies p1 p2 furthermore satisfies p3 j k strictly positive ensuring p4 holds requires bit care algorithm proposed paper iteration k search direction computed via solving qp tilting parameter j k iteratively adjusted ensure four properties satisfied resultant algorithm shown locally superlinearly convergent globally convergent without resorting firstorder direction far solution generation new iterate requires solution one qp two closely related linear least squares problems contrast algorithm presented 1 tilting parameter starts positive asymptotically approaches zero recently great deal interest interior point algorithms nonconvex nonlinear programming see eg 5 6 24 4 16 23 algorithms generate feasible iterates typically require solution linear systems equations order generate new iterates sqptype algorithms however often advantage methods context applications number variables large evaluations objectivesconstraint functions gradients highly timeconsuming indeed algorithms use quadratic programs successive models away solution progress expensive function evaluations often significantly better achieved algorithms making use mere linear systems equations models section 2 present details new fsqp algorithm section 3 show mild assumptions iteration globally convergent well locally superlinearly convergent algorithm implemented tested show section 4 numerical results quite promising related issues discussed section 5 finally section 6 offer concluding remarks discuss extensions algorithm currently explored algorithm begin making assumptions force throughout assumption 1 set x nonempty assumption 2 functions f continuously differentiable assumption 3 x 2 x ix 6 set frg j linearly independent point x 2 r n said karushkuhntucker kkt point problem p exist scalars kkt multipliers j 1 well known assumptions necessary condition optimality point x 2 x kkt point note x 2 x qp x h j always consistent 0 constraints indeed qp x h j always unique solution fl see convexity unique kkt point ie exist multipliers j together fl satisfy gammaj 2 simple consequence first equation 2 used throughout analysis affine relationship amongst multipliers namely parameter j assigned new value iteration j k iteration k ensure dx k necessary properties strict positivity j k sufficient guarantee properties p1 p3 satisfied turns however enough ensure away solution adequate tilting feasible set force j k bounded away zero away kkt points p finally p4 requires j k tend zero sufficiently fast 0 tends zero ie solution approached 14 similar effect achieved first computing course want avoid given estimate e k active set ix k compute estimate k 0 solving equality constrained qp st equivalent change variables solving linear least squares problem let k set active constraints including objective descent constraint hrfx k k show section 3 e sufficiently large furthermore prove k small choosing sufficient guarantee global local superlinear convergence proper choice proportionality constant c k algorithm statement important convergence analysis critical satisfactory numerical performance discussed section 4 15 given x h feasible descent direction maratos correction c denoted 15 taken solution qp st exists norm less minfkdk cg given scalar satisfying 2 3 c given large scalar otherwise c set zero indeed large c meangingless may jeopardize global convergence section 1 mentioned linear least squares problem could used instead qp compute version maratos correction c asymptotic convergence properties given goal reduce computational cost per iteration makes sense use approach thus iteration k take correction c k solution c exists large specifically norm larger k equalityconstrained qp equivalent least squares problem change variables st direct extension alternative considered 14 making use best available metric objective compared pure least squares objective kd c k 2 yield somewhat better iterate without significantly increasing computational requirements affecting convergence analysis another advantage using metric h k asymptotically matrix underlying ls c underlying ls e resulting computational sav ings case ls c inconsistent computed solution c k large simply set c k zero proposed algorithm follows parameters ff fi used armijolike search bending exponent ls c ffl c c used update rule j k algorithm fsqp 0 parameters positive definite computation search arc compute k active set k associated multipliers ii compute c exists satisfies kd c k otherwise set c first value sequence satisfies updates set x k1 x k ii compute h k1 new symmetric positive definite estimate hessian lagrangian iii select c k1 2 c c unique solution unique associated multipiers compute e associated multipliers case e else set j k1 c k1 iv set k 3 convergence analysis much analysis especially local analysis devoted establishing relationship dx h j sqp direction 0 x h given x x h symmetric positive definite 0 kkt point solution 0 x h exists multiplier vector 0 given ae mg estimate e kkt point ls e x h thus unique solution e x h exists multiplier vector e note components e j 62 play role optimality conditions 31 global convergence section establish mild assumptions fsqp 0 generates sequence iterates fx k g property accumulation points kkt points p begin establishing properties tilted sqp search direction dx h j lemma 1 suppose assumptions 1 3 hold given h symmetric positive definite x 2 x j 0 dx h j welldefined unique kkt point qp x h j dx h j bounded compact subsets x theta p theta r p set symmetric positive definite n theta n matrices r set nonnegative real numbers proof first note feasible set qp x h j nonempty since consider cases separately 2 4 clear solution solution qp 0 well known assumptions 0 x h welldefined unique continuous claims follow suppose j 0 case fl solution qp x h j solves unconstrained problem ae oe ae oe since function minimized 6 strictly convex radially unbounded follows dx h j flx h j welldefined unique global minimizer convex problem qp x h j thus unique kkt point problem boundedness dx h subsets x theta p theta r follows first equation 2 regularity assumptions 3 shows since j 0 multipliers bounded lemma 2 suppose assumptions 1 3 hold given h symmetric positive definite j 0 flx h ii dx h x kkt point p moreover either thus conditions holds multipliers qp x h related proof prove note since qp x h j optimal value qp nonpositive since h 0 quadratic term objective nonnegative implies suppose dx h feasibility first qp constraint implies flx h finally suppose flx h clear achieves minimum value objective thus uniqueness gives suppose dx h 2 exist multiplier vector scalar multiplier 0 begin showing 0 proceeding contradiction suppose 3 note thus complementary slackness condition 2 optimality conditions 7 assumption 3 sum vanishes contradicting 8 thus 0 immediate x kkt point p multipliers finally prove necessity portion part ii note x kkt point p 1 shows kkt point uniqueness points lemma 1 yields result next two lemmas establish line search step 2 algorithm fsqp 0 well defined lemma 3 suppose assumptions 1 3 hold suppose x 2 x kkt point p h symmetric positive definite j 0 ii proof follow immediately lemma 2 fact dx h flx h must satisfy constraints qp x h j lemma 4 suppose assumptions 1 3 hold kkt point p algorithm stop step 1i iteration k hand whenever algorithm stop step 1i line search well defined ie step 2 yields step k equal fi j k proof suppose j latter case cannot hold stopping criterion step 1i would stopped algorithm iteration k gamma 1 hand view optimality conditions 5 fact x k always feasible p see x k kkt point p multipliers 0 otherwise thus lemma 2 algorithm stop step 1i first claim thus proved also established step 2 reached second claim follows immediately lemma 3 assumption 2 previous lemmas imply algorithm welldefined addi shows algorithm fsqp 0 generates finite sequence terminating point xn xn kkt point problem p concentrate case infinite sequence fx k g gen erated ie algorithm never satisfies termination condition step 1i note view lemma 4 may assume throughout proceeding make assumption concerning estimates h k hessian lagrangian assumption 4 exist positive constants oe 1 oe 2 k lemma 5 suppose assumptions 1 4 hold sequence fj k g generated algorithm fsqp 0 bounded sequence fd k g bounded subsequences fx k g bounded proof first claim follows update rule step 3iii algorithm second claim follows lemma 1 assumption 4 given infinite index set k use notation gamma x mean lemma 6 suppose assumptions 1 3 hold suppose k infinite index set x k bounded k k gamma 0 sufficiently large qp multiplier sequences bounded k given accumulation point unique solution qp proof view assumption 2 frfx k g k2k must bounded lemma 2i first constraint qp thus gamma 0 prove first claim let j 0 62 ix exists g j 0 sufficiently large view assumption 2 since k gamma 0 fl k gamma 0 fj k g bounded k clear sufficiently large proving first claim boundedness f k g k2k follows nonnegativity 3 prove f k g k2k using complementary slackness first equation 2 write proceeding contradiction suppose f k g k2k unbounded without loss generality assume k k k 1 0 k 2 k define note k 2 k k k k dividing 10 k k k 1 taking limits appropriate subsequence k follows assumptions 2 4 boundedness f k g j 1 contradicts assumption 3 established f k g k2k bounded complete proof let k 0 k infinite index set gamma j assume without loss generality h k k gamma taking limits optimality conditions 2 shows indeed finally uniqueness points lemma 1 proves result lemma 7 suppose assumptions 1 4 hold k infinite index set k gamma 0 accumulation points fx k g k2k kkt points p proof suppose k 0 k infinite index set x k gamma x 2 x view assumption 4 lemma 5 assume without loss generality h k gamma h positive definite matrix j k view lemma 6 0 0 unique solution qp follows lemma 2 x kkt point p state prove main result subsection theorem 1 assumptions 1 4 algorithm fsqp 0 generates sequence fx k g accumulation points kkt points p proof suppose k infinite index set x k gamma x view lemma 5 assumption 4 may assume without loss generality gamma j k considered separately suppose first j exists infinite index set k 0 k either e gamma 0 latter case holds clear x gamma x since gamma 0 thus lemma 7 x kkt point suppose instead e second set equations 5 one easily see sufficiently large using argument similar used lemma 6 one show f e k g k2k 0 bounded sequence thus taking limits 5 appropriate subsequence k 0 shows x kkt point p consider case j 0 show k gamma 0 proceeding contradiction without loss generality suppose exists 0 kd k k k 2 k nonpositivity optimal value objective function qp assumption 4 see view 9 since j 0 exists j 0 constraints qp using assumption 2 easily shown exists k 2 k k large enough rest contradiction argument establishing k exactly proof proposition 32 14 finally follows lemma 7 x kkt point p 32 local convergence details often quite different overall analysis section inspired occasionally follows panier tits 14 15 key result proposition 1 states appropriate assumptions arc search eventually accepts full step one fact established along way titled direction k approaches standard sqp direction sufficiently fast superlinear convergence follows classical analysis mjd powells first step strengthen regularity assumptions assumption three times continuously differentiable point x said satisfy second order sufficiency conditions strict complementary slackness p exists multiplier vector ffl pair x satisfies 1 ie x kkt point p positive definite subspace ffl j 0 j 2 ix strict complementary slackness order guarantee entire sequence fx k g converges kkt point x make following assumption recall already established weaker assumptions every accumulation point kkt point p assumption 5 sequence fx k g accumulation point x satisfies second order sufficiency conditions strict complementary slackness well known assumption 5 guarantees entire sequence converges proof see eg proposition 41 14 lemma 8 suppose assumptions 1 2 3 5 hold sequence generated algorithm fsqp 0 converges point x satisfying second order sufficiency conditions strict complementary slackness point forward denote unique multiplier vector associated kkt point x p readily checked symmetric positive definite h 0 kkt pair qp 0 announced first main step show sequence tilted sqp directions approaches true sqp direction sufficiently fast achieved lemmas 9 18 order define 0 k equal 0 computed algorithm fsqp 0 k define 0 k multiplier vector 0 4 let 0 g following lemma proved 15 reference 14 identical assumptions lemma 9 suppose assumptions 1 2 3 5 hold iii k sufficiently large following two equalities hold 0 next establish entire tilted sqp direction sequence converges 0 order establish dx h j continuous neighborhood positive definite complicating analysis fact yet establish sequence fj k g fact converge given j 0 define set ae rfx gammaj oe lemma 10 suppose assumptions 1 2 3 5 hold given j 0 set n j linearly independent proof let h symmetric positive definite note view lemma 2 suppose claim hold ie suppose exists scalars j j 2 f0g ix zero gammaj 0 11 view assumption 3 0 6 0 scalars j unique modulo scaling factor uniqueness fact dx first scalar equations optimality conditions 2 imply kkt multipliers qp thus view 3 0 contradicts 11 gives hence n j linearly independent lemma 11 suppose assumptions 1 2 3 5 hold let j 0 accumulation point fj k g given symmetric positive definite unique solution qp x h j second order sufficiency conditions hold strict complementary slackness proof view lemma 2 qp unique solution define lagrangian function l r n theta r theta r theta r r suppose kkt multipliers 2 holds let index first constraint qp x h j ie hrfx di fl note since fl active constraint index set qp note define including 0 k defined subset thus set active constraint gradients qp n j consider hessian lagrangian qp x h j ie second derivative respect first two variables fl given arbitrary h 2 r n1 decompose clearly h 6 0 h r 2 l0 0 hy zero ff 6 0 since h ff follows r 2 l0 0 positive definite n j tangent space active constraints qp x h j 0 0 thus established second order sufficiency conditions hold finally follows lemma 2ii together assumption 5 implies strict complementarity qp 0 0 lemma 12 suppose assumptions 1 2 3 5 hold k subsequence fj k g converges say k finally proof first proceed contradiction show first two claims hold addition gamma 0 0 12 ie suppose infinite index set k 0 k either k bounded away k bounded away zero view assumption 4 loss generality assuming h k gamma h symmetric positive definite h view lemmas 10 11 may thus invoke result due robinson theorem 21 21 conclude view lemma 2ii gamma 0 0 k contradiction hence first two claims hold 12 next proceeding contradiction suppose k 6 0 since fh k g fj k g bounded exists infinite index set k fh k g fj k g converge k bounded away zero contradicts 12 thus immediately follows first constraint qp lemma 13 suppose assumptions 1 2 3 5 hold k sufficiently large proof since bounded view lemma 12 k lemma 6 implies k ix k sufficiently large suppose hold sufficiently large thus exists infinite index set k j 0 62 k k 2 k view lemma 5 exists infinite index set k 0 k j 0 j k lemma 12 shows j 0 k sufficiently large k 2 k 0 complementary slackness implies contradiction claim proved define given vector define notation note view lemma 9iii k large enough optimality conditions 4 yield r following wellknown result used lemma 14 suppose assumptions 1 2 3 5 hold r invertible k large enough inverse remains bounded k 1 lemma 15 suppose assumptions 1 2 3 5 hold k sufficiently large e uniquely defined e k proof view lemma 13 optimality conditions 5 lemma 14 k large enough estimate e k corresponding multiplier vector well defined unique solution r claim follows 13 lemma 16 suppose assumptions 1 2 3 5 hold iii k sufficiently large proof claim follows step 3iii algorithm fsqp 0 since view lemma 12 lemma 15 lemma 9 fd k g fd e converge 0 view lemma 12 establishes ii proved finally claim iii follows claim ii lemma 13 assumption 5 focus attention establishing relationships k c true sqp direction 0 k lemma 17 suppose assumptions 1 2 3 5 hold proof view lemma 15 k sufficiently large e k exist uniquely defined e k lemmas 12 9 ensure step 3iii algorithm fsqp 0 chooses sufficiently large thus follows clear lemma 13 optimality conditions 2 k k satisfy r k sufficiently large 1 jix j vector jix j ones thus follows 13 assumption 2 lemmas 12 14 16 view claim claim ii follows finally since qp constraint lemma clear okd k lemma 18 suppose assumptions 1 2 3 5 hold c okd 0 proof let expanding see j 2 0 1 z gammag assumption 2 conclude c okd 0 sufficiently large view lemma 13 c k welldefined satisfies thus r first order kkt conditions ls c us exists multiplier c k 2 r jix j r also optimality conditions 15 view lemma 17 q k c r c equivalently 0 r result follows lemma 14 order prove key result full step one eventually accepted line search assume matrices fh k g suitably approximate hessian lagrangian solution define projection r assumption lim 0 following technical lemma used lemma 19 suppose assumptions 1 2 3 5 hold exist constants ii k sufficiently large sufficiently large proof show part note view first qp constraint negativity optimal value qp objective assumption 4 proof part ii identical lemma 44 14 show iii note 15 k sufficiently large k satisfies r thus write r result follows assumption 3 lemma 17iiii proposition 1 suppose assumptions 1 2 3 6 hold sufficiently large proof following 14 consider expansion g j delta x k ix k sufficiently large used assumption 2 lemmas 17 18 boundedness sequences 16 3 follows g j k sufficiently large result trivially holds j 62 ix k large enough full step one satisfies feasibility condition arc search test remains show sufficient decrease condition satisfied well first view assumption 2 0 lemmas 17 18 top equation optimality conditions 2 equation 3 lemma 17i boundedness sequences obtain last line 2 lemma 17iiii yield taking inner product 19 k adding subtracting quantity using 20 finally multiplying result 1gives2 hrfx k k lemmas 17 combining 18 21 22 using fact k large enough hand arguments identical used following equation 49 14 show k sufficiently large thus sufficient decrease condition satisfied consequence lemmas 17 18 proposition 1 algorithm generates convergent sequence iterates satisfying twostep superlinear convergence follows theorem 2 suppose assumptions 1 2 3 6 hold algorithm generates sequence fx k g converges 2step superlinearly x ie lim 0 proof given follows step step minor modifications 18 sections 23 finally note qsuperlinear convergence would follows assumption 6 replaced stronger assumption lim 0 see eg 2 4 implementation numerical results implementation fsqp 0 c differs number ways algorithm stated section 2 readily checked none differences significantly affect convergence analysis section 3 like existing c implementation fsqp cfsqp see 12 distinctive character linear affine constraints simple bounds exploited provided nature constraints made explicit thus general form problem description tackled implementation min fx st componentwise details implementation spelled many including update rule h k exactly cfsqp implementation qp tilting effected connection linear constraints simple bound since clearly un tilted sqp direction feasible constraints addition nonlinear constraint assigned tilting parameter j j thus qp replaced k updated independently based independently adjusted c j algorithm description analysis required remain bounded bounded away zero practice though performance algorithm critically dependent upon choice c k implementation adaptive scheme chosen new values c j selected step 3 based previous values c j outcome arc search step 2 preselected parameter full step one accepted left unchanged ii step one accepted even though trial points feasible j c j k decreased minfffi c c j iii infeasibility encountered arc search j g j caused step reduction trial point c j k increased k kept constant g j said cause step reduction trial point x g j violated ie g j x constraints checked x g j found satisfied point see order constraints checked arc search stressed section 2 maratos correction computed using inequalityconstrained qp qp c instead ls c done numerical experiments order meaningfully compare new algorithm cfsqp inequalityconstrained qp indeed used implementation qp c ls e involves index sets almost active constraints binding constraints first define n machine precision next binding sets defined bn bl mn qp multiplier corresponding nonlinear constraints qp multipliers corresponding affine constraints upper bounds lower bounds respectively course bending required c k connection affine constraints simple bounds hence n simply set c otherwise following modification qp c used st since simple bounds included computation c k possible x k k satisfy bounds take care simply clip c k bounds satisfied specifically upper bounds perform following j 62 bu cj cj procedure mutatis mutandis executed lower bounds note procedure effect convergence analysis section 3 since locally active set correctly identified full step along k always accepted least squares problem ls e used compute e k modified similarly specifically implementation e k computed n 0 case use st implementation arc search step 2 cfsqp specif ically feasibility checked sufficient decrease testing trial point aborted soon infeasibility detected like cfsqp linear bound constraints checked first nonlinear constraints order maintained follows start arc search given iterate x k order reset natural numerical order ii within arc search constraint found violated trial point index moved beginning list order others left unchanged aspect algorithm intentionally left vague sections 2 3 updating scheme hessian estimates h k implementation use bfgs update powells modification 19 specifically define attempt better approximate true multipliers k normalize follows scalar k1 2 0 1 defined 08 rank two hessian update note clear whether resultant sequence fh k g fact satisfy assumption 6 update scheme known perform well practice qps linear least squares subproblems solved using qpopt 7 comparison sake qpopt also used solve qp subproblems cfsqp default qp solver cfsqp public domain code qld see 22 opted qpopt allows warm starts thus fairer cfsqp comparison implementation qps solved former alls qps codes active set solution given iteration used initial guess active set qp next iteration order guarantee algorithm terminates finite number iterations approximate solution stopping criterion step 1 changed small finally following parameter values selected always set h experiments run sun microsystems ultra 5 workstation first set numerical tests selected number problems 9 provided feasible initial points contained equality con straints results reported table 1 performance implementation fsqp 0 compared cfsqp qpopt qp solver column labeled lists problem number given 9 column labeled algo selfexplanatory next three columns give size problem following conventions section columns labeled nf ng give number objective function eval uations nonlinear constraint function evaluations iterations required solve problem respectively finally fx objective function value final iterate ffl value ffl chosen order obtain approximately precision reported 9 problem results reported table 1 encouraging performance implementation algorithm fsqp 0 terms number iterations function evaluations essentially identical cfsqp algorithm fsqp expected payoff using fsqp 0 instead fsqp however large problems cpu time expended linear algebra specifically solving qp linear least squares subproblems much less assess carried comparative tests cops suite problems 3 first five problems cops set 3 considered problems either involve nonlinear equality constraints readily reformulated without constraints specifically problem sphere equality constraint changed constraint chain equality constraint replaced two inequalities lefthand side constrained values solution always 5 sawpath discarded involves variables many constraints situation new algorithm targeted results obtained various instances four problems presented table 2 format table identical table 1 except additional column labeled nqp column list total number qp iterations solution two major qps reported qpopt note qpopt reports cfsqp cfsqp cfsqp 9 19 7 60000000e00 43 cfsqp table 1 numerical results hockschittkowski problems zero iteration result first step onto working set linear constraints happens optimal fair fsqp 0 thus count work involved solving ls e either also count qp iterations solving qp c correction qp invoked identically algorithms reason smaller ffl cam allowed cfsqp reach globally optimal objective function values per 3 results show typical significantly lower number qp iterations new algorithm case hockschittkowski prob lems roughly comparable behavior two algorithms terms number function evaluations note two instances nqp count less cfsqp fsqp 0 different local minima reached makes comparison meaningless finally abnormal terminations sphere50 sphere100 due qpopts failure solve qpthe tilting qp case cfsqp one issue interest whether convergence results still hold weaker assumptions wit qi wei showed 20 algorithm 15 still enjoys global convergence limit points kkt local twostep superlinear convergence assumption 3 licq replaced mangasarianfromovitz constraint qualification mfcqor even condition slightly weaker mfcq showed algorithm slightly modified local superlinear convergence preserved without strict complementarity assumption provided strong secondorder sufficiency condition ssosc assumed algorithm fsqp 0 stated however licq strict complementarity essential connection step 3 iii first assumption 3 needed order ls e welldefined close solution x second strict positivity multipliers associated active constraints solution needed order components e k nonnegative solution approached barring condition e may never hold update rule j k1 c k1 delta kd k k 2 may used close solution case superlinear convergence would take place careful modifications algorithm fsqp 0 might least theory accommodate weaker assumptions though first absence assumption 3 ls e could possibly replaced qp ji e inequality constraints essentially penalty cpu cost second replacing nonnegativity condition requirement type ej gammaffl k ej reg ular multiplier see 20 ffl k 0 would made go zero appropriate slow rate may possible preserve convergence kkt points insuring even absence strict comple mentarity test would satisfied close solution thus allowing superlinear convergence take place would require detailed analysis though may likely hurt help practice second issue worth discussing possible low cost solution qp two linear leastsquares problems indexes constraints appearing ls e ls c iteration k generally different e 28 142 776859 1e4 cfsqp 42 8177 44 350 776859 cfsqp 591 345458 154 2771 783873 cfsqp 795 28328 246 587 660675 cfsqp failure cfsqp 977 3784 575 1259 481189 cfsqp table 2 numerical results cops problems former k latter however proved k large enough sets equal ix case readily checked ls e involve matrix thus latter solved low cost former solved unfortunately linear systems arising solution qp different particular involve j k 6 conclusions presented new sqptype algorithm generating feasible erates main advantage algorithm presented reduction amount computation required order generate new iterate may important applications function evaluations dominate actual amount work compute new iterate useful many contexts case saw previous section preliminary results seem indicate decreasing amount computation per iteration come cost increasing number function evaluations required find solution number significant extensions algorithm fsqp 0 ex amined difficult extend algorithm handle minimax problems real issue arises handle minimax objectives least squares subproblems several possibilities desired global local convergence properties examined another extension important engineering design incorporation scheme efficiently handle large sets constraints andor objectives examine schemes along lines developed 11 25 work remains done exploit close relationship two least squares problems quadratic program careful implementation able use relationships great advantage computationally starters updating cholesky factors h k instead h k iteration would save factorization subproblems finally possible extend class problems p handled algorithm include nonlinear equality con straints course able generate feasible iterates constraints scheme studied 10 could used order guarantee asymptotic feasibility maintaining feasibility inequality constraints r variant topkisveinott method solving inequality constrained optimization problems sequential quadratic programming interior point algorithm large scale nonlinear programming formulation theory newton interiorpoint method nonlinear programming primaldual interior method nonconvex nonlinear programming users guide qpopt 10 fortran package quadratic programming successive quadratic programming based feasible directions algorithm test examples nonlinear programming codes nonlinear equality constraints feasible sequential quadratic programming feasible sequential quadratic programming finely discretized problems sip users guide cfsqp version 25 c code solving large scale constrained nonlinear minimax optimization problems exact penalty functions finite dimensional control optimization problems superlinearly convergent feasible method solution inequality constrained optimization problems combining feasibility computational methods optimization convergence variable metric methods nonlinearly constrained optimization calculations fast algorithm nonlinearly constrained optimization calculations constant positive linear dependence condition application sqp methods perturbed kuhntucker points rates convergence class nonlinearprogramming algorithms qld fortran code quadratic programming primaldual interiorpoint method nonconvex optimization multiple logarithmic barrier parameters strong convergence properties interior point algorithm nonconvex nonlinear programming sqp algorithm finely discretized continuous minimax problems minimax problems many objective functions users guide fsqp version 37 fortran code solving nonlinear minimax optimization problems methods feasible directions tr ctr dudy lim yewsoon ong busung lee inverse multiobjective robust evolutionary design optimization presence uncertainty proceedings 2005 workshops genetic evolutionary computation june 2526 2005 washington dc zhibin zhu efficient sequential quadratic programming algorithm nonlinear programming journal computational applied mathematics v175 n2 p447464 15 march 2005 l bauwens c hafner j v k rombouts multivariate mixed normal conditional heteroskedasticity computational statistics data analysis v51 n7 p35513566 april 2007 david cardoze alexandre cunha gary l miller todd phillips noel walkington bzierbased approach unstructured moving meshes proceedings twentieth annual symposium computational geometry june 0811 2004 brooklyn new york usa matthew j tenny stephen j wright james b rawlings nonlinear model predictive control via feasibilityperturbed sequential quadratic programming computational optimization applications v28 n1 p87121 april 2004 daniel mueller helmut graeb ulf schlichtmann tradeoff design analog circuits using goal attainment wave front sequential quadratic programming proceedings conference design automation test europe april 1620 2007 nice france ong k lum p b nair hybrid evolutionary algorithm hermite radial basis function interpolants computationally expensive adjoint solvers computational optimization applications v39 n1 p97119 january 2008 borys shchokin farrokh janabisharifi design kinematic analysis rotary positioner robotica v25 n1 p7585 january 2007