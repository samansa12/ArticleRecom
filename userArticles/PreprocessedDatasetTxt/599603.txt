principal components approach combining regression estimates goal combining predictions multiple learned models form improved estimator combining strategy must able robustly handle inherent correlation multicollinearity learned models identifying unique contributions progression existing approaches limitations respect two issues discussed new approach pcrast based principal components regression proposed address limitations evaluation new approach collection domains reveals 1 pcrast robust combining method 2 correlation could handled without eliminating learned models 3 principal components learned models provided continuum regularized weights pcrast could choose b introduction combining set learned models improve classification regression estimates area much research machine learning neural networks learned model may anything decisionregression tree neural network challenge problem decide models rely prediction much weight give suppose physician wishes predict persons percentage body fat pbf collection patient records simple measurementsattributes height weight chest circumference leg circumference etc along measurement pbf derived water displacement test task predict pbf future patients using simple measurements without performing expensive water displacement test physician derived several models predicting pbf using various linear regression methods several neural network configurations existing heuristic functions goal combine learned models obtain accurate prediction obtained single model general problem combining estimates robustly focus paper one major issue combining set learned models amount correlation set predictors high degree correlation expected learned models attempting perform prediction task correlation reflects amount agreement linear dependence models making set predictions models agree correlation redundancy present cases one models may expressed linear combination various numerical coefficients models high degree correlation model set cause combining schemes produce unreliable estimates statistical terms referred multicollinearity problem another issue combining predictions learned models detecting models unique contribution predicting target outcome models generated using different learning algorithms likely make contributions example neural network may discover useful nonlinear interactions amongst initial attributes whereas standard linear regression method may employ attribute deletion strategy simplifies prediction task good combining strategy must able weigh model according unique contribution tradeoff exists solving problems mentioned solutions multicollinearity problem likely ignore unique contributions model hand methods good finding unique contributions model susceptible multicollinearity problem point two extremes prediction error minimized sought focus paper present study algorithm solving problems multicollinearity discovering unique contributions paper begins defining task combining regression estimates section 2 discussing limitations existing approaches respect problems discussed advanced approaches solving multicollinearity problem described section 3 novel approach called pcr based principal components regression outlined section 4 analytical empirical analyses given sections 5 6 respectively related work discussed section 7 directions future work given section 8 concluding remarks given section 9 2 motivation problem combining set learned models defined using terminology 25 suppose two sets data given training set test set test rain used build set functions element approximates fx goal find best approximation fx using f approaches problem limit space approximations fx linear combinations elements f ie ff coefficient weight focus paper develop method setting coefficients overcomes limitations earlier approaches brief summary combining regression estimates 3 approaches provided progressing simpler complex methods pointing limitations along way simplest method combining members f taking unweighted average ie ff perrone cooper refer basic ensemble method bem written equation also written terms misfit function functions describe deviations elements f true solution written thus perrone cooper show long x mutually independent zero mean error estimating fx made arbitrarily small increasing size f since assumptions break practice developed general approach finds optimal 1 weights allowing correlated nonzero means generalized ensemble method gem written edelta expected value function c symmetric sample covariance matrix misfit function goal minimize note misfit functions calculated training data fx required main disadvantage approach involves taking inverse c unstable redundancy misfits leads linear dependence rows columns c turn leads unreliable estimates c gamma1 circumvent sensitivity redundancy perrone cooper propose method discarding members f strength agreement another member exceeds certain threshold unfortunately approach checks linear dependence redundancy pairs could linear combination several members f instability problem would manifest also depending high threshold set member f could discarded still degree uniqueness utility ideal method weighting members f would neither discard models suffer redundancy model set next approach reviewed linear regression lr gem lr closely related gem form linear regression added constraint 1 weights lr found follows 2 general form linear regression linear regression constant term lrc lrc calculated way member predicts 1 according 17 extra constant term necessary ie equal zero practice e like gem lr lrc subject multicollinearity problem finding ff involves taking inverse matrix f matrix composed strongly agree members f linear dependence present given limitations methods goal research find method finds weights learned models low prediction error without discarding original models without subject multicollinearity problem 3 methods handling multicollinearity abovementioned methods multicollinearity leads inflation variance estimated weights ff consequently weights obtained fitting model particular sample may far optimal values circumvent problem several approaches developed 1 one method handling multicollinearity build models make decorrelated errors adjusting bias learning algorithm 24 data combining regression estimates 5 sees 19 approach ameliorates solve problem redundancy inherent part task combining estimators 2 gradient descent procedures ie widrowhoff learning gd eg eg 12 search coefficients making iterative multiplicative exponentiated updates ffcoefficients function performance training data avoids matrix inversion step susceptible multicollinearity problem potential problems gradient descent approaches possibility getting trapped local minima choosing appropriate initial weights deciding large weight updates 3 least squares regression methods rely matrix inversion finding weights ie lr lrc made reliable constraining types weights may produce ridge regression ridge 23 parameter may used restrict regularize ffcoefficients breiman 2 devised approach based constrained least squares regression 16 coefficients required nonnegative focus paper flexible approach weight regularization based principal components regression described section 4 discussion turns precise description weight regularization effective handling multicollinearity problem leblanc tibshirani 17 proposed several ways constraining regularizing weights help produce estimators lower prediction error 1 shrink ff towards 1k number learned models 2 3 ff 0 breiman 2 provides intuitive justification constraints pointing strongly satisfied interpolative weighting scheme extreme case uniformly weighted set learned models likely produce prediction maximumand minimum predicted values learned models without constraints guarantee resulting predictor stay near range generalization may poor effective weight regularization technique must decide appropriate level constraint placed weights demonstrate selecting number principle components principal components regression allows appropriate amount weight regularization selected given set learned models 4 pcr algorithm pcr algorithm may broken four parts representation regres sion search evaluation section 41 discusses first two parts describing 6 christopher merz michael pazzani model set may mapped new representation using principal components analysis resulting components may used build regression model section 42 discusses latter two parts algorithm asterisk pcr denotes search number principal components retain tightly coupled evaluation metric given model 41 representation regression pcr named partly modeling method core principal components regression see 3 summary section discusses central role pcr plays representation regression pcr pcr representation final regression estimate f x restricted linear combinations learned models f ie ff j coefficient weight pcr uses intermediate representation order derive final regression estimate main idea map original learned models new set models using principal components analysis pca new models decomposition original models predictions n independent components useful initial components retained build estimate f mapping reversed get weights original learned models following discussion elaborates process intermediate representation derived using principal components analysis pca define f matrix learned models predictions f pca takes input square symmetric covariance matrix f denoted f output pca new representation called principal components ie g principal component column vector matrix pc associated principal component eigenvalue j denoting percentage variance component j captures original matrix f one advantage representation components independent means correlation pc pc j zero 6 j another advantage components ordered eigenvalues ie combining regression estimates 7 given new representation goal choose number principal components include final regression retaining first k meet preselected stopping criterion choosing k search aspect pcr covered section 42 k selected estimate f derived via linear least squares regression using pc 1 pck ie known principal components regression pcr finally weights ff derived original learned models expanding 5 according fl kj jth coefficient kth principal component ffcoefficients calculated follows equations 2 7 make core pcr algorithm summarized table 1 third step ie choosing k constitutes search aspect pcr next section elaborates process table 1 pcr algorithm input f matrix predictions models f 1 2 3 4 5 ff 6 return ff 42 search procedure evaluation main search component pcr step 3 involves choosing k see table 1 basic idea include successive principal components regression estimate fx see eqn 5 n components used 3 reason searching k manner principal components ordered amount variance capture original learned models first principal component explains variance data models agree subsequent orthogonal components capture variations models predictions therefore number components retained directly affects much attention paid variations predictions learned models value k 1 k n lowest estimated error chosen step important choosing many principal components may result underfitting overfitting respectively evaluation criterion selecting k measure error possible value k table 2 shows vfold crossvalidation used estimate error k given k partition v held evaluated regression equation derived modified set principal components pc gammav pc gammav pc examples partition v removed k smallest crossvalidation error chosen k approaches choosing k explored 20 table 2 choose cutoff algorithm input f matrix predictions models f gamma eigenvectors derived pcr target output output k number components retained 1 form v random partitions f 2 partition v ffl create new principal components f gammav f examplesrows partition v removed ffl 3 return arg min combining regression estimates 9 5 understanding pcr analytically section provides analysis illuminates pcr addresses open problems discussed section 1 artificial data sets used show pcr provides continuum regularized weights original learned models section 51 shows pcr produces highly regularized set weights avoid multicollinearity problem section 52 demonstrates pcr handles problem detecting areas specialization learned model producing less regularized set combining weights section 6 evaluate pcr real problems 51 multicollinearity problem multicollinearity problem described section 3 leads increase variance estimated weights ff resulting prediction error quite high weights sensitive minor changes data avoid weights must regularized weight regularization pcr controlled via number principal components retained let pcr k denote instantiation pcr first principal components retained consider deriving ffweights using first principal component defined linear combination members f highest average correlation members f case weights fl 1 tend quite similar learned models fairly accurate ie e equation 7 shows flweights turn multiplied constant fi 1 derived equation 6 thus resulting nearly uniform later principal components serve refinements already included producing less constrained weight sets finally nth principal component included resulting unconstrained estimator theoretically equivalent standard linear regression lr experiment conducted using artificial data set demonstrate weight sets derived pcr become less regularized number principal components retained grows 1 n f gaussian function mean zero standard deviation one ie f n0 1 model derived follows c n0 01 produce ten unique models total twenty models f first ten models duplicated second set ten creating multicollinearity model produce slight variation f c standard deviation 01 one would expect high degree regularization order data extreme multicollinearity fact models uniformly distributed f artificial data set a1 derived using equations consists 200 training examples 100 test examples figure 1 displays collection possible weight sets derived pcr a1 yaxis range coefficient values xaxis number principal components used derive ffweights line traces single models weight ff derived using first k principal components weights start small positive values pcr 1 ff 120 principal components included weights become less regularized eg weights become negative continues k approaches n point weights take broad range values pcr chose stop corresponding error curve experiment shown figure 2 graph yaxis mean absolute error xaxis figure 1 k increases approaches n error rate also increases lowest error rate occurred value pcr chose experiment repeated 20 times pcr consistently choosing highly regularized weights figure 1 ffweights single run artificial data set a1 line corresponds ff derived using first k principal components 100100300number principal components retained weight value 52 discovering niches purpose section demonstrate pcr chooses less regularized weights order capture unique abilities learned model predicting combining regression estimates 11 figure 2 error curve one run artificial data set a1 point corresponds error rate associated ff weights derived using first k principal components 20010305number principal components retained f less regularized weights needed errors committed learned models patterns error cannot canceled simple uniform weighting demonstrate pcr handles situation another artificial data set created model performs particularly well certain range target values data set a2 generated similar fashion a1 f n0 1 derived follows ae 02 function produces set 20 models model performs particularly well ie minor offset interval c 025 otherwise model randomly guesses uniformly 70 90 percent true value particular point x j plus minor offset data set a2 200 training examples 100 test examples generated using function figure 3 displays weights function number principal components retained data set a1 weights become less regularized k increases range values narrower even figure 1 corresponding error curve test data plotted figure 4 error rate starts high decreases k approaches nine increases k exceeds ten case lowest point error curve initial decrease error stems pcr including unique contributions model captured principal components derivation ffweights increase error k exceeds ten due multicollinearity contained model set experiment repeated 20 times pcr consistently choosing appropriate amount regularization note figure 4 plots error measured unseen test data pcr uses estimate error derived training data figure 3 ffweights single run artificial data set a2 line corresponds ff derived using first k principal components 101030number principal components retained weight value combining regression estimates 13 figure 4 error curve single run artificial data set a2 point corresponds error rate associated ff weights derived using first k principal components 20001003005007number principal components retained 53 trading bias variance prediction error learned model attributed two components due bias model due variance model elaborate decomposition prediction error see 8 bias algorithm measures consistently models produces various data sets size differ true function f variance measures much algorithms predictions fluctuate possible data sets decrease overall generalization error algorithm necessary decrease error due bias andor error due variance consider pcr algorithm 1 nearly uniform weights produced essentially ignore patterns predictions f patterns f useful predicting f pcr consistently predictions producing biased result corresponds points error curve figure small values k result higher error hand multicollinearity present f weight estimates may sensitive minor changes data causing predictions high variance corresponds points error curve figure 2 larger values 14 christopher merz michael pazzani k produce error pcr attempts find minimum error curve error dominated either bias variance 54 computational complexity computational complexity pcr analyzed independent model generation process given set n models built examples three largest procedures 1 calculation covariance matrix performed takes 2 inversion matrix general computing inverse matrix cubed number columnsrows matrix inversions performed n theta n matrices taking 3 time inversion procedure performed total n determining fi coefficients final model partition l 1 used determining k note choose cutoff algorithm table 2 may optimized computing fi coefficients using n principal components fi coefficients derived using subset components principal components uncorrelated therefore matrix inversion takes ov time v typically ten 3 singular value decomposition matrix svd n theta n matrix takes 3 therefore total time complexity pcr 2 maxmn 6 empirical evaluation pcr two experiments run compare pcr combining strategies first experiment aims evaluate combiners dozen models half neural networks half adaptive regression splines purpose experiment twofold evaluate combiners using stacking described evaluate combiners abilities combine models generated using different learning algorithms second experiment tests combiners ability handle large number correlated models combiners evaluated model sets size 10 50 parameter v choose cutoff algorithm set 10 61 regression data sets table 3 summarizes eight data sets used source column lists uci data sets taken uci machine learning repository 21 cmu data sets taken statistics library carnegie mellon university 22 qsar data sets taken qsar home page 15 ucimc proprietary combining regression estimates 15 table 3 data set descriptions data set examples attributes numeric source baseball 263 bodyfat 252 14 14 cmu dementia 118 26 26 ucimc hansch housing 506 12 12 uci imports 160 15 15 uci data set uci medical center imports data set 41 examples missing values used due limitations one learning algorithms used 62 constituent learners set learned models f generated using backpropagation networks bp 28 multivariate adaptive regression splines mars 7 ex periments preliminary bp runs conducted find network topology gave good performance data set combining methods would work well improve upon single model 63 combining methods combining methods evaluated consist methods discussed sections 2 3 well pcr 1 pcrn demonstrate pcrs least regularized weight sets respectively elaborate description given methods briefly mentioned section 3 procedures based widrowhoff learning 12 gradient descent gd exponentiated gradient procedures eg eg gamma iterative approaches weights ff revised multiplicativeexponentiated dates revision attempts move weights direction lower mean squared error training data ridge regression equation deriving weights similar deriving ficoefficients pcr using n principal components major difference theta identity matrix multiplied constant added matrix pc pc effect increases resulting regression coefficients generated ordinary linear regression lr shrink towards zero proportionally ffcoefficients derived pcr end result restricted set coefficients iterative approach used searching discussed 23 stacked constrained regression scr procedure 2 also included evaluation two main components approach stacking constrained regression stacking 32 simply method approximating matrix predictions f idea rather using actual predictions learned models better use estimate estimate give information correct errors learned model estimated predictions generated using 10fold crossvalidation technique noted stacking component computationally expensive learned model final set 10 approximations must generated major component scr constrained regression ffweights obtained using ordinary least square regression restriction weights nonnegative simpler version stacked constrained regression without stacking component referred cr also included evaluate utility constrained regression alone 64 experiment 1 experiment aims evaluate combining strategies smaller number learned models generated different learning algorithms smaller model set used make evaluation scr tractable twelve models generated six generated using mars version 35 7 first three models variables entered unrestricted restricted linear fashion respectively three models generated entering variables unrestricted fashion model deleting one three relevant variables determined diagnostic output preliminary run mars six bp models generated using three different network topologies random weight initialization thirty runs conducted data set trial data randomly divided 70 training data 30 test data tables 4 5 report means standard deviations absolute error rate rows tables divided two blocks former block consists crude methods obtaining highly constrained unconstrained weights latter block consists advanced methods capable producing weight sets varying degrees reg ularization boldfaced entries indicate methods significantly different pcr via twotailed paired ttests p 001 65 discussion experiment 1 observing combining methods first block rows reveals regularization appears necessary baseball cpu dementia hansch data sets little regularization appears necessary servo data set method first block particularly well bodyfat housing data sets indicating moderate amount regularization required examining advanced methods handling multicollinearity second block rows reveals pcr eg cr best overall perfor combining regression estimates 17 table 4 means standard deviations absolute error rate combining strategies first four data sets method baseball bodyfat cpu dementia gem 65e333e4 191276 3701065 131828 table 5 means standard deviations absolute error rate combining strategies last four data sets method hansch housing imports servo gem 6229128 641102 1129253e3 0364005 bem gd eg cr pcr table 6 average rankings cr eg pcr data set data set cr eg pcr baseball 693 64 717 bodyfat 573 58 313 cpu 923 827 7767 dementia 1127 997 913 hansch 613 583 837 housing 848 792 752 imports 6867 693 78 servo 8267 803 573 mances pcr statistically indistinguishable best method hansch data set case eg cr 35 relative reduction error pcr eg cr statistically indistinguishable leading method bodyfat data set pcr 96 relative reduction error eg cr gd eg better methods first block difficulty finding good weight set methods occasionally converge poor local minima spite setting initial weights learning rate kivinen warmuth 12 recommend another interesting result constrained regression cr tends outperform constrained regression stacking scr slight losses two data sets raises issue whether stacking beneficial component scr algorithm real data sets extra work appear improve results average rankings also calculated methods given run method assigned rank according number methods lower error rates ranks averaged runs data set table 6 reports results three best combining strategies ie pcr cr eg pcr consistently performed well respect ranking scores closest competitors cr eg better average ranking pcr three data sets figure 5 shows relative error reduction made pcr compared best individual model data set pcr improves performance much 105 largest loss 43 increase error overall improvement occurred five data sets average reduction 25 66 experiment 2 second experiment tests combiners see well perform large number correlated models combiners evaluated model sets size 10 50 20 trials run data sets trial data randomly divided 70 training data 30 test data experiment collection networks built differed initial weights topology extreme effort produce networks decorrelated errors even networks issue extreme combining regression estimates 19 figure 5 relative change error pcr respect best individual model data set 006 00200200601data set change housing baseball cpu servo hansch imports dementia multicollinearity would still exist e models included linear dependence amongst goes showing well multicollinearity problem handled linear dependence verified observing eigenvalues principal components values covariance matrix models f table 7 reports results three representative data sets terms distinguishing combiners ie bodyfat cpu housing means standard deviations absolute error given methods data sets two new methods included table 7 pcr 1 pcrn representing pcr stopping first last component respectively serve show pcrs performance relative using highly constrained unconstrained weight sets row particular method column size f given data set boldfaced entries indicate methods significantly different pcr via twotailed paired ttest p 001 table 7 results many learned models data bodyfat cpu housing gem eg 67 discussion experiment 2 experiment 1 pcr performed similarly eg cr results table 7 distinguish pcr eg cr bodyfat data set eg cr converge weight sets near uniform resulting poor performance relative pcr pcr approach among leaders three data sets bodyfat housing data sets weights produced bem pcr 1 gd gamma tended constrained weights lr tended unconstrained larger collection models less constrained weights gem lr ridge pcrn severely harmed performance cpu domain uniform weighting performed better biggest demonstration pcrs robustness ability gravitate towards constrained weights produced earlier principal components appropriate ie cpu data set similarly uses less constrained principal components closer pcrn preferable bodyfat housing domains 7 related work several combining strategies exist addition combining strategies described sections 2 3 63 next three sections discuss two general approaches data resampling techniques methods assigning weights function example predicted 71 general approaches hashem schmeiser 9 developed combining scheme similar gem well less constrained version require weights sum one like gem method susceptible multicollinearity problem combining regression estimates 21 opitz shavlik 24 attempt assign model weight according estimate accuracy ie estimate model accuracy based performance validation set intuitively model gets weight estimated performance increases relative estimated cumulative performance models weights derived using approach less susceptible multicollinearity problem less robust intercorrelations models considered technique pruning weights neural network given 18 method also applicable fi coefficients produced pcr threshold set pruning principal components function training error principal component small fi weight small eigenvalue pruned ie fi 2 pcr similar retains principal components function training error however pruning technique focuses discarding components negligible impact final equation criterion pcr prunes later principal components small eigenvalues unnecessarily large fi weight 72 resampling strategies resampling strategies another approach generating combining learned models approaches model generation phase tightly coupled model combination stage goal generate set models likely make uncorrelated errors higher variance thus increasing potential payoffs combining stage model generated using algorithm different training data data particular model obtained sampling original training examples according probability distribution probability distribution defined particular approach bagging boosting bagging 1 method exploiting variance learning algorithm applying various version data set averaging uniformly overall reduction variance prediction error variations training data obtained sampling original training data replacement probability example drawn uniform number examples drawn size original training set underlying theory approach indicates models weighted uniformly unlike pcr bagging limited single learning algorithm another resampling method roots known boosting initially developed schapire 29 boosting based idea set moderately inaccurate rulesofthumb ie learned models generated combined form accurate prediction rule initial development research purely theoretical subsequent refinements 5 4 produced practical 22 christopher merz michael pazzani implementations boosting approach technique assigns weight example training data adjusts learning model initially examples weighted uniformly learning subsequent models examples reweighted follows easy examples predicted low error previously learned hypotheses ie learned models get lower weight hard examples frequently predicted high error given higher weight data sets learned model resampled replacement according weight distribution examples 4 common combining strategy boosting described freund schapires algorithm ith models weight function error ffl ie scheme learned models less error distribution examples see tend get higher weights boosting bagging emphasis placed model generation model combination possible elaborate combining scheme like pcr may effective method combining models generated two recent experimental evaluations boosting bagging given 5 27 approaches proven quite effective currently limited single learning algorithm kong dieterrich 13 point combining heterogeneous learning algorithms reduce bias well variance bias errors various algorithms different krogh vedelsby 14 developed method known query committee 30 6 6 approach collection neural networks trained simultane ously patterns large ambiguity ie ensembles predictions tend vary considerably likely included next round training 73 nonconstant weighting functions combining approaches weigh learned model function example predicted prevalent method literature dynamically deciding weight collection regressors classifiers mixture experts approach 10 consists several different expert learned models ie multilayer perceptrons plus gating network decides experts used case expert reports target attribute probability distribution given example gating network selects one experts appear appropriate target distribution example training weight changes localized chosen experts gating network experts accurate example 5 given responsibility example experts inaccurate example given less responsibility weights experts specialize quite different cases unmodified experts become localized weights combining regression estimates 23 decoupled weights experts end specializing small portion input space jordan jacobs 11 expanded approach allowing learned mod elsexperts generalized linear models experts leaves treestructured architecture whose internal nodes gating functions gating functions make soft splits allowing data lie simultaneously multiple regions currently weights generated pcr change function example predicted comparison two approaches needed tresp taniguchi 31 derived collection nonconstant weighting functions used combine regressors classifiers proposed methods weigh learned model according reliability region given example reliability defined terms either models accuracy region given example amount variability models predictions region approaches require weights positive sum one methods proposed evaluated empirically may prove useful extending methods like pcr allow weights learned models change function example classified 8 limitations future work pcr limited combining regression estimates linear weights one direction currently explored extension pcr classification task accomplished one pcrlike model possible class preliminary results indicate effective method combining classifiers another direction future work expand pcrs abilities allowing nonconstant weighting likely model performs consistently throughout space possible examples allowing learned models weight change respect example would extend pcrs ability find strengths weaknesses model 9 summary conclusion investigation suggests principal components set learned models useful combining models form improved estimator demonstrated principal components provide continuum weight sets ranging highly regularized unconstrained algorithm pcr developed attempts automatically select subset components provides lowest prediction error experiments collection domains demonstrated pcrs ability identify unique contributions learned model robustly handling inherent redundancy amongst models notes 1 optimal refers weights minimize mean square error training data 2 note constraint gem form regularization 17 purpose regularizing weights provide estimate less biased training sample thus one would expect gem lr produce identical weights 3 least squares regression using n principal components denoted pcrn equivalent standard linear regression original members f 4 note resampling technique replaced reweighting technique learning algorithm capable directly accepting weighted set examples 5 accurate means less error weighted average errors expertsusing outputs gating network decide weight experts error less accurate prediction example error weighted average r heuristics instability model selection stacked regressions applied regression analysis decisiontheoretic generalization online learning application boosting experiments new boosting algorithm multivariate adaptive regression splines neural networks biasvariance dilemma improving model accuracy using optimal linear combinations trained neural networks adaptive mixtures local experts hierarchical mixtures experts em algorithm exponentiated gradient descent versus gradient descent linear predictors neural network ensembles qsar modelling society home page solving least squares problems combining estimates regression classification fast pruning using principal components bias variance combination least squares estimators classification regression combining models uci repository machine learning databases cmu statlib home page generating accurate diverse members neuralnetwork ensemble networks disagree ensemble methods hybrid neural networks numerical recipes c art scientific computing learning internal representations error propagation strength weak learnability query committee combining estimators using nonconstant weighting functions stacked generalization tr ctr michael j pazzani daniel billsus adaptive web site agents autonomous agents multiagent systems v5 n2 p205218 june 2002 b kotsiantis local averaging heterogeneous regression models international journal hybrid intelligent systems v3 n2 p99107 january 2006 slobodan vucetic zoran obradovic collaborative filtering using regressionbased approach knowledge information systems v7 n1 p122 january 2005 nageswara sv rao fusers perform better best sensor ieee transactions pattern analysis machine intelligence v23 n8 p904909 august 2001 hillol kargupta byunghoon park fourier spectrumbased approach represent decision trees mining data streams mobile environments ieee transactions knowledge data engineering v16 n2 p216229 february 2004 niall rooney david patterson chris nugent pruning extensions stacking intelligent data analysis v10 n1 p4766 january 2006 niall rooney david patterson chris nugent nonstrict heterogeneous stacking pattern recognition letters v28 n9 p10501061 july 2007