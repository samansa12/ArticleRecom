compilerdirected cache polymorphism classical compiler optimizations assume fixed cache architecture modify program take best advantage cases may best strategy loop nest might work best different cache configuration transforming nest given fixed cache configuration may possible due data dependences working fixed cache configuration also increase energy consumption loops best required configuration smaller default fixed one paper take alternate approach modify cache configuration nest depending access pattern exhibited nest call technique compilerdirected cache polymorphism cdcp specifically paper make following contributions first present approach analyzing data reuse properties loop nests second give algorithms simulate footprints array references reuse space third based reuse analysis present optimization algorithm compute cache configurations nest experimental results show cdcp effective finding nearoptimal data cache configurations different nests arrayintensive applications b introduction todays microprocessor systems include several special architectural features eg large onchip caches use significant fraction onchip transistors complex energyhungry features meant applicable across different application domains however effectively wasted applications cannot fully utilize implemented rigid manner example loops given arraybased embedded application take advantage large onchip cache also working fixed cache configuration increase energy consumption loops best required configuration performance angle smaller default fixed one larger cache result large per access energy conventional approach address locality problem caches problem maximizing number cache hits employ compiler optimization techniques 8 current compiler techniques generally work assumption fixed cache memory architecture try modify program behavior new behavior becomes compatible underlying cache configuration however several problems method first compilerdirected modifications sometimes effective data dependences prevent necessary program transformations second available cache space sometimes cannot utilized efficiently static configuration cache match different requirements different programs andor different portions program third current compiler techniques adapted scientific compilation domain take energy issues account general alternative approach locality problem use reconfigurable cache structures dynamically tailor cache configurations meet execution profile application hand approach potential address locality problem cases optimizing application code alone fails however previous research area 1 9 mainly focused implementation employment mechanisms designs lacks softwarebased techniques direct dynamic cache reconfigurations recently compilerdirected scheme adapt cache assist proposed 6 work focuses cache opposed cache assist paper propose strategy optimizing compiler decides best cache configuration nest application code specifically paper make following contributions first present techniques analyzing data reuse properties given loop nest constructing formal expressions reuse patterns second develop algorithms simulate footprints array references simulation approach much efficient classical cyclebased simulation techniques simulates data reuse space third develop optimization algorithm computing optimized cache configurations loop nest also provide program level algorithm selecting dynamic cache configurations focus behavior array references loop nests loop nests important part arrayintensive media signal processing application programs cases computation performed loop nests dominates execution time programs thus behavior loop nests determines performance energy behavior applications previous research 8 shows performance loop nests directly influenced cache behavior array references also recently energy consumption become important issue embedded systems 9 conse quently determining suitable combination cache memory configuration optimized software challenging problem embedded design world rest paper organized follows section 2 reviews basic concepts notions representations arraybased codes section 3 concepts related cache behavior cache misses interferences data reuse data locality analyzed section 4 introduces compilerdirected cache polymorphism technique presents complete set algorithms implement present experimental results section 5 show effectiveness technique finally section 6 concludes paper summary discusses future work topic 2 arraybased codes paper particularly targeted arraybased codes since performance loop nests dominates overall performance arraybased codes optimizing nests particularly important achieving best performance many embedded signal video processing applications optimizing data locality majority data references satisfied cache instead main memory improve performance energy efficiency loop nests following ways first significantly reduce number misses data cache thus avoiding frequent accesses lower memory hierarchies second reducing number accesses lower memory hierarchies increased cache hit rate helps promote energy efficiency entire memory system section discuss basic notions arraybased codes loop nests array references well assumptions made 21 representation programs assume application code optimized format shown figure 1 assumption 1 array application code optimized declared global declaration section program arrays declared global section referenced loop code assumption necessary algorithms discussed following sections optimization stage computing cache configuration loop nests assumption 1 ensures exploitable relative base address array involved global declaration section arrays mainint argc char argv f loop nest 0 loop nest loop nest l figure 1 format program f figure 2 format loop nest since loop nests main structures arraybased pro grams program codes loop nests neglected also assume nest independent oth ers shown figure 1 application contains number independent nests interloopnest data reuse accounted assumption relaxed achieve potentially effective utilization reconfigurable caches one future research note several compiler optimizations loop fusion fission code sinking used bring given application code format 12 assumption 2 loop nests program lexical level global level internesting two different loop nests assumption 3 nests code perfectlynested ie array operations array references occur innermost loop assumption vital analysis make implementation easier plan relax future work 22 representation loop nests work loop nests form boundaries dynamic cache reconfigurations occur figure 2 shows format loop nest format stands loop index vector corresponding lower bound upper bound stride loop index different instances array references nest note may different references array different references different arrays function f jk subscript expression function th subscript j th array reference dk number dimensions corresponding array 23 representation array references loop nest loop index vector reference ar j array dimensions expressed assume subscript expression functions f jk affine functions loop indices loopinvariant con stants rowmajor storage layout assumed arrays c language assuming loop index vector n depth vector number loops nest array reference represented 1 vector left side equation called array reference subscript vector f matrix defined access matrix rightmost vector known constant offset vector c thus equation also written 12 3 cache behavior section review basic concepts cache behavior noted earlier arrayintensive applications cache behavior largely determined footprints data manipulated loop nests paper first propose algorithm analyzing cache behavior different arrays different array references given loop nest based information gathered analysis propose another algorithm compute cache memory demand order achieve perfect cache behavior loop nest analyzed suggest cache configuration 31 cache misses three types cache misses compulsory cold misses capacity misses conflict interference misses different types misses influence performance program different ways note data caches used current embedded systems implemented setassociative caches directmapping caches order achieve high speed low power low implementation cost thus caches interference misses dominate cache behavior particularly arraybased codes stressed since cache interferences occur highly irregular manner difficult capture accurately 11 ghosh et al proposed cache miss equations 4 analytical framework compute potential cache misses direct code optimizations cache behavior 32 data reuse data locality data reuse data locality concepts discussed 12 detail basically two types data reuses temporal reuse spatial reuse given loop nest reference accesses memory location across different loop iter ations termed temporal reuse reference accesses cache block necessarily memory location call spatial reuse consider temporal reuse special case spatial reuse different references accessing memory location say grouptemporal reuse exists whereas different references accessing cache block termed groupspatial reuse note group reuse occurs among different references array loop nest reused data item found cache say reference exhibits locality means data reuse guarantee data locality convert data reuse locality catching reused item cache classical looporiented compiler techniques try achieve modifying loop access patterns 4 algorithmsforcachepolymor performance energy behavior loop nests largely determined cache behavior thus optimize cache behavior loop nests utmost important satisfying highperformance energy efficiency demands arraybased codes least two kinds approaches perform optimizations cache behavior conventional way compiler algorithms transform loops using interchange versal skewing tiling transformations transform data layout match array access pattern mentioned earlier alternative approach modify underlying cache architecture depending program access pattern recent research work 7 explores potential benefits second approach strategy presented 7 based exhaustive simulation main drawback simulationbased strategy extremely time consuming consider fixed set configurations typically simulating nest possible cache configurations makes approach unsuitable practice section present alternative way determining suitable cache configurations different sections nests given code 41 compilerdirected cache polymorphism existence cache interferences main factor degrades performance loop nest cache interferences disrupt data reuse loop nest preventing data reuse converted locality note self interferences crossinterferences prevent data item used still cache objective determine cache configurations help reduce interferences basic idea behind compilerdirected cache polymorphism cdcp analyze source code arraybased program determine data reuse characteristics loop nests compile time compute suitable nearoptimal cache configuration loop nest exploit data locality implied reuse nearoptimal cache configuration determined nest eliminates interference misses keeping cache size associativity control way optimizes execution time energy time fact increasing either cache capacity associativity increases energy consumption approach source codes modified obviously optimized f figure 3 example code loop nest fore algorithms run mean code modifications sake cache morphism high level approach described follows first use compiler transform source codes intermediate format second step loop nest processed basic element cache configuration loop nest references array assigned different uniform reference sets uniform set analyzed determine reuse exhibit different loop levels array algorithm used simulate footprints reuse space within layout space array following loop nest level algorithm optimizes cache configurations ensuring data locality finally code generated dynamic cache configurations activated runtime appropriate points application code 42 array references uniform reference sets every array reference expressed equation 2 f subscript vector access matrix loop index vector c constant vector information stored array reference leaf array node parent loopnest node intermediate codes consider piece code figure 3 loop nest first reference array represented following access matrix aa constant offset vector gamma ca reference array b also represented access matrix b constant offset vector gamma c b definition uniform reference set similar uniformly generated set 3 two references array access matrix differ constant offset vectors two references said belong uniform reference set constructing uniform reference sets array provides efficient way analyzing data reuse said array references uniform reference set data access patterns data reuse characteristics also identifying uniform reference sets allows us capture group reuse easily 43 algorithm reuse analysis following sections use bottomup approach introduce algorithms implementing compiler input access matrix amlambdan uniform reference set array node loopnest node given cache block size bk sz output selfreuse pattern vector gammagammagamma srpn uniform set begin initial selfreuse pattern vector gammagammagamma current loop level clp innermost loop current dimension level cdn highest dimension set index occurring flag iof element access matrix acdn clp break go next lower dimension level cdn lowest dimension iof false set reference temporal reuse level else cdn acdn clp set reference spatial reuse level go next higher loop level clp outermost loop level end figure 4 algorithm 1 selfreuse analysis directed cache polymorphism technique first algorithms analyzing data reuses including selfreuses groupreuses provided uniform reference set subsection 431 selfreuse analysis reuse analysis references array loop nest first constructed several uniform reference sets selfreuses temporal spatial analyzed level uniform set algorithm works access matrix detailed algorithm shown figure 4 algorithm checks loop index variable innermost loop outermost loop see whether occurs subscript expressions references j th loop index variable j occur subscript expres sion reflection access matrix elements j th column 0 means iterations th loop change memory location accessed ie array reference selftemporal reuse j th loop index variable occurs lowest fastest dimension ie th dimension distance contiguous loop iterations checked al gorithm sclp stride clp th loop bk sz given cache block size elmt sz size array elements distance acdn clp sclp two contiguous iterations reference within cache block spatial reuse loop level 432 groupreuse analysis group reuses exist among references uniform reference set grouptemporal reuse occurs different references access data location across loop iterations groupspatial reuse exists different references access cache block different loop iterations algorithm 2 figure 5 exploits simplified version group reuse exists one loop level groupspatial reuse found particular loop level algorithm figure 5 first checks whether level input uniform reference set cs array node loopnest node given cache block size bk sz output groupreuse pattern vector gammagammagammagamma grpn uniform set begin initial groupreuse pattern vector gammagammagammagamma pair constant vectors c1 c1 c2 differ j th element check j th row access matrix find first occurring loop index variable nonzero element starting innermost loop say continue else check k th column access matrix occurs j th dimension lowest dimension array init distakm 0 else grpk 0 else init distakm 0 end figure 5 algorithm 2 groupreuse analysis grouptemporal reuse pairs references reuse level set group spatial reuse otherwise omits current reuse found grouptemporal reuse found loop level element corresponding level groupreuse vector gammagammagamma grpn directly set grouptemporal reuse array uniform reference sets particular loop nest using algorithm 1 algorithm 2 reuse information loop level collected example code subsection 43 references array selfspatial reuse loop level l selftemporal reuse loop level j group reuse loop level j reference array b selfspatial reuse loop level note contrast previous work reuse analysis eg 12 approach simple computes reuse information without solving system equations 44 simulating footprints reuse spaces next step approach transform data reuses real data localities straightforward idea make data cache large enough hold data reuse spaces arrays note data reuse spaces necessary kept cache first reference since reuse data discussed earlier cache interferences significantly affect overall performance nest thus objective technique find nearoptimal cache configuration reduce eliminate majority cache interferences within nest informal definition nearoptimal cache configuration follows definition 1 nearoptimal cache configuration possibly smallest cache size associativity achieves nearoptimal number cache misses increase either cache size associativity configuration deliver significant improvement order figure nearoptimal cache configuration would contain entire reuse space loop nest real cache behavior reuse spaces must made available potential optimizations section provide algorithm simulates exact footprints memory addresses array references reuse spaces suppose given loop index vector array reference particular value expressed follows sa starting address array reference different base address memory address first array element array constant part equation suppose data type size array elements elmt sz depth dimension dimensional bound vectors gamma constant offset vector derived following equation ae integrated coefficients loop index variables suppose access matrix amlambdan cof j derived follows ddk lj ae note equation 3 address array reference particular loop iteration calculated offset layout space array algorithm provided section using formulations simulate footprints array references loop iteration within reuse spaces following two observations give basis simulate reuse spaces observation 1 order realize reuse carried innermost loop one cache block needed array reference observation 2 order realize reuse carried noninnermost loop minimum number cache blocks needed array reference number cache blocks visited loops inner since assumed subscript functions affine array reference patterns reuse space different iterations loop level reuse exactly thus need simulate first iteration loop reuse currently ex ploiting example loop level j loop vector reuse exploiting simulation space defined kj varies lower bound l k upper bound uk algorithm 3 shown figure 6 first calls algorithms 1 2 simulates footprints significant reuse space array particular loop nest footprints marked array bitmap 45 computation optimization cache configurations loop nests input array node loopnest node given cache block size bk sz output arraylevel bitmap footprints begin initial array size ar sz number cache blocks allocate arraylevel bitmap abm size ar sz initial abm zeros initial highest reuse level rs n depth loop nest uniform reference set call algorithm 1 selfreuse analysis call algorithm 2 groupreuse analysis highest reuse level set rs lev urs lev rs lev n references array lonly use lower bound apply equation 3 get reference address f transfer block id bk set array bitmap abm bk else loop indexes varies value j lower bound upper bound references array apply equation 3 get reference address f transfer block id bk set array bitmap abm bk end figure algorithm 3 simulation footprints reuse spaces previous subsections reuse spaces array particular loop nest determined footprints also simulated layout space array array bitmap indicating cache blocks visited iterations reuse spaces applying algorithm 3 discussed earlier phenomena cache interferences disturb reuses prevent array references realizing data localities across loop iterations thus algorithm reduce cache interferences result better data localities within reuse spaces crucial subsection provide loopnest level algorithm explicitly figure display cache interferences among different arrays accessed within loop nest main point approach map reuse space array real memory space time degree conflict number interferences among different arrays cache block stored loopnest level bitmap since selfinterference array already solved algorithm 3 using array bitmap algorithm mainly focuses reducing groupinterference might occur among different arrays wellknown one effective way avoid interferences increase associativity data cache used algorithm based definition nearoptimal cache configuration algorithm tries find smallest data cache smallest associativity achieves significantly reduced cache interferences nearly perfect performance loop nest figure 7 shows detailed algorithm algorithm 4 computes optimizes cache configuration given loop nest algorithm 4 starts cache block size bk sz lower bound eg 16 bytes goes upper bound eg 64 bytes particular bk sz first applies algorithm 3 obtain array bitmap abm array allocates loopnest level bitmap input loopnest node global list arrays declared lower bound block size bk sz lb upper bound block size bk sz ub output optimal cache configurations diff bk sz begin array loop nest call algorithm 3 get array bitmap abm create initial loopnest level bitmap lbm size smallest 2 n size largest array block lbm size array bitmap abm map abm loopnest bitmap lbm relative baseaddress array base addr indicate degree conflict block block id array size base addrlbm abm block id set largest degree conflict lbm set cache set optimal cache conf current cache conf assoc assoc upper bound half number sets current cache set highest value lbm lbm size set cache size assoc lbm size assoc assoc upper bound cache size optimal cache size set optimal cache conf current cache conf give optimal cache conf bk sz doubling bk sz end figure 7 algorithm 4 compute optimize cache configurations loop nests lbm arrays within nest whose size smallest value power 2 greater equal largest array size abms remapped lbm relative array base addresses value bits lbm indicates conflict particular cache block following optimization carried halving size lbm remapping lbm largest value bits lbm also shows smallest cache associativity needed avoid interference corresponding cache block process ended upper bound associavitity met nearoptimal cache configuration block size bk sz computed one smallest cache size well smallest associativity 46 global level cache polymorphism compilerdirected cache polymorphism technique make changes source code instead uses compiler source code parsing generates internal code intermediate format local algorithms global program level algorithm algorithm 5 figure 8 presented subsection obtain directions cache configurations nest program cache reconfiguration mechanisms algorithm first generates intermediate format original code collects global information arrays source code applies algorithm 4 loop nests obtains nearoptimal cache configurations configurations stored cache configuration list ccl loop nest corresponding input source codespd output performance data cache configurations loop nest begin initial cacheconfiguration list ccl use one suif pass generate intermediate code format construct global list arrays declared relative base address loop nest array loop nest construct uniform reference sets references call algorithm 4 optimize cache configurations loop nest store configurations ccl block size activate reconfiguration mechanisms loop nest using configuration ccl output performance data well cache configuration loop nest end figure 8 algorithm 5 global level cache polymorphism define n 8 int annn bnnn f int j k l f figure 9 example arraybased code node ccl nearoptimal cache configurations different block sizes nestlevel optimization done algorithm 5 activates cache reconfiguration mech anisms modified version shade simulator used simulation shade directed use nearoptimal cache configurations ccl loop nest execution performance data loop nest different cache configurations generated output since current cache reconfiguration mechanisms vary cache size cache ways fixed cache block size cache optimization done different fixed cache block sizes means algorithms paper suggest nearoptimal cache configuration loop nest given block size following section experimental results verifying effectiveness technique presented 47 example subsection focus example code figure 9 illustrate compilerdirected cache polymorphism technique works simplicity code contains one nest algorithm 5 starts one suif pass convert source code intermediate code program node one loopnest node loopnest node represented index vector index lower bound vector gamma upper bound vector gamma stride vector gamma within nest arrays b references ar 1 ar 2 ar 3 ar b represented access matrices constant vectors follows 1 1 2 1 3 also global array list generated b array references ar 1 ar 2 grouped one uniform reference set ar 3 put another one array b hand one uniform reference set algorithm 4 invoked starts smallest cache block size bk sz say 16 bytes uses algorithm 3 obtain array bitmap abma array abm b array b bk sz within algorithm 3 first call algorithm 1 algorithm 2 analyze reuse characteristics given array example first uniform set array selfspatial reuse level l grouptemporal reuse level j second uniform set selfspatial reuse level l selftemporal reuse level j reference array b selfspatial reuse level highest level reuse used array algorithm 3 generate abm footprints reuse space assume integer 4 bytes size case abma abm b 128 bits shown follows two abms passed algorithm 3 algorithm 4 turn algorithm 4 creates loopnest bitmap lbm size equal largest array size max abms remaps abma abm b lbm since array relative base address 0 byte array b 2048 determine lbm follows name arrays nests brief description alternate direction integral apsc 17 3 mesoscale hydro model bmcmc 11 3 molecular dynamic water computation tomcatc 9 8 mesh generation arraybased computation vpentac 9 8 nasa ames fortran kernel molecular dynamics water table 1 arraybased benchmarks used experiments maximum value bits lbm indicates number interference among different arrays nest thus least associativity required avoid interference example algorithm 4 starts cache associativity 2 compute nearoptimal cache configuration time size lbm halved lbm remapped resulting associativity reaches upper bound eg 16 outputs smallest cache size smallest associativity nearoptimal configuration block size bk sz example nearoptimal cache configuration 2kb 2way associative cache lbm optimization shown follows following algorithm 4 continues compute nearoptimal cache configurations larger cache block sizes doubling previous block size block size reaches upper bound eg 64 bytes algorithm stops pass nearoptimal configurations different block sizes algorithm 5 receiving configurations algorithm activates shade simulate example code executable cache configurations performance data generated output algorithm 5 5 experiments 51 simulation framework section present simulation results verify effectiveness cdcp technique technique implemented using suif 5 compiler shade 2 eight arraybased benchmarks used simulation work benchmark loop nests dominate overall execution time benchmarks number arrays benchmark number loop nests listed table 1 first objective see cache configurations returned cdcp scheme scheme based exhaustive simulation using shade consider three different block line sizes 16 32 64 bytes note work particularly targeted l1 onchip caches 52 selected cache configurations subsection first apply exhaustive simulation method using shade simulator method original program codes divided set small programs program single nest shade simulates loop nests individually possible l1 data cache configurations within following ranges cache sizes 1k 128k setassociativity 1 way 16 ways block size 16 32 64 bytes number data cache misses used metric comparing performance optimal cache configuration certain cache block size smallest one terms cache size set associativity achieves performance number misses cannot improved number misses cannot reduced 1 increasing cache size andor set associa tivities left portion table 2 shows optimal cache configurations selected shade loop nest different benchmarks well different cache block sizes compilerdirected cache polymorphism technique directly takes original source code suif spd format applies algorithm 5 generate nearoptimal cache configurations loop nest source code instruction simulation configuration op timization thus expected fast finding nearoptimal cache configuration execution engine modified version shade cdcp directly applies cache configurations activate reconfiguration mechanisms dynamically cache configurations determined shown right part table 2 sum table 2 loop nest given benchmark optimal cache configurations shade nearoptimal cache configurations cdcp technique block sizes 16 32 64 bytes given notation 8k4s used indicate 8k bytes 4way set associative cache block size 32 bytes table b means bytes k denotes kilobytes indicates megabytes table 2 observe cdcp ability determine cache capacities byte granularity cases cache configuration determined cdcp less equal one determined exhaustive simulation 53 simulation results two sets cache configurations loop nests given table 2 simulated program level configurations cdcp cache size less 1k simulated 1k cache size parameters unmodified best comparison performance shown cache hit rate instead miss rate figure 10 gives performance comparison shade exhaustive simulation cdcp using block size 16 bytes figure 10 performance comparison cache configurations block size 16 shade vs cdcp see figure 10 benchmarks adic apsc bmcmc wssc results obtained shade cdcp close hand shade outperforms cdcp benchmarks ef luxc tomcatc vpentac cdcp codes shade cdcp adi aps 3 4k2s 4k8s 8k8s 2k16s 4k8s 8k8s bmcm eflux 3 128k16s 128k16s 128k1s 128k8s 256k2s 256k2s 6 128k16s 128k16s 128k1s 128k8s 256k2s 256k2s tomcat 3 128k4s 128k8s 128k1s 64k1s 128k2s 256k2s 6 1k2s 1k4s 2k4s 64b4s 128b4s 256b2s 7 64k4s 128k8s 128k8s 32k4s 64k8s 128k16s tsf 3 4k4s 4k16s 8k4s 4k1s 4k1s 4k1s vpenta 3 1k4s 2k2s 2k8s 256b4s 512b2s 1k2s 5 1k4s 2k4s 4k2s 256b4s 512b2s 1k2s 6 1k2s 2k2s 2k8s 128b8s 256b4s 512b8s 7 1k2s 1k2s 1k16s 64b1s 128b2s 256b4s wss 3 1k2s 1k2s 1k2s 64b2s 128b4s 256b4s 6 1k2s 1k2s 1k2s 32b2s 64b1s 128b2s table 2 cache configurations loop nest benchmarks shade vs cdcp outperforms shade tsfc figures 11 12 show results block sizes 32 64 bytes separately note benchmarks performance difference shade cdcp decreases block size increased 32 64 bytes especially benchmarks adic apsc bmcmc wssc performances two approaches almost benchmarks tsfc vpentac cdcp strategy consistently outperforms shade block size 32 64 bytes exhaustive shade simulation searching range cache sizes 1k 128k explained earlier cdcp constraints come nonstandard cache size obviously use much larger andor much finer granular cache size exhaustive simulation would drastically increase simulation time suitable practice contrast figure performance comparison cache configurations block size 32 shade vs cdcp cdcp strategy determine nearoptimal cache configuration without much increase search time figure 12 performance comparison cache configurations block size 64 shade vs cdcp detailed study break performance comparison loop nest level benchmark apsc figure 13 shows comparison loop nest benchmark different cache block sizes figure 13 loopnest level performance comparison cache configurations aspc shade vs cdcp results loop nest level comparison show cdcp technique effective finding nearoptimal cache configurations loop nests benchmark especially block sizes 32 64 bytes common block sizes used embedded processors since cdcp analysisbased simulationbased expect even desirable codes large input sizes energy perspective cacti power model 10 used compute energy consumption l1 data cache loop nest benchmarks different cache configurations listed table 2 use 018 micron technology cache configurations detailed energy consumption figures given table 3 codes shade cdcp adi aps bmcm eflux 6 25730 26661 3750 13233 7955 8213 tomcat 284 275 281 284 275 743 7 94613 188652 251909 96477 219840 570500 tsf vpenta 5 1884 2169 1087 1884 974 983 wss 6 748 738 746 748 276 746 table 3 energy consumption microjoules l1 data cache loop nest benchmarks configurations table 2 shade vs cdcp experimental results conclude strategy generates competitive performance results exhaustive simulation ii general results much lower power consumption configuration selected exhaustive simulation consequently approach strikes balance performance power consumption 6 conclusions future work paper propose new technique compilerdirected cache polymorphism optimizing data locality arraybased embedded applications keeping energy consumption control contrast many previous tech energy estimation available cacti due small cache configuration niques modify given code fixed cache architec ture technique based modifying reconfiguring cache architecture dynamically loop nests presented set algorithms collectively allow us select nearoptimal cache configuration nest given application experimental results obtained using set arrayintensive applications reveal approach generates competitive performance results consumes much less energy compared exhaustive simulation based framework plan extend work several direc tions first would like perform experiments different sets applications second intend use cache polymorphism granularities smaller loop nests finally would like combine cdcp loopdata based compiler optimizations optimize hardware software coordinated manner 7 r selective cache ways ondemand cache resource allocation shade fast instructionset simulator execution profiling strategies cache local memory management global program transformation cache miss equations analytical representation cache misses stanford compiler group morphable cache architectures potential benefits improving data locality loop transformations reconfigurable caches application media processing integrated cache timing power model cache interference phenomena data locality optimizing algorithm tr strategies cache local memory management global program transformation data locality optimizing algorithm shade fast instructionset simulator execution profiling cache interference phenomena improving data locality loop transformations cache miss equations selective cache ways reconfigurable caches application media processing morphable cache architectures ctr min zhao bruce childers mary lou soffa predicting impact optimizations embedded systems acm sigplan notices v38 n7 july min zhao bruce r childers mary lou soffa modelbased framework approach profitdriven optimization proceedings international symposium code generation optimization p317327 march 2023 2005 min zhao bruce r childers mary lou soffa approach toward profitdriven optimization acm transactions architecture code optimization taco v3 n3 p231262 september 2006