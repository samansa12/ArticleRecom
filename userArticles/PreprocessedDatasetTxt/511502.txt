evaluating strategies similarity search web finding pages web similar query page related pages important component modern search engines variety strategies proposed answering related pages queries comparative evaluation user studies expensive especially large strategy spaces must searched eg tuning parameters present technique automatically evaluating strategies using web hierarchies open directory place user feedback apply evaluation methodology mix document representation strategies including use text anchortext links discuss relative advantages disadvantages various approaches examined finally describe efficiently construct similarity index chosen strategies provide sample results index b introduction goal webpage similarity search allow users nd web pages similar query page 12 partic ular given query document similaritysearch algorithm work supported national science foundation grant iis0085896 supported nsf graduate research fellowship z supported nsf grant iis0118173 microsoft research graduate fellowship x supported nsf graduate research fellowship copyright held authorowners www2002 may 711 2002 honolulu hawaii usa acm 1581134495020005 provide ranked listing documents similar document given small number similaritysearch strategies one might imagine comparing relative quality user feedback however user studies signicant cost time resources moreover instead comparing small number options interested comparing parametrized methods large parameter spaces number strategies quickly exceed evaluated using user studies situation extremely desirable automate strategy comparisons parameter selection best parameters result accurate ranked similarity listings arbitrary query doc uments paper develop automated evaluation methodology determine optimal document representation strategy particular view manually constructed directories yahoo 26 open directory project odp 21 kind precompiled user study evaluation methodology uses notion document similarity implicitly encoded hierarchical directories induce correct ground truth orderings documents similarity given query document using statistical measure 13 compare similarity rankings obtained dierent parameter settings algorithm correct rankings underlying assumption parameter settings yield higher values measure correspond parameters produce better results demonstrate evaluation methodology applied reasonably sized set parameter settings includ ing choices document representation term weighting schemes determined eective similarity search web many possible ways represent document purpose supporting eective similarity search following brie describes representation axes considered use evaluation methodology described three approaches selecting terms include vector equivalently multiset representing web page u 1 words appearing u contentbased approach 2 document identiers eg urls document v links u linkbased approach 3 words appearing inside near anchor v anchor links u anchorbased approach usual contentbased approach ignores available hyperlink data susceptible spam particular relies solely information provided pages author ignoring opinions authors web pages 3 linkbased approach investigated 12 suers shortcoming pages inlinks sucient citation data either allowed queries appear results queries problem especially pronounced attempting discover similarity relations new pages yet cited su ciently see section 5 linkbased ap proach vectors documents even related ones fact orthogonal third approach relies text near anchors referred anchorwindow 9 appears useful web similaritysearch task indeed use anchorwindows previously considered variety web ir tasks 2 1 9 11 anchorwindow often constitutes handbuilt summary target document 1 collecting explicit handsummarization implicit handclassication present referring documents expect aggregating inlinks frequency relevant terms dominate frequency irrelevant ones thus resulting distribution expected signature reliable concise representation document anchorwindow contributes several terms anchorbased strategy requires fewer citations linkbased strategy prevent interdocument orthogonality however result reducing orthogonal ity anchorbased strategy nontrivial implement efciently 14 discuss later previously established highdimensional similaritysearch technique based hashing used eciently implement anchorbased strategy three general strategies document representation involve additional specic considerations term weighting width anchorwindows discuss section 3 note many additional parameters could considered weighting schemes font sizes font types titles etc goal search parameter space exhaustively rather chose reasonable set parameters present evaluation methodology obtain insight qualitative eects basic parameters best parameters including choice document representation term weighting schemes determined using evaluation methodology must scale similarity measure build similarity index web whole develop indexing approach relying minhashing technique 10 5 construct similaritysearch index roughly 75 million urls demonstrate scalability approach stage algorithm trivially parallelizable indexing approach scale billion accessible documents currently web 1 2 evaluation methodology quality rankings returned system determined similarity metric document features commercial search engines generally several hundreds even thousands machines disposal used previous work 12 relied user studies assess query response quality however user studies time consuming costly wellsuited research involves comparison many parameters instead use automated method evaluation uses orderings implicit humanbuilt hierarchical directories improve quality systems rankings clustering literature numerous methods automatic evaluation proposed 17 steinback et al 25 divide methods two broad classes internal quality measures average pairwise document simi larity indicate quality proposed cluster set based purely internal cluster geometry statistics without reference ground truth external quality mea sures entropy measures test accordance cluster set ground truth primarily investigating various feature selection methods similarity metrics work restrict attention external measures overall outline evaluation method follows use hierarchical directory induce sets correct ground truth similarity orderings compare orderings produced similarity measure using particular set parameters correct partial orderings using statistical measure outlined claim parameter settings similarity measure yield higher values statistical measure correspond parameters produce better results standpoint user system 21 finding ground truth ordering unfortunately available ground truth form either exact documentdocument similarity values correct similarity search results problem 1 similardocument notion similar formalize notion similarity web documents using external quality measure great deal ordering information implicit hierarchical web directories mentioned example document recreationaviationunpowered class average similar documents class outside class furthermore document likely similar documents recreationaviation classes entirely outside region tree intuitively similar documents source documents sources class followed sibling classes certainly cases location hierarchy accurately ect document similarity consider documents recreationautos almost certainly similar shoppingautos recreationsmoking sample cases affect evaluation criteria since average statistics many documents formalize notion distance source document another document hierarchy dene familial distance definition 1 let familial distance f source document another document class hierarchy distance ss class specic class dominating 2 treated hierarchy tree ignoring softlinks denoted sux unrelated documents query document class documents sibling class documents cousin class documents document hierarchy figure 1 mapping hierarchy onto partial order ing given source document system however collapsed directory xed depth three ignored relatively documents depth therefore four possible values familial distance depicted figure 1 name distances follows distance 0 documents class distance 1 siblings documents sibling classes distance 2 cousins documents classes rst cousins distance 3 unrelated lowest common ancestor documents classes root given source document wish use familial distances documents construct partial similarity ordering documents general principle average true similarity documents source document decreases monotonically familial distance document given principle denition familial distance source document hierarchical directory derive partial ordering documents direc tory note give numerical interpretation familial distance values depend stated monotonicity principle source document average similar sameclass document siblingclass document average similar siblingclass document cousinclass document definition 2 let familial ordering f documents respect source document f fa bj f f bg ordering weak given source pairs documents comparable majority distinctions made however among documents similar source documents much less similar notion correct total similarity ordering somewhat suspect beyond certain point pages simply unrelated familial ordering makes distinctions documents distant category forms bulk documents repository course principle true similarity decreases monotonically familial distance always hold however reasonable expect average ranking system 3 accords better familial ordering better one accords less closely 22 comparing orderings point derived partial ordering given hierarchical directory query source document belongs hierarchy wish use partial ordering evaluate correctness almost total ordering produced system 4 perhaps common method comparing two rankings spearman rank correlation coecient measure best suited comparing rankings ties value corresponds pearson coecient 24 two main problems using spearman correlation coecient present work first mentioned tremendous number ties one rankings namely ground truth ranking second since concerned certain regions rankings others eg top would like natural way measure directly many important ranking choices made correctly given goals natural measure kruskalgoodman 13 definition 3 orderings b b intuitively certain number document pairs given ordering makes judgments pairs comparing two orderings look pairs documents orderings make judgment value 1 perfect accord 0 expected value random ordering 1 indicates perfect reversed accord claim two rankings b dier values respect ground truth ordering higher better ranking 23 regions orderings thus given directory query document similarity measure sim construct two orderings documents directory ground truth familial ordering ordering induced similarity measure sims calculate corresponding value value gives us measure quality ranking query document respect similarity measure directory however need give sense good rankings across query docu ments principle directly extend statistic follows iterate documents aggregating concordant discordant pairs dividing total number pairs order precisely evaluate results however calculated three partial values emphasized dier ent regions ordering partial based fraction correct comparable pairs certain type types 3 course ranking system cannot make use directory statement hold 4 ordering produces ties two documents 1 2 exactly similarity source document happens nearly always orthogonal 1 2 similarity 0 source document httpwwwaabgaorg source title american assoc botanical gardens arboreta source category homegardensclubsandassociations settings window size 32 stem dist term weighting rank sim category homegardensclubsandassociations homegardensclubsandassociations homegardensclubsandassociations homegardensclubsandassociations 50 007 homegardensplants 100 006 homeapartmentlivinggardening settings window size 0 stem term weighting rank sim category homegardensclubsandassociations businessindustriesconstructionandmaintenance recreationtravelreservations 50 013 recreationtravelreservations 100 013 businessindustriesconstructionandmaintenance figure 2 orderings obtained two dierent parameter settings respect source document contrast give best worst settings document shown give rank similarity source document category omit url document calculated pairs documents d1 d2 d1 class source document d2 sibling class calculated pairs documents d1 d2 d1 class source document d2 cousin class calculated pairs documents class source document d2 unrelated class partial values allowed us inspect various similarity measures performed various regions rankings example sibling performance indicates well ne distinctions made near top familial ranking unrelated performance measures well coarser distinctions made unrelated unusually low relation sibling also good indicator situations top list highquality precision standpoint many similar documents ranked low therefore omitted top list almost always features sparse documents actually similar appeared orthogonal figure 2 show example ects assumption larger values statistic correspond parameter settings yield better results 3 document representation section discuss specic document representation term weighting options chose evaluate using technique outlined let web document u represented bag w u terms used representing u eg terms found content anchorwindows u links u corresponding weights remains discuss words placed documents bag weight 31 choosing terms content anchorbased approaches chose remove html comments javascript code tags except alt text nonalphabetic characters stopword list containing roughly 800 terms also applied anchorbased approach must also decide many words left right anchor avu anchor linking page v page u included bu experimented three strategies decision cases anchortext avu included well title document u three strategies follow basic choose xed window size w always include w words left w words right avu 5 specically use w 2 f0 4 8 16 32g syntactic use sentence paragraph htmlregion detection techniques dynamically bound region around avu gets included bu primary document features capable triggering window cuto paragraph boundaries table cell bound aries list item boundaries hard breaks follow sentence boundaries technique resulted narrow windows averaged close 3 words either direction topical use simple technique guessing topic boundaries bound region gets included primary features trigger bounding heading beginnings list ends table ends particularly common case handled windows documents composed several regions beginning descriptive header consisting list urls topic header regions found topical heuristics averaged 21 words size either side anchor 32 stemming terms explored eect three dierent stemming variations nostem term left appears stoplist dropped stem term stemmed using porters well known stemming algorithm 22 remove word endings stemmed version term appears stemmed version stoplist dropped stopstem term stemmed purposes checking whether term stem stoplist term dropped otherwise original unstemmed term added bag stopstem variant benecial case usefulness term determined properties stem accurately properties term 5 stopwords get counted determining window cuto 33 term weighting consideration generating document bags terms frequency scaled clear benet tfidf family weighting functions attenuate weight terms high document frequency monotonic term weighting schemes however amplify weight terms low document frequency amplication fact good adhoc queries rare term query given importance case judging document similarities rare terms much less useful often typos rare names nontopical terms adversely aect similarity measure therefore also experimented nonmonotonic termweighting schemes attenuate high low documentfrequency terms idea midfrequency terms greatest resolving power new 23 20 call schemes nonmonotonic document frequency nmdf functions another component term weighting consider substantial impact quality metric distance weighting using anchorbased approach given window size instead treating terms near anchor avu equally weight based distance anchor anchorwords given distance 0 see section 5 use distancebased attenuation function conjunction large anchor windows signicantly improves results evaluation measure 4 document similarity metric metric use measuring similarity document bags jaccard coecient jaccard coecient two sets b dened simj previous section explained represent web documents using bags ie multisets purposes paper extend jaccard sets bags applying bag union bag intersection done taking max min multiplicity terms union intersection operations respectively reasons focus jaccard measure rather classical cosine measure mainly scalability con siderations scaling similaritysearch technique massive document datasets rely minhashing tech nique main idea hash web documents documents similar according similarity measure mapped bucket probability equal similarity creating hash function cosine measure knowledge open problem hand creating hashes possible jaccard measure see 5 used evaluation methodology verify jaccard coecient cosine measure yield comparable results 6 evidence intuitive appeal measure provided 19 jaccard coecient outperforms competitor measures task dening similarities words note bulk work presented depend whether jaccard cosine 6 omit description experiments focus work used section 7 require use jaccard coecient 5 experimental results parameter evaluation evaluating various strategies discussed section 3 employ methodology described section 2 sampled open directory 21 get 300 pairs clusters third level hierarchy depicted previously figure 1 7 source data used web crawl stanford webbase containing 42 million pages 15 urls sample clusters 51469 linked document crawl could thus used anchorbased approaches testset urls linked close 1 million pages repository used support anchor based strategy studied 8 section describes evaluation strategies suggested section 3 veried three measures yield exceptions relative order parameter settings sense agreement indication robustness measures report results sibling statistic graphs cousins unrelated measures behave similarly graphs shown section dierence scores dierent parameter settings might seem quite small ie second decimal digit notice however graph explore eect single parameter di mension independently add eect parameter dimensions dierence becomes substantial 51 results choosing terms values bags generated using various anchorwindow sizes using topical syntactic window bounding using purely links using purely page contents given figure 3 results anchorbased approach using large windows provides best results according evaluation criteria may seem counterintuitive taking small windows around anchor would expect fewer spurious words present documents bag providing concise representation experiments revealed fact larger windows provide benet figure 4 shows fraction document pairs within open directory cluster orthogonal ie common words given representation see smaller window sizes many documents considered similar fact orthogonal case amount reweighting scaling improve results representations simply provide enough accessible similarity information orthogonal pairs also see content link approaches documents cluster largely orthogonal linkbased approach documents within cluster pairwise orthogonal revealing serious limitation purely linkbased approach incoming links thought opaque descrip tors two pages many inlinks intersection 7 urls present third level collapsed third level ancestor category 8 odp pages course excluded data set avoid bias furthermore high orthogonality gures linkbased approach shown figure 4 show partial odp mirrors could signicant impact results contents links figure 3 document representations larger xed anchor windows always gave better results topical dynamic windows achieved similar results shorter average window size0103050709w32 w16 w8 w4 w0 semantic syntactic contents links fraction pairs orthogonal figure 4 intracluster orthogonality various anchor window types small windows pure links resulted document bags largely thogonal making similarity hard determine inlinks empty say little two pages 9 may discuss topic new never cocited case anchorwindowbased approach chance bags two pages orthogonal much lower inlink instead represented single opaque url represented descriptive terms constituents inlink note pure link based approach shown similar cocitation algorithm 12 10 also experimented dynamically sized syntactic topical windows described section 3 window types behave roughly according average window size values orthogonality surprisingly although dynamicwindow heuristics appeared effective isolating desired regions increase region quality overwhelmed trend larger windows pro 9 using svd could potentially glean information pure link approach despite orthogonality assuming enough linkage 12 furthermore veried cocitation algorithm described 12 yields similar scores scores links strategy shown above0428043204360440 anchorwindow content links anchorwindow content anchorwindow figure 5 hybrid bag types adding documents contents gave better results anchor windows alone though adding link ids lowered gamma viding better results 11 addition varying window size also choose include terms multiple types anchor content links described section 3 document representation figure 5 shows combining content anchorbased bags improve sibling score 12 intuition variation particular document incoming links documents contents dominate bags otherwise document many incoming links anchorwindowbased terms dominate way documents bag automatically depend much information available 52 results term weighting previous section saw anchorbased approach large windows performs best initial intuition however smaller windows would provide concise representation completely without merit fact improve performance substantially evaluation criteria weighting terms based distance anchor prevent falling trap making similar documents appear orthogonal small windows time giving spurious terms much weight large windows figure 6 shows results term weights scaled log results frequency based weighting shown figure 7 suggest attenuating terms low document frequency addition attenuating terms high document frequency usually done increase perfor mance let tf terms frequency bag df terms overall document frequency figure 7 log refers weighting tf refers weighting tf df nmdf refers weighting logscale gaussian tf e 1 logdf see figure 8 53 results stemming 11 however gap substantially closed high inlink pages values figure 5 generated distancebased weighting scheme described distance frequency distance frequency none figure frequency distance weighting improved results improved results combined043045047nmdf sqrt log none figure 7 types frequency weighting sqrt gave best results monotonic frequency weighting schemes nmdf gave slightly better results investigate eects three stemming ap proaches figure 9 shows sibling values nos tem stopstem stem parameter settings see stopstem improves value stem provides additional although much less statistically signicant 13 improvement mentioned section 32 eect stopstem nostem increase eective reach stopword list words detected stopwords yet share stem another word detected stopword removed small additional impact stem stopstem due collapsing word variants single term 6 scaling large repositories assume selected parameters maximize quality similarity measure explained section 2 discuss eciently nd similar documents web whole 13 nostem stopstem stem stopstem average dierences approximate magnitude however pairwise variance stemstoptem extremely high comparison document frequency figure 8 nonmonotonic document frequency nmdf weighting039041043nostem stopstem stem figure 9 stemming variants stemming gave best results definition 4 two documents similar jaccard coecient bags greater problem 2 similardocument eciency consider preprocess repository web w query webdocument q w web documents w similar q found eciently section develop scalable algorithm called dexallsimilar solve problem realistic web repository size tackling problem 2 tradeo work required preprocessing stage work required query time nd documents similar q explored two approaches note since q chosen w queries known advance using property showed previous work 14 efciently precompute store answers possible queries case preprocessing stage compute intensive query processing trivial disk lookup alternative strategy discuss detail section builds specialized index preprocessing delays similarity computation query time describe index compact generated eciently allowing us scale large repositories modest hardware resources furthermore computation required query time reasonable web bag bags repository fragments merging parsing signature extraction inverted index index h index preprocessing query processing mhsignatures query processing figure 10 schematic view approach schematic view indexallsimilar algorithm shown figure 10 next two sections explain indexallsimilar two stage algorithm rst stage generate bags web document reposi tory second stage generate vector signatures known minhash signatures bag index allow ecient retrieval document ids given signa tures signatures given document ids 61 bag generation explained previous sections bag document contains words content text document ii anchorwindows documents point bag generation algorithm scans web repository produces bag fragments doc ument document one content bag fragment possibly many anchor bag fragments bag fragments generated sort collapse form bags urls apply nmdf scaling discussed section 33 nally normalize frequencies sum constant 62 generation document similarity description document similarity index dsi resented bag words w words found content anchor text document f corresponding normalized frequencies scaling nmdf function exists family h hash functions see 7 pair documents u v p hash function h chosen random family h simj u v jaccard similarity two documents bags family h dened imposing random order set words representing url u lowest rank accord ing random order element bu practice quite inecient generate fully random permutation words therefore broder et al 7 use family random linear functions form use approach see broder et al 6 indyk 16 theoretical background technique based property compute bag vector minhash signatures mhsignatures value ith mhsignature two documents indicates similar documents particular generate vector mhu mhsignatures document u algorithm processquery input query document q output similar documents fetch mhvector q j 1 iterate mhq documents jth mhsignature q docu 2 ijmhq simdocu sort set docids fdoc g sim scores simdoc output figure 11 query processing expected fraction positions two documents share mhsignatures equal jaccard similarity document bags generate two data structures disk rst h consecutively stores mhu document u ie 4byte mhsignatures document since document ids consecutively assigned fetching signatures document given document id requires exactly 1 disk seek appropriate oset h followed sequential read 4byte signatures second structure generated inverting rst position j mhvector mhsignature h appears position j mhvector ijh list containing ids every document u mhu algorithm retrieving ranked list documents similar query document q using indexes h given figure 11 constructing indexes h choice needed ensure whp documents similar query document retrieved processquery depends solely particular shown 7 choice independent number documents well size lexicon since found previous experiments documents within open directory category similarity least 015 chose safely choose value 10 14 7 experimental results employed strategies produced best values see section 5 conjunction scalable algorithm described see section 6 run experiment sizable web repository particular used anchorwindows distance frequency term weighting stemming content terms included provide description dataset behavior algorithms well examples results obtained 71 efficiency results latest stanford webbase repository contains roughly million pages crawl performed january 2001 large scale experiment used 45 million page subset generated bags 75 million urls merging bag fragments generated 80 mhsignatures 14 chose heuristically properties web whole dier open directory given additional resources decreasing increasing would appropriate algorithm step time generation bag fragments 24 hours merging anchorbag fragments 8 hours mhsignature generation 22 hours query processing 3 seconds type data space web repository 45m pagescompressed 100 gb merged bags 42 gb mhsignatures h 24 gb inverted mhsignatures ltered 5 gb figure 12 timing results space usage 4 bytes long 75 million document bags three machines amdk6 550mhz used process web repository parallel produce bag fragments subsequent steps merging fragments mhsignature generation query processing took place dual pentiumiii 933 mhz 2 gb main memory timing results various stages index sizes given gure 12 query processing step dominated cost accessing smaller ondisk indexes improve performance ltered remove urls low indegree 3 fewer inlinks note urls remain h urls appear queries simply appear results course slight increase query time given resources need ltered way also note maintained wholly mainmemory partitioning across several machines instance query processing time drops fraction second 72 quality retrieved documents accurate comparisons existing search engines dif cult since one needs make sure systems use web document collection found however related pages functionality commercial search engines often return navigationally opposed topically similar results instance wwwmsncom criteria similar moneycentralmsncom part microsoft msn however former would useful result someone looking nancial sites claim use evaluation methodology led us use strategies ect notion similarity embodied popular odp directory illustration provided sample queries gure 13 gure 14 given top 10 words weight bags query urls 15 8 related work relevant work algorithms lated pages functionality provided several major search engines unfortunately details algorithms publicly available dean et al 12 propose algorithms discussed sections 1 51 nding related pages based connectivity web text pages idea using hyperlink text document representation exploited past attack variety ir problems 1 3 8 9 11 18 15 display terms unstemmed commonly occurring variant novelty paper however consists fact make priori assumption best features document representation rather develop evaluation methodology allows us select best features among set dierent candidates approaches algorithmically related ones presented section 6 used 7 4 although dierent problem identifying mirror pages 9 acknowledgments would like thank professor chris manning professor je ullman mayur datar insights invaluable feedback 10 r using common hypertext links identify best phrasal description target web documents categorization context anatomy largescale hypertextual web search engine filtering nearduplicate documents resemblance containment documents syntactic clustering web enhanced hypertext categorization using hyperlinks automatic resource compilation analyzing hyperlink structure associated text finding interesting associations without support pruning topical locality web finding related pages world wide web measures association cross classi scalable techniques clustering web repository web pages small minwise independent family hash functions data clustering review authoritative sources hyperlinked environment measures distributional similarity automatic creation literature abstracts open directory project odp algorithm sux stripping introduction modern information retrieval nonparametric statistics behavioral sciences comparison document clustering techniques tr enhanced hypertext categorization using hyperlinks minwise independent permutations extended abstract syntactic clustering web automatic resource compilation analyzing hyperlink structure associated text anatomy largescale hypertextual web search engine finding related pages world wide web authoritative sources hyperlinked environment data clustering topical locality web introduction modern information retrieval resemblance containment documents ctr ullas nambiar subbarao kambhampati answering imprecise database queries novel approach proceedings 5th acm international workshop web information data management november 0708 2003 new orleans louisiana usa ullas nambiar subbarao kambhampati providing ranked relevant results web database queries proceedings 13th international world wide web conference alternate track papers posters may 1921 2004 new york ny usa ana g maguitman filippo menczer heather roinestad alessandro vespignani algorithmic detection semantic similarity proceedings 14th international conference world wide web may 1014 2005 chiba japan kumiko tanakaishii hiroshi nakagawa multilingual usage consultation tool based internet searching search engine less qa proceedings 14th international conference world wide web may 1014 2005 chiba japan gang luo chunqiang tang yingli tian answering relationship queries web proceedings 16th international conference world wide web may 0812 2007 banff alberta canada wang dazhen chen yuhui nearreplicas web pages detection efficient algorithm based single md5 fingerprint proceedings 8th conference 8th wseas international conference automation information p318320 june 1921 2007 vancouver british columbia canada dmitri roussinov leon j zhao weiguo fan mining context specific similarity relationships using world wide web proceedings conference human language technology empirical methods natural language processing p499506 october 0608 2005 vancouver british columbia canada ullas nambiar subbarao kambhampati mining approximate functional dependencies concept similarities answer imprecise queries proceedings 7th international workshop web databases colocated acm sigmodpods 2004 june 1718 2004 paris france jaroslav pokorny web searching information retrieval computing science engineering v6 n4 p4348 july 2004 ya zhang chaohsien chu xiang ji hongyuan zha correlating summarization multisource news kway graph biclustering acm sigkdd explorations newsletter v6 n2 p3442 december 2004 ronald fagin ravi kumar mohammad mahdian sivakumar erik vee comparing aggregating rankings ties proceedings twentythird acm sigmodsigactsigart symposium principles database systems june 1416 2004 paris france moiss g de carvalho marcos andr gonalves alberto h f laender altigran da silva learning deduplicate proceedings 6th acmieeecs joint conference digital libraries june 1115 2006 chapel hill nc usa quanzhi li yifang brook wu people search searching people sharing similar interests web journal american society information science technology v59 n1 p111125 january 2008 carlo bellettini alessandro marchetto andrea trentini webuml reverse engineering web applications proceedings 2004 acm symposium applied computing march 1417 2004 nicosia cyprus baoning wu vinay goel brian davison topical trustrank using topicality combat web spam proceedings 15th international conference world wide web may 2326 2006 edinburgh scotland sariel harpeled vladlen koltun dezhen song ken goldberg efficient algorithms shared camera control proceedings nineteenth annual symposium computational geometry june 0810 2003 san diego california usa dniel fogaras balzs rcz scaling linkbased similarity search proceedings 14th international conference world wide web may 1014 2005 chiba japan eirinaki vazirgiannis varlamis sewep using site semantics taxonomy enhance web personalization process proceedings ninth acm sigkdd international conference knowledge discovery data mining august 2427 2003 washington dc steven beitzel eric c jensen abdur chowdhury david grossman using titles category names editordriven taxonomies automatic evaluation proceedings twelfth international conference information knowledge management november 0308 2003 new orleans la usa filippo menczer mapping semantics web text links ieee internet computing v9 n3 p2736 may 2005 maria halkidi benjamin nguyen iraklis varlamis michalis vazirgiannis thesus organizing web document collections based link semantics vldb journal international journal large data bases v12 n4 p320332 november junghoo cho hector garciamolina taher haveliwala wang lam andreas paepcke sriram raghavan gary wesley stanford webbase components applications acm transactions internet technology toit v6 n2 p153186 may 2006 mayank bawa tyson condie prasanna ganesan lsh forest selftuning indexes similarity search proceedings 14th international conference world wide web may 1014 2005 chiba japan gurmeet singh manku arvind jain anish das sarma detecting nearduplicates web crawling proceedings 16th international conference world wide web may 0812 2007 banff alberta canada ping li kenneth w church sketch algorithm estimating twoway multiway associations computational linguistics v33 n3 p305354 september 2007 p ferragina gulli personalized search engine based websnippet hierarchical clustering softwarepractice experience v38 n2 p189225 february 2008