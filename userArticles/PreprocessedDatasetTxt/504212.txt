analysis comparison two general sparse solvers distributed memory computers paper provides comprehensive study comparison two stateoftheart direct solvers large sparse sets linear equations largescale distributedmemory computers one multifrontal solver called mumps supernodal solver called superlu describe main algorithmic features two solvers compare performance characteristics respect uniprocessor speed interprocessor communication memory requirements solvers preorderings numerical stability sparsity play important role achieving high parallel efficiency analyse results various ordering algorithms performance analysis based data obtained runs 512processor cray t3e using set matrices real applications also use regular 3d grid problems study scalability two solvers b introduction consider direct solution sparse linear equations distributed memory computers communication message passing normally using mpi study detail two stateoftheart solvers mumps amestoy duff lexcellent koster 1999 amestoy duff lexcellent 2000 superlu li demmel 1999 first uses multifrontal approach dynamic pivoting stability second based supernodal technique static pivoting iterative refinement discuss detailed algorithms used two codes section 3 two important factors affecting performance codes use preprocessing preorder matrix diagonal entries large relative offdiagonals strategy used compute ordering rows columns matrix preserve sparsity discuss aspects detail section 4 compare performance two codes section 5 show comparison fraught difficulties even though authors codes involved study section 6 regular grids problems used illustrate analyse difference two approaches originally planned comparison sparse codes given difficulties found assessing codes know well moment shelved ambitious project however feel lessons learned present exercise invaluable us future wider study given us insight behaviour sparse direct codes feel useful share wider audience stage addition valuable information comparative merits multifrontal versus supernodal approaches examined parameter space comparison exercise identified several key parameters influence differing degree two approaches test environment throughout paper use set test problems evaluate performance algorithms test matrices come forthcoming rutherfordboeing sparse matrix collection duff grimes lewis 1997 1 industrial partners project 2 tim davis collection 3 sparsekit2 4 eecs department uc berkeley 5 parasol test matrices available parallab bergen norway 6 two smaller matrices garon2 lnsp3937 included set matrices used section 41 illustrate differences numerical behaviour two solvers note experiments consider symmetric matrices test set superlu cannot exploit symmetry unable produce ldl factorization however since test examples section 6 symmetric web page httpwwwcseclrcacukactivitysparsematrices project 20160 3 web page httpwwwciseufledudavissparse 4 web page httpmathnistgovmatrixmarketdatasparskit 5 matrix ecl32 included rutherfordboeing collection 6 web page httpwwwparallabuibnoparasol real unsymmetric assembled rua matrix name order entries bbmat 38744 1771722 054 rutherfordboeing cfd department uc berkeley garon2 13535 390607 100 davis collection cfd lhr71c 70304 1528092 000 davis collection chem eng rutherfordboeing cfd mixtank 29957 1995041 100 parasol polyflow sa collection cfd twotone 120750 1224224 028 rutherfordboeing circuit sim wang4 26068 177196 100 rutherfordboeing semiconductor table 21 test matrices strsym number nonzeros matched nonzeros symmetric locations divided total number entries structurally symmetric matrix value 10 show results symmetric unsymmetric factorization versions mumps matrices mixtank invextr1 modified outofrange underflow values matrix files keep sparsity pattern want replace underflow values zeros instead replaced entries exponent smaller 300 numbers mantissa exponent 300 linear system righthand side vector generated true solution vector ones results presented paper obtained cray t3e900 512 dec ev5 processors 256 mbytes memory per processor 900 peak megaflop rate per processor nersc lawrence berkeley national laboratory also refer experiments 35 processor ibm sp2 665 mhertz processor 128 mbytes physical memory 512 mbytes virtual memory 266 peak megaflop rate per processor gmd bonn germany used parasol project performance characteristics two machines listed table 22 computer cray t3e900 ibm sp2 frequency processor 450 mhertz 66 mhertz peak uniproc performance 900 mflops 264 mflops effective uniproc performance 340 mflops 150 mflops peak communication bandwidth 300 mbytessec 36 mbytessec latency bandwidtheffective performance 088 024 table 22 characteristics cray t3e900 ibm sp2 factorization matrix wang4 using mumps used estimate effective uniprocessor performance computers 3 description algorithms used section briefly describe main characteristics algorithms used solvers highlight major differences complete description algorithms reader consult previous papers authors algorithms amestoy et al 1999 amestoy et al 2000 li demmel 1998 li demmel 1999 algorithms described computational tree whose nodes represent computations whose edges represent transfer data case multifrontal method mumps steps gaussian elimination performed dense frontal matrix node schur complement contribution block remains passed assembly parent node case supernodal code superlu distributed memory version uses rightlooking formulation computed factorization block columns corresponding node tree immediately sends data update block columns corresponding ancestors tree codes accept pivotal ordering builtin capability generate ordering based analysis pattern summation performed symbolically however present version mumps symbolic factorization markedly less efficient input ordering given since different logic used case default ordering used mumps approximate minimum degree amd amestoy davis duff 1996a default superlu multiple minimum degree mmd liu 1985 however experiments using minimum degree ordering considered amd ordering since codes generate using subroutine mc47 hsl 2000 usually far quicker mmd produces symbolic factorization close produced mmd also use nested dissection orderings nd sometimes use onmetis ordering metis karypis kumar 1998 sometimes nested dissectionhaloamd ordering scotch pellegrini roman amestoy 1999 depending performs better particular problem addition sometimes beneficial precede ordering performing unsymmetric permutation place large entries diagonal scaling matrix diagonals modulus one offdiagonals modulus less equal one use mc64 code hsl perform preordering scaling duff koster 1999 indicate clearly done effect using preordering matrix discussed detail section 41 finally mc64 used matrices always scaled approaches pivot order defined analysis symbolic factorization stages mumps modulus prospective pivot compared largest modulus entry row accepted greater threshold value typically 0001 01 default value 001 note although mumps choose pivots diagonal largest entry column might unavailable pivoting stage entries row fully summed threshold pivoting strategy common sparse gaussian elimination helps avoid excessive growth size entries matrix factorization directly reduces bound backward error prospective pivot fails test happens kept schur complement passed parent node eventually rows entries column available pivoting root pivot chosen column thus numerical factorization respect threshold criterion cost increasing size frontal matrices causing work fillin forecast superlu approach static pivoting strategy used keep pivotal sequence chosen analysis magnitude potential pivot tested threshold ffl 12 jjajj ffl machine precision jjajj norm less value immediately set value sign modified entry used pivot corresponds halfprecision perturbation original matrix entry result factorization exact iterative refinement may needed note iterative refinement obtained accurate solution cases tested problems still occur extended precision blas li demmel bailey henry hida iskandar kahan kapur martin tung yoo 2000 could used 31 mumps main parallel features parallelism within mumps two levels first uses structure assembly tree exploiting fact computations nodes ancestors descendents independent initial parallelism source tree parallelism number leaf nodes reduces one root second level subdivision elimination operations blocking frontal matrix blocking gives rise node parallelism either rows referred 1dnode parallelism rows columns root referred 2dnode parallelism node parallelism depends size frontal matrix delayed pivots known factorization time therefore determined dynamically tree node assigned processor priori subassignment blocks frontal matrix done dynamically machine dependent parameters mumps control efficiency code designed take account uniprocessor multiprocessor characteristics computers dynamic distributed scheduling approach need precise description performance characteristics computer approaches based static scheduling pastix henon ramet roman 1999 machine dependent parameters mumps associated block sizes involved parallel blocked factorization algorithms dense frontal matrices main objective maintain minimum granularity efficiently exploit potential processor providing sufficient tasks exploit available parallelism target machines differ several respects important ones illustrated table 22 found smaller granularity tasks could used cray t3e ibm sp2 relatively faster rate communication megaflop rate cray t3e ibm sp2 see table 22 say communication relatively efficient cray t3e dynamic scheduling major original feature approach used mumps critical part algorithm process associated tree node decides reassign work corresponding partitioning rows set socalled worker processes call node onedimensional parallel node earlier versions mumps fixed block size used partition rows work distributed processes starting least loaded process load process determined amount work number operations allocated yet processed determined cheaply since block size fixed possible process charge onedimensional parallel node give additional work processes already loaded happen near leaf nodes tree sparsity provides enough parallelism keep processes busy hand insufficient tasks might created provide work idle processes situation likely occur close root tree new algorithm available since version 41 mumps block size onedimensional partitioning dynamically adjusted process charge node early processing tree near leaves gives relatively bigger block size reducing number worker processes whereas close root tree block size automatically reduced compensate lack parallelism assembly tree bound block size partitioning onedimensional parallel node interval lower bound needed maintain minimum task granularity control volume messages upper bound interval less critical default chosen eight times lower bound used estimating maximum size communication buffers factors large dynamic strategy partitioning distributing work onto processors could cause trouble large number processors 128 case quite beneficial take account global information help local decisions example one could restrict choice worker processes set candidate processors determined statically analysis phase notion commonly used design static scheduling algorithms henon et al 1999 could reduce overhead dynamic scheduling algorithm reduce increase communication volume increasing number processors improve local decision tuning parameters controlling block size 1d partitioning would easier estimation memory required factorization would accurate large number processors performance software improvements could thus expected feature available current version 41 mumps implemented future release see adding feature one could address current limitations mumps approach see section 52 solution phase also performed parallel uses asynchronous communications forward elimination back substitution case forward elimination tree processed leaves root similar way factorization back substitution requires different algorithm processes tree root leaves pool readytobeactivated tasks used change distribution factors generated factorization phase hence type 2 3 node parallelism also used solution phase 32 superlu main parallel features superlu also uses two levels parallelism although advantage taken node parallelism blocking supernodes pivotal order fully determined analysis phase assignment blocks processors done statically priori factorization commences 2d blockcyclic layout used execution pipelined since sequence predetermined matrix partitioning based notion unsymmetric supernode introduced demmel eisenstat gilbert li liu 1999 supernode defined matrix factor l supernode range columns l triangular block diagonal full nonzero structure elsewhere either full zero supernode partition used block partition row column dimensions diagonal blocks square n supernodes nbyn matrix n 2 blocks nonuniform size figure 31 illustrates block partition offdiagonal blocks may rectangular need full furthermore columns block u necessarily row structure call dense subvector block u segment p processes also arranged 2d mesh dimension blockcyclic layout mean block j l u mapped onto process coordinate process mesh factorization block li j needed processes process row similarly block ui j needed processes process column partitioning mapping controlled user first user set maximum block size parameter symbolic factorization algorithm identifies supernodes chops large supernodes smaller ones sizes exceed parameter supernodes may smaller parameter due sparsity blocks defined supernode boundaries supernodes smaller maximum block size never larger experience shown good value parameter ibm sp2 around 40 cray t3e around 24 second user set shape process grid 2 theta 3 3 theta 2 square grid better performance expected rule thumb used cray t3e define grid shapes4301204441 00130143 1global matrix5process mesh5 figure 31 2d blockcyclic layout used superlu 2d mapping block column l resides one process namely column processes example figure 31 second block column l resides column processes f1 4g process 1 owns two nonzero blocks contiguous global matrix main numerical kernel involved numerical factorization block update corresponding rankk update schur complement see figure 32 earlier versions superlu computation based level 25 blas call level 2 blas routine gemv matrixvector product multiple vectors segments matrix li k kept cache across multiple calls extent mimics level 3 blas gemm matrixmatrix product performance however difference level 25 level 3 still quite large many machines eg ibm sp2 motivated us modify kernel following way order use level 3 blas best performance distinguish two cases corresponding two shapes ukj block ffl segments ukj height shown figure 32 since nonzero segments stored contiguously memory call gemm directly without performing operations zeros ffl segments ukj different heights shown figure 32 b case first copy segments temporary working array leading zeros padded necessary call gemm using li k instead ukj perform extra floatingpoint operations padding zeros copying incur run time cost data must loaded cache anyway working storage bounded maximum block size tunable parameter example usually use theta 40 ibm sp2 24 theta 24 cray t3e depending matrix level 3 blas kernel improved uniprocessor factorization time 20 40 ibm sp2 performance gain also observed cray t3e clear extra operations well offset benefit efficient level 3 blas routines b uk ai j li k uk j uk figure 32 illustration numerical kernels used superlu current factorization algorithm two limitations parallelism explain examples problems speculate algorithm may improved future following matrix notation zero blocks left blank nonzero block mark box process owns block ffl parallelism sparsity consider matrix 4by4 blocks mapped onto 2by2 process mesh660 1 although node 2 parent node 1 elimination tree associated processes column 2 depend column 1 process 1 depends l block process 0 process 3 could start factorizing column 2 time process 0 factorizing column 1 process 1 starts factorizing column 2 current algorithm requires column processes factorize column synchronously thereby introducing idle time process 3 relax constraint allowing diagonal process 3 case factorize diagonal block send factored block offdiagonal processes using mpi isend even offdiagonal processes ready column would eliminate artificial interprocess dependencies potentially reduce length critical path note kind independence comes sparsity also 2d processtomatrix mapping even interesting study would formalize 2d task dependencies task graph perform optimal scheduling ffl parallelism directed acyclic elimination graphs gilbert liu 1993 often referred elimination dags edags consider another matrix 6by6 blocks mapped onto 2by3 process mesh66664 columns 1 3 independent elimination dags column process sets f0 3g f2 5g could start factorizing columns 1 3 simultaneously however since process 2 also involved update task block 5 associated step 1 algorithm gives precedence tasks step 1 task step 3 process 2 factorize column 3 immediately may change task precedence giving factorization task later step higher priority update tasks previous steps former likely critical path would exploit better task independence coming elimination dags expect improvements large impact sparse andor unsymmetric matrices orderings give wide bushy elimination trees nested dissection triangular solution algorithm also designed around distributed 2d data structure forward substitution proceeds bottom elimination tree root whereas back substitution proceeds root bottom algorithm based sequential variant called inner product formulation execution program completely messagedriven process selfscheduling loop performing appropriate local computation depending type message received entirely asynchronous approach enables large overlap communication computation helps overcome much higher communication computation ratio phase 33 first comments algorithmic differences approaches use level 3 blas perform elimination operations however mumps frontal matrices always square possible zeros frontal matrix especially delayed pivots matrix structure markedly asymmetric present implementation takes advantage sparsity counts measured assume frontal matrix dense shown amestoy puglisi 2000 one detect exploit structural asymmetry frontal matrices new algorithm significant gains memory time perform factorization obtained example using mumps new algorithm number operations factorize matrices lhr71c twotone would reduced 30 37 respectively approach tested shared memory multifrontal code amestoy duff 1993 hsl 2000 however yet available current version mumps superlu advantage taken sparsity blocks usually dense matrix blocks smaller used mumps addition superlu uses sophisticated data structure keep track irregularity sparsity thus uniprocessor megaflop rate superlu much worse mumps performance penalty extent alleviated reduction floatingpoint operations better exploitation sparsity rule thumb mumps tend perform particularly well matrix structure close symmetric superlu better exploit asymmetry note even ordering input two codes computational tree generated case different case mumps assembly tree generated mc47 used drive mumps factorization phase superlu directed acyclic computational graphs dags built implicitly figures 33 34 use vampir trace nagel arnold weber hoppe solchenbach 1996 illustrate typical parallel behaviour approaches traces correspond zoom middle factorization phase matrix bbmat 8 processors cray t3e black areas correspond time spent communications related mpi calls line two processes corresponds one message transfer plots see superlu distinct phases local computation interprocess communication whereas mumps hard distinguish process performs computation transfers message due asynchronous scheduling algorithm used mumps may better chance overlapping communication computation 4 impact preprocessing numerical issues section first study impact solvers preprocessing matrix preprocessing first use row column permutations permute large entries onto diagonal section 41 report compare structural numerical impact preprocessing phase performance accuracy solvers phase symmetric ordering minimum degree nested dissection used study relative influence orderings performance solvers section 42 also comment relative cost analysis phase two solvers 41 use preordering place large entries onto diagonal cost analysis phase duff koster 1999 developed algorithm permuting sparse matrix diagonal entries large relative offdiagonal entries also written computer code mc64 available hsl 2000 implement algorithm use option 5 mc64 maximizes product modulus diagonal entries scales permuted matrix diagonal entries modulus one offdiagonals modulus less equal one importance preordering scaling clear mumps limit amount numerical pivoting factorization increases overall cost factorization superlu expect permutation even crucial reducing amount small pivots modified set 12 jjajj mc64 code duff koster 1999 quite efficient normally require little time relative matrix factorization even latter executed many processors mc64 runs one processor results section show always case moreover matrices unsymmetric symmetric nearly symmetric structure common problem class problem mc64 performs unsymmetric permutation tend destroy symmetry pattern since codes use symmetrized pattern sparsity ordering see section 42 mumps uses one also symbolic numerical factorization overheads markedly unsymmetric pattern high conversely initial matrix unsymmetric example lhr71c unsymmetric permutation may actually help increase structural symmetry thus giving second benefit subsequent matrix factorization show effects using mc64 examples table 41 table 44 illustrate relative cost main steps analysis phase mc64 used preprocess matrix see table 41 unsymmetric matrices lhr71c twotone mc64 really needed mumps superlu factorize matrices efficiently matrices zeros diagonal static pivoting approach used superlu unless zeros made nonzero fillin large enough perturbed process process process process process process process 6 108 108 5 process 7 108 mpi application 905s 90s 895s 89s figure 33 illustration asynchronous behaviour mumps factorization phase process 0 process 1 process process 3 process 4 process 5 process 6 process 7 mpi vtapi comm 932s 93s 928s figure 34 illustration relatively synchronous behaviour superlu factorization phase matrix solver ordering strsym nonzeros flops factors bbmat mumps amd 054 461 415 fidapm11 mumps amd 100 161 97 garon2 mumps amd 100 24 03 mixtank mumps amd 100 391 644 twotone mumps amd 028 2350 12211 wang4 mumps amd 100 116 105 table 41 impact permuting large entries onto diagonal using mc64 size factors number operations estimation given analysis enough memory perform factorization strsym denotes structural symmetry ordering factorization factorization nearby matrix obtained case mumps dramatically higher fillin obtained without mc64 makes also necessary use mc64 mumps main benefit using mc64 structural numerical permuted matrix fact larger structural symmetry see column 4 table 41 symmetric permutation obtained permuted matrix efficient preserving sparsity superlu benefits similar way symmetrization computation symmetric permutation based assumption even superlu preserves better asymmetric structure factors performing symbolic analysis directed acyclic graph exploiting asymmetry factorization phase compare example results mumps superlu matrices lhr71c mixtank twotone matrix iter bbmat table 42 illustration convergence iterative refinement use mc64 also improve quality factors numerical behaviour factorization phase reduce number steps iterative refinement required reduce backward error machine precision illustrated table 42 show number steps iterative refinement required reduce componentwise relative backward error arioli demmel duff 1989 machine precision 22 theta 10 gamma16 cray t3e iterative refinement stop either required accuracy reached convergence rate slow berr decrease least factor two true error reported jjx true jj table illustrates impact use mc64 quality matrix solver without iter ref iterative refinement berr bbmat mumps 74e11 13e06 2 32e16 30e09 lhr71c mumps enough memory superlu enough memory mixtank mumps 19e12 48e09 2 59e16 14e11 twotone mumps 50e07 13e05 3 13e15 21e11 matrix solver without iter ref iterative refinement berr bbmat mumps 12e11 65e08 2 27e16 35e09 lhr71c mumps 11e05 99e00 3 32e13 10e00 mixtank mumps 48e12 23e08 2 42e16 40e11 twotone mumps 32e13 16e10 2 16e15 23e11 table 43 comparison numerical behaviour backward error berr forward error err solvers nb indicates number steps iterative refinement initial solution obtained solvers prior iterative refinement additionally shows thanks numerical partial pivoting initial solution almost always accurate mumps superlu usually markedly observations confirmed larger number test matrices table 43 stopping criterion applied runs runs table 42 case mumps mc64 also result reduction number offdiagonal pivots number delayed pivots example matrix invextr1 number offdiagonal pivots drops 1520 109 number delayed pivots drops 2555 42 one also see table 42 eg bbmat mc64 always improve numerical accuracy solution obtained superlu expected see matrices fairly symmetric pattern eg matrix fidapm11 table 41 use mc64 leads significant decrease symmetry solvers results significant increase number operations factorization additionally recollect time spent mc64 dominate analysis time either solver see table 44 even matrices fidapm11 invextr1 provide gain subsequent steps thus solvers default use mc64 fairly symmetric matrices practice default option mumps package mc64 automatically invoked structural symmetry found less 05 superlu zeros diagonal numerical issues must also considered automatic decision analysis phase difficult finally compare figure 41 time spent two solvers analysis phase reordering based amd mc64 invoked since time spent bbmat ecl32 invextr1 fidapm11 mixtank rma10 wang42610seconds mumps figure 41 time comparison analysis phases mumps superlu mc64 preprocessing used amd ordering used amd similar cases gives good estimation cost difference matrix solver preprocess total mc64 amd bbmat mumps amd 47 30 mc64amd 72 21 31 mixtank mumps amd 32 08 twotone mumps amd 127 87 table 44 influence permuting large entries onto diagonal using mc64 time seconds analysis phase mumps superlu analysis phase two solvers note superlu currently tied specific ordering code take advantage information available ordering algorithm tighter coupling ordering case mumps amd reduce analysis time superlu however analysis phase superlu asymmetric structures needed factorization computed directed acyclic graph gilbert liu 1993 unsymmetric matrix must built mapped onto processors mumps main data structure handled analysis assembly tree produced directly byproduct ordering phase data structures introduced phase dynamic scheduling used factorization simple massage tree partial mapping computational tasks onto processors performed analysis 42 use orderings preserve sparsity matrices mc64 used show table 45 impact choice symmetric permutation fillin floatingpoint operations factorization observed amestoy et al 1999 use nested dissection significantly improve performance mumps see superlu also although lesser extent benefit use nested dissection ordering examine influence ordering performance section 5 also notice orderings superlu exploits asymmetry matrix somewhat better mumps see bbmat structural symmetry 053 expect asymmetry problem better exploited mumps approach described amestoy puglisi 2000 implemented matrix ordering solver nz lu flops bbmat amd mumps 461 415 412 340 nd mumps 358 257 nd mumps 248 209 nd mumps 162 81 mixtank amd mumps 391 644 nd mumps 196 132 table 45 influence symmetric sparsity orderings fillin floatingpoint operations factorization unsymmetric matrices mc64 used 5 performance analysis general matrices 51 performance numerical phases section compare performance study behaviour numerical phases factorization solve two solvers sake clarity report results best terms factorization time sparsity ordering approach best ordering mumps different superlu results orderings provided means results nested dissection minimum degree orderings given illustrate different sensitivity codes choice ordering note even ordering given solver usually perform number operations general superlu performs fewer operations mumps exploits better asymmetry matrix although execution time less mumps level 3 blas effect although results often matrix dependent try much possible identify general properties two solvers point maximum dimension unsymmetric test matrices 120750 see table 21 511 study factorization phase show table 51 factorization time solvers smaller matrices report table 52 results 64 processors observe mumps usually faster superlu significantly small number processors believe two reasons first mumps handles symmetric regular data structures better superlu mumps uses level 3 blas kernels bigger blocks used within superlu result megaflop rate mumps one processor average twice superlu factorization also evident results smaller test problems table 52 results 3d grid problems section 6 note even matrix twotone performs three times fewer operations mumps mumps 25 times faster superlu four processors small number processors also notice superlu always fully benefit reduction number operations due use nested dissection ordering see bbmat superlu using 4 processors furthermore one notice matrices structurally asymmetric superlu much less scalable mumps example matrix lhr71c table 52 speedups 25 83 obtained superlu mumps respectively due two parallel limitations current superlu algorithm described section 32 first superlu fully exploit parallelism elimination dags second pipelining mechanism fully benefit sparsity factors blocked column factorization implemented also explains superlu fully benefit case mumps better balanced tree generated nested dissection ordering see ordering significantly influences performance codes see results matrices bbmat ecl32 particular mumps generally outperforms superlu even large number processors nested dissection ordering used hand use minimum degree ordering superlu faster mumps large number processors also see unsymmetric problems neither solver provides enough parallelism benefit using 128 processors exception matrix ecl32 using amd ordering requiring flops factorization superlu continues decrease factorization time 512 processors lack large unsymmetric systems gives us data points regime one might expect independently ordering 2d distribution used superlu provide better scalability hence eventually better performance large number processors mixed 1d 2d distribution used mumps analyse scalability solvers consider three dimensional regular grid problems section 6 matrix ord solver number processors bbmat amd mumps 457 240 165 137 119 112 91 126 superlu 682 231 133 91 67 57 47 61 58 mixtank nd mumps 408 130 78 56 44 39 42 42 54 twotone mc64 mumps 403 226 186 147 144 143 140 143 table 51 factorization time seconds large test matrices cray t3e indicates enough memory matrix ordering solver number processors fidapm11 amd mumps 316 117 84 65 57 57 lhr71c mc64amd mumps 133 43 29 17 15 16 rma10 amd mumps 81 31 22 21 20 21 wang4 amd mumps 306 111 70 52 43 39 563 194 139 79 58 56 table 52 factorization time seconds small test matrices cray t3e indicates enough memory better understand performance differences observed tables 51 52 identify main characteristics solvers show table 53 average communication volume speed communication depend much number size messages also indicate maximum size messages average number messages overlap communication computation mumps uses fully asynchronous communications sends receives use nonblocking sends synchronous scheduled approach used superlu also enables overlapping communication computation matrix ord solver number processors max vol mess max vol mess max vol mess bbmat amd mumps 49 44 3240 33 63 1700 29 20 2257 nd mumps 22 7 2214 28 43 1441 15 48 3228 fidapm11 amd mumps 25 28 3000 24 22 1471 24 6 1323 mixtank nd mumps 35 twotone mc64 mumps 88 61 5076 29 139 4144 21 table 53 maximum size messages max mbytes average volume communication vol mbytes number messages per processor mess large matrices factorization results table 53 difficult make definitive comment average volume communication overall broadly comparable sometimes mumps sometimes superlu lower volume occasionally significant amount however although average volume messages 64 processors comparable solvers one two orders magnitude difference average number messages therefore average size messages due much larger number messages involved fanout approach superlu compared multifrontal approach mumps note mumps number messages includes messages one integer required dynamic scheduling algorithm update load processes average volume communication per processor solver depends much number processors superlu increasing number processors generally decrease communication volume per processor always case mumps note adding global information local dynamic scheduling algorithm mumps help increase granularity level 2 node subtasks without losing parallelism see section 31 thus result decrease average volume communication large number processors 512 study solve phase already discussed section 41 difference numerical behaviour two solvers showing general superlu involve steps iterative refinement mumps obtain accuracy solution section focus time spent obtain solution apply enough steps iterative refinement ensure componentwise relative backward error berr less p gamma8 step iterative refinement involves forward backward solve also matrixvector product original matrix mumps user provide input matrix general distributed format amestoy et al 1999 functionality used parallelize matrixvector products superlu parallelization matrixvector product easier input matrix duplicated processors table 54 report time perform one solution step using factorized matrix solve necessary berr greater p time improve solution using iterative refinement lines ir superlu except ecl32 mixtank require iterative refinement one step iterative refinement required always enough reduce backward error mumps iterative refinement required matrix invextr1 backward error already close one processor 4 8 processors step iterative refinement required berr initial solution already equal case time reported row ir corresponds time perform computation backward error first observe compare example tables 51 54 small number processors less 8 solve phase almost two orders magnitude less costly factorization large number processors solve phases relatively less scalable factorization phases difference drops one order magnitude applications large number solves might required per factorization could become critical performance might addressed future show solution times smaller matrices table 55 run iterative refinement performance reported tables 54 55 would appear suggest regularity structure matrix factors generated factorization phase mumps responsible faster solve phase superlu 256 processors 512 processors solve phase superlu sometimes faster mumps although cases fastest solve time recorded mumps usually fewer number processors cost iterative refinement significantly increase cost obtaining solution superlu static pivoting likely iterative refinement required obtain accurate solution numerically difficult matrices see bbmat twotone mumps use partial pivoting factorization reduce number matrices iterative refinement required set invextr1 requires iterative refinement solvers use mc64 preprocess matrix also considered reduce number steps iterative refinement even avoid need use cases see section 41 matrix order solver number processors bbmat amd mumps 053 038 031 032 032 036 040 056 twotone mc64 mumps 103 092 097 098 098 103 113 141 table 54 solve time seconds large matrices cray t3e shows time spent improving initial solution using iterative refinement indicates enough memory matrix ord solver number processors table 55 solve time seconds small matrices cray t3e 52 memory usage section study memory used factorization function solver used number processors see table 56 want first point dynamic scheduling approach threshold pivoting used mumps analysis phase cannot fully predict space required processor upper bound therefore used memory allocation static task mapping approach used superlu memory used predicted analysis phase section compare memory actually used solvers factorization phase includes reals integers communication buffers storage initial matrix however included seen amestoy et al 1999 input matrix also provided general distributed format handled efficiently solver option available mumps superlu initial matrix currently duplicated processors 7 matrix ordering solver number processors avg max avg max avg max bbmat amd mumps 147 176 52 nd mumps 114 118 44 53 28 35 43 44 nd mumps 132 139 39 44 25 28 28 17 22 mixtank nd mumps 84 87 29 31 19 21 twotone mc64 mumps 167 180 57 67 42 table memory used factorization megabytes per processor notice table 56 significant reduction memory required increasing number processors also see general superlu usually requires less memory mumps although less apparent many processors used showing better memory scalability mumps one observe little difference 7 mumps note storage reported still includes another internal copy initial matrix distributed arrowhead form necessary assembly operations multifrontal algorithm average maximum memory usage showing algorithms well balanced superlu better two note memory scalability critical globally addressable platforms parallelism increases total memory used purely distributed machines t3e main factor remains memory used per processor allow large problems solved enough processors available 6 performance analysis 3d grid problems analyse understand scalability solvers report section results obtained 11point discretization laplacian operator threedimensional nx ny nz grid problems consider set 3d cubic nxnynz rectangular nx nx4 nx8 grids nested dissection ordering used size grids used number operations timings reported table 61 increasing number processors tried much possible maintain constant number operations per processor keeping much possible shape grids possible satisfy constraints thus number operations per processor completely constant nprocs grid size ldl factorization lu factorization flops time flops time flops time cubic grids nested dissection 4 36 134 199 268 281 268 533 rectangular grids nested dissection 128 208 52 26 2431 274 4858 536 4856 607 table 61 factorization time seconds cray t3e lu factorization performed mumpsuns superlu ldl mumpssym since test matrices symmetric use mumps compute either ldl factorization referred mumpssym lu factorization referred mumpsuns compute lu factorization note given matrix unsymmetric solvers superlu mumpsuns perform roughly twice many operations mumpssym overcome problem number operations per processor nonconstant first report figures 61 62 megaflop rate per processor three approaches cubic rectangular grids respectively context megaflop rate meaningful grid problems number operations almost identical mumpsuns superlu see table 61 thus corresponds absolute performance approach used given problem first notice 8 processors independently grid shape mumpsuns twice fast superlu also much higher megaflop rate mumpssym 128 processors rectangular cubic grids three solvers similar megaflop rates per processor figures 63 64 show parallel efficiency cubic rectangular grids respectively efficiency solver p processors computed ratio megaflop rate per processor p processors megaflop rate 1 processor terms efficiency superlu generally efficient cubic grids mumpsuns even relatively small number processors mumpssym relatively efficient mumpsuns mumpssym efficiency comparable superlu large number processors superlu significantly efficient mumpsuns peak ratio methods reached cubic grids 128 processors superlu three two times efficient mumpsuns mumpssym respectively finally report table 62 quantitative evaluation overhead due parallelism cubic grids using analysis tool vampir nagel et al 1996 rows computation report percentage time spent numerical factorization mpi calls idle time due communications synchronization reported rows overhead table nprocs grid size mumpssym mumpsuns superlunx36 computation 69 76 87 overhead 31 24 13nx46 computation 67 69 75 overhead 33 31 25nx57 computation 50 36 56 overhead 50 64 44 table 62 percentage factorization time cubic grids spent computation overhead due communication synchronization table 62 shows superlu less overhead either version mumps also observe better parallel behaviour mumpssym respect mumpsuns analysed processors rate mumpssym mumpsuns figure 61 megaflop rate per processor cubic grids nested dissection processors rate mumpssym mumpsuns figure 62 megaflop rate per processor rectangular grids nested dissection 040812 processors efficiency mumpssym mumpsuns figure 63 parallel efficiency cubic grids nested dissection 12802061processors efficiency mumpssym mumpsuns figure 64 parallel efficiency rectangular grids nested dissection amestoy et al 2000 mainly due fact node level parallelism provides relatively parallelism symmetric context 7 concluding remarks paper presented detailed analysis comparison two stateofthe art parallel sparse direct solversa multifrontal solver mumps supernodal solver superlu analysis based experiments using massively parallel distributedmemory machinethe cray t3e dozen matrices different applications analysis addresses efficiency solvers many respects including role preordering steps costs accuracy solution sparsity preservation total memory required amount interprocessor communication times factorization triangular solves scalability found solvers strengths weaknesses summarize observations follows ffl solvers benefit numerical preordering scheme implemented mc64 although superlu benefits greater extent mumps mumps helps reduce number offdiagonal pivots number delayed pivots superlu may reduce need small diagonal perturbations number iterative refinements however since permutation asymmetric may destroy structural symmetry original matrix cause fillin operations tends introduce greater performance penalty mumps superlu although recent work amestoy puglisi 2000 might affect conclusion default mumps use mc64 fairly symmetric matrices ffl mumps usually provides better initial solution due effect dynamic versus static pivoting one step iterative refinement superlu usually obtains solution level accuracy ffl solvers accept input fillin reducing ordering applied symmetrically rows columns mumps performs better nested dissection minimum degree exploit better tree parallelism provided nested dissection ordering whereas superlu exploit level parallelism parallel efficiency less sensitive different orderings ffl given ordering superlu preserves sparsity asymmetry l u factors better superlu usually requires less memory mumps smaller numbers processors 64 processors mumps requires 2530 memory average ffl although total volume communication comparable solvers mumps requires many fewer messages especially large numbers processors difference two orders magnitude partly intrinsic algorithms multifrontal versus fanout partly due 1d mumps versus 2d superlu matrix partitioning ffl mumps usually faster factorization solve phases speed penalty partly comes code complexity order preserve irregular sparsity pattern partly due communication messages processors superlu shows better scalability 2d partitioning scheme better job keeping processors busy despite fact introduces messages said introduction started exercise intention comparing wider range sparse codes however demonstrated preceding sections task conducting comparison complex feel though experience gained task useful extending comparisons future following tables summarize major characteristics parallel sparse direct codes aware clear description terms used tables given heath ng peyton 1991 code technique scope availability reference capss multifrontal spd wwwnetliborgscalapack heath raghavan 1997 mumps multifrontal symuns wwwenseeihtfrapomumps amestoy et al 1999 pastix fanin spd see caption x henon et al 1999 pspases multifrontal spd wwwcsumnedumjoshipspases gupta karypis kumar 1997 spooles fanin symuns wwwnetliborglinalgspooles ashcraft grimes 1999 superlu fanout uns wwwnerscgovxiaoyesuperlu li demmel 1999 fanout uns wwwcsucsbeduresearchs fu jiao yang 1998 wsmp z multifrontal sym ibm product gupta 2000 table 71 distributed memory codes x wwwdeptinfolabriubordeauxfrrametpastix uses qr storage statically accommodate lu fillin z object code ibm available numerical pivoting performed code technique scope availability reference gspar interpretative uns grund borchardt grund horn 1997 multifrontal uns wwwcseclrcacukactivityhsl amestoy duff 1993 multifrontal qr rect wwwcseclrcacukactivityhsl amestoy duff puglisi 1996b panelllt leftlooking spd ng ng peyton 1993 pardiso leftright looking uns schenk schenk gartner fichtner 2000 psldlt leftlooking spd sgi product rothberg 1994 psldu leftlooking uns sgi product rothberg 1994 leftlooking uns wwwnerscgovxiaoyesuperlu demmel et al 1999 table 72 shared memory codes object code sgi available acknowledgments want thank james demmel jacko koster rich vuduc helpful discussions grateful chiara puglisi comments early version article help presentation also want thank john reid comments first version paper r unsymmetrized multifrontal lu factorization fully asynchronous multifrontal solver using distributed dynamic scheduling spooles objectoriented sparse matrix library parallel numerical methods large systems differentialalgebraic equations industrial applications algorithms permuting large entries diagonal sparse matrix appear siam journal matrix analysis applications wsmp watson sparse matrix package part direct solution symmetric sparse systems version 10 httpwww mapping scheduling algorithm parallel sparse fanin numerical factorization httpwww making sparse gaussian elimination scalable static pivoting scalable sparse direct solver using static pivoting hybridizing nested dissection halo approximate minimum degree efficient sparse matrix ordering efficient sparse cholesky factorization distributedmemory multiprocessors tr parallel algorithms sparse linear systems elimination structures unsymmetric sparse italicluitalic factors supernodal cholesky factorization algorithm sharedmemory multiprocessors modification minimumdegree algorithm multiple elimination approximate minimum degree ordering algorithm highly scalable parallel algorithms sparse matrix factorization efficient sparse lu factorization partial pivoting distributed memory architectures supernodal approach sparse partial pivoting design use algorithms permuting large entries diagonal sparse matrices asynchronous parallel supernodal algorithm sparse gaussian elimination making sparse gaussian elimination scalable static pivoting preconditioning highly indefinite nonsymmetric matrices algorithms permuting large entries diagonal sparse matrix fully asynchronous multifrontal solver using distributed dynamic scheduling mapping scheduling algorithm parallel sparse fanin numerical factorization ctr laura grigori xiaoye li new scheduling algorithm parallel sparse lu factorization static pivoting proceedings 2002 acmieee conference supercomputing p118 november 16 2002 baltimore maryland mark baertschy xiaoye li solution threebody problem quantum mechanics using sparse linear algebra parallel computers proceedings 2001 acmieee conference supercomputing cdrom p4747 november 1016 2001 denver colorado olaf schenk klaus grtner solving unsymmetric sparse systems linear equations pardiso future generation computer systems v20 n3 p475487 april 2004 patrick r amestoy iain duff jeanyves lexcellent xiaoye li impact implementation mpi pointtopoint communications performance two general sparse solvers parallel computing v29 n7 p833849 july xiaoye li james w demmel superludist scalable distributedmemory sparse direct solver unsymmetric linear systems acm transactions mathematical software toms v29 n2 p110140 june anshul gupta recent advances direct methods solving unsymmetric sparse systems linear equations acm transactions mathematical software toms v28 n3 p301324 september 2002 timothy davis column preordering strategy unsymmetricpattern multifrontal method acm transactions mathematical software toms v30 n2 p165195 june 2004 patrick r amestoy iain duff stphane pralet christof vmel adapting parallel sparse direct solver architectures clusters smps parallel computing v29 n1112 p16451668 novemberdecember