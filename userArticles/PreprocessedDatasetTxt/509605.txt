compiling stencils high performance fortran many fortran90 hpf programs performing dense matrix computations main computational portion program belongs class kernels known stencils stencil computations commonly used solving partial differential equations image processing geometric modeling efficient handling stencils critical achieving high performance distributedmemory machines compiling stencils efficient code viewed important companies built specialpurpose compilers handling others added stencilrecognizers existing compilersin paper present general compilation strategy stencils written using fortran90 array constructs strategy capable optimizing single multistatement stencils applicable stencils specified shift intrinsics arraysyntax equally well strategy eliminates need patternrecognition algorithms orchestrating set optimizations address overhead intraprocessor interprocessor data movement results translation fortran90 array constructs experimental results show code produced strategy beats matches best code produced specialpurpose compilers patternrecognition schemes known us addition strategy produces highly optimized code situations others fail producing several orders magnitude performance improvement thus provides stencil compilation strategy robust predecessors b introduction highperformance fortran hpf14 extension fortran90 attracted considerable attention promising language writing portable parallel programs hpf offers simple programming model shielding programmers intricacies concurrent programming managing distributed data programmers express data parallelism using fortran90 array operations use data layout directives direct partitioning data computation among processors parallel machine many programs performing dense matrix computations main computational portion program belongs class kernels known stencils hpf gain acceptance vehicle parallel scientific programming must achieve high performance important class problems compiling stencils efficient code viewed important companies built specialpurpose compilers handling 4 5 6 others added stencilrecognizers existing hpf compilers 1 2 previous approaches stencil compilation significant limitations restricted types stencils could handle paper focus problem optimizing stencil computations matter instantiated programmer execution distributedmemory archi tectures strategy orchestrates set optimizations address overhead intraprocessor interprocessor data movement results translation array constructs additional optimizations address issues scalarizing array assignment statements loop fusion data locality next section briefly discuss stencil computations execution cost distributedmemory machines section 3 give overview compilation strategy discuss individual optimizations section 4 present extended example show strategy handles difficult case experimental results given section 5 section 6 compare strategy known efforts computations section introduce stencil computations give overview execution cost distributedmemory machines also introduce normalized intermediate form compiler uses stencils 21 stencils stencil stylized matrix computation group neighboring data elements combined calculate new value typically combined form sum products type computation common solving partial differential equations image processing geometric modeling fortran90 array assignment statement figure 1 commonly referred 5point stencil statement src dst arrays c1c5 either scalars arrays interior element result array dst computed corresponding element source array src neighboring figure 1 5point stencil computation figure 2 9point stencil computation figure 3 problem 9 purdue set elements src north west south east 9point stencil computes grid elements exploiting cshift intrinsic might specified shown figure 2 previous two examples stencils specified single array assignment statement need always case consider 9point stencil programmer attempted optimize program hand stencil preprocessed optimization phases compiler might presented code shown figure 3 1 goal work generate highlyoptimized code stencil computa tions regardless written hpf reason designed optimizer target general normalized input form stencil stencillike computations translated normal form factoring expressions introducing temporary arrays fact intermediate form used several distributedmemory compilers 18 23 3 normal form several distinguishing characteristics ffl cshift intrinsics temporary arrays inserted perform data movement needed operations array sections different processor mappings example taken problem 9 purdue set 21 adapted fortran benchmarking thomas haupt npac figure 4 intermediate form 5point stencil computation ffl cshift intrinsic occurs singleton operation righthand side array assignment statement applied whole arrays ffl expression actually computes stencil operates operands perfectly aligned thus communication operations required example given 5point stencil computation presented figure 1 cm fortran compiler would translate sequence statements shown figure 4 rest paper assume stencil computations normalized form arrays distributed block fashion although concentrate stencils expressed using cshift intrinsic techniques presented generalized handle eoshift intrinsic well 22 stencil execution execution stencil computation distributedmemory machine two major components data movement associated set cshift operations calculation sum products first phase stencil computation data movement associated cshift operations performed illustrate data movement single cshift using example figure 5 shows effects cshift 1 along second dimension twodimensional blockdistributed array cshift operation performed distributed array two major actions take place 1 data elements must shifted across processing element pe boundaries sent appropriate neighboring pe interprocessor component shift figure 5 dashed lines represent type data movement case transfer column data neighboring processors 2 data elements shifted within pe copied appropriate locations destination array intraprocessor component shift solid lines figure 5 represent data movement following data movement second phase stencil computation execution loop nest calculate sum products loop nest stencil computation figure 5 constructed compilation two steps first compiler applies scalarization 24 replace fortran 90 array operations serial loop nest operates individual data elements next compiler transforms loop nest spmd code 8 spmd code synthesized reducing loop bounds pe computes values data owns copy transformed loop nest known subgrid loop nest executes pe parallel machine due nature stencils make many distinct array references subgrid loops easily become memory bound loops cpu must often sit idle waits array elements fetched memory 3 compilation strategy section start overview compilation strategy present individual component optimizations given stencil computation normal form described section 21 optimize applying sequence four optimizations first addresses intraprocessor data movement associated cshift operations eliminating possible second rearranges statements separate blocks computation operations communication operations optimizes stencil promoting loop fusion computation operations prepares communication operations optimization following phase next interprocessor data movement cshift operations optimized eliminating redundant partiallyredundant communication finally looplevel transformations applied optimize computation 31 optimizing intraprocessor data movement intraprocessor data movement associated shift intrinsics completely eliminated possible accomplished optimization call offset arrays 15 optimization determines source array src destination array dst cshift share memory locations case interprocessor data movement needs occur exploit overlap areas 11 receive data copied processors accomplished appropriate references destination array rewritten refer source array indices offset shift amount principal challenge determine source destination arrays share storage established set criteria determine safe profitable create offset array criteria algorithm used verify described detail elsewhere 15 22 general approach allows source destination arrays shift operation share storage destructive updates either array shift offset small constant determined destination array assignment statement cshiftsrcshiftdim may offset array perform following transformations code transformations take advantage data may shared source array src destination array dst move required data pes first replace shift operation call routine moves offprocessor data src overlap area call overlap shiftsrcshiftdim replace uses array dst reached definition use array src newly created references src carry along special annotations representing values shift dim finally creating subgrid loops scalarization phase alter subscript indices used offset arrays array subscript used offset reference src identical subscript would generated dst exception dimth dimension incremented shift amount algorithm devised verifying criteria performing transformations based upon static single assignment ssa intermediate representation 9 algorithm validating use offset array shift operation transforms program propagates information optimistic manner propagation continues references transform one criteria violated criterion violated may necessary insert array copy statement program maintain original semantics inserted copy statement performs intraprocessor data movement avoided overlap shift due offset array algorithms optimistic nature able eliminate intraprocessor data movement associated shift operations many difficult situations particular determine offset arrays exploited even definition uses separated program control flow allows stencil compilation strategy eliminate intraprocessor data movement situations strategies fail 32 statement reordering follow offset array optimization context partitioning optimization 17 optimization partitions set fortran90 statements groups congruent array statements 2 scalar expressions communication operations assists compilation stencils following two ways 1 first grouping congruent array statements together ensure subgrid loops generated via scalarization loop fusion much computation possible placed within loop without causing loops overfused 22 loops overfused code produced resulting parallel loops exhibits worse performance code separate parallel loops also structure subgrid loops produced regular characteristics increase chances loop transformations performed later successful exploiting data reuse data locality 2 second grouping together communication operations simplify task reducing amount interprocessor data movement discuss next subsection accomplish context partitioning use algorithm proposed kennedy c kinley 16 algorithm developed partition parallel serial loops fusible groups use partition fortran90 statements congruence classes algorithm works data dependence graph ddgwhich must acyclic since apply set statements within basic block dependence graph contains loopindependent dependences thus acyclic complete description context partitioning algorithm available elsewhere 17 22 along discussion advantages simd mimd machines context partitioning key ability optimize multistatement stencils fully singlestatement stencils stencil compilation strategy capability 33 minimizing interprocessor data movement intraprocessor data movement eliminated partitioned statements groups congruent operations focus attention interprocessor data movement occurs calls cshift due nature offset arrays presented many opportunities eliminate redundant partially redundant data movement call optimization communication unioning 22 since combines set communication operations produce smaller set operations two key observations allow us find eliminate redundant interprocessor data movement first shift operations including overlap shift commutative array statements congruent operate arrays identical distributions cover iteration space thus arrays shifted order shift operations manner like without affecting result second since overlap shifts move data overlap areas subgrids shift large amount given direction dimension may subsume shifts smaller amounts direction dimension formally overlap shift amount dimension k redundant exists overlap shift amount j dimension k jjj jij since already applied context partitioning optimization program restrict focus individual groups calls overlap shift eliminate redundant data movement using communication unioning first use commutative property rewrite shifts multioffset arrays overlap shifts lower dimensions occur first used input overlap shifts higher dimensions reorder calls overlap shift sorting shifted dimension lowest highest scan overlap shifts lowest dimension keep largest shift amount direction others eliminated redundant communication unioning proceeds process overlap shifts higher dimension ascending order performing following three actions 1 scan overlap shifts given dimension determine largest shift amount direction 2 look source arrays already offset arrays indicating multioffset array use annotations associated source array create rsd used optional fourth argument call overlap shift argument indicates data elements adjacent overlap areas also moved shift operation mapping annotations rsd simply matter adding annotations corresponding rsd dimension annotation added lower bound rsd shift amount negative otherwise added upper bound shift amounts larger rsds subsume smaller rsds 3 generate single overlap shift direction using largest shift amount including rsd needed overlap shifts dimension eliminated procedure eliminates redundant offsetshift communication including partially redundant data movement associated accessing corner elements stencils algorithm unique based upon understanding analysis shift intrinsics rather based upon patternmatching done many stencil compilers optimization eliminates communication shifted array except single message direction dimension number messages stencil thus minimized example consider 9point stencil computation presented figure 2 original stencil specification required twelve cshift intrinsics applying communication unioning four calls overlap shift shown figure 6 required figures 710 display data movement results calls figures contain 5 theta 5 subgrid solid lines surrounded overlap area dashed lines portions figure communication unioning 9point stencil figure 7 first half 9point stencil com munication figure 8 result communication operation adjacent subgrids also shown figure 7 depicts data movement specified first two calls result data movement shown figure 8 overlap areas properly filled data movement last two calls shown figure 9 notice last two calls pick data overlap areas filled first two calls thus populate overlap area elements needed subsequent computation shown figure 10 figure 9 second half 9point stencil communication figure 10 result communication operation 34 optimizing computation finally scalarization produced subgrid loop nest optimize applying set looplevel transformations designed improve performance memorybound programs transformations include unrollandjam addresses memory refer ences loop permutation addresses cache references optimize program exploiting reuse data values optimizations described detail elsewhere 7 19 addressed paper 4 extended example section trace compilation strategy extended example detailed examination shows strategy able produce code matches beats handoptimized code also demonstrates able handle stencil computations cause methods fail exercise chosen use problem 9 purdue set 21 adapted fortran benchmarking thomas haupt npac 20 13 program kernel shown figure 3 arrays u rip rin twodimensional distributed blockblock fashion kernel computes standard 9point stencil identical computed singlestatement stencil shown figure 2 reason written fashion reduce memory requirements given singlestatement 9point stencil fortran90 compilers generate 12 temporary arrays one cshift greatly restricts size problem solved given machine contrast problem 9 specification computed 3 temporary arrays since liveranges last 6 cshifts overlap reduces temporary storage requirements factor four additionally assignments cshifts rip rin perform common subexpression elimination removing four duplicate cshifts original specification stencil figure 11 shows comparison execution times singlestatement cshift stencil figure 2 multistatement problem 9 stencil figure 3 programs compiled ibms xlhpf compiler executed 4processor sp2 varying problem sizes seen singlestatement stencil specification exhausted available memory larger problem sizes even though pe 256mbytes real ram 41 program normalization step compilation stencil code figure 3 using strategy presented paper figure 12 shows stencil code normalization six cshifts subexpressions assignment statements array hoisted statements assigned compilergenerated temporary arrays since live ranges temporary arrays overlap single temporary shared among statements alternatively cshift could receive temporary array would affect results stencil compilation strategy execution time ms subgrid size squared exceeded memory cshift specification purdue problem 9 figure 11 comparison two 9point stencil specifications figure problem 9 normalization 42 offset array optimization shift operations identified hoisted assignment state ments apply offset array optimization example algorithm determines shifted arrays made offset arrays seen figure 13 cshift operations changed overlap shift operations references assigned arrays replaced offset references source array u intraprocessor data movement thus eliminated addition notice temporary arrays compilergenerated tmp array figure 13 problem 9 offset array optimization figure 14 problem 9 context partitioning optimization userdefined rip rin longer needed compute stencil uses arrays routine need allocated reduction storage requirements allows larger problems solved given machine 43 context partitioning optimization offset array optimization apply context partitioning algorithm algorithm begins determining congruence classes present section code example two congruence classes array statements congruent communication statements dependence graph computed next two types dependences exist code true dependences overlap shift operations expressions use offset arrays true antidependences exist multiple occurrences array since figure 15 problem 9 communication unioning optimization dependences two classes statements communication class statements congruent array class context partitioning algorithm able partition statements perfectly two groups result shown figure 14 since array statements adjacent scalarization able fuse single loop nest similarly communication statements adjacent communication unioning successful task 44 communication unioning optimization turn attention interprocessor data movement specified overlap shift operations described section 33 first exploit commutativity overlap shift operations rewrite multidimensional overlap shifts lower dimensions shifted first rewriting necessary example since dimension 1 shifts occur first seen figure 14 next look shifts across first dimension since single shift distance one direction redundant communication eliminate second dimension find shifts distance one however discover four multioffset arrays examining annotations offset arrays create rsds summarize overlap areas necessary generate two calls overlap shift include rsds eliminate overlap shift calls second dimension resulting code shown figure 15 communication unioning reduced amount communication minimum single communication operation dimension direction 45 scalarization memory optimizations figure shows code scalarization code contains 4 interprocessor communication operations intraproceesor data movement performed final transformations refine loop bounds generate node program accesses subgrids local pe strategy generated single loop nest due nature stencil computations ripe opportunities memory hierarchy optimization hand final code optimizing node compiler performs looplevel transformations scalar replacement unrollandjam enddo enddo figure problem 9 scalarization important note strategy also produces exact code given singlestatement 9point stencil figure 2 example shows stencil compilation algorithm capable fully optimizing stencils matter instantiated programmer 5 experimental results measure performance boost supplied step stencil compilation strategy ran set tests 4processor ibm sp2 started generating naive translation problem 9 test case fortran77mpi considered original version successively applied transformations outlined preceding section measured execution time results shown figure 17 analyzing results figure 17 worthwhile compare results shown figure 11 problem 9 code performance original mpi version code example already order magnitude faster code produced ibms xlhpf compiler 0475 seconds versus 477 seconds largest problem size applying offset array optimization fortran77mpi test case shown figure 13 execution time improves 45 equivalent speedup 180 next applying context partitioning shown figure 14 scalarization able merge computation single loop nest improving execution time additional 31 point reduced execution time original program 62 speedup 264 shown figure 15 communication unioning optimization eliminates four communication operations reduces execution time 41 compared contextoptimized version applying memory optimizations scalar replacement unrollandjam reduce execution time another 14 execution time original program trimmed 81 equivalent speedup 519 comparing code code produced ibms xlhpf compiler shows speedup factor 52 lest someone think chosen ibms xlhpf compiler straw man collected additional performance numbers generated third version 9point execution time ms subgrid size squared original program offset arrays context partitioning communication unioning memory optimizations figure 17 stepwise results stencil compilation strategy problem 9 executed sp2 stencil computation one using array syntax similar 5point stencil shown figure 1 9point stencil computation computes interior elements matrix elements 2n1 dimension graph comparing execution time two 9point stencil specifications given figure 18 ibm xlhpf compiler used cases interesting note array syntax stencil xlhpf compiler produced performance numbers tracked best performance numbers problem sizes except largest 10 advantage important note stencil compilation strategy presented handles three specifications 9point stencil equally well algorithm based upon analysis optimization base constructs upon stencils built algorithm designed handle lowest common denominator form compiler transform stencil computations 6 related work one first major efforts specifically address compilation stencil computations distributedmemory machine stencil compiler cm2 also known convolution compiler 4 5 6 compiler eliminated intraprocessor data movement optimized interprocessor data movement exploiting cm2s polyshift communication 10 final computation performed handoptimized library microcode took advantage several loop transformations specialized register allocation scheme general compilation methodology produces style code specialized compiler eliminate intraprocessor data movement minimize interprocessor execution time ms subgrid size squared exceeded memory cshift specification purdue problem 9 array syntax figure 18 comparison three 9point stencil specifications data movement finally use looplevel optimizer perform unrollandjam optimization accomplish data reuse stencil compilers multistencil swath cm2 stencil compiler many limitations however could handle single statement stencils stencil specified using cshift intrinsic arraysyntax stencils would accepted since compiler relied upon pattern matching stencil specific form sum terms coefficient multiplying shift expression variations possible finally programmer recognize stencil computation extract program place subroutine compiled stencil compiler compilation scheme handles strict superset patterns handled cm2 stencil compiler words avoid general problem restricting domain applicability 6 placed restrictions upon work strategy optimizes singlestatement stencils multistatement stencils cshift intrinsic stencils arraysyntax stencils equally well since optimizations designed incorporated hpf compiler benefit computations slightly resemble stencils also commercially available compilers handle certain styl ized singlestatement stencils maspar fortran compiler avoids intraprocessor data movement singlestatement stencils written using array notation accomplished scalarizing fortran90 expression avoiding generation cshifts using dependence analysis find loopcarried dependences indicate interprocessor data movement interprocessor data moved local copying required ever compiler still performs data movement singlestatement stencils written using shift intrinsics strategy shared many fortran90hpf compilers focus handling scalarized code cm2 stencil compiler methodology strict superset strategy gupta et al 12 describing ibms xlhpf compiler state able reduce number messages multidimensional shifts exploiting methods similar however describe algorithm accomplishing unknown whether would able eliminate redundant communication arises shifts dimension direction different distances portland groups pghpf compiler described bozkus et al 1 2 performs stencil recognition optimizes computation using overlap shift communication also perform subset communication unioning optimization however limited singlestatement expressions cases general several different methods handling specific forms stencil computations strategy handles general form stencil computations earlier methods 7 conclusion paper presented general compilation scheme compiling hpf stencil computations distributedmemory architectures strategy optimizes computations orchestrating unique set optimizations optimizations eliminate unnecessary intraprocessor data movement resulting cshift intrinsics rearrange array statements promote profitable loopfusion eliminate redundant interprocessor data movement optimize memory accesses via looplevel transformations optimizations general enough included generalpurpose hpffortran90 compiler benefit many computations fit stencil pattern strength optimizations operate normal form stencil computations readily translated enables us optimize stencil computations regardless whether written using array syntax explicit shift trinsics whether stencil computed single statement multiple statements approach significantly general stencil compilation approaches previous compilers even though focused compilation stencils distributedmemory machines paper techniques presented equally applicable optimizing stencil computations sharedmemory scalar machines exception reducing interprocessor movement acknowledgments work supported part ibm corporation center research parallel computation nsf science technology center darpa contract dabt6392c0038 work also supported part defense advanced research projects agency rome laboratory air force materiel command usaf agreement number f306029610159 us government authorized reproduce distribute reprints governmental purposes notwithstanding copyright annotation thereon views conclusions contained herein authors interpreted representing official policies endorsements either expressed implied defense advanced research projects agency rome laboratory us government r techniques compiling executing hpf programs sharedmemory distributedmemory parallel systems compiling data parallel programs message passing programs massively parallel mimd systems stencil compiler connection machine models cm2200 stencil compiler connection machine model cm5 compiling fortran 77d 90d mimd distributedmemory machines efficiently computing static single assignment form control dependence graph communications software connection machine systems cm2 cm200 updating distributed variables local computations hpf compiler ibm sp2 low level hpf compiler benchmark suite high performance fortran forum optimizing fortran 90 shift operations distributedmemory multicomputers context optimization simd execution optimization techniques simd fortran compilers improving data locality loop transfor mations applications benchmark set fortrand high performance tran problems test parallel vector languages optimizing fortran90dhpf distributedmemory computers compiler massively parallel distributed memory mimd computer optimizing supercompilers supercomputers tr updating distributed variables local computations fortran ten gigaflops efficiently computing static single assignment form control dependence graph compiler optimizations improving data locality polyshift communications software connection machine system cm200 hpf compiler ibm sp2 improving data locality loop transformations pghpfmyampersandmdashan optimizing high performance fortran compiler distributed memory machines optimizing fortran90dhpf distributedmemory computers optimizing supercompilers supercomputers optimizing fortran 90 shift operations distributedmemory multicomputers ctr david wonnacott achieving scalable locality time skewing international journal parallel programming v30 n3 p181221 june 2002 kandemir 2d data locality definition abstraction application proceedings 2005 ieeeacm international conference computeraided design p275278 november 0610 2005 san jose ca hitoshi sakagami hitoshi murai yoshiki seo mitsuo yokokawa 149 tflops threedimensional fluid simulation fusion science hpf earth simulator proceedings 2002 acmieee conference supercomputing p114 november 16 2002 baltimore maryland g chen kandemir optimizing interprocessor data locality embedded chip multiprocessors proceedings 5th acm international conference embedded software september 1822 2005 jersey city nj usa armando solarlezama gilad arnold liviu tancau rastislav bodik vijay saraswat sanjit seshia sketching stencils acm sigplan notices v42 n6 june 2007 steven j deitz bradford l chamberlain lawrence snyder eliminating redundancies sumofproduct array computations proceedings 15th international conference supercomputing p6577 june 2001 sorrento italy gerald roth ken kennedy loop fusion high performance fortran proceedings 12th international conference supercomputing p125132 july 1998 melbourne australia ya kalinov l lastovetsky n ledovskikh posypkin compilation vector statements c language architectures multilevel memory hierarchy programming computing software v27 n3 p111122 mayjune 2001 mary jane irwin compilerdirected proactive power management networks proceedings 2005 international conference compilers architectures synthesis embedded systems september 2427 2005 san francisco california usa zhang zhengqian kuang baiming feng jichang kang autocfdnow precompiler effectively parallelizing cfd applications networks workstations journal supercomputing v38 n2 p189217 november 2006 daniel j rosenkrantz lenore r mullin harry b hunt iii minimizing materializations arrayvalued temporaries acm transactions programming languages systems toplas v28 n6 p11451177 november 2006