learning structure data application ozone prediction paper propose algorithm structure learning predictive expert systems based probabilistic network representation idea simplest structure minimum number links acceptable predictive capability algorithm starts building tree structure based measuring mutual information pairs variables adds links necessary obtain certain predictive performance applied method ozone prediction mxico city ozone level used global indicator air quality different parts city important predict ozone level day least several hours advance reduce health hazards industrial losses occur ozone reaches emergency levels obtained first approximation treestructured dependency model predicting ozone one part city observe even three parameters estimations acceptablea causal network representation structure learning techniques produced interesting results ozone prediction problem firstly got insight dependence structure phenomena secondly got indication important important variables ozone forecasting taking account measurement computational costs ozone prediction could reduced thirdly obtained satisfactory short term ozone predictions based small set important parameters b introduction learning defined process system improves performance 1 since first days research artificial intelligence ability learn considered one essential attributes intelligent system considerable amount research done area learning focused acquiring concepts examples called inductive learning development expert systems motivated research learning automate process knowledge acquisition considered one main problems construction knowledgebased systems important aspect inductive learning obtain model represents domain knowledge accessible user par ticular useful obtain dependency information variables involved phenomena factors important certain variable particular interest predictive expert systems want forecast variables based known parameters useful know parameters incidence unknowns ones much influ ence knowledge representation paradigma captures dependency information probabilistic network probabilistic networks pn 3 also known bayesian networks causal networks probabilistic influence diagrams graphical structures used representing expert knowledge drawing conclusions input data explaining reasoning process user pn directed acyclic graph dag whose structure corresponds dependency relations set variables represented network nodes parameterized conditional probabilities links required specify underlying distri bution structure network makes explicit dependence independence relations variables important representing knowledge domain ii efficient probability propagation fig 1 probabilistic network use pn representation learning divided naturally two aspects parameter learning structure learning 2 parameter learning obtaining required probability distributions certain structure structure learning obtaining topology network including variables relevant particular problem depen dencies interested second aspect obtaining dependency structure certain phenomena get better understanding use predictive tool section 2 give brief introduction probabilistic networks section 3 reviews previous work structure learning section 4 introduces methodology obtaining dependency structure predictive systems section 5 describe problem ozone prediction mexico city present experimental results section 6 finally give conclusions possible directions future work 2 probabilistic networks probabilistic network graphical representation dependencies independencies probabilistic reasoning expert systems node represents discrete random variable arc probabilistic dependency variable end link dependent variables origin eg c dependent pn figure 1 indicated link 1 think graph figure 1 representing joint probability distribution variables b g equation 1 obtained applying chain rule using dependency information represented network topology pn gives direct information dependency relationships variables involved particular represents variables conditionally independent given another variable definition conditionally independent b given c represented graphically node c sep arating b network general c subset nodes network removed make subsets nodes b disconnected independence pn network tested criteria called dseparation 2 dag representation g probability distribution p imap 2 independencies represented g present p minimal imap imap minimum number links link removed independency relation g present p formally probabilistic network minimal imap joint probability distribution p 2 words graph minimum number links faithfully represents probabilistic independencies set random variables given knowledge base represented probabilistic network used reason consequences specific input data called probabilistic reasoning consists instantiating input variables propagating effect network update probability hypothesis variables contrast previous approaches eg mycin c b fig 2 network structures tree b polytree c multiplyconnected prospector 4 updating certainty measures consistent probability theory based application bayesian calculus independencies represented network probability propagation general network complex problem efficient algorithms certain restricted structures alternative approaches complex net works pearl 3 developed method propagating probabilities networks tree structured ie node one incoming link one parent example probabilistic tree shown figure 2 probabilistic tree every node one parent except one node denoted root incoming links given certain evidence v represented instantiation corresponding variables posterior probability variable taking value b bayes theorem given dependencies represented tree separates two independent subtrees one formed descendants b every node thus decompose evidence variables two sets v gammab represents data rooted b v b data contained rest network 3 written given two subtrees conditionally independent given b substituting 5 4 applying alge bra obtain ff normalizing constant equation 6 provides product rule updating probability every node network combining evidence coming descendants one coming parent shows prior probability required except root node p following terms 2 write 6 equation constitutes basis propagation mechanism probabilistic tree need store vectors node update corresponding parameters neighbors fixed conditional probability matrix p node implemented message passing scheme node acts simple process communicates neighbours fa ther sons initially network equi librium information arrives nodes called data nodes instantiated information propagated network node sending messages parent sons node uses information update local parameters update posterior probability required messages reach root node propagate topdown reach leaf nodes propagation terminates network comes new equilibrium information propagates tree single pass time parallel proportional diameter network extension polytrees proposed kim pearl 5 polytree node multiple parents still singly connected graph polytree depicted figure 2 b main difference algorithm trees multiparent nodes conditional distribution given parents quired time propagation still linearly proportional diameter network complex multiply connected net works see fig 2 c alternative techniques probability propagation clustering 6 conditioning 2 stochastic simulation 2 methods efficient certain types structures mainly sparse networks general probability propagation complex network nphard problem 7 thus efficiency reasons also clarity expressiveness important obtain simplest structure minimum number links models appropriately phenomena interest complete graph trivial imap probability distribution would useful terms knowledge representation computational efficiency 3 structure learning approaches structure learning consists finding topology network dependency relationships variables involved expert systems obtain structure expert representing network experts knowledge causal relations domain complex problems might expert complete understanding domain obtain dependency independency relations herhis knowledge could de ceiving also knowledge acquisition could expensive time consuming process interested using empirical observations obtain improve structure probabilistic network previous research done inducing structure pn statistical data chow liu 8 presented algorithm representing probability distribution dependency tree later extended rebane pearl 9 recovering causal polytrees chow lius 8 motivation reducing memory requirements storing ndimensional discrete probability distribution developed method approximating probability distribution product secondorder dis tributions equivalent probabilistic tree thus joint probability distribution represented x ji cause parent x variable one parent except one root parent method restricted tree structure considered approximation original distribution dependency tree optimization problem used quantity measures difference information contained two distributions x problem reduced finding tree dependency distribution approximates original distribution p ip p minimal find optimum tree use entropy measure mutual information two variables defined x logp assign every branch tree weight corresponds mutual information variables connected link weight tree sum weights branches shown 8 maximizing total branch weight equivalent minimizing closeness measure tree maximum weight optimum tree dependency approximation p result makes possible find optimum tree structure simple algorithm uses nn gamma 12 secondorder distributions correspond possible branches n variables ordered according weight two maximum weight selected first two branches tree branches selected decreasing order whenever form cycle previously selected ones variables covered branches thus obtain treestructured pn sample data need estimate joint frequencies mutual information pairs variables construct optimum tree previous algorithm rebane pearl 9 extended chows method developing similar algorithm construction polytree statistical data polytree singly connected network node multiple parents joint probability distribution expressed fx set parents variable x algorithm constructing polytree starts using tree recovering algorithm constructing skeleton network without directionality links checks local dependencies variables uses information determine directionality branches local dependency tests applied connected variable triples checking variable pairs dependent independent partially determine directionality corresponding links test applied nodes starting outermost ones leafs wards possible directionalities found general possible find direction branches external semantics needed completion 2 recent work focused two aspects combine statistical data expert knowledge induce multiplyconnected networks data first approach based combing expert knowledge data overcome limitations previous techniques obtain general complete dependence structure sucar et al 10 start structure derived subjective rules initial topology develop methodology based statistical techniques improve structure testing independence assumptions altering structure satisfied kwoh gillies 11 extended work creating hidden nodes improve structure bayesian tree independence assumptions hold srinivas et al 12 combine expert knowledge dependence information obtained statistical tests iterative algorithm approximating structure bayesian net work expert knowledge use includes variables hypothesis root nodes evidence leaf nodes partial knowledge 6 authors causality independence variables second approach cooper herskovits 13 developed bayesian method induction probabilistic networks data given certain assumptions probability distri butions developed algorithm obtaining probable bayesian network given database cases algorithm probability certain structure given data also obtained handle missing data hidden variables recently lam bacchus 14 developed alternative technique inducing multiplyconnected networks based rissa nens minimal description length mdl principle algorithm tries make trade accuracy complexity structure ob tained favoring simpler structures even accurate complex one chow lius algorithm two important limitations restricted tree structures ii obtain directions links causality rebane pearls extension still restricted aspects generality directionality lam bacchus 14 point approach 13 assumes uniform distribution possible network structures could favor much complex structure even slightly accurate approach based mdl principle 14 overcomes difficulty considering accuracy com plexity still based certain heuristics always obtain optimum solu tion approaches assume existence expert knowledge always available special case predictive systems certain characteristics explain following section make previous algorithms inap propriate particular previous techniques consider variables level predictive systems accuracy terms un important factor 4 structure learning predictive system predictive system one variable whose value unknown estimated based known variables possible spatial temporal predictions first case unknown observable estimated measured parameters second case unknown future predicted form present past measure ments instance pollution prediction might want estimate pollution level part measurements one day advance interested obtaining dependence structures predictive systems special characterists ffl usually one variable want predict considered hypothesis root node ffl variables evidence nodes different levels influence hypothesis ffl evidence nodes direct influence hypothesis influence could evidence nodes thus propose algorithm structure learning predictive expert systems based previous observations idea simplest structure minimum number links acceptable predictive capability approach start pn minimum possible number links connects variables involved n variables smallest connect graph tree arcs constitute skeleton network predictive accuracy tree good enough consider structure otherwise start add links according certain criteria obtain desired performance algorithm following 1 obtain initial tree structure chow lius algorithm 2 make hypothesis variable root node fixes directions links 3 produce ordering variables xng starting root following tree according order mutual information variables 4 test predictive capability network 41 satisfactory stop1 play outlook temperature humidity windy tree links links fig 4 initial probabilistic tree golf example 42 number links less maximum add link network 4 link added highest mutual information produce cycle ii node start link predecessor node end according previous ordering 43 number links equal maximum stop0 stop 1 indicates successful termination stop 0 could achieve desired predictive performance predictive capability tested statistically performing predictions different data training set comparing predictions actual values unkown maximum number links completely connected graph n n gamma 12 theoretical justification step 42 algorithm based general procedure obtaining minimal imap pn every independence relation represented network valid 15 consists defining ordering variables constructing graph parents variable subset predecessors makes independent rest predecessors number arcs reaches maximum obtain totally connected graph represents joint probability distribution n variables without independence assumptions mention trivial imap distribution complete graph still desired predictive accuracy means training data inadequate generating appropriate structure either cases required larger sample parameters included set variables need considered illustrate procedure use small hypothetical example predicting play table 1 shows variables values examples table 2 shows set examples used training table 3 dependency links variable pairs ordered mutual table 1 variables golf prediction example variable values play play dont play outlook sunny overcast rain temperature continuous humidity continuous windy true false table 2 set training data golf prediction example outlook temp hum windy play sunny 85 85 false false sunny overcast rain 70 96 false true rain 68 80 false true rain overcast sunny 72 95 false false sunny 69 70 false true rain sunny overcast 72 90 true true overcast 81 75 false true rain 71 96 true false table 3 set links variable pairs ordered mutual information data table link variables 9 windy play formation small data set used illustrate ideas initial tree structure overimposed complete graph depicted figure 4 possible ordering variables case fplay outlook temperature humidity windyg figure 3 next 3 steps algorithm shown assuming tree structure good enough small example test predictive accuracy one new link added according mutual informa tion direction determined variable ordering algorithm terminate obtain desired accuracy generate complete graph 10 links example interesting aspect notice unless need complete graph usually eliminate variables predicting unknown tree structure eliminate variables ones directly connected hypothesis root node independence relations represented pn tree node independent variables given direct parent sons root node direct sons general network complete links present subset nodes make node independent remaining nodes thus variables one known use independence information eliminate parameters simplify estimation problem following section introduce problem ozone prediction mexico city apply previous algorithm obtain dependence structure phenomena play outlook temperature humidity windy play outlook temperature humidity windy play outlook temperature humidity windy b c fig 3 structures produced second stage structure learning algorithm golf example first additional link b second c third 5 ozone prediction mexico city air quality mexico city major problem air pollution one highest world high average daily emissions several primary pollutants hydrocarbons nitrogen oxides carbon monoxide others pollution due primarily transportation industrial emissions primary pollutants exposed sunshine undergo chemical reactions yield variety secondary pollu tants ozone important besides health problems may cause ozone considered indicator air quality urban areas air quality monitored mexico city stations five com plete nine variables measured 5 main stations including wind direction velocity temperature relative humidity sulphur dioxide carbon monoxide nitrogen dioxide ozone measured every minute 24 hours day averaged every hour important able forecast pollution level several hours even day advance several reasons including 1 able take emergency measures pollution level going certain threshold 2 help industry make contingency plans advance minimize cost emergency measures 3 estimate pollution area measurements 4 take preventive actions places schools reduce health hazards produced high pollution levels mexico city ozone level used global indicator air quality different parts city concentrations ozone given imeca mexican air quality index important predict ozone level day least several hours advance using variables measured different stations previous work 16 done using neural network techniques forecast ozone mexico city results encouraging estimating ozone level 4 hours advance problem techniques get insight structure phenom ena useful know dependencies different variables measured specially influence ozone concen tration provide better understanding problem several potential benefits ffl determine factors important ozone concentration mexico city ffl simplify estimation problem taking account relevant information ffl find critical primary causes pollution mexico city could help future plans reduce 6 experimental results started applying learning algorithm obtain initial structure phenomena considered 47 variables 17 9 measurements 5 stations plus hour month taken used nearly 400 random samples applied first step algorithm obtain tree structure best approximates data distribution treestructured bayesian network shown figure 5 considered ozone one station pedregal unknown estimate one hour advance using measurements make ozonepedregal hypothesis variable consider root probabilistic tree shown figure 5 initial structure get idea relevance influence variables estimating ozonepedregal nodes closest root important ones faraway nodes important case observe 3 variables ozonemerced ozonexalostoc wind velocity pedregal greatest influence ozonepedregal tree structure good approximation real structure 3 nodes make ozonepedregal independent rest variables see figure 7 thus first test structure estimated ozonepedregal using 3 variables estimation done probability propagation algorithm trees presented section 2 algorithm works discrete variables continuos variables discretized fixed size intervals made two experiments 1 estimate ozonepedregal using 100 random samples taken training data 2 estimate ozone pedregal 100 samples taken data used training results subset 20 representative samples case shown figures 6 8 observe even three param eters estimations quite good training data average error absolute difference real estimated ozone concentration 112 imeca 121 nottraining data 268 imeca 221 results judged taking account first approximation dependency model considering 3 variables estimating ozone pedregal neural network model 16 46 inputs average error similar set test nottraining data o3t o3l o3q vvt cot hora rhf tmpt o3f rhl rht coq col cof fig 5 bayesian tree represents ozone phenomena 5 stations mexico city nodes represent variables according following nomenclature measured variables name formed two parts measurementstation using following abbreviations measurements o3ozone so2sulphur dioxide cocarbon monoxide no2nitrogen dioxide noxnitrogen oxides vvwind velocity dvwind direction tmptemperature rh relative humidity monitoring stations tpedregal ftlanepantla qmerced lxalostoc xcerro de la estrella two variables correspond time measurements taken horahour mesmonth data used train test c45 18 error test set 1764 tree produced c45 given figure 9 interesting note c45 considered wind direction pedregal principal attribute northsouth wind direction 120 increases levels ozone whether southnorth direction ozone levels mainly located within first intervals 70 imecas tested accuracy c45 pruning tree different depths considering relevant attributes leaves labeled may ority class training set level depth 4 accuracy c45 2186 considering three important attributes ie depth 2 mayority class branch tree c45 error 2480 ozone pedregal ozone merced ozone xalostoc pedregal fig 7 reduced tree predicting ozonepedregal accuracy c45 complete tree higher reduced dependency model tree pruned depth 4 lower 3 attributes difficult compare algorithms use different representation decision node decision tree variable node bayesian network still consider accuracy similar two different representations learning algorithms expect higher accuracy dependency model extended variables relations advantage dependency model generally easier understand relevance attribute predicting certain variable explicitly represented difficult obtain decision tree attribute repeated different nodes different depths second advantage gives probability measure value range hypothesis general available decision tree finally bayesian network used predict variable subset attributes known decision tree one variable attributes known practical purposes ozone measured imecas divided several intervals size 50 air quality corresponding emergency measures based intervals experiments probabilistic model aprox 90 predictions fall 50 imecas interval measured ozone level fig 6 real vs estimated levels ozonepedregal using 3 variables training data 7 conclusions future work causal network representation structure learning techniques produced interesting results ozone prediction problem firstly got insight dependence structure phenomena example ozone pedregal influenced ozone stations wind velocity due fact pollution south pedre gal mexico city large part produced industrial plants north dominant northsouth wind direction secondly got indication important important variables ozone forecasting taking account could reduce measurement computational costs ozone predic tion thirdly dependency information could used improving alternative prediction techniques neural networks respect ozone prediction mexico city plan continue work several aspects ffl improve structure bayesian network using second part algorithm ffl obtain dependence structure variables interest particular ozone stations ffl test predictive capability using vari ables assuming influential ones unknown reliable ffl improve longer term predictions using additional information weather forecasting variables structure learning bayesian networks general several research issues remain addressed firstly problem obtaining optimal structure general case considering models accuracy computational complexity secondly general algorithm obtaining directions links network thirdly structure learning algorithms consider observable variables many cases introduction variables called hidden virtual nodes produce simpler structures improved predictive capability addressing issues future research work fig 8 real vs estimated levels ozonepedregal using 3 variables nottraining data rhf rhl hora 484 28687219227104323177fig 9 decision tree ozonepedregal produced c45 node represents decision value variable value less value shown node left branch followed otherwise right one leaves represent different classes ozonepedregal c0 c19 discretization used bayesian network notes 1 data taken public domain file r machines learn probabilistic reasoning intelligent sys tems evidential reasoning hierarchy hypothesis uncertainty management expert systems computational model combined causal diagnostic reasoning inference systems local computations probabilities graphical structures application expert systems computational complexity probabilistic inference using bayesian networks approximating probability distributions dependence trees objective probabilities expert systems using hidden nodes bayesian networks automated construction sparse bayesian networks unstructured probabilistic models domain information bayesian method induction probabilistic networks data learning bayesian net works approach based mdl principle causal networks semantics expressiveness duction dependence structures data application ozone prediction tr ctr elias kalapanidas nikolaos avouris feature selection air quality forecasting genetic algorithm approach ai communications v16 n4 p235251 december cipriandaniel neagu nikolaos avouris elias kalapanidas vasile palade neural neurofuzzy integration knowledgebased system air quality prediction applied intelligence v17 n2 p141169 septemberoctober 2002