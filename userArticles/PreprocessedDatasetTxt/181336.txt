combining static dynamic scheduling distributedmemory multiprocessors loops large source parallelism many numerical applications important issue parallel execution loops schedule workload well balanced among processors existing loop scheduling algorithms designed sharedmemory multiprocessors uniform memory access costs approaches suitable distributedmemory multiprocessors data locality major concern communication costs high paper presents new scheduling algorithm data locality taken account approach combines worlds static dynamic scheduling twolevel overlapped fashion way data locality considered communication costs limited performance new algorithm evaluated cm5 messagepassing distributedmemory multiprocessor b introduction loops exhibit parallelism present numerical programs therefore distributing workload loop evenly among processors critical factor efficient execution kind parallel programs loop scheduling strategy assigns iterations processors machine way finish workload less time simple strategy static scheduling determines assignment iterations processors compiletime many situations workload iterations unpredictable compile time dynamic scheduling strategies developed handle situations solving assignment problem runtime considering crossiteration data dependencies classify loops three major types doserial doall doacross pol88 paper primarily concerned doall parallel loops data dependence pair iterations otherwise published proc acm intl conf supercomputing manchester uk july 1115 1994 loop taken serial point view scheduling loop uniform execution times different iterations eg matrix multiplication semiuniform depend index loop eg adjoint convolution nonuniform depend data eg transitive closure always obtain optimal near optimal static schedule uniform loops parallel machine p processors must simply partition set iterations loop p chunks dpne iterations n length loop assign chunks processors still obtain near optimal static schedule semiuniform loops example consider parallelized adjoint convolution algorithm exhibits triangular iteration space establish near optimal static schedule using kind cyclic loop distribution scheme assign iteration rp 1 p processor processor clearly possible define optimal static schedule nonuniform loop data known runtime loops frequent many applications image processing sparse matrix computation partial differential equations among others dynamic scheduling strategies used balancing workload nonuniform parallel loop designed sharedmemory machines uniform memory access cost uma therefore data locality taken con sideration nevertheless distributedmemory machine data locality major concern must optimized paper concentrates nonuniform parallel loops discusses efficient solution scheduling kind loops distributedmemory parallel machines strategy combines features static dynamic scheduling techniques succeeds keeping cost associated remote memory accesses low section 2 describes scheduling strategies proposed literature specially distributedmemory machines approach performing loop scheduling explained section 3 section 4 shows experimental results obtained cm5 finally paper concluded section 5 related work find static scheduling solutions proposed literature pkp89 optimal compiletime scheduling strategy proposed perfectlynested parallel loops constant bounds sarkar hennessy sh86 proposed scheduling method based splitting program sequential tasks bb91 scheduling solved linear optimization problem tawbi feautrier tf92 tested heuristics solve processor allocation scheduling problem application symbolic analysis derive optimal static scheduling found hp93 nonuniform problems necessary balance load dynamically runtime selfscheduling ss ty86 simplest strategy processor repeatedly selects executes one iteration iterations executed experimental comparison selfscheduling static prescheduling algorithms found zha91 fixedsize chunk scheduling kw85 reduces high synchronization overhead found ss scheduling chunks one iteration units gss pk87 schedules large chunks beginning loop low synchronization overhead small ones toward end loop trying balance workload factoring scheduling hsf92 allocation iterations processors proceeds phases phase part remaining iterations divided equally among available processors trapezoid scheduling tn93 variation gss size successive chunks decreases linearly instead exponentially luc92 tapering scheduling proposed similar gss trapezoid scheduling different function decreasing size chunks dynamic scheduling designed sharedmemory parallel machines efficient method synchronization data sharing found distributedmemory parallel machines big difference access time local memory remote mem ories synchronization overhead datasharing cost high comparison access cost local data much better scheduling performance obtained locality data exploited recently dynamic scheduling strategies exploit data locality appeared literature ml94 affinity dynamic scheduling ads introduced numa machines algorithm gss trapezoid scheduling assigns large chunks iterations start loop execution uses deterministic assignment policy ensure chunk always assigned processor way data locality exploited works case data reused several times happens example parallel loop surrounded sequential one chunk executed repeatedly localitybased dynamic scheduling lds designed numa machines described ltss93 lds considers data space partitioned throughout processors computes sizes chunks using gsslike scheme chunk assigned processor demand includes iterations whose data stored local memory processor assigned case messagepassing machines rudolph polychronopoulos rp89 implemented evaluated static self chunk guidedself scheduling ipsc2 hypercube multicomputer first attempt evaluate sharedmemory dynamic scheduling algorithms distributedmemory multiprocessor foreverf executed processors static scheduling level dynamic scheduling levelg figure 1 hybrid scheduling hs algorithm distributedmemory machines purely dynamic scheduling strategies poor performers due relatively high synchronization overhead hand static scheduling schemes suitable nonuniform prob lems situations hybrid scheme perform bet ter exploit data locality exhibit relatively low synchronization overhead balance workload dynamically time example approach presented lsl92 twophase safe selfscheduling sss described first phase subset iterations loop distributed uniformly among processors second phase sss activated runtime processor becomes idle chunk yet executed iterations chosen assigned dynamically requesting processor strategy considers centralized queue iterations processors access data distributed safe selfscheduling dsss adaptation sss messagepassing machines discussed sll93 data partitioned small blocks size distributed partial redundancy among processors dsss proceeds sss chunks iterations assigned processors corresponding data dsss generalized ls93 dynamic phase dsss fired processor becomes idle processor charge scheduling duties distributes demand rest workload among processors inherently dynamic redistribution load may imply low performance paper describes new approach hybrid schedul ing propose twolevel scheme phases static dynamic overlap ads uses similar twolevel scheme exploitation datalocality major concern loadbalancing accomplished demand processor becomes idle case datalocality considered try predict advance workload unbalances order obtain better loadbalance reduce communication overhead processors executing statically distributed workload dynamic redistribution rest workload action hence communication overhead associated dynamic level scheduling hidden local computations experiments show twolevel strategy obtain good performance 3 scheduling scheme parallel loop consider rest paper following simple structure loop bodyi 1 hybrid scheduling hs propose accomplished two levels shown figure 1 static level first statically partition evenly distribute iteration space parallel loop among processors machine denote p number processors processor execute total iterations parallel loop 1 local loop bodyi note use lowercase indices represent local variables uppercase global ones denote ff numeric address processor relation local global indices depends static distribution scheme used example case block distributed loop form local loop body depends data distribution scheme chosen choose partition data throughout local memories processors local loop bodyi identical loop bodyi prefer replicate data local memories local loop bodyi equal loop bodyi combinations found partial replication data considered dynamic level second level requires splitting local iteration space set chunks sizes chunks may constant variable indeed may use one dynamic scheduling algorithms found literature order accomplish partitioning local iteration spaces example gss algorithm considered set n local iterations would partitioned total iteration groups chunks e size chunk number therefore local loop transformed upperbound lowerbound upperbound localloopbodyi chunk sizec computes size chunk number c actions associated dynamic level hs carried boundary execution one chunk following one way establish hybrid degree two extremes static dy namic choosing size number chunks certain unbalance workload detected dynamic level hs becomes activated tries reduce unbalance given threshold redistribution local computations still going specifically execution chunk iterations starting next one processors carry actions order arrange fastest slowest one lightly loaded heavily loaded processor point execution redistribution load chunks erations executed necessary redistribution process overlaps execution local chunks critical point hs arrange efficiently classification processors order determine processors send load receive solve workers local queue empty f send load msg scheduler remove chunk local queue execute chunk removedg else firsttime yes f send load needed msg scheduler scheduler local queue empty f increment workload counter remove chunk local queue execute chunk removedg else firsttime yes f select mhlw heavily loaded worker send load request msg mhlw workers dead local queue empty messages traveling exit load msgs received figure 2 static scheduling level problem assigning classification work one pro cessors processors workers one addition charge dynamic scheduling duties therefore workers must send messages periodically scheduler information needed order establish classification right times chunk boundaries implemented hs messagepassing distributedmemory machine using simple protocol seven different types messages meaning messages described table 1 rest section dedicated detailed description levels hs 31 static scheduling level figure 2 shows detailed description static level hs points view workers scheduler also worker level worker sends one empty load message scheduler executing one local chunks therefore time elapsed two consecutive load messages equal time elapsed beginning execution one local chunk next one scheduler carries classification workers execution local chunks coded workloadredistribution function figure 2 worker becomes idle sends load needed message scheduler requesting nonexecuted chunk worker search worker accomplished scheduler dynamic level unless requesting worker scheduler note case load request message sent selected worker mhlw minimum number nonexecuted chunks parameterized constant transferlimit mechanism type meaning load worker sends scheduler execution local chunk load needed worker sends scheduler becomes idle load migrated worker sends requesting processor next nonexecuted local chunk request failed worker sends requesting processor local queue almost empty data returned used return remotely processed data block original place load request scheduler requests worker send next nonexecuted chunk processor load finished chunks execute remotely table 1 messages used hs implementation scheduler read load msgs received update workload counters classify corresponding workers according workload workers classification list f remove mllw lightly loaded worker classification list choose mhlw heavily loaded worker classification list send load migrated msg mllw next nonexecuted local chunk else send load request msg mhlw mllw requesting processorg message sent f mllw sent load needed msg f send load finished msg mllw reduce number workers alive workers dead local queue empty messages traveling exit figure 3 workload redistribution migration remaining chunks avoided communication latency would probably eliminate benefits associated balancing remaining load specially true use algorithms like gss partition local loops last chunks small 32 workload redistribution periodically scheduler tries evenly redistribute load among workers execution local chunks scheduler reads load messages received counting determine relative local speed workers note rather instantaneous measure scheduler counts load messages received execution last local chunk based information scheduler classifies workers sent load messages order decreasing speed classification completed scheduler orders heavyloaded processors send chunks lightloaded ones see figure 3 load redistribution accomplished rest workers executing local chunks worker send load message starting execute remote chunk see next subsection way fast worker receive number successive remote chunks frequency arrival load messages reduced consequence thus scheduler eventually classify worker slow send remote load becomes fast later stage hap pens mechanism scheduler tries periodically delay fastest processors alleviate load slowest ones tradeoff accuracy measures relative speeds communication cost associated measure accuracy achieve sensitive scheduler changes local loads thus better balancing load response changes faster precise order obtain high accuracy workers must send load messages high frequency reducing size chunks increase communication overhead intolerable levels tradeoff classification accuracy associated communication cost established choosing size chunks iterations order prevent thrashing remote chunks incorporate two simple mechanisms first remote chunk always executed destination processor redistribution remote chunks several workers prohibited second workload counters used select slow workers load vector figure 3 modified load redistribution process chosen mhlw sends one nonexecuted local chunks workload counter decremented ac cordingly way single slow heavy loaded worker prevented sending many chunks risk processor lose local chunks likewise workload counter corresponding destination worker incremented remote chunk assigned trying prevent bulk arrivals 33 dynamic processor coordination hide overlap communication latency redistributing load executing local computations time processors continue execute local chunks transferred chunks traveling interconnection network local chunk following one workers check arrival control mes sage possibly remote chunk executed arrived workers immediately begin execution following local chunk otherwise execute remote chunk finally return processed remote data owner last action necessary want modify original static data distribution cases modification irrelevant example workers pending messages f switch type message received f case load finished msg pending messages exit break case load request msg send load migrated msg requesting processor next nonexecuted chunk else send request failed msg requesting processor break case load migrated msg execute migrated chunk send data returned msg migrated data owner necessary load finished msg received else messages traveling exit break case request failed msg load finished msg received else messages traveling exit break case data returned msg store remotely processed data block original location else messages traveling exit breakgg figure 4 dynamic scheduling level workers want execute parallel loop several times one another case delay reconstruction original data distribution last time parallel loop executed due communication latency lose part load balance decision redistributing load redistribution carried different times example possible fast processor enter heavily loaded part local computation receives previously transferred remote chunk case increase load currently slow processor nevertheless increase error balancing load overcome benefit reducing communication overhead net result beneficial experiments show workload redistribution phase explained section scheduler determines set workers must send chunks set must receive need establish dynamic processor coordination order efficiently transfer chunks among selected workers coordination workers check different kinds messages execute actions according control scheme shown figures 4 5 one argue number workers large scheduler become bottleneck large number workers scheduler would probably execute first local chunks redistribute rest among workers depends scheduler pending messages f switch type message received f case load needed msg select mhlw heavily loaded worker send load migrated msg requesting processor next nonexecuted chunk else send load request msg mhlw send load finished msg requesting processor reduce number workers alive workers dead local queue empty messages traveling exit break case load migrated msg execute migrated chunk send data returned msg migrated data owner necessary break case request failed msg break case data returned msg store remotely processed data block original location else messages traveling exit breakgg figure 5 dynamic scheduling level scheduler static load initially assigned scheduler relation loads workers indeed experiments show general efficiency parallel loop lower scheduler one initially lightly loaded processors moreover scheduler charge periodic classification workers core workload redistribution process may introduce another potencial performance bottleneck paper show results using one scheduler possibility several schedulers considered future 4 experimental evaluation implemented hs cm5 using cmmd messagepassing library thi91 addition hs order make comparisons fully static fully dynamic schedulings implemented well measured performance scheduling algorithms using two benchmark programs 41 benchmark programs chosen matrix multiplication mm simpli fied transitive closure tc benchmark programs mm typical example regular uniform prob lem tc represents irregular non uniform one workload parallel loop depends heavily data content ffl matrix multiplication program three perfectly nested loops innermost one contains reduction operation two loops fully parallel experiments parallelized program outermost loop regularity program useful evaluating effectiveness hs compared optimal static scheduling transitive closure tc program 3depth nested loop well outermost loop serial next one parallel reason first loop dropped experiments one iteration outermost loop considered follows sequential innermost loop may may exe cuted hence computation nonuniform depending input data program allow us evaluate performance hs fully dynamic environment 42 scheduling algorithms implemented following scheduling algorithms static scheduling static hs data partitioned throughout local memories hsp hs data replicated local memories hsr dynamic scheduling data partitioned throughout local memories considering scheduler worker dynamicp charge scheduling duties dynamicps static scheduling consists blockwise static distribution iterations parallelized loop matrices partitioned block distributed well case mm matrices c block partitioned rows partitions stored respective local matrices c matrix b replicated local code processor follows tc matrix block partitioned rows well stored local matrix first row replicated renamed a1 local tc due fact execution time loop body parallelized loop tc small introduced another loop surrounding granularity control loop number loopbodysize iterations loop permits parameterizing loop body size studying effect experiments order obtain initially poor workload balance static scheduling tc benchmark taken input matrix 1s first half rows 0s rest data static scheduling performs around 50 efficiency way clearly evaluate benefits including dynamic actions scheduling strategy irregular problem considered data distribution scheme static level hsp hsp hsr analyze global impact data migration performance hs partitioning scheme may imply great amount data migration case scheme chosen partition local loops gss0 scheduler gss2 rest workers gss2 obtain reasonably small communication overhead due relatively reduced number messages sent scheduler workers chunks presents bigger sizes hand using gss0 scheduler improve sensi tiveness dynamic workload changes hence obtain reasonable load balancing selected value 2 1 transferlimit parameter workers sched uler algorithm gss0 chosen fully dynamic schedulings cases data migrated always considered returns original owner execution remote chunk 43 results interpretation figure 6 displays efficiency versus number processors mm case two different matrix sizes comparing performance three scheduling algorithms static hybrid dynamic computation efficiency executed scheduling algorithm one processor see best performer static scheduling worst performer dynamic one hs performs quite similar static scheduling especially case sufficiently large matrices behavior explained fact workers process almost workload loop evenly distributed across local memories compiletime workers send load messages less time sched uler execution time chunks sufficiently large messages arrive scheduler difference time due different communication paths taken smaller execution time single chunk way workload counters contain number difference one unit processors thus scheduler never decides migrate local chunks hs works similar static strategy overhead associated control messages possible load unbalance may appear mm 10010009070503 number processors efficiency number processors09070503 mm 400400 hsr number processors efficiency b hsr mm 400400 number processors099097095093 efficiency figure efficiency matrix multiplication last local chunk processors case transferlimit parameter prevents migration remaining chunks besides explains versions hsp hsr perform roughly similar way shown figure 6 b tc program hsp performs best large loop bodies see figure 7 experimented two matrix sizes three values upper bound granularity control loop loopbodysize 10 100 1000 dimension input matrix andor size parallelized loop body sufficiently large efficiency hsp rises around 75 80 instead best purely dynamic scheduling always static one results shows advantages considering data locality static level hs overlapping chunk migrations local computations dynamic level hs data size loop body small chunk migration costs partially hidden local computation time case performance hs poor becomes worse number processors increases note large input matrix efficiency hsp remains around 75 number processors considered therefore use one scheduler sufficient order balance load among dozens processors input matrix chosen half 1s half 0s splits processors two classes heavily loaded lightly loaded processors initial static loop distribution half processors charge computations half idle experimented influence initial workload assigned scheduler performance hs results shown figure 8 data partitioned replicated versions hs considered one hand important point large loop bodies efficiency hs greatly affected data distribution scheme chosen hand efficiency noticeably worse lightly loaded initially idle scheduler chosen reasonable scheduler kind goes workload redistribution level frequently thus sends many messages heavily loaded processors ordering migrate many chunks hence total communication cost high behavior prevented scheduler heavy workload expense probably worse workload balance indeed found many cases load balanced worse scheduler heavily loaded worker nevertheless efficiency former case better due limited communication cost number processors0503012 4 8 efficiency static number processors efficiency number processors060402 efficiency number processors efficiency number processors efficiency number processors efficiency figure 7 efficiency transitive closure conclusions studied problem loop scheduling distributedmemory multiprocessors loop regular compiletime static scheduling near optimal solution need migrate load among processors loop irregular execution time loop body depends data example thus need consider runtime dynamic techniques order manage load imbalances well dynamic scheduling algorithms work well sharedmemory environment memory distributed take data locality account want keep communication overhead certain limit believe best solution would combine static dynamic strategies loop scheduling algorithm want obtain high performance distributedmemory multiprocessor described algo rithm hybrid scheduling hs static dynamic nature presented two levels conception permits overlapping local computations migration part workload way large part high communication overhead associated migrations contribute completion time order orchestrate dynamic workload balance one workers time charge dynamic scheduling duties evaluated performance hs cm5 regular program matrix multiplication hs performs near static scheduling depending data size case hs carry load migration overhead associated dynamic level low irregular program transitive closure hs performs best loop body sufficiently large many data sizes load well balanced communication overhead associated migrations low results indicate combination effects general beneficial machine cm5 important result performance hs sensitive data distribution scheme chosen benchmarks tested important many applications easy find best data dis tribution investigations include depth study influence data distribution chosen static level performance moreover working multischeduler extension hs obtaining experimental results different kind distributedmemory machine acknowledgements research benefited significantly discussions suggestions constantine polychronopou los material experimental results obtained research stay center supercomputing research development university illinois urbanachaimpaign would like thank david padua compiler group support gave us research supported part cicyt grant tic920942c0303 xunta de galicia grant xuga20604a90 r scheduling algorithm parallelizable dependent tasks symbolic analysis basis parallelization optimization scheduling programs factoring method scheduling parallel loops allocating inde pending subtasks parallel processors selfscheduling distributedmemory machines scheduling parallel loops variable length iteration execution times parallel comput ers locality loop scheduling numa multi processors dynamic scheduling method irregular parallel programs using processor affinity loop scheduling sharedmemory multiprocessors guided selfscheduling practical scheduling scheme parallel supercomputers utilizing multidimensional loop parallelism largescale parallel processor systems parallel programming compilers efficient messagepassing scheduler based guided selfscheduling compiletime partitioning scheduling parallel programs schedul ing nonuniform parallel loops distributed memory machines processor allocation loop scheduling multiprocessor com puters processor selfscheduling multiple nested parallel loops dynamic static load scheduling performance numa shared memory multi processors tr allocating independent subtasks parallel processors compiletime partitioning scheduling parallel programs guided selfscheduling practical scheduling scheme parallel supercomputers utilizing multidimensional loop parallelism large scale parallel processor systems dynamic static load scheduling performance numa shared memory multiprocessor factoring method scheduling parallel loops dynamic scheduling method irregular parallel programs processor allocation loop scheduling multiprocessor computers selfscheduling distributedmemory machines efficient messagepassing scheduler based guided self scheduling parallel programming compilers trapezoid selfscheduling using processor affinity loop scheduling sharedmemory multiprocessors symbolic analysis ctr babak hamidzadeh lau ying kit david j lilja dynamic task scheduling using online optimization ieee transactions parallel distributed systems v11 n11 p11511163 november 2000