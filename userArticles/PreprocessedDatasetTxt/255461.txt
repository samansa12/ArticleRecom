pruning algorithms rule learning prepruning postpruning two standard techniques handling noise decision tree learning prepruning deals noise learning postpruning addresses problem overfitting theory learned first review several adaptations pre postpruning techniques separateandconquer rule learning algorithms discuss fundamental problems primary goal paper show solve problems two new algorithms combine integrate pre postpruning b introduction separateandconquer rulelearning systems gained popularity recent success inductive logic programming algorithm foil quinlan 1990 analyze different pruning methods type inductive rule learning algorithm discuss problems main contribution paper two new algorithms topdown pruning tdp approach combines pre postpruning incremental reduced pruning irep efficient integration preand postpruning prepruning decisions combining pre postpruning integrating pre postpruning postpruning prepruning literals postpruning decisions figure 1 pruning methods separateandconquer rule learning algorithms pruning common framework avoiding problem overfitting noisy data basic idea incorporate bias towards general simpler theories order avoid overly specific theories try find explanations noisy examples prepruning methods deal noise learning instead trying find theory complete consistent given training data heuristics socalled stopping criteria used relax constraint stopping learning process although positive examples may yet explained negative examples may still covered current theory final theory learned one pass see figure 1 separateandconquer rule learners like cn2 clark niblett 1989 foil quinlan 1990 fossil furnkranz 1994 use form noise handling another family algorithms deals noise learning postpruning algorithms typically first induce theory complete consistent training data theory examined rules conditions discarded seem explain characteristics particular training set thus reflect true regularities domain figure 1 shows schematic depiction process quality found rules conditions commonly evaluated separate set training examples seen learning postpruning algorithms include reduced error pruning rep brunk pazzani 1991 grow cohen 1993 shown effective noisehandling however also inefficient waste time learning overfitting concept description subsequently pruning significant portion rules conditions one remedy problem combine pre postpruning purpose prepruning heuristics used reduce entirely prevent amount overfitting learning pruning efficient sketched third part figure 1 particular implementation approach topdown pruning tdp furnkranz 1994 uses simple algorithm generate set theories pruned different degrees top generaltospecific order accuracies theories evaluated separate set data specific theory accuracy comparable accuracy best theory far submitted subsequent postpruning phase experiments show initial topdown search better starting theory efficient overfitting phase classical postpruning algorithms search typically return theory closer final theory postpruning phase also sped less pruning operations needed get final theory motivated success method developed rigorous approach tightly integrates pre postpruning instead learning entire theory pruning thereafter incremental reduced error pruning irep furnkranz widmer 1994 prunes single clauses right learned new algorithm entirely avoids learning overfitting theory using postpruning methods prepruning stopping criterion shown figure 1 method significant speedup achieved noisy domains avoids problems approaches incorporate postpruning irep also learns accurate theories learning algorithms many rule learning algorithms try construct rules socalled separateandconquer strategy method roots early days machine learning covering algorithm famous aq family michalski 1980 michalski mozetic hong lavrac 1986 cn2 clark niblett 1989 clark boswell 1991 combined aqs covering strategy greedy informationbased test selection id3 quinlan 1983 yielded powerful rule learning algorithm term separateandconquer coined pagallo haussler 1990 context learning decision lists finally separateandconquer learning basic control structure foil algorithm efficiently inducing logic programs quinlan 1990 pioneered significant research field relational learning inductive logic programming figure 2 shows basic separateandconquer rule learning algorithm input algorithm set positive negative examples target concept output set rules able prove given positive examples none negative examples represent rules form prolog clauses general separateandconquer learning algorithm foil procedure separateandconquerexamples negativecover returntheory figure 2 separateandconquer rule learning algorithm concept literal1 propositional learning cn2 conditions tests values certain attributes concept relational learning foil one also specify relations attributes head conditions rule general prolog literals consider set rules prolog program ie rules checked order one fires example fulfilled conditions rule consequently classified instance learned concept rules fires instance considered member concept separateandconquer constructs rules successively adding conditions righthand side current rule process repeated enough conditions found rule negative examples positive examples covered rule separated training set next rule learned remaining examples hence name separateandconquer rules learned way positive examples left method guarantees positive example covered least one rule completeness rule covers negative example consistency simple separateandconquer algorithm figure 2 severe drawback realworld data may noisy noisy data problem many learning algorithms hard distinguish rare exceptions erroneous examples fundamental algorithm figure 2 forms complete consistent theory e tries explain positive examples none negative examples presence noise therefore attempt find explanations negative examples erroneously classified positive try exclude positive examples negative classification training set explanations noisy examples typically complicated exhibit low predictive accuracy classifying unseen examples problem known overfitting noise one remedy problem try increase predictive accuracy considering complete consistent theories also simple theories may overgeneral training examples final theory allowed deliberately cover negative training examples leave positive training examples uncovered order learn simpler predictive theories usually achieved via pruning heuristics procedure prepruningexamples negativecover stoppingcriteriontheorynewclausecover exit exit returntheory figure 3 rule learning algorithm using prepruning 3 prepruning figure 3 shows adaptation simple separateandconquer algorithm order address noisy data prepruning heuristic algorithm identical one figure 2 except inner loop contains call subroutine stoppingcrite rion stopping criterion heuristic determines stop adding conditions rule stop adding rules concept description current rule new condition added fulfills stopping criterion inner loop terminate incomplete clause added concept description clause contains literal assumed clause found explains remaining positive examples theory without clause returned remaining positive examples thus considered noisy classified negative returned theory separateandconquer algorithms employ stopping criteria noise handling commonly used among encoding length restriction heuristic used inductive logic programming algorithm foil quinlan 1990 based minimum description length principle rissanen 1978 tries avoid learning complicated rules cover examples making sure number bits needed encode current clause less number bits needed encode instances covered 1 ffl significance testing first used propositional cn2 induction algorithm clark niblett 1989 later relational learner mfoil dzeroski bratko 1992 tests significant differences distribution positive negative 1 number bits needed encode training instances log 2 nlog 2 number training instances p positive instances covered current clause literals encoded specifying relation log 2 relations bits variables log 2 variabilizations bits whether negated 1 bit sum terms literals reduced log 2 n since ordering literals within clause general irrelevant examples covered rule overall distribution positive negative examples comparing likelihood ratio statistic 2 2 distribution 1 degree freedom desired significance level insignificant rules rejected103050709011013015017010 09 cutoff complexity complexity20406080100 cutoff accuracy accuracy figure 4 accuracy complexity vs cutoff ffl cutoff stopping criterion used separateandconquer learning system fossil 1994 fossil uses search heuristic based statistical corre lation enables judge relevance literals uniform scale 0 1 thus user require conditions considered clause construction certain minimum correlation value cutoff parameter property used simple robust criterion filtering noise expected tuples originating noise data low correlation predicates background knowledge different settings values cause different amounts prepruning setting results learning theory complete consistent current training set every literal correlation 00 hand general empty theory learned trivial learning problems background literals correlation 10 figure 4 shows typical plot accuracy rule complexity vs different values cutoff parameter commonly used krk endgame classification task 10 noise added 3 accurate rules found cutoff values approximately 025 035 higher cutoff values result overgeneral theories lower settings cutoff obviously result overfitting data thus fossils cutoff parameter may viewed means directly controlling overfitting avoidance bias schaffer log pn pn pn pn 3 short description krk domain along experimental setup found beginning section 71 procedure postpruningexamples splitratio examples growingset pruningset loop exit loop returntheory figure 5 postpruning algorithm 1993 wolpert 1993 setting cutoff 03 good general heuristic seems independent noise level data furnkranz 1994 4 postpruning prepruning approaches try avoid overfitting rule generation postpruning approaches first ignore problem overfitting noise learn complete consistent concept description resulting theory subsequently analyzed necessary simplified generalized order increase predictive accuracy unseen data postpruning approaches commonly used decision tree learning algorithms cart breiman friedman olshen stone 1984 id3 quinlan 1987 assistant niblett bratko 1986 overview comparison various approaches found mingers 1989 esposito malerba semeraro 1993 41 reduced error pruning common among methods reduced error pruning rep simple algorithm adapted decision tree learning quinlan 1987 separateand conquer rule learning framework pagallo haussler 1990 brunk pazzani 1991 beginning training data split two subsets growing set usually 23 pruning set 13 first phase attention paid noise data concept description explains positive none negative examples learned growing set resulting theory simplified greedily deleting conditions rules theory deletion would result decrease predictive accuracy measured pruning set pseudocode version algorithm shown figure 5 subroutine prunetheory simplifies current theory deleting conditions rules usually one time resulting set theories selects one highest accuracy pruning set continues prune theory repeated accuracy best pruned theory predecessor rep shown learn accurate theories prepruning algorithm foil krk domain several levels noise brunk pazzani 1991 42 problems reduced error pruning although rep quite effective raising predictive accuracy noisy domains brunk pazzani 1991 several shortcomings discuss section particular suggest postpruning incompatible separateandconquer learning strategy efficiency cohen 1993 shown worstcase time complexity rep bad random data n number examples growing initial concept hand n therefore long run costs pruning far outweigh costs generating initial concept description already higher costs using prepruning algorithm entirely avoids overfitting hillclimbing rep employs greedy hillclimbing strategy literals clauses deleted concept definition predictive accuracy pruning set greedily maximized possible operator leads decrease predictive accuracy search process stops local maximum however noisy domains theory generated growing phase much specific see figure 4 rep prune significant portion theory ample opportunity err way therefore also expect reps specifictogeneral search slow also inaccurate noisy data separateandconquer strategy postpruning algorithms originate research decision tree learning usually wellknown divideandconquer learning strategy used node current training set divided disjoint sets according outcome chosen test algorithm recursively applied sets independently although separateandconquer approach shares many similarities divide andconquer strategy one important difference pruning branches decision tree never affect neighbouring branches whereas pruning literals rule affect subsequent rules figure 6 illustrates postpruning decision tree learning works right half initially grown tree covers sets c training instances pruning algorithm decides prune two leaves ancestor node becomes leaf covers examples cd left branch decision tree influenced operation hand pruning literal clause means clause generalized ie cover positive instances along negative instances consequently additional positive negative instances removed training set cannot influence learning subsequent clauses example figure 6 b first three rules simplified covers examples original version covered also examples third rule covered several examples second rule covered third rule could easily removed postpruning algorithm necessarily case example guarantee pruning training examples pruning training examples training examples c b figure postpruning divideandconquer b separateandconquer learning algorithms second rule one pruned versions good explanations remaining set examples b2 b2 subset original set b pruning operators generalize concept ie increase set covered examples might well good explanation b2 needs totally different set literals explanation superset b thus learner may lead garden path unpruned clauses beginning theory may change evaluation candidate literals subsequent clauses wrong choice literal cannot undone pruning 43 grow algorithm solve problems section 42 particular efficiency topdown postpruning algorithm based technique used pagallo haussler 1990 proposed cohen 1993 like rep grow algorithm first finds theory overfits data instead pruning intermediate theory deletion results decrease accuracy pruning set first step intermediate theory augmented generalizations clauses second step clauses expanded theory iteratively selected form final concept description clause improves predictive accuracy pruning set found generalizations clauses intermediate theory formed repeatedly deleting final sequence conditions clause error growing set goes least thus grow improves upon rep replacing bottomup hillclimbing search rep topdown approach instead removing useless clause literal specific theory adds promising generalization rule initially empty theory results significant gain efficiency along slight gain accuracy experiments cohen 1993 show however asymptotic time complexity grow postpruning method still complexity initial rule growing phase recently shown cameronjones 1994 explanation speedup gained topdown strategy starts empty theory many noisy domains much closer final theory overfitting theory also seen figure 4 look complexities specific theory complexities optimal theories cutoff 025 035 thus surprising grow shown outperform rep variety datasets cohen 1993 however still suffers inefficiency caused need generating overly specific theory first pass combining pre postpruning section 4 seen intermediate theory resulting initial overfitting phase much complex final theory postpruning inefficient case work performed learning phase undone pruning phase natural solution problem would start pruning phase simpler theory idea first investigated cohen 1993 efficient postpruning algorithm grow see section 43 combined weak prepruning heuristics speed learning phase goal prepruning context entirely prevent overfitting reduce amount thus subsequent postpruning phase less work less likely go wrong however always danger predefined stopping criterion overgeneralize theory section therefore discuss alternative approach searches appropriate starting point postpruning phase 51 topdown pruning one advantage fossils simple efficient cutoff stopping criterion furnkranz 1994 closeness search heuristic fossil needs mere comparison heuristic value best candidate literal cutoff value order decide whether add candidate literal clause hand property used generate theories could learned fossil setting cutoff parameter see figure 7 basic idea behind algorithm following assume fossil trying learn theory cutoff 10 unless one literal background knowledge perfectly discriminates positive negative examples case trivial examples parentab childba find literal correlation 10 thus learn empty theory procedure alltheoriesexamples cutoff 00 theories theory returntheories figure 7 algorithm generate theories learnable fossil however remember literal maximum correlation use information following way make another call fossil cutoff set exactly maximum correlation value least one literal one produced maximum correlation added theory typically followed several literals correlation value higher new cutoff result new theory usually little specific predecessor maximum correlation literals cut remembered obviously values old cutoff new maximum theory would learned thus choose value cutoff next run also expected new theory specific previous one process repeated certain setting cutoff literal pruned maximumprunedcorrelation 00 thus specific theory reached figure 8 shows complete series theories generated fossil 1000 noisefree examples domain distinguishing legal illegal positions kingrookking chess endgame setting cutoff parameter would yield one six theories training set seen theories generated less general specific order topdown simpler theories expected accurate noisy domains best theories learned iterations therefore may possible stop generation theories soon reasonably good theory found order avoid expensive learning many overlyspecific theories may save lot work figure 4 indicates besides also possible reuse parts previous theory point highest cutoff occurred total cost generating complete series concept descriptions may much higher cost generating merely specific theory least cases cutoff occurs near end learned theory frequently case based ideas conceived algorithm shown figure 9 uses basic algorithm figure 7 find best theory order avoid overgeneralization tries find specific among reasonably good theories learned fossil uses theory starting point reduced error pruning precisely generates theories generaltospecific order evaluates designated test set data usually 13 stops measured accuracy one theories falls measured accuracy best theory far minus one standard error classification 4 last theory within 1se margin hopefully little specific 4 based idea cart breiman friedman olshen stone 1984 general 6704 correct 0 positive 100 negative e e 8842 correct 6553 positive 9967 negative 9760 correct 9339 positive 9967 negative 9936 correct 9848 positive 9979 negative 9932 correct 9860 positive 9967 negative 9742 correct 9260 positive 9979 negative figure 8 generating series theories krk domain procedure tdpexamples splitratio examples growingset pruningset repeat loop exit loop returntheory figure 9 combining pre postpruning topdown pruning general subsequently generalized using reduced error pruning initial generaltospecific search good theory named method topdown pruning tdp algorithm succeeds finding starting theory close final theory expect algorithm faster basic rep initial search good starting theory ffl speed growing phase expensive theories generated 5 ffl speed pruning phase pruning starts simpler theory thus number possible pruning operations smaller preliminary experiments turned sometimes cutoff happens point small fraction available positive examples covered clearly theories useless therefore added constraint theories cover 50 positive examples growing set evaluated pruning set theory fulfill criterion improved adding clauses achieved lowering cutoff value would needed start new clause 6 pruned decision tree within one se best selected standard classification error computed n p probability misclassification estimated pruning set n number examples pruning set 5 argument course apply noisy domains nonnoisy domains specific theory general precise thus algorithm slower generate theories cutoff 00 6 note method may yield theory learnable original fossil value cutoff parameter changed generation theory 52 experimental results compared topdown pruning tdp reduced error pruning rep terms accuracy runtime krk endgame domain 10 artificial noise added setup experiments described detail beginning section 71 algorithms split supplied data sets growing ca 23 pruning sets ca 13 algorithms used reduced error pruning described brunk pazzani 1991 postpruning phase order exclude possible influences underlying learning algorithm ran rep using fossil basic learning module 7 average accuracy 10 runs 100 250 500 750 rep pruning 8484 8688 8711 8921 pruning 9467 9672 9780 9851 tdp pruning 8915 9102 9589 9585 pruning 9514 9593 9829 9870 table 1 accuracy krk domain 10 noise table 1 shows tdp worse rep terms predictive accuracy rep better training set size 250 tdp heavily overpruned one cases tdp started theory 9842 correct unfortunately one literals support pruning set consequently pruned thus yielding theory mere 8134 happen rep got caught 9136 correct theory even get 9842 theory increasing training set sizes tdp seems slightly superior rep although differences probably small statistically significant comparing accuracies intermediate theories shows tdp starts significantly better theories rep see first line table 1 obviously topdown search better starting theories successful particular higher training set sizes rep sometimes gets stuck local optimum returns bad theories however seen rep may profit rare cases tdp less likely get stuck local optimum pruning starts initial theory already quite close final theory problem local optima greedy hillclimbing also likely appear tdps topdown search starting theory least domain intermediate theories usually appear iterations tdps toplevel loop comparing runtimes rep tdp table 2 confirms tdp significantly faster rep fact even faster reps initial phase overfitting alone tdp find fairly general theories rep generates huge theories fit noisy examples expectedly increasing training set sizes costs rep dominated pruning process tdp hand even manages decrease pruning time growing training set sizes 250 500 significant runtime increase 500 750 examples mainly due one 10 sets much specific theory learned 85594 cpu secs growing 139935 cpu secs pruning time remaining 9 sets average runtime 11674 cpu secs growing 1288 cpu secs pruning 7 version rep using fossil better version using foil section 71 show results obtained using implementation foil generate initial theory rep average runtime 10 runs 100 250 500 750 rep growing 666 7522 39717 84576 pruning 293 9146 124848 292266 total 959 16668 164565 376842 tdp growing 723 5137 8017 19066 pruning 124 2249 1639 15152 total 847 7386 9656 34218 table 2 runtime krk domain 10 noise results confirm tdp exhibits fast convergence towards good theories faster rep learning pruning starting theories learned fossil become increasingly accurate training set grows means learning faster also less less pruning done 6 integrating pre postpruning algorithm present section motivated observation postpruning incompatible separateandconquer learning strategy discussed section 42 problem attempted solve postpruning approaches take account pruning clause generalize eventually covers examples training set may influence evaluation candidate literals subsequent clauses 61 incremental reduced error pruning basic idea incremental reduced error pruning irep instead first growing complete concept description pruning thereafter individual clause pruned right generated ensures algorithm remove training examples covered pruned clause subsequent clauses learned thus avoided examples influence learning following clauses figure shows pseudocode version algorithm usual current set training examples split growing usually 23 pruning set usually 13 however entire theory one clause learned growing set literals deleted clause greedy fashion deletion would decrease accuracy clause pruning set resulting rule added concept description covered positive negative examples removed training growing pruning set remaining training instances redistributed new growing new pruning set ensure two sets contains predefined percentage remaining examples sets next clause learned predictive accuracy pruned clause predictive accuracy empty clause ie clause body fail clause added concept description irep returns learned clauses thus accuracy pruned clauses pruning set also serves stopping criterion postpruning methods used prepruning heuristics procedure irep examples splitratio examples growingset pruningset negativecover loop exit loop accuracyclausepruningset accuracyfailpruningset exit returntheory figure 10 integrating pre post pruning incremental reduced error pruning algorithm prune entire set clauses prunes one successively named incremental reduced error pruning irep expect irep improve upon postpruning algorithms aimed solving problems discussed section 42 efficiency ireps asymptotic complexity size training set significantly lower complexity growing overfitting theory shown omegagamma n 2 log n assumptions cohen 1993 rep growing one clause purely random data costs n log n approximately logn literals tested n examples irep considers every literal clause pruning ie log n literals evaluated n examples final clause found ie log n times thus costs pruning one clause n log 2 n assuming size final theory constant overall costs hillclimbing similarly grow irep uses topdown approach instead reps bottomup final programs found removing unnecessary clauses literals overly specific theory repeatedly adding clauses initially empty theory however grow still generate intermediate overly specific concept description irep directly constructs final theory separateandconquer strategy irep learns clauses order used prolog interpreter subsequent rules learned clause completed learned pruned covered examples removed reason problem incompatibility learning strategy pruning strategy cannot appear irep 62 experimental results table 3 shows comparison runtimes postpruning algorithms irep krk domain 10 artificial noise added algorithms used foils information gain criterion search heuristic column initial rule growth refers initial growing phase rep grow common columns rep grow give results pruning phases total runtime rep grow runtime initial rule growth plus runtime rep grow irep phases tightly integrated total value runtime given domain initial rule growth rep grow irep krk1000 10 212989 2312534 80689 11535 table 3 average runtime obvious irep significantly faster postpruning algorithms fact always faster reps grows initial growing phase alone irep avoids learn intermediate overfitting theory also seen grows pruning algorithm much faster reps confirms results cohen 1993 order get idea asymptotic complexity various algorithms performed loglog analysis cameronjones 1994 estimate asymptotic complexity dividing differences logarithms two runtimes differences logarithms corresponding training set sizes thus estimating slope loglogplot tabulated slopes adjacent training set sizes table 4 domain initial rule growth rep grow irep 100250 261 411 271 154 500750 226 378 315 146 7501000 216 400 279 112 table 4 loglog analysis runtimes noisy krk data fact table suggests irep subquadratic time complexity consistent conjecture ireps time complexity omegagamma n log 2 n general results get consistent analysis performed cameronjones 1994 random data surprising view noiselevel degree randomness data particular evidence supports result rep complexity omegagamma n 4 initial rule growing phase 2 log n shown cohen 1993 also confirms main result cameronjones 1994 namely asymptotic complexity grow asymptotic complexity initial rule growing phase originally suggested cohen 1993 however experiments absolute values runtime grows pruning phase negligible compared initial overfitting phase rep often gets caught local maxima able generalize right level interestingly observed despite topdown search strategy grow also occasionally overfits noise data phenomenon also predicted cameronjones 1994 irep hand stop generating clauses whenever found clause support pruning set therefore irep expected fast runtimes purely random data rep grow expensive high chance first clause fit examples pruning set stop algorithm immediately without accepting single clause thus effectively avoid overfitting domain initial rule growth rep grow irep krk500 10 8429 9762 9817 9848 krk1000 10 8565 9801 9830 9955 table 5 average accuracy terms accuracy table 5 irep also superior postpruning algorithms although seems sensitive small training set sizes reason bad distribution growing pruning examples may cause ireps stopping criterion prematurely stop learning redistributing examples new growing pruning sets learning new clause cannot help little redundancy data small sample size however larger example set sizes irep outperforms algorithms 7 experimental evaluation tested algorithms presented paper variety domains algorithms implemented sicstus prolog major parts implementations common particular shared interface data used procedures splitting training sets mode type symmetry information background relations used restrict search space wherever applicable information gain used search heuristic rep grow irep fossils correlation heuristic used fossil tdp runtimes measured cpu seconds sun sparcstations elc 71 summary experiments krk domain first summarize experiments domain recognizing illegal chess positions krk endgame muggleton bain hayesmichie michie 1989 domain become standard benchmark problem relational learning systems cannot solved trivial way propositional learning algorithms background knowledge contain relations like signs 10 training instances deliberately reversed generate artificial noise data learned concepts evaluated test sets 5000 noisefree ex amples used stateoftheart relational learner foilquinlan 1990 benchmark 8 foil 61 implemented c used default settings except v 0 option set avoid introduction new variables necessary task algorithms argument modes declared input effect prevent recursion algorithms trained identical sets sizes 100 1000 examples reported results averaged 10 runs except training set size 1000 6 runs performed complexity task algorithms figure shows curves accuracy runtimes 5 different training set sizes irep bad start 8455 accuracy 100 examples achieves highest accuracy predictive accuracy foil poorly stopping criterion encoding length dependent training set size thus weak effectively prevent overfitting noise 1000 examples foil learns concepts 20 rules incomprehensible furnkranz 1994 irep hand consistently produces 9957 correct understandable 4rule approximation correct concept description theory correctly identifies illegal positions except ones white king black king white rook thus blocks check would make position illegal white move postpruning approaches rep grow equal tdp lose accuracy compared three however rarely find 4th rule specifies white king white rook must square also seen prepruning approach taken fossil needs many examples order make heuristic pruning decisions reliable fossil hand fastest algorithm foil although implemented c slower increasing training set sizes learns clauses fossil see also 1994 rep proves pruning method inefficient grow efficient pruning algorithm still suffers expensive overfitting phase tdp faster rep grow able start postpruning much better theory rep grow irep however learns much better theory faster growing pruning phase tdp fact irep postpruning integrated prepruning criterion little slower fossil much accurate thus said truly combines merits postpruning accuracy prepruning efficiency becomes also apparent figure 12 accuracy standard deviations observed different runs plotted logarithm runtime 8 current version foil available anonymous ftp ftpcssuozau 1297881 file name pubfoilnsh integer n experiments performed version 61 training training irep grow fossil foil 6150015002500 100 200 300 400 500 600 700 800 900 1000 run time cpu secs training runtime vs training irep grow fossil foil 61 figure 11 krk domain 10 noise different training set sizes runtime cpu secs irep grow fossil foil 61 figure 12 krk domain 10 noise 1000 examples 72 mesh domain also tested algorithms finite element mesh design problem first studied described detail dolsak muggleton 1992 problem mesh design break complex objects number finite elements order able compute pressure deformations force applied object basic problem manual mesh design selection optimal number finite elements edges structure several authors tried ilp methods problem dolsak muggleton 1992 dzeroski bratko 1992 quinlan 1994 available background knowledge consists attributebased description edges topological relations edges setup experiments quinlan 1994 ie learned rules four five objects data set tested learned concept fifth object learned theories tested quinlan 1994 little different setup used dzeroski bratko 1992 instead actually predicting value number finite elements edge merely checked possible values whether value could derived learned rules basic difference tested ground instances whereas dzeroski bratko 1992 tested target predicate unbound value number finite elements positive examples setting also test learned theories negative examples make sure overgeneral table 6 two numbers given five sets first number accuracy positive examples second number shows accuracy testing negative examples well given runtimes total runtimes learning pruning irep clearly faster postpruning algorithms without losing predictive accuracy tdp finds accurate starting theory rep shorter time span consequently pruning time much shorter reps learned theory little accurate however tdp faster grow although starts pruning phase algorithm accuracy fossil 9097 000 1599 initial theory rep grow 8742 3147 635569 grow 8927 2375 988032 initial theory tdp 8899 2889 376294 irep 9014 1281 47125 table experiments mesh domain simpler theory reason implementation tdp uses rep prune theory results initial search good starting theory might worthwhile improve tdp using grow algorithm postpruning phase also indicates domain tdps initial topdown search effective krk domain work left postpruning phase algorithm faster accurate irep fossil cutoff 03 however fossil couldnt discover significant regularities data thus consistently learned empty theories literals background knowledge correlation 03 nevertheless still best algorithm terms accuracy shows poorly algorithms domain hope able improve results domain trying faster algorithms new data set dolsak bratko jezernik 1994 contains total 10 objects thus hopefully provides redundancy however comparative study new data set big interesting phenomenon although pruning literals generalizes clauses positive examples covered pruned theories whole cover fewer positive examples obviously many learned rules generalization improve accuracy much removing entire rule therefore overall accuracy theory primarily optimized deleting many rules cover positive examples also equal greater number negative examples also taken evidence regularities detected basic separateandconquer induction module reliable 73 propositional data sets also experimented data sets uci repository machine learning databases previously used compare propositional learning algorithms appendix holte 1993 gives summary results achieved various algorithms commonly used data sets uci repository short description sets selected 9 experiments remaining sets used either description data sets unclear two classes could handled implementation learning algorithms lymphography data set removed 6 examples classes normal find fibrosis order get 2class problem data used described holte 1993 data sets task learn definition minority class datasets background knowledge consisted relations one variable breast cancer accuracy stnd dev range time fossil 7333 456 1766 1968 grow 6846 472 1539 18367 tdp 7174 379 1243 17331 irep 7089 523 1958 2897 hepatitis accuracy stnd dev range time fossil 7607 577 2343 21740 rep 7696 393 1080 10228 grow 7645 424 1114 10239 tdp 7942 388 1187 11624 irep 7866 280 734 6040 sick euthyroid accuracy stnd dev range time fossil 9758 040 135 89140 rep 9755 032 106 504023 grow 9752 047 164 463526 irep 9748 050 170 97070 table 7 results breast cancer hepatitis sick euthyroid domains one constant argument wherever appropriate comparisons two different variables data type allowed well experiments value fossils cutoff parameter set 03 runtimes datasets measured cpu seconds sun sparcstations elc except mushroom krkpa7 datasets quite big thus run considerably faster sparcstation s10 experiments followed setup used holte 1993 ie algorithms trained 23 data tested remaining 13 however 10 runs performed algorithm data set results found tables 7 8 9 line shows average accuracy 10 sets standard deviation range difference maximum minimum accuracy encountered runtime algorithm results c45 decision tree learning system extensive noisehandling capabilities quinlan 1993 taken experiments performed holte 1993 meant indicator performance stateoftheart decision tree learning algorithms data sets short look shows results vary terms accuracy quite consistent glass g2 accuracy stnd dev range time fossil 7732 479 1596 21642 rep 7776 431 1473 9331 grow 7563 469 1697 9311 irep 7631 489 1595 6301 votes accuracy stnd dev range time fossil 9535 117 334 10522 rep 9584 139 392 5741 grow 9563 136 392 5384 tdp 9522 154 449 6217 irep 9475 175 695 2243 votes vi accuracy stnd dev range time fossil 8907 264 813 8894 rep 8672 346 1078 16326 grow 8749 335 1093 13749 irep 8725 327 1075 3878 table 8 results glass votes domains runtimes irep fastest algorithm 6 9 test problems secondbest 2 remaining 3 tables also confirm grow usually faster rep tdps results consistent faster rep grow cases indicates initial topdown search good starting theory overfit data much initial rule growing phase rep grow fossils runtimes unstable fastest algorithm datasets far slowest data sets differences accuracies statistically significant 9 significant differences found krkpa7 chess endgame domain tdp fossil performed significantly 1 worse algorithms fossil significantly 5 better tdp votes vi domain outperformed 5 sometimes 1 algorithms 9 used range test used quickly determine significant differences medium values small n 20 sample sizes mittenecker 1977 value 0152 significance level 5 0210 significance level 1 medium values r ranges found tables 7 9 krkpa7 accuracy stnd dev range time fossil 9517 266 863 238361 rep 9784 054 201 424308 grow 9748 041 106 421900 irep 9774 036 132 178550 lymphography 2 classes accuracy stnd dev range time c45 4 classes 7752 446 fossil 8722 439 1723 2079 grow 8210 528 1753 1842 mushroom accuracy stnd dev range time fossil 9996 003 011 353819 grow 9957 066 156 208881 table 9 results chess krkpa7 lymphography mushroom domains lymphography domain general c45 seems little superior algorithms one cannot count results lymphography rule learning algorithms presumably easier 2class task however rule learning algorithms seem competitive allow structured analysis grouped 9 domains 3 subclasses table 7 contains domains overfitting seems harmful ie reps postpruning phase significantly least 5 improves upon concepts learned initial overfitting phase 10 table 8 contains domains pruning make significant difference finally table 9 contains domains pruning cannot recommended exemplified mushroom data overfitting phases learned 100 correct concept descriptions significantly better 5 learned pruning might justifiably argued used separate run pruning data comparison main purpose however compare different pruning approaches evaluate merits pruning results initial overfitting phases rep grow tdp may nevertheless indicator latter come additional cost algorithms mushroom krkpa7 domains known free noise medical domains table 7 noisy therefore assume grouping domain corresponds amount noise contained data 8 conclusion paper discussed different pruning techniques separateandconquer rule learning algorithms conventional prepruning methods efficient always accurate postpruning methods latter however tend expensive learn overspecialized theory first addition inefficiency pointed fundamental incompatibility postpruning methods separateand conquer rule learning systems solution investigated two methods combining integrating pre postpruning algorithms tdp performs initial topdown search hypothesis space find theory overfitting training data still fairly general theory used starting theory subsequent postpruning phase tries generalize theory appropriate level systematic algorithm varying cutoff parameter prepruning algorithm fossil provides efficient way generating theories generaltospecific order good starting theory often found considerably less time would needed generating specific theory fits training examples course pruning phase simpler theory also shorter pruning phase specific theory irep integrates pre postpruning one algorithm instead postpruning entire theories rule pruned right learned experiments show approach effectively combines efficiency prepruning accuracy postpruning domains high redundancy realworld databases typically large noisy thus require learning algorithms efficient noisetolerant irep seems appropriate choice purpose irep tdp deliberately designed closely resemble basic postpruning algorithm rep instance already pointed tdp improved using grow instead rep tdps postpruning phase case irep chosen accuracy pruning set basic pruning stopping criterion order get fair comparison rep concentrate methodological differences postpruning ireps efficient integration pre postpruning important advantage postpruning methods way evaluating theories rules ireps case entirely independent basic learning algorithm pruning stopping criteria improve performance eliminate weaknesses instance pointed cohen 1995 accuracy estimates lowcoverage rules high variance therefore irep likely stop prematurely overgeneralize domains susceptible small disjuncts problem holte acker porter 1989 cohen 1995 also points deficiencies accuracybased pruning criterion shows stopping criterion based description length better pruning criterion significantly improve ireps accuracy without loss efficiency another way improving irep tried furnkranz 1995 irep tries improve upon rep pruning rule level instead theory level investigated way taking tried improve upon irep algorithm prunes literal level resulting algorithm 2 rep seemed little stable low training set sizes significant differences runtime could observed appeared little slower irep although asymptotically algorithms clearly subquadratic currently investigating merits avoiding loss information caused need splitting training set separate growing pruning sets particular techniques based wellknown minimal description length principle could provide valuable alternatives acknowledgments research sponsored austrian fonds zur forderung der wissenschaftlichen forschung grant number p10489mat financial support austrian research institute artificial intelligence provided austrian federal ministry science research would like thank gerhard widmer patiently reading improving numerous versions paper r classification regression trees investigation noisetolerant relational concept learning algorithms rule induction cn2 recent improvements cn2 induction algorithm efficient pruning methods separateandconquer rule learning systems fast effective rule induction decision tree pruning search state space fossil robust relational learner tight integration pruning learning tight integration pruning learning extended abstract incremental reduced error pruning concept learning problem small disjuncts simple classification rules perform well commonly used datasets pattern recognition ruleguided inference empirical comparison pruning methods decision tree induc tion planung und statistische auswertung von experimenten 8th experimental comparison human machine learning formalisms learning decision rules noisy domains boolean feature discovery empirical learning learning efficient classification procedures application chess end games learning logical definitions relations minimum description length principle categorical ories modeling shortest data description overfitting avoidance bias overfitting avoidance bias tr learning decision rules noisy domains simplifying decision trees boolean feature discovery empirical learning experimental comparison human machine learning formalisms rule induction cn2 learning nonrecursive definitions relations linus c45 programs machine learning overfitting avoidance bias simple classification rules perform well commonly used datasets fossil learning logical definitions relations empirical comparison pruning methods decision tree induction cn2 induction algorithm learning noisy examples decision tree pruning search state space tight integration pruning learning extended abstract complexity batch approaches reduced error rule set induction ctr jos ranilla oscar luaces antonio bahamonde heuristic learning decision trees pruning classification rules ai communications v16 n2 p7187 jos ranilla oscar luaces antonio bahamonde heuristic learning decision trees pruning classification rules ai communications v16 n2 p7187 april johannes frnkranz peter flach roc n rule learning towards better understanding covering algorithms machine learning v58 n1 p3977 january 2005 johannes frnkranz round robin classification journal machine learning research 2 p721747 312002 marco muselli diego liberati binary rule generation via hamming clustering ieee transactions knowledge data engineering v14 n6 p12581268 november 2002 johannes frnkranz separateandconquer rule learning artificial intelligence review v13 n1 p354 jan 1999 b kotsiantis zaharakis p e pintelas machine learning review classification combining techniques artificial intelligence review v26 n3 p159190 november 2006