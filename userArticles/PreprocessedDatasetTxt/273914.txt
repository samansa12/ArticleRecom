theory neuromata finite automatonthe socalled neuromaton realized finite discrete recurrent neural network working parallel computation mode considered size neuromata ie number neurons descriptional complexity ie number bits neuromaton representation studied proved constraint time delay neuromaton output play role within polynomial descriptional complexity shown regular language given regular expression length n recognized neuromaton thgrn neurons proved network size worst case optimal hand generally equivalent polynomial length regular expression given neuromaton two specialized constructions neural acceptors optimal descriptional complexity thgr b introduction neural networks 7 models computation motivated ideas brain functioning computational power efficiency traditionally investigated 4 14 15 19 21 within framework computer science one less commonly studied task addressing comparison computational power neural networks traditional finite models computation recognizers regular languages appears finite research supported ga cr grant 201950976 discrete recurrent neural network used language recognition parallel mode time step one bit input string presented network via input neuron output neuron signals possibly constant time delay whether input string read far belongs relevant language way language recognized neural acceptor briefly neuromaton clear neuromata recognize regular languages 12 similar definition neural acceptor appeared 2 8 problem language recognition neural networks explored context finite automata shown 2 every mstate deterministic finite automaton realized discrete neural net om 3 neurons least neurons necessary construction upper lower bound improved 6 8 showing thetam 1 neurons suffice worst case network size cannot decreased assuming either ologmtime simulation delay 6 polynomial weights 8 moreover several experiments train secondorder recurrent neural networks examples behave like deterministic finite automata either practical exploitation rule extraction also done 5 13 20 22 using standard neural learning heuristics backpropagation present paper relate size neural acceptors length regular expressions one hand known possess expressive power finite automata hand represent tool whose descriptional efficiency exceed deterministic finite automata first section 2 3 respectively introduce basic formalism dealing regular languages neuromata prove constant time delay neuromaton output play role within linear neuromata size therefore restrict neuromata respond one computational time step input string read section 4 prove regular language described regular expression length n recognized neuromaton consisting neurons subsequently section 5 show general result cannot improved regular language given regular expression length n requiring neural acceptors n therefore respective neuromaton construction regular expression sizeoptimal used example constructive neural learning learning algorithm regular expressions example strings 3 first employed hand section 6 construction proven efficiently reversible exists neuromaton every equivalent regular expression exponential length next section 7 present two specialized constructions neural acceptors single nbit string recognition require 1 neurons either connections constant weights 1 weights size o2 number bits required entire string acceptor description cases proportional length string means automata constructions optimal descriptional complexity point view exploited part complex neural network design exam ple construction cyclic neural network o2 n neurons edges computes boolean function 9 section 8 introduce concept hopfield languages languages recognized socalled hopfield acceptors hopfield neuromata based symmetric neural networks hopfield networks hopfield networks studied widely outside framework formal languages convergence properties formal language theoretical point view prove interesting fact namely class hopfield languages strictly contained class regular languages hence represent natural proper subclass regular languages furthermore formulate necessary sufficient socalled hopfield condition stating regular language hopfield language section 9 show construction hopfield neuromaton neurons regular language satisfying hopfield condition thus obtain complete characterization class hopfield languages far closure properties hopfield languages concerned show class hopfield languages closed union intersection concatenation complement closed iteration finally section 10 investigate complexity emptiness problem regular languages given neuromata hopfield acceptors prove problems pspacecomplete somewhat surprising identical problems regular expressions deterministic nondeterministic finite automata known nlcomplete 10 11 confirms fact section 6 neuromata stronger regular expressions descriptional complexity point view next consequence obtain equivalence problem neuromata pspacecomplete well previous results jointly point fact neuromata present quite efficient tool recognition regular languages subclasses respectively also description addition abovementioned constructions generalized analog neural networks 18 preliminary version paper concerning general neuromata hopfield languages respectively appeared 16 17 regular languages recall basic notions language theory 1 introduce definition regular expressions determine regular languages concept deterministic finite automaton defined kleenes theorem correspondence finite automata regular languages mentioned well alphabet finite set symbols string alphabet sigma finitelength sequence symbols sigma empty string denoted e string symbols x strings concatenation x string xy string xx ntimes abbreviated x n length string x denoted jxj total number symbols x language alphabet sigma set strings sigma let l 1 l 2 two languages language l 1 delta l 2 called concatenation l 1 l 2 g let l language define n 1 iteration l denoted l language l n0 l n similarly positive iteration definition 3 set regular expressions alphabet defined minimal language alphabet f0 following conditions 1 2 ff fi 2 also ff writing regular expression omit many parentheses assume higher precedence concatenation latter higher precedence example 01 may written abbreviate ntimes ff n jff n j remains n delta jffj definition 4 set set regular languages ff denoted regular expressions ff follows 1 2 ff fi 2 ff also use regular expression ff corresponding positive iteration ff definition 5 deterministic finite automaton 5tuple finite set automaton states sigma input alphabet case gamma q transition function q 0 2 q initial state automaton f q set accepting states definition 6 generalized transition function automaton defined following way 1 2 fg language recognized finite automaton theorem 1 kleene language l regular f recognized finite automaton ie neuromata section formalize concept neural acceptor socalled neu romaton discrete recurrent neural network neural network short exploited language recognition following way network com putation input string presented bit bit network means single predetermined input neuron neurons network work paral lel following possible constant time delay output neuron shows whether input string already read relevant language similar definition appeared 2 8 definition 7 neural acceptor briefly neuromaton 7tuple set n neurons including input neuron inp 2 output neuron 2 v set edges set integers weight function use abbreviation z threshold function abbreviation initial state network graph v e called architecture neural network n size neuromaton number bits needed whole neuromaton representation especially weight threshold functions called descriptional complexity neuromaton formally due notational consistency arbitrary xml 2 f0 1g l 1 input neuroma ton assume oriented paths inp architecture v e length least k 1 state neural network discrete time mapping 1g beginning neural network computation state 0 set 0 time step network computes new state old state tgamma1 follows otherwise neural acceptor n input x 2 f0 1g denote state output neuron 2 v time 1g language recognized neuromaton n time delay k first show respect language recognition capabilities constant time delay neuromaton output play role within linear neuromata size precisely languages recognized time delay k neuromaton size n recognized time delay 1 neuromaton size o2 k n neuromaton size n oriented paths inp architecture v e length least 1 exists neuromaton n init size 2 k n proof idea proof construct n way checks ahead possible computations n next steps cancels wrong computations match actual input read time delay purpose besides input output neurons inp neuroma ton n consists 2 k blocks n x foresee computation n one possible next k bits x input denote neurons blocks way n indexed relevant k bits state neuron v x n equal state neuron v 2 v n computation next bits ie first k gamma 1 bits x 2 f0 1g k performed achieved follows block n yb 2 f0 1g new state old state block n ay 2 f0 1g therefore neurons n ay connected neurons n yb corresponding edges labeled relevant weights respect original weight function w n block n yb constant input b taken account modifying thresholds neurons block especially input bit c 2 f0 1g n indicates computations n ay 6 c valid therefore input neuron inp cancels computations setting neuron states n ay zero thus neurons 2 kgamma1 blocks n ay states previous input bit match also enables block n yb execute correct computation influence one invalid block either n 0y n 1y connected n yb suppressed due zero states neurons blocks n x connected x lead output neuron well labeled weights know half influence moreover among remaining ones corresponding neurons v x v 2 v blocks n x state distance inp n least k last k input bits cannot influence output sufficient multiply threshold 2 kgamma1 order preserve function computed way correct recognition accomplished lookahead formal definition neuromaton n follows ay ay init v ya kgamma1 vy state neuron v neuromaton n input 2 1g kgamma1 time step k gamma 1 term bwhinp vi threshold definition takes account weight constant input block n y1 2 f0 1g corresponding original weight associated edge leading input inp relevant neuron v neuromaton n definition w ensures setting states neurons block n 0y 2 f0 1g kgamma1 zero iff input inp 1 similarly definition weights w together term aw hinp threshold definition cause zero states neurons block n 1y 2 clearly size neuromaton n 2 2 following lemma 1 restrict neuromata respond one computational time step input string read size constant multiplication factor large size equivalent neuromata constant time delay therefore rest paper assume time delay neuromaton recognition 1 neuromaton architecture contain edge input output neuron also denote l 1 n ln neuromaton n next prove neural acceptor viewed finite automaton 12 therefore neuromata recognize exactly regular languages due theorem 1 theorem 2 let language recognized neuromaton n l regular proof let neural acceptor define deterministic finite automaton sigmag q transition function defined 2 q x 2 sigma follows finally sigmag proposition follows theorem 1 2 4 upper bound show regular language given regular expression length n may recognized neuromaton size idea recognition neuromaton compare input string possible strings generated regular expression report via output neuron whether strings match input therefore constructed architecture neural network corresponds structure regular expression neuromaton proceeds oriented network paths correspond strings generated expression time match part input string read far theorem 3 every regular language l 2 rl denoted regular expression exists neuromaton n size jffj l recognized n ie proof let regular language denoted regular expression ff construct neuromaton n jffj first build architecture v e neural network n ff recursively respect structure regular expression ff purpose define sequence graphs corresponding whole expression ff recursively partitioned shorter regular subexpressions v corresponding elementary subexpressions 0 1 ff say vertices type 0 1 sake notational simplicity identify subexpressions ff vertices graphs 1 2 assume already constructed subexpression ff different 0 1 hence besides empty language empty string regular expression fi denote union concatena tion iteration subexpressions fi respect relevant regular operation vertex fi fractioned possibly new vertices corresponding subexpressions fi arise graph v really rigorous first remove vertex fi add new vertices however due notational simplicity insist rigor therefore identify one new vertices old fi write rather inexactly example fi form fi fl moreover ffl fi v g ffl fi e v fiigg ffl fi form fi g ffl fi form fi g ffl fi form fiig construction finished contains subexpressions 0 1 define network architecture following way define weight function w threshold function neuron type 1 neuron type 0 initial state defined 0 set v contains three special neurons inp start well neurons type 0 1 one subexpression 0 1 ff hence jv example neuromaton regular language 10 figure 1 types neurons depicted inside circles representing neurons thresholds depicted weights edges constant inputs gamma1 inp start 101 figure 1 neuromaton 10 prove construction v easy observe graph corresponds structure regular expression ff means every string oriented path start leading 2 v p containing vertices relevant types 0 1 hand path corresponding string l neural acceptor n ff passes possible paths match network input beginning noninput neuron start 2 v active state 1 sends signal connected neurons subsequently becomes passive state 0 due dominant threshold connected neurons type 0 1 compare types network input become active match otherwise remain passive due weight threshold values follows neuron type 1 becomes active iff inp 2 v active least one j 6 inp hj ii 2 e active neuron type 0 becomes active iff inp 2 v passive least one j 6 inp hj ii 2 e active way relevant paths traversed traverse ends neuron realizes logical disjunction active iff prefix input string read far belongs l completes proof 5 lower bound section show lower n number neurons worst case necessary recognition regular languages described regular expressions length n consequence follows construction neuromaton section 4 sizeoptimal standard technique employed purpose given length regular expression define regular language corresponding set exponential number prefixes language prove prefixes must bring neuromaton exponential number different states order provide correct recognition imply desired lower bound definition 9 denote ln pi k pn respectively following regular languages hi clear pn n 1 set prefixes language ln prove several lemmas concerning properties regular languages regular expression defines language ln definition 9 fact length abbreviation repeated concatenation included determining length therefore first show regular expression ff n linear length denoting language ln number prefixes pn shown exponential respect n proof regular expression denotes language ln definition 9 subsequently factor n gamma times subexpression 1e 0 obtain desired regular expression linear length jff defines language ii follows definition 9 jpi k following lemma shows prefixes pn completed strings ln lemma 3 proof ii follow definition 9 iii assume language ln defined via iteration definition 9 henceforth write pi prove two different prefixes pn completed suffix one resulting strings ln one lemma proof assume x exist distinguish two cases without loss generality suppose n ii lemma 3 obtain x 1 due ii lemma 3 2 write x fe 0g g without loss generality ffl denote z pi lemma 3 z 1 0 2 ln z 2 1 2 ln ii lemma 3 implies x 1 ln closed concatenation contrary suppose x 2 exist hence z ii lemma 3 follows z contradiction thus x 2 62 ln 2 ready prove following theorem concerning lower bound theorem 4 neuromaton n recognizes language n neurons proof neuromaton n recognizes language must different states reached taking input prefixes pn different x completed lemma 4 x 1 2 ln x 2 62 ln implies n needsomegagamma n binary neurons 2 6 neuromata stronger although results sections 4 5 seems descriptional complexity point view neuromata regular expressions polynomially equiva lent section show exists neuromaton every equivalent regular expression exponential length means neu romata construction regular expressions described section 4 efficiently reversible theorem 5 every n 1 exists regular language ln recognized neuromaton nn size descriptional complexity 2 regular expression ff n defines define finite language set binary strings length neuromaton recognizes ln binary nbit counter accepts input string iff length neuron corresponding ith bit counter change state iff 1 however corresponding boolean function cannot computed one neuron therefore small subnetwork three neurons introduced purpose v disjunction b least one v state required takes two computational steps means counter two times slower required count till moreover neuron v 0 generates binary sequence 0011 special neuron rst introduced suppress firing output neuron v ngamma2 bits read formal definition counter follows clearly size nn order descriptional complexity 2 let ff n minimal length regular expression defines language prove jff n expression ff contain iterations defines finite language part containing iterations would denote empty language thus could omitted generate strings expression ff n read left right side without returning since iteration allowed string ln length ff n must contain least symbols f0 1g hence jff theorem 5 shows sense big gap descriptive power regular expressions neuromata discuss issue later sections 10 11 7 neural string acceptors previous results showed neuromata descriptive capabilities regular expressions section study powerful confine certain subclass regular expressions deal simplest case considering fixed binary strings f0 1g present two constructions neural acceptors single string recognition nbit strings require 1 neurons either connections constant weights 1 weights o2 number bits required entire string acceptor description cases proportional length string means constructions optimal descriptional complexity point view studying elementary case useful single string recognition often part complicated tasks techniques developed architectural neural network design example sometimes improve construction neuromata section 4 regular expression consists long binary sub strings example application constructive neural learning strings viewed training patterns resulting network composed neural acceptors strings work parallel theorem 6 string 2 f0 1g n exists neural acceptor size 1 neurons connections constant weights thus descriptional complexity neuromaton thetan proof sake simplicity suppose first positive integer p idea neural acceptor construction split string 2 f0 1g n p pieces length p encode p substrings using p 2 p binary weights edges leading p comparator neurons c input string gradually stored per p bits p buffer neurons buffer full relevant comparator neuron c compares assigned part string corresponding part input string x stored buffer sends result comparison next comparator neuron synchronization comparison performed 2p clock neurons neurons tick time step network computation neurons tick period p steps last comparator neuron c p represents output neuron reports end whether input string x matches formal definition neuromaton recognition string follows see also figure 2 define denote fi fi put ae remaining weights thresholds set 1 finally initial state x z z z z z z z z z z z z z z z z z z z z z z z z z z l l l l l l l l l l l l l l l l l l l l z z z z z z z z z z z z z z z z z z z z z z ae ae ae ae ae ae ae ae ae ae ae ae ae ae ae ae ae ae ae ae ae ae ae ae omega omega omega omega omega omega omega omega omega omega omega omega omega omega omega omega omega omega inp sp start figure 2 architecture neural string acceptor edges note weights w constant required due neuron c keep state 1 becomes active order transfer possible positive result comparison next comparator neuron therefore relevant feedback must exceed sum inputs achieve threshold value avoided inserting auxiliary neurons remember result preceding comparisons neighbor comparator neurons neurons constant feedback one input previous comparator neuron exceeded construction neural acceptor also easily adapted p technique proof theorem 3 employed recognition last r bits string resulting architecture size connected neural string acceptor identifying neuron start abovementioned neuron x z z z z z z z z z z z z z z z z z z z z z z z z z z l l l l l l l l l l l l l l l l l l l l z z z z z z z z z z z z z z z z z z z z z z ae ae ae ae ae ae ae ae ae ae ae ae ae ae ae ae ae ae ae ae ae ae ae ae omega omega omega omega omega omega omega omega omega omega omega omega omega omega omega omega omega omega l l l l z z start q3 q2 q1 inp reset sp figure 3 architecture neural string acceptor 1 theorem 7 string 2 f0 1g n exists neural acceptor size neurons 1 weights size o2 thus descriptional complexity neuromaton thetan proof design desired neural acceptor similar construction proof theorem 6 except number comparator neurons reduced two neurons c c case substrings encoded op 2 weights size o2 p corresponding connection leading clock neurons comparison neurons contents input buffer viewed binary number first converted integer integer compared relevant encoded part string comparator neuron c see whether smaller equal time compared comparator neuron c see whether greater equal neuron realizes logical conjunction comparator outputs added indicate whether part input string matches corresponding part string however leads one computational step neural acceptor correct synchronization achieved exploiting abovementioned additional architecture recognition last r bits q1 string details synchronization omitted well complete formal definition neural string acceptor give definition weights relevant comparisons 2 pgammaj igamma2pj weights w defined differences clock neurons active part input buffer architecture neural string acceptor depicted figure 3 instead abovementioned conjunction neuron reset added realize negation comparator conjunction possibly terminate clock 2 piotr indyk 9 pointed latter string acceptor construction theorem 7 also exploited building cyclic neural network o2 n neurons edges computes boolean function case binary vector function values encoded string acceptor position relevant bit part vector includes desired output given first nbits input possible corresponding 2 n bit substrings generated presented acceptor achieve relevant part function value vector relevant bit extracted position determined last nbits input 8 hopfield languages section 7 restricted special subclass regular expres sions section concentrate special type neural acceptors socalled hopfield neuromata based symmetric neural networks hopfield networks networks weights symmetric therefore architecture neuromata seen undirected graph hopfield networks traditionally studied 4 21 used due favorite convergence properties networks also particular interest natural physical realizations exist eg ising spin glasses optical computers using concept hopfield neuromata define class hopfield languages recognized particular acceptors neuromaton based symmetric neural network hopfield network hi called hopfield acceptor hopfield neuromaton language recognized hopfield neuromaton n called hopfield language first show class hopfield languages strictly contained within class regular languages purpose formulate necessary condition socalled hopfield condition regular language hopfield language intuitively hopfield languages cannot include words potentially infinite substrings allow hopfield neuromaton converge forget relevant information previous part input string recognized idea proof find necessary condition prevent hopfield neuromaton converging definition 11 regular language l said satisfy hopfield condition f every theorem 8 every hopfield language satisfies hopfield condition proof let hopfield language recognized hopfield neuro ng define integer vectors size n theta 1 let gammafinpg integer matrix size n theta n note matrix w symmetric since n hopfield acceptor let us present input string v 1 x acceptor n sufficiently large 0 networks computation must start cycling input network 2 n1 possible states different states cycle x corresponding input bits state followed state index shifting permutation inverse permutation let r composed permutations r 1 let c binary vectors size n theta 1 follows definition 8 state cycle define integer symbol denotes transposition vector obviously p using fact matrix w symmetric obtain moreover x 2 write know therefore c implies p e p cannot c 2 n time case inequality c strict complementary case c 2 simultaneously impossible well since number 1s c 2 would greater number 1s c therefore conclude c 2 consequently implies cycle length p 2 hence every v 2 2 f0 1g either completes proof l satisfies hopfield condition 2 example follows theorem 8 regular languages 000 hopfield languages satisfy hopfield condition 9 hopfield condition sufficient section prove necessary hopfield condition definition 11 stating regular language hopfield language sufficient well construction hopfield neuromaton shown regular language satisfying hopfield condition theorem 9 every regular language satisfying hopfield condition exists hopfield neuromaton n size jffj l recognized n hence l hopfield language proof architecture hopfield neuromaton regular language ff satisfying hopfield condition given general construction proof theorem 3 result obtain oriented network n 0 corresponds structure regular expression ff neuron n n 0 besides special neurons inp start associated one symbol 2 f0 1g ff ie type task n check whether agrees input bit transform n 0 equivalent hopfield network n supposing ff contains iterations binary substrings two bits standard technique 15 21 transformation acyclic neural networks hopfield networks employed idea consists adjusting weights prevent propagating signal backwards preserving original function neurons transformation starts neuron carried opposite direction oriented edges ends neuron start neuron whose outgoing weights already adjusted threshold incoming weights multiplied sufficiently large integer exceeds sum absolute values outgoing weights sufficient suppress influence outgoing edges neuron transformation accomplished oriented paths leading start labeled decreasing sequences weights problem lies realizing general iterations using symmetric weights consider subnetwork n 0 corresponding iteration substring ff let subnetwork arisen subexpression fi ff proof theorem 3 abovementioned transformation performed path leading incoming edges outgoing ones labeled decreasing sequence weights order avoid backward signal spreading signal propagated output subnetwork back subnetwork input iteration requires one hand integer weight associated connection small enough order suppress backward signal propagation hand weight sufficiently large enough influence subnetwork input neuron clearly two requirements contradictory consider simple cycle c subnetwork consisting oriented path passing one backward edge leading end path ie output beginning ie input let types neurons cycle c establish iteration 2 f0 1g jaj 2 moreover suppose x 2 f0 1g 2 x k k 2 hopfield condition set v 2 postfix l associated path leading c similarly set v 1 prefix l associated path leading start c every contradicts hopfield condition prefix v 1 exists otherwise cycle c could realized iteration two bits therefore 6 x k implies strings contain substring form b b b hence string form either notational simplicity confine former case latter remains similar furthermore consider 1 ba 2 minimal ja 2 j shift decreasing sequence weights c start end neuron n relevant weights n 0 modified abovementioned procedure ensure consistent linkage c within n 0 example means edges leading output neuron c input neurons evaluated sufficiently large weights realize corresponding iterations problem lies signal propagation neuron n b neuron n assume support small weight connection n b n new neuron id copies state input neuron inp connected neuron n via sufficiently large weight strengthens small weight connection n b obviously neuron n b b 1 new neuron id active time enable required signal propagation n b n together hand neuron n b active neuron id passive due fact copying input prevents neuron n becoming active time however symbols b ff neurons n b 0 outside cycle c within subnetwork edges n lead situation corresponds concatenated union operation within fi case active neurons n b 0 id would cause neuron n fire avoid add another new neuron n 0 behaves identically neuron n symbol 1g thus neurons connected n linked n 0 edges originally outgoing n n b 0 corresponding b 0 reconnected lead n 0 similar approach used opposite case abovedescribed procedure applied simple cycle c subnetwork corresponding iteration fi cycles necessarily disjoint decomposition 1 ba 2 minimal ja 2 j ensures consistent synthesis similarly whole transformation process performed iteration ff case iteration part another iteration magnitude weights inner iteration need accommodated embody outer iteration also possible neuron id support iterations point finally number simple cycles ff jffj hence size resulting hopfield neuromaton remains order jffj figure 4 preceding construction illustrated example hopfield neuromaton regular language 10 simple cycle consisting neurons clarified details notice decreasing sequence weights 751 cycle starting ending neuron n well neuron id enables signal propagation n b n neuron n 0 identical n also created neuron originally connected n see figure 1 2 start inp id id figure 4 hopfield neuromaton 10 corollary 1 let l regular language l hopfield language f l satisfies hopfield condition finally briefly investigate closure properties class hopfield languages theorem 10 class hopfield languages closed union concatenation intersection complement closed iteration proof closeness hopfield languages union concatenation follows corollary 1 obtain hopfield neuromaton complement negate function output neuron multiplying associated weights threshold 2 adding 1 threshold hence hopfield languages closed intersection well finally due theorem 9 1010 hopfield language whereas 1010 hopfield language satisfy hopfield condition theorem 8 2 emptiness problem order illustrate descriptional power neuromata investigate complexity emptiness problem regular languages given neuromata hopfield acceptors prove problems pspacecomplete definition 12 given hopfield neuromaton n hopfield neuromaton emptiness problem denoted nep hnep issue deciding whether language recognized hopfield neuromaton n nonempty theorem 11 nep hnep pspacecomplete proof show nep hnep 2 pspace input string hopfield neuromaton n guessed bit bit acceptance checked simulating network computation polynomial space witness nonemptiness next show nep pspacehard let arbitrary language pspace x 2 f0 1g polynomial time construct corresponding neuromaton n x 2 iff n 2 nep let polynomial space bounded turing machine recognizes first cyclic neural network n 0 simulates constructed polynomial time using standard technique 4 idea construction tape cell subnetwork simulates tape head position computation ie local transition rule neighbor subnetworks connected enable head moves input x encoded initial state n 0 end neural network computation one neuron n 0 called result signals whether x 2 neural network n 0 embodied neuromaton n follows input neuron inp n connected neuron output neuron n identified neuron result n 0 accepts x iff neuron active end simulation iff ln contains words length equal length computation x iff n 2 nep thus x 2 iff n 2 nep completes proof nep pspacecomplete hopfield neuromata similar simulation achieved using symmetric neural network n 0 convergent computation arbitrary asymmetric neural network simulated symmetric network polynomial size 4 assume without loss generality stops every input hence hnep pspacecomplete well 2 theorem 11 somewhat surprising identical problems regular expressions deterministic nondeterministic finite automata known nlcomplete 10 11 sense shows big gap descriptive power regular expressions neuromata confirms result section 6 discuss issue later section 11 difference descriptive power regular expressions neu romata also illustrated complement operation emptiness problem complement regular expression becomes pspacecomplete 1 emptiness problem complexity complement neuromaton change output neuron easily negated next consequence show neuromaton equivalence problem pspacecomplete well given two hopfield neuromata n 1 n 2 hopfield neuromaton equivalence problem denoted neqp hneqp issue deciding whether languages recognized hopfield neuromata ie whether corollary 2 neqp hneqp pspacecomplete proof prove neqp 2 sufficient show complement pspace purpose input string neuro mata guessed acceptance one two neuromata rejection one checked simulating network computations polynomial space witness nonequivalence ln 1 show neqp pspacehard complement nep denoted conep pspacecomplete well polynomial time reduced neqp let neuromaton n instance conep polynomial time construct corresponding instance n 1 n 2 neqp ln empty iff neuromaton n 1 identified n let n 2 arbitrary small neuromaton recognizes empty language easy see correct reduction hence neqp pspacecomplete pspacecompleteness hneqp achieved similarly 2 conclusions paper explored alternative formalism regular language representation based neural networks comparing socalled neuromata classical regular expressions obtained result within polynomial descriptive complexity nondeterminism captured regular expressions simulated parallel mode neuromata deterministic nature opinion descriptive power neuromata consists efficient encoding transition function transition function deterministic nondeterministic finite automata usually specified list values old state input symbolnew state function neuromata given vector formulae one neuron evaluates encoding interpreted like general program point view easy observe table transition rules deterministic finite automata special case program moreover neuromata encode nondeterministic transition function efficiently using parallelism way behavior neuromata exponential number possible states described polynomial size however process reversible neural transition program cannot generally rewritten polynomial size table polynomial length regular expression moreover number neuromaton states although exponential limited matching lower bound neuromaton size achieved using standard technique exists regular language requires exponential number neuromaton states recognized also complex neural transition rule specification makes emptiness problem neuromata harder classical formalism finite automata regular expressions hand complement operation nondeterministic case formalism cause exponential growth descriptional complexity complement neuromata easily achieved also investigated hopfield neuromata studied widely due convergence properties shown hopfield neuromata determine proper subclass regular languages socalled hopfield languages via socalled hopfield condition completely characterized class hopfield languages conclude neuromata present quite efficient tool recognition regular languages subclasses respectively also description acknowledgement grateful markus holzer piotr indyk petr savicky stimulating discussions related topics paper thank tereza bedanova realized pictures latex environment r design analysis computer algorithms efficient simulation finite automata neural nets learning regular expressions pattern matching complexity issues discrete hopfield net works learning extracting finite state automata secondorder recurrent neural networks bounds complexity recurrent neural network implementations finite state machines optimal simulation automata neural nets personal communication note space complexity decision problems finite automata representation events nerve nets finite au tomata computational complexity neural networks survey circuit complexity neural networks discrete neural computation theoretical foundation complexity issues discrete neurocomputing learning finite state machines selfclustering recurrent networks tr efficient simulation finite automata neural nets neurocomputing note space complexity decision problems finite automata learning extracting finite state automata secondorder recurrent neural networks circuit complexity neural networks learning finite machines selfclustering recurrent networks discrete neural computation learning extracting initial mealy automata modular neural network model bounds complexity recurrent neural network implementations finite state machines design analysis computer algorithms computational complexity neural networks hopfield languages learning regular expressions pattern matching