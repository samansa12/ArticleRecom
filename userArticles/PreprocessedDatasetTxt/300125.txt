compression algorithm probability transition matrices paper describes compression algorithm probability transition matrices compressed matrix probability transition matrix general compression error free error appears small even high levels compression b introduction many discrete systems described markov chain model state markov model discrete state dynamical system n states markov chain model defined n theta n matrix q called 1step probability transition matrix qi j probability going state state j one step nstep behavior described nth power q q n many systems number states enormous computational advantage reducing n previous methods reducing number states referred compression aggregation lumping methods focused techniques provide good estimations steadystate behavior markov model focus paper however transient behavior goal produce algorithm compressing q matrices way yields good estimates transient behavior markov model algorithm described paper compresses q matrix smaller q matrix less states general compression without error goal provide algorithm compresses original q matrix without significant error although computing compressed matrix might take time savings resulting using compressed matrix subsequent computations offset compression time organization paper follows section 2 introduces compression algorithm compresses pairs states taking weighted average row entries two states followed summing two columns associated two states section 2 also introduces important concepts row column equivalence important identifying pairs states compressed error section 3 provides mathematical justification taking weighted average row entries shows weights simply column sums probability mass section 4 proves pairs states row column equivalent lead perfect compression section 5 introduces analysis error uses define metric row column similarity used find pairs states yield almost perfect compression later sections illustrate utility compression algorithm experiments 2 compression algorithm high level entries q probability system tran work supported arpa order d10603 ai center code 5514 naval research laboratory 4555 overlook avenue washington dc 20375 phone 2027679006 fax 2027673172 spearsaicnrlnavymil w spears sition state j one step given currently state 1 suppose states j chosen compression new compressed state referred state fi jg compressing states j together means combined state represents either state state j since disjunctive situation probability transition state k compressed state simply sum stated another way part compression algorithm sum columns probability numbers however general transitions compressed state complicated compute clearly probability transitioning compressed state state p fijgk must lie somewhere p ik p jk depending much time spent states j thus weighted average row entries appears called weights reflect amount time spent states j precisely weighted average investigated section 3 algorithm compressing two states j together follows 2 compute weighted average ith jth rows place results rows j b sum ith jth columns place results column remove row j column j compression algorithm two steps takes input matrix q u uncompressed averages row entries producing intermediate rowaveraged matrix q r step b sums column entries produce final compressed matrix q c step sole source error since general difficult estimate amount time spent states j compression algorithm outlined important define meant perfect compression mentioned analysis nstep transition probabilities ie transient behavior markov chain realized computing q n large q matrices computationally expensive would less expensive compress q raise nth power compression algorithm worked well nth power compressed matrix q c nearly identical compressing nth power uncompressed matrix q u words perfect compression occurred q n c turns two situations perfect compression obtained first situation referred row equivalence two states j identical rows ie 8k p case weighted averaging produce error since weights irrelevant second situation referred column equivalence state column entries real multiple q column entries state j ie 8k p intuition situation occurs ratio time spent state state j precisely q details found section 4 however arbitrary matrices compressing arbitrarily chosen pair states necessarily lead good results thus goal identify pairs states j upon compression algorithm work well turns pairs states row column similar good candidates compression justification measures provided section 5 1 notation p n denotes entries nstep probability transition matrix q n 2 algorithm written way makes amenable mathematical analysis compressing probability transition matrices 3 high level course simple compression algorithm must repeated many pairs states one wants dramatically reduce size q matrix high level compression algorithm simply compress repeat long possible find pair states j similar 3 compression algorithm detail previous section compression algorithm described two steps step error occur care must taken mathematically justify weighted averaging rows done attempting force q 2 similar possible c later sections generalize higher powers mathematically difficult fortunately suffices force q 2 u similar possible q much simpler focuses rowaveraged matrix q r explicitly intuition behind compression done correctly passage new compressed state affect 2step transition probabilities little possible 3 shown 4 theta 4 q matrix generalized arbitrary n theta n matrix result weighted row averaging procedure outlined earlier particular presentation motivated concern comprehension hence completely formal completely formal presentation appendix 31 weighted averaging 4 theta 4 matrix consider general uncompressed 4 theta 4 matrix q u markov chain model 4 states well general intermediate matrix notation r ij j q r j used prevent confusion p ij q u without loss generality goal compress 3rd 4th states rows columns matrix since 3rd 4th states compressed rows 1 2 q r must q u ie averaging rows 3 4 affect rows 1 2 denoting f3 4g compressed state intermediate matrix r f34gk represent weighted average rows 3 4 q u recall step compressstates34 place average rows 3 4 rows 3 4 q r trick determine r f34g1 r f34g2 r f34g3 r f34g4 order produce reasonable compression done considering 3 formally shown q 2 c row column equivalent situations see section 4 4 w spears 2 11 2 12 2 13 2 2 21 2 22 2 23 2 2 31 2 32 2 33 2 2 41 2 42 2 43 2 notation 2 ij used prevent confusion p 2 ij q 2 u since goal necessary p 2 ij similar possible 2 ij p 2 ij values computed using p ij values 2 ij values require unknowns r f34g1 r f34g2 r f34g3 r f34g4 example p 2 11 computed multiplying q u however 2 11 computed multiplying q u 2 ideal situation would like equal implies write another formula r f34g1 considering p 2 21 2 2 would like equal implies similarly consideration p 2 31 2 31 yields consideration p 2 41 2 41 yields happened four elements first column q lead four expressions r f34g1 general four expressions r f34g1 hold simultaneously although investigate conditions hold later best estimate take weighted average four expressions r f34g1 related concept averaging probabilities see appendix details yields compressing probability transition matrices 5 note final expression r f34g1 weighted average row entries p 31 p 41 weights column sums columns 3 4 general elements q kth column constrain r f34gk note expression r f34gk weighted average row entries p 3k p 4k weights column sums columns 3 4 32 weighted averaging n theta n matrix previous results 4 theta 4 matrix extended n theta n matrix without loss generality compress states n elements column k yield n expressions r fn gamma1ngk best estimate see appendix details note weights column sums columns generalizing compressing two arbitrary states j yields l p li p ik l p lj p jk l l p lj sums probability mass columns j q u equation 31 indicates compute r fijgk entries q r note computed using weighted average row entries rows j weights simply column sums justifies row averaging component compression algorithm described previous section intuitively stated column mass columns j provide good estimates relative amount time spent states j estimates used weights average transitions state k j k producing probability transition combined state fi jg k 33 mathematical restatement compression algorithm weighted averaging rows j explained necessary sum columns j order complete compression algorithm whole algorithm expressed simply follows assume two states chosen compression let denote set n states let nonempty sets one contains two chosen states composed exactly one state let denote column mass state compressed matrix q c 6 w spears j2sy corresponds taking weighted average two rows corresponding two chosen states summing two corresponding columns entries q matrix remain unchanged consider example states 2 3 compressed case described applying following column equivalent matrix q u produces perfect results c summary section justified use column mass weights row averaging portion compression algorithm whole compression algorithm stated succinctly mathematical function compress arbitrary pair states however stated earlier compression arbitrary pairs states need lead good compression goal identify states investigated next section relies upon concepts row column equivalence 4 special cases compression perfect compression working well compressed version q n u nearly identical q n c suggested section 2 two situations perfect compression occur first situation two states row equivalent intuition row average two identical rows involve error thus compression perfect second situation two states column equivalent intuition situation column c equal qc j ratio time spent state state j exactly q circumstances weighted row average also produce error section prove q n c two states compressed either row equivalent column equivalent hold n q u matrix size n theta n method proof treat compression algorithm linear transformation f show fq n fq u compressing probability transition matrices 7 41 row equivalence compression algorithm subsection prove two states row equivalent compression states described linear transformation matrix multiplication compression algorithm compresses n theta n matrix q u however sake mathematical convenience matrix transformations n theta n matrices without loss generality assumed states compressed comes time expressing final compression nth row column simply ignored producing matrix ffl notation used denote entries important derivation assume states row equivalent thus 8k pn using equation 31 compute row averages yields compressed matrix form theorem 41 states n row equivalent q expressed follows precisely q c thus compression two row equivalent states expressed simply tq u first performs row averaging trivial second performs column summing reader also note elements appear important derivation q true however purpose elements ensure since fact also used help prove q n 8 w spears 42 column equivalence compression algorithm subsection prove two states column equivalent compression states described linear transformation assume without loss generality states column equivalent thus 8k using equation 31 compute row averages yields compressed matrix form qpn gamma11 pn1 theorem 42 states n column equivalent q 4 0 0q expressed follows qpn gamma11 pn1 precisely q c thus compression two column equivalent states expressed simply xq u x performs row averaging performs column summing reader note elements x important derivation q could used instead true however purpose elements ensure since fact used help prove q n c end section compressing probability transition matrices 9 43 necessary lemmas proving q n c row column equivalent states necessary prove simple lemmas idea show q u row column equivalent q n u allow previous linear transformations applied q n u well q u let square matrices b defined matrices row column vectors respectively 6 4 1 matrix product ab represented using dot product notation lemma 43 row equivalence invariant postmultiplication proof suppose states j row equivalent states j ab must row equivalent lemma 44 column equivalence invariant premultiplication proof suppose states j b column equivalent b states j ab must column equivalent lemma 45 row column equivalence invariant raising power proof thus states j row equivalent q row equivalent q n lemma 43 similarly q states j column equivalent q column equivalent q n lemma 44 lemma 45 indicates previous linear transformations applied u produce q n two states q u row column equivalent 44 theorems perfect compression given previous theorems concerning linear transformations lemma 45 possible state prove theorems perfect compression q matrix considered q u theorems theorem 46 q row equivalent q r implies q n r c proof q row equivalent q n lemma 45 q r r q n c theorem 47 q column equivalent q r implies q n r c proof q column equivalent q n lemma 45 q r q n r q n c two theorems illustrate validity trying force q 2 u similar possible q u q r section 3 theorem 48 q row equivalent q n c proof q row equivalent q n lemma 45 q c theorem 49 q column equivalent q n c proof q column equivalent q n lemma 45 q c theorems hold n row column equivalent n theta n q matrices highlight importance row column equivalence two states row column equivalent compression two states perfect ie 5 error analysis similarity metric previous sections explained merge pairs states explained row column equivalent pairs yield perfect compression course highly unlikely pairs states found perfectly row equivalent column equivalent goal find similarity metric measures row column similarity ie close pairs states row column equivalent metric formed correctly pairs states similar yield less error compressed section derive expression error use similarity metric pairs states use q u estimate error mentioned desirable entries two matrices similar possible consider compressing two states j entries q 2 entries q 2 error associated x yth element q using equation 31 r fijgk substituting denote ff ij measure row similarity rows j column explained simplifies denote fi ij compressing probability transition matrices 11 fi ij x considered measure column similarity columns j row x shown explicitly since magnitude error important sign absolute value error considered jerror ij x recall error ij x error associated x yth element states j compressed total error whole matrix x jerror ij x x simplified x jff ij yj understand equation consider situation states j row equivalent 8y indicates 8y ff ij thus error associated compressing row equivalent states j shown earlier sections consider situation states j column equivalent 8x qp xj trivial show 8x fi ij consequence error associated compressing column equivalent states j shown earlier sections given natural similarity metric expression error x jff ij yj similarity close zero error close zero pairs states judged amount error ensue compressed 4 compression algorithm written follows compress repeat long possible find pair states j similarity ij ffl role ffl threshold pairs states similar threshold compressed raising ffl one compress states commensurate increase error paper thus far fully outlined compression algorithm pairs states identified situations compression perfect namely pairs states row column equivalent performing error analysis natural measure similarity derived pairs states row column similar yield small amounts error compression algorithm following section outlines experiments showing degree compression achieved practice 4 useful think dissimilarity metric 6 experiments order evaluate practicality compression algorithm tested markov chains derived field genetic algorithms gas ga population individuals evolves generation generation via darwinian selection perturbation operators recombination mutation individual population considered point search space see 8 overview gas different population ga state markov chain p ij probability ga evolve one population another j one generation time step number states grows extremely fast size population increases size individuals increase details mapping gas markov chains found 6 use examining transient behavior found 2 5 61 accuracy experiments first set experiments examine accuracy compressed markov chains using q n c compute probability distribution p n states time n answer questions q n must combined set initial conditions concerning ga generation 0 thus priori probability ga state time 0 p 0 6 given probability ga particular state j time n also possible compute probabilities set states define predicate red j set j states make p red j true probability ga one states j time n paper j represents set states contain least one copy optimum ie set populations least one individual optimum function value markov model used compute p n j probability least one copy optimum population time n compression algorithm thus evaluated using q n truth q n c estimate compute p n j different values n closer estimate ground truth better compression algorithm working since goal compute probabilities involving states containing optimum j set j states compressed nonj states consequently compression algorithm run separately sets states algorithm repeat new compressed states created state j set current compressed model find similar state j j set ii similarity ij ffl compressstatesij b state nonj set current compressed model find similar state j nonj set ii similarity ij ffl compressstatesij 5 ga q zero entries thus ergodic 6 states j compressed p 0 fijg compressing probability transition matrices 13 n search space 1 n search space 2 n search space 3 n search space 4 fig 61 p n j ffl 00 015 455 bold curves represent exact values nonbold curves represent values computed compressed matrix theory compression algorithm could result two state model involving j nonj practice would require large values ffl unacceptable error p n j computations four different search spaces chosen ga particular set four search spaces chosen experience shown hard get single compression algorithm perform well also order see well compression algorithm scales larger markov chains four population sizes chosen ga 10 12 14 16 four choices population size produced markov chains 286 455 680 969 states respectively thus compression algorithm tested sixteen different markov chains 7 naturally setting ffl crucial success experiments experiments indicated value 015 yielded good compression minimal error sixteen markov chains results shown figure 61 results experiments omitted sake brevity almost identical values p n j computed n ranging 2 100 compressed uncompressed markov chains graphed curves bold curves represent exact p n j values nonbold curves represent values computed compressed matrix figures clearly indicate compressed matrix yielding negligible error see amount compression affected size markov chain consider table 61 gives percentage states removed sixteen chains interesting particular search spaces amount compression increasing n increases still yielding negligible error 80 states removed yielding q c matrices roughly 3 size terms memory requirements original q u matrix also interesting note different search spaces consistently compressed different 7 see 2 definition search spaces 14 w spears table percentage states removed search space 1 85 88 90 92 search space 2 71 76 81 84 search space 3 65 73 79 82 search space 4 64 73 79 82 degrees example third fourth search spaces consistently compressed less first search space investigation nature search spaces may help characterize arbitrary markov chains hardeasy compress algorithm 62 timing experiments necessary examine computational cost compression algorithm prior work 2 9 focused heavily insights gained actually examining q n u involved computations order n 3 multiply q u repeatedly thus primary motivation producing compression algorithm gain insights efficiently dramatically reducing n since second search space quite representative terms performance compression algorithm draw timing results experiments particular search space table 62 gives amount cpu time minutes needed compute q n u n ranges 2 100 table 63 gives amount time needed compress q u q c well time needed compute c n ranges 2 100 8 clearly compression algorithm achieves enormous savings time actually necessary compute powers q u table time minutes compute q n computation time 27 125 447 1289 table time minutes compress qu compute q n c compression time 02 09 30 95 computation time 24 76 179 381 another common use q n u compute probability distribution p n states time n previous subsection prior distribution p 0 known advance however efficiently done multiplying p 0 repeatedly ie repeated n times produce p n computation instead n 3 table 64 table 65 give amount time needed compute p n q u q c respectively despite obvious benefits computing p n q c compression algorithm advantageous case since time needed timing results sun sparc 20 code written c available author compressing probability transition matrices 15 table time minutes compute p n computation time 01 03 07 14 table time minutes compress qu compute p n compression time 02 09 30 95 computation time 002 002 003 compress q u exceeds time produce p n q u however still occasions compressing q u using q c compute p n fact efficient first necessary compute p n large number different prior distributions recall q c depend prior information hence need recomputed second occasion necessary compute p n large n eg 10 indicates times order 10 8 sometimes required situations cost compression algorithm amortized finally compression also advantageous prior distribution known advance 9 summary compression algorithm advantageous necessary actually examine powers q u directly computing probability distributions states compression algorithm advantageous prior distribution initially unknown large number prior distributions considered transient behavior long period time required 7 related work goal paper provide technique compressing aggregating discretetime markov chains dtmcs way yields good estimates transient behavior markov model section summarizes work closely related considerable body literature concerning approximation transient behavior markov chains techniques include computation matrix expo nentials use ordinary differential equations krylov subspace methods 10 however techniques continuoustime markov chains ctmcs use infinitesimal generator matrix instead probability transition matrix possible discretize ctmc obtain dtmc stationary probability vector ctmc identical dtmc however 10 notes transient solutions dtmcs corresponding ctmcs indicating techniques problematic computing transient behavior dtmcs also considerable work aggregation dtmcs almost theoretical analyses aggregation eg block aggregation 5 utilize functional form b matrices determine partitioning aggregation states 3 4 functional form must satisfy two axioms linearity state 9 also important emphasize likely compression algorithm extensively optimized producing much better timing results w spears partitioning linearity implies b depend explicitly entries q u state partitioning implies aggregated transition probabilities depend upon probabilities associated aggregated states eg aggregation states j depend p ii p ij p ji p jj neither axiom true compression column equivalent states paper reflected fact general instead paper row column equivalence yielding desirable properties respect powers q u current results indicate relevance axioms reexamined aggregation technique closely related work paper described 10 11 12 aggregation technique partitions set states denoting steady state probability state j2sy compression performed manner steady state behavior compressed system original system aggregated matrix computed via method stochastic complementation via iterative aggre gation disaggregation methods former work arbitrary matrices generally computationally expensive latter efficient nearly completely see 1 however emphasis always steadystate behavior transient behavior difference emphasis seen noting difference choice weights focus paper column mass instead steady state values sense compression algorithm presented paper generalization steady state aggregation steady state matrix column equivalent every pair states column masses renormalized steady state probabilities thus compression algorithm generalization aggregation formula transient behavior 10 leads intriguing hypothesis new compression algorithm accurate describing transient behavior less accurate describing steady state behavior preliminary results appear confirm hypothesis 8 summary discussion paper introduced novel compression algorithm probability transition matrices output algorithm smaller probability transition matrix less states algorithm designed aggregate arbitrary necessarily ncd probability transition matrices dtmcs order obtain accurate estimations transient behavior thus appears fill gap existing transient techniques focus ctmcs existing aggregation techniques dtmcs focus steadystate behavior number potential avenues expansion research first possibility compress two states multiplestate compression may yield better results allowing accurate estimation error another avenue derive estimates error propagates higher powers q c current similarity metric necessarily good indicator error note lemma 45 implies b compressing probability transition matrices 17 higher powers q c although empirically results quite good however avenues greatly increase computational complexity algorithm comparison related work indicates new compression algorithm considered generalization traditional aggregation formulas indicates yet third avenue research fact column mass turns yield better weights weighted average transient behavior may possible smoothly interpolate column mass steady state probabilities transient behavior approaches steady state course presupposes existence steady state distribution efficient algorithms exist compute distributions current algorithm also quite deliberately ignores roles priors p 0 order general algorithm possible however priors known may possible use information improve weighted averaging procedure see appendix thus reducing error situations finally amount compression achieved negligible error useful indicator whether system modeled correct level gran ularity probability transition matrix hard compress system probably modeled reasonable level granularity however ease compression indicates system modeled much detail cases monitoring states chosen compression similarity metric yield important information characteristics system approach could used characterize systems defined probability transition matrix still well understood higher level acknowledgements thank diana gordon pointing method evaluating compression algorithm show q n c diana also pointed sections needed mathematical refinement also thank anonymous reviewers constructive comments r using markov chains analyze gafos aggregation markov processes axiomatization linear aggregation inputoutput models modelling genetic algorithms markov chains survey methods computing large sparse matrix exponentials arising markov chains overview evolutionary computation analyzing gas using markov models semantically ordered lumped states introduction numerical solution markov chains numerical experiments iteration aggregation markov chains modeling simple genetic algorithms tr