software trace cache paper explores use compiler optimizations optimize layout instructions memory target enable code make better use underlying hardware resources regardless specific details processorarchitecture order increase fetch performance software trace cache stc code layout algorithm broader target previous layout optimizations target improvement instruction cache hit rate also increase effective fetch width fetch engine stc algorithm organizes basic blocks chains trying make sequentially executed basic blocks reside consecutive memory positions maps basic block chains memory minimize conflict misses important sections program evaluate analyze detail impact stc code layout optimizations general three main aspects fetch performance instruction cache hit rate effective fetch width branch prediction accuracy results show layout optimized codes special characteristics make amenable highperformance instruction high rate nottaken branches execute long chains sequential instructions also make effective use instruction cache lines mapping useful instructions execute close time increasing spatial temporal locality b introduction instruction fetch bandwidth may become major limiting factor future aggressive wideissue superscalars conse quently crucial develop software hardware techniques interact deliver multiple basic blocks processor every cycle unfortunately many important codes hard instance database codes several integer spec research supported cicyt grant tic0511 authors generalitat de catalunya grants aci 97 26 josep l larribapey josep torrellas 1998fi00306 aptind alex ramirez commission cultural educational scientific exchange united states america spain josep l larribapey josep torrellas mateo valero nsf grant mip9619351 josep torrellas cepba alex ramirez wants thank fellow pbcs time efforts authors want thank xavi serrano help setting analyzing postgresql university illinois urbana champaign usa applications frequent control flow transfers high instructioncache miss rates characteristics make supplying high number useful instructions difficult task even presence aiding devices like hardware trace cache htc 4 12 software side possible reorder code memory easier supply useful instructions execution unit code reordering target elimination cache conflicts 5 6 8 7 10 13 addition also map sequentiallyexecuted basic blocks consecutive memory positions 7 10 13 aspects may increase number useful instructions fetched per access future wideissue superscalars paper focus interaction hardware software provide high instruction bandwidth start presenting fullyautomated compiletime code reordering technique focuses maximizing sequentiality instructions still trying minimize instruction cache misses call technique software trace cache stc compare results obtained stc obtained htc alone combination techniques results obtained postgresql database arcade game spec95 benchmark results show large codes loops deterministic execution sequences like postgres95 database management system stc offers similar better results htc moreover combine stc htc obtain encouraging results specifically number fetched instructions per cycle obtained combination stc small htc comparable htc double size alone finally stc useful even combination large htc due instruction cache miss rate reduction paper structured follows section 2 describes fetch mechanism htc section 3 describes characterizes instruction reference stream variety workloads section 5 uses simulations evaluate various combinations stc htc section 6 discusses related work section 7 draw final remarks 2 fetch mechanism simulate aggressive sequential fetch unit similar described 12 shown figure 1 core fetch unit model composed interleaved instruction cache icache multiple branch predictor bp interleaved branch target buffer btb return address stack ras fetch unit designed fetch many contiguous instructions possible limits posed width data path branch predictor throughput work assume limit 16 instructions 3 branches per cycle core fetch unit icache tcache fill buffer fetch commit next fetch address hit next address logic fetch address decode figure 1 fetch unit model used simulation complete hardware trace cache mechanism simulations n16 m3 two consecutive icache lines accessed per cycle allowing us fetch sequential code crossing cache line boundary btb accessed parallel icache used predict address indirect jumps subroutine calls return address subroutines accurately predicted using ras assumed instructions predecoded allowing branches control transfers detected target pcrelative branches calculated obtained btb using outputs bp btb information regarding instructions represent control transfers obtain instruction mask select valid instructions fetched icache lines generate fetch address next cycle allow fetch unit stop indirect jumps break execution sequence add breaking bit btb informs fetch unit predicted jump address break sequence summarizing instruction fetch stops one conditions instructions fetched ffl 3 branches fetched ffl branch predicted taken ffl indirect jump predicted break execution sequence ffl system call fetched ffl misprediction btb miss high branch prediction accuracy use 4kb gag correlated branch predictor 14bit history length extended allow multiple branches predicted single cycle 256entry btb enhanced breaking bit 256entry ras also simulated core fetch unit conjunction basic trace cache model described 12 complete fetch unit trace cache tcache call hardware trace cache htc fill buffer reads instructions either fetch unit speculative trace con struction commit stage nonspeculative trace construction stores special purpose buffer trace completed stored tcache conjunction branch outcomes led instruction sequence leading instruction branch outcomes encountered future trace fed directly tcache decode unit fill buffer stops building trace conditions core fetch unit except case sequence breaks tcache able store noncontiguous instructions contiguous memory positions 3 software trace cache number useful instructions per cycle provided processor broadly determined three factors branch prediction accuracy instruction cache miss rate execution noncontiguous basic blocks deal last two problems propose code reordering technique uses whole memory space software trace cache store popular sequences basic blocks order avoid sequence breaks reorder basic blocks program change taken branches nontaken ones moving unused basic blocks execution path inlining basic blocks popular functions reduce instruction cache miss rate map popular traces reserved area icache algorithm based profile information means results obtained depend representativity training inputs popular execution paths given input set need related execution paths different input set running training set benchmark obtain directed graph basic blocks weighted edges edge connects two basic blocks p q q executed p weight edge w pq equal total number times q executed p weight basic block w p obtained adding weight outgoing edges branch probability edge bpq obtained w pqw p unexecuted basic blocks pruned graph next describe select seeds starting basic blocks code sequences algorithm builds basic block traces selected seeds mapping algorithm used allocate traces minimizing instruction cache misses 31 seed selection obtain ordered list seeds sorting entry points functions decreasing frequency execution tries expose maximum temporal locality first traces built start frequently referenced functions possible obtain better results seed selection based internal structure code show 11 however access source code applications always granted gaining deep understanding code time consuming task may offer improvement large enough compensate effort 32 trace building using weighted graph obtained running training set starting selected seeds implement greedy algorithm build basic block traces targeting increase code sequentiality given basic block algorithm follows frequently executed path implies visiting subroutine called basic block following control transfer highest probability used valid transitions basic block noted future examination algorithm use two parameters called exec threshold branch threshold trace building algorithm stops successor basic blocks visited weight lower exec threshold outgoing arcs branch probability less branch threshold case start next acceptable transition noted building secondary execution paths seed basic blocks reachable given seed included main secondary sequences proceed next seed figure 2a shows example weighted graph figure 2b shows resulting sequences use exec thresh 4 branchthresh 04 starting seed following likely outgoing edge basic block build sequence 2b transitions b1 c5 discarded due branch threshold noted transition a3 a5 valid transition start secondary trace a5 successors already visited sequence ends start secondary trace a6 weight lower exec threshold execution threshold b resulting sequences weighted graph discarded trace secondary branch threshold discarded branch threshold discarded trace main a3 a3 branch probabilitynode weight a8a1106 424302020 unconditional branch fallthrough subroutine call conditional branch subroutine return figure 2 trace building example code replication order increase code sequentiality introduce limited form code replication method allow main execution path subroutine replicated call points introduce two new threshold values execrep branchrep thresholds control amount code replicating sequence replicated call probability passes branchrep threshold starting basic block sequence passes execrep threshold example figure 2 found new call c1 say a7 would replicate sequence c1 c4 include a7 a8 main execution path threshold selection loop algorithm repeatedly selects set values four thresholds generates resulting traces pass basic blocks included previous passes pruned newly formed traces limiting amount code replicated pass iteratively selecting less less restrictive values thresholds build traces grouped passes decreasing frequency execution values selected exec branch threshold determine number basic blocks included pass algorithm generating larger smaller groups traces target pack given pass traces similar popularity keeping total number instructions control paper selected exec threshold pass contained approximately 4kb replicated code maximize effect code replication used least restrictive execrep branchrep thresholds 33 trace mapping shown figure 3 map code sequences decreasing order popularity concentrating likely used code first memory pages mapping popular sequences close equally popular ones reducing conflict among also popular sequences map reserved area cache leaving gaps create conflict free area cfa shielding popular traces interference code popular traces least popular traces000000000000000000000000000000000000111111111111111111111111111111111111111111111 icache size cfa instruction cache icache figure 3 trace mapping direct mapped instruction cache mapping algorithm applied set associative cache minor modifications complete study different factors determine instruction cache miss reduction offered mapping code sequences comparison code mapping algorithms postgresql database found 11 4 locality study objective stc build compile time popular traces built run time htc also stc targets minimization icache miss rate time analyze instruction reference stream wide set workloads characterizing instruction locality execution path determinism affect performance offered stc information intend predict performance increase expect using stc workload reference locality affect icache miss rate reduction offered technique basic block size number loops determinism program execution influence increase code sequentiality accomplished basic block reordering 41 workloads used four classes workloads trying cover wide range applications common integer floating point codes commercial workloads arcade games recent studies shown commercial workloads behave like common integer codes like specint set also well known floating point codes different behavior integer codes workloads include whole spec 95 benchmarks use postgresql 632 database management system commercial workload xblast 22 arcade game example little studied workload executions simulations needed develop work done using alpha 21164 processor dec atom trace driven simulation used different input sets obtain profile information obtain simulation results ensure improvements valid inputs profiled ones benchmarks run completion training simulation 42 code analysis examining profile information obtained running training set classify workloads attending characteristics affect performance technique code locality amount loops conditional branches subroutine calls basic block size number sequence breaks examine code locality determine number static instructions needed gather 75 90 99 dynamic instruction references shown table 1 total code size benchmark cfa size selected 32 64kb instruction caches also shown table 1 observe codes large working sets like applu apsi fpppp gcc postgres fit even 32kb caches furthermore codes exhibit little temporal locality like gcc fit 75 references 32kb cache select cfa size gathers 75 90 instruction references still leaving reasonable space rest code obviously larger caches allow larger cfa code replication example xblast concentrates 90 dynamic references 2362 instructions 9448 bytes almost fit 8kb cfa use 32kb instruction cache 64kb cache allow cfa grow 16kb next examine code sequentiality original layout observe floating point benchmarks large basic blocks 35 instructions average leading large code sequences 57 consecutive instructions aver age meanwhile average sequence length integer benchmarks usually 12 instructions less 2 consecutive basic blocks executed typical basic block size around 57 instructions dynamic references code cfa size benchmark 75 90 99 size 32kb 64kb 103su2cor 979 1839 4197 129741 8 104hydro2d 1223 1977 5371 125946 8 107mgrid 147 218 1029 112421 4 4 125turb3d 1065 1771 2828 121181 8 8 129compress 243 338 525 21991 4 8 134perl 987 1582 3006 108227 8 147vortex 751 1486 5128 172690 8 24 postgres 2716 5221 11748 374399 xblast 1100 2362 6326 430664 8 table 1 number static instructions needed accumulate 75 90 99 dynamic references total code size including unreferenced instructions selected cfa size instruction caches finally examined classification dynamic basic blocks executed benchmark different types basic block considered shown table 2 percentage basic blocks type executed shown table 3 last two columns show percentage branch loop basic blocks behave fixed way fbfl always taken always taken low proportion fixed loop branches means loop executes iterations less 20 bb type description target f fallthrough next instruction j unconditional branches pc relative unconditional branches indirect conditional branches pc relative subroutine call pc relative subroutine call indirect r subroutine returns indirect table 2 basic block types considered changing order basic blocks program reduce number unconditional branches change taken conditional branches taken ones also inlining popular functions eliminate subroutine calls returns increment number sequentially executed instructions note number sequence breaks due loop branches unpredictable conditional branches depend organization code consider indirect jumps separate way eliminated target address unknown may jump unexpected address ever reorder code frequent target address break execution sequence reduce number loops compiler optimizations like loop unrolling used yet included work consequently stc offer little advantage codes lots loops fixed conditional branches also codes subroutine calls benefit fact stc builds execution sequences crossing procedure calls number predictable basic block transitions determined fallthrough basic blocks pcrelative unconditional branches conditional branches fixed behavior subroutine calls percentage fallthrough basic blocks around 1020 codes stc performance determined rest basic block classes mainly percentage loop basic blocks criteria expect postgres 34 loop basic blocks 128 subroutine calls 43 conditional branches 762 behave fixed way one benefit stc hand 32 basic blocks executed ijpeg end loop branch barely 30 conditional branches behave fixed way makes difficult enlarge execution sequences among fp codes apsi looks best candidate large proportion fixed conditional branches loops subroutine calls 5 simulation results selecting appropriate cfa size bench mark measured increase number instructions executed two sequence breaks obtained stc table 4 shows percentage basic block transitions break sequence average number consecutive instructions executed benchmark original code proposed layout measured running test set number consecutive instructions executed represents performance limit sequential fetch unit even limited bus width branch predictor throughput branch mispredictions would still limited taken branches table 4 shows stc improves performance limit average original reordered benchmark bb size breaks seq len breaks seq len su2cor 198 52 377 hydro2d 149 69 216 53 283 mgrid 620 89 700 90 688 applu 234 50 465 58 407 turb3d 219 47 467 36 606 apsi 263 55 480 44 594 average compress 677 58 117 62 109 li 420 49 85 37 112 ijpeg 167 68 244 perl 536 54 100 38 139 vortex 476 55 87 27 176 average 685 57 116 42 158 postgres 458 51 90 25 183 xblast 525 62 84 27 195 table 4 percentage basic block transitions average number consecutive instructions executed original reordered code average bb size code layouts expected fp benchmarks barely reduce percentage sequence breaks best results obtained hydro2d apsi reductions 2025 translate sequence length increases 2431 expected due reduced proportion loops ex ecuted hand mgrid actually increased percentage sequence breaking bb transitions 89 90 fp benchmark higher proportion loop basic blocks executed integer codes obtain sequence length increases 100 m88ksim vortex postgres xblast meanwhile ijpeg experience noticeable improve ment roughly corresponds expected section 4 general terms integer codes experience significant reductions percentage sequence breaking bb transitions reordering codes execute 23 consecutive basic blocks raising average performance limit 158 instructions 51 fetch unit simulation table 5 shows simulation results fetch unit described section 2 using 32kb instruction cache icache simulated code layouts core fetch unit combination trace caches tcache 16 32kb code layout either original code base optimized layout corresponding cfa xkb cfax present number fetched instructions per access three separate results average number instructions core fetch unit icache provides average number instructions tcache provides average global performance tcache present core fetch unit global performance also separate icache tcache miss rates presented terms misses per line access two icache line accesses one tcache line access fetch unit access also present branch misprediction rate final performance metric number fetched instructions per cycle fipc fipc obtained dividing fipa estimated number cycles per access cpa instruction cache misses cause fetch engine stall increasing cpa branch mispredictions cause fetch unit fetch instructions wrong execution path effectively wasting fetch cycles used fixed number cycles icache miss assumed icache lines missed could served simultaneously also assumed average number penalty cycles branch misprediction icache miss penalties used 3 6 cycles branch misprediction penalties 4 8 12 cycles depend execution core processor software trace cache main effects stc increase fipa provided core fetch unit reduction icache miss rate codes show large improvements one numbers others seem unaffected found unexpected side effects branch prediction accuracy example reordering code postgres increases fipa core fetch unit 75 103 instructions 103 instructions per access still far away 183 shown table 4 performance limit limited bus width branch predictor throughput accuracy taken branches basic block type fixed branches 101tomcatv 33 41 31 282 590 02 10 12 920 493 103su2cor 118 59 20 444 253 01 52 53 659 924 104hydro2d 98 16 06 463 391 01 13 13 665 881 107mgrid 52 01 01 132 811 125turb3d 118 64 04 469 293 22 04 26 815 148 145fpppp 198 27 27 563 96 00 45 45 462 169 146wave5 152 17 08 352 319 25 51 76 905 883 126gcc 95 42 22 588 113 40 30 70 399 217 129compress 123 59 130li 211 32 19 398 116 73 39 112 444 508 132ijpeg 138 50 00 437 321 16 11 27 298 351 134perl 211 36 26 459 92 21 67 88 690 534 147vortex 160 62 05 444 178 01 75 76 637 326 postgres 221 25 12 434 34 128 09 136 762 263 xblast 208 10 01 501 112 22 62 84 651 135 table 3 percent dynamic basic blocks type workload percent conditional loop branches fixed behavior increase causes global fipa performance stc alone come quite close htc codes example core fetch unit provides 101 instruction per access reordered xblast using 16kb tcache original code obtains 108 instructions per access also observe stc drastically reduces icache miss rate fp integer codes large codes like apsi xblast postgres obtain miss rate reductions around 90 final icache miss rate around 1 32kb cache branch misprediction rate increases slightly reordered code reordered code reduces number taken branches introducing zeroes history register gag predictor leading worse utilization history table example branch misprediction rate m88ksim increases 49 79 code reordered mainly due reduction sequence breaks 61 27 see table 4 hardware trace cache fact traces provided tcache built fill buffer makes fipa provided tcache independent tcache geometry also reading instructions form dynamic stream makes traces independent code layout tcache miss rate seem depend code layout tcache size order reduce tcache miss rate techniques like partial matching 4 proposed also reduce number instruction provided htc mechanism assumes tcache always able provide instructions core fetch unit statement may true consider increased fipa performance stc reduced trace length caused techniques may worth adding functionality htc compiletime optimization obtain similar results htc visible impact branch prediction accuracy stc htc interaction best results obtained combining stc htc core fetch unit able provide instructions tcache miss lower icache miss rate example combination stc 16kb tcache vortex provides 122 instructions per access htc provides 113 larger codes tcache remember executed code sequences core fetch unit used extensively cases like gcc stc proves useful combining small tcache provide better results tcache double size alone combining stc large tcache still improves results larger codes fipa increase minimum small ones stc htc improve fipa also targets reduction cpa minimizing icache misses small codes like hydro2d already low icache miss rate stc useful htc providing much better performance cases like li increased branch misprediction rate actually hinder performance htc hand largest codes like postgres xblast stc alone offer similar better results htc alone cases combining compiletime runtime techniques offers best results raising fipc 59 stc 46 16kb htc 65 combination cases using combination stc small htc offers similar better results htc double size benefits stc obvious icache miss penalty increases branch misprediction penalty small htc proves useful icache miss penalty low conclude large codes loops stc provide better results htc alone combination stc small htc provides similar better results much larger htc alone combined large tcache stc still able provide performance improvements due reduced icache miss rate setup fipa miss rate branch fipc miss penbr pen bench tcache layout icache tcache global icache tcache mispr 68 612 38 64 base 117 117 008 03 115 114 115 116 hydro2d 16kb base 114 155 154 009 33 03 150 148 150 152 base 91 155 155 008 05 03 150 148 151 152 base 138 138 13 28 118 114 124 123 apsi 16kb base 135 159 153 15 231 28 129 124 135 134 base 133 159 155 14 165 28 130 135 137 136 base 69 69 116 49 32 30 41 35 base 65 138 111 145 369 49 44 40 57 50 base 64 137 119 135 254 49 47 42 61 54 base 77 77 102 100 32 27 37 38 43 gcc 16kb base 71 136 98 124 594 100 35 29 42 42 base 69 136 101 124 516 100 36 30 42 44 base 74 74 01 50 53 46 53 61 li 16kb base 64 135 106 02 404 50 68 58 68 82 base 61 134 111 02 320 50 70 59 70 85 base 117 117 007 97 83 72 83 97 ijpeg 16kb base 97 155 145 01 174 97 96 82 96 114 base 88 155 145 01 144 97 96 82 96 115 base 76 76 78 74 35 31 42 41 vortex 16kb base 67 138 108 81 421 74 46 39 53 55 base 67 137 113 82 347 74 47 40 54 57 base 75 75 93 30 41 39 52 43 postgres 16kb base 72 142 106 100 518 30 53 50 68 57 base 70 142 116 99 357 30 58 54 74 62 63 52 65 81 base 71 71 48 77 39 34 44 45 xblast 16kb base 70 137 108 68 434 77 49 42 56 59 base 68 137 111 72 379 79 49 42 56 59 table 5 simulation results 32kb instruction cache 6 related work much work code mapping algorithms optimize instruction cache miss rate works targeted less aggressive processors need fetch instructions multiple basic blocks per cycle hwu chang 7 use function inline expansion group traces basic blocks tend execute sequence observed profile code map traces cache functions executed close placed page pettis hansen 10 propose profile based technique reorder procedures program basic blocks within procedure aim minimize conflicts frequently used functions placing functions reference close memory also reorder basic blocks procedure moving unused basic blocks bottom function code even splitting procedures two moving away unused basic blocks torrellas et al 13 designed basic block reordering algorithm operating system code running conservative vector processor map code form sequences basic blocks spanning several functions keep section cache address space reserved frequently referenced basic blocks comparison stc pettis hansen method torrellas et al method found 11 gloy et al 5 extend pettis hansen placement algorithm procedure level consider temporal relationship procedures addition target cache information size procedure hashemi et al 6 kalamaitianos et al 8 use cache line coloring algorithm inspired register coloring technique map procedures resulting number conflicts minimized techniques developed vliw processors like trace scheduling 3 also identify frequent execution paths program techniques trying optimize scheduling instructions execution core processor performance instruction fetch en gine individual instructions moved crossing basic block boundary optimize ilp execution core processor inserting compensation code undo wrongly placed instructions wrong path taken traces define logical basic blocks need actually moved order obtain desired effect sense techniques stc may complementary one optimizes instruction fetch optimizes instruction scheduling using profile information hardware side techniques like branch address cache 14 collapsing buffer 2 trace cache 4 12 approach problem fetching multiple noncontiguous basic blocks cycle branch address cache collapsing buffer access nonconsecutive cache lines interleaved icache cycle merge required instructions accessed line trace cache require fetching nonconsecutive basic blocks icache stores dynamically constructed sequences basic blocks special purpose cache techniques require hardware extensions fetch unit target icache miss rate reduction relying techniques works examined interaction runtime compiletime techniques regarding instruction fetch mechanism chen et al 1 examined effect code expanding optimizations loop unrolling function inlining instruction cache miss rate also example software hardware cooperation patel et al 9 identify branches fixed behavior avoid making prediction increasing potential trace cache conclusions paper present profile based code reordering technique targets optimization instruction fetch performance aggressive wide superscalar processors carefully mapping basic blocks program store frequently executed traces memory using instruction cache software trace cache stc obtaining better performance sequential fetch unit complementing hardware trace cache htc mechanism better failsafe mechanism results show large codes loops deterministic execution sequences like database appli cations stc offer similar better results htc alone however optimum results come combination software hardware approaches number fetched instructions per cycle obtained combination stc small trace cache comparable htc double size alone storage popular traces instruction cache leads new view fetch unit trace cache tightly coupled contents instruction cache traces redundantly stored caches effectively wasting space displacing potentially useful traces trace cache yet another example need software hardware work together order obtain optimum performance minimum cost r effect code expanding optimizations instruction cache design optimization instruction fetch mechanism high issue rates trace scheduling technique global microcode compaction alternative fetch issue techniques trace cache mecha nism procedure placement using temporal ordering informa tion efficient procedure mapping using cache line coloring achieving high instruction cache performance optimizing compiler improving trace cache effectiveness branch promotion trace packing profile guided code posi tioning code reordering decision support systems optimized instruction fetch trace cache low latency aprroach high bandwith instruction fetching optimizing instruction cache performance operating system intensive workloads increasing instruction fetch rate via multiple branch prediction branch address cache tr ctr code reordering limited branch offset acm transactions architecture code optimization taco v4 n2 p10es june 2007