asynchronous parallel pattern search nonlinear optimization introduce new asynchronous parallel pattern search apps parallel pattern search quite useful engineering optimization problems characterized small number variables say fifty less objective functions expensive evaluate defined complex simulations take anywhere seconds many hours run target platforms apps loosely coupled parallel systems widely available exploit algorithmic characteristics pattern search design variants dynamically initiate actions solely response messages rather routinely cycling fixed set steps gives versatile concurrent strategy allows us effectively balance computational load across available processors allows us incorporate high degree fault tolerance almost additional overhead demonstrate effectiveness preliminary implementation apps standard test problems well engineering optimization problems b introduction interested solving unconstrained nonlinear optimization problem introduce family asynchronous parallel pattern search apps methods pattern search 15 class direct search methods admits wide range algorithmic possibilities exibility aorded denition pattern search 23 16 adapt design nonlinear optimization methods intended eective variety parallel distributed computing platforms motivations several first optimization problems interest us typically dened computationally expensive computer simulations complex physical processes simulation may take anywhere seconds many hours computation single processor discuss x2 dominant computational cost pattern search methods lies objective function evalu ations even objective function inexpensive compute relative cost additional work required within single iteration pattern search negligible given considerations one feature pattern search exploit compute multiple independent function evaluations simultaneously eort accelerate search process improve quality result ob tained thus approach take advantage parallel distributed computing platforms also practical reason independent computational environment using pattern search methods problems interest simply put problems dened expensive computer simulations complex physical processes often cannot rely gradient f conduct search typically procedure exists evaluation gradient creation procedure deemed untenable approximations gradient may prove unreliable instance accuracy function trusted signicant decimal digits dicult construct reliable nitedierence approximations gradient finally theory pattern search assumes f continuously dierentiable pattern search methods eective nondierentiable even discontinuous problems precisely explicitly rely derivative information drive search thus focus pattern search practical computational reasons however nature problems interest features current distributed computing environments raise second issue address work original investigation parallel pattern search pps methods 1 7 22 made two 1 original investigations focused parallel direct search pds precursor general pps methods discussed fundamental assumptions parallel computation environment 1 processors homogeneous tightly coupled 2 amount time needed complete single evaluation objective eectively constant time reexamine two assumptions clearly given current variety computing platforms including distributed systems comprising looselycoupled often heterogeneous commercial otheshelf components 21 rst assumption longer valid second assumption equally suspect standard test problems used assess eectiveness nonlinear optimization algorithm typically closedform algebraic expressions function thus standard assumption xed choice n evaluations complete constant time valid however given interest optimizing problems dened simulations complex physical processes often use iterative numerical techniques assumption evaluations complete constant computational time often hold fact behavior simulation given input dicult assess advance since behavior simulation vary substantially depending variety factors problems computing environments interest longer assume computation proceeds lockstep single synchronization step end every iteration global reduction used 22 neither appropriate eective following factors holds function evaluations complete varying amounts time even equivalent processors processors employed computation possess dierent performance characteristics processors varying loads goal introduce class apps methods make eective use variety computing environments well devise strategies accommodate variation completion time function evaluations approach outlined x3 third nal consideration address paper incorporating fault tolerant strategies apps methods since one intent use software largescale heterogeneous systems combination commodity parts shared resources raises growing concern reliability individual processors participating computation embark lengthy computation want reasonable assurance producing nal result even subset processors fail thus goal design methods anticipate failures respond protect solution process rather simply checkpointing intermediate computations disk restarting event failure instead considering methods heuristics adaptively modify search strategy discuss technical issues detail x4 x5 provide numerical results comparing apps pps standard engineering optimization test problems nally x6 outline additional questions pursue although rst embark design asynchronous parallel optimization algorithms aware little work particularly area nonlinear programming approaches developing asynchronous parallel newton quasinewton methods proposed 4 8 though assumptions underlying approaches dier markedly address specically assume solving linear system equations iteration dominant computational cost optimization algorithm dimensions problems interest relatively large dierent line inquiry 20 considers use quasinewton methods context developing asynchronous stochastic global optimization algorithms focus nding local minimizers parallel pattern search proceeding discussion apps methods let us rst review features direct search general pattern search particular direct search methods characterized neither requiring explicitly approximating derivative information engineering literature direct search methods often called zeroorder methods opposed rstorder methods method steepest descent secondorder methods newtons method indicate highest order term used local taylor series approximation f characterization direct search perhaps useful emphasizes higherorder methods derivatives used form local approximation function used derive search direction predict length step necessary realize decrease instead working local approximation f direct search methods work directly f pattern search methods comprise subset direct search methods rigorous formal denitions pattern search 16 23 primary characteristic pattern search methods sample function predened pattern points lie rational lattice enforcing structure form points pattern well simple rules outcome search subsequent updates standard global convergence results obtained purposes feature pattern search amenable parallelism candidates pattern dened function values points computed independently thus concurrently make concrete consider following particularly simple version pattern search algorithm iteration k iterate x k 2 r n steplength parameter k 0 pattern p points denoted g purposes simple example choose fe represents jth unit vector discuss end section choices possible several algorithmic options open us one possibility look successively pattern points x k either nd point x fx fx k exhaust 2n possibilities extreme could determine x 2 fx k 2ng requires us compute fx k 2n vectors set fig 1 illustrates pattern points among search x 2 r z figure 1 simple instance pattern search either variant pattern search none pattern points reduces ob jective set x reduce setting otherwise set repeat process suitable stopping criterion k tol satised several things note two search strategies outlined first even though pattern instances two dierent algorithms dierent search strategies could conceivably produce dierent sequences iterates even dierent local minimums second design search strategies ects intrinsic assumptions nature function computing environment search executed clearly rst strategy evaluates one function value time conceived execution single processor cautious strategy computes function values needed suggests frugality respect number function evaluations allowed second strategy could certainly executed single processor one could make argument could algorithmic advantages also clearly strategy easily make use multiple processors straightforward derive pps second strategy illustrated fig 2 proceeding description apps however need make one remark pattern already seen easily derive two dierent search strategies using basic pattern requirements outcome search mild fail nd point reduces value f x k must try smaller value k otherwise accept new iterate point pattern produces decrease latter case may choose modify k either case free make changes pattern used next iteration though left pattern unchanged examples given however changes either step length parameter pattern subject certain algebraic conditions outlined fully 16 2 reduction parameter usually 1but number set 0 1 initialization select pattern g select steplength parameter 0 select stopping tolerance tol select starting point x 0 evaluate fx 0 iteration 1 evaluate concurrently 2 determine x fx synchronization point 3 fx fx k set x else set x 4 k1 tol exit else repeat figure 2 pps algorithm still remains question constitutes acceptable pattern borrow following technical denition 6 16 pattern must positive spanning set r n addition add condition spanning set composed rational vectors denition 1 set vectors fd positively spans r n vector z 2 r n written nonnegative linear combination vectors set ie z 2 r n exists positive spanning set contains least n1 vectors 6 trivial verify set vectors used dene pattern examples positive spanning set 3 3 terminology positive spanning set misnomer proper name would non negative spanning set asynchronous parallel pattern search ineciencies processor utilization pps algorithm shown fig 2 arise objective function evaluations complete approximately amount time happen several reasons first objective function evaluations may complex simulations require dierent amounts work depending input parameters second load individual processors may vary last groups processors participating calculation may possess dierent computational characteristics objective function evaluations take varying amounts time processors complete share computation quickly wait remaining processors contribute results thus adding processors correspondingly search directions actually slow pps method given fig 2 increased synchronization penalty limiting case slow objective function evaluation one never com pletes could happen processor fails course calculations situation entire program would hang next synchronization point designing algorithm handle failures plays role discussion section given detailed coverage next design apps addresses limitations slow failing objective function evaluations based peertopeer approach rather masterslave although masterslave approach advantages critical disadvantage although recovery failure slave processes easy cannot automatically recover failure master process peertopeer scenario processes equal knowledge process charge single direction search pattern order fully understand apps let us rst consider single processors algorithm synchronous pps peertopeer mode shown fig 3 subscripts dropped illustrate process handles data set directions processes forms positive spanning set exception initialization nalization communication process peers global reduction step 2 terminate processors detect convergence time since identical albeit independent values trial 4 asynchronous peertopeer version pps see fig 4 allow process maintain versions x best x trial etc unlike synchronous pps values may always agree values processes process decides next based current information available nds point along search direction improves upon best point knows far broadcasts message processors letting know also checks messages processors replaces best point 4 heterogeneous environment danger processors may value trial slight dierences arithmetic way values stored see 2 iteration 1 compute x trial x best direction 2 determine f associated x via global reduction minimizing f trial values computed step 1 3 f best fx best f best g fx g else trial 1 4 trial tol go step 1 else exit figure 3 peertopeer version synchronous pps iteration consider incoming triplet fx received another processor best fx best f best best g fx trial best 1 compute x trial x best direction 2 g 3 f best fx best f best best g fx best broadcast new minimum triplet fx best f best best g processors else trial 1 4 trial tol goto step 0 else broadcast local convergence message pair fx best f best g 5 wait either enough processes converged point b better point received case exit case b goto figure 4 peertopeer version apps incoming one improvement neither trial point incoming messages better performs contraction continues convergence trickier issue synchronous version processors reach trial tol time instead processor converges direction owns waits processes either converge point produce better point since every good point broadcast process every process eventually agrees best point nal apps algorithm slightly dierent version fig 4 spawn objective function evaluation separate process motivation may sometimes want stop objective function evaluation completes event good point received another processor create group apps daemon processes follow basic apps procedure outlined fig 4 except objective function evaluation executed separate process result apps daemons working peertopeer mode owning single slave objective function evaluation process apps daemon see fig 5 works primarily message processing center receives three types messages return spawned objective function evaluation new minimum convergence messages apps daemons daemon receives return message determines current trial point new minimum broadcasts point processors trial used generate new minimum saved used determine far step along search direction alternative would reset trial 0 every time switch made new point scaling information lost may lead unnecessary additional function evaluations comparison trial best fvalues encounter important caveat heterogeneous computing 2 comparison values f etc controls ow apps method depend comparisons give consistent results across processors therefore must ensure values compared level precision available processors words safe comparison declares mach mach maximum mach new minimum message means another processor found point thinks best receiving daemon must decide agrees case must decide handle tiebreaking consistent manner f best need able say point best indeed points comparing equal ie x best x tie breaking scheme following f best compare best select larger value values also equal check next see indeed two points rather comparing x best x directly measuring norm dierence use unique identier included point thus two points equal return objective function evaluation receive f trial 1 update x best andor trial f trial f best fx best f best best g fx trial f trial trial g ii broadcast new minimum message triplet best f best best g processors b else x best point used generate x trial trial best c else trial 1 2 check convergence spawn next objective function evaluation trial tol compute x trial x best trial spawn new objective function evaluation b else broadcast convergence message best f best best g processors including new minimum message receive triplet fx g 1 f best best locally converged ag true else ag false b set fx best f best best g fx g c ag true break current objective function evaluation spawn compute x trial x best trial spawn new objective function evaluation convergence message receive triplet triplet fx g 1 go though steps new minimum sure point x best 2 temporary master consider processes far converged x best enough processes converged associated directions form positive spanning set output solution shutdown remaining apps daemon processes exit figure 5 apps daemon message types actions fvalues values unique identiers match 5 certain cases current objective function evaluation terminated favor starting one based new best point imagine following scenario suppose three processes b c start value x best generate x trial spawn objective function evaluations objective function evaluation takes several hours process nishes objective function evaluation process nd improvement contracts spawns new objective function evaluation minutes later process b nishes objective function evaluation nds improvement broadcasts new minimum processes process receives message terminates current objective function evaluation process order move better point may save several hours wasted computing time however process c still working rst objective function evaluation waits complete considering moving new x best daemon receives convergence message records converged direction possibly checks convergence design method requires daemon cannot locally converge point evaluated least one trial point generated best point along search direction point associated boolean convergence table sent every message process locally converges adds true entry spot convergence table sends convergence message order actually check convergence sucient number processes useful temporary master avoid redundant computation dene temporary master process lowest process id usually process 0 always case consider faults discussed next section temporary master checks see converged directions form positive spanning set outputs result terminate entire computation checking positive spanning set done follows let v candidate positive basis solve nonnegative least squares problems according following theorem theorem 31 set positive spanning set set positive span 1 vector 1s alternatively check positive basis rst verifying v spanning set using say qr factorization pivoting solving linear program theorem 32 wright 24 spanning set positive spanning set maximum following lp 1 5 system miss two points equal generated via dierent paths rst case use software nonnegative least squares problem netlib due lawson hanson 14 second case software implementation complicated since need qr factorization linear program solver latter particularly hard come freely available portable easytouse format 4 fault tolerance apps move toward variety computing environments including heterogeneous distributed computing platforms brings increased concern fault tolerance parallel algorithms large size diversity components complex architecture systems create numerous opportunities hardware failures computational experience conrms reasonable expect frequent failures addition size complexity current simulation codes call question robustness function evaluations fact application developers testify possible generate input parameters simulation codes fail complete successfully thus must contend software failures well hardware failures great deal work done computer science community regard fault tolerance however much work focused making fault tolerance transparent user possible often entails checkpointing entire state application disk replicating processes fault tolerance traditionally used looselycoupled distributed applications depend complete business database applications lack interdependence atypical scientic applications checkpointing replication adequate techniques scientic applications incur substantial amount unwanted overhead however certain scientic applications characteristics exploited ecient elegant fault tolerance algorithmdependent variety fault tolerance already received considerable amount attention scientic computing community see eg 11 12 approaches rely primarily use diskless checkpointing signicant improvement traditional approaches nature apps even reduce overhead fault tolerance dispense checkpointing altogether three scenarios consider addressing fault tolerance apps 1 failure function evaluation 2 failure apps daemon failure host scenarios shown figure 6 approaches handling daemon host failures similar one another function evaluation failure treated somewhat dierent manner function evaluation fails respawned parent apps daemon failure occurs specied number times trial point daemon fails 6 apps daemon fails rst thing temporary master check convergence since defunct daemon may process check died next checks whether directions owned remaining daemons form positive basis convergence still guaranteed nothing done otherwise dead daemons restarted host fails apps daemons running host restarted dierent host according rules stated daemon failures faulty host removed list viable hosts longer used exit function evaluation 1 number tries point less maximum allowed number respawn function evaluation 2 else shutdown daemon apps daemon failed 1 record failure 2 temporary master check convergence converged output result terminate computation b directions corresponding remaining daemons form positive spanning set respawn failed daemons host failed 1 remove host list available hosts figure tolerance messages actions two important points made regarding fault tolerance apps first single points failure apps algorithm scenarios requiring master coordinate eorts master xed fail performing tasks another master steps take means degree fault tolerance apps constrained underlying communication architecture current implementation apps uses pvm single point failure master pvm daemon 9 expect harness 1 successor pvm eliminate disadvantage second point interest 6 situation handled dierent ways dierent applications attempts evaluate certain point could abandoned without terminating daemon checkpointing replication processes necessary algorithm recongures new apps daemons require small packet information existing process order take failed daemon left therefore able take advantage characteristics apps order elegantly incorporate high degree fault tolerance little overhead despite growing concern fault tolerance parallel computing world aware one parallel optimization algorithm incorporates fault tolerance fatcop 3 fatcop parallel mixed integer program solver implemented using condorpvm hybrid communication substrate fatcop implemented masterslave fashion means single point failure master process addressed master checkpoint information disk via condor recovery requires user intervention restart program event failure contrast apps recover failure type process including failure temporary master checkpointing whatsoever 5 numerical results compare pps 7 apps several test problems well two engineering problems thermal design problem circuit simulation problem tests performed cplant supercomputer sandia national labs livermore california cplant cluster dec alpha miata 433 mhz processors tests used 50 nodes dedicated sole use 51 standard test problems compare apps pps 8 16 24and 32 processors six four dimensional test problems broyden2a broyden2b chebyquad epowell toint trig vardim 18 5 since function evaluations extremely fast added extra busy work order slow better simulate types objective functions interested 8 parameters apps pps set follows let problem dimension let p number processors rst 2n search directions g remaining p 2n directions vectors randomly generated dierent seed every run normalized unit length set search directions positive spanning set initialize 7 using implementation positive basis pps outlined fig 3 rather wellknown parallel direct search pds 22 pds based positive basis framework quite dierent method described fig 3 making comparisons dicult 8 precisely busy work solution 100 101 nonnegative least squares problem added two additional twists way updated tests first search direction yields best point two times row doubled broadcast second smallest allowable new minimum least three contractions required local convergence way guaranteed several evaluations along search direction point method process function function init idle total id evals breaks time time time summary 2725 706 004 007 2472 summary 235 na 022 610 3063 table 1 detailed results epowell eight processors considering summary results examine detailed results two sample runs given table 1 process reports counts timings times reported seconds wall clock times apps asynchronous number function evaluations varies process case much 25 furthermore apps sometimes breaks functions midway execution hand every process pps executes number function eval uations breaks apps pps initialization time longer rst process since charge spawning remaining tasks idle time varies task task overall much lower apps pps apps process idle locally converged pps process may potentially idle time every iteration waits completion global reduction total wall clock time varies process process since starts stops slightly dierent times summary information average processes except case total time case maximum times reported search directions generated randomly every run pps apps generates dierent path solution possibly dierent solutions case multiple minima 9 nondeterministic nature apps gets dierent results every run even search directions identical therefore problem report average summary results 25 runs problem procs function evals apps idle time total time name apps pps breaks apps pps apps pps broyden2a 8 4059 3700 814 007 095 388 488 chebyquad 8 7306 6200 1674 005 161 686 811 toint trig 8 5383 4100 1097 004 111 499 560 table 2 results collection four dimensional test problems test results summarized table 2 tests run fairly favorable environment ppsa cluster homogeneous dedicated processors 9 exception pps 8 extra search directions solution path every runonly timings dier primary diculty pps cost synchronization global reduction terms average function evaluations per processor apps pps required number general apps pps number function evaluations per processor decreased number processes increased expect idle time apps less pps indeed idle time two orders magnitude less furthermore idle time pps increases number processors goes apps faster average pps 22 24 cases total time apps either stayed steady reduced number processors increased contrast total pps time increased number processors increased due synchronization penalty comparing apps pps simple problems necessarily indicative results typical engineering problems next two subsections yield meaningful comparisons given types problems pattern search best suited 52 twafer thermal design problem engineering application concerns simulation thermal deposition furnace silicon wafers furnace contains vertical stack 50 wafers several heater zones goal achieve specied constant temperature across wafer throughout stack simulation code twafer 10 yields measurements discrete collection points wafers objective function f dened least squares n discrete wafer temperatures j prescribed ideal x unknown power parameters heater zone consider four seven zone problems problem used following settings apps pps rst n1 search directions points regular simplex centered origin remaining generated randomly normalized unit length set diculties implementation point view quite common dealing simulation codes twafer legacy code expects input le specic name produces output le specic name names les cannot changed twafer cannot hooked directly pvm consequence must write wrapper program runs input lter executes twafer via system call runs output lter input le twafer must contain entire description furnace wafers changing values within le input lter generates input le twafer using template input le template le contains tokens replaced optimization variables output le twafer contains heat measurements discrete points output lter reads values computes least squares dierence ideal temperature order determine value objective function additional caveat twafer must executed uniquely named subdirectory input output les confused twafer process may accessing disk lastly twafer executed via system call apps way terminating execution prematurely apps terminate wrapper program twafer continue run consuming system resources therefore allow function evaluations run completion allow breaks another feature twafer nonnegativity constraints power settings use simple barrier function returns large value eg problem method procs fx function idle total evals time time 4 zone apps 20 067 3346 017 39594 4 zone pps 20 066 3799 4477 50388 table 3 results four seven zone twafer problems results twafer problem given table 3 four zone results averages ten runs seven zones results averages nine runs tenth pps run failed due node fault tenth apps run several faults although get nal solution summary data incomplete also list value objective function solution observe pps yields slightly better function values compared original value 1000 average cost function evaluations time average function evaluation execution time four zone problem 13 seconds seven zone problem 104 seconds however number function evaluations includes instances bounds violated case twafer code executed execution time essentially zero since simply return pps substantial amount idle time relatively high apps idle time seven zone problem due single run idle time particularly high nodes 634 seconds average 53 spice circuit simulation problem problem match simulation data experimental data particular circuit order determine characteristics case 17 variables representing inductances capacitances diode saturation currents transistor gains leakage inductances transformer core parameters objective function dened n number time steps v sim j x simulation voltage time step input x v exp j experimental voltage time step j spice3 19 package used simulation like twafer spice3 communicates via le input output use wrapper program input lter spice complicated twafer variables problem dierent scales since apps mechanism scaling handled within input lter computing ane transformation apps variables additionally variables upper lower bounds use simple barrier function output lter spice also complicated twafer spice output les consists voltages matched experimental data experimental data two cycles output voltage measured approximately fig 7 simulation data contains approximately 10 cycles last complete cycles used early cycles stable cycles must automatically identied data aligned experimental data furthermore time steps simulation may dier time steps experiment simulation data interpolated piecewise constant match experimental data function value initial point 465 apps parameters set follows search directions generated way test problems set tolerance corresponds less 1 change circuit parameter allow breaks since function evaluation called wrapper program via system call results apps pps spice problem reported table 4 case reporting results single runs give results 34 50 processors average spice run time approximately 20 seconds however dierentiate times boundary conditions violated spice code actually executed increasing number processors 47 results 39 reduction execution time apps 4 pps 34 50 processors apps faster pps even produces slightly better objective value compared starting value 400 solution two constraints binding 55time voltage figure 7 spice results solid line represents experimental output dashed line represents simulation output optimization dotted line represents starting point optimization method procs fx function idle total evals time time apps 34 263 575 11192 133055 apps 50 269 506 6322 80729 pps 34 288 530 52148 171224 pps 50 349 470 90548 164653 table 4 results 17 variable spice problem initial final fx total procs procs time 34 34 278 161846 50 table 5 apps results 17 variable spice failure approximately every seconds table 5 shows results running apps faults case used program automatically killed one pvm process every seconds pvm processes apps daemons wrapper programs spice3 simulation executed via system call continues execute even wrapper terminates regardless spice3 program longer communicate apps eectively dead results quite good case 34 processors every apps task fails must restarted order maintain positive basis nal number apps processes 34 total time increased 21 despite approximately 50 failures furthermore time still faster pps case 50 processors nal number processors 32 recall tasks restarted enough remaining form positive basis case 50 processors solution time increased 29 still faster pps case however quality solution degraded likely due fact solution lies boundary search directions failed needed convergence see lewis torczon 17 6 conclusions newlyintroduced apps method superior pps terms overall computing time homogeneous cluster environment generic test problems engineering applications expect dierence even pronounced larger problems terms execution time number variables heterogenous cluster environments unlike pps apps required synchronizations thus gains advantage reducing idle time apps fault tolerant see results spice problem 34 processors suer much slowdown case faults forthcoming work kolda torczon 13 show unconstrained case apps method converges even case faults assumptions pattern search 23 although engineering examples used work bound constraints apps method fully designed purpose evidenced poor results spice problem faults 50 processors future work explore algorithm implementation theory constrained cases implementation described daemons function evaluations pairs however multiprocessor mpp compute nodes means several daemonfunction evaluation pairs per node alternative implementation apps developed exactly one daemon per node regardless many function evaluations assigned part alternative implementation ability dynamically add new hosts become available readd previously failed hosts incorporated another improvement implementation addition function value cache order avoid reevaluating point challenge deciding two points actually equal especially dicult without knowing sensitivity function changes variable importance positive bases pattern raises several interesting research questions first might consider best way generate starting basis desire pattern maximizes probability maintaining positive basis event failures another research area aect conditioning positive basis convergence numerical studies indicated quality positive basis may issue last supposing enough failures occurred longer positive basis may ask easily determine fewest number vectors add positive basis current implementation simply restarts failed processes acknowledgments thanks jim kohl ken marx juan meza helpful comments advice implemenation apps test problems r harness next generation distributed virtual machine practical experience numerical dangers heterogeneous computing fatcop fault tolerant condorpvm mixed integer program solver convergence numerical results parallel asynchronous quasinewton method testing class methods solving minimization problems simple bounds variables theory positive linear dependence asynchronous parallel newton method pvm parallel virtual machine users guide tutorial network parallel computing model low pressure chemical vapor deposition hotwall tubular reactor convergence asynchronous parallel direct search solving least squares problems pattern search works rank ordering positive bases pattern search algorithms build beowulf guide implementation application pc clusters pds direct search methods unconstrained optimization either sequential parallel machines note positively spanning sets tr ctr genetha gray tamara g kolda algorithm 856 appspack 40 asynchronous parallel pattern search derivativefree optimization acm transactions mathematical software toms v32 n3 p485507 september 2006 ismael vaz lus n vicente particle swarm pattern search method bound constrained global optimization journal global optimization v39 n2 p197219 october 2007 steven benson manojkumar krishnan lois mcinnes jarek nieplocha jason sarich using ga tao toolkits solving largescale optimization problems parallel computers acm transactions mathematical software toms v33 n2 p11es june 2007 genetha anne gray tamara g kolda ken sale malin young optimizing empirical scoring function transmembrane protein structure determination informs journal computing v16 n4 p406418 fall 2004 jack dongarra ian foster geoffrey fox william gropp ken kennedy linda torczon andy white references sourcebook parallel computing morgan kaufmann publishers inc san francisco ca