accurate bidiagonal reduction computing singular value decomposition bidiagonal reduction preliminary stage fastest stable algorithms computing singular value decomposition svd available however bestknown error bounds bidiagonal reduction methods matrix form orthogonal varepsilonm machine precision fmn modestly growing function dimensions aa preprocessing technique analyzed higham linear algebra appl 309 2000 pp 153174 uses orthogonal factorization column pivoting obtain factorization aq left beginarrayc ct 0 endarray right pt q orthogonal c lower triangular p permutation matrix bidiagonal reduction applied resulting matrix cto reduction new givensbased bidiagonalization algorithm proposed produces bidiagonal matrix b satisfies c bounded componentwise delta c satisfies columnwise bound based upon growth lower right corner c u v orthogonal nearly working precision reduction good menu algorithms obtain singular values bidiagonal matrix b relative accuracy thus obtaining svd c much accurate obtained standard bidiagonal reduction procedures additional operations required standard bidiagonal reduction algorithm golub kahan j soc indust appl math ser b numer anal 2 1965 pp 205224 using givens rotations instead householder transformations compute matrix v 2n33 flops compute column norms b introduction consider problem reducing theta n matrix bidiagonal form find orthogonal matrices u nthetan denote b 11 use shorthand use matlablike form also use matlab notation submatrices thus ai j k denotes submatrix consisting rows j columns k likewise denotes columns k ai rows j matrix denote th singular value x ng also let x moorepenrose psuedoinverse x let ji givens rotation angle ij applied columns j department computer science engineering pennsylvania state university university park pa 168026106 email barlowcsepsuedu url httptrantorcsepsuedubarlow research jesse l barlow supported national science foundation grants ccr9201612 ccr9424435 part work done author visiting university manchester department mathematics manchester m13 9pl uk reduction 11 usually done preliminary stage computing singular value decomposition number good algorithms computing singular value decomposition bidiagonal matrices know zeroshift qr algorithm 13 bisection 4 dqds algorithm 15 compute singular values b relative accuracy also know reasonable expect algorithm compute singular values matrix relative accuracy unless matrix acyclic graph 11 totally sign compound 12 thus surprising algorithm expected produce bidiagonal form general matrix relative accuracy fixed precision arithmetic jacobi algorithm accurate method finding singular values general matrix algorithm requires bidiagonal reduction unfortunately jacobi algorithm usually slower simplicity assume reduction followed qr algorithm produce svd 20n 3 flops one jacobi sweep requires 7n 3 flops thus jacobi competitive must converge three sweeps rarely happens paper present bidiagonal reduction method often preserve accuracy singular value decomposition reduction computed two stages first stage using householder factorization method cox higham 10 reduce lower triangular nthetan floating point arithmetic machine unit first stage reduction satisfies fn modestly sized functions ae growth factor given 10 similar reduction recommended demmel veselic 14 using jacobi method thus difference algorithm reduction c second stage apply new bidiagonal reduction algorithm c algorithm produces bidiagonal matrix b n theta n matrices orthogonal matrices u v smallest integer kc kckf growth ae v bounded provided theta principal submatrix corresponding krylov matrix nonsingular submatrix singular standard backward error bounds 17 apply good bound jacobi method achieves 14 27 28 much better achieved standard algo rithm moreover algorithm implemented less 2 8 flops lawsonhansonchan svd 26 pp1071209 using fast versions givens rotations additional overhead may reduced 827n 3 flops procedure bidiagonal reduction important differences golubkahan householder transformation based procedure 17 givens transformations used construction right orthogonal matrix v clearly 2 theta 2 householder transformations could also used ffl matrix preprocessed householder factorization maximal column pivoting using new procedure due cox higham 10 ffl first column v e 1 first column identity matrix ffl computation matrices u v interleaved different manner preserve accuracy small columns next section give algorithm producing bidiagonal form x4 prove bounds 1213 x5 give tests conclusion 2 reduction triangular form giving bidiagonal reduction pro cedure preprocess reducing lower triangular form using householder transformation based procedure due cox higham 10 based upon row column pivoting procedure powell reid 30 uses simpler form pivoting procedure follows 1 reorder rows 2 using maximal column pivoting algorithm businger golub 8 factor nthetan lower triangular particular householder factorization algorithm strong numerical stability properties let c computed lower triangular factor cox higham 10 based analysis complicated pivoting strategy powell reid 30 showed orthgonal matrix u 0 2 ae growth factor bounded p 2 column oriented error bound 22 holds standard householder factorization 31 second rowwise error bound 23 shown algorithms kind row column permutations stability similar results givens based algorithms given barlow 3 barlow handy 5 givens based algorithm 3 could substituted coxhigham algorithm gulliksen 21 given new framework orthogonal transformations property cox higham demonstrate householders original version householder transformation must used bounds hold bound hold parletts 29 version householder transformation used columns matrix c satisfy fact reduction satisfying property 24 suitable preprocessing step lead results given give algorithms computing bidiagonal reduction c 3 bidiagonal reduction algorithms 31 golubkahan bidiagonal reduction usual bidiagonal reduction algorithm given golub kahan 17 pp208210theorem 1 see also golub reinsch 19 pp404405 given square matrix c since c lower triangular exactly rank deficient zero diagonals minor obvious modifications bidiagonal reduction algorithms section necessary c zero diagonals algorithm 31 golubkahan bidiagonal reduction 1 find orthogonal transformation u 1 2 find orthogonal transformation v k b find orthogonal transformation u k 3 bidiagonal reduction c given golub kahan 17 used householder transformations describe algo rithm case requires 4n 3 2 flops matrices u v kept factored form u v accumulated requires 8n 3 2 flops 32 givens based bidiagonal reduction algorithm new algorithm bidiagonal reduction c algorithm 32 new procedure bidiagonal reduction present givensbased bidiagonal reduction procedure n theta n matrix c satisfying 24 x4 show new algorithm achieve error bounds form 1518 1 determine smallest integer 22 empty matrix use algorithm 31 complete steps 24 2 compute vector z given 22 product givens rotations compute let u 1 orthogonal transformation 3 let v k product givens rotations satisfies b find orthogonal transformation u k ck n k n u 4 bidiagonal reduction c given use standard givens rotations steps 24 algorithm require 10n 3 flops use fast givens rotations described hammarling 22 gentleman 16 bareiss 2 barlow 3 anda park 1 would produce algorithm approximately golubkahan householder based procedure step one requires minimum length solution least squares problem 11 already upper triangular procedure lawson hanson 26 pp77 83 would allow us compute orthogonal factorization matrix flops maximum complexity step step never costs 8 table 31 summarizes complexity two bidiagonal reduction algorithms table complexity bidiagonal reduction algorithms compute 31 algorithm 32sg algorithm 32fg givens rotations fast givens rotations 4 growth factors error bounds 41 bounding columns c show advantages algorithm 32 bidiagonal reduction consider effects orthogonal transformations right used form v algorithm 31 let u matrix algorithm 31 consider orthogonal equivalence let form f k 12 zero except last row therefore effect zeros lower k theta n gamma k block f following lemma bounds effect large class orthogonal transformations right lemma 41 let f nthetan partitioned according v 11 nonsingular let 22 proof matching blocks 41 42 yields using fact v 11 nonsingular block gaussian elimination yields 43 result lemma 41 generalizes case given next lemma 42 let f 2 mthetan partitioned according 22 0 v form 41 let g fv partitioned proof note 22 v 1 partition v according 41 first note v 11 nonsingular v nonsingular evaluating 22 leads gammav 2 thus k 22 f 23 13 23 blocks c unaffected apply lemma 41 f 1 f 1 obtain 45 following corollary relates growth 2 3 block growth gaussian elimination corollary 43 let f g v lemma 42 v growth factor k steps gaussian elimination v euclidean norm proof let l unit lower triangular r upper triangular using fact v 22 solves 22 22 v 22 result performing k steps gaussian elimination v thus taking norms yields 22 v growth factor k steps gaussian elimination v unfortunately bound growth gaussian elimination given row ordering better orthogonal matrices matrices following result proven barlow zha 7 proposition 44 let nthetan nonsingular plr factoriza tion gaussian elimination row ordering p l lower triangular r upper triangular growth factor ae k let x factorization v orthogonal upper triangular gaussian elimination partial v obtains plr factorization k ae k x suppose c 2 nthetan lower triangular matrix satisfying j note row pivoting procedure used previous section assures us small block c 22 may isolated nthetan given x blocks satisfying 22 c 21 j 4 22 c 22 note 2 prove results krylov matrix associated x give following lemma krylov matrices x lemma 45 let nthetan given 47410 matrix x 11 47 nonsingular let assume chosen kzk proof proof verification formulae first consider first term rows satisfy lemma second term may bounded kp 1 kp 1 induction step simply reccurances 411 412 bound norms simple substitution shows k k1 prove following lemma lemma 46 let z nthetan nthetan lemma 45 let nthetan krylov matrix let k partitioned form k 11 k 12 defined 47 k 11 nonsingular proof first note two lower triangular matrices given 0 0 0 given k 11 k 12 k 22 thus leading bound let k 11 k 12 k 22 reconstruct factorization taking norms yields desired result well known matrix v bidiagonal reduction orthgonal factor krylov matrix k 18 pp472473 allows us find following bound growth factor bidiagonal reduction caveat results theta matrix k 11 nonsingular proposition 47 let v lemma 42 let c 31 let x 47410 let k lemma 46 assume k 11 nonsingular proof proposition 44 since matrix k orthogonal factorization v set lanczos vector bidiagonal reduction c first classical equivalence since jx 22 corollary 48 let c satisfy 46 value let z defined 6 0 let algorithm 32 applied c let u orthogonal transformations generated algorithm define let c k defined let ae v defined 413415 proof since k product givens rotations standard order directly apply results lemma 42 first let since taking advantage rotations commute write thus structure 41 using terminology lemma 42 2and 22 combining results lemma 42 proposition 47 obtains note ability factor 2 feature givens rotation based algorithm bidiagonal reduction 42 error bounds implications error bounds paper stated two theorems first one proven x7 second consequence first theorem 49 let c 2 nthetan let nthetan bidiagonal matrix computed algorithm 32 floating point arithmetic machine contents c k passes thorough main loop algorithm 32 exist u modestly growing functions theorem 410 let c 2 nthetan satisfy 24 let nthetan bidiagonal matrix computed algorithm 32 floating point arithmetic machine unit let c r contents c k passes thorough main loop algorithm 32 let smallest integer let ae v defined 413 exist u satisfying 416 modestly growing function g 5 delta b c satisfy 417 ffib theorem 49 proof proof result simply matter bounding value k k k orthogonal equivalence yields 2 application 421 yields 422 standard error bounds bidiagonal reduction form 417 ffic satisfies bound form golubkahan procedure bound probably good expect lead error bounds singular values form satisfactory large singular values little use small ones following 4 theta 4 example illustrates difference standard bidiagonal reduction approach advocated example 41 let 4 theta 4 matrix small parameters using matlab value chose yields matrix two singular values clustered 1 two distinct singular values smaller ffl digits displayed perform algorithm 31 without preprocessing x2 obtain use bisection routine 4 obtain singular values computed singular vector matrices invariant subspaces double singular value 1 singular values 3 4 correct however individual singular vectors singular values 3 4 wrong algorithm 32 used reduction x2 obtains computed singular values number digits displayed moreover corresponded computed jacobi method 15 significant digits computed singular vector matrices gamma0408248 gamma0408248 gamma0553113 0600611 gamma0408248 gamma0408248 gamma0243588 gamma0779315 gamma0408248 gamma0408248 07967 0178704c c version jacobi method coded pa yoon obtains slightly different singular vector matrices however singular vectors oe 3 oe 4 essentially subspace clustered singular value near 1 also essentially quite recently number papers singular values vectors matrices structured perturbations give two results relevant singular values perturbation given theorem 49 first due kahan 13 lemma 411 let nthetan let 1 lemma 411 says forward errors matrix b make small relative difference singular values following result shows nonorthogonality u v theorem 49 causes small relative change singular values theorem given 24 pp423424problem 18 lemma 412 let 2 mthetan let nthetap p n standard bounds eigenvalue perturbation 18 chapter 8 taking square roots leads errors b nonorthogonality transformations u v make small relative changes singular values important effect error ffic theorems 49 410 characterize effect error ffic 417 use generalization results published barlow demmel 4 demmel veselic 14 gu 20 version proven barlow slapnicar 6 work preparation consider family symmetric matrices associated family eigenproblems 4 let x ith eigenpair 425 define sffi set indices given set sffi set eigenvalues relative error bounds found next theorem gives bound proof follows theorem 4 4 p773 lemma 413 let x ith eigenpair hermitian matrix 424 let sffi defined 426 2 sffi jx jx proof first assume simple point standard eigenvalue perturbation theory sufficiently small x x x x thus jx simple 2 0 ffi bound 429 follows integrating 0 ffi kato25 theorem ii61p139 shown eigenvalues hi real analytic even multiple moreover kato 25 p143 goes point finite number multiple continuous piecewise analytic throughout interval 0 ffi thus obtain 427428 integrating intervals analytic proof following proposition given slapnicar 6 proposition 414 let singular value decomposition nthetan orthogonal proof prove lemma noting oe hi 0 associated eigenvector matrix eh 424 given thus lemma 413 oe satisfies 430 u formula proposition 414 used bound error singular values caused stages bidiagonal reduction stage x2 bounds form 1314 thus 23 value ae growth factor bounded moorepenrose pseudoinverse consider error bound theorem 49 context consider u v ith left right singular vectors c rather preceding paragraph let defined 18 bounded matrix satisfying 24 often small singular values modest values c pointed demmel veselic 14 algorithm columnwise backward error bounds take advantage fact error bounds computed singular vectors c also better bidi agonal reduction satisfying theorem 49 bounds see papers veselic demmel 14 barlow slapnicar 6 5 numerical tests performed two sets numerical tests bidiag onal reduction algorithms tests compared singular values obtained jacobi method described demmel veselic 14 two test sets follows example 51 test set 1 use set cholesky factor hilbert matrix dimension k r k upper triangular matrix positive diagonals satisfied h k k theta k matrix whose j entry example 52 test set 2 set transposes matrices example 51 reduction x2 produces upper triangular matrix thus resulting bidiagonal matrices tended graded example 53 test set 3 computed svd l 35 example 52 using matlab svd routine obtain constructed 50 matrices form f 50theta35 matrix generated matlab function randn generating matrices normally distributed entries matrix test sets reduced matrix c using algorithm x2 three separate routines used find svd c plots bidiagonal reduction svd factor hilbert matrices fromtofig 51 relative error plots example 51 algorithm j jacobi method described 14 ffl algorihtm g bidiagonalization method algorithm 32 followed bisection routine demmel kahan 13 ffl algorihtm h bidiagonalization method algorithm 31 followed bisection routine matrix calculated two ratios 1in joe g oe j 1in oe j oe j ith singular values calculated algorithms j g h respectively thus trying measure well svds calculated two bidiagonal reduction algorithms agreed jacobi algorithm graphs values three test sets given figures 51 52 53 logarithmic plot singular values matrix r 90 example 51 given figure 54 clearly singular values l 90 example 52 span range three examples singular values spanned wide range seen figures maximum relative error singular value either test set algorithm g h 10 gamma13 approximately 10 3 times plots bidiagonal reduction svd factor hilbert matrices fromtofig 52 relative error plots example 52 machine precision matlab seems measurable difference algorithms g h quality singular values produced error analysis produced x4 indicates expect better accuracy algorithm g explain singular values produced algorithms accurate suspect matrix c produced reduction x2 almost always good bidiagonal reduction either algorithm know analysis explain happens algorithm h 6 conclusion presented new bidiagonal reduction algorithm four difference standard algorithm although cost 8 2 8 flops golubkahan algorithm analysis shows new reduction gives better guarantee accurate singular values numerical tests seemed indicate performed coxhigham householder factorization routine preprocessor new algorithm golubkahan routine produced singular values even accurate known theory predicts thus strongly recommend preprocessing step whenever feasible matrix c resulting reduction usually highly graded suspect still much understand behavior bidiagonal redcution graded matrices extra cost new bidiagonal reduction method far less implementation jacobi method date gives reasonable guarantee relative accuracy singular values larger 32 tests confirm behaves well plots bidiagonal reduction svd fortrials small singular values fig 53 relative error plots example 53 singular values smaller 32 r fast plane rotations dynamic scaling numerical solution weighted linear least squares problem g transformations stability analysis galgorithm note application sparse least squares problems computing accurate eigensystems scaled diagonally dominant matrices direct solution weighted equality constrained least squares problems optimal perturbation bounds hermitian eigenvalue prob lem growth factors gaussian elimination linear least squares solutions householder transformations improved algorithm computing singular value decomposition stability householder qr factorization weighted least squares problems computing accurate singular values eigenvalues matrices acyclic graphs computing singular value decomposition high relative accuracy accurate singular values bidiagonal matrices jacobis method accurate qr accurate singular values differential qd algorithms least squares computations givens rotations without square roots calculating singular values pseudoinverse matrix matrix computations singular value decomposition least squares solutions studies numerical linear algebra backward error analysis constrained weighted linear least squares problem using weighted qr factorization note modifications givens plane rotation accuracy stability numerical algorithms matrix analysis short introduction perturbation theory linear operators solving least squares problems fast accurate eigensystem computation jacobi methods fast accurate eigenvalue methods graded positive definite matrices numer analysis algorithms reflectors bisectors applying householders method linear least squares prob lems algebraic eigenvalue problem tr