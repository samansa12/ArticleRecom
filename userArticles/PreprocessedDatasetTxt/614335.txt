interactive timedependent particle tracing using tetrahedral decomposition abstractstreak lines particle traces effective visualization techniques studying unsteady fluid flows realtime applications accuracy often sacrificed achieve interactive frame rates physical space particle tracing algorithms produce accurate results although usually expensive interactive applications efficient physical space algorithm presented paper developed interactive investigation visualization large unsteady aeronautical simulations performance increased applying tetrahedral decomposition speed point location velocity interpolation curvilinear grids preliminary results batch computations 1 showed approach six times faster common algorithm uses newtonraphson method trilinear interpolation results presented show tetrahedral approach also permits interactive computation visualization unsteady particle traces statistics given frame rates computation times single multiprocessors benefits interactive feature detection unsteady flows also demonstrated b introduction unsteady particle tracing relatively new visualization technique emerged need visualize unsteady timedependent data sets steadystate flow simulations require one set grid solution data describe flow unsteady flow simulations may comprise hundreds thousands time steps data time step associated grid solution file size data sets run hundreds gigabytes 23 numerical techniques visualizing unsteady flows mirror used experimental fluid mechanics include path lines time lines streak lines path line generated tracing path single particle also called particle path time line generated tracing line particles released time streak line generated continuously injecting particles fixed location lines also called filament lines popular visualization technique also simplest generate wind tunnels produced injecting smoke flow 4 stream lines curves tangential vector field generally used visualize unsteady flows show actual motion particles fluid rather theoretical trajectories particles infinite velocity steady flows stream lines identical path lines streak lines unsteady flows differ significantly example fig 1 shows stream lines path lines streak lines time lines computed unsteady simulation flow around oscillating airfoil techniques shown 128th time step important flow feature data set vortex shedding caused stalling streak lines time lines revealed vortices well stream lines path lines latter techniques fail capture features show actual motion fluid time fig 1 comparison stream lines path lines streak lines time lines unsteady flow goal study interactive computation visualization streak lines large unsteady computational fluid dynamics cfd simulations computational flow visualization streak lines generated releasing particles discrete intervals usually accordance simulation time steps continuous injection particles leads rapid growth number particles flow must traced leave flow field simulation ends may several thousand active particles unsteady flow essential advection tracing process efficient possible ii previous work many algorithms presented particle tracing steady flows yet relatively consider extension unsteady flows extension trivial time varying nature flow grid adds complexity almost every part algorithm 3 consequently unsteady particle traces usually computed batch process played back upon completion 5678 significant problem approach particle injection points must chosen advance usually excessive number particles released 10000 prevent missing important flow structures better approach interactively position streak line injectors study particle motion real time interactive unsteady particle tracing achieved bryson levit 9 simplifying algorithm data set transformed velocity vector field uniform computational space make numerical integration simpler resampled grid enable entire data set stored memory however many subtle features flow lost result particle tracing computational space recently come scrutiny 10 shown poor accuracy compared physical space schemes iii physical vs computational space tracing bodyfitted curvilinear grids widely used model complex geometries aerospace vehicles cfd flow solvers internally transform curved grids uniform cartesian space usually called computational space make numerical calculations simpler efficient output solution data generally transformed back physical grid space post analysis visualization flow solvers particle tracing algorithms may operate computational physical space computational space schemes require curvilinear grid associated velocity field remapped computational space particles advected mapping done preprocessing step whole grid makes particle tracing efficient 9 main disadvantage tracing computational space transforming jacobian matrices usually approximations transformed vector field may discontinuous also irregularities grid cells collapsed edges transformed velocities may infinite 11 analyses sadarjoen et al 10 hultquist 12 steady flows shown mapping technique produces significant errors distorted curvilinear grids calculating jacobian matrices unsteady flows moving grids inaccurate matrix three additional timedependent terms must approximated 13 remark would preferable accurate computational space data saved directly flow solver however provision plot3d data file format used widely nasa storing fields motivated improvement physical space particle tracing physical space tracing schemes preferred interpolation integration processes done curvilinear grid eliminates need evaluate jacobian matrices transform velocity field disadvantage task locating particles complicated hence expensive problem addressed paper explicit point location technique presented yielded significant speedup common point location technique newtonraphson iterative method iv physical space tracing algorithm physical space algorithms proceed first searching element cell bounds given point termed cell search point location process found velocity evaluated point interpolating nodal velocities unsteady flows velocity components usually need interpolated temporally well spatially necessitates loading two time steps data memory intermediate positions grid may also interpolated grid changes time particles path determined solving differential equation field line dt r particles location v particles velocity time integrating 1 yields r r v r 2 integral term right hand side evaluated numerically using multistage method eg rungekutta bulirschstoer multistep method eg backwards differentiation adamsbashforth issues concerning accuracy stability methods discussed darmofal haimes 14 regardless solved end result displacement added current position rt gives new particle location time td essential steps timedependent particle tracing algorithm follows 1 specify injection point particle physical space xyzt 2 perform point location locate cell contains point 3 evaluate cells velocities coordinates time interpolating simulation time steps 4 interpolate velocity field determine velocity vector current position xyz 5 integrate local velocity field using equation 2 determine particles new location time td 6 estimate integration error reduce step size repeat integration error large 7 repeat step 2 particle leaves flow field exceeds last simulation time step important note step 5 may involve repeated applications steps 2 3 4 depending numerical integration scheme used 4th order rungekutta scheme used study actually requires three repetitions advance time tdt refer section ive details point location tetrahedral cells core problem particle tracers given arbitrary point x physical space cell point lie natural coordinates natural coordinates also called barycentric computational coordinates local nondimensional coordinates cell strictly computational coordinates different globally defined widely used trilinear interpolation function provides opposite mapping required point location determines coordinates x given natural coordinate xhz unfortunately cannot inverted easily nonlinear products usually solved numerically using newtonraphson method using point location technique based tetrahedral elements natural coordinates evaluated directly physical coordinates tetrahedral elements permit use linear interpolation function map natural physical coordinates note x 1 x 2 x 3 x 4 physical coordinates vertices tetrahedron see fig 2 natural coordinates xhz vary per usual 0 1 nondimensional cell x z x z fig 2 tetrahedron geometry natural nondimensional physical coordinate spaces equation 3 inverted analytically nonlinear terms solution natural coordinates given physical point x z z z constants 3x3 matrix determinant v actually 6 times volume tetrahedron given natural coordinates xhz evaluated 104 floating point operations implementing equations figure halved precomputing common terms evaluating matrix coefficients determinant b tetrahedral decomposition hexahedral cells curvilinear grids must decomposed tetrahedra order use equation 4 since data sets timedependent flows usually extremely large impractical decomposition preprocessing step must performed fly particles enter cells hexahedral cell divided minimum five tetrahedra fig 3 decomposition unique diagonal edges alternate across cell since faces hexahedron usually nonplanar important ensure adjoining cells matching diagonals prevent gaps achieved alternating odd even decomposition illustrated fig 3 curvilinear grid correct configuration selected simply adding integer indices specific node node lowest indices used practice choosing odd configuration sum odd even configuration sum even guarantees continuity cells remark odd even configuration may switched impact point location affect velocity interpolation different vertices used practice cfd grids fine resolution differences slight detected regions high velocity gradients fig 3 subdivision alternates two configurations ensure continuity cells c cellsearch scheme equation 4 allows natural coordinates evaluated directly physical coordinates relatively little effort four conditions must valid point lie within tetrahedron one invalid point outside tetrahedron particle tracing algorithms happens particles cross cell boundaries problem arises tetrahedron advance next solution quite simple since natural coordinates tell direction move example x0 particle would crossed x0 face similarly h0 z0 particle would crossed h0 z0 face respectively fourth condition violated ie 1xhz0 particle would crossed diagonal face cellsearch proceeds advancing across respective face adjoining tetrahedron lookup tables needed identify tetrahedron given appendix occasionally two conditions 5 may violated particle crosses near corner cell traverses several cells cases worst violator four conditions used predict next tetrahedron even bounding tetrahedron immediate neighbour always moving direction worst violator search rapidly converge upon correct tetrahedron tests numerous grids including complex cgrids singular points never turned case technique failed locate bounding tetrahedron cell search procedure described used cell sought nearby within cells previous one usually case particle tracing since majority particles cross one cell time two situations cell sought likely nearby start particle trace ii jumping grids multizone data sets circumstances cell search preceded another scheme order prevent weaving across large grid one tetrahedron time use boundary search technique described buning 15 velocity interpolation tetrahedron unsteady flows velocity field changes time well space since velocity fields solved discrete time steps discrete locations grid must interpolated time space present algorithm interpolations handled separately d1 temporal interpolation temporal interpolation performed first using linear function applied two closest time steps example given time lies time steps l l1 velocity u arbitrary grid node ijk given time fraction tt l l1 l note equation 6 evaluates velocity given node spatial interpolation performed stage note also grid moves time temporal interpolation grid positions also required used similar linear interpolation function purpose temporal grid interpolation must precede point location although temporal velocity interpolation however convenient perform since time fraction interpolants temporal interpolations applied locally single tetrahedron current one used point location velocity interpolation procedures d2 spatial interpolation one three techniques may used spatial interpolation velocity physical space linear interpolation 16 volume weighted interpolation 15 linear basis function interpolation 17 three mathematically equivalent 118 produce identical interpolation functions authors showed previously 1 linear basis function efficient technique application reused natural coordinates computed point location using numbering convention fig 2 linear basis function spatial velocity interpolation x h z natural coordinates computed equation 4 u 1 u 2 u 3 u 4 velocity vectors vertices e numerical integration scheme numerical integration equation 2 performed 4th order rungekutta scheme timedependent flow moving grid geometry takes form r r r particle position v velocity vector position dt time step four stages rungekutta scheme span three time values tdt2 tdt therefore require new grid velocity data one fortunately need interpolated tetrahedron surrounds particle however particle moves time space integration may lie different tetrahedra four intermediate stages equation 8 point location velocity interpolation must therefore performed every stage remark numerical integration schemes require fewer velocity evaluations rungekutta scheme eg bulirschstoer method 19 may substituted improve performance unsteady particle tracing algorithm f step size adaptation integration step size fixed constant value along entire particle path regulated achieve specified number steps per cell particle may understeer around bends flow changes direction rapidly prevented using adaptive step size control scheme integration step size changed according error tolerance error tolerance computed using standard numerical technique step doubling 19 whereby particle advanced forward given point using step size dt process repeated point using two half steps size dt2 step size reduced distance endpoints greater specified tolerance number usually deduced trial error technique implemented tested found expensive interactive particle tracing performed large number point locations heuristic technique adapting step size suggested darmofal haimes 20 measured angle velocity vectors successive points along particle path estimate change velocity direction implemented scheme similar one adapted step size according angle successive line segments path line fig 4 particle path r r r cos fig 4 step size adaptation based change path line direction schemes worked well practice ran approximately three times faster step doubling algorithm cases initial step size estimated using u magnitude velocity current position v determinant equation 4 ensures initial particle displacement commensurate length scale tetrahedron following initial estimate step size halved angle q n large doubled small q n 3 kept limits 3 q n 15 experimentation several data sets found scheme produced particle traces accuracy step doubling scheme based error tolerance 5 latter adaptation angles 15 degrees upper limit 3 degrees lower limit used v interactive performance evaluation previous paper authors 1 performance accuracy tetrahedral method compared conventional particle tracing algorithm batch computations streak lines conventional algorithm originally developed buning 15 steady flows extended unsteady flows lane 8 used iterative newtonraphson method point location trilinear functions grid velocity interpolations execution profiles confirmed velocity interpolation point location computationally expensive tasks conventional algorithm tetrahedral method computation times improved factor six still maintaining accuracy streak lines tetrahedral method since implemented c virtual windtunnel 9 interactive visualization system studying unsteady flows results presented demonstrate interactive performance tetrahedral method system interactive tests performed sgi onyx four 75mhz r8000 processors five gigabytes ram large memory capacity enables moderately sized unsteady flow simulations loaded physical memory thus avoiding disk latencies accessing data tapered cylinder data set 7 used interactive tests see fig 5 100 simulation time steps 131k grid points grid dimensions 64 x 64 x 32 time step consisted approximately 15 megabytes velocity data fig 5 tapered cylinder data set used interactive performance tests ten streak lines shown could computed rendered 33 frames per second find important flow features unsteady flow large number particles must injected flow time respect objective determine many streak lines could generated interactively tetrahedral method fig 6 shows aggregate time taken compute render streak lines function number streak lines 100 time steps computations performed runtime use precomputed fields fig 6 shows linear relationship number streak lines computation time also shows near linear speedup computations distributed two three processors linear trends occur streak lines comprised discrete particles advected independently running multi processors virtual windtunnel creates p1 light weight processes p number processors using sgi parallel programming primitives mfork one processor always reserved virtual windtunnel operating system graphics tasks 1001030number streak lines aggregate time compute renderframes 3 processors processors sec fig 6 time taken compute render 100 frames function number streak lines timings given one two three processors performance expressed terms frame rate fig 7 one frame corresponds one simulation time step frame rate 10 frames per second accepted baseline interactive visualization 9 results fig 7 show 15 streak lines could computed rendered rate one processor likewise 32 streak lines could computed rendered two processors 44 streak lines three processors 10 framessec important note number particles streak line increase linearly time earlier frames much quicker compute later ones frame rates shown averaged steps higher frame rates could obtained reducing number time steps processors processors processor frames per second number streak lines interactive threshold fig 7 frame rates computing rendering streak lines one two three processors another interesting metric particle advection rate total number particle advections lines n time steps given number advections dividing time taken compute streak lines gives particle advection rate interactive threshold see fig 7 advection rate 8k particlessec one processor 14k particlessec two processors 22k particlessec three processors vi interactive streak lines feature detection illustrated fig 1 particle paths individual particles give misleading results sometimes fail detect important flow features injecting large numbers particles likely capture features although actually seeing difficult particles clump together clouds lot mixing recirculation flow found useful interactively probe flow using rake 10 40 injectors however users warned problem interactive streak lines streak lines unlike stream lines take time propagate user move rake injectors quickly best periodically stop let streak lines cycle time steps moving another location benefits near realtime performance interactive rake adjustments demonstrated fig 8 example user wanted visualize von karman vortex shedding wake tapered cylinder general area interest first identified using wide rake injectors rake shortened used probe region flow reversal desired location found number injectors increased make streak lines coherent highlight vortex shedding particle colour depicts velocity magnitude fig 8 flow features easily detected unsteady flow interactively moving streak line injectors interactive streak lines also used visualize flow delta wing high angle attack shown fig 9 data set 100 simulation time steps singlezone grid like tapered cylinder nine times larger velocity field time step nearly 14 megabytes entire data set nearly 14 gigabytes although data set significantly larger test case performance particle tracing degraded particle advection requires local cell searches interpolations previous results 1 shown performance penalty multizone grids global searches required particles move new grids fig 9 local global flow structures visualized interactively changing number streak lines length rake unsteady flow around delta wing vii future directions tapered cylinder delta wing data sets used study small compared multizone data sets currently computed nasa ames research center small number time steps large multizone data sets fit physical memory even largest workstations two approaches investigated allow scientists visualize data first simply subsetting grid reduce size localize region interest second using large disk array high bandwidth disk array 96 disks 200 gigabytes storage capacity currently evaluated application data transfer rates 300 megabytes per second measured potentially enable us read solution files megabytes data per time step interactive frame rates viii conclusions tetrahedral decomposition made point location velocity interpolation tasks efficient particle tracing algorithm timedependent flows streak lines computed rendered interactive frame rates sgi onyx workstation data set 100 time steps almost linear scaling performance measured one two three processors used 44 streak lines could computed visualized 10 frames per second calculations performed three processors permitted interactive positioning adjustment streak line injectors aided vortex detection unsteady flow node numbering convention different node numbering conventions used odd even tetrahedral decompositions simplify lookup tables used point location appendix ic2834 x z a752x z fig 10 odd b even numbering conventions b tetrahedral decomposition following diagrams illustrate hexahedral cell decomposed 5 tetrahedra note tetrahedra drawn natural coordinate system refer fig 2 details x x z x z x z x z tetrahedron 1 tetrahedron 2 tetrahedron 3 tetrahedron 4 tetrahedron 5 x x z x z x z x z tetrahedron 1 tetrahedron 2 tetrahedron 3 tetrahedron 4 tetrahedron 5 b fig 11 odd b even tetrahedral decompositions c point location lookup tables two lookup tables used locate tetrahedron bounds particle first table predicts tetrahedron particle move exits current tetrahedron particular face exit face determined four conditional tests based natural coordinates xhz refer section ivc details note table applies odd even tetrahedral decompositions conditional test tetrahedron 1 tetrahedron 2 tetrahedron 3 tetrahedron 4 tetrahedron 5 z 0 4 1 2 3 1 fig 12 adjacent tetrahedron lookup table second table used update cell indices ijk particle moves different hexahedral cell cell divided five tetrahedra particles may cross several tetrahedra actually leave cell previous table number current tetrahedron exit face needed identify appropriate row column entry cell table indicates neighbouring tetrahedron hexahedral cell conditional test tetrahedron 1 tetrahedron 2 tetrahedron 3 tetrahedron 4 tetrahedron 5 z 1xhz 0 cell cell cell cell cell conditional test tetrahedron 1 tetrahedron 2 tetrahedron 3 tetrahedron 4 tetrahedron 5 z 1xhz 0 cell cell cell cell cell b fig 13 cell index lookup tables odd b even tetrahedral decompositions acknowledgements grateful steve bryson helping us install tetrahedral method virtual windtunnel also grateful following people allowing us use data sets sungho ko oscillating airfoil dennis jespersen creon levit tapered cylinder neal chaderjian delta wing work supported nasa contract nas 212961 r optimization timedependent particle tracing using tetrahedral decomposition software model visualization large unsteady 3d cfd results scientific visualization large scale unsteady fluid flow analysis visualization complex unsteady threedimensional flows numerical simulation streaklines unsteady flows numerical simulation flow past tapered cylinder visualization timedependent flow fields virtual windtunnel environment exploration threedimensional unsteady flows particle tracing algorithms 3d curvilinear grids sources error graphical analysis cfd results interactive numerical efficient solution methods navierstokes equations analysis 3d particle path integration algorithms numerical algorithms cfd postprocessing finite element analysis fluid dynamics finite element method displayed finite element method engineering numerical recipes visualization 3d vector fields variations stream tr ctr guy albertelli roger crawfis efficient subdivision finiteelement datasets consistent tetrahedra proceedings 8th conference visualization 97 p213219 october 1824 1997 phoenix arizona united states ralph bruckschen falko kuester bernd hamann kenneth joy realtime outofcore visualization particle traces proceedings ieee 2001 symposium parallel largedata visualization graphics october 2223 2001 san diego california falko kuester ralph bruckschen bernd hamann kenneth joy visualization particle traces virtual environments proceedings acm symposium virtual reality software technology november 1517 2001 baniff alberta canada dale lawrence christopher lee lucy pao roman novoselov shock vortex visualization using combined visualhaptic interface proceedings conference visualization 00 p131137 october 2000 salt lake city utah united states jens kruger peter kipfer polina kondratieva rudiger westermann particle system interactive visualization 3d flows ieee transactions visualization computer graphics v11 n6 p744756 november 2005 john g hagedorn steven g satterfield john kelso whitney austin judith e terrill adele p peskin correction location orientation errors electromagnetic motion tracking presence teleoperators virtual environments v16 n4 p352366 august 2007 dale lawrence lucy pao christopher lee roman novoselov synergistic visualhaptic rendering modes scientific visualization ieee computer graphics applications v24 n6 p2230 november 2004 j schroeder berk geveci mathieu malaterre compatible triangulations spatial decompositions proceedings conference visualization 04 p211218 october 1015 2004 shyhkuang ueng christopher sikorski kwanliu outofcore streamline visualization large unstructured meshes ieee transactions visualization computer graphics v3 n4 p370380 october 1997