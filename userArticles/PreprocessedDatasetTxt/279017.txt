hierarchical latent variable model data visualization abstractvisualization proven powerful widelyapplicable tool analysis interpretation multivariate data visualization algorithms aim find projection data space twodimensional visualization space however complex data sets living highdimensional space unlikely single twodimensional projection reveal interesting structure therefore introduce hierarchical visualization algorithm allows complete data set visualized top level clusters subclusters data points visualized deeper levels algorithm based hierarchical mixture latent variable models whose parameters estimated using expectationmaximization algorithm demonstrate principle approach toy data set apply algorithm visualization synthetic data set 12 dimensions obtained simulation multiphase flows oil pipelines data 36 dimensions derived satellite images matlab software implementation algorithm publicly available world wide web b introduction many algorithms data visualization proposed neural computing statistics communities based projection data onto twodimensional visualization space algorithms usefully display structure simple data sets often prove inadequate face data sets complex single twodimensional projection even nonlinear may insufficient capture interesting aspects data set example projection best separates two clusters may best revealing internal structure within one clusters motivates consideration hierarchical model involving multiple twodimensional visualization spaces goal toplevel projection display entire data set perhaps revealing presence clusters lowerlevel projections display internal structure within individual clusters presence subclusters might apparent higherlevel projections allow possibility many complementary visualization projections consider projection model relatively simple example based linear projection compensate lack flexibility individual models overall flexibility complete hierarchy use hierarchy relatively simple models offers greater ease interpretation well benefits analytical computational simplification philosophy modelling complexity similar spirit mixture experts approach solving regression problems 1 algorithm discussed paper based form latent variable model closely related principal component analysis pca factor analysis top level hierarchy single visualization plot corresponding one model considering probabilistic mixture latent variable models obtain soft partitioning data set clusters corresponding second level hierarchy subsequent levels obtained using nested mixture representations provide successively refined models data set construction hierarchical tree proceeds top driven interactively user stage algorithm relevant model parameters determined using expectationmaximization em algorithm next section review latentvariable model section 3 discuss extension mixtures models extended hierarchical mixtures section 4 used formulate interactive visualization algorithm section 5 illustrate operation algorithm section 6 using simple toy data set apply algorithm problem involving monitoring multiphase flows along oil pipes section 7 interpretation satellite image data section 8 finally extensions algorithm relationships approaches discussed section 9 latent variables begin introducing simple form linear latent variable model discuss application data analysis give overview key concepts leave detailed mathematical discussion appendix aim find representation multidimensional data set terms two latent hidden variables suppose data space ddimensional coordinates data set consists set ddimensional vectors ft n g consider twodimensional latent space together linear function maps latent space data space w theta 2 matrix ddimensional vector mapping 1 defines twodimensional planar surface data space introduce prior probability distribution px latent space given zeromean gaussian unit covariance matrix 1 defines singular gaussian distribution data space mean covariance matrix hy finally since expect data confined exactly twodimensional sheet convolve distribution isotropic gaussian distribution ptjx oe 2 data space mean zero covariance oe 2 unit matrix using rules probability final density model obtained convolution noise model prior distribution latent space form z since represents convolution two gaussians integral evaluated analytically resulting distribution pt corresponds ddimensional gaussian mean covariance matrix ww considered general model conditional distribution ptjx given gaussian general covariance matrix independent parameters would obtain standard linear factor analysis 2 3 fact model closely related principal component analysis discuss log likelihood function model given likelihood used fit model data hence determine values parameters w oe 2 solution given sample mean case factor analysis model determination w oe 2 corresponds nonlinear optimization must performed iteratively isotropic noise covariance matrix however shown tipping bishop 4 5 exact closed form solution follows introduce sample covariance matrix given nonzero stationary points likelihood occur two columns matrix u eigenvectors corresponding eigenvalues diagonal matrix r arbitrary 2 theta 2 orthogonal rotation matrix furthermore shown stationary point corresponding global maximum likelihood occurs columns u comprise two principal eigenvectors ie eigenvectors corresponding two largest eigenvalues combinations eigenvectors represent saddlepoints likelihood surface also shown maximumlikelihood estimator oe 2 given clear interpretation variance lost projection averaged lost dimensions unlike conventional pca however model defines probability density data space important subsequent hierarchical development model choice radially symmetric rather general diagonal covariance matrix ptjx motivated desire greater ease interpretability visualization results since projections data points onto latent plane data space correspond small values oe 2 orthogonal projection discussed appendix although explicit solution maximumlikelihood parameter values shown tipping bishop 4 5 significant computational savings sometimes achieved using following em expectationmaximization algorithm 6 7 8 using 2 write log likelihood function form z regard quantities x n missing variables posterior distribution x n given observed n model parameters obtained using bayes theorem consists gaussian distribution estep involves use old parameter values evaluate sufficient statistics distribution form 2 theta 2 matrix h denotes expectation computed respect posterior distribution x mstep maximizes expectation completedata log likelihood give f e nd whx e denotes new quantities note new value f w obtained 9 used evaluation oe 2 10 model trained alternately evaluating sufficient statistics latentspace posterior distribution using 7 8 given oe 2 w estep reevaluating oe 2 w using 9 10 given hx n hx n x mstep shown stage em algorithm likelihood increased unless already local maximum demonstrated appendix n data points dimensions evaluation sample covariance matrix requires approach finding principal eigenvectors based explicit evaluation covariance matrix must least order computational complexity contrast em algorithm involves steps ond saving computational cost consequence latent space whose dimensionality purposes visualization algorithm fixed 2 scale substitute expressions expectations given estep equations 7 8 mstep equations obtain following reestimation formulae f e shows dependence data occurs sample covariance matrix thus em algorithm expressed alternate evaluations 11 12 note 12 involves combination old new quantities form em algorithm introduced illustrative purposes would involve cost due evaluation covariance matrix seen data point n induces gaussian posterior px n jt n distribution latent space purposes visualization however convenient summarize distribution mean given hx n illustrated figure 1 note x prior posterior figure 1 illustration projection data point onto mean posterior distribution latent space quantities obtained directly output estep 7 thus set data points projected onto corresponding set points fhx n ig 2dimensional latent space 3 mixtures latent variable models perform automatic soft clustering data set time obtain multiple visualization plots corresponding clusters modelling data mixture latent variable models kind described section 2 corresponding density model takes form 0 number components mixture parameters mixing coefficients prior probabilities corresponding mixture components ptji component independent latent variable model parameters w oe 2 mixture distribution form second level hierarchical model em algorithm extended allow mixture form 13 fitted data see appendix b details derive em algorithm note addition fx n g missing data also includes labels specify component responsible data point convenient denote missing data set variables z ni z generated model zero otherwise prior expectations variables given corresponding posterior probabilities responsibilities evaluated extended estep using bayes theorem form although standard em algorithm derived treating fx n g z ni jointly missing data efficient algorithm obtained considering twostage form em complete cycle algorithm commence old set parameter values w oe 2 first use parameters evaluate posterior probabilities r ni using 14 posterior probabilities used obtain new values e using following reestimation formulae e r ni 15 e new values e used evaluation sufficient statistics posterior distribution x ni finally statistics used evaluate new values f e oe 2 using f r ni hx ni x e r ni kt gamma2 r ni hx r ni f derived appendix b single latent variable model substitute expressions hx ni ni given 17 18 respectively 19 20 see reestimation formulae f take form f f data dependence expressed terms quantities defined r ni matrix clearly interpreted responsibilityweighted covariance matrix reasons computational efficiency form em algorithm given 17 20 preferred large hierarchical mixture models extend mixture representation section 3 give hierarchical mixture model formulation quite general applied mixtures parametric density model far considered twolevel system consisting single latent variable model top level mixture 0 models second level extend hierarchy third level associating group g latent variable models model second level corresponding probability density written ptji latent variable models jji correspond sets mixing coefficients one satisfy 1 thus level hierarchy corresponds generative model lower levels giving refined detailed representations model illustrated figure 2 determination parameters models third level viewed missing data problem missing information corresponds labels specifying model generated data point information labels provided copy second level mixture model en mode third level mixture model figure 2 structure hierarchical model log likelihood model 24 would take form however given set indicator variables z ni specifying model second level generated data point n log likelihood would become z ni ln fact partial probabilistic information form posterior responsibilities r ni model generated data points n obtained second level hierarchy taking expectation 26 obtain log likelihood third level hierarchy form r ni r ni constants particular case r ni 0 1 corresponding complete certainty model second level responsible data point log likelihood 27 reduces form 26 maximization 27 performed using em algorithm discussed appendix c form em algorithm simple mixture discussed section 3 except estep posterior probability model generated data point n given note r ni constants determined second level hierarchy r njji functions old parameter values em algorithm expression 29 automatically satisfies relation responsibility model second level given data point n shared partition unity corresponding group offspring models third level corresponding em algorithm derived straightforward extension discussion given section 3 appendix b outlined appendix c shows mstep equations mixing coefficients means given e e posterior expectations missing variables z nij given finally w ij oe 2 ij updated using mstep equations f r nij hx nij x e r nij kt gamma2 r nij hx r nij f substitute estep equations mstep equations obtain set update formulae form f e f summations n expressed terms quantities defined ij interpreted responsibility weighted covariance matrices straightforward extend hierarchical modelling technique desired number levels parametric family component distributions 5 visualization algorithm far described theory behind hierarchical mixtures latent variable models illustrated overall form visualization hierarchy figure 2 complete description algorithm considering construction hierarchy application data visualization although tree structure hierarchy predefined interesting possi bility greater practical applicability build tree interactively multilevel visualization algorithm begins fitting single latent variable model data set value given sample mean low values data space dimensionality find w oe 2 directly evaluating covariance matrix applying 4 5 however larger values may computationally efficient apply em algorithm scheme initializing w oe 2 given appendix em algorithm converged visualization plot generated plotting data point n corresponding posterior mean hx n latent space basis plot user decides suitable number models fit next level selects points x plot corresponding example centres apparent clusters resulting points data space obtained 1 used initialize means respective submodels initialize remaining parameters mixture model first assign data points nearest mean vector either compute corresponding sample covariance matrices apply direct eigenvector decomposition use initialization scheme appendix followed em algorithm determined parameters mixture model second level obtain corresponding set visualization plots posterior means hx ni used plot data points useful plot data points every plot modify density ink proportion responsibility plot particular data point thus one particular component takes responsibility particular point point effectively visible corresponding plot projection data point onto latent spaces mixture two latent variable models illustrated schematically figure 3 resulting visualization plots used select submodels desired figure 3 illustration projection data point onto latent spaces mixture two latent variable models responsibility weighting 28 incorporated stage decided partition particular model level easily seen 30 result training equivalent copying model unchanged next level equation 30 ensures combination copied models generated submodelling defines consistent probability model represented lower three models figure 2 initialization model parameters direct analogy secondlevel scheme covariance matrices also involving responsibilities r ni weighting coefficients 23 data point principle plotted every model given level density ink proportional corresponding posterior probability given example 28 case third level hierarchy deeper levels hierarchy involve greater numbers parameters therefore important avoid overfitting ensure parameter values welldetermined data consider principal component analysis see three non data points sufficient ensure covariance matrix rank two hence first two principal components defined irrespective dimensionality data set case latent variable model four data points sufficient determine w oe 2 see need excessive numbers data points leaf tree dimensionality space largely irrelevant finally often also useful able visualize spatial relationship group models one level parent previous level done considering orthogonal projection latent plane data space onto corresponding plane parent model illustrated figure 4 model hierarchy except lowest level plot projections associated models level next section illustrate operation algorithm applied simple toy data set presenting results study realistic data sections 7 8 figure 4 illustration projection one latent planes onto parent plane 6 illustration using toy data first consider toy data set consisting 450 data points generated mixture three gaussians threedimensional space gaussian relatively flat small variance one dimension covariance differ means two pancakelike clusters closely spaced third well separated first two structure data set chosen order illustrate interactive construction hierarchical model visualize data first generate single toplevel latent variable model plot posterior mean data point latent space plot shown top figure 5 clearly suggests presence two distinct clusters within data user selects two initial cluster centres within plot initialize second level leads mixture two latent variable models latent spaces plotted second level figure 5 two plots right shows evidence structure submodel generated based mixture two latent variable models illustrates indeed two distinct clusters third step data exploration hierarchical nature approach evident latter two models attempt account data points already modelled immediate ancestor indeed group offspring models may combined siblings parent still define consistent density model illustrated figure 5 one second level plots copied shown dotted line combined thirdlevel models offspring plots generated parent extent offspring latent space ie axis limits shown plot indicated projected rectangle within parent space using approach illustrated figure 4 rectangles numbered sequentially leftmost submodel 1 order display relative orientations latent planes number plotted side rectangle corresponds top corresponding offspring plot original three clusters individually coloured seen red yellow blue data points almost perfectly separated third level figure 5 summary final results toy data set data point plotted every model given level density ink proportional posterior probability model given data point 7 oil flow data example complex problem consider data set arising noninvasive monitoring system used determine quantity oil multiphase pipeline containing mixture oil water gas 9 diagnostic data collected set three horizontal three vertical beamlines along gamma rays two different energies passed measuring degree attenuation gammas fractional path length oil water hence gas readily determined giving 12 diagnostic measurements total practice aim solve inverse problem determining fraction oil pipe complexity problem arises possibility multiphase mixture adopting one number different geometrical configurations goal visualize structure data original 12dimensional space data set consisting 1000 points obtained synthetically simulating physical processes pipe including presence noise dominated photon statistics locally data expected intrinsic dimensionality 2 corresponding two degrees freedom given fraction oil fraction water fraction gas redundant however presence different flow configurations well geometrical interaction phase boundaries beam paths leads numerous distinct clusters would appear hierarchical approach kind discussed capable discovering structure results fitting oil flow data using 3level hierarchical model shown figure 6 homogeneous annular laminar figure results fitting oil data colours denote different multiphase flow configurations corresponding homogeneous red annular blue laminar yellow case toy data discussed section 6 optimal choice clusters subclusters relatively unambiguous single application algorithm sufficient reveal interesting structure within data complex data sets appropriate adopt exploratory perspective investigate alternative hierarchies selection differing numbers clusters respective locations example shown figure 6 clearly highly successful note apparently single cluster number 2 top level plot revealed two quite distinct clusters second level data points homogeneous configuration isolated seen lie twodimensional triangular structure third level 8 satellite image data final example consider visualization data set obtained remotesensing satellite images data point represents 3x3 pixel region satellite land image pixel four measurements intensity taken different wavelengths approximately red green visible spectrum two near infrared gives total 36 variables data point also label indicating type land represented central pixel data set previously subject classification study within statlog project 10 applied hierarchical visualization algorithm 600 data points 100 drawn random six classes 4435point data set result fitting 3level hierarchy shown figure 7 note class labels used colour data red soil cotton crop grey soil damp grey soil soil vegetation stubble damp grey soil figure 7 results fitting satellite image data points play role maximum likelihood determination model parameters figure 7 illustrates data approximately separated classes damp grey soil continuum clearly evident component 3 second level one particularly interesting additional feature appear two distinct separated clusters cotton crop pixels mixtures 1 2 second level evident single toplevel projection study original image 10 indeed indicates two separate areas cotton crop 9 discussion presented novel approach data visualization statistically principled illustrated real examples effective revealing structure within data hierarchical summaries figures 5 6 7 relatively simple terpret yet still convey considerable structural information important emphasize data visualization objective measure quality difficult quantify merit particular data visualization tech nique one reason doubt multitude visualization algorithms associated software available effectiveness many techniques often highly datadependent would expect hierarchical visualization model useful tool visualization exploratory analysis data many applications relation previous work concept subsetting isolating data points investigation traced back maltson dammann 11 developed friedman tukey 12 exploratory data analysis conjunction projection pursuit subsetting operations also possible current dynamic visualization software xgobi 13 however approaches two limitations first partitioning data performed hard fashion mixture latent variable models approach discussed paper permits soft partitioning data points effectively belong one cluster given level second mechanism partitioning data prone suboptimality clusters must fixed user based single twodimensional projection hierarchical approach advocated paper user selects first guess cluster centres mixture model em algorithm utilized determine parameters maximize likelihood model thus allowing centres widths clusters adapt data full multidimensional data space also similarity method earlier hierarchical methods script recognition 14 motion planning 15 incorporate kohonen selforganizing feature map 16 offer potential visualization well performing hard clustering key distinction approaches different levels hierarchies operate different subsets input variables operation thus quite different hierarchical algorithm described paper model based hierarchical combination linear latent variable models related latent variable technique called generative topographic mapping gtm 17 uses nonlinear transformation latent space data space optimized using em algorithm straightforward incorporate gtm place linear latent variable models current hierarchical framework described model applies continuous data variables easily extend model handle discrete data well combinations discrete continuous vari ables case set binary data variables k 2 f0 1g express conditional distribution binary variable given x using binomial distribution form logistic sigmoid function w k k th column w data 1ofd coding scheme represent distribution data variables using multinomial distribution form defined softmax normalized exponential transformation form data set consisting combination continuous binary categorical variables formulate appropriate model writing conditional distribution ptjx product gaussian binomial multinomial distributions appropriate estep em algorithm becomes complex since marginalization latent variables needed normalize posterior distribution latent space general analytically intractable one approach approximate integration using finite sample points drawn prior 17 similarly mstep complex although tackled efficiently using iterative reweighted least squares one important consideration present model parameters determined maximum likelihood criterion need always lead interesting visualization plots currently investigating alternative models optimize criteria separation clusters possible refinements include algorithms allow selfconsistent fitting whole tree lower levels opportunity influence parameters higher levels userdriven nature current algorithm highly appropriate visualization context development automated procedure generating hierarchy would clearly also interest software implementation probabilistic hierarchical visualization algorithm matlab available httpwwwncrgastonacukphivis acknowledgements work supported epsrc grant grk51808 neural networks visualization highdimensional data grateful michael jordan useful discussions would like thank isaac newton institute cambridge hostpitality r hierarchical mixtures experts em algo rithm introduction latent variable models multivariate analysis part 2 classification mixtures principal component analysers mixtures probabilistic principal component analysers maximum likelihood incomplete data via em algorithm em algorithms ml factor analysis neural networks pattern recognition analysis multiphase flows using dualenergy gamma densitometry neural networks neural statistical classification technique determining coding subclasses pattern recognition problems projection pursuit algorithm exploratory data analysis interactive highdimensional data visualiza tion script recognition hierarchical feature maps learning fine motion using hierarchical extended kohonen map gtm generative topographic mapping chapman hall tr ctr peter tino ian nabney hierarchical gtm constructing localized nonlinear projection manifolds principled way ieee transactions pattern analysis machine intelligence v24 n5 p639656 may 2002 michalis k titsias aristidis likas mixture experts classification using hierarchical mixture model neural computation v14 n9 p22212244 september 2002 tienlung sun wenlin kuo visual exploration production data using small multiples design nonuniform color mapping computers industrial engineering v43 n4 p751764 september 2002 neil lawrence andrew j moore hierarchical gaussian process latent variable models proceedings 24th international conference machine learning p481488 june 2024 2007 corvalis oregon alexei vinokourov mark girolami probabilistic framework hierarchic organisation classification document collections journal intelligent information systems v18 n23 p153172 marchmay 2002 daniel boley principal direction divisive partitioning data mining knowledge discovery v2 n4 p325344 december 1998 hiroshi mamitsuka essential latent knowledge proteinprotein interactions analysis unsupervised learning approach ieeeacm transactions computational biology bioinformatics tcbb v2 n2 p119130 april 2005 ting su jennifer g dy automated hierarchical mixtures probabilistic principal component analyzers proceedings twentyfirst international conference machine learning p98 july 0408 2004 banff alberta canada ian nabney yi sun peter tino ata kaban semisupervised learning hierarchical latent trait models data visualization ieee transactions knowledge data engineering v17 n3 p384400 march 2005 michael e tipping christopher bishop mixtures probabilistic principal component analyzers neural computation v11 n2 p443482 feb 15 1999 unsolved information visualization problems ieee computer graphics applications v25 n4 p1216 july 2005 kuiyu chang j ghosh unified model probabilistic principal surfaces ieee transactions pattern analysis machine intelligence v23 n1 p2241 january 2001 wang yue wang jianping lu sunyuan kung junying zhang richard lee jianhua xuan javed khan robert clarke discriminatory mining gene expression microarray data journal vlsi signal processing systems v35 n3 p255272 november pradeep kumar shetty r srikanth ramu modeling online recognition pd signal buried excessive noise signal processing v84 n12 p23892401 december 2004