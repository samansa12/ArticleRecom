outofcore streamline visualization large unstructured meshes abstractthis paper presents outofcore approach interactive streamline construction large unstructured tetrahedral meshes containing millions elements outofcore algorithm uses octree partition restructure raw data subsets stored disk files fast data retrieval memory management policy tailored streamline calculations used streamline construction small amount data brought main memory demand carefully scheduling computation data fetching overhead reading data disk significantly reduced good memory performance results outofcore algorithm makes possible interactive streamline visualization large unstructuredgrid data sets single midrange workstation relatively low mainmemory capacity 515 megabytes also demonstrate approach much efficient relying virtual memory operating systems paging algorithms b introduction visualization software tools designed data fit main memory single workstation many scientific applications data desirable accuracy overwhelm memory capacity scientists desktop workstation particularly true data obtained three dimensional aerodynamics calculations fine unstructured tetrahedral meshes needed model arbitrarily complex configurations airplane although adaptive meshing techniques applied reduce resolution meshes resulting meshes may contain tens millions tetrahedral cells rapidly increasing cpu performance memory capacity beginning allow scientist study data resolutions many scientists access workstations 500 megabytes one gigabyte memory capable visualizing millions tetrahedral cells capability also allows scientists model problems even greater resolution moreover every scientist constant access highend workstations solve problem previous research mainly focused use parallel distributed computers multiresolution data representations example pv3 3 parallel breaks problem domain space places partition individual workstation streamlines calculated distributed interactive manner partic ular pv3 couple visualization calculations simulation approach attractive scientists open distributed computing environment also shown work well distributedmemory parallel computer like ibm sp2 4 another popular approach make use supercomputer like cray visualization calculations highend graphics workstation displaying streamlines streamline visualization approach preferable distributed approach since streamline calculations parallelize well finally multiresolution data representations allow user explore data lower resolution according computers performance still memorylimited highest resolutions recently visualization software companies 1 well corporate research laboratories 11 begun look problem attempted provide viable solutions software products solutions might advanced graphics workstations general visualization purpose outofcore approach intends enable interactive streamline visualization large unstructured grid data midrange workstations even pcclass machines moderate amount main memory 11 outofcore outofcore processing new fact long used cope large data many computational problems engineering science involve solution extremely large linear system fit computers main memory using outofcore method solution absence large memory space parallel computers another example database applications large database constructed outofcore approach operating system smart enough handle memory contention caused using bruteforce algorithms data visualization solving linear systems database construc tion answer modern operating systems good managing multiple jobs providing time sharing via paging swapping cannot make memory appear nowhere particular data access random irregular typical unstructured data visualization poor locality referencing leads thrashing virtual memory example unstructuredgrid data generally store coordinates solution values node grid point node indices triangular faces tetrahedral cells shown figure 1 node face cell data stored contiguously disk space according spatial relationship visualization calculations accessing two neighboring cells may invoke references farther apart disk space consequently constant paging forced fetch disk resident data memory overload eventually becomes io overload moderate paging common desperation swapping often intolerable evident many commercial free visualization software packages fail handle large data sets average workstation research motivated local scientists need interactive visualization mechanism study data desirable resolution particle tracing one important capabilities requested 12 outofcore streamline visualization algorithm streamlines paths massless particles released steady flow fields 15 plotting streamlines fundamental technique visualizing vector field data sets generated scientific computations 7 9 14 streamlines extended construct types objects like streamtubes streamribbons 2 5 14 streamline usually constructed using stepwise numerical integration integration involves following steps 1 selecting initial point 2 locating cell containing point node node node id3 1051 node cell n z coordiates node 34 z coordiates z coordiates node 1051 z coordiates node 1052 z coordiates node 36 z coordiates node 37 z coordiates node boundary face 1 boundary face 2 boundary face f solution data node 277 solution data node 278 solution data node 279 solution data node 280 solution data node 281 solution data node 282 solution data node 283 solution data node node node node node node node node node node node node node node node node node figure 1 typical data structure unstructured meshes normally node face cell data stored separate chunks accessing two cells next spatial domain may invoke references corresponding data items scattering across disk space 3 interpolating vector field calculating new point using numerical integration method 4 termination condition met go step 2 outofcore algorithm designed based following observations ffl streamline calculations incremental local integration step needs small amount data one two tetrahedral cells ffl calculating multiple streamlines concurrently cheaper calculating one streamline maximizes locality reference increases memory performance dramatically ffl data packing essential reduce number disk reads data packed way fetching cells small neighborhood done one disk read ffl much efficient read small chunks data disk moving larger chunk data disk would likely disrupt interactivity streamline ready enter neighboring chunk resulting algorithm contains two steps preprocessing interactive streamline con struction preprocessing step determines connectivity calculates additional quantities interpolation functions coordinates transformation functions restructures raw data stores information compact octree representation disk step needs done second step requires graphical user interface facilitate picking seed points tracing streamlines begins interactive streamline construction step rely operating system fetch required data instead memory management policy designed efficiently utilize minimum memory space fetch data disk streamlines integrated octants octants based principles preemption timesharing way streamlines constructed interactively using megabytes memory space midrange workstation like sun sparc20 rest paper organized follows section 2 illustrates data preprocessing step streamline construction algorithm described section 3 memory management policy explained section 4 tests performed compare virtual memory algorithm study performance memory management policy local disk access nonlocal disk access measure average cost overhead test results presented discussed section 5 followed concluding remarks future research directions 2 data preprocessing efficient visualization operations unstructuredgrid data obtained preprocessing irregularity mesh topology perform streamline visual ization two important operations 1 identifying tetrahedra cell containing user specified seed point 2 computing velocity locations node points fast cell searching methods like one presented 9 need additional data cell connectivity information coordinate transformation functions data also needed integration step could computed fly streamline construction computational cost would high interactive visualization flow solutions defined node locations interpolation must used compute flow variable values locations attain maximum efficiency run time interpolation function cell also precomputed stored data tetrahedral cells use linear basis function interpolation 6 14 summary preprocessing step first determines cell connectivity computes coordinate transformation interpolation functions cell finally partitions reorganizes raw data computed data using octree structure facilitate fast data retrieval achieve interactive visualization cannot avoid precomputing storing data additional storage space required actually makes outofcore approach even attractive issues techniques calculating transformation interpolation functions well connectivity information found previous research 13 14 21 data partitioning two approaches partitioning unstructured data sets first approach divide cells totally disjoint groups since data sets unstructured geometric shapes resulting groups generally irregular advantage approach data redundancy introduced however one disadvantages spatial relationship groups difficult determine specifically becomes difficult verify whether two groups adjacent identify group specified point located second approach partition data set superimposing regular framework subset formed grouping cells intersect contained within region framework framework could regular 3d mesh kway tree octree 10 since data sets unstructured cell may intersect several regions regular framework thus data redundancy inevitable approach partition physical domain corresponding octree figure 2 octree data partitioning major advantage second approach spatial information subsets easily obtained example octree employed framework data partition octant containing seed point identified searching octree root leaves within olog n steps n number octants neighbors octant also found applying technique one neighboring octants contains next point current streamline outofcore setting octrees used framework data partitioning since unstructured grids highly adaptive shape resolution octrees widely employed many computer graphics visualization applications allows us refine data partitioning regions grids dense subsets relatively equal data size ie terms number tetrahedral cells figure 2 simple example octree shown note framework regular 3d mesh searches may completed constant time high resolution regular mesh must used accommodate original meshs irregularity based octree structure data partitioning carried topdown manner first whole data set considered one octant octant decomposed eight child octants using three cutting planes perpendicular x z axes number tetrahedral cells child octant exceeds predefined limit maximum octant size child octant partitioned procedure performed recursively octants contain fewer cells maximum octant size cells octant stored file current implementation enables straightforward access octant though large number files may created maximum octant size small alternative way store octants single file method must employ indexing algorithm sizes octants different number octants large octant stores bounding box octant number cells octant center octant id file used storing cells octant shown figure 3 center octant three cutting planes intersect position point set arithmetic average centers cells octant number cells file id pointers child octants x z center octant bounding box child0 child1 child7 figure 3 data structure octree node center cell defined arithmetic average vertices choice keeps size eight child octants level tree octree created represents structure data partitions stored file partitioning completed file read first beginning streamline visualization session typical octree requires one megabyte storage space structure octree nodes allows efficient systematic way retrieving needed data calculating streamlines 22 outofcore data preprocessing sizes data consider data preprocessing step must also performed outofcore manner done allocating eight buffers memory opening eight disk files store cells read input data file top level eight buffers disk files correspond roots eight child octants bounding regions cells read memory incrementally cell assigned buffer intersects corresponding bounding region mentioned previously cell may assigned one buffer whenever buffer full cells buffers dumped corresponding disk file cells processed octant size examined octant cells maximum octant size another round partitioning proceeds eight buffers disk files created octree completely built next step find cell connectivities calculate coefficients coordinate transformation well interpolation functions one octant file processed time note maximum octant size determines number octants generated larger octant size implies less data redundancy thus less disk space used problem keeping large octants main memory harder achieve consistent performance remember streamline visualization moving many smaller data chunks generally less expensive moving larger pieces since normally small portion data chunk accessed streamline calculation moreover higher hit rate would achieved many smaller octants core hand maximum octant size relatively small larger number octants generated preprocessing step would become expensive data redundancy becomes higher disk space required store data however octants resident main memory streamline constructions attain consistent performance test results provided section 5 show selections maximum octant size influence performance outofcore method 3 streamline construction two operations repeatedly performed integrating streamlines first one compute new positions streamlines second one move required data disk main memory already compared cpu speed memory access time disk io relatively slow order narrow gap computation datafetching carefully scheduled furthermore memory space limited resource important fully utilize memory space store information calculation computation carried minimum interruption order achieve goals outofcore streamline construction algorithm based two fundamental operating system concepts preemption timesharing 12 based availability data streamline construction may following three states waiting ready tracing needed octant main memory streamline ready state enter tracing state next positions calculated otherwise streamline waiting state waiting needed octant brought disk memory space occupied octant longer involved computing new streamline positions released reused short outofcore program following preprocessing stage consists following steps ffl initialization read octree created data partitioning step disk allocate memory space holding octants create data structures needed streamline construction ffl construction streamlines 1 get initial positions selected user 2 identify octants streamlines enter flag octant id memory block size octant pointer used 006 500k free n1 figure 4 octant table 3 fetch octants main memory 4 integrate streamlines octants main memory leave octants 5 go back 2 termination condition streamlines met 31 initialization initialization step first reads octree disk creates following data structures ffl octant table keep track octants main memory ffl three queues scheduling computations octant table used store information octants resident main memory one octant associated entry octant table entry contains four fields figure 4 shows table n entries first field flag indicating whether entry allocated octant second field contains id octant third field stores size memory space allocated octant last one pointer starting position octant three queues created keep data streamlines construction queues named waiting queue ready queue finished queue streamline kept waiting queue enters octant resident main memory otherwise ready queue streamline completely integrated stored finished queue three queues octant table employed schedule streamline construction octantfetching streamlines processed time using less memory space head tail x z velocity data streamline streamline object next object segment point record streamline number points octant id figure 5 data structures streamline 32 construction given initial seed point streamline object created streamline object stores list streamline positions number points streamline id octant containing recently integrated position streamline streamline point coordinates velocity magnitude angular rotation rate flow local flow expansion rate 14 recorded data structure streamline object depicted figure 5 initially ids octants contain initial positions identified entered streamline objects streamline objects kept waiting queue next step streamline construction streamline objects waiting queue examined one one long still space preallocated main memory octant identified streamline object read octant table octant streamline object read streamline object moved waiting queue ready queue subsequently streamline objects ready queue processed one one fourth order rungekutta method 8 used calculate new streamline points time angular rotational rate local flow expansion rate computed streamribbons streamtubes constructed 13 streamline leaves octant octant containing new position streamline identified octant table searched check whether octant already main memory data cell containing current streamline position found streamline construction continues streamline object moved end waiting queue another streamline object selected ready queue processing ready queue waiting queue cpu streamline construction finished queue figure object scheduling streamline reaches physical domain boundary current time step exceeds predefined limit streamline object deleted ready queue stored finished queue ready queue empty octants main memory longer involved streamline construction memory space occupied released free space pool octant table entries marked free waiting queue searched new set octants fetched main memory another round streamline construction begins streamline integration completed streamline objects finished queue example illustrating migration streamline objects streamline construction depicted figure 6 4 memory management octants produced preprocessing stage may different sizes therefore unwise use fixed size memory blocks holds octant efficient utilization memory space memory management policy designed support outofcore streamline visualization program first size memory space dedicated outofcore program selected user presently size measured number cells greater maximum octant size memory space decomposed memory blocks different sizes size memory blocks created determined two parameters maximum octant size parameter called block size level two values either controlled user set automatically based information obtained preprocessing step block size level represents number different block sizes example value one blocks size equal maximum octant size set k sizes blocks maximum octant size blocks created descending order sizes largest block generated first block second largest size created creation process 6 mbytes 3 mbytes mbytes free space table free memory block figure 7 free memory space pool remaining memory space small creating block particular size size skipped block next smaller size created however remaining memory space smaller smallest block size process stops smallest block created rerun process remaining memory space large enough creating blocks memory blocks created put free space pool pool table created bookkeeping number entries table equals block size level entry list blocks size maintained example free space pool shown figure 7 block size level three octant fetched main memory size octant retrieved octree free space pool searched find memory block large enough hold octant searching starts list smallest blocks bestfit block may found block assigned octant removed free space pool octant longer involved computations memory block released free space pool block size level important parameter determining memory utilization efficiency cannot small large former results large blocks space inefficient store smaller octants would cause excessive octant fetching latter results many smaller blocks might small therefore never used tests run study effects parameter upon outofcore program results presented next section test tested outofcore visualization algorithm ibm rs6000 workstation 128 megabytes main memory well sun sparc20 workstation 64 megabytes main memory note algorithms need 520 megabytes 64128 megabytes achieve interactive visualization ibm workstation larger memory space allows us compare performance outofcore method programs relying virtual memory management addition three sets tests conducted sun workstation first set tests used reveal maximum octant size memory space size block size level affect overall performance outofcore program second set tests overhead produced fetching data scheduling computations recorded analyzed third set studies effect caused storing data nonlocal disk tests wall clock time used measure cost tests run batch mode rendering display time included currently rendering done software fast streamline construction rate incremental software rendering make interactive viewing streamline formation possible 51 outofcore method versus virtual memory order reveal strength outofcore method two streamline construction methods rely virtual memory implemented testing three programs use numerical method integrate streamlines two virtualmemorybased methods attempt store much data possible main memory first program cell record contains four vertex indices four neighboring cell indices size cell record bytes four neighboring cell indices cell record calculated preprocessing stage though coefficients coordinate transformation function computed fly streamline construction second program cell record stores four vertex indices four neighboring cell indices 3 theta 4 matrix coordinate transformation function coefficients stored eight integers 12 floats kept cell record size cell record 80 bytes neighboring cell indices coordinate transformation function coefficients precomputed preprocessing stage outofcore program cell record holds information second program maximum octant size set 20000 cells memory space equal six times maximum octant size dedicated program block size level set three data sets tests artificially created dividing cube one two three four five million tetrahedral cells sets memory requirement storing streamlines vertices cell records larger user space main memory example four million tetrahedral cells would require least 128 megabytes dedicated user space ten megabytes generate artificial data sets three components vector field determined following formula wx figure 8 streamline visualization artificial data set table 1 vmbased method 1 data size initiate construct total streamline visualization data set shown figure 8 data sets stored disk binary format data set one hundred streamlines constructed using three programs maximum number time steps streamline set 5000 ibm rs6000 model 560 workstation used tests machine 128 megabytes main memory 512 megabytes paging space two costs measured using wall clock time seconds first one initialization cost mainly time read test data second one cost constructing 100 streamlines total cost calculated adding two tests results summarized figure 9 logarithmic scale used yaxis large small numbers plotted window time breakdown case listed table 1 2 3 compared two virtualmemorybased programs performance 5total time data size millions tetrahedral cells outofcore vs incore methods incore incore precomputed outofcore precomputed figure 9 outofcore versus vmbased methods table 2 vmbased method 2 data size initiate construct total table 3 outofcore method data size initiate construct total ofcore program almost two orders magnitude better initialization cost grows slowly data size streamline construction cost small constant virtualmemorybased programs try keep much data main memory streamline construction test results show lot time devoted allocating memory space reading data sets data size equal two million cells initialization cost grows dramatically since size required memory space already exceeds size physical main memory space operating system swap data paging space create memory space input data situation becomes worse data size increased three million cells second program handle data set four million cells operating system signals system error quits program initialization stage completed therefore shown table 2 first virtualmemorybased program requires less memory space handle larger data sets however coefficients coordinate transformation functions computed fly streamline construction consequently cost constructing streamlines program high compared two programs initialization costs program tolerable data size three million cells data size reaches four million cells initialization cost becomes high total cost equal 49 minutes 47 seconds case data set five millions cells program needs totally 57 minutes 14 seconds construct 100 streamlines outofcore program consumes less 41 seconds perform operation therefore performance first second programs acceptable interactive visualization test results two important findings first virtual memory system operating system helpful visualization application second speed constructing streamlines severely degraded coefficients coordinate transformation functions precomputed achieve interactive visualization doubt must trade space time case use less expensive disk space employ memory management policy tailered streamline calculations 52 maximum octant size size memory space block size level three parameters influence performance outofcore program maximum octant size size memory space block size level tests conducted sun workstation explore three parameters affect performance outofcore program find optimal combination three parameters tests maximum octant size set 10000 20000 30000 40000 cells respectively cell represented 80 bytes information explained section 51 sizes memory space set 4 6 8 10 times maximum octant size block size level varies 1 8 tests performed follows value maximum octant size ffl subdivide data set based maximum octant size ffl memory space size create memory space block size level based block size level construct 100 streamlines 3 measure report cost convenience smaller data set 178 million cells used tests data set comes wind tunnel simulation visualization results shown figure 10 note streamtubes software rendered computational cost data partitioning preprocessing together 20 minutes workstation note cost depends maximum octant size data stored local disk workstation initial points 100 streamlines randomly selected maximum number time steps streamline 5000 test results shown figures 11 12 13 14 costs constructing streamlines using maximum octant size shown individual figure curves plotted figure represent costs using different sizes main memory space varying block size level comparing test results conclude dataset maximum octant size essential parameter outofcore program performance program significantly improved parameter reduced 30000 cells 20000 cells outofcore program favors smaller octant size obvious costs decline memory space increased 4 times 6 times maximum octant size matter maximum octant size however increasing memory space improve performance memory space 4 6 times maximum octant size outofcore program performs better block size level increases significant improvement obtained changing block size level memory space larger current setting best performance thus obtained maximum octant size set 10000 cells memory space used 64 megabytes equivalent 6 times maximum octant size block size level 3 cost constructing 100 streamlines 25 seconds figure 10 streamline visualization windtunnel data set summary outofcore program performs better maximum octant size smaller allocated memory space larger block size level higher nevertheless improvement made changing three parameters limits reasons described follows streamline construction small portion cells visited streamlines octant even whole data set performance improved loading larger number cells main memory instead improved loading cells actually used integration streamline using smaller maximum octant size higher block size level larger memory space octants stay main memory percentage cells directly involved integration becomes higher computation accomplished two consecutive octantfetchings overhead fetchingdata reduced however many octants read overhead octantfetching becomes high increase overhead cancels gain local computations performance reach limit 53 average cost overhead another set tests conducted sun workstation measure overhead caused datafetching streamline scheduling study overhead affects behavior program data set 48 millions tetrahedral cells used data set time block size level maximum octant size10000 cells08mb 32 mbytes 4 times 64 mbytes 8 times figure 11 timing program maximum octant size10000 cells30507090 time block size level maximum octant size20000 cells16mb 64 mbytes 4 times figure 12 timing program maximum octant size20000 cells time block size level maximum octant size30000 cells24mb 96 mbytes 4 times 192 mbytes 8 times figure 13 timing program maximum octant size30000 cells30507090 time block size level maximum octant size40000 cells32mb 192 mbytes 6 times figure 14 timing program maximum octant size40000 cells figure 15 streamline visualization airplane data set obtained computational fluid dynamics simulation flow passing around airplane body visualization results shown figure 15 note portion airplane modeled 407 megabytes memory required store vertex cell records data set maximum octant size memory space size block size level fixed tests maximum octant size set 40000 cells memory space size four times maximum octant size block size level three test performed calculating 10100 streamlines maximum number time steps streamline limited 5000 total cost overhead measured test total cost includes overhead cost integrating streamlines test results presented figure 16 overhead includes costs searching fetching octants selecting memory blocks scheduling streamline objects total cost overhead divided total number time steps used streamline construction obtain average cost average overhead single step computation average costs single step computation shown figure 17 note average cost fluctuates test cases seed points randomly selected therefore length streamline varies also note average cost decrease much streamlines constructed concurrently increasing overhead due streamline scheduling octant searching cancels time number streamlines execution time wall clock figure total cost constructing streamlines benefit octant sharing finally average overhead divided average cost produce percentage cost due overhead percentages cost due overhead single step calculation depicted figure 18 according test results overhead high 40 percent overall cost also measure difference cost constructing one streamline time multiple streamlines one test one hundred streamlines constructed one one thus streamline scheduling octant searching required memory allocation trivial since one octant one streamline resident main memory time average cost tracing streamline 076 seconds circumstances hand test reveals average cost constructing 100 streamlines concurrently 056 seconds per streamline observe 263 improvement performance due octant sharing even though overhead introduced multistreamline execution 54 local disk versus nonlocal disk previous tests data stored local disk workstations environ ments data may stored nonlocal disk fileserver connected workstations via network order explore effect storing data nonlocal execution time microseconds number streamlines average cost one step figure 17 average cost single step computation2832364010 20 number streamlines percentage overhead one step figure average overhead single step computation time block size level maximum octant size10000 cells08mb 32 mbytes 4 times 64 mbytes 8 times figure 19 timing constructing streamlines using nonlocal disk disk set another set tests repeat tests described section 52 using data set however data stored nonlocal disk two sets test results presented figures 19 20 penalty using nonlocal disk apparent latency network significantly affects overall performance percentage cost resulting network latency 59 65 maximum octant size 10000 cells increases 68 76 maximum octant size 40000 cells total cost increased least 100 since network shared several computers program performance unstable general cases program performs better maximum octant size smaller similar results reported previous tests performance improve memory space allocated octant fetching becomes frequent order fill additional memory space triggers nonlocal disk io 6 conclusions presented efficient outofcore algorithm visualizing large unstructured vector field data sets single workstation moderate size main memory using octree structure data sets partitioned subsets stored disk files subsets read main memory demand memory management policy time block size level maximum octant size40000 cells32mb 192 mbytes 6 times figure 20 timing constructing streamlines using nonlocal disk designed allocate memory space storing tests conducted explore performance algorithm implementation test results demonstrate outofcore algorithm enables interactive streamline visualization data sets several millions tetrahedral cells average workstation example data set 178 million cells computational cost constructing 100 streamlines concurrently many 5000 integration points 25 seconds sun sparc20 using 64 megabytes main memory space also show visualization requirements cannot achieved using virtual memory use highend workstation like sun ultra sparc sgi indigo2 would increase interactivity test results reveal performance program better data division finer block size level higher memory space used larger also show outofcore program runs much faster data stored local disk future work includes making use hardware rendering capability graphics workstation optimizing preprocessing step designing outofcore algorithms types visualization operations surface volume rendering acknowledgment research supported part national aeronautics space administration nasa contract nas119480 national science foundation acerc center thanks tom crockett constructive suggestions r final progress report phase sbir software architecture efficient visualization large unsteady cfd results visualization 3d vector fields variations stream pv3 distributed system largescale unsteady cfd visualization application pv3 coprocessing visualization environment 3d unstructured mesh calculations ibm sp2 parallel computer constructing stream surface steady 3d vector fields interactive time dependent particle tracing using tetrahedral decomposition numerical analysis vectorized particle tracer unstructured grids design analysis spatial data structures streaming pipeline operating system concepts elementary fluid mechanics tr ctr douglass davis william ribarsky nickolas faust jiang intent perception outofcore visualization applied terrain proceedings conference visualization 98 p455458 october 1823 1998 research triangle park north carolina united states ralph bruckschen falko kuester bernd hamann kenneth joy realtime outofcore visualization particle traces proceedings ieee 2001 symposium parallel largedata visualization graphics october 2223 2001 san diego california ricardo farias cludio silva outofcore rendering large unstructured grids ieee computer graphics applications v21 n4 p4250 july 2001 falko kuester ralph bruckschen bernd hamann kenneth joy visualization particle traces virtual environments proceedings acm symposium virtual reality software technology november 1517 2001 baniff alberta canada tahsin kurc mit atalyrek chialin chang alan sussman joel saltz visualization large data sets active data repository ieee computer graphics applications v21 n4 p2433 july 2001 naveen kumar polapally raghu machiraju dhabhaleshwar panda feature estimation efficient streaming proceedings 2002 ieee symposium volume visualization graphics october 2829 2002 boston massachusetts douglas davis william ribarsky jiang nickolas faust sean ho realtime visualization scalably large collections heterogeneous objects case study proceedings conference visualization 99 celebrating ten years p437440 october 1999 san francisco california united states tahsin kurc mit atalyrek chialin chang alan sussman joel saltz visualization large data sets active data repository ieee computer graphics applications v21 n4 p2433 july 2001 james ahrens kristi brislawn ken martin berk geveci c charles law michael papka largescale data visualization using parallel data streaming ieee computer graphics applications v21 n4 p3441 july 2001 shyhkuang ueng outofcore encoding large tetrahedral meshes proceedings 2003 eurographicsieee tvcg workshop volume graphics july 0708 2003 tokyo japan david ellsworth bryan green patrick moran interactive terascale particle visualization proceedings conference visualization 04 p353360 october 1015 2004 vijay kumar benjamin rutt tahsin kurc umit catalyurek joel saltz sunny chow stephan lamont maryann martone imaging visual analysislarge image correction warping cluster environment proceedings 2006 acmieee conference supercomputing november 1117 2006 tampa florida shyhkuang ueng k sikorski outofcore method computing connectivities large unstructured meshes proceedings fourth eurographics workshop parallel graphics visualization september 0910 2002 blaubeuren germany c charles law william j schroeder kenneth martin joshua temkin multithreaded streaming pipeline architecture large structured data sets proceedings conference visualization 99 celebrating ten years p225232 october 1999 san francisco california united states sara mcmains joseph hellerstein carlo h squin outofcore build topological data structure polygon soup proceedings sixth acm symposium solid modeling applications p171182 may 2001 ann arbor michigan united states yijen chiang cludio silva william j schroeder interactive outofcore isosurface extraction proceedings conference visualization 98 p167174 october 1823 1998 research triangle park north carolina united states shyhkuang ueng yanjen su chitang chang lod volume rendering fea data proceedings conference visualization 04 p417424 october 1015 2004 paolo cignoni claudio montani claudio rocchini roberto scopigno external memory management simplification huge meshes ieee transactions visualization computer graphics v9 n4 p525537 october david ellsworth patrick j moran accelerating large data analysis exploiting regularities proceedings 14th ieee visualization 2003 vis03 p74 october 2224 wagner correa james klosowski claudio silva visibilitybased prefetching interactive outofcore rendering proceedings ieee symposium parallel largedata visualization graphics p2 october 2021 peter lindstrom cludio silva memory insensitive technique large model simplification proceedings conference visualization 01 october 2126 2001 san diego california ricardo farias cludio silva outofcore rendering large unstructured grids ieee computer graphics applications v21 n4 p4250 july 2001 papadomanolakis anastassia ailamaki julio c lopez tiankai tu david r ohallaron gerd heber efficient query processing unstructured tetrahedral meshes proceedings 2006 acm sigmod international conference management data june 2729 2006 chicago il usa gokul varadhan dinesh manocha outofcore rendering massive geometric environments proceedings conference visualization 02 october 27november 01 2002 boston massachusetts brent woods bradley clymer joel saltz tahsin kurc parallel implementation 4dimensional haralick texture analysis diskresident image datasets proceedings 2004 acmieee conference supercomputing p48 november 0612 2004 yijen chiang ricardo farias cludio silva bin wei unified infrastructure parallel outofcore isosurface extraction volume rendering unstructured grids proceedings ieee 2001 symposium parallel largedata visualization graphics october 2223 2001 san diego california michael beynon chialin chang umit catalyurek tahsin kurc alan sussman henrique andrade renato ferreira joel saltz processing largescale multidimensional data parallel distributed environments parallel computing v28 n5 p827859 may 2002