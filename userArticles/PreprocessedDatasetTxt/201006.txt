effective cache prefetching busbased multiprocessors compilerdirected cache prefetching potential hide much high memory latency seen current future highperformance processors however prefetching without costs particularly sharedmemory multiprocessor prefetching negatively affect bus utilization overall cache miss rates memory latencies data sharing simulate effects compilerdirected prefetching algorithm running range busbased multiprocessors show despite high memory latency architecture necessarily support prefetching well cases actually causing performance degradations pinpoint several problems prefetching sharedmemory architecture additional conflict misses reduction datasharing traffic associated latencies multiprocessors greater sensitivity memory utilization sensitivity cache hit rate prefetch distance measure effect performance solve problems architectural techniques heuristics prefetching could easily incorporated compiler 1 victim caching eliminates cache conflict misses caused prefetching directmapped cache 2 special prefetch algorithms shared data significantly improve ability basic prefetching algorithm prefetch individual misses 3 compilerbased shareddata restructuring eliminates many invalidation misses basic prefetching algorithm predict combined effect improvements make prefetching effective much wider range memory architectures b introduction several factors contribute increasing need processors tolerate high memory latencies particularly multiprocessor systems certainly widening gap speed cpus memory increases memory latencies uniprocessors multiprocessors alike13 fast processors also increase contention multiproces sors lengthening actual latency seen cpus cpu queuing interconnect second parallel workloads exhibit interconnect operations caused data sharing among processors resulting research supported onr grant n0001492j1395 nsf pyi award mip9058439 authors address department computer science engineering fr35 university washington seattle wa 98195 delays greater memory subsystem contention finally processors memory become physically distributed memory latencies necessarily increase softwarecontrolled cache prefetching technique designed make processor speeds tolerant memory latency softwarecontrolled cache prefetching cpu executes special prefetch instruction data loaded point near future best case data arrives cache needed cpu cpu sees load hit lockupfree caches17 21 25 27 allow cpu continue execution prefetch hide prefetch latency cpu paper address issue prefetching busbased shared memory multiprocessors goal work gauge impact performance architectures pinpoint factors responsible experiments simulate parallel workloads busbased multiprocessor coupled prefetching algorithm represents ideal current compilerdirected prefetching technology oracle predict cache misses apart misses caused data sharing use identify architectures workloads prefetching improves performance performance degrades results give us insight particular problems multiprocessors pose prefetching allow us introduce changes memory subsystem prefetching algorithm shared data allocation solve although studies closely model busbased system extend multiprocessor architectures memory contention issue show prefetching busbased multiprocessor unlike uniprocessor needs done care universal win issues increased pressure cache parallel machines greater sensitivity memory subsystem utilization interaction prefetching data sharing effects cause performance improvements prefetching less expected even nonexistent basic prefetching scheme observe speedups higher 29 degradations high 6 address issues various architectural compilerbased techniques victim caching compilerbased shared data restructuring two special prefetch algorithms shared data end result prefetching shown effective much wider range memory architectures effective regions already viable combination techniques achieve speedups prefetching simulated memory speeds ranging 6 83 remainder paper organized follows section 2 describes related work compilerdirected prefetching multiprocessor prefetching section 3 describes methodology justifies choice simulation environment section 4 presents results basic prefetch strategy used conjunction highly efficient cache miss predictor highlights drawbacks prefetching strategy following three sections explore detail issues prevented better multiprocessor prefetching performance present architectural compiler techniques make prefetching effective section 5 examines problem prefetches dont complete time section 6 cache conflict misses caused prefetching section 7 data sharing issues principally difficulty predicting invalidation misses section 8 shows effect combining techniques conclusions appear section 9 related work work builds previous study 29 pinpointed problems prefetching shared memory machine additional conflict misses data sharing traffic multiprocessors greater sensitivity memory utilization determining best prefetch distance measured effect performance paper showed despite high memory latencies many busbased multiprocessors support prefetching well cases prefetching actually causes performance degradation paper extends work several ways first examine conflict misses detail use new memory architecture alternative victim cache virtually eliminate caused prefetching section 6 second present new prefetching strategy makes effective use exclusive prefetching section 73 third make use sophisticated shared data restructuring techniques section 72 make prefetching viable fourth improved methodology previous study several respects example trace different region pverify application capture parallelism also model hardware barriers accurately increased accuracy shortterm sharing activity topopt application makes frequent use barriers although need make processors tolerant high memory latency much severe multiprocessors uniprocessors studies cache prefetching concentrated uniprocessor architectures1 6 5 23 3 dash18 hardware support cache prefetching date published results microbenchmark throughput tests noteworthy exception work mowry gupta22 simulations driven three parallel programs providing analysis potential speedups programmer directed cache prefetching however multiprocessor architecture examined sixteen dash clusters connected highthroughput interconnection network one processor per cluster avoids types contention interference wish study result include full effects contention shared bus found effect crucial prefetching performance architectures examined addition provide detailed analysis multiprocessor cache misses identifying key components affect performance scheme deals programmerdirected prefetching emulates compilerdirected simulate sharedmemory references simulate shared private interference two caches key element study mowry et al23 detail compilerbased prefetching algorithm uniprocessor model simulated prefetching algorithms emulate uses several techniques selective prefetching targeting memory accesses likely miss cache 3 simulation environment prefetching studies use tracedriven simulation traces generated coarsegrain explicitly parallel workloads prefetch instructions inserted traces simulate several types busbased multiprocessors differ extent contention affects memory latency ie vary bus speeds also examine cache architectures without section 6 victim cache several prefetching strategies used differ often prefetching done section details simulation environment 31 prefetching algorithms softwaredirected prefetching schemes either cache prefetch brings data data cache closest processor prefetch data separate prefetch buffer prefetching study concerns cachebased prefetching baseline prefetching algorithm contains optimized prefetcher nonshared ie uniprocessor data misses depend cache configuration accurately predicts nonsharing cache hits misses never prefetches data used emulate algorithm adding prefetch instructions address traces generated shared memory multiprocessor candidates prefetching identified running processors address stream uniprocessor cache filter marking data misses prefetch instructions placed instruction stream distance ahead accesses miss number cpu instructions prefetch actual access referred prefetch distance since offline algorithm technique represents ideal current prefetching algorithms ie one prefetches scalars array references accurately identifies leading references first access cache line capacity conflict misses mowry et al23 shown compiler algorithms approximate well already predicting compulsory capacity misses array references existing algorithms improve get closer ideal using prefetcher ideal respect nonsharing misses enables us pinpoint exact cause remaining miss observed cpu prefetching explained section 4 baseline algorithm strive emulate compilerbased algorithm mowry et al23 best example rather programmerdirected approach mowry gupta22 feel prefetching predominantly domain compilers rather programmers mowry gupta show prefetching inserted programmer intimately familiar application effective methodology necessarily indicate compilerdirected prefetching would mowry et al23 despite targeted uniprocessor represents best available compilerdirected prefetching algorithm architecture unreasonable expect would perform well large number multiprocessor architectures without enhancement overhead associated prefetch simulations relatively low single instruction prefetch access continue assume existence effective efficient prefetching algorithms mowry et al report overheads uniprocessor compiler algorithm typically less 15 increased instruction count impact total execution time typically half mowry guptas programmerdirected scheme experienced overheads 1 8 total execution time chen baers4 implementation mowry et als compiler algorithm multiprocessor including two bench marks mp3d water experienced prefetch instruction overhead 24 total execution time simulations overheads never 4 instruction count 2 total execution time multiprocessor writeinvalidate cache coherency protocol data prefetched either shared mode case subsequent write might require extra invalidating bus operation exclusive mode would cause cached copies cache line invalidated latter referred exclusive prefetch simulations support types prefetches mowry gupta specified otherwise however prefetch shared mode 32 workload address traces generated mptrace12 sequent symmetry19 running following coarse grained explicitly parallel applications written c see table 1 topopt7 performs topological optimization vlsi circuits using parallel simulated annealing algorithm pverify20 determines whether two boolean circuits functionally identical statistics amount shared data programs found 10 locusroute commercial quality vlsi standard cell router mp3d solves problem involving particle flow extremely low density water evaluates forces potentials system water molecules liquid state latter three part stanford splash benchmarks26 contrast two applications optimized programmers processor locality shared number dynamic data data percent percent program data set data processes set size references reads private pverify c88021berk12 130 kb 12 128 kb 54 million 82 59 topopt aplalomim 20 kb 9 20 kb 58 million 85 69 locusroute primary1 16 mb 12 709 kb 72 million 75 87 mp3d 10000 molecules 19 mb 12 459 kb 82 million 69 75 water 343 molecules 227 kb 12 237 kb 67 million 76 94 table 1 workload used experiments restricted practical limit trace lengths multiprocessor tracedriven simulations balance must struck desire large data sets dont fit cache tracing reasonable portion program large data structure one could easily end tracing single loop may may indicative rest program attempt solve scaling back data sets local cache sizes single order magnitude relative might considered reasonable configuration current moderately parallel multiprocessors thus maintain realistic ratio data set offchip cache size ensuring cases neither critical data structures dynamic cache working sets fit simulated cache exception topopt still interesting high degree write sharing large number conflict misses even small shared data set size application begin collecting traces right parallel execution begins initially simulate without collecting statistics 500000 data references avoid coldstart effects simulate least 5 million data references application table 1 shared data column gives total amount shared data allocated application dynamic data set size column shows total amount private shared data touched traced portion program number references simulated statistics kept given references colum last two columns show percent data references reads percent private data benchmark 33 multiprocessor simulations prefetch accesses added traces run charlie9 multiprocessor simulator modified handle prefetching lockupfree caches splittransaction bus protocol victim caches besides modeling cpu cache bus hardware low level charlie carries locking barrier synchronization therefore interleaving accesses different processors changed behavior memory subsystem charlie ensures legal interleaving maintained instance processors vie locks may acquire order traced run still acquire legal order enter critical sections one processor time modeled data caches assuming instruction caches insignificantly low miss rates caches direct mapped writeback one per processor simulations presented kbytes simulations include private shared data order include effects interference two cache cache coherency scheme illinois coherency protocol24 invalidationbased protocol important feature purposes privateclean state exclusive prefetches simulate 16deep buffer hold pending prefetches processor sufficiently large almost always prevent processor stalling buffer full bus uses roundrobin arbitration scheme favors blocking loads prefetches consider systems high memory latency prefetching less useful possibly harmful little latency hide processors execute single cycle per instruction 2cycle data loads coupled memory subsystem latency 100 cycles given latency examine spectrum memory architectures high low memory bandwidth resulting range bus utilizations memoryinterconnect model splittransaction bus protocol system enough parallelism memory bankscontrollers make address bus memory access relatively conflict free data bus transfer bottleneck varying speed data bus vary maximum throughput memory subsystem way able model spectrum values ratio bus latency bus bandwidth since factor many phenomena describe sensitive without varying minimum latency thus prevent results dominated large changes memory latency since effectiveness prefetching obviously highly sensitive parameter specific model splittransaction bus results reflective memory architecture potential saturate also ran simulations larger block sizes results presented add great deal insight already presented essentially larger block sizes increased amount sharing traffic due false sharing thus increased importance techniques use deal sharing otherwise impact effectiveness prefetching aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa total miss rate aa aa cpu miss rate adj cpu miss rate miss rate prefetch strategy locusroute water topopt pverify mp3d figure 1 total cpu miss rates five workloads 8cycle data bus latency simulations paper data transfer portion memory latency varied 4 32 cycles total 100 cycles split transaction bus architecture described data transfer latency 4 cycles would make address transmission memory lookup 96 cycles processor cycle speed 400 mhz would model memory subsystem memory latency 250 nanoseconds peak throughput 32 gbytessecond 32 cycle latency corresponds memory throughput 400 mbytessecond 4cycle latency corresponds transfer 64 bits across bus every cpu cycle also section 6 add victim caches16 architecture hit victim cache takes 4 cycles longer hit main cache much less memory access saves least one bus operation 4 basic prefetching simulate memory architecture basic prefetching algorithm prefetching serves baseline calculating speedups prefetching algorithms execution times experiments given relative prefetching memory architecture cache configuration basic prefetch algorithm prefetch instructions inserted traces potential cache miss identified cache filter algorithm prefetch distance 100 instructions means barring cpu stalls code prefetch associated load would execute 140 cycles given processor model gives memory subsystem minimum latency 100 cycles time complete prefetch contention delays large processor otherwise slowed basic prefetching strategy identified figures pref compare basic prefetch strategy prefetching identified np results 32 kbyte cache 32byte cache line shown figures 1 2 tables 2 3 effect prefetching miss rates figure 1 terminology becomes ambiguous presence prefetching use following terms misses total miss rate refer prefetch nonprefetch accesses hit cache cpu misses cpu miss rate misses nonprefetch accesses thus observed cpu prefetch misses occur prefetch accesses accesses prefetches still progress since cpu must stall count cpu misses refer prefetchinprogress misses often comprise nonnegligible portion cpu miss rate adjusted cpu miss rate include therefore adjusted cpu miss rate includes accesses cause cpu stall entire memory latency cpu miss rate adjusted cpu miss rate plus prefetchinprogress miss rate total miss rate cpu miss rate plus prefetch miss rate miss rates cumulative figure 1 total miss rate combined height three bars cpu miss rate combined height black diagonally striped bars without prefetching total miss rate cpu miss rate adjusted cpu miss rate identical data figure 1 8cycle datatransfer latency component miss rate varies significantly across memory throughputs prefetchinprogress difference cpu miss rate adjusted cpu miss rate rises data bus gets slower several observations made miss rate results first cpu miss rates fell significantly 32 71 3577 adjusted results shown figure 1 use oracle prefetcher one might naively expect even misses covered three reasons didnt happen first prefetchinprogress misses account significant part cpu miss rate applications second prefetching actually introduces additional cache conflict misses last importantly data sharing among processors produces invalidation misses cases largest single component cpu miss rate oracle prefetcher doesnt predict misses result invalidations total miss rates increase simulations prefetching previous studies uniprocessors multiprocessors focused cpu miss rates uniprocessor likely memory interconnect bandwidth absorb extra memory traffic increase total miss rate significant relative decrease cpu miss rate multiprocessor systems total miss rate important metric indicative demand bottleneck component machine particularly true busbased multiprocessor also multiprocessor contention memory interconnect significant bus becomes saturated system performance track throughput bus speeding cpus beneficial effect since bus demand function total miss rate rather cpu miss rate total miss rate better indicator performance architectures bus memory bottlenecked system prefetching reduces cpu miss rate expense total miss rate may hurt performance table 2 see miss rates affect databus utilization memory architecture varied bus utilization number cycles bus use divided total cycles simulation bus utilization results misleading interpreted correctly two reasons bus utilization increase one workload produces bus operations second number bus operations occur shorter time period instance bus utilization increased prefetching 4cycle pverify simulation total miss rate increased execution time reduced pref data transfer latency program alg 4 cycles 8 cycles locus np 21 33 56 89 mp3d np 48 65 90 100 pverify np 46 68 96 100 topopt np 13 20 33 51 water np 10 14 22 38 table 2 selected bus utilizations workload np pref locusroute 127 137 mp3d 424 438 pverify 577 620 topopt 524 599 water 051 055 table 3 bus demand per access bdpa 8cycle data bus latency result prefetching successfully overlapping memory access instruction execution order see bus demand independent execution speed table 3 gives bus demand per access bdpa total number bus cycles used divided total number memory accesses cache hits misses bdpa unlike bus utilization independent execution time thus gives better indication additional demand placed memory subsystem prefetching results tables 2 3 indicate applications bus demand increased prefetching expected given total miss rates figure 1 data table 3 8cycle data transfer latency data bus speeds shown practical purposes scales linearly bus speed figure 2 see effects prefetching execution time different memory subsystems figure execution time prefetching bus speed normalized execution time prefetching memory architecture examine table 2 figure 2 see whenever bus utilization greater 90 without prefetching use prefetching resulted increase execution time region enough spare bus bandwidth absorb extra demand prefetching placed increases execution time bus saturated dramatic however total miss rates rise small amounts prefetching increasingly positive effect execution time bus loads become lighter bus gets faster general performance improvements large two reasons first already discussed prefetching causes increase memory latency due increased contention processors bus addition overhead prefetching cpu execution time although relatively small experiments 1 execution time pref scheme summary data bus latency085095105 data bus latency085095105 data bus latency085095105 data bus latency085095105 data bus latency relative execution time relative execution time topopt water locusroute mp3d pverify figure 2 relative execution time five workloads prefetching pref normalized prefetching execution time results indicate benefits prefetching busbased multiprocessor marginal except case high bandwidth memory subsystem even using highly efficient cache miss predictor largest gain execution time observed 29 speedup largest degradation 6 order gain insight much improvement actually possible prefetching look processor utilization without prefetching instance average processor utilization water 82 fastest bus 81 slowest bus since best memorylatency hiding technique bring processor utilization 1 best speedup could achieved water 12 hand processor utilization mp3d ranged 39 22 room speedup 25 fast bus 45 slow bus ignoring overhead prefetch instructions mp3d best speedups falls far short maximum potential workloads average processor utilization without prefetching locusroute ranged 64 54 pverify ranged 36 17 topopt 13 11 topopt makes use hardware barriers much low processor utilization due synchronization delays aaa aaaaaa aaa aaa aaaaaa aaa aaa aaaaaa aaa aaa aaaaaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa top src data nonsharing prefetched aa aa nonsharing prefetched invalidated prefetched aa aa invalidated prefetched prefetch progress number prefetch strategy mp3d pverify topopt figure 3 sources cpu misses mp3d pverify topopt 8cycle data bus latency also noted predicting effectiveness prefetching particular workload difficult workloads achieve largest improvement largest degradation depending upon memory subsystem architecture applications put heavy load memory system see larger memory latency due contention benefit hiding latency applications however first enter bus saturation begin degrading prefetching simulations exemplified mp3d pverify high bus utilization one reason results prefetching disappointing another large number cpu misses despite use oracle miss predictor order understand various sources magnitudes remaining cpu misses analyze different types cpu misses figure 3 shows breakdown cpu misses three applications topopt pverify mp3d 8cycle data transfer latency misses shown fall following categories either invalidation misses tags match state marked invalid nonsharing misses first use data replaced cache miss type either prefetched disappeared cache use prefetched miss predicted fifth type miss prefetchinprogress means prefetch access presented memory subsystem complete time cpu requested data sum total five types misses combined height five bars cpu miss rate component total miss rate pref prefetch misses shown prefetch misses misses effectively hidden turned hits perspective cpu prefetch access goal prefetching turn many misses np prefetch misses possible hopefully without incurring many additional cache misses use oracle predicting nonsharing misses allows us categorize nonsharing misses precisely imperfect prefetcher would necessarily clear whether miss caused prefetches permuting memory reference pattern imperfections prefetcher results observe remain significant portion nonsharing cpu misses covered prefetching since oracle prefetcher perfectly predicts nonsharing misses absence prefetches remain either caused prefetch access replacing data still used nonsharing prefetched figure prefetched cache line replaced use nonsharing prefetched figure implies degree conflict prefetched data current working set significant two components cpu miss rate particularly important represent cache misses covered prefetching also result bus accesses werent necessary without prefetching nonsharing prefetched misses require extra bus access prefetch access wasted nonsharing prefetched misses without prefetching access cache hit therefore remaining nonsharing miss represents increased demand bus due prefetching prefetchinprogress misses represent much 27 cpu misses figure slowest data bus 32 cycles much 62 total contribution observed memory latency less however prefetchinprogress miss latency typically much less entire memory access latency see section 5 perhaps conspicuous result figure 3 prefetch algorithm affected invalidation misses one concern motivated study hypothesis increasing interval cache seeks hold cache line prefetching would exacerbate data sharing problem resulting invalidate operations invalidation misses results dont bear pref prefetching strategy seen prefetching reduced number invalidation cpu misses clearly limit effective prefetching doesnt address invalidation misses may true prefetching exacerbate data sharing problem exposed performance applications effects data sharing much greater extent results presented far suggest three primary opportunities improvement multiprocessor prefetcher large number prefetchinprogress misses conflict misses additional bus load introduced prefetching inability hide reduce latencies due data sharing areas investigated following sections reducing prefetchinprogress misses pref prefetch distance relatively close bestcase memory latency 100 cycles contention cause real latency much higher however reason mowry et al23 suggest using larger prefetch distance ensure prefetched data time arrive section examine effect increasing prefetch distance 400 instructions label prefetching strategy lpd lpd strategy wanted prefetch distance high enough cause prefetchinprogress misses insignificant impact execution time bus speeds without necessarily removing aaa aaaaaa aaa aaa aaaaaa aaa aaa aaaaaa aaa aaa aaa aaaaaa aaa aaa aaaaaa aaa aaa aaaaaa aaa aaa pver src data aa aa aa aa aa aa aa aa aaa aaa aaa aaa aaa nonsharing prefetched aaa aaa nonsharing prefetched invalidated prefetched aaa aaa aaa invalidated prefetched prefetch progress number prefetch strategy mp3d pverify topopt figure 4 sources cpu misses mp3d pverify topopt long prefetch distance strategy lpd data bus latency 8 cycles lower values prefetch distance quite achieve figure 4 presents effect longer prefetch distance individual components miss rate figure 5 shows effect execution time figure 4 sake consistency given data bus latency 8 cycles prefetchinprogress misses serious problem 32cycle bus number prefetchinprogress misses increased application least factor four shown increasing prefetch distance 100 400 successfully eliminates majority prefetchinprogress misses bus speed shown virtually eliminated worst case 32cycle bus lpd strategy eliminates average 63 prefetchinprogress misses cost conflict misses earlier prefetch begun likely replace data still used also longer prefetched data sits cache used likely replaced numbers pverify topopt indicate latter particularly critical trading prefetchinprogress misses conflict misses wise prefetchinprogress misses cheapest type misses processor wait access progress complete instead entire access time mp3d example prefetchinprogress misses represent 27 cpu misses 8cycle bus add 2 total execution time also incurring prefetchinprogress miss increase load bus prefetchinduced conflict miss represents extra bus operation compared prefetching even mp3d lpd adds least number conflict misses improvement execution time lpd pref results indicate increasing prefetch distance point virtually prefetches complete pay argues prefetching algorithms strive receive prefetched data exactly time penalty cycles late small processor must stall cycles much less time full access penalty cycles early also small chances losing data use slight period mowry et al 23 also studied prefetch distance noting one programs degraded data bus latency085095105 data bus latency085095105 data bus latency085095105 data bus latency085095105 data bus latency locusroute mp3d water pverify topopt relative execution time relative execution time lpd figure 5 relative execution time long prefetch distance strategy increasing prefetch distance manually restructured four others avoid conflicts causing phenomenon inability long prefetch distance improve prefetching performance closely tied general problem prefetchinduced conflict misses therefore reexamine lpd strategy next section look one cache architecture designed reduce conflict misses 6 victim caching reduce conflict misses shown prefetching increase number replacement misses cache due conflict current working set part future working set prefetched results based directmapped cache section investigate modified cache organization reduce magnitude conflict consequently whether improves effectiveness prefetching order see effect alternate cache organization magnitude prefetchinduced conflicts simulate configuration addition small 8 entry fullyassociative victim cache16 aa aaaa aa aa aa aa aa aa aaaa aa aa aaaa aa aaa aaaaaa aaa np pref npvict prefvict lpdvict top src data aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa np pref npvict prefvict lpdvict nonsharing prefetched aaa aaa aaa nonsharing prefetched invalidated prefetched aaa aaa aaa invalidated prefetched prefetch progress number mp3d pverify topopt prefetch strategy aaa aaaaaa aaa aaa aaa aaa aaa aaa aaaaaa aaa aa aaaa aa aa aaaa aa np pref npvict prefvict lpdvict figure sources cpu misses mp3d pverify topopt victim cache although caches higher levels associativity greater one also lessen impact prefetch induced conflicts chose examine victim caches seem represent less costly approach terms criticalpath cache access time size complexity appears ideally suited problem prefetchinduced conflicts chen2 showed victim cache matched performance 2way setassociative cache context hardware prefetching tests actually showed 8line victim cache general somewhat less effective 2way setassociative cache reducing number conflict misses sufficient eliminate particular problem prefetchinduced conflicts section show victim cache small cache blocks recently replaced main cache good match prefetchinduced conflicts targets conflict misses data recently replaced cache incur prefetchinduced conflict miss number instructions time block replaced cache accessed resulting conflict miss bounded prefetch distance victim cache may cases require hardware adding 2way setassociativity cache example case 8line victim cache requires 8 lines data tags state order 8 x bits 32byte line size 32bit addresses 2way setassociative cache require one extra tag bit per cache line lru bit per set 1536 bits configuration however caches configuration 64 kbyte larger would require increasingly storage 8line victim cache cachecoherent multiprocessor victim caches slightly complex uniprocessors need hardware support snooping victim cache tags victim cache distinct advantage add delay critical path cache lookup directmapped cache true conventional associative cache victim cache lookup occurs main cache miss access victim cache takes longer access main cache also ties cache longer swap main victim caches much less costly main data bus latency075085095105 data bus latency075085095105 data bus latency075085095105 data bus latency075085095105 data bus latency lpdvict prefvict npvict locusroute mp3d water pverify topopt relative execution time relative execution time figure 7 execution times relative prefetching victim cache five workloads victim cache memory access require bus operation figure 6 see victim caching indeed significant impact number replacement misses caused prefetching example pref strategy victim cache prefetchinduced conflict misses combination prefetched nonprefetched nonsharing misses represent cpu miss rate topopt 8cycle results strategy victim cache account 1 cpu miss rate figure 7 shows execution time results victim caches results normalized np results without victim caches results make several observations first although still see small performance degradations prefetching much smaller without victim caches fact worst case prefvict 1 npvict perhaps small enough allow us ignore prefetch induced conflict problem architecture supports appropriate cache configuration eg setassociative cache directmapped victim cache level second observe nearly cases combined effect speedup prefetching victim cache together greater sum individual contributions effect prefetching benefit victim cache without example topopt 4cycle bus pref provides 14 speedup np prefvict provides 22 speedup npvict two reasons occurs already discussed victim cache eliminates negative sideeffect prefetching additional conflict misses previously detracted potential speedups prefetching also victim cache lowers overall miss rate decreases load bus means configuration less sensitive bus contention effects shown limit effectiveness prefetching example region without victim cache saturated bus pverify 16 cycles case point thus saw benefit prefetching may victim cache longer bus saturation would expect increase prefetching effectiveness due victim cache would even greater real prefetching system without oracle determine potential cache misses since conflict misses difficult compiler identify capacity misses third observation increasing prefetch distance longer clearly harmful one applications topopt lpdvict noticeably inferior prefvict victim cache catches additional conflicts caused increased prefetch distance one case locusroute increasing prefetch distance effective comparing lpdvict prefvict effective high bus utilization short saturation number delays associated prefetchinprogress misses greatest cases lpdvict clearly better worse prefvict performance loss due prefetchinprogress misses never great perhaps important aspect result allows compiler place prefetches earlier performance prefetching less sensitive exact placement prefetches cache strictly direct mapped allow compiler much flexibility prefetch placement results 8line victim cache 4line cache found sufficient eliminate enough prefetchinduced conflicts applications studies pref still visibly outperformed lpd applications lastly observe although eliminated drawbacks resulted occasional performance degradations prefetching victim cache prefetching still provide significant speedups bus saturated bus 90 utilized npvict pverify 1632 cycles mp3d 24 32 cycles although cache organization forgiving cache conflicts mitigates drawbacks prefetching memorybottlenecked system prefetching still attack problem bottleneck componentthat total number interconnectmemory operations victim cache also fails help largest single component cpu miss rate invalidation misses dealt following section 7 reducing shared data latencies currently know available compilerbased prefetching algorithms dealing invalidation misses show section simple heuristics simpler instance data flow analysis across processes recognizing writeshared data blindly prefetching often prefetching writes differently reads recognizing readmodifywrite patterns improve performance several opportunities reduce impact sharing traffic observed pref scheme certainly better job predicting prefetching invalidation misses achieve much better miss coverage clear figures 3 4 6 current miss predictor although extremely efficient predicting nonsharing misses inadequate predicting invalidation misses prefetch algorithm emulate tailored uniprocessor even better algorithms appear predicting invalidation misses remain much difficult problem predicting nonsharing misses due nondeterministic nature invalidation traffic investigate two mechanisms making prefetching effective presence data sharing traffic section 71 examines better heuristic prefetching invalidation misses section 72 studies effect prefetching performance compiler algorithm reduces invalidation misses restructuring shared data addition prefetching increase sharing traffic ways obvious data shown far successful prefetch write miss shared data increase bus traffic even causes cpu misses causing unnecessary invalidate operation section 73 show exclusive prefetching solve problem also general problem unnecessary invalidate operations 71 prefetching invalidation misses saw section 4 figure 3 clear limit effectiveness prefetching invalidation misses shared data none traditional uniprocessorbased prefetching strategies looked far successfully reduces less predictable invalidation misses largest component cpu misses workloads fact effectively prefetch nonsharing misses invalidation misses become critical performance application prefetching example seen figure 3 without prefetching invalidation misses represent 29 cpu miss rate mp3d prefetching represent 70 cpu miss rate 96 adjusted cpu miss rate words prefetching made applications much sensitive data sharing problem improve coverage externally caused therefore less predictable misses introduce redundant prefetches cache lines known writeshared redundant uniprocessor sense ie issued data would reside cache invalidations emulate prefetch algorithm prefetches writeshared data exhibits poor temporal locality premise longer shared cache line resided cache without accessed likely invalidated ran writeshared data trace 16line associative cache filter get firstorder approximation temporal locality selecting misses prefetching prefetched addition prefetches identified pref strategy labeled pws increases prefetching instruction overhead still less 4 improves coverage invalidation misses compiler algorithm could obtain effect using mowry et als 23 algorithm assuming much smaller cache size dealing data known write shared aaa aaaaaa aaa aaa aaa aaa aaa aa aa aa aa aaa aaaaaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa top src data nonsharing prefetched aaa aaa aaa aaa aaa aaa aaa aaa aa aa aa aa aa aa aa nonsharing prefetched invalidated prefetched aa aa aa invalidated prefetched prefetch progress mp3d pverify topopt prefetch strategy number figure 8 sources cpu misses enhanced writeshared data prefetching see figure 8 coverage invalidation misses improves considerably pws invalidation portion cpu miss rate drops significantly 20 91 average 56 drop result range bus utilizations data transfer latencies prefetching already viable improvements execution time achieved workloads seen figure 9 fastest bus 4 cycle results allow us see benefit improved prefetching writeshared data clearly isolation memory contention effects architecture speedup pws relative pref ranged 0 water 15 pverify pref 27 faster prefetching pws achieves 47 speedup prefetching cpu miss rates pws 11 64 lower pref one reason consistent reduction cpu misses writeshared algorithm although pws increases number prefetches significantly increase number prefetchinduced cpu conflict misses 72 restructuring shared data nondeterministic behavior interprocessor sharing predicting invalidation misses multiprocessors difficult predicting nonsharing misses algorithm uniprocessors multiprocessors section investigate extent reducing sharing traffic compilerbased shared data restructuring eliminate reduce need multiprocessor specific prefetching algorithms sharing traffic consists true false sharing amount true sharing inherent algorithm used program false sharing eliminated improved processor locality shared data false sharing occurs cache line shared two processor caches accessing different data one processor modifies data location causes invalidation others cache cache coherency maintained cache block basis record false sharing miss invalidation miss caused write another processor word local cache line local processor accessed 2 table 4 dubois et als definition false sharing8 calculates false sharing lifetime cache line accurate data bus latency075085095105 data bus latency075085095105 data bus latency075085095105 data bus latency075085095105 data bus latency et data locus pws locusroute mp3d water pverify topopt relative execution time relative execution time figure 9 execution times relative prefetching five workloads enhanced writeshared data prefetching shows benchmarks half invalidation misses could attributed false sharing even splash benchmarks handtuned processor locality although total amount false sharing benchmarks rather low show results 32byte cache line previous work28 11 demonstrates false sharing goes significantly larger block sizes 14 15 algorithm presented restructuring shared data reduce false sharing technique promise improving overall performance purpose study interested whether makes prefetching viable table 5 figure 10 show result prefetching strategies restructured topopt pverify programs improved less significantly already optimized processor locality programmerbased restructuring restructured programs certainly run faster original programs evidenced 6 definition use however measured small differences definition rate less concerned exactly much false sharing exists measured many sharing misses eliminate total total invalidation total false workload miss rate miss rate sharing miss rate pverify 452 118 111 topopt 416 190 139 locus 088 014 008 mp3d 200 058 019 water table 4 total invalidation false sharing miss rates prefetching prefetch cpu total total total workload discipline mr mr inval mr fs mr pverify np 426 426 012 009 pref 191 454 018 009 pws 191 459 018 010 topopt np 137 137 015 005 table 5 miss rates restructured programs data transfer latency 8 cycles decrease total miss rate pverify 67 decrease topopt reductions total miss rate achieved part significant reductions false sharing miss rate pverify reduction offset somewhat increase nonsharing miss rate topopt increase data locality achieved side effect data restructuring causing nonsharing miss rate also decrease use tracedriven simulation difficult accurately compare execution times different traces since always clear exactly fraction total execution time captured means cannot calculate raw performance improvement restructuring however measure incremental performance prefetching program restructured exactly makes restructuring interesting research since invalidation misses shown limiting factor performance prefetching strategies particularly pref figure 10 shows performance pref pws strategies applied two restructured programs figure results normalized execution times restructured programs without prefetching despite fact restructured programs less sensitive memory latencies due lower total miss rate experienced general greater improvements prefetching original programs seen comparing figure 9 figure 10 example pverify 4 cycles experiences speedups pref strategy 27 without restructuring pref relative np 69 prefrestr relative nprestr pws strategy 47 without 70 restructuring validates assertion invalidation misses limit effectiveness prefetching algorithms although prefrestr pwsrestr show improvement nprestr pref pws showed np programs improvement much significant prefrestr consequently distinction pref pws almost nonexistent restructured programs surprising data bus latency070090110 data bus latency relative execution time pverify topopt pwsrestr prefrestr relative execution time figure 10 execution times relative prefetching restructured program pverify topopt applying two prefetching strategies restructured programs pws algorithm restructuring attempting attack problem conclude restructuring effective significantly reducing invalidation miss rate simpler uniprocessorbased prefetch algorithm used place one tuned multiprocessor data sharing 73 exclusive prefetching prefetching strategies simulated thus far prefetches looked bus like reads illinois coherency protocol line read cache exclusive mode cache currently holds line otherwise cached shared mode read case prefetch write miss would fetch shared data shared mode write likely hit would require invalidate operation bus turns one bus operation read intent modify loads data invalidates one operation two avoided exclusive prefetch prefetches data cache exclusive mode invalidating copies caches migratory sharing one processor time typically accessing cache line exclusive prefetching saves bus operation however interprocessor contention cache lines exclusive prefetch writeshared data cause many invalidation misses problem unnecessary invalidate operations limited sharedmode prefetching general problem parallel applications wrex prefetching strategy targets extra invalidates cause sharedmode prefetching rdex strategy also targets general problem wrex prefetching strategy expected miss write algorithm issues exclusive prefetch line prefetch misses line brought cache exclusive mode invalidating copies caches prefetch hits cache bus operation initiated even cache line shared state illinois protocol reads cache lines arent currently another cache enter exclusive state immediately difference pref wrex miss occurs line shared among caches figure 11 see wrex prefetching strategy ineffective reducing execution time pref data bus latency0600801000 8 data bus latency0600801000 8 data bus latency0600801000 8 data bus latency0600801000 8 data bus latency rdexpws locusroute mp3d water pverify topopt relative execution time relative execution time figure 11 execution times relative prefetching five workloads exclusive prefetching four five applications topopt execution time dropped 2 primary reason write misses many writes preceded short distance read location words specific problem unnecessary invalidate operations caused prefetching write accesses evident one applications aggressive use exclusive prefetching attack general problem unnecessary invalidates unnecessary invalidate operation occurs readmodifywrite pattern results read miss followed write hit cache line currently shared another cache result two bus operations shared read followed invalidate preceded exclusive prefetch write hit require second bus operation long intervening accesses another processor cache line prefetch write compiler recognize readmodifywrite pattern short span instructions issue exclusive prefetch leading read miss mowry gupta22 take advantage programmerdirected prefetching study therefore rdex prefetching strategy modify wrex algorithm also exclusive prefetching read miss followed write word within 100 instructions chose recognize read aa aa aa aa aa aaaa aa aa aaaa aa aa aaaa aa aa aaaa aa aaa aaa aaa aaa aaaaaa aaa aaa aaa aaaaaa aaa aaa aaa aaaaaa aaa aaa aaa aaaaaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa nonsharing prefetched aaa aaa nonsharing prefetched invalidated prefetched aaa aaa invalidated prefetched prefetch progress mp3d pverify topopt number prefetch strategy figure 12 sources cpu misses exclusive prefetching modifywrite pattern words rather cache lines felt access patterns complex would difficult compiler recognize patterns cache line basis access patterns simple eg uniform access array elements results analysis word cache line would identical figure 12 see rdex like wrex reduce miss rates surprising since goal eliminate invalidate operations arent shown miss rate graphs effect reduced invalidation traffic seen fact execution time decreases without reduction miss rate rdex strategy significantly increase invalidation miss rate either significant cost issuing invalidations early terms additional invalidation misses guaranteed always true however exclusive prefetching needs done caution particular avoided interprocessor contention cache line expected high workload np pref wrex rdex rdex pws locusroute mp3d 424 438 438 376 333 pverify 577 620 620 605 563 topopt 524 599 594 594 598 water 051 055 054 048 047 table bus demand per access 8cycle data bus latency first application prefetching paper actually decrease number bus operations relative prefetching see table 6 figure 11 bus demands improved aggressive exclusive prefetching strategy consequence improved execution time bus speeds significance result shows prefetching win even memorysaturated multiprocessor data bus latency0600801000 8 data bus latency0600801000 8 data bus latency locusroute mp3d water pverify topopt relative execution time relative execution data bus latency0600801000 8 data bus latency rdexpwsvict normalized np rdexpwsvict normalized npvict pref normalized np figure 13 execution times applying rdex pws prefetching strategies system victim caches also see figure 11 rdex strategy used conjunction pws made write shared algorithm significantly effective two work well together two reasons first exclusive prefetching lowers bus demands thus making pws useful bus saturated effective wider range bus speeds second pws allows rdex attack component misses responsible vast majority unnecessary invalidate operationsinvalidation misses instance see rdex pws provides speedups high 28 rdex alone 34 pws alone combination provides speedups high 45 pref fact five applications rdex pws noticeably outperforms strategies even slowest memory subsystem 8 putting together demonstrated several architectural compileroriented techniques increase effectiveness prefetching section want see far weve come words use techniques together effective prefetching range memory architectures applications studying figure 13 applied several techniques particular applied combination pws rdex prefetching strategies architecture victim caches figure pref result normalized np first rdex pwsvict result normalized npvict allows us see overall increase effectiveness enhanced prefetching strategy independent benefits victim cache combinationrdex pwsvict achieved speedups due prefetching much significant original basic prefetcher pref high 83 addition performance degradations minimum speedup 6 results indicate right cache architecture careful application prefetching compiler performance improvements extensive covering wide range memory bandwidths cases significant third line figure 13 shows absolute performance gain achieved combination using victim cache composite prefetching algorithm applying architectural prefetching techniques achieved speedups 9 95 base architectures opposed maximum speedup 29 slowdowns much 6 base uniprocessorstyle prefetcher see none individual solutions provided dramatic improvements taken together total solution significant due fact different techniques attacked different aspects prefetching problem case actually synergy different approaches interpreting results remembered oraclebased prefetch algorithm likely underestimates prefetch instruction overhead overestimates ability identify nonsharing misses latter mixed effectmore prefetching minimizes cpu miss rate also maximizes bus demand due cache conflicts nonetheless results give us high confidence combination techniques prefetching made profitable across wide array multiprocessor memory architectures 9 summary conclusions multiprocessor system limited memory bandwidth compilerdirected prefetching algorithms guaranteed improve performance even successfully reduce cpuobserved miss rate increase load memory subsystem prefetchinduced cache conflicts unnecessary invalidate operations difficulty hiding invalidation misses slowdowns possible performance unpredictable found applications benefited prefetching applications suffered architecture varied changing bus speeds also show prefetching effective reducing effect nonsharing cache misses makes applications sensitive data sharing problem assortment architectural compiler techniques however drawbacks prefetching alleviated cache design forgiving cache conflicts directmapped cache case additional victim cache eliminate prefetchinduced conflict misses increase cpu miss rate load bus memory increases effectiveness prefetching makes application less sensitive prefetch distance allowing compiler freedom prefetch placement prefetch algorithm targeted invalidation misses well nonsharing misses greatly increase coverage prefetching sharedmemory multiprocessor prefetch algorithm makes effective use exclusive prefetching significantly reduce number invalidate operations thus reduce load memory subsystem restructuring shared data increase processor locality thus reduce number invalidation misses effective makes prefetching effective allows use simpler uniprocessororiented prefetching algorithm two techniques victim cache architectural improvement restructuring shared data compiler algorithm shown elsewhere improve performance independent prefetching also make prefetching effective applied although individual contribution one techniques dramatic combined effect several techniques significant combination techniques prefetching made viable across much wider range parallel applications memory subsystem architectures even memory subsystem represents bottleneck multiprocessor system acknowledgements authors would like thank jeanloup baer insightful comments paper several stages work reviewers journal provided many insightful comments improved presentation paper tor jeremiassen provided execution traces restructured executables r software prefetching data prefetching highperformance processors reducing memory latency via nonblocking prefetching caches performance study software hardware data prefetching schemes efficient architecture loop based data preloading data access microarchitectures superscalar processors compilerassisted data prefetching topological optimization multiple level array logic detection elimination useless misses multiprocessors simulation analysis data sharing shared memory multiprocessors simplicity versus accuracy model cache coherency overhead eliminating false sharing techniques inline tracing sharedmemory multiprocessor computer technology architecture evolving interaction computing perprocess summary sideeffect information static analysis barrier synchronization explicitly parallel programs improving directmapped cache performance addition small fullyassociative cache prefetch buffers dash proto type logic overhead performance symmetry multiprocessor system logic verification algorithms parallel implementation tolerating latency softwarecontrolled prefetching sharedmemory multiprocessors design evaluation compiler algorithm prefetching splash stanford parallel applications sharedmemory shared data placement optimizations reduce multiprocessor cache miss rates limitations cache prefetching busbased multiprocessor tr logic verification algorithms parallel implementation techniques efficient inline tracing sharedmemory multiprocessor lockupfree caches highperformance multiprocessors software prefetching highbandwidth data memory systems superscalar processors tolerating latency softwarecontrolled prefetching sharedmemory multiprocessors data access microarchitectures superscalar processors compilerassisted data prefetching computer technology architecture simplicity versus accuracy model cache coherency overhead reducing memory latency via nonblocking prefetching caches design evaluation compiler algorithm prefetching efficient architecture loop based data preloading detection elimination useless misses multiprocessors limitations cache prefetching busbased multiprocessor performance study software hardware data prefetching schemes data prefetching highperformance processors microprocessors users manual false sharing spatial locality multiprocessor caches dash prototype computing perprocess summary sideeffect information static analysis barrier synchronization explicitly parallel programs lockupfree instruction fetchprefetch cache organization lowoverhead coherence solution multiprocessors private cache memories splash stanford parallel applications sharedmemory simulation analysis data sharing shared memory multiprocessors ctr john mellorcrummey david whalley ken kennedy improving memory hierarchy performance irregular applications using data computation reorderings international journal parallel programming v29 n3 p217247 june 2001 aleksandar milenkovic achieving high performance busbased sharedmemory multiprocessors ieee concurrency v8 n3 p3644 july 2000 pablo ibez vctor vials jos l briz mara j garzarn characterization improvement loadstore cachebased prefetching proceedings 12th international conference supercomputing p369376 july 1998 melbourne australia john mellorcrummey david whalley ken kennedy improving memory hierarchy performance irregular applications proceedings 13th international conference supercomputing p425433 june 2025 1999 rhodes greece parthasarathy ranganathan vijay pai hazim abdelshafi sarita v adve interaction software prefetching ilp processors sharedmemory systems acm sigarch computer architecture news v25 n2 p144156 may 1997