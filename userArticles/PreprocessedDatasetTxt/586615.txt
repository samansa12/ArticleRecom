spatially adaptive splines statistical linear inverse problems paper introduces new nonparametric estimator based penalized regression splines linear operator equations data noisy local roughness penalty relies local support properties bsplines introduced order deal spatial heterogeneity function estimated estimator shown consistent weak conditions asymptotic behaviour singular values linear operator furthermore usual nonparametric settings shown attain optimal rates convergence good performances confirmed means simulation study b introduction statistical linear inverse problems consist indirect noisy observations parameter function generally interest problems occur many areas science genetics dna sequences mendel sohn rice 1982 optics astronomy image restoration craig brown 1986 biology natural sciences tikhonov goncharsky 1987 data linear transform original signal f corrupted noise k known compact linear operator defined separable hilbert space h supposed following l 2 0 1 space square integrable functions defined 01 ffl white noise unknown variance oe 2 problems also called illposed problems operator k compact consequently equation 1 inverted directly since k gamma1 bounded operator reader referred tikhonov arsenin 1977 seminal book illposed operator equations osullivan 1986 review statistical perspective illposed problems following restrict ourself integral equations kernel ks include deconvolution vast literature numerical analysis hansen 1998 neumaier 1998 references therein statistics dealing inverse problems eg wahba 1977 mendelsohn rice 1982 nychka cox 1989 abramovich silverman 1998 among others actually since model 1 inverted directly even data corrupted noise one regularize estimator adding constraint estimation procedure tikhonov arsenin 1977 regularization linear generally based windowed singular value decomposition svd k indeed since k compact admits following decomposition orthonormal bases h singular values sorted decreasing order 1 estimators f proposed literature based either finite rank depending sample size approximation k achieved truncating basis expansions obtained means svd adding regularization smoothing parameter eigenvalues makes k sequence filtering coefficients f j controls regularity tikhonov method f jk truncated svd method see hansen 1998 exhaustive review methods rate decay singular values indicates degree illposedness problem singular values decrease rapidly illposed problem nevertheless estimators based svd two main defaults one hand basis functions depend explicitly operator k function interest instance well known fourier basis singular functions k deconvolution problems provide parsimonious approximation true function smooth regions rapidly oscillates regions hand usual regularization procedures allow deal spatial heterogeneity function recovered several authors proposed spatially adaptive estimators based wavelet decomposition donoho 1995 abramovich silverman 1998 attain minimax rates convergence particular operators k homogeneous oper ators approach quite different relies spline fitting local roughness penalties spatially adaptive splines computed means knots selection procedures require sophisticated algorithms friedman 1991 stone et al 1997 denison et al 1998 paper address topic knots selection estimator proposed penalized regression splines whose original idea traces back osullivan 1986 ruppert carroll 1999 actually ruppert carrolls method con sists penalizing jumps function interior knots controlled smoothing parameter order manage highly variable part smooth part estimator arti cle similar approach proposed using fact bspline functions local supports derivative bspline order q combination two bsplines order able define local measures squared norm given order derivative function interest thus curvature estimator controlled locally means smoothing parameters associated local measures roughness asymptotic properties estimator given local penalties controlled local smoothing parameters whose values must chosen carefully practical situations order get accurate estimates generalized cross validation gcv criterion widely used nonparametric regression generally allows select good values smoothing parameter green silverman 1994 unfortunately gcv seems fail select effective smoothing parameter values framework adaptive splines inverse problems giving often un dersmoothed estimates investigation needed cope important practical topic beyond scope paper nevertheless small monte carlo experiment performed show potential new approach organization paper follows section 2 spatial adaptive regression splines estimator defined section 3 upper bounds rates convergence given particular case usual nonparametric framework also tackled spatially adaptive estimator shown attain optimal rates convergence section 4 simulation study compares behavior estimator penalized regression splines proposed osullivan 1986 finally section 5 gathers proofs splus programs carrying estimation available request 2 spatially adaptive spline estimates estimator proposed based spline functions lets briefly recall definition known properties functions suppose q k integers let qk space spline functions defined 0 1 order q q 2 k equispaced interior knots set qk set functions defined ffl polynomial degree interval ffl continuously differentiable 0 1 space qk known dimension q k one derive basis means normalized bsplines fb q 1978 dierckx 1993 functions non negative local support furthermore remarkable property bsplines derivative bspline order q expressed linear combination two bsplines order precisely kj jth normalized bspline qgamma1k b qk vector bsplines qk lets define qk weighted differentiation gives coordinates qgamma1k derivative function qk iterating process one easily obtain coordinates given order derivative function qk applying kq gammamthetakq matrix delta defined follows consider penalized least squares regression estimator penalty proportional weighted squared norm given order derivative functional coefficient effect give preference certain local degree smoothness using local support properties bsplines adaptive roughness penalty controlled local positive smoothing parameters ae may take spatial heterogeneity account penalized bsplines estimate f thus defined qk solution following minimization problem min 2r qkn j jth element kk denotes usual l 2 0 1 norm let n theta q c qk k q theta k q matrix whose generic element inner product two bsplines z 1b q ki tb q lets define ae c qgammamk ae delta ae diagonal matrix diagonal elements solution minimization problem 10 given naen vector r n elements remark 2 1 ae estimator defined 9 estimator proposed osullivan 1986 min 2r qkn furthermore usual nonparametric settings ie kind penalized regression splines already used different purposes kelly rice 1991 used nonparametrically estimate doseresponse curves cardot diack 1998 demonstrated could attain optimal rates convergence besse et al 1997 performed principal components analysis unbalanced longitudinal data cardot 2000 studied asymptotic convergence principal components analysis sampled noisy functional data remark 2 2 local penalty defined 10 may viewed kind discrete version continuous penalty defined follows local roughness continously controlled function aet 3 asymptotic results convergence results derived according seminorm induced linear operator k named knorm empirical norm kfk belongs null space k thus estimated norm allows us measure distance estimate recoverable part function f considered wahba 1977 lets define easily seen km space polynomial functions defined 0 1 degree less ensure existence convergence estimator need following assumptions regularity f repartition design points moments noise operator h2 ffl independent distributed ffl lets denote fn empirical distribution design se quence ng ae 0 1 suppose converges design measure f continuous bounded strictly positive density h 0 1 furthermore lets suppose exists sequence fdng positive numbers tending zero sup h4 kernel ks belongs l 2 0 1 theta 0 1 fixed function 7 ks continous function whose derivative belongs exists c 0 8g 2 km kkgk c kgk words assumption h5 means null space k contain non null polynomial whose degree less condition rather weak dealing deconvolution problems excludes operator equations differentiation assumption h3 norm l 2 0 1 df equivalent l 2 0 1 dt norm respect lebesgue measure assumptions h5 h3 ensure invertibility g nae hence unicity f provided n sufficiently large finally assumption h4 technical assumption ensures certain amount regularity operator k relaxed particular operator equations precisely implies k hilbertschmidt operator ie sequence singular values k lets define 0 state two main theorems article theorem 31 suppose n tends infinity kon hypotheses h1h5 f kn best upper bound f kn gamma2p obtained strong assumption decay ae n goes infinity actually ae small want however well known practical situations small value ae leads bad estimates undesirable oscillations thus empirical knorm considered effective criterion evaluate asymptotic performance estimator theorem 32 suppose n tends infinity kon one following upper f ae 6 remark 3 1 upper bounds empirical knorm different surprinsigly difference entirely caused bias term whereas one expect would result variance bounds obtained knorm error depend directly accurately empirical measure fn design points approximates true measure f furthermore larger amount regularization needed estimator convergent instance sequence dn decreases usual rate one choose ae n gammapm2p1 f consistent since ae ae 2 goes infinity choose ae ae n gammapm4p3m k n 14p3m asymptotic error f remark 3 2 bound may optimal particular operator equations since demonstration relies general arguments without assuming particular decay singular values k excepted implicit conditions imposed h4 thus must interpreted upper bound rates convergence assumptions h4 operator k rate convergence least one given 15 remark 3 3 consistency estimator eq 12 proposed osullivan 1986 direct consequence theorem 32 upper bounds rates convergence obtained 15 3 4 supposed interior knots equispaced theorem 32 remains true provided distance two successive knots satisfies asymptotic condition remark 3 5 usual nonparametric framework estimator qk defined follows min 2r qkn writing ae c qgammamk ae delta defined 11 demonstration convergence estimator immediate consequence convergence 9 since remains study asymptotic behaviour cn already done agarwall studden 1980 shown get h1 h2 h3 f usual optimal rates convergence attained k n 12p1 note conditions ae since cn well conditioned matrix 4 simulation study section small monte carlo experiment performed order compare behaviour two estimators defined section 2 simulated ns 100 samples composed noisy measurements convoluted function equidistant design points 0 1 z 1k ds gamma05 gamma05 gamma05 gamma05 x x 226f adapt pens b fig 1 convoluted function kf noisy observation b true function f adaptive spline osullivans penalized spline estimates median fit noise ffl gaussian distribution standard deviation 02 signaltonoiseratio 1 8 integral equation 18 practically approximated means quadrature rule function f drawn fig 1 flat regions oscillates others need choose smoothing parameter values compute es timates tuning parameters control regularity estimators numerous number knots order q splines order derivation involved roughness penalty vector smoothing parameters fortunately parameters importance control behaviour estimators indeed appears usual nonparametric settings crucial parameters elements ae regularization parameters number knots locations minor importance eilers marx 1996 besse et al 1997 provided numerous enough capture variability true function f number derivatives used 10 controls roughness penalty rather important since two different values lead two different estimators may mechanical interpretation value chosen practitioners fixed order splines case lot applications literature consider set equispaced knots 0 1 build estimator thus deal 44theta44 square matrices nevertheless number smoothing parameters remains large ae 2 r 42 face problem used method proposed ruppert carrol 1999 consists select subset n k n k k smoothing parameters ae including edges ae criterion gcv aic used select smoothing parameter values optimized according subset variables values smoothing parameters determined linear interpolation gammaae j subset smoothing parameters may chosen priori one priori knowledge spatial variability true function following consider n equispaced quantile smoothing parameters 14 cardot02pens adapt mse fig 2 boxplot mean square error estimation estimate median error 023 penalized spline 008 adaptive spline first consider generalized cross validation criterion order choose values ae computationally fast widely used automatic procedure proved efficient many statistical settings green silverman 1994 unfortunately seems fail one smoothing parameter systematically gives small smoothing parameter values lead undersmoothed estimates actually think would better consider penalized version gcv takes account number smoothing parameters future work go direction thus defined exact empirical risk order evaluate accuracy estimate smoothing parameter values chosen minimizing risk compare best attainable penalized splines defined 12 adaptive splines estimators samples boxplots empirical risk drawn fig 2 show use local penalties may lead substantial improvements estimate median error 008 adaptative spline whereas 023 penalized spline penalized spline estimates whose curvature controlled one parameter manage flat regions oscillatories regions function f thats undesirable oscillations penalized spline estimate appear intervals 0 02 07 1 whereas use local smoothing parameters allows cope efficiently problem see fig 1 5 proofs lets decompose mean square error squared bias variance term according x norm successively fk ng fkg f x f x f x lets study term separately technical lemmas gathered end section 51 bias term 511 empirical bias define qk naen 0 thermore easy show k solution minimization problem min 2r qkn criterion 20 written equivalently empirical knorm min kn theorem xii1 de boor 1978 regularity assumption h1 exists qk sup constant c depend k furthermore kn z z ck gamma2p 1 lim n 1n ds df 1 assumptions h3 h4 hand since f k solution 20 kn kn lemma 52 k finally empirical bias bounded follows f kn 512 knorm bias lets denote f ae solution following optimization problem min ae solution 23 using continuity k one gets lemma 52 function defined 21 writing remains study last term right side inequality complete proof lets denote k k q elements ae c qgammamk ae qk nae n iey easy check classical interpolation theory pursuing calculus begun 25 appealing lemmas 51 53 one gets ae nae ae nae od 2 ae 4 od 2 ae 6 completes proof 52 variance 521 empirical variance lets denote k nk qkthetaqk matrix elements exactly matrix n gamma1 0 embarking calculus lets notice qk ae c qgammamk ae delta nonnegative matrix hence tr nae k nk furthermore largest eigenvalues g gamma1 nae k nk less one thus q nonnegative matrix one trg gamma1 nae k nk et al 1998 lemma 65 thus h2 empirical variance term bounded follows f kn naen tr nae k nk tr nae k nk 522 knorm variance using decomposition 27 one obtains readily f naen tr hand one easily check nae nae k nk lemma 51 using equations 28 29 condition finally gets f 53 technical lemmas lemma 51 assumptions h3 h4 one proof let g function qk integration parts followed holder inequality invoking assumption h3 kn z 1jkgtj jdkgtjdt z 1k ds one hand lemma 53 kkgk similar arguments since assumption h4 dk bounded operator also get kd kgk matching previous remarks one finally obtains desired result lets denote n null space lemma 52 two positive constants c 1 c 2 ae c qgammamk ae delta u c 1 k ae c qgammamk ae delta u proof grammian matrix c qgammamk positive agarwall studden 1980 exists two positive constants c 3 c 4 easy check matrix ae c qgammamk ae positive largest eigenvalue proportional ae 2 k gamma1 smallest eigenvalue proportional ae remains study complete proof lets begin first point defined 7 furthermore writing weighted difference follows remembering delta obtained iterations differentiation process one gets completes proof first point lets suppose u 2 n since furthermore sum u expressed matrix way spatially adaptive splines 21 matrix l kind discretized laplacian matrix defined follows eigenvalues 2 1 gamma cos bill 1969 null space l spanned constant vector smallest non null eigenvalue proportional 2 k q gamma2 hence since smallest eigenvalue ae c qgammamk ae proportional ae gets desired result 1 proof complete iterating calculus lemma 53 ae nae proof lets denote k adjoint operator k 2 r qk kk kk inequality 31 one gets kc qk bounded since k continuous first point complete 22 cardot lets recall g ae c qgammamk ae delta decompose function lemma 52 one gets ae c qgammamk ae delta g2 c ae h5 one min thus smallest eigenvalue g ae satisfies min writing g nae gamma g get lemma 51 consequently smallest eigenvalue g nae satisfies min g nae ae r wavelet decomposition approaches statistical inverse problems asymptotic integrated mean square error using least squares bias minimizing splines simultaneousnonparametricregressions unbalanced longitudinal data convergence en moyenne quadratique de lestimateur de la regression par splines hybrides nonparametric estimation smoothed principal components analysis sampled noisy functions inverse problems astronomy practical guide splines automatic bayesian curve fitting curve surface fitting splines nonlinear solution linear inverse problems waveletvaguelette decomposition flexible smoothing bsplines penalties discussion multivariate adaptive regression splines discussion introduction matrices applications statistics nonparametric regression generalized linear models monotone smoothing application doseresponse curves assessment synergism deconvolution microfluometric histograms bsplines solving illconditioned singular linear systems tutorial regularization convergence rates regularized solutions integral equations discrete noisy data statistical perspective illposed inverse problems polynomial splines tensor product extended linear modeling solutions illposed problems practical approximate solutions linear operator equations data noisy local asymptotics regression splines confidence regions tr curve surface fitting splines simultaneous nonparametric regressions unbalanced longitudinal data solving illconditioned singular linear systems rankdeficient discrete illposed problems ctr herv cardot pacal sarda estimation generalized linear models functional data via penalized likelihood journal multivariate analysis v92 n1 p2441 january 2005