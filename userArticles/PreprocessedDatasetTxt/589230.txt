tensor methods large sparse unconstrained optimization tensor methods unconstrained optimization first introduced schnabel chow siam j optim 1 1991 pp 293315 described methods small moderatesized problems major contribution paper extension methods large sparse unconstrained optimization problems extension requires entirely new way solving tensor model makes methods suitable solving large sparse optimization problems efficiently present test results sets problems hessian minimizer nonsingular singular results show tensor methods significantly efficient reliable standard methods based newtons method b introduction paper describe tensor methods solving unconstrained optimization problem open set containing x assume f least twice continuously differentiable n large tensor methods unconstrained optimization general purpose methods primarily intended improve upon performance standard methods especially problems deficiency also intended least efficient standard methods problems r 2 fx nonsingular tensor methods unconstrained optimization base iteration upon fourth order model objective function fx current iterate rfx c r 2 fx c first second analytic derivatives f x c finite difference approximations tensor terms x c c 2 nthetanthetan v c 2 nthetanthetanthetan symmetric use notation rfx c delta consistent tensor notation c delta 3 also simplicity abbreviate terms form dd ddd dddd 2 3 4 respectively proceeding define tensor notation used nthetanthetan nthetanthetanthetan tensor terms selected model interpolates small number function gradient values previous iterations results c v c lowrank tensors crucial efficiency tensor method tensor method requires function derivative evaluations per iteration hardly storage arithmetic operations standard method based newtons method standard methods solving unconstrained optimization problems widely described literature general references topic include dennis schnabel 9 fletcher 11 gill murray wright 13 paper propose extensions standard methods use analytic finite difference gradients hessians standard method unconstrained optimization newtons method bases iteration upon quadratic model fx method defined r 2 fx c nonsingular consists setting next iterate x minimizer 13 ie distinguishing feature newtons method r 2 fx c nonsingular local minimizer x sequence iterates produced 14 converges quadratically x however newtons method generally linearly convergent best r 2 fx singular 14 methods based 12 shown reliable efficient standard methods small moderate size problems 18 test results obtained nonsingular singular problems improvement tensor method newtons method substantial ranging 30 50 iterations function derivative evaluations improvement even dramatic singular problems furthermore tensor method solves several problems newtons method fails solve tensor algorithms described 18 qrbased algorithms involving orthogonal transformations variable space algorithms effective minimizing tensor model hessian dense stable numerically especially hessian singular however efficient sparse problems destroy sparsity hessian due orthogonal transformation variable space preserve sparsity hessian developed entirely new way solving tensor model employs sparse variant cholesky decomposition makes new algorithms well suited sparse problems remainder paper organized follows x2 briefly review techniques used form tensor model introduced schnabel chow 18 x3 describe efficient algorithms minimizing tensor model hessian sparse xx4 5 discuss globally convergent modifications tensor methods unconstrained optimiza tion consist line search backtracking model trust region techniques high level implementation tensor method given x6 x7 describe comparative testing implementation based tensor method versus implementation based newtons method present summary statistics test results finally summary work discussion future research given x8 2 forming tensor model section briefly review techniques forming tensor model unconstrained optimization introduced 18 stated previous section tensor method unconstrained optimization bases iteration upon fourth order model nonlinear function fx given 12 choices c v c 12 cause third order term c delta 3 fourth order simple useful forms tensor terms selected tensor model interpolates function gradient information set p necessarily consecutive past iterates x remainder paper restrict attention 1 reasons choice performance tensor version allows p 1 similar overall constraining p 1 method simpler less expensive implement case derivation third fourth order tensor terms p 1 explained detail 18 interpolation conditions past point x given schnabel chow 18 choose c v c satisfy 21 22 first show interpolation conditions 21 22 uniquely determine c delta 3 v c delta 4 multiplying 22 yields let ff fi 2 defined 21 23 obtain following system two linear equations two unknowns ff fi2 ff defined system 2425 nonsingular therefore values ff fi uniquely determined hence interpolation conditions uniquely determine c delta 3 v c delta 4 since interpolation conditions choice c v c vastly underdetermined schnabel chow 18 choose c v c first selecting smallest symmetric v c frobenius norm fi determined 2425 substitute value v c 22 obtaining set n linear equations n 3 unknowns c j k 1 finally schnabel chow 18 choose smallest symmetric c v c frobenius norm satisfy equations 2627 min vc2 nthetanthetanthetan subject v c delta min nthetanthetan subject c delta solution 28 somega somega somega tensor v nthetanthetanthetan called fourth order rankone tensor use notationomega consistent 18 solution 29 somega somega b 210 notation nthetanthetan called third order rankone tensor j unique vector 210 satisfies 26 given determined minimum norm problems 29 28 rank 2 1 respectively key form store solve tensor model efficiently whole process forming tensor model requires 2 arithmetic operations storage needed forming storing tensor model total 6n information refer 18 3 solving tensor model hessian sparse section give efficient algorithms finding minimizer tensor model 12 hessian sparse substitution values c v c 12 results tensor model stated x2 consider case tensor model interpolates fx rfx previous iterate ie 1 generalization p 1 fairly straightforward constraint mainly motivated computational results allow p 1 test results showed almost improvement case 1 tensor method therefore considerably simpler cheaper terms storage cost per iteration 31 case 1 hessian nonsingular show minimization 31 reduced solution third order polynomial one unknown plus solution three systems linear equations involve coefficient matrix r 2 fx c conciseness use notation necessary condition local minimizer 31 derivative tensor model respect must zero yields first premultiply equation 32 sides obtain cubic equation fi unknowns premultiply equation 32 b sides obtain another cubic equation fi unknowns fi thus obtain system two cubic equations two unknowns fi solved analytically show compute solutions system two cubic equations two unknowns computing solutions single cubic equation unknown fi let first calculate value function fi using equation 33 ie note denominator equation 35 equal zero either assume fi 6 0 otherwise tensor model would reduced newton model would quadratic fi therefore thus real valued minimizers tensor model 31 may exist 0 easy check order defined cannot zero fi 6 0 w 6 0 substitute expression equation 34 obtain third order polynomial one unknown fi roots equation 36 computed analytically substitute values fi equation 35 calculate values simply substitute values fi equation 32 obtain values major cost whole process calculation h compute values determine potential minimizers criterion select values guarantee descent path x c x c model among selected steps choose one closest current iterate x c euclidean norm sens tensor model minimizer use standard newton step step direction current iteration 32 case 2 hessian rank deficient hessian matrix rank deficient transform tensor model given 31 following procedure let ffi new unknown substituting expression 31 yields following tensor model function ffi 2 ds b let obtain modified tensor model advantage transformation matrix h likely nonsingular rank r 2 fx c least n gamma 1 necessary sufficient condition h nonsingular given following lemma let g h denote rfx c r 2 fx c respectively lemma 31 let h 2 nthetan css nonsingular 6 6 6 6 6 4 h cs cs nonsingular note submatrix premultiplied constant c symmetrize augmented matrix proof prove exists exist h cs cs suppose first h conversely exists v w satisfying 39 otherwise contradicts 39 thus h singular singular corollary 32 let h 2 nthetan css nonsingular h cs full row rank proof follows lemma 31 lemma 33 let h 2 nthetan css nonsingular h cs full row rank proof part follows corollary 32 assume h cs full row rank since h rank nthetangamma1 full column rank since h cs full row rank v h 2 full column rank 310 equivalent v thus n theta n matrix nonsingular analogously n theta n matrix nonsingular therefore nonsingular 2 ffi local minimizer 38 derivative tensor model 38 respect ffi must zero yields premultiplying equation 312 sides results cubic equation fi two unknowns fis fis fis premultiplication equation 312 b sides yields another cubic equation fi two unknowns fi therefore obtain system two cubic equations two unknowns fi solve analytically since equation 313 linear compute function fi substitute expression equation 314 obtain equation one unknown fi let g denominator equation 315 equal zero either equation 313 would quadratic fi therefore hence real valued minimizers tensor model 38 may exist 1 straightforward verify 314 defined reduces following cubic equation fi calculated expressions fi equation 316 substitute following equation obtained equation 314 neither equation 314 obtain fiw gamma2 third order polynomial one unknown fi roots equation 317 computed analytically determine values fi substitute equation 315 calculate corresponding values simply substitute values fi equation 312 obtain values ffi dominant cost whole process computation similar nonsingular case minimizer ffi selected exists descent path current point x c x c closest x c obtain tensor step set appropriate choice step used previous iteration simply right scale solve linear systems form nthetan sparse use augmented matrix defined lemma 31 write h cs cs x n 318 sparse factorized efficiently long last row column pivoted last iterations fact combine nonsingular singular cases factorizing h shift factorization augmented matrix h discovered singular rank n gamma 1 however use schur complement method obtain solution augmented matrix updating solution system b choice motivated fact schur complement method simpler convenient use factorization augmented matrix describe updating scheme x6 schur complement method shows rank deficient case rare practice h rank less use standard newton step step direction current iteration 4 line search backtracking techniques line search global strategy used conjunction tensor method large sparse unconstrained optimization similar one used nonlinear equations 4 6 strategy shown successful large sparse systems nonlinear equations also found superior approach used schnabel chow 18 main difference two approaches always tries full tensor step first provides enough decrease objective function terminate otherwise find acceptable next iterates newton tensor directions select one lower function value next iterate schnabel chow hand always find acceptable next iterates newton tensor directions choose one lower function value next iterate practice approach almost always requires fewer function evaluations retaining efficiency iteration numbers global framework line search methods unconstrained minimization given algorithm 41 algorithm 41 global framework line search methods unconstrained minimization let x c current iterate tensor step n newton step minimizer tensor model found else find acceptable x n newton direction n using algorithm a631 page 325 9 find acceptable x tensor direction using algorithm a631 page 325 9 fx n else endif endif else find acceptable x n newton direction n using algorithm a631 page 325 9 endif 5 model trust region techniques two computational methods generally used approximately solving trust region problem based standard model subject jj jj ffi c current trust region radius locally constrained optimal hook step dogleg step ffi c shorter newton step locally constrained optimal step 16 finds c jj c takes dogleg step modification trust region algorithm introduced powell 17 however rather finding point curve c approximates curve piecewise linear function subspace spanned newton step steepest descent direction gammarf x c takes x point approximation jj x gamma x c eg 9 details unfortunately two methods hard extend tensor model fourth order model trust region algorithms based 519 well defined always possible find unique point x curve jj x gamma x c additionally value fx c rfx c along curve c monotonically decreasing x c x n makes process reasonable properties extend tensor model fourth order model may convex furthermore analogous curve c expensive compute reasons consider different trust region approach tensor methods trust region approach discussed section twodimensional trust region step subspace spanned steepest descent direction tensor standard step main reasons lead us adopt approach easy construct closely related dogleg type algorithms subspace step may close optimal trust region step algorithms practice byrd schnabel shultz 7 shown unconstrained optimization using standard quadratic model analogous twodimensional minimization approach produces nearly much decrease quadratic model optimal trust region step almost cases twodimensional trust region approach tensor model computes approximate solution subject jj jj performing twodimensional minimization subject jj jj g tensor step steepest descent direction respectively ffi c trust region radius approach always produce step reduces quadratic model least much dogleg type algorithm reduces piecewise linear curve subspace iteration tensor algorithm trust region method either solves 520 minimizes standard linear model twodimensional subspace spanned standard newton step steepest descent direction decision whether use tensor standard model made using following criterion minimizer tensor model found rfx c selected trust region algorithm else selected trust region algorithm endif define twodimensional trust region step tensor methods show convert problem subject jj jj unconstrained minimization problem first make g orthogonal performing householder transformation normalize g obtain since subspace spanned tensor step steepest descent direction written square l 2 norm expression set ffi 2 obtain following equation fi function ff substituting expression fi 525 resulting 521 yields global minimization problem one variable ff given 526 bellow thus problems 521 equivalent let g hg c ff transform problem subject jj jj unconstrained minimization problem use procedure described show 527 equivalent following global minimization problem one variable ff c algorithm 51 twodimensional trust region tensor methods let tensor step n standard step x c current iterate x next iterate steepest descent direction ffi c current trust region radius given 523 524 respectively n obtained analogous way applying transformations 522 523 1 tensor model selected solve problem 526 using procedure described algorithm 34 6 else fstandard newton model selectedg solve problem 528 using procedure described algorithm 34 6 endif 2 tensor model selected ff global minimizer 526 else fstandard newton model selectedg ff global minimizer 528 endif 3 f check new iterate update trust region radiusg pred global step successful else decrease trust region go step 1 endif pred pred standard newton model selected methods used adjusting trust radius steps given algorithm page 338 9 initial trust radius supplied user set length initial cauchy step 6 high level algorithm tensor method section present overall algorithm tensor method large sparse unconstrained optimization algorithm 61 high level description iteration tensor method described xx35 summary test results implementation presented x7 algorithm 61 iteration tensor method large sparse unconstrained optimization let x c current iterate tensor step n newton step 1 calculate rfx c decide whether stop 2 calculate r 2 fx c 3 calculate terms c v c tensor model tensor model interpolates fx rfx past point 4 find potential minimizer tensor model 31 5 find acceptable next iterate x using either line search twodimensional trust region global strategy go step 1 step 1 gradient either computed analytically approximated algorithm a563 given dennis schnabel 9 step 2 hessian matrix either calculated analytically approximated graph coloring algorithm described 8 note crucial supply analytic gradient finite difference hessian matrix requires many gradient evaluations otherwise methods described paper may practical inexact type methods may preferable procedures calculating c v c step 3 discussed x2 step 4 calculates described xx 34 newton step n also computed product minimization tensor model newton step n modified newton step r 2 fx c safely positive definite 0 otherwise obtain perturbation use modification ma27 10 advocated gill murray ponceleon saunders 12 method first compute ldl hessian matrix using ma27 package change block diagonal matrix e modified matrix block diagonal positive definite guarantees el positive definite well note hessian matrix modified already positive definite tensor newton algorithms terminate jj rfx c jj another implementation issue deserves attention find solution augmented system 318 hessian matrix rank deficient use schur complement method update solution x obtained solving hx b requires h must full rank thus modifications necessary order method work modified factorization phase ma27 able detect row column indices first pivot less equal given tolerance tol note rank hessian matrix less skip whole updating scheme perturb matrix described previous paragraph also modified solve phase ma27 whenever zero pivot corresponding solution component set zero way solution solution h e matrix h minus row column singularity occurred since components remaining one also component corresponding zero pivot set 0 afterwards obtain solution augmented system using schur complement method coefficient matrix matrix h augmented two rows columns ie n 1st row column ones singularity detected n2nd row column cs cs respectively schur complement method implemented first invoking ma39ad 1 form schur complement h extended matrix 2 2 lower right submatrix c lower left 2 n submatrix b upper right n 2 submatrix augmented matrix schur complement factorized qr factors next ma39bd 1 solves extended system 318 using following wellknown scheme 1 solve 2 solve 3 solve 4 7 test results tested tensor newton algorithms variety nonsingular singular test problems following present discuss summary statistics test results computations performed sun using double precision arithmetic first tested program set unconstrained optimization problems 3 minpack2 2 collections problems nonsingular hessians solution also created singular test problems proposed 4 19 modifying nonsingular test problems cute collection follows let function minimize f number element functions many cases f minimizer x f 0 x nonsingular according 4 19 create singular systems nonlinear equations 71 forming nthetak full column rank 1 k n hence k unconstrained optimization simply need define singular function 73 know r 2 using 72 73 created two sets singular problems r 2 respectively using respectively reason choosing unit vectors columns matrix mainly preserve sparsity hessian transformation 72 test problems used standard line search backtracking strategy test problems exception rank problems ran analytic gradients hessians provided cute minpack2 collections rank test problems modified analytic gradients provided cute collection take account modification 72 hand used graph coloring algorithm 8 evaluate finite difference approximation hessian matrix summary test problems whose hessians solution ranks n presented table 1 descriptions test problems detailed results given appendix table 1 columns better worse represent number times tensor method better worse respectively newtons method one gradient evaluation tie column represents number times tensor standard methods required within one gradient evaluation set problems summarize comparative costs tensor standard methods using average ratios three measures gradient evaluations function evaluations execution times average gradient evaluation ratio geval total number gradients evaluations required tensor method divided total number gradients evaluations required standard method problems measure used average function evaluation feval execution time time ratios average ratios include problems successfully solved methods excluded cases tensor standard methods converged different minimizer however statistics better worse tie columns include cases one two methods converges exclude cases methods converge also excluded problems requiring number gradient evaluations less equal 3 methods finally columns ts st show number problems solved tensor method standard method number problems solved standard method tensor method respectively improvement tensor method standard method problems rank averaging 48 function evaluations 52 gradient evaluations 59 execution times due part rate convergence tensor method faster newtons method known linearly convergent constant3 problems rank improvement tensor method standard method also substantial averaging 30 function evaluations 37 gradient evaluations 34 execution times test results obtained nonsingular problems tensor method 9 worse standard method function evaluations 31 33 better gradient evaluations execution times respectively main reason tensor method requiring average function evaluations standard method problems full tensor step provide sufficient decrease objective function therefore tensor method perform line search newton tensor directions causes number function evaluations required tensor method inflated result intend investigate possible global frameworks line search methods could potentially reduce number functions evaluations tensor method obtain experimental indication local convergence behavior tensor newton methods problems rankr 2 fx examined sequence ratios produced newton tensor methods problems ratios typical problem given table 2 almost cases standard method exhibits local linear convergence constant near 2 consistent theoretical analysis local convergence rate tensor method faster typical final ratio around 001 whether superlinear convergence remains determined done similar experiments problems rankr 2 fx tensor method show fasterthanlinear convergence rate enough information since tensor method solved total four nonsingular problems five rank 7 rank newtons method failed solve reverse never occurred clearly indicates tensor method likely robust newtons method overall results show extra information function gradient past step direction quite useful achieve advantages tensor methods 8 summary future research paper presented efficient algorithms solving large sparse unconstrained optimization using tensor methods described new methods minimizing tensor model efficient problems hessian matrix large sparse implementations using tensor methods shown considerably efficient especially problems rank tensorstandard pbs solved average ratiotensorstandard table 1 summary cute minpack2 test problems using line search iteration k standard method tensor method 9 0600 0126 14 0969 22 0896 26 0667 28 0666 table 2 speed convergence brybnd problem rankr 2 fx modified 72 started x 0 ratios second third columns defined 7 hessian matrix small rank deficiency solution typical gains standard newton methods range 40 50 function gradient evaluations computer time size consistency efficiency gains indicate tensor method may preferable newtons method solving large sparse unconstrained optimization problems analytic gradients andor hessians available firmly establish conclusion additional testing required including test problems large size sparse problems function gradient expensive evaluate finite difference approximation hessian matrix graph coloring algorithm 8 may costly hence quasinewton methods may preferable use case methods involve lowrank corrections current approximate hessian matrix currently attempting extend tensor methods quasinewton methods large sparse unconstrained minimization problems also considered solving large sparse structured unconstrained optimization problems using tensor methods variant explore possibility using exact third fourth order derivative information calculation derivatives simplified using concept partial separability structure already proven useful building quadratic models large scale nonlinear problems 15 however calculation minimizer exact tensor model problematic need solve sparse system nonlinear equations obvious approach solve equations use newtonlike method method characterized approximation jacobian used newton process simple idea use fixed jacobian step advantage jacobian already obtained current tensor iteration however potential slow convergence scheme may make cost tensor iteration prohibitive currently investigating possible approaches modified newtons method approximated jacobian matrix incorporate useful information iterative method nonlinear gmres work cooperation nick gould 5 reported near future almost done implementation testing twodimensional trust region global strategy described x5 work reported forthcoming paper also implementing algorithms discussed paper software package package uses one past point formation tensor terms makes additional cost storage tensor method standard method small package available soon acknowledgments thank professor bobby schnabel suggestions minimize tensor model hessian rank deficient nick gould discussing number implementation issues tatung chow reviewing first draft paper cerfacs colleage jacko koster numerous suggestions r minpack2 test problem col lection cute constrained unconstrained testing environment solving large sparse systems nonlinear equations nonlinear least squares problems using tensor methods sequential parallel computers tensor methods largescale unconstrained opti mization tensolve software package solving systems nonlinear equations nonlinear least squares problems using tensor methods approximation solution trust region problem minimization twodimensional subspaces estimating sparse hessian matrices numerical methods unconstrained optimization nonlinear equations set fortran subroutines solving sparse symmetric sets linear equations practical method optimization preconditioners indefinite systems arising optimization nonlinear least squares problems practical optimization analysis newtons method irregular singularities unconstained optimization partially separable functions levenbergmarquardt algorithm implementation theory new algorithm unconstrained optimization tensor methods unconstrained optimization using second derivatives tensor methods nonlinear equations tr