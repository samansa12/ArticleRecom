new family online algorithms category ranking describe new family topicranking algorithms multilabeled documents motivation algorithms stems recent advances online learning algorithms algorithms present simple implement time memory efficient evaluate algorithms reuters21578 corpus new corpus released reuters 2000 corpora algorithms present outperform adaptations topicranking rocchios algorithm perceptron algorithm also outline formal analysis algorithm mistake bound model knowledge work first report performance results entire new reuters corpus b introduction focus paper problem topic ranking text documents use reuters corpus release 2000 running example corpus hundred dierent topics document corpus tagged set topics relevant content instance document late august 1996 discusses bill bill clinton increase minimum wage whole 90 cents document associated 9 topics four labour economics unemployment retail sales example shows semantic overlap topics given feed documents reuters newswire task topic ranking concerned ordering topics according relevance document independently framework use paper supervised learning framework receive training set documents provided set relevant topics given labeled corpus goal learn topicranking function gets document outputs ranking topics machine learning community setting often referred multilabel classication problem motivation machine learning algorithms problem stem decision theoretic view permission make digital hard copies part work personal classroom use granted without fee provided copies made distributed profit commercial advantage copies bear notice full citation first page copy otherwise republish post servers redistribute lists requires prior specific permission andor fee sigir02 august 1115 2002 tampere finland namely output algorithms predicted set relevant topics quality predictions measures successful identifying set relevant topics paper combine techniques statistical learning theory natural goals information retrieval tasks adopting generalizing techniques online prediction algorithms particular task topic ranking starting point perceptron algorithm 8 despite age simplicity perceptron algorithm variants proved surprisingly eective broad range applications machine learning information retrieval see instance 3 6 references therein original perceptron algorithm designed binary classication prob lems family algorithms borrows core motivation perceptron algorithm largely generalizes complex problem topic ranking since family algorithms uses multiclass multilabel feedback refer various variants mmp algorithm learning algorithms multilabeled data devised machine learning community two notable examples multilabel version adaboost called adaboostmh 10 multilabel generalization vap niks support vector machines elissee weston 2 two multilabel algorithms take general approach reducing multilabel problem multiple binary problems comparing pairs labels starting point similar use implicit reduction pairs yet using simple precomputation mmp much simpler implement use svm adaboostmh note however contrast adaboostmh svm designed batch settings examples given advance mmp like perceptron used online settings examples presented one time experiments two corpora reuters suggest mmp oer viable alternative existing algorithms used processing massive datasets 2 problem setting discussed text processing task paper concerned category topic ranking problem given access stream documents document associated zero categories predened set topics denote set possible categories number dierent topics k new distribution reuters corpus refer reuters2000 102 dierent topics reuters21578 consists 91 dierent topics since semantic overlap topics document typically associated one topic formally document labeled set relevant topics reuters2000 corpus average size 32 reuters21578 average size 124 say topic r also referred class category relevant given document r set relevant topics machine learning community setting also often referred multilabel classication problem numerous dierent information ltering routing tasks practical use multilabel problems focus paper design analysis implementation categoryranking algorithms given document algorithms consider return list topics ranked according relevance however discussed feedback document set thus topics document training corpus ranked rather marked relevant nonrelevant document represented using vector space model 9 vectors r n denote document vector representation topicranking algorithms discuss paper use mechanism algorithm maintains set k prototypes wk analogous representation documents prototype vector specic vectorspace representation used based pivoted length normalization singhal et al 12 description deferred sec 42 focus paper new family algorithms builds prototype examples ie corpus documents assigned set relevant topics set prototypes induce ranking topics according similarity vector representation document given document x innerproduct wk x induce ordering relevance level topic say topic r ranked higher topic x ws x given feedback ie set relevant topics document x say ranking induced prototypes perfect relevant topics ranked higher nonrelevant topics perfect ranking pair topics r 2 scores induced wr higher one induced ws ws x measure quality perfect topicranking size gap lowest score among relevant topics highest score among nonrelevant topics min r2y f f ws adopting terminology used learning theory refer quantity margin document x respect set relevant topics prototypes wk clearly perfect ranking document implies margin positive margin also computed prototypes induce perfect ranking case margin negative illustration margin given fig 1 illustration show margin case perfect ranking left nonperfect one right cases 9 dierent topics relevant topics marked circles nonrelevant squares value margin length arrow positive values denoted arrow pointing negative figure 1 illustration notion margin perfect ranking left nonperfect ranking margin arrow pointing notion margin rather implicit algorithms discuss paper however plays important role formal analysis algorithms 3 categoryranking algorithms mmp descendant perceptron algorithm online algorithm gets example outputs rank ing updates hypothesis maintains set prototypes wk update prototypes takes place predicted ranking prefect online algorithms become especially handy training corpus large since require minimal amounts mem ory case batch training ie training corpus provided advance example example need augment online algorithm wrapper several approaches proposed adapting online algorithms batch settings detailed discussion given 3 approach take paper simplest implement run algorithm online fashion provided training corpus use nal set topic prototypes obtained single pass data topicranking hypothesis leave sophisticated schemes future research description omit index document set relevant topics denote x respectively given document x dene errorset induced ranking wk respect relevance topics set categories inconsistent predicted ranking formally errorset dened x ws xg clearly predicted ranking perfect e empty size e large natural question arises type loss one try minimize whenever e empty describe three dierent losses yield simple ecient algo rithms loss functions stem learning theoretic insights also exhibit good performance respect common information retrieval performance measures nice property three losses encapsulated single update scheme prototypes rst loss indicator function equals 1 another way loss simply counts number times topicranking per fect second loss size e therefore examples ranked perfectly suer zero loss rest initialize set loop obtain new document rank topics according wk x obtain set relevant topics ranking x perfect 1 r 2 set ws x 2 r 62 set ws x 3 set normalization factor r nr loss1 4 update r 2 c 5 update r 62 c return wk figure 2 topicranking algorithm examples suer losses proportion poorly predicted rankings perform terms number disordered topics third loss normalized version second loss namely jejjy yjjyj since maximal size error set jy yjjyj third loss bounded one document rank prediction error mmp moves prototypes whose indices constitute errorset prototypes correspond set relevant topics moved toward x correspond nonrelevant topics moved away x motivation prototypeupdate stems perceptron algorithm straightforward verify form update increase value innerproducts x prototypes subset relevant categories similarly decrease rest innerproducts put another way update prototypes geared towards increasing rankingmargin associated example thus reducing ranking losses amount move prototype depends ranking type loss employ dene nr number topics wrongly ranked respect topic r r index relevant topic r 2 nr number nonrelevant topics ranked r analogously r index nonrelevant topic nr number relevant topics r prototype wr moved direction x proportion nr values computed normalize scaling factor denoted c value c depends version loss used scaling factor c ensures sum nr equals loss trying make small rst loss second loss third loss summarize topic ranking perfect prototype wr corresponding wrongly ranked relevant topic changed nrcx analogous modication made wrongly ranked prototypes set nonrelevant topics pseudo code describing algorithm given fig 2 analyze performance algorithm need use notion margin generalizing notion margin one document entire corpus dene margin denoted set prototypes wk corpus minimal margin attained documents min r2y f f ws xg purpose analysis assume total norm prototypes 1 1 assumption satised global normalization prototypes change induced ranking ready state main theorem shows mmp make bounded number mistakes compared set prototypes constructed hindsight obtaining examples achieves perfect ranking corpus theorem 1 mistake bound let input sequence mmp kg denote assume exists set prototypes k unit norm achieves perfect ranking entire sequence margin mmp obtains following bounds 2 due space restrictions proof omitted manuscript 4 experiments section describe experiments performed compare three variants mmp simple extension perceptron algorithms rocchios algorithm start description datasets used experiments 41 datasets evaluated algorithms two text corpora corpora provided reuters reuters21578 documents corpus collected reuters newswire 1987 data set available httpwwwdaviddlewiscomresources number fraction data set number fraction data set figure 3 distribution number relevant topics reuters21578 left reuters2000 used modapte version corpus preprocessed documents follows words converted lower case digits mapped single token designating digit non alphanumeric characters discarded also used stoplist remove frequent words number dierent words left preprocessing 27747 preprocessing corpus contains 10789 documents associated one topics number dierent topics modapte version reuters21578 91 since corpus relatively small size used 5fold cross validation experiments use original partition training test sets document reuters21578 corpus multilabeled practice number documents relatively small 80 documents associated single topic left hand side fig 3 show distribution number relevant topics average number relevant topics per document 124 reuters2000 corpus contains 809 383 documents collected reuters newswire year period 1996 0820 19970819 since corpus large used rst two thirds corpus training remaining third evaluation training set consisted documents posted 19960820 1997 0410 resulting 521 439 training documents size corpus used evaluation 287 944 preprocessed follows converted uppercase characters lowercase replaced non alphanumeric characters whitespaces discarded words appeared training set number dier ent words remained preprocessing 225 329 document collection associated zero topics 103 dierent topics entire cor pus however 102 appear training set remaining category marked gmil millennium sues tags 5 documents test set therefore discarded category unlike reuters21578 corpus document corpus tagged multiple topics 70 documents associated least three dierent topics average number topics associated document 320 distribution number relevant topics per document appears right handside fig 3 42 document representation algorithms evaluated use document representation implemented pivoted length normalization singhal et al 12 termweighting al gorithm algorithm considered one eective algorithms document ranking retrieval brie outline pivoted length normalization let l denote number times word term indexed l appears document indexed let denote number unique words appear document indexed l number documents term indexed l appears total number documents corpus denoted using denitions idf weight word indexed l logmu l average frequency terms appearing document avgd l l l average frequency number unique terms documents using denitions tf weight word indexed l appears document indexed slope parameter among 0 1 set 03 leads best performance experiments reported 12 43 algorithms comparison addition three variants mmp also implemented two algorithms rocchios algorithm 7 perceptron algorithm mmp algorithms use pivoted length normalization vector space representation employ form category ranking using set prototypes wk rocchio implemented adaptation rocchios method adapted ittner et al 5 text categoriza tion variant rocchio set prototypes vectors wk set follows r i2rr x l r x l rr set documents contain topic r one relevant topics r c r complement ie documents r one relevant topics following parameterization 5 set 4 last suggested amit singhal private communication normalize prototypes unit norm perceptron also implemented perceptron algo rithm since perceptron algorithm designed binary classication problems decomposed multilabel problem multiple binary classication problems topic r set positive examples constitute docu ments indices rr r c r set negative exam ples ran perceptron algorithm binary problems separately independently therefore obtained set prototypes wk output corresponding output perceptron algorithm round number averaged cumulative loss 1 perceptron mmp l1 mmp l2 mmp l3 averaged cumulative loss 2 perceptron mmp l1 mmp l2 mmp l3 round number averaged cumulative loss 3 perceptron mmp l1 mmp l2 mmp l3 figure 4 roundaveraged performance measures function number training documents processed reuters21578 loss1 left loss2 middle loss3 right averaged cumulative loss 1 perceptron mmp l1 mmp l2 mmp l3 averaged cumulative lossperceptron mmp l1 mmp l2 mmp l3 round number averaged cumulative lossperceptron mmp l1 mmp l2 mmp l3 figure 5 roundaveraged performance measures function number training documents processed reuters2000 loss1 left loss2 middle loss3 right logscale used xaxis 44 feature selection datasets number unique terms preprocessing stage described still large 27 747 words reuters21578 225 339 words reuters 2000 since used crossvalidation reuters21578 actual number unique terms slightly lower 25 061 average yet still relatively large therefore employed feature selection corpora used weights prototypes generated adaptation rocchios algorithm described mean feature selection topic sorted terms according weights assigned rocchio took topic maximum hundred terms top portion 25 sorted lists ensures topic least 100 terms combined set selected terms used feature set various algorithms size 3 468 reuters21578 9 325 reuters2000 average number unique words per document crossvalidated training sets reduced 49 37 reuters21578 137 121 reuters2000 feature selection stage applied algorithms representation documents restricted selected terms 45 evaluation measures since paper concerned classical ir methods recent statistical learning algorithms used two sets performance measures compare eective ness algorithms first evaluated algorithm respect three losses employed variants mmp second set performance measures based measures used evaluating document retrieval systems specically performance measure used evaluation recall r precision r oneerror coverage averageprecision maxf1 also provide precisionrecall graphs give formal descriptions evaluation measures used experiments describing evaluation measures used experiments need following denition given set prototypes xing example x dene rankx r ranking topic indexed r list topics sorted according prototypes thus rankvalue topranked topic 1 bottomranked topic k performance measure test set average value measure documents set recall recall value r ratio number topics set relevant topics whose rank r size relevant topics precision precision value r ratio number topics set relevant topics whose rank r position r oneerr oneerror abbreviated oneerr takes values 0 1 indicated whether topranked element algorithm loss1 100 loss2 loss3 100 rocciho perceptron 1571 487 313 mmp l 3 1913 092 060 table 1 comparison performance various algorithms testset different learningtheoretic performance measures reuters21578 algorithm oneerr 100 coverage avgp maxf1 rocciho 1448 129 090 085 perceptron 959 427 091 089 mmp l 3 1451 096 090 084 table 2 comparison performance various algorithms testset dierent retrieval performance measures reuters21578 belongs set relevant topics zero error error 1 therefore average oneerr test corpus ects fraction documents topranked topic list relevant topics coverage coverage value ects far need go rankedlist topics order retrieve relevant topics convenience denition implies documents single relevant topic perfect ranking achieves coverage zero avgp average precision abbreviated avgp name implies average precision taken positions relevant topics r2y average precision perhaps common performance measure document retrieval maxf1 f1 value r dened maxf1 maximal f1 value obtained information f1 performance measure see 13 would like note natural relations learning theoretic losses loss1 loss3 retrievalbased performance measures instance whenever oneerr 1 also since topranked algorithm loss1 100 loss2 loss3 100 rocciho 7071 1242 333 perceptron 3886 1043 264 mmp l 3 3457 290 075 table 3 comparison performance various algorithms testset different learningtheoretic performance measures reuters2000 algorithm oneerr 100 coverage avgp maxf1 rocciho 2442 989 073 063 perceptron 604 945 087 086 mmp l 3 649 398 091 087 table 4 comparison performance various algorithms testset dierent retrieval performance measures reuters2000 topic one relevant topics clearly ranking perfect thus oneerr loss1 also coverage loss2 coincide 46 results let us rst discuss performance online algorithms variants mmp perceptron training sets fig 4 fig 5 show performance respect loss1 loss2 loss3 reuters21578 reuters2000 respectively round new document compute cumulative loss algorithms divided number documents processes ie round number loss im note used logscale xaxis expected see gures version mmp performs well respect loss measure trained addition perceptron performs well respect loss1 especially reuters 21578 relatively good performance perceptron respect loss1 might attributed fact reuters21578 corpus practically singlelabeled thus loss1 classication error used perceptron practically synonymous discuss sequel performance documents processed also highly correlated performance algorithms unseen test data type behavior indeed agrees formal analysis online algorithms 4 performance algorithms test sets summarized four tables summary performances respect learningtheoretic information retrieval measures reuters21578 given table 1 table 2 respectively analogous summary reuters 2000 given table 3 table 4 addition also provide fig 6 precisionrecall graphs corpora performance mmp perceptron algorithm respect learningtheoretic losses consistent behavior test data variant mmp performs well respect loss employs training moving retrieval performance measures see recall precision rocciho perceptron mmp l1 mmp l2 mmp l3 precision rocciho perceptron mmp l1 mmp l2 mmp l3 figure versus recall xaxis graphs various algorithms reuters21578 left reuters2000 right respect performance measures exception coverage best performing topicranking algorithm mmp loss1 coverage mmp trained either loss2 loss3 achieve best performance also see strong correlations loss1 oneerr loss2 coverage one possible explanation improved performance loss1 compared loss2 loss3 gives document weight update losses prefer documents large number relevant topics plan investigate conjecture theoretically empirically future work perceptron algorithm performs surprisingly well datasets respect performance mea sures main deciency algorithms relatively poor performance terms coverage achieves worst coverage value cases behavior also observed precisionrecall graphs precision perceptron algorithm low recall values competitive variants mmp even better variants employ loss2 loss3 reuters 21578 long recall 09 alas recall increases precision perceptron algorithm drops sharply high recall values exhibits worst pre cision one possible explanation behavior perceptron tailored classication thus implicit classication loss hinge loss employs insensitive induced ranking despite attempts implement stateoftheart version rocchio takes account phenomena like length documents rocchios performance worst especially surprising since headtohead comparison rocchio recent variants adaboost 11 results two algorithms practically indistin guishable despite fact took two orders magnitude train latter amit singhal google oered one possible explanation relatively poor per formance rocchio originally designed document trieval furthermore recent improvements employ length normalization tuned trecs document retrieval tasks despite similarity nature document ranking topic ranking problem seems exhibit dierent statistical characteristics might require new adaptations topic ranking problems 5 paper presented new family algorithms called mmp topicranking simple imple ment algorithmic approach suggested paper attempts combine formal properties statistical learning techniques eectiveness practical information retrieval algorithms online framework used derive mmp makes algorithm practical massive datasets new release reuters believe experiments show mmp indeed makes nontrivial step toward provably correct practically eective methods ir quite possible extensions modica tions mmp might improve eectiveness one possible extension currently exploring better fusion irmotivated losses average precision core algorithm preliminary results indicate improvement plausible straightforward yet powerful extensions discussed paper particular mmp incorporated kerneltechniques 14 1 robust methods converting mmp batch algorithm see instance 3 references therein also possible evaluated future work acknowledgments thanks amit singhal clarications suggestions pivotedlength normalization thanks also noam slonim discussions ofer dekel benjy weinberger help preprocessing corpora leo kontorovich comments manuscript also would like acknowledge nancial support eu project kermit ist200025341 kermit group members useful discussions 6 r introduction support vector machines large margin classi weak learning text categorization low quality images feature selection relevance feedback information retrieval perceptron probabilistic model information storage organization brain developments automatic text retrieval improved boosting algorithms using con boosting rocchio applied text pivoted document length normalization information retrieval statistical learning theory tr weak learning pivoted document length normalization feature selection perception learning usability case study text categorization large margin classification using perceptron algorithm boosting rocchio applied text filtering improved boosting algorithms using confidencerated predictions introduction support vector machines information retrieval ctr ryan mcdonald koby crammer fernando pereira flexible text segmentation structured multilabel classification proceedings conference human language technology empirical methods natural language processing p987994 october 0608 2005 vancouver british columbia canada nadia ghamrawi andrew mccallum collective multilabel classification proceedings 14th acm international conference information knowledge management october 31november 05 2005 bremen germany shenghuo zhu xiang ji wei xu yihong gong multilabelled classification using maximum entropy method proceedings 28th annual international acm sigir conference research development information retrieval august 1519 2005 salvador brazil vaughan r shanks hugh e williams adam cannane indexing fast categorisation proceedings twentysixth australasian conference computer science research practice information technology p119127 february 01 2003 adelaide australia yoav freund raj iyer robert e schapire yoram singer efficient boosting algorithm combining preferences journal machine learning research 4 p933969 1212003 fernando ruizrico jose luis vicedo maraconsuelo rubiosnchez newpar automatic feature selection weighting schema category ranking proceedings 2006 acm symposium document engineering october 1013 2006 amsterdam netherlands franca debole fabrizio sebastiani analysis relative hardness reuters21578 subsets research articles journal american society information science technology v56 n6 p584596 april 2005 efficient learning label ranking soft projections onto polyhedra journal machine learning research 7 p15671599 1212006