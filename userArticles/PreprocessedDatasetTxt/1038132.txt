collaborative filtering maximum entropy authors describe novel maximumentropy maxent approach generating online recommendations user navigates collection documents show handle highdimensional sparse data represent collection ordered sequences document requests representation maxent approach several advantages 1 naturally model longterm interactions dependencies data sequences 2 query model quickly learned makes method applicable highvolume web servers 3 obtain empirically highquality recommendations although maxent learning computationally infeasible implemented straightforward way authors explored data clustering several algorithmic techniques make learning practical even high dimensions present several methods combining predictions maxent models learned different clusters conducted offline tests using six months worth data researchindex popular online repository 470000 computer science documents show maxent algorithm one accurate recommenders compared techniques correlation mixture markov models mixture multinomial models individual similaritybased recommenders currently available researchindex even various combinations current researchindex recommenders b introduction related work recommender systems attempt automate process word mouth recommendations within community typical application environments dynamic many respects users come go users preferences goals change items added removed user navigation dynamic process recommendation domains also often high dimensional sparse tens hundreds thousands items among known particular user consider instance problem generating recommendations researchindex aka citeseer 1 online digital library computer science papers receiving thousands user accesses per hour site automatically locates computer science papers found web indexes full text allows browsing via literature citation graph isolates text around citations among services 5 figure 1 gives typical screen shot document details page researchindex shown title authors paper download options number similaritybased recommenders predicting user documents possible interest based features current document archive contains 470000 documents including full text document citation links documents wealth user access data many documents pavlov yahoo inc work done nec laboratories america pennock senior research scientist overture services e manavoglu cl giles pennsylvania state university searching world wide web 1998 make steve lawrence c lee giles science homesearch context related view download neccomlawrencechscience98psgz plguwaterloocaclaclarkcslg98ps cached psgz ps pdf djvu image update help neccomhomepageslawrenpubdl homepages slawrence cgiles hpsearch update links search engines irregular indexing outofdate index fraction web metasearch coverage 320m abstract coverage recency major world wide web search engines analyzed yielding surprising results coverage one engine significantly limited single engine indexes onethird indexable web coverage six engines investigated varies order magnitude combining results six engines yields 35 times many documents average compared results one engine analysis overlap update context citations paper problem general purpose search engines cover limited portion web take several months update new information 9 10 one solution build greater variety special purpose search engines react quickly changes within indexing approach increasingly falls behind eort index reasonably large share accessible information 1 new approaches based hierarchical distribution index data smart incremental changedriven updates data cited browsing semistructured texts web using formal correct design evaluation web prefetching caching techniques davison 2002 correct survey rdf data web eberhart 2002 correct active bibliography related documents adaptive web page recommendation service balabanovic 1997 correct geometric assembly planning wilson 1992 correct generalising techniques type debugging bruce mcadam 2000 correct similar documents based text searching web general scientific information access lawrence giles 1999 correct 10 efficient identification web communities flake lawrence giles 2000 correct 09 power play efficiency forecast accuracy pennock lawrence 2000 correct related documents cocitation 29 anatomy largescale hypertextual web search engine brin page 27 accessibility information web context lawrence giles 1999 httpwwwwwwmetricscom 22 authoritative sources hyperlinked environment kleinberg 1997 lawrence cl giles searching world wide web science 280498100 1998 httpciteseernjneccomlawrence98searchinghtml article lawrence98searching steve lawrence c lee giles searching world wide web rate article best comment article 1 2 1102003 1026 searching world wide web lawrence giles researchindex wysiwyg969httpciteseernjneccomlawrence98searchinghtml fig 1 typical screen shot document details page researchindex user given wealth information document well recommendations possible documents interest eight accesses per user average userdocument data matrix exceedingly sparse thus challenging model paper work researchindex data since rich data source properties typical many recommendation application areas 10 believe methods work little adaptation domains two conceptually different ways making rec ommendations content filtering methods recommend solely based features document eg showing documents written authors documents textually similar methods shown work well within researchindex 1 collaborative filtering methods 9 work assessing similarities users based overlap document requests recommending given user documents likeminded users accessed previously common measures similarity users include cor relation mean squared error vector similarity bayesian similarity measures approaches include application statistical machine learning techniques bayesian networks dependency networks singular value decomposition latent class models currently researchindex employs several similaritybased recommenders contentbased user independent hand collaborative filtering algorithms userspecific context order inde pendent rank recommendations depend context users current navigation recency effects objective design superior least complementary modelbased recommendation algorithm researchindex 1 tuned particular user hand 2 takes recent documents greater account lead user far astray current search goal paper compare several probabilistic modelbased collaborative recommenders standard correlation method similaritybased recommenders currently available researchindex probabilistic formulation recommending task posed p next hu next next document requested user u hu history document requests user u present session formulation allows number choices model p found one best maximum entropy maxent approach explicitly models time user associated set sessions session modeled ordered time sequence document accesses approach seems well aligned recent efforts model recommendation problem sequential rather static problem maxent model effectively estimates probability next visited document given recently visited document bigrams past indicative documents triggers knowledge first application maxent collaborative filtering one published formulations makes accurate recommendations context dynamic user session 11 1 also implemented evaluated two probabilistic models mixture markov models mixture multinomial models section ii describe maxent markov multinomial models detail one simplest yet accurate oftenused recommendation algorithms correlation employ baseline also compare similaritybased predictors already available researchindex including merged combinations predictors take users navigation histories account present researchindex contains 470000 docu ments 15 actively accessed slightly half requested data set thus face problem fitting distribution variable takes order 100000 values able fit markov multinomial mixture models available data directly despite huge scale however maxent model cannot feasibly learned entire data set due high computational cost develop greedy hierarchical approach clustering provides feasible implementation clustering allows us overcome sparsity high dimensionality data reducing original problem set smaller subproblems corresponding clusters describe recombine clusterspecific models order make recommendations across entire data set allow direct comparison nonclustering approaches ie mixture models correlation similaritybased recommenders contrasts previous work 6 provided withincluster recommendations algorithms trade accuracy speed memory use results demonstrate maxent approach offers one best performance profiles maxent accurate modelbased approaches accurate correlation orders magnitude faster correlation maxent even compares favorably current similaritybased recommenders researchindex significant since test data biased favor similaritybased recommenders recommenders installed running data generated overall contributions paper summarized follows represent data collection ordered sequences document requests allows us model longterm interactions dependencies data sequences present novel maximum entropy approach generating online recommendations suggest document clustering approach speeding training model compare maximum entropy approach 1 mixture multinomial models 2 mixture markov models 3 correlationone best widely used collaborative filtering recommenders 4 similaritybased recommenders currently available researchin dex 5 combinations similaritybased recom menders offline experiments 6 months worth data researchindex show maxent allows fast model querying relatively compact whole performs well better competitors terms accuracy rest paper organized follows section ii give descriptions probabilistic models motivate choice using maximum entropy describe features used learning section iii presents greedy algorithm clustering documents describes clustering helps decompose original prediction task order render maxent learning computationally feasible experimental results comparisons given section iv section v draw conclusions ponder future work ii probabilistic models assume given data set consisting ordered sequences fixed alphabet refer individual items alphabet documents simply items document define history h sofar observed ordered subsequence current sequence prev last observed document h task predict next document next given history h intend accomplish task learning probabilistic model p next available training data 2 explore several alternatives p 2 absolutely precise write p next h data simplicity omit data conditioning mixture multinomials mixture multinomials essentially prescribes disregard sequential information treat available document accesses history h bag items next next next kt n c number mixture components kt probability document number accessed n number times document number accessed h fitting model done using expectationmaximization em algorithm described 2 number parameters model linear number documents number mixture components b mixture markov models first order markov model main assumption current document depends history h last observed document h ie prev equations defining mixture markov models follows next next next h first equation standard equation mix ture second equation uncovers component modeled 0k probability observing h 0 first document history hh1k probability observing transition document number h document number h1 history document index next model also learned using em algorithm number parameters quadratic number documents linear number components 1 model reduces regular markov model component mixture note regular markov model depends socalled bigrams first order markov terms ie frequencies pairs consecutive documents c trigger maximum entropy model distribution p next h also modeled maximum entropy distribution main motivation using maximum entropy estimation combine attractive features markov multinomial models particular believe document prev requested immediately prior next influence next reason essential model sequence information least pair prev next markov model hand also believe documents h beyond prev also influence next want something sophisticated multinomial model essentially zerothorder model higher first order markov model would probably suffice cannot afford build curse dimensionality thus restrict models reliably estimated loworder statistics still look whole h naturally lead us consider maximum entropy model select two flavors loworder statistics features bigrams first order markov terms one type triggers another order introduce long term dependence next documents occurred history session define trigger pair documents b p next substantially different next b measure quality triggers order rank compute mutual information events discard 10 low scoring triggers retain bigrams note quantity quality selected triggers depends length h since average transaction document requests see table ii choose 10 maximum length history purposes finding informative triggers set featuresbigrams triggers case together maximum entropy objective function shown done 4 lead following form conditional maximum entropy model next next h 3 f features z h normalization constant ensuring distribution sums 1 z exp set parameters needs found following set equations restrict distribution next h expected value feature observed training data dh document following h training data lhs equation 5 represents expectation normalization factor feature f h respect distribution p dh rhs frequency normalization factor feature training data exist efficient algorithms finding parameters eg generalized improved sequential conditional iterative scaling algorithms known converge constraints imposed p consistent fairly general assumptions maximum entropy model also shown maximum likelihood model employing gaussian prior zero mean parameters yields maximum aposteriori solution shown accurate related maximum likelihood solution smoothing techniques maximum entropy models use gaussian smoothing experiments maximum entropy model iii dimensionality reduction via clustering mentioned section models mixture markov multinomial models fit highdimensional data directly using em algorithm however depending number selected features learning maximum entropy model directly raw training data could take months computation goodman 3 arrives similar conclusion shown 6 problem solved employing clustering discuss simple fast greedy clustering algorithm show nice feature algorithm finds topically related clusters finally discuss combine maximum entropy models learned different clusters order provide global recommendations clustering based user navigation goal clustering based user navigation patterns maximize probability document cluster c requested user session leave c ie consist documents c intuition within relatively small time frame users interested single topic succeed finding topically related clusters achieve goal instead looking contents individual document user queries take purely collaborative approach cluster documents solely based user navigation sequences order come clustering scanned processed training data pair consecutively requested documents computed count gathered statistics nothing else bigram counts even though bigram counts fairly sparse ie every document much incoming outgoing traffic clustering data still challenging tried pagegather 8 algorithm clustering bigrams idea behind pagegather create attribute interaction graph based bigrams identify cliques graph using one wellknown techniques join tree clustering algorithm returned us one large clique lot smaller ones whereas would prefer set clusters balanced sizes thus decided employ straightforward hierarchical greedy algorithm pseudocode looks follows input bigrams counts bi j number clusters c output set c clusters algorithm 1 set frequent bigram 2 docs j bi j n docs n transitions 3 j unassigned nc c 4 snc 5 nc new cluster j 6 else unassigned 7 snc 8 else j unassigned 9 snc 10 end 11 bi 12 end 13 n 14 return algorithm starts empty clusters cycles documents picking pairs documents current highest joint visitation frequency prompted bigram frequency lines 1 2 documents selected pair unassigned new cluster allocated lines 3 5 one documents selected pair assigned one previous clusters second document assigned cluster lines 6 10 algorithm repeats lower frequency n long n 2 clustering assume user requests document ith cluster si considerably likely prefer next document si rather sj j ie next assumption reasonable construction clusters represent densely connected terms traffic components traffic across clusters small compared traffic within cluster previous results described 6 obtained clustering broke individual user sessions subsessions subsession consisted documents belonging cluster problem thus reduced series prediction problems cluster also studied clusters trying find documents within cluster topically related ran code previously developed nec labs uses information gain find top features distinguish cluster rest table shows top features created clusters top features quite consistent descriptors suggesting one session researchindex user typically interested searching among topicallyrelated documents least one session defined thus straightforward greedy clustering algorithm successful finding attractive topicallyrelated clusters b combining predictions different clusters order able make predictions raw test data need ability combine predictions learned maximum entropy models different clusters predictor would work unclustered data used following idea borrowed goodman 3 consider arbitrary clustering c set documents document belongs one one cluster probability p next h represented top features clusters autonomous agent agent clustering distance classification kernel svm support documents query web queries pages web engines routing address network ip packets speed rate compression images coefficients security intrusion detection host rate packet long wide scheduling service qos services location next next c hp ch next cd next hp cd next h 6 first equality rule full probability second equality holds since document unique cluster assignment scheme section iiia greedy clustering directly applicable setting use maximum entropy model next cluster remaining question learn weights p cd next h goodman 3 suggests learning separate maximum entropy model represent weights however found following ad hoc approximation scheme works reasonably well practice recall every document associated cluster scan history h accumulate frequency cluster represented document h occurred h instance documents h cluster c c would get nonzero weight 1 would make predictions within c half documents h c 1 half would make predictions clusters weigh equally experimental results use technique combining predictions iv experimental results comparisons preprocessing researchindex data obtained log file recorded 6 month worth researchindex data roughly viewed series requests time user document chronologically partitioned log roughly 5 million training requests covering 159 days 17 million test requests covering 50 days preprocessed training test sets follows document indexed researchindex assigned unique document id whenever user accesses site cookieenabled browser identified new returning user activity recorded server side unique user id uid time stamp tid first processing step aggregated requests uid broke sessions fixed uid session defined sequence document requests two consecutive requests seconds apart experiments chose user inactive 120 seconds next request considered mark start new session next processing step included heuristics identifying discarding sessions belonging robots obviously contaminate browsing patterns human users collapsing consecutive document accesses single instance document objective predict interests user beyond currently requested doc getting rid documents occurred log finally discarding sessions containing one document preprocessing step data represented collection ordered sequences document requests possibly multiple sequences per user models recommenders took data input properties data described table ii b parameters competing models performed clustering training data set using greedy algorithm described section iiia maximum entropy model learned separately cluster bigrams 90 top triggers retained evaluation discussed section iic global predictions maximum entropy model test data obtained combining individual cluster maximum entropy models described section iiib compared maximum entropy approach following models singlecomponent markov model mixture markov models 30 components mixture multinomials 60 components correlation method individual similaritybased predictors researchindex merged similaritybased predictor described computation reasons optimize adjustable parameters models number components mixture variance prior maximum entropy models number clusters 1000 following list similarity recommenders available researchindex detailed descriptions found 5 active bibliography finds documents make similar citations also close sense word sentence similarity displays documents significant number identical sentences recommends documents similar current one terms word occurrences training test total number users 567472 212339 total number documents 263156 233361 average requests per user 86785 80977 standard deviation requests per user 319832 276857 minimum requests per user 2 2 maximum requests per user 4486 3421 users viewed displays documents requested users also visited current document site shows documents listed web site current document cited lists documents cite current one cocitation recommends documents often cited together current document cites displays documents cited current document lists recommendations individual similaritybased predictors populated taking information currently stored system cases 35 top recommendations per document per recommender call researchindex merge recommender obtained pulling similaritybased recommenders togetheressentially corresponds currently available recommending system researchindex c evaluation metrics models trained training data evaluated test data evaluation performed scanning sequences document requests document document document sequence predicting identity next document used average height predictions test data main evaluation criteria height prediction corresponds rank requested document within recommendation list given model recommender formal definition follows assuming probability estimates p next available model p fixed history h possible values next first sort descending order p find distance terms number documents actually requested know test data top sorted list height tells us deep list user must go order see document actually interests height perfect prediction 0 ie requested document first one list researchindex recommenders used order predictions stored system given document specific recommender predictions occurrences document test data height set infinity make comparisons among different recommenders easier binned heights predictors used bins intervals 0 k 10 recommenders bins computed two statistics hits total number recommendations falling within bin normalized total number recommendations height average height predictions within bin best performing model would place predictions inside bins smaller values k within bins average heights would low possible note hits statistics primary comparison height statistic plays role ranking recommenders similar hits ratios primary goal experiments establish competing recommending strategies provides best accuracy terms hits height statistics also compared recommenders respect average online time total memory taken generate prediction time essential characteristic current implementation researchindex ignores recommendations take 001 seconds make memory less important since principle attractively accurate reasonably fast recommending system reside separate computer memory resources available recommending expectation models competitive similaritybased recommenders respect memory since latter use small lists predictions per docu ment former model attribute interactions quite expensive simple example consider case modeling 250 000 attributes roughly much bigrams take quadratic memory number tributes would result roughly 1250 000 2 4bytes 116gigabytes luckily see many bigrams 0 counts need stored hand one keeps 7 numbers per document would case similaritybased recommenders would result 250 000 7 4bytes 66megabytes time taken generate recommendations next using probabilistic model p next h could quite high indeed operation amounts sequentially generating next h next sorting resulting list 3 important role reducing time requirements probabilistic models thus belongs selecting limited set candidates next depending h since selecting candidates set nonobvious certain models straightforward review choices 3 notable exception dependency networks capable generating predictions next simultaneously markov mixtures candidate set generated naturally assessing bigrams prev next document prev multinomial mixtures view absence obvious choice candidate set chosen set similaritybased recommendations available prev correlation candidate set tremendous time requirements result maximum entropy candidate set naturally defined set documents belonging clusters extracted h described section iiib thus expect models fast enough fit recommending time limit requirements mentioned accurate enough compete existing methods hit cited active bibliography sentence similarity users viewed text similarity cocitation cites site researchindex merge fig 2 ratio hits total number document requests bins 1 5 10 various similaritybased recommenders currently used researchindex bin hit markov 1 comp markov correlation maxent multmix 60 comp researchindex merge fig 3 ratio hits total number document requests bins 1 5 10 different probabilistic models also shown correlation researchindex merge recommender bin average height correlation maxent multmix 60 comp researchindex merge fig 4 average heights predictions bins 1 5 10 four best recommenders respect hits ratio see figure 3 experimental results figure 2 present graph showing number hits bin recommending methods currently used researchindex follows plot line expectations researchindex merge recommender best similaritybased recommender figure 3 show plot similar figure 2 probabilistic collaborative filtering models correlation researchindex merge recommender maximum entropy outperforms recommenders including researchindex merge k 3 also one top predictors values k mixture multinomial models appears one top models k 5 notice surprising since list candidates mixture multinomials created predictions individual similaritybased recommenders performance single component markov model improves larger bins interestingly mixture markov models fails improve performance single component model probably due overfitting cocitation worst active bibliography best standalone similaritybased recommender sentence similarity cited perform better co citation however numbers hits still less half maximum entropy correlation improves performance larger values k scoring one best figure 4 report average height predictions recommenders bins note treat average height within bins secondary statistic used comparison recommenders similar hit ratios unlike hit ratios lower height better best 4 competing techniques respect hit ratios namely maximum entropy multinomial mixture correlation researchindex merge maximum entropy best respect average height three fairly close tie overall conclude modelbased approach provides attractive alternative terms quality predictions currently used recommenders iii average time per 1000 predictions total memory used various models cited 00062 38 active bibliography 00082 60 sentence similarity 00056 33 users viewed 00078 59 text similarity 00080 63 cocitation cites 00072 72 site 00068 60 rsearchindex merge 00144 240 mix mult mix markov 1 comp 00703 973 mix markov maxent 76281 4581 correlation 1hour 809 table iii present comparison various models respect average time taken memory required make prediction table clearly illustrates models chose experiments average faster than100 second currently maximum time allowed system generate recommendation line expectations memory requirements models much higher individual recommenders however argued constitute practical problem recommender attractive reasonably fast performance running stand alone machine maximum entropy approach also possibility reducing memory consumption either limiting maximum size cluster increasing number clusters former case maximum size cluster fixed 1000 memory usage drops 458 megabytes 276 megabytes negligible decrease performance increasing number clusters probably also result less accurate models require better modeling p clusterh v conclusions future work described modelbased collaborative filtering approach generating document recommendations researchindex represented data collection sequences document requests ordered time models mixtures markov multinomial models easy fit sparse highdimensional data without additional preprocessing using em algorithm demonstrated employing clustering documents based user navigation patterns also apply maximum entropy modeling researchindex data maximum entropy approach provides us unique advantage able model longterm interactions dependencies data sequences definition maximum entropy model combines zero first order markov terms well triggers high information content empirically demonstrated maximum entropy delivers performance compares well currently existing individual similaritybased recommenders researchindex also outperforms combined recommender number experimental conditions well correlation one popular collaborative filtering techniques another model performed well experiments mixture multi nomials models generated fast enough recommendations used realtime high volume web servers note application domain question recommender best cannot answered solely offline experiments historical logs conducted work researchindex instance enforces certain interface showing top 3 documents according available similaritybased recommenders clearly affects user interaction system consequently biasing logs future work plan perform live testing clustering approach various models researchin dex using recommender framework evaluator researchindex referee 1 also plan enhance maximum entropy approach better model p clusterh could also modeled using maximum entropy advocated goodman 3 recent work 7 suggests difficult prediction problems improvement beyond plain maximum entropy models sought employing mixtures maximum entropy models expectation combining could yield better accuracy faster recommendations however still need explore feasibility fitting model data also plan evaluate different clustering methods documents try combine prediction results different clusterings idea started exploring use contentbased recommendation information clustering clustering based user queries yet another interesting area research combining collaborative filtering content recommendations mining log files researchindex observed different patterns browsing different groups users users never followed recommendations users used totally different order similaritybased predictors believe personalization recommendations boost systems performance vi acknowledgements would like thank steve lawrence making available researchindex log data eric glover running naming code clusters kostas tsioutsiouliklis darya chudova many useful discussions r open framework practical testing recommender systems using researchiindex maximum likelihood incomplete data via em algorithm fast maximum entropy training statistical methods speech recognition digital libraries autonomous citation indexing maximum entropy approach collaborative filtering dynamic mixtures conditional maximum entropy models adaptive web sites automatically synthesizing web pages analysis recommender algorithms ecommerce tr ctr xin jin yanzan zhou bamshad mobasher maximum entropy web recommendation system combining collaborative content features proceeding eleventh acm sigkdd international conference knowledge discovery data mining august 2124 2005 chicago illinois usa