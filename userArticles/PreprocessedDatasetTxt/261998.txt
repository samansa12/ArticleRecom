practical algorithms selection coarsegrained parallel computers abstractin paper consider problem selection coarsegrained distributed memory parallel computers discuss several deterministic randomized algorithms parallel selection also consider several algorithms load balancing needed keep balanced distribution data across processors execution selection algorithms carried detailed implementations algorithms discussed cm5 report experimental results results clearly demonstrate role randomization reducing communication overhead b better practice deterministic counterpart due low constant associated algorithm parallel selection algorithms useful practical applications dynamic distribution multidimensional data sets parallel graph partitioning parallel construction multidimensional binary search trees many parallel algorithms selection designed pram model 2 3 4 9 14 various network models including trees meshes hypercubes reconfigurable architectures 6 7 13 16 22 recently bader etal 5 implement parallel deterministic selection algorithm several distributed memory machines including cm5 ibm sp2 intel paragon paper consider evaluate parallel selection algorithms coarsegrained distributed memory parallel computers coarsegrained parallel computer consists several relatively powerful processors connected interconnection network commercially available parallel computers belong category examples machines include cm5 ibm sp1 sp2 ncube 2 intel paragon cray t3d rest paper organized follows section 2 describe model parallel computation outline primitives used algorithms section 3 present two deterministic two randomized algorithms parallel selection selection algorithms iterative work reducing number elements consider iteration iteration since guarantee number elements removed every processor leads load imbalance section 4 present several algorithms perform load balancing load balancing algorithms used selection algorithm requires load balancing section 5 report analyze results obtained cm5 detailed implementation selection load balancing algorithms presented section 6 analyze selection algorithms meshes hypercubes section 7 discusses parallel weighted selection conclude paper section 8 preliminaries 21 model parallel computation model coarsegrained parallel machine follows coarsegrained machine consists several relatively powerful processors connected interconnection network rather making specific assumptions underlying network assume twolevel model computation twolevel model assumes fixed cost offprocessor access independent distance communicating processors communication processors startup overhead data transfer rate 1 complexity analysis assume constant independent link congestion distance two processors new techniques wormhole routing randomized routing distance communicating processors seems less determining factor amount time needed complete communica tion furthermore effect link contention eased due presence virtual channels fact link bandwidth much higher bandwidth node interface permits us use twolevel model view underlying interconnection network virtual crossbar network connecting processors assumptions closely model behavior cm5 experimental results presented discussion architectures presented section 6 22 parallel primitives following describe important parallel primitives repeatedly used algorithms implementations state running time required primitives model parallel computation analysis run times primitives described fairly simple omitted interest brevity interested reader referred 15 follows p refers number processors 1 broadcast broadcast operation one processor element data broadcast processors operation performed log p time 2 combine given element data processor binary associative commutative op eration combine operation computes result combining elements stored processors using operation stores result every processor operation also performed log p time 3 parallel prefix suppose x p data elements processor p containing x letomega binary associative operation parallel prefix operation stores value x processor p operation performed log p time 4 gather given element data processor gather operation collects data stores one processors accomplished log 5 global concatenate gather except collected data stored processors operation also performed log 6 transportation primitive transportation primitive performs manytomany personalized communication possibly high variance message size total length messages sent received processor bounded time taken communication 2t lower order terms op p outgoing incoming traffic bounds r c instead communication takes time c lower order terms either 3 parallel algorithms selection parallel algorithms selection also iterative work reducing number elements considered iteration iteration elements distributed across processors iteration performed parallel processors let n number elements p number processors begin processor given n otherwise easily achieved using one load balancing techniques described section 4 let n j number elements processor p beginning iteration j algorithm 1 median medians selection algorithm total number elements total number processors labeled 0 list elements processor p jl desired rank among total elements processor p step 1 use sequential selection find median list l l r 2 step 3 p0 find median say mom broadcast processors step 4 partition l mom mom give index split index step 5 count combineindex add calculates number elements step 6 rank count else step 7 loadbalancel step 8 step 9 p0 perform sequential selection find element q rank l figure 1 median medians selection algorithm let k j rank element need identify among n j elements use notation describe selection algorithms presented paper 31 median medians algorithm median medians algorithm straightforward parallelization deterministic sequential algorithm 8 recently suggested implemented bader et al 5 algorithm figure load balancing beginning iteration beginning iteration j processor finds median n j elements using sequential deterministic algorithm medians gathered one pro cessor finds median medians median medians estimated median n j elements estimated median broadcast processors processor scans set points splits two subsets containing elements less equal greater estimated median respectively combine operation comparison k j determines two subsets discarded value k j1 needed next iteration selecting median medians estimated median ensures estimated median least guaranteed fraction number elements least guaranteed fraction elements sequential algorithm ensures worst case number iterations required algorithm olog n let n j thus finding local median splitting set points two subsets based estimated median requires j j th iteration remaining work one gather one broadcast one combine operation therefore worstcase running time algorithm log p running time n log n log p log n p log n algorithm requires use load balancing iterations load balancing assuming load balancing ignoring cost load balancing running time algorithm reduces p log 32 bucketbased algorithm bucketbased algorithm 17 attempts reduce worstcase running time algorithm without requiring load balance algorithm shown figure 2 first order keep algorithm deterministic without balanced number elements processor median medians replaced weighted median medians local medians computed processor however estimated median taken weighted median local medians median weighted number elements corresponding processor guarantee fixed fraction elements dropped consideration every iteration number iterations algorithm remains olog n dominant computational work median medians algorithm computation local median scanning local elements split two sets based estimated median order reduce work repeated every iteration bucketbased approach preprocesses local data olog p buckets 0 every element bucket smaller element bucket j accomplished finding median local elements splitting two buckets based median recursively splitting buckets log pbuckets using procedure thus preprocessing local data olog p buckets requires n log log p time bucketing data simplifies task finding local median task splitting local data two sets based estimated median find local median identify bucket containing median find rank median bucket containing median algorithm 2 bucketbased selection algorithm total number elements total number processors labeled 0 list elements processor p jl desired rank among total elements processor p step 0 partition l p log p buckets equal size r 2 bucket j 2 bucketk r whilen c constant step 1 find bucket bktk containing median element using binary search remaining buckets followed finding appropriate rank bktk find median let n number remaining keys p 2 step 3 p0 find weighted median say wm broadcast step 4 partition l wm wm using buckets give index split index step 5 count combineindex add calculates number elements less wm step 6 rank count else step 7 step 8 p0 perform sequential selection find element q rank l figure 2 bucketbased selection algorithm olog log p time using binary search local median located bucket sequential selection algorithm n time cost finding local median reduces n log p split local data two sets based estimated median first identify bucket contain estimated median elements bucket need split thus operation also requires olog log log p time preprocessing worstcase run time selection olog log p log log p log log p log n p log n n log p log log log p therefore worstcase run time bucketbased approach n log without load balancing algorithm 3 randomized selection algorithm total number elements total number processors labeled 0 list elements processor p jl desired rank among total elements processor p whilen c constant step step 1 step 2 generate random number nr processors 0 step 3 pk nr step 4 partition l mguess mguess give index split index step 5 count combineindex add calculates number elements less mguess step 6 rank count else step 7 step 8 p0 perform sequential selection find element q rank l figure 3 randomized selection algorithm 33 randomized selection algorithm randomized median finding algorithm figure 3 straightforward parallelization randomized sequential algorithm described 12 processors use random number generator seed produce identical random numbers consider behavior algorithm iteration j first parallel prefix operation performed processors generate random number 1 n j pick element random taken estimate median parallel prefix operation processor determine estimated median broadcasts processor scans set points splits two subsets containing elements less equal greater estimated median respectively combine operation comparison k j determines two subsets discarded value k j1 needed next iteration since iteration approximately half remaining points discarded expected number iterations olog n 12 let n j thus splitting set points two subsets based median requires j j th iteration remaining work one parallel prefix one broadcast one combine operation therefore total expected running time algorithm p log p expected running time n log n practice one expect n j max reduces iteration iteration perhaps half especially true data randomly distributed processors eliminating order present input fact load balancing operation end every iteration ensure every iteration j n j load balancing ignoring cost running time algorithm reduces p log log n even without load balancing assuming initial data randomly distributed running time expected n log n 34 fast randomized selection algorithm expected number iterations required randomized median finding algorithm olog n section discuss approach due rajasekharan et al 17 requires olog log n iterations convergence high probability figure 4 suppose want find k th smallest element among given set n elements sample set keys random sort element rank e expected rank k set points identify two keys l 1 l 2 ranks ffi small integer high probability rank l 1 k rank l 2 k given set points elements either l 1 l 2 eliminated recursively find element rank remaining elements number elements sufficiently small directly sorted find required element ranks l 1 l 2 k k iteration repeated different sample set make following modification may help improve running time algorithm practice suppose ranks l 1 l 2 k instead repeating iteration find element rank k among n elements discard elements less l 2 find element rank remaining elements ranks l 1 l 2 k elements greater l 1 discarded rajasekharan et al show expected number iterations median finding algorithm olog log n expected number points decreases geometrically iteration n j number points start j th iteration sample j keys sorted thus cost sorting j log n j dominated j work involved scanning points algorithm 4 fast randomized selection algorithm total number elements total number processors labeled 0 list elements processor p jl desired rank among total elements processor p whilen c constant step step 1 collect sample l l r picking n n ffl n elements random p l r step 2 p0 step 3 pick k1 k2 ranks ijsj jsjlogne ijsj jsjlogne step 4 broadcast k1 k2the rank found k1 k2 high probability step 5 partition l l r k1 k1 k2 k2 give counts less middle high splitters s1 s2 step 6 step 7 cless combineless add step 8 rank 2 cless cmid else else step 9 step 10 p0 perform sequential selection find element q rank l figure 4 fast randomized selection algorithm iteration j processor p j randomly selects n j n j n j elements selected elements sorted using parallel sorting algorithm sorted processors containing elements l jand l jbroadcast processor finds number elements less l jand greater l j contained using combine operations ranks l j 1 l j computed appropriate action discarding elements undertaken processor large value ffl increases overhead due sorting small value ffl increases probability selected elements l j 1 l j lie one side element rank k j thus causing unsuccessful iteration experimentation found value 06 appropriate randomized median finding algorithm one iteration median finding algorithm takes j log log n iterations required median finding requires n log log n log p log log n time load balancing ensure n j reduces half every iteration assuming ignoring cost load balancing running time median finding reduces log log log log n even without load balancing running time expected n log log n 4 algorithms load balancing order ensure computational load processor approximately every iteration selection algorithm need dynamically redistribute data every processor nearly equal number elements present three algorithms performing load balancing algorithms also used problems require dynamic redistribution data restriction assignment data processors use following notation describe algorithms load balancing initially processor total number elements processors ie 41 order maintaining load balance suppose processor set elements stored array view n elements globally sorted based processor array indices j element processor p appears earlier sorted order element processor p j order maintaining load balance algorithm parallel prefix based algorithm preserves global order data load balancing algorithm first performs parallel prefix operation find position elements contains global order objective redistribute data processor p contains algorithm 5 modified order maintaining load balance number total elements total number processors labeled 0 list elements processor p size n processor p step increment navg step 1 step 2 diff step 3 diff j positive p j labeled source diff j negative p j labeled sink step 4 p source calculate prefix sum positive diff array p src else calculate prefix sums sinks using negative diff p snk step 5 l step 7 calculate range destination processors p l pr using binary search p snk step 8 whilel r elements p l increment l step 5 l step 7 calculate range source processors p l pr using binary search src step 8 l r receive elements p l increment l figure 5 modified order maintaining load balance elements positions n avg global order using parallel prefix operation processor figure processors send data amount data send processor similarly processor figure amount data receive processor communication generated according data redistributed model computation running time algorithm depends maximum communication generatedreceived processor maximum number messages sent processor nmax navg e1 maximum number elements sent n max maximum number elements received processor n avg therefore running time nmax order maintaining load balance algorithm may generate much communication necessary example consider case processors n avg elements except p 0 one element less p pgamma1 one element n avg optimal strategy transfer one extra element p p p 0 however algorithm transfers one element p messages since preserving order data important selection algorithm following modification done algorithm every processor retains minfn original elements processor n excess labeled source otherwise processor needs n avg gamma n elements labeled sink excessive elements source processors number elements needed sink processors ranked separately using two parallel prefix operations data transferred sources sinks using strategy similar order maintaining load balance algorithm algorithm figure 5 call modified order maintaining load balance algorithm modified omlb implemented 5 maximum number messages sent processor modified omlb op maximum number elements sent n maximum number elements received processor n avg worstcase running time op 42 dimension exchange method dimension exchange method figure 6 load balancing technique originally proposed hypercubes 1121 iteration method processors paired balance load locally among eventually leads global load balance algorithm runs log p iterations iteration 0 processors differ th least significant bit position ids exchange balance load iteration 0 processors number elements iteration ppairs processors communicate parallel processor communicates nmaxelements iteration therefore running time log however since 2 j processors hold maximum number elements iteration j likely either n max small far fewer elements nmaxare communicated therefore running time practice expected much better predicated worstcase 43 global exchange algorithm similar modified order maintaining load balance algorithm except processors large amounts data directly paired processor small amounts data minimize number messages figure 7 modified order maintaining load balance algorithm every processor retains minfn original elements processor n excess la algorithm 6 dimension exchange method number total elements total number processors labeled 0 list elements processor p size n processor p step 1 p step 2 exchange count elements p step 3 step 4 send elements l navg processor p l step 5 n else step 4 receive n l gamma navg elements processor p l step 5 increment n n l gamma navg figure exchange method load balancing beled source otherwise processor needs n avg gamma n elements labeled sink source processors sorted nonincreasing order number excess elements processor holds similarly sink processors sorted nonincreasing order number elements processor needs information number excessive elements source processor collected using global concatenate operation processor locally ranks excessive elements using prefix operation according order processors obtained sorting another global concatenate operation collects number elements needed sink processor elements ranked locally processor using prefix operation performed using ordering sink processors obtained sorting using results prefix operation source processor find sink processors excessive elements sent number element sent processor sink processors similarly compute information number elements received source processor data transferred sources sinks since sources containing large number excessive elements send data sinks requiring large number elements may reduce total number messages sent worstcase may one processor containing excessive elements thus total number messages sent algorithm op processor send data maximum number elements received processor n avg worstcase run time op algorithm 7 global exchange load balance number total elements total number processors labeled 0 list elements processor p size n processor p step increment navg step 1 j 0 step 2 diff step 3 diff j positive p j labeled source diff j negative p j labeled sink step 4 k 2 0 sources descending order maintaining appropriate processor indices also sort diff k sinks ascending order step 5 p source calculate prefix sum positive diff array p src else calculate prefix sums sinks using negative diff p snk step 6 p source calculate prefix sum positive diff array p src else calculate prefix sums sinks using negative diff p snk step 7 l 8 r step 9 calculate range destination processors p l pr using binary search p snk step 10 whilel r elements p l increment l step 7 l 8 r step 9 calculate range source processors p l pr using binary search src step 10 l r receive elements p l increment l figure 7 global exchange method load balancing selection algorithm runtime median medians n randomized n log n fast randomized n log log n table 1 running times various selection algorithm assuming including cost load balancing selection algorithm runtime median medians n bucketbased n log randomized n log log n fast randomized n log log n log p log log n table 2 worstcase running times various selection algorithms 5 implementation results estimated running times various selection algorithms summarized table 1 table 2 table 1 shows estimated running times assuming processor contains approximately number elements end iteration selection algorithm expected hold random data even without performing load balancing also observe experimentally table 2 shows worstcase running times absence load balancing implemented selection algorithms load balancing techniques cm 5 experimentally evaluate algorithms chosen problem finding median given set numbers ran selection algorithm without load balancing load balancing algorithms described except bucketbased approach use load balancing run resulting algorithms 32k 64k 128k 256k 512k 1024k 2048k numbers using 2 4 8 16 32 64 128 processors algorithms run total number elements falls p 2 point elements gathered one processor problem solved sequential selection found appropriate experimentation avoid overhead communication processor contains small number elements value total number elements run algorithms two types inputs random sorted random case n p elements randomly generated processor eliminate peculiar cases using random data ran experiment five different random sets data used average running time random data sets constitute close best case input selection algorithms sorted case n numbers chosen numbers containing numbers n sorted input close worstcase input selection algorithms example first iteration selection algorithm using input approximately half processors lose data half retains data without load balancing number active processors cut half every iteration true even modified order maintaining load balance global exchange load balancing algorithms used every iteration half processors contain zero elements leading severe load imbalance load balancing algorithm rectify data collected illustrated order save space execution times four different selection algorithms without using load balancing random data except median medians algorithm requiring load balancing global exchange used 128k 512k 2048k numbers shown figure 8 graphs clearly demonstrate four selection algorithms scale well number processors immediate observation randomized algorithms superior deterministic algorithms order magnitude example median medians algorithm ran least 16 times slower bucketbased selection algorithm ran least 9 times slower either randomized algorithms order magnitude difference uniformly observed even using load balancing techniques also case sorted data surprising since constants involved deterministic algorithms higher due recursively finding estimated median among deterministic algorithms bucketbased approach consistently performed better median medians approach factor two random data sorted data bucketbased approach use load balancing ran 25 slower median medians approach load balancing iteration parallel selection algorithm processor also performs local selection algorithm thus algorithm split parallel part processors combine results local selections sequential part involving executing sequential selection locally processor order convince randomized algorithms superior either part ran following hybrid experiment ran deterministic parallel selection algorithms replacing sequential selection parts randomized sequential selection running time hybrid algorithms deterministic randomized parallel selection algorithms made following observation factor improvement randomized parallel selection algorithms deterministic parallel selection due improvements sequential parallel parts large n much improvement due sequential part large p improvement due parallel part conclude randomized algorithms faster practice drop deterministic algorithms consideration time seconds number processors median medians bucket based randomized fast randomized001500250035004500550065 time seconds number processors randomized fast randomized05152535 time seconds number processors median medians bucket based randomized fast randomized00400801201602024 time seconds number processors randomized fast randomized261014 time seconds number processors median medians bucket based randomized fast randomized01030507 time seconds number processors randomized fast randomized figure 8 performance different selection algorithms without load balancing except median medians selection algorithm global exchange used random data sets time seconds number processors random data n512k balance mod order maintaining load balance dimension exchange global exchange01030507 time seconds number processors random data n2m balance mod order maintaining load balance dimension exchange global exchange005015025035045 time seconds number processors sorted data n512k balance mod order maintaining load balance dimension exchange global exchange020611418 time seconds number processors sorted data n2m balance mod order maintaining load balance dimension exchange global exchange figure 9 performance randomized selection algorithm different load balancing strategies random sorted data sets facilitate easier comparison two randomized algorithms show performance separately figure 8 fast randomized selection asymptotically superior randomized selection worstcase data random data expected running times randomized fast randomized algorithms n log n n log log n respectively consider effect increasing n fixed p initially difference log n log log n significant enough offset overhead due sorting fast randomized selection randomized selection performs better n increased fast randomized selection begins outperform randomized selection large n algorithms converge execution time since n dominates reversing point view find fixed n increase p randomized selection eventually perform better readily observed graphs effect various load balancing techniques randomized algorithms random data shown figure 9 figure 10 execution times consistently better without using load balancing using three load balancing techniques load balancing time seconds number processors random data n512k balance mod order maintaining load balance dimension exchange global exchange01030507 time seconds number processors random data n2m balance mod order maintaining load balance dimension exchange global time seconds number processors sorted data n512k balance mod order maintaining load balance dimension exchange global time seconds number processors sorted data n2m balance mod order maintaining load balance dimension exchange global exchange figure 10 performance fast randomized selection algorithm different load balancing strategies random sorted data sets random data almost always negative effect total execution time effect pronounced randomized selection fast randomized selection explained fact fast randomized selection fewer iterations olog log n vs olog n less data iteration observation load balancing negative effect running time random data easily explained load balancing processor elements sends elements another processor time taken send data justified time taken process data future iterations time sending suppose processor sends elements another processor processing data involves scanning iteration based estimated median discarding part data random data expected half data discarded every iteration thus estimated total time process data om time sending data also om observation constants involved load balancing taking time reduction running time caused time seconds number processors comparing two randomized selection algorithms using sorted data n512k randomized fast randomized020611418 time seconds number processors comparing two randomized selection algorithm using sorted datas n2m randomized fast randomized figure 11 performance two randomized selection algorithms sorted data sets using best load balancing strategies algorithm gamma load balancing randomized selection modified order maintaining load balancing fast randomized selection consider effect various load balancing techniques randomized algorithms sorted data see figure 9 figure 10 even case cost load balancing offset benefit randomized selection however load balancing significantly improved performance fast randomized selection figure 11 see comparison two randomized algorithms sorted data best load balancing strategies algorithm gamma load balancing randomized selection modified order maintaining load balancing fast randomized algorithm performed slightly better strategies see large n fast randomized selection superior also observe see figure 11 figure 8 fast randomized selection better comparative advantage randomized selection sorted data finally consider time spent load balancing randomized algorithms random sorted data see figure 12 figure 13 types data inputs fast randomized selection spends much less time randomized selection balancing load reflective number times load balancing algorithms utilized olog log n vs olog n clearly cost load balancing increases amount imbalance number processors random data overhead due load balancing quite tolerable range n p used experiments sorted data significant fraction execution time randomized selection spent load balancing load balancing never improved running time randomized selection fast randomized selection benefited load balancing sorted data choice load balancing algorithm make significant difference running time consider variance running times random sorted data number processors01030507time seconds randomized selection random data load balancing time number processors0206101418 time seconds randomized selection sorted data load balancing time figure 12 performance randomized selection algorithm different load balancing strategies balancing n order maintaining load balancing dimension exchange method global exchange g number processors010305time seconds fast randomized selection random data load balancing time number processors020610time seconds fast randomized selection sorted data load balancing time od g figure 13 performance fast randomized selection algorithm different load balancing strategies balancing n order maintaining load balancing dimension exchange method global exchange g primitive twolevel model hypercube mesh broadcast log p log p log combine log p log p log parallel prefix log p log p log gather log global concatenate log transportation op table 3 running time basic communication primitives meshes hypercubes using cutthrough routing transportation primitive refers maximum total size messages sent received processor randomized algorithms randomized selection algorithm ran 2 25 times faster random data sorted data see figure 12 using load balancing strategies little variance running time fast randomized selection figure 13 algorithm performs equally well best worstcase data case 128 processors stopping criterion results execution one iteration runs thus load balancing detrimental effect overall cost decided choose stopping criterion provide fair comparison different algorithms however appropriate fine tuning stopping criterion corresponding increase number iterations provide time improvements load balancing 2m data size 128 processors 6 selection meshes hypercubes consider analysis algorithms presented cutthrough routed hypercubes square meshes p processors running time various algorithms meshes hypercubes easily obtained substituting corresponding running times basic parallel communication primitives used algorithms table 3 shows time required parallel primitive twolevel model computation hypercube p processors p p theta mesh analysis omitted save space similar analysis found 15 load balancing achieved using communication pattern transportation primitive 24 involves two alltoall personalized communications processor n elements sent worstcase time order maintaining load balance op hypercube mesh respectively n op exchange load balancing algorithm hypercube worstcase run time log p n log p mesh log global exchange load balancing algorithmm time complexities modified order maintaining load balancing algorithm hypercube mesh costs must added selection algorithms analysis algorithms load balancing desired table running times primitives remain hypercube hence analysis experimental results obtained twolevel model valid hypercubes thus time complexity selection algorithms hypercube twolevel model discussed paper ratio unit computation cost unit communication cost large ie processor much faster underlying communication network cost load balancing offset advantages fast randomized algorithm without load balancing superior performance practical scenarios load balancing mesh results asymptotically worse time requirements would expect load balancing useful small number processors large number processors even one step load balancing would dominate overall time hence would effective following present results performance best case worst case data mesh 1 deterministic algorithms communication primitives used deterministic selection algorithms gather broadcast combine even though broadcast combine require time twolevel model cost absorbed time required gather operation identical mesh twolevel model hence complexity deterministic algorithms mesh remains twolevel model total time requirements median medians algorithm n log n best case n log n worst case bucketbased deterministic algorithm runs n log time worst case without load balancing 2 randomized algorithms communication randomized algorithm includes one prefixsum one broadcast one combine communication time mesh one iteration randomized algorithm log p p p making overall time complexity n log n best case n log log n worst case data fast randomized algorithm involves parallel sort sample use bitonic sort sample n ffl 0 chosen n elements sorted mesh sorting sample n ffl elements using bitonic sort takes log acceptably small keep sorting phase dominating every iteration runtime fast randomized selection mesh n best case data worst case data time requirement would n log log p 7 weighted selection corresponding weight w attached problem weighted selection find element x x l example weighted median element divides data set sum weights w two sets 1 2 approximately equal sum weights simple modifications made deterministic algorithms adapt weighted selection iteration j selection algorithms set j elements split two subsets j 1 j 2 count elements used choose subset desired element found weighted selection performed follows first elements j divided two subsets j 1 j 2 selection algorithm sum weights elements subset j 1 computed let k j weight metric iteration j k j greater sum weights j 1 problem reduces performing weighted selection k j otherwise need perform weighted selection k 1 method retains property guaranteed fraction elements discarded iteration keeping worst case number iterations olog n therefore median medians selection algorithm bucketbased selection algorithm used weighted selection without change run time complexities randomized selection algorithm also modified way however modification fast randomized selection work algorithm works sorting sample data set picking two elements high probability lie either side element rank k sorted order weighted selection weights determine position desired element sorted order thus one may tempted select sample weights however work since weights elements considered order sorted data list elements sorted according weights make sense hence randomized selection without load balancing best choice parallel weighted selection conclusions paper tried identify selection algorithms suited fast execution coarsegrained distributed memory parallel computers surveying various algorithms identified four algorithms described analyzed detail also considered three load balancing strategies used balancing data execution selection algorithms based analysis experimental results conclude randomized algorithms faster order magnitude determinism desired bucketbased approach superior median medians algorithm two randomized algorithms fast randomized selection load balancing delivers good performance types input distributions little variation running time overhead using load balancing wellbehaved data insignificant load balancing techniques described used without significant variation running time randomized selection performs well wellbehaved data large variation running time best worstcase data load balancing improve performance randomized selection irrespective input data distribution 9 acknowledgements grateful northeast parallel architectures center minnesota supercomputing center allowing us use cm5 would like thank david bader providing us copy paper corresponding code r deterministic selection olog log n parallel time design analysis parallel algorithms parallel selection olog log n time using optimal algorithm parallel selection practical parallel algorithms dynamic data redistribution technical report cmucs90190 time bounds selection parallel median algorithm introduction algorithms dynamic load balancing distributed memory multiprocessors expected time bounds selection selection reconfigurable mesh introduction parallel algorithms introduction parallel computing design analysis algorithms efficient computation sparse interconnection networks unifying themes parallel selection derivation randomized sorting selection algorithms randomized parallel selection programming hypercube multicomputer efficient parallel algorithms selection searching sorted matrices finding median random data accesses coarsegrained parallel machine ii load balancing hypercube tr ctr ibraheem alfuraih srinivas aluru sanjay goil sanjay ranka parallel construction multidimensional binary search trees ieee transactions parallel distributed systems v11 n2 p136148 february 2000 david bader improved randomized algorithm parallel selection experimental study journal parallel distributed computing v64 n9 p10511059 september 2004 marc daumas paraskevas evripidou parallel implementations selection problem case study international journal parallel programming v28 n1 p103131 february 2000