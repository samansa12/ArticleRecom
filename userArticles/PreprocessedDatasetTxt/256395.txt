performance multistage bus networks distributed shared memory multiprocessor abstracta multistage bus network mbn proposed paper overcome shortcomings conventional multistage interconnection networks mins single bus hierarchical bus interconnection networks mbn consists multiple stages buses connected manner similar mins bandwidth stage switch mbn similar min switch except single bus connection instead crossbar mbns support bidirectional routing exists number paths source destination pair paper develop self routing techniques various paths present algorithm route request along path minimum distance analyze probabilities packet taking different routes derive performance analysis synchronous packetswitched mbn distributed shared memory environment compare results equivalent bidirectional min bmin finally present execution time various applications mbn bmin executiondriven simulation show mbn provides similar performance bmin offering simplicity hardware faulttolerance conventional min b introduction order achieve significant performance parallel computing necessary keep communication overhead low possible communication overheads multiprocessor system depend great extent underlying interconnection network interconnection network either static dynamic dynamic networks connect input output enabling switches applicable shared memory message passing multiprocessors among dynamic ins hierarchical buses rings 1 2 multistage interconnection networks mins 3 4 commercially employed strictly hierarchical bus architecture 1 number buses connected form tree processors memories use multiple buses makes hierarchical busbased systems scalable compared popular single bus multiprocessors however bandwidth interconnection decreases one moves toward top tree thus scalability hierarchical bus system becomes limited bandwidth topmost level bus bandwidth problem alleviated fat tree design 5 simplicity bus based designs availability fast broadcasting mechanism factors make busbased systems attractive mins hand offer uniform bandwidth across stages network bandwidth network increases proportion increase system size making min highly scalable interconnection switches min made small crossbar switches system size grows bigger switches used keep number stages hence memory latency low 6 however complexity crossbar switch grows square size therefore total network cost becomes predominant larger systems observed traffic network low making crossbar based min switches highly underutilized system using private caches common todays shared memory multiprocessors effective traffic handled switches network reduced novel interconnection scheme called multistage bus network mbn introduced paper combines positive features hierarchical buses mins mbn consists several stages buses equal number buses stage provides uniform bandwidth across stages forms multiple trees processors memories unlike hierarchical bus networks mbns comprise multiple buses higher levels reducing traffic higher levels maintaining cache coherence major problem shared memory multiprocessors unlike mins snoopy cache coherence protocols applied mbn 7 improve performance large extent also mbn provides much better fault tolerance reliability compared conventional min 8 known distributed shared memory organization better scalability centralized organization 2 3 organization request response packet make uturns network reach destination quickly since intermediate levels mbn consist buses bidirectional connections four different routing techniques 8 presented paper also develop equations probabilities taking path based memory requests order realistic comparison mins introduce design analysis corresponding bidirectional min bmin paper bmin allows uturns packet routed based techniques presented paper mbn recently xu ni 9 discussed uturn strategy bidirectional mins applicable ibm sp architecture 4 however min employed sp architectures clusterbased works differently proposed mbn bmin paper analyze performance mbn distributed shared memory multiprocessors based different self routing techniques unlike previous analysis 8 present analysis based routing along minimum four paths given source destination pair mbn inherent fault tolerance capabilities due number switch disjoint paths source destination pair paper concentrate routing performance evaluation router fig 1 distributed shared memory multiprocessor using queueing analysis executiondriven simulations various applications executiondriven simulator extended version proteus 10 simulates behavior cache coherent distributed shared memory multiprocessor various applications rest paper organized follows present structure introduce four types self routing techniques mbn section 2 define routing tags required implement four routing strategies section 3 present algorithm optimal path network given sourcedestination pair section performance analysis mbn bmin presented section 4 results comparison conventional bidirectional mins presented section 5 section 6 presents executiondriven simulation specifications results finally section 7 concludes paper ii structure mbn consider distributed shared memory dsm architecture throughout paper environment memory modules directly connected corresponding processors shown figure 1 address space shared example hierarchical bus interconnection two levels buses shown figure 2a 1 example 16 processors 4 memories four level 1 buses one level 2 bus naturally top level bus bottleneck system order improve performance number buses must connected top level interleaved memory design connection shown figure 2b 1616 system two levels buses memory p processor 16 processor hierarchial bus system b 1616 mbn based system using 44 switches memory p processor level 1 bus fig 2 hierarchical bus interconnection mbn propose bus along controller placed switch analogous min switch network called multistage bus network mbn n n multistage network using k k switches l log k n stages switches numbered stage 0 stage l gamma 1 shown fig 3a every switch set left connections closer processor side set right connections closer memory side construction 44 mbn switch incorporating bus bus access controller output buffers shown figure 3b control lines associated port carry arbitration information bus access controller suzuki et al studied similar bus structure 11 also propose bidirectional min bmin structure comparison difference switch architectures bmin mbn evident figs 3b c bmin switch crossbar whereas mbn switch bus networks packet stage passed stage vice versa using destination tag digits k k mbn switch 2k packets k inputs either side potentially competing bus cycle one packet bus access controller chooses one random others queued transmitted later hand k k bmin switch 2k inputs connected 2k outputs requests different destinations k k mbn bmin switches support forward backward turn buffer stage control lines 1616 multistage network controller bus access bus lines lines control output input output input output input lines b 44 mbn switch architecture crossbar controller c 44 bmin switch architecture fig 3 comparing switch architectures around connections explained next section describe structure mbn structure bmin similar processors p connected left connections mbn switches stage connected right connections stage l gamma 1 memory module also directly connected processor p called local memory p source assigned tag destination assigned destination tag digits k gamma ary system digits 0 0 significant lgamma1 least significant digits connection stages mbn kshuffle 6 means right connection position 0 1 lgamma1 stage connected left connection position 1 lgamma1 0 stage 2 memory request satisfied internally local memory source tag destination tag request tags different request travels remote memory mbn example 1616 mbn 22 switches shown figure 4 may may shuffle interconnection first stage switches routings developed based figure 4 shuffle 1st stage hence set processors memories connected one switch first stage another switch position last stage exists kshuffle connection first stage different set processors connected first stage last stage switches figure 4 request travels forward direction starts processor side passes stages 0 1l gamma 1 order travels backward direction starts memory side passes reverse direction stages l gamma 11 0 shown figure 5 packet also travel left right make uturn intermediate stage shown figure 6 called routing similarly figure 7 shows backward gamma ubu routing message enters network right makes uturn four routings provide four distinct paths source destination mbn result fault tolerance reliability mbn much better conventional min exact expressions mbn reliability derived 8 also valid bmins introduced paper conventional mins like omega delta gsn 6 destination tag used purpose self routing request forward direction case mbn destination tag also used self routing forward direction since stage 0 connections straight instead kshuffle destination tag used self routing backward direction explained later routing tag backward routing case obtained reverse shuffling destination tag one digit order determine take turn two routing techniques involving uturns need combine source tag destination tag form combined tag following definitions needed develop exact routing algorithms later forward routing tag frt destination tag memory request ie brt backward routing tag brt destination tag reverse shuffled one digit 0 1 lgamma1 destination brt b 0 b 1 b j gamma1modl definition 3 ct combined tag ct digitwise exclusiveor source tag destination tag ie operation j means c note although digits k gamma ary digits ct binary definition 4 rct rotated combined tag rct combined tag ct reverse shuffled right rotated one digit ie rct r 0 r j gamma1modl definition 5 fts forward turning stage fts defined rightmost nonzero position rotated combined tagrct r definition 6 bts backward turning stage bts defined leftmost nonzero position combined tag ct routing tags frt brt used self routing case forward backward directions respectively tags rct ct used find uturn stages fts bts respectively uturn stages fts bts used determine take forward backward turns uturn routings various routing schemes possible mbn described iii routing algorithms mbn section first present four routing techniques mbn present algorithm chooses path minimum distance although techniques described mbn equally valid bmin routing techniques forward routing forward fw routing request source processor moves stage 0 stage l gamma 1 mbn destination memory example fw routing source 0011 destination 1011 shown bold line figure 4 jth digit forward routing tag frt used switch stage j selfrouting thus request started fig 4 forward fw routing mbn position left stage 0 switched position 0 1 j lgamma2 0 right stage 0 undergoes kshuffle reaches position 1 2 j lgamma2 0 0 input stage 1 gets switched 1 2 j lgamma2 0 1 output stage 1 general request arrives position left stage j switched position j lgamma2 0 1 right stage j goes kshuffle except last stage arrives position j1 lgamma2 0 1 j j left stage j 1 finally reaches destination 0 1 j output last stage mbn b backward bw routing backward bw routing request source node moves backward stage l gamma 1 stage 0 destination node example bw routing 0011 1011 shown using bold line figure 5 jth digit backward routing tag brt used switch stage j selfrouting thus request started position right stage l gamma 1 switched position 0 1 j lgamma2 lgamma2 left stage l gamma 1 undergoes reverse kshuffle reaches position lgamma2 0 1 j lgamma2 right stage l gamma 2 general request arrives position j j1 lgamma2 right stage j switched position j j1 lgamma2 j gamma1modl jth digit brt b fig 5 backward bw routing mbn left stage j goes reverse shuffle except last stage arrives position left stage c forwardu routing forwardu routing request starts source stage 0 follows fw routing using frt stage fts1 reaches left stage fts position fts fts1 lgamma2 0 1 fts gamma1 fts gamma1 fts takes uturn instead getting switched right stage fts request switched left stage fts position routing using brt stage 0 finally reaches position 0 1 fts gamma1 fts lgamma2 lgamma1 left stage 0 example fu routing 0010 0110 shown bold line figure 6 backwardu bu routing backwardu routing request starts source stage l gamma 1 follows bw routing using brt stage bts1 reaches position right stage bts stage bts takes uturn instead getting switched left stage bts request gets switched right stage bts position dbts dbts1 lgamma2 routingusing frt stage l gamma 1 finally request reaches position right stage l gamma 1 fig 6 forwardu fu routing mbn example bu routing 0010 0110 shown figure 7 b optimal path algorithm distance source destination mbn defined minimum number switches packet travel conventional min distance always equal l number stages network case mbn however distance may less l fu bu routing chosen fu bu forwardu backwardu routings used turning stage happens less center stage network therefore net savings terms distances given source destinations detailed expressions overall savings distances mbn given section 4 present algorithm choose optimal routing given sourcedestination pair optimal path algorithm 1 stage fig 7 backwardu bu routing mbn 2 3 4 5 l bl2c 7 request local memory 8 else 9 find fts bts based tags rct ct respectively 10 11 select forwardu fu routing backwardu bu routing 12 else 13 select forwardu fu routing 14 else bts u 15 select backwardu bu routing 16 else 17 select forward fw routing backward bw routing optimal path algorithm chooses route minimum path length given source destination algorithm computes tags described earlier section uses comparison tags decide four routings would give minimum path length network algorithm defined center stage mbn must pointed optimal routing two nodes fixed given network hence optimal path precomputed stored table read request issued need execute algorithm every time message sent source destination request local memory case traversal mbn required requests pass least one stage mbn memories connected processor first last stage mbn called cluster memories similarly processors one switch away memories called cluster processors memories requests cluster memories require one switch traversed thus routing taken serve purpose satisfied check fu bu routing would next possible minimum path fts bl2c bts dl2e turning stages mbn center stage would reduce total path length less l thus fu bu routing selected none conditions true fts bl2c bts dl2e case forward fw routing backward bw routing options actual path lengths terms number switches traversed presented ffl local memory 0 switches mbn traversed ffl forward routing backward routing l switches destination fwbw routing fu routing bu routing path lengths routings given source0 different destinations ffl forwardu routing 1 2 memories 2 theta fts ffl backwardu routing 1 2 memories 2 theta l path length equations used form table given source destination example table 1 shows path lengths source 0 different destinations 10241024 network 1023 path length routing quite different thus routing algorithm required route request optimal path example destination 2 backwardu routing result optimal path length hand destination 256 forwardu routing result optimal path length two requests use forward backward routing strategies iv performance mbn multistage bus network mbn analyzed distributed shared memory environment shown figure 1 also analyze bmin compare results mbn cases memory module directly connected processor p called local memory p requests processor local memory called internal requests carried internal bus processor local memory memory also receive external requests originate processors carried mbn network operation distributed memory system processors reached switch size k first stage last stage p connected thus external request destined cluster processor memory returns first stage forwardu routing last stage backwardu routing without going whole mbn however request neither local cluster memory request may take one four routings described earlier internal external requests arrive memory queue one selected service fcfs basis remaining requests queued buffer memory receiving request memory module send reply packet either directly local processor another processor network depending whether request internal external compare performance mbn bmin transmission request reply packets goes network following routings given earlier paper shall assume synchronous packet switched system analyzing multistage networks since buffer size four gives effect infinite buffer 12 13 simplicity shall assume infinite buffer mbn bmin analysis extended finite buffers equations fairly complicated 13 since aim analyze routing schemes prefer give basic infinite buffer analysis bus service time mbn link service time bmin transfer message forms one system cycle time service times memory modules assumed integral multiples system cycle time processor represented delay center given cycle submits memory request given probability busy computation sends memory request processor becomes idle memory response packet case read acknowledgment case write obtained various system parameters defined size mbn min switches number processors memories system number stages probability processor submits memory request given cycle provided busy probability processor requests local memory provided made memory request probability request passes stage switch stage number local requests processor per cycle number remote requests processor per cycle delay network considering stages length memory module delay memory module utilization fraction time processor busy performance analysis mbns bmins carried following assumptions 12 13 packets generated source node independent identically distributed random processes point time processor either busy internal computations waiting response memory request pending request busy processor generates packet probability p cycle probability request local memory internal request probability memory module external request reply memory travels opposite direction path mbn bmin may noted case min like butterfly 3 reply traverse direction ie processor memory side reach requesting processor min unidirectional links 9 bidirectional links used stages hence requesting reply messages may travel forward backward directions respectively messages processor memory generated using probabilities specified request probability p request probability defined section 3 used means estimating processor behavior terms memory requests processor busy computation ie request outstanding switches memory module send memory request cycle processor decides whether message sent based probability average takes 1p cycles send request processor ffl local memory request probability given request made memory probability used decide whether request local external memory though simple probabilities play important role inputs analysis request memory processor waits acknowledgment acknowledgment received processor useful computation one cycle based probabilities decides whether continue send another request memory processor utilization processor utilization p u defined fraction time processor busy determined waiting time service time faced request various service centers number applications large portion requests made cluster processors 8 studied performance mbn varying probabilities cluster requests study forwardu backwardu routings allowed first last stages requests routed forward fw routing processor utilization case given following equation paper message mbn bmin sent along minimum distance case 2 ffl ff corresponds expected delay local memory request served ffl fi corresponds expected delay serving requests cluster memories ffl fl corresponds expected delay serving requests except cluster memories follow fu bu routing corresponds expected delay serving requests folllow forward routing fw backward routing bw derivation terms ff fi fl j presented terms depend routing probabilities along path b amount traffic network c service demand individual service centers thus get nonlinear equation p u single variable solved using iteration techniques b routing probabilities path delays routing probabilities path delays derived mbn bmin assumption nonlocal memories equally addressed processor equations modified case nonuniform remote memory references since path length backward routing bw fw routing derive term j based fw routing multiply 2 include bw routing similar method used fu bu routing well local memory requests ff local memory request involve switch traversal thus delay servicing request memory module given probability processor request memory p request local memory deduce routing fi requests cluster processors travel first last stage switch take fu bu routing destination processor sourcedestination pairs bits except least significant log 2 k bits ct zero entail type routing thus number cluster memories given source k size mbn bmin switch switch stage 0 traversed reaching cluster memory sending back acknowledgment given external memory requested probability requesting cluster memories expressed thus 2 theta r 0 delay switch traversals dm memory service delay get following equation theta noncluster fu bu routing fl forwardu backwardu routing request traverses one direction particular stage explained section 2 makes uturn reach destination processor thus given turning stage fts path length said 2 theta fts 1 fts traversed stages left fts traversed twice necessarily switch fts bl2c path length optimization bts dl2e optimal path length already covered cluster memories 2 consider fts bl2c similar derivation done bts dl2e also know number destinations total n gamma 1 given turning stage 1 fts defined rightmost bit tag bits left position 1 gives us k number ways discussed section 3 rotated combined tag rct defined digitwise exor source destination tags thus rct tag made 1s 0s ie rct tag bitwise regardless source destination tags number ways bit rct 1 k gamma 1 thus given external memory requested equation probability noncluster fu bu routing delay routing dependent stage uturn going take place thus within summation equation include delay switch traversed particular path discussed turning stage fts traverse stages left fts twice thus delay except turning stage 2 theta term multiplied two considers acknowledgment packets also request acknowledgment also traverse turning stage memory module delay r dm including delay 2 theta probability gives us equation fl forward routing j finally sourcedestination pairs dont fall routing size mbn fu bu routing fw bw routing ii number destinations different network sizes using different routings routing categories forward routing path taken since forward routing backward routing last choice type sourcedestination pairs simply express j type routing switches traversed thus giving summation switch response times thus expected delay routings expressed fi p fl p given equations 4 6 respectively equations 0 9 valid local memory accessed probability memories addressed equal probabilities ie 1 actual case interaction tasks within cluster equations easily extended include cases table 2 shows number destinations reached processor 0 routings function network size switch size network 2 theta 2 observed table significant number connections benefit routings fw bw commonly adopted today also number processors use fu bu routing two successive network sizes explain behavior example consider corresponding network sizes 64 128 respectively networks though different sizes number destinations fu routing addition one stage introduces true center stage true center stage l 6 since center bit ct tag 0 fu bu routing apparent addition center stage increase number possible fu bu routings delays r 0 r n dm depend amount traffic network turn function p u b service demand individual service centers queueing analysis delays given next c queueing delays switches order make analysis simpler stage network considered isolation stages consider queueing center n inputs let probability packet one inputs given cycle q service demand packet service center cycles number requests coming queue service time previous request form binomial distribution number q mean number arriving requests variance q average queue length q queueing center found using pollaczekkhinchine pk mean value formula 14 e throughput requests et hence using littles law mean response time center r derived l l bus mbn switch queue b bmin switch queue fig 8 queues bmin mbn switches queueing models mbn switch bmin switch shown figure 8 mbn switch contention bus packets k right ports k left ports switch stage probability packet visits stage calculate mean switch response time r mbn switches using following equation network delay n sum response times stages packet visits routed network case bmin 2k inputs 2k outputs switch request probability input output bmin switch stage p u following model shown fig 8b calculate response time bmin switch using total network delay n sum response times switches different stages networks mean number arriving requests memory module internal external requests memory module respectively variance hence average memory queue length mean memory response time delay packet request takes optimal path source destination number switches traversed would depend nature ct rct delays derived inserted equations 357 8 turn plugged equation 2 obtain processor utilization response time network get nonlinear equation p u single variable solved using iteration techniques iteration technique used compute processor utilization p u presented follows 1 initialize p u guess expected processor utilization better guess lesser number iterations computation 2 calculate request probabilities stage network memory module intermediate step might calculate static values p probability stage network traversed 3 calculate mean switch response times memory response time r r respectively 4 based values calculate network delay memory delay using equations provided ff fi fl j 5 based values calculate new processor utilization p u 6 repeat steps 25 new p u within tolerance last p u initial value 05 p u accuracy 000001 used generate analytical results presented next section v results discussions performed extensive cyclebycycle simulations verify proposed routings work measured routing probabilities network delays 16 simulation done using synchronous packetswitched distributed memory environment simulation specifications analysis detailed view making network operation clear processor utilization request probability simulationmbnm01 analysismbnm01 simulationmbnm09 analysismbnm09 fig 9 comparison analysis simulation processor utilizations mbn varying simulations cycle considered time required transmission packet one output buffer switch next stage output buffer includes transmission packet link time switch takes route corresponding destination buffer minimum time taken packet reach memory based number switches routing covers four routings discussed section 2 used simulations simulation compares source destination running optimal routing algorithm chooses proper routing choice backward forward routing made follows memory requests could use either forward fw backward bw routing use forward routing acknowledgements packets use backward routing keep load distribution routings apart differences routing decisions based solely tags generated optimal routing algorithm probabilities fed simulation input parameters memories except local memories equally likely addressed upon memory request section present relative performance bmin mbn start comparing response time request probability simulationmbnm01 analysismbnm01 simulationmbnm09 analysismbnm09 fig 10 comparison analysis simulation response time mbn varying m010305070 processor utilization request probability mbnm01 bminm01 cminm01 mbnm09 bminm09 cminm09 fig 11 comparison processor utilizations varying response time request probability mbnm01 bminm01 cminm01 mbnm09 bminm09 cminm09 fig 12 comparison response time varying results simulation versus obtained analysis mbn many simulation experiments run verify analytical models developed paper simulation results closely matched analysis varied parameters present results 64 theta 64 system 2 theta 2 switches memory service time assumed 4 cycles processor utilization p u defined average amount useful work processor given cycle response time defined average difference time processor submits memory request time gets reply back figures show comparison analysis simulation results processor utilization response time mbn figures analytical results match closely simulation indicating independence queues assumed analysis cause much error plots show results analysis simulation function memory request probability p plot memory request probability p varied 01 10 two values local memory request probability 01 09 chosen larger values requests satisfied without going mbn thus processor utilization number processors p01mbn p01bmin p05mbn p05bmin fig 13 processor utilization scalability mbn vs bmin varying p much higher fig 9 response time much lower fig 10 p gets larger requests generated response time increases due processor utilization reduces higher amount traffic queueing delays figures 11 12 show comparison performance mbn conventional min cmin proposed bidirectional min bmin conventional min similar network employed butterfly machine 3 request response packets travel one direction processor memory side hand bmins allow four routings proposed mbn two plots show processor utilization response time three networks two different values mbn behaves exactly similar cmin bmin terms processor utilization response time also networks 09 01 bmin performs better mbn mbn performs better cmin figures 13 14 show processor utilization response times various system sizes results obtained local memory request probability fixed 05 two different values p 01 05 see figures even system size grows response time number processors p01mbn p01bmin p05mbn p05bmin fig 14 response time scalability mbn vs bmin varying p performance mbn remains close bmin curves cmin left clarity observed mbn always performs better cmin also seen figures system size doubles reduction performance big indicating mbn highly scalable given traffic load range processor utilization remains approximately 05 04 system size changes 32 1024 switches request probability p much greater effect performance finally present processor utilization response time mbn obtained different switch sizes different number processors table iii places table left empty n theta n mbn cannot built using k theta k switches request probability p local memory request probability fixed 05 decrease increased 4 8 mbn less efficient due increased contention delay 8x8 busbased switch hand 512 theta 512 system k increased 2 8 good improvement number switches entire network still quite high keeping contention low enough gain performance procs different switch sizes mbn bmin mbn bmin mbn bmin mbn bmin mbn bmin mbn bmin iii processor utilization response time mbn bmin varying k n compare mbns performance bmins see switch size increases bmin gives higher processor utilization lower response time increase performance due lower contention crossbar switch however bmin gives increased performance expense cost 8 cost parameter based number connection points switch presented number connections k 2 k theta k switch bus number connections 2k thus total cost bmin mbn kn log k n 2n log k n respectively include parameters along processor utilization response time costeffectiveness mbn higher bmin shown 8 4 theta 4 switch size works costefficient different network sizes workload inputs vi executiondriven simulation results execution time application multiprocessor architecture ultimate parameter indicates performance order show mbn performs similar bidirectional min bmin study performance using executiondriven simulation various applications simulator based proteus 10 originally developed mit however original simulator modeled indirect interconnection networks based analytical model modified simulator extensively exactly model bmin mbn using 2 theta 2 switches packetswitching strategy system considered paper private cache memories operate based directorybased cachecoherence protocol 15 node configuration network cache memory network interface proc fig 15 node configuration network interface interface simulator modeled shown figure 15 cache controller cc memory controller mc connected network interface directly communicate nodes system parameters used simulation given table iv details simulation found 16 17 benchmark applications selected numerical applications workload evaluating network performance cachecoherent sharedmemory environment applications multiplication two 2d matrix matmul floydwarshalls allpairshortestpath algorithm fwa blocked lu factorization dense 2d matrix lu 1d fast fourier transform fft simulation rarefied flows objects wind tunnel mp3d matrix multiplication done two 128 theta 128 double precision matrices principal data structures four shared twodimensional arrays real numbers two input matrices transpose matrix one output matrix shared data size 512 kbytes floydwarshalls algorithm used graph 128 nodes random weights assigned edges principal data structures two shared twodimensional arrays integers one distance matrix another predecessor matrix shared data size 128 kbytes program goes many iterations number vertices iteration particular row distance predecessor matrix read processors iteration followed barrier parameter value number processors 64 shared memory size per node 32 kbytes cache size 8 kbytes cache line size 32 bytes cache access time 1 memory access time 8 switching delay 1 link width 16 bits flit length 16 bits packet size 8 bytes iv simulation parameters blocked lu decomposition application done 256 theta 256 matrix using 8 theta 8 blocks principal data structure twodimensional array first dimension block second contains data points block manner data points block operated processor allocated contiguously false sharing line interference eliminated implemented cooleytukey 1d fft algorithm simulations done input points principal data structures two 1d arrays complex numbers data sharing first log 2 stages n number data points p number processors rest log 2 p stages every data point shared two processors stages instead using two separate input output arrays interleave arrays avoid large number conflict misses mp3d threedimensional particle simulator used rarefied fluid flow simulation used 16000 molecules default geometry provided splash 18 uses 14 theta 24 theta 4 2646cell space containing single flat sheet placed angle free stream simulation done 5 time steps shared data set size 800 kbytes work partitioned molecules statically scheduled processors clump size 8 used details benchmarks application memory references cache misses messages generated matmul 9215571 743258 1474220 lu 112270761 706579 1671854 characteristics applications used evaluation application bmin latency mbn latency cmin latency matmul 22448 23125 53848 vi average message latencies using different networks simulation environment found 17 b simulation results characteristics applications measured simulation given table v table shows number shared memory references cache misses shared memory references total number messages generated execution begin presenting average message latencies experienced applications run shared memory environment values shown table vi show response time messages significantly differ use mbn opposed bmin however response time cmin significantly higher mbn bmin applications thus time taken serving cache misses higher using cmin mbn use uturn routing strategies one factors reducing average latencies interconnection networks introduction uturns reduces response times messages also distributes messages throughout switches use application stage 05 stage 1 stage 2 stage 3 stage 4 turn uturns matmul 79327 185033 343747 313942 157439 3996773 213 mp3d 138163 295884 752859 684434 345831 7664693 2244 vii number uturns taken different stages mbn write stall time read stall time parallel execution synchronization time execution time million cycles execution time million cycles execution time million cycles execution time million cycles execution time million cycles4mbn matmul4mbn mbn packet buffers per link 64 bytes mbn ctfft2mbn48 mp3d cmin cmin cmin bmin bmin bmin bmin bmin cmin cmin fig 16 executionbased simulation results fu bu strategies shown table vii lists number packets made uturn stage mbn bmin network model allow uturns last stage network since turn last stage similar turn first stage counted six stages 2 theta 2 switches 64 theta 64 network turns first last stage 0 5 constitute messages sent cluster memories last column table shows percentage packets taking uturns application number uturns quite significant evaluation based execution time applications gives performance interconnection networks realistic environment simulator gives execution time application millions cycles also measured times different activities shown fig 16 timings shaded differently following activities ffl time spent computation synchronization ffl read stall time maximum read stall time experienced processor ffl write stall time maximum write stall time experienced processor graph fig 16 divided 5 sets 3 bars set different application shows execution time using mbn vs bmin vs cmin figure shows bmin mbn give much better performance cmin improvement overall execution time mainly due reduction read stall time write stall time directly related network latency seen results mbn performance similar bmin performance observed applications considered write stall time lower read stall time fewer write misses vii conclusions paper presented multistage bus network mbn offers better performance reliability conventional multistage interconnection network min equivalent bidirectional min bmin also presented performance comparison self routing schemes different paths developed based routing tags algorithm presented find path minimum distance source destination probabilities taking different paths derived queueing analysis presented evaluating performance mbn bmin analysis verified simulations comparisons results min made shown performance mbn similar bmin terms response time processor utilization much better conventional min emphasize potential mbn executionbased evaluation also presented cachecoherent shared memory multiprocessor environment use directorybased cache coherence protocol performance mbn environment also shown almost equal bmin terms message latency execution time five applications fact along simplicity hardware better faulttolerance makes mbn viable alternative existing min r hierarchical cachebus architecture shared memory multiprocessors technical summary butterfly parallel processor overview version 1 sp1 highperformance switch network architecture connection machine cm5 design performance generalized interconnection networks multistage bus network mbn interconnection network cache coherent multipro cessors performance reliability multistage bus network optimal software multicast wormholerouted multistage networks proteus highperformance parallel architecture simulator outputbuffer switch architecture asynchronous transfer mode performance multistage interconnection networks multiprocessors finite buffer analysis multistage interconnection networks queueing systems volume new solution coherence problems multicache systems distributed shared memory multiprocessors using multistage bus networks evaluating virtual channels cache coherent shared memory multiprocessors splash stanford parallel applications sharedmemory tr ctr l n bhuyan h wang r iyer impact ccnuma memory management policies application performance multistage switching networks ieee transactions parallel distributed systems v11 n3 p230246 march 2000 kenneth hoganson mapping parallel application communication topology rhombic overlappingcluster multiprocessors journal supercomputing v17 n1 p6790 aug 2000 ravishankar r iyer laxmi n bhuyan design evaluation switch cache architecture ccnuma multiprocessors ieee transactions computers v49 n8 p779797 august 2000 chadi aljundi jeanluc dekeyser mtahar kechadi isaac scherson universal performance factor multicriteria evaluation multistage interconnection networks future generation computer systems v22 n7 p794804 august 2006 ravishankar iyer hujun wang laxmi narayan bhuyan design analysis static memory management policies ccnuma multiprocessors journal systems architecture euromicro journal v48 n13 p5980 september 2002