choosing regularization parameters iterative methods illposed problems numerical solution illposed problems often accomplished discretization projection onto finite dimensional subspace followed regularization discrete problem high dimension though typically compute approximate solution projecting discrete problem onto even smaller dimensional space via iterative methods based krylov subspaces work present common framework efficient algorithms regularize second projection rather show determining regularization parameters based final projected problem rather original discretization firmer justification often involves less computational expense prove results approximate equivalence approach forms regularization present numerical examples b introduction linear discrete illposed problems form 1 min x equivalently 2 arise example discretization firstkind fredholm integral equations occur variety applications shall assume fullrank matrix 2 1 discrete illposed problems illconditioned gap singular value spectrum typically right hand side b contains noise due measurement andor approximation error noise combination illconditioning means exact solution 1 2 little relationship noisefree solution worthless instead use regularization method determine solution approximates noisefree solution regularization methods replace original operator betterconditioned related one order diminish effects noise data produce regularized solution original problem sometimes regularized problem large solve exactly case typically compute approximate solution projection onto even smaller dimensional space perhaps via iterative methods based krylov subspaces conditioning new problem controlled one regularization parameters specific method large regularization parameter yields new wellconditioned problem solution may far noisefree solution since new operator poor approximation small regularization parameter generally yields solution close noisecontaminated exact solution 1 2 hence distance noisefree solution also large thus work supported national science foundation grants ccr 9503126 ccr9732022 army research office muri grant daag559710013 dept computer electrical engineering northeastern university boston 02115 z dept computer science institute advanced computer studies university mary land college park md 20742 olearycsumdedu key issue regularization methods choose regularization parameter balances error due noise error due regularization wise choice regularization parameter obviously crucial obtaining useful approximate solutions illposed problems problems small enough rank revealing factorization singular value decomposition computed wellstudied techniques computing good regularization parameter techniques include discrepancy principle 8 generalized crossvalidation gcv 9 lcurve 15 larger problems treated iterative methods though parameter choice much less understood regularization applied projected problem generated iterative method essentially two regularization parameters one standard regularization algorithms tikhonov truncated svd one controlling number iterations taken one subtle issue standard regularization parameter correct discretized problem may optimal one lowerdimensional problem actually solved iteration observation leads research discussed paper first glance appear lot work associated selection good regularization parameter many algorithms proposed literature needlessly complicated regularizing projection iterative method regularizing lower dimensional problem actually solved much difficulty vanishes purpose paper present parameter selection techniques designed reduce regularization work iterative methods krylov subspace tech niques paper organized follows x2 give overview regularization methods considering follow x3 surveying methods choosing corresponding regularization parameters x4 show parameter selection techniques original problem applied instead projected problem obtained iterative method greatly reducing cost without much degradation solution give experimental results x5 conclusions future work x6 2 regularization background following shall assume e b true denotes unperturbed data vector e denotes zeromean white noise also assume b true satisfies discrete picard condition spectral coefficients b true decay faster average singular values assumptions easy see exact solution 1 2 hopelessly contaminated noise let denote singular value decomposition columns u v singular vectors singular values ordered oe 1 oe solution 1 2 given consequence white noise assumption ju ej roughly constant discrete picard condition guarantees ju oe matrix illconditioned small singular values magnify corresponding coefficients e second sum large contribution noise approximate null space renders exact solution x defined 3 worthless following regularization methods try different ways lessen contribution noise solution information methods see example 17 21 tikhonov regularization one common methods regularization tikhonov regularization 34 method problem 1 2 replaced problem solving l denotes matrix often chosen identity matrix discrete derivative operator positive scalar regularization parameter ease notation assume solving 4 equivalent solving analogy 3 solution contributions noise components u e values oe much smaller 3 thus x closer noisefree solution x large however far original operator x far x true solution 2 conversely small singular values new operator close thus x x small singular values greatly magnify noise components 22 truncated svd truncated svd method regularization regularized solution chosen simply truncating expansion 3 regularization parameter number terms dropped sum observe small terms dropped sum x resembles x effects noise large large however important information could lost case alternative yet related approach tsvd approach introduced rust 31 truncation strategy based value spectral coefficient strategy include sum 3 terms corresponding spectral coefficient b whose magnitude greater equal tolerance ae regarded regularization parameter 23 projection iterative methods solving 5 7 impractical n large fortunately regularization achieved projection onto subspace see example 7 truncated svd example one projection solution constrained lie subspace spanned singular vectors corresponding largest values projections economical general constrain regularized solution lie kdimensional subspace c n spanned columns n theta k matrix q k example choose x k min kaq equivalently idea appropriately chosen subspace operator q k aq k better conditioned original operator hence x k reg approximate x true well subspace projection often achieved use iterative methods conjugate gradients gmres qmr krylov subspace methods matrix contains orthonormal columns generated via lanczos tridiagonalization bidiagonalization process 27 1 case q k basis kdimensional krylov subspace ie subspace k k c k spanned vectors c matrix k vector c regularized solutions x k reg generated iteratively subspaces built krylov subspace algorithms cg cgls gmres lsqr tend produce early iterations solutions resemble x true subspace spanned right singular vectors corresponding largest singular values later iterations however methods start reconstruct increasing amounts noise solution due fact large k operator q k aq k approaches illconditioned operator fore choice regularization parameter k stopping point iteration dimension subspace important 1 24 hybrid methods projection plus regularization another important family regularization methods often referred hybrid methods 17 introduced oleary simmons 27 methods combine projection method direct regularization method tsvd tikhonov regularization problem projected onto particular subspace dimension k typically restricted operator 9 still illconditioned therefore regularization method applied projected problem since dimension k usually small relative n regularization restricted problem much less expensive yet appropriately chosen subspace end results similar achieved applying direct regularization technique original problem become precise similar solutions x45 projected problems usually generated iteratively lanczos method approach useful sparse structured way matrixvector products handled efficiently minimal storage 3 existing parameter selection methods section discuss sampling parameter selection techniques proposed literature differ amount priori information required well decision criteria 31 discrepancy principle extra information available example estimate variance noise vector e regularization parameter chosen rather easily morozovs discrepancy principle 25 says ffi expected value kek 2 regularization parameter chosen norm residual corresponding regularized solution x reg usually small values regularization parameter correspond closer solution noisy equation despite call k rather 1k regularization parameter x l fig 1 example typical lcurve particular lcurve corresponds applying tikhonov regularization problem example 2 predetermined real number note methods based knowledge variance given example 12 5 32 generalized crossvalidation generalized crossvalidation gcv parameter selection method depend priori knowledge noise variance idea golub heath wahba 9 find parameter minimizes gcv functional denotes matrix maps right hand side b onto regularized solution x tikhonov regularization example gcv chooses regularization parameter dependent one data measurement 11 1213 33 lcurve one way visualize tradeoff regularization error error due noise plot norm regularized solution versus corresponding residual norm set regularization parameter values result lcurve introduced lawson popularized hansen 15 figure 1 typical example regularization parameter increases noise damped norm solution decreases residual increases intuitively best regularization parameter lie corner lcurve since values higher residual increases without reducing norm solution much values smaller norm solution increases rapidly without much decrease residual practice points lcurve computed corner located approximate methods estimating point maximum curvature 19 like gcv method determining regularization parameter depend specific knowledge noise vector 34 disadvantages parameter choice algorithms appropriate choice regularization parameter especially projection algorithms difficult problem method severe flaws basic cost added cost disc gcv lcurve rusts tsvd omn 2 om log om log om log projection table summary additional flops needed compute regularization parameter four regularization methods various parameter selection techniques notation q cost multiplication vector p number discrete parameters must tried k dimension projection n problem dimensions discrepancy principle convergent noise goes zero relies knowing information often unavailable incorrectly estimated even correct estimate variance solutions tend oversmoothed 20 pg 96 see also discussion x61 15 one noted difficulty gcv g flat minimum making difficult determine optimal numerically 35 lcurve usually tractable numerically limiting properties nonideal solution estimates fail converge true solution n 1 36 error norm goes zero 6 methods assume knowledge error norm including gcv latter property 6 discussion references parameter choice methods see 5 17 cost methods tabulated table 1 35 previous work parameter choice hybrid methods first glance appears tikhonov regularization multiple systems form 5 must solved order evaluate candidate values discrepancy principle lcurve techniques suggested literature solving systems using projection methods chan ng 4 example note systems involve closely related matrices matrices suggest solving systems simultaneously using galerkin projection method sequence seed systems although economical storage unnecessarily expensive time exploit fact fixed k krylov subspace k k values frommer maass 8 propose two algorithms approximating satisfies discrepancy principle 10 first truncated cg approach use conjugate gradients solve k systems form 5 truncating iterative process early large using previous solutions starting guesses later problems like chan ng algorithm exploit redundancy generating krylovsubspaces second method propose however exploit redundancy cg iterates k systems updated simultaneously extra matrixvector products stop shifted cg algorithm kax one values thus number matrixvector products required twice number iterations particular system converge note algorithms propose x4 finding good value based key observation regarding krylov subspace methods usually require less work shifted cg algorithm calvetti golub reichel 3 compute upper lower bounds lcurve generated matrices c using lanczos bidiagonalization process approximate best parameter tikhonov regularization without projection x4 choose instead approximate best parameter tikhonov regularization projected problem since approximation continuous problem actually used kaufman neumaier 21 suggest envelope guided conjugate gradient approach tikhonov lcurve problem method complicated methods propose maintain nonnegativity constraints variables substantial work also done tsvd regularization projected problems bjorck grimme van dooren 2 use gcv determine truncation point projected svd emphasis stable ways maintain accurate factorization many iterations needed use full reorthogonalization implicit restart strategies oleary simmons 27 take somewhat different viewpoint problem preconditioned appropriately massive number iterations unnecessary viewpoint echoed current work implicitly assume problem leftpreconditioned filtered 27 example place 4 solve min x 2for square preconditioner see 14 26 24 23 preconditioners appropriate certain types illposed problems note could alternately considered right preconditioning amounts solving tikhonov case min setting note either left right preconditioning effectively changes balance two terms minimization 4 regularizing projected problem section develop nine approaches regularization using krylov methods many krylov methods proposed ease exposition focus two lsqr algorithm paige saunders 29 gmres algorithm saad schultz 33 lsqr algorithm paige saunders 29 iteratively computes bidiag onalization introduced golub kahan 10 given vector b algorithm follows 29 alg bidiag 1 compute scalar fi 1 vector u 1 length one fi 1 similarly determine ff 1 v 1 ff 1 nonnegative scalars ff i1 fi i1 chosen u i1 v i1 length one end vectors u called left right lanczos vectors respectively algorithm rewritten matrix form first defining matrices ff k denoting ith unit vector following relations established u subscript denotes dimension identity suppose want solve min denotes kdimensional subspace spanned first k lanczos vectors solution seek form x vector k length k corresponding residual relations observe exact arithmetic since u k1 exact arithmetic orthonormal columns therefore projected problem wish solve min solving minimization problem equivalent solving normal equations involving bidiagonal matrix typically k small reorthogonalization combat effects inexact arithmetic might might necessary matrix b k may illconditioned singular values approximate small singular values therefore solving projected problem might yield good solution k however use methods section 3 regularize projected problem discuss options detail alluded x4 idea generate k reg regularized solution 18 compute regularized solution 16 reg used algorithm gmres instead lsqr would derive similar relations though u v matrices identical b matrix upper hessenberg rather bidiagonal conjugate gradients would yield similar relationships cost comparisons methods see tables 1 2 storage comparisons given tables 3 4 41 regularization projection mentioned earlier terminate iteration k steps projected solution onto k dimensional subspace regularizing effect sometimes sufficient determining best value k accomplished instance one three methods parameter choice 1 discrepancy principle case stop iteration smallest value k kr k k ffi lsqr gmres recurrence relations determining kr k k using scalar computations without computing either r k x k 29 32 2 gcv projected problems see x41 defined either lsqr gmres operator aa given u pseudoinverse matrix b k thus 11 gcv functional 17 note fact two distinct definitions b hence two definitions denominator gk small enough k two comparable definition use less expensive calculate 18 x74 3 lcurve determine lcurve associated lsqr gmres estimates needed several values k using either algorithm compute kr k k 2 scalar calculations paige saunders give similar method computing kx k k 2 29 gmres cost computing using method gcv one must go iterations beyond optimal k order verify optimum 19 42 regularization projection plus tsvd projection alone regularize compute tsvd regularized solution projected problem 19 need svd requires ok 3 operations also computed svd b kgamma1 ok 2 operations 13 clearly still need use type parameter selection technique find good value k first notice easy compute norms residual solution resulting neglecting smallest singular values jk component e 1 direction jth left singular vector b k fl j jth singular value ordered largest smallest residual solution 2norms fi 1 kgammak x using fact use three sample methods 1 discrepancy principle let r k denote quantity b gamma ax k note 13 orthonor mality kr k k 2 equal first quantity 20 therefore choose k largest value kr k value exists 2 gcv another alternative choosing k use gcv compute k projected problem gcv functional kth projected problem obtained substituting b k b substituting expression residual 20 numerator 11 3 lcurve many lcurves one value k coordinate values 20 form discrete lcurve given k desired value k chosen without forming approximate solutions residuals k increases value k chosen discrepancy principle monotonically nondecreasing 43 regularization projection plus rusts tsvd standard tsvd use rusts version tsvd regularization projected problem requires compute svd using previous notation rusts strategy set ae ae ik q k right singular vectors b k k aeg focus three ways determine ae 1 discrepancy principle using notation previous section norm regularized solution given fi 1 ae ik according discrepancy principle must choose ae residual less ffi practice would require residual evaluated sorting values j ik j adding terms order residual norm less ffi 2 gcv let us denote cardi k ae cardinality set k ae 11 easy show gcv functional corresponding projected problem regularization technique given ae ik ae practice k first sort values j ik smallest largest define k discrete values ae j equal values ae 1 smallest set ae values sorted magnitudes svd expansion coefficients j finally take regularization parameter ae j g k ae j minimum 3 lcurve standard tsvd one lcurve value k fixed k define ae gcv reorder fl way j ik j reordered sorted solution residual norms plotted functions ae value ae j corresponding corner selected regularization parameter 44 regularization projection plus tikhonov finally let us consider using tikhonov regularization regularize projected problem 18 integer k thus given regularization parameter would like solve min equivalently min solution either formulation satisfies using 13 15 see k also satisfies av k b therefore using x thus k n backprojected regularized solution x approaches solution 4 need address choose suitable value 1 discrepancy principle note exact arithmetic r hence kb k k therefore use discrepancy principle requires choose kr k discrete trial values j given k take largest value j kr k exists increase k test 2 gcv let us define b k operator mapping right hand side projected problem onto regularized solution projected problem given svd b k denominator gcv functional defined projected problem refer 11 k numerator simply kr k 2 values k n feasible compute singular values b k 3 lcurve lcurve comprised points kb k k using 25 orthonormality columns v k see points precisely kr k discrete values quantities kr k k 2 obtained updating respective estimates k gamma 1st iteration 2 45 correspondence direct regularization projection plus regularization section argue projection plus regularization approaches expected yield regularized solutions nearly equivalent direct regularization counterpart following theorem establishes desired result case tikhonov vs projection plus tikhonov theorem 41 fix 0 define x kth iterate conjugate gradients applied tikhonov problem let k exact solution regularized projected problem derived original problem b set z k z proof discussion beginning x44 equations 23 24 follows k solves b columns v k lanczos vectors respect matrix righthand side b lanczos vectors generated respect matrix righthand side b therefore v k k precisely kth iterate conjugate gradients applied pg 495 hence z k 2 2 technical details approach found 28 pp 197198 obtain implementation details estimating kx k k kr k taken paige saunders algorithm httpwwwnetliborglinalglsqr projection plus disc gcv lcurve table summary flops projection plus inner regularization various parameter selection techniques addition oqk flops required projection k number iterations ie size projection taken p number discrete parameters must tried let us turn case tsvd regularization applied original problem vs projection plus tsvd approach direct computation convinces us two methods compute regularized solution arithmetic exact approximate result holds exact arithmetic take k iterations n let singular value decomposition b k denoted define theta j matrix w sj regularized solution obtained tsvd regularization projected problem reg denotes leading j theta j principle submatrix gamma k k taken enough larger j v k q k w kj u leading principle submatrix sigma expect x k reg good approximation x made precise following theorem theorem 42 let k j contain first j columns u respectively let reg kbk proof using representations x 2 b obtain reg conclusion follows bounding term 2 note typically oe j ae oe n 1oe j large results relating value k necessary hypothesis theorem hold interested reader referred theory kanielpaige saad 30 x124 basic cost added cost disc gcv lcurve tsvd oq o1 om om rusts tsvd oq om om om projection okn o1 ok ok table summary additional storage four regularization methods three parameter selection techniques original matrix theta n q nonzeros p number discrete parameters must tried k iterations used projection factorizations assumed take q storage projection plus disc gcv lcurve rusts tsvd ok ok table summary storage including storage matrix projection plus inner regularization approach various parameter selection techniques p denotes number discrete parameters tried regularization methods also requires us save basis v else regenerate order reconstruct x 5 numerical results section present two numerical examples experiments carried using matlab hansens regularization tools 16 ieee double precision floating point arithmetic since exact noisefree solutions known examples evaluated methods using two norm difference regularized solutions exact solutions examples applied rusts method original problem ae taken magnitudes spectral coefficients b sorted increasing order 51 example 1 200theta200 matrix true solution x true example generated using function baart hansens regularization toolbox generated true computed noisy vector b b e e generated using matlab randn function scaled noise level kb truek condition number order 10 19 many values tested log displays values regularization parameters chosen three parameter selection techniques applied together one four regularization methods original problem since 53761egamma4 set ffi defines discrepancy principle close approximation 55egamma4 last column table gives value parameter yielded regularized solution minimum relative error compared true solution relative error values regularized solutions corresponding parameters table 5 given table 6 note using gcv determine regularization parameter rusts tsvd resulted extremely noisy solution huge error corners lcurves tikhonov projection tsvd methods determined using hansens lcorner function modification points corresponding solution norms greater 10 6 tsvd methods rusts tsvd ae 1223egamma4 9645egamma7 1223egamma4 1259egamma4 1223egamma4 projection table example 1 parameter values selected method disc gcv lcurve optimal rusts tsvd 1213 7e14 1213 1213 projection 1134 1207 1134 1134 table example 1 comparison kx true 4 regularization methods original problem regularization method chosen using methods indicated considered otherwise false corner resulted next projected using lsqr regularized projected problem one three regularization methods considered three methods computed regularization parameters projected problem using discrepancy gcv lcurve computed corresponding regularized solutions parameters selected case iterations 10 40 given tables 7 9 respectively lcorner routine used determine corners respective lcurves comparing table 6 8 observe computing regularized solution via projection plus tikhonov projection size 10 using either discrepancy principle lcurve find regularization parameter gives results good techniques used tikhonov original problem determine regularized solution similar statements made projection plus tsvd projection plus rusts tsvd also note tikhonov without projection none errors tables optimal parameter selection techniques ever gave parameter error minimal 52 example 2 255 theta 255 matrix example symmetric toeplitz matrix bandwidth 16 exponential decay across band 3 true solution vector x true displayed top picture figure 2 generated true computed noisy vector b b e e generated using matlab randn function scaled noise level kek kb truek vector b shown bottom figure 2 condition number generated discrete using log gamma1 norm noise vector 716egamma2 took value ffi defines discrepancy principle 800egamma2 example took 61 iterations lsqr reach minimum relative error 948egamma2 several iterations needed lcurve method 3 generated using matlab command rusts tsvd aek 1679egamma4 1773egamma4 1679egamma5 1679egamma5 table example 1 iteration 10 regularization parameters selected projection plus tikhonov tsvd rusts tsvd disc gcv lcurve optimal rusts tsvd 1213 1663 1213 1213 table example 1 iteration 10 comparison kx true projection plus tikhonov tsvd rusts tsvd disc gcv lcurve optimal rusts tsvd aek 9201egamma5 1225egamma4 9201egamma5 9201egamma5 table example 1 iteration 40 regularization parameters selected projection plus tikhonov tsvd rusts tsvd disc gcv lcurve optimal rusts tsvd 1162 1162 1162 1162 table example 1 iteration 40 comparison kx true projection plus tikhonov tsvd rusts tsvd 50 100 150 200 250 22610exact solution 50 100 150 200 250 226 fig 2 example 2 top exact solution bottom noisy right hand side b rusts tsvd ae 2183egamma2 2586egamma6 1477egamma2 1527egamma2 projection table example 2 parameter values selected method projection performed left preconditioned system disc gcv lcurve optimal rusts tsvd projection table example 2 comparison kx true 4 regularization methods original problem estimate stopping parameter likewise dimension k projected problem around 60 obtain good results projectionplusregularization ap proaches much larger 60 lcurve applied projected tikhonov regularized problem give good estimate corner respect tikhonov regularized original problem therefore projection based techniques chose work left preconditioned system refer discussion end x 35 preconditioner chosen 22 parameter defining preconditioner taken values regularization parameters chosen three parameter selection techniques applied together one four regularization methods original problem given table 11 last column table gives value parameter gave regularized solution minimum relative error range discrete values tested respect true solution relative errors resulted computing solutions according parameters table 11 table 12 note gcv tsvd rusts tsvd ineffective corners lcurves tikhonov projection tsvd methods determined using hansens lcorner function modification points corresponding largest solution norms tsvd methods considered otherwise false corner detected lcorner routine next projected using lsqr note since matrix preconditioner symmetric could used minres 22 regularized projected problem one three methods considered three methods computed regularization parameters projected problem using dis crepancy gcv lcurve computed corresponding regularized solutions parameters selected case iterations 15 25 given tables 13 15 respectively relative errors regularized solutions generated accordingly given tables 14 16 used lcorner routine determine corners respective lcurves except case rusts tsvd method latter case rusts tsvd aek 3558egamma2 3558egamma2 3558egamma2 3558egamma2 table example 2 iteration 15 regularization parameters selected projection plus tikhonov tsvd rusts tsvd disc gcv lcurve optimal rusts tsvd table example 2 iteration 15 comparison kx true projection plus tikhonov tsvd rusts tsvd always sharp corner could picked visually comparing table 11 tables 13 15 see parameter chosen applying lcurve method projectedplustikhonov problem parameter chosen applying lcurve original problem moreover comparison table 12 tables 14 16 shows relative errors regularized solutions computed accordingly comparable applying tikhonov original problem parameter similar results shown cases exception discrepancy principle work well projection plustsvd problems gcv effective projected problems 6 conclusions work given methods determining regularization parameter regularized solution original problem based regularizing projected problem proposed approach applying regularization parameter selection techniques projected problem economical time storage presented results fact regularized solution obtained backprojecting tsvd tikhonov solution projected problem almost equivalent applying tsvd tikhonov original problem almost depends size k examples indicate practicality method illustrate regularized solutions usually good computed using original system computed fraction time using fraction storage note similar approaches valid using krylov subspace methods computing projected problem work address potential problems loss orthogonality iterations progress discussion however assume either k naturally small compared n preconditioning applied enforce condition possibly reason found modest k roundoff appear degrade either lsqr estimates residual solution norms computed regularized solution following sense regularization parameters chosen via projectionregularization corresponding regularized solutions comparable chosen generated original discretized problem tikhonov approach paper assumed regularization disc gcv lcurve optimal rusts tsvd aek 4828egamma2 7806egamma3 4828egamma2 4828egamma2 table example 2 iteration 25 regularization parameters selected projection plus tikhonov tsvd rusts tsvd disc gcv lcurve optimal rusts tsvd table example 2 iteration 25 comparison kx true projection plus tikhonov tsvd rusts tsvd operator l identity related preconditioning operator allowed us efficiently compute kr k k kx k k multiple values efficiently k l identity invertible first implicitly transform problem standard form 17 lx solve equivalent system min projection plus regularization schemes may applied transformed problem clearly projection based schemes useful long solving systems involving l done efficiently r estimation lcurve via lanczos bidiagonal ization galerkin projection method solving multiple linear systems minimum reconstruction error choice regularization pa rameters efficient methods application deconvolution problems using lcurve determining optimal regularization pa rameters equivalence regularization truncated iteration solution illposed image reconstruction problems fast cgbased methods tikhonovphillips regularization generalized crossvalidation method choosing good ridge parameter calculating singular values pseudoinverse matrix matrix computations theory tikhonov regularization fredholm equations first kind stable fast algorithm updating singular value decom position preconditioned iterative regularization illposed problems analysis discrete illposed problems means lcurve matlab package analysis solution discrete illposed problems use lcurve regularization discrete illposed problems regularization applied inverse illposed problems regularization illposed problems envelope guided conjugate gradients symmetric cauchylike preconditioners regularized solution 1d illposed problems pivoted cauchylike preconditioners regularized solution illposed problems solution functional equations method regularization iterative image restoration using approximate inverse preconditioning bidiagonalizationregularization procedure large scale discretization illposed problems algorithm 583 symmetric eigenvalue problem truncating singular value decomposition illposed problems iterative methods sparse linear systems gmres generalized minimal residual algorithm solving nonsymmetric linear systems solutions illposed problems pitfalls numerical solution linear illposed problems tr ctr angelika bunsegerstner valia guerraones humberto madrid de la vega improved preconditioned lsqr discrete illposed problems mathematics computers simulation v73 n1 p6575 6 november 2006 e f santos bassrei l curve approaches selection regularization parameter geophysical diffraction tomography computers geosciences v33 n5 p618629 may 2007 g landi lagrange method regularization discrete illposed problems computational optimization applications v39 n3 p347368 april 2008 alexander b konovalov vitaly v vlasov olga v kravtsenyuk vladimir v lyubimov spacevarying iterative restoration diffuse optical tomograms reconstructed photon average trajectories method eurasip journal applied signal processing v2007 n1 p1818 1 january 2007