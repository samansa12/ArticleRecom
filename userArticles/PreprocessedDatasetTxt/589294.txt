spectral bundle method semidefinite programming central drawback primaldual interior point methods semidefinite programs lack ability exploit problem structure cost coefficient matrices restricts applicability problems small dimension typically semidefinite relaxations arising combinatorial applications sparse wellstructured cost coefficient matrices huge order present method allows us compute acceptable approximations optimal solution large problems within reasonable timesemidefinite programming problems constant trace primal feasible set equivalent eigenvalue optimization problems convex nonsmooth programming problems solved bundle methods propose replacing traditional polyhedral cutting plane model constructed subgradient information semidefinite model tailored eigenvalue problems convergence follows traditional approach proof included completeness present numerical examples demonstrating efficiency approach combinatorial examples b introduction development interior point methods semidefinite programming 19 31 1 46 increased interest semidefinite modeling techniques several fields control theory eigenvalue optimization combinatorial optimization fact interior point methods proved useful reliable solution methods semidefinite programs moderate size however problem defined large matrix variables huge number constraints interior point methods grow terribly slow consume huge amounts memory efficient methods today 15 23 2 32 45 29 primaldual methods require iteration interior point method factorization dense matrix order equal number constraints one three factorizations positive semidefinite matrix variables within line search typical workstation restricts number constraints 2000 size matrix variables 500 reasonable performance required larger problems time memory requirements prohibitive important realize either primal dual matrix generically dense even cost coefficient matrices sparse recently pure dual approach proposed 4 offers possibilities exploit sparsity early judge potential method combinatorial optimization semidefinite relaxations introduced 27 time mainly considered theoretical tool obtaining strong bounds 11 28 40 development interior point methods hopes soared high relaxations could practical value within short time several approximation algorithms relying semidefinite programming published based approach goemans williamson 8 implementational side 14 16 20 cutting plane approaches semidefinite relaxations konradzusezentrum fur informationstechnik berlin takustrae 7 14195 berlin germany helmbergzibde httpwwwzibdehelmberg universitat klagenfurt institut f mathematik universitatsstr 6567 9020 klagenfurt aus tria franzrendlunikluacat financial support austrian fwf project p12660 mat greatfully acknowledged constrained quadratic 01 programming problems proved yield solutions high quality however mentioned expensive compute even problems small size hundred 01 variables problems arising practical applications starting thousand 01 variables reach believe method proposed paper open door problems size although combinatorial applications primary concern stress method restricted kind problems fact useful alternative interior point methods whenever number constraints order matrices quite large transform standard dual semidefinite program eigenvalue optimization problem reformulating semidefinite constraint nonnegativity constraint minimal eigenvalue slack matrix variable lifting constraint cost function means lagrange multiplier correct value lagrange multiplier known advance primal feasible matrices constant trace case combinatorial applications mind paper develop bundle method solving problem minimizing maximal eigenvalue affine matrix function additional linear objective term functions well known convex nonsmooth general method optimizing nonsmooth convex functions bundle method see eg 21 42 17 18 step function value subgradient function computed specific point means collected subgradients cutting plane model function formed minimizer cutting plane model augmented regularization term yields new point case eigenvalue optimization subgradient formed means eigenvector maximal eigenvalue extremal eigenvalues associated eigenvectors large symmetric matrices computed efficiently lanczos methods see eg 9 lanczos methods need subroutine computes product matrix vector allows exploit kind structure present matrix polyhedral cutting plane model used traditional bundle algorithms updated new subgradient information approximate well subdifferential thus function vicinity current point eigenvalue optimization problems subdifferential generated semidefinite set particular intersection simple affine constraint face semidefinite cone suggests use instead traditional polyhedral cutting plane model semidefinite cutting plane model works approximation face semidefinite cone specialization cutting plane model main contribution paper semidefinite bundle approach allows intriguing interpretation terms original semidefinite program cutting plane model requires dual slack matrix semidefinite program positive semidefinite respect subspace vectors thus may interpreted relaxation dual semidefinite program general optimal solution relaxed semidefinite problem produce indefinite dual slack matrix one negative eigenvalues corresponding eigenvectors slack matrix used update subspace order improve relaxation process iterated process trivially provides optimal solution subspace grows full space however show algorithm generically dimension subspace bounded roughly square root number constraints spectral bundle method 3 still considered large introduction aggregate subgradient guarantees convergence restricted bundle sizes extreme bundle may consist one new eigenvector maximal eigenvalue contrast classical algorithms cullum donath wolfe 6 polak wardi 38 require iteration computation eigenvectors eigenvalues within distance maximal eigenvalue thus close optimal solution number least large multiplicity maximal eigenvalue optimal solution quadratically convergent algorithm overton 35 step computed complete spectral decomposition matrix guess exact multiplicity maximal eigenvalue optimal solution recent work 33 34 oustry reinterprets algorithm overton within framework ulagrangian introduced 26 embeds first order method ensure global convergence global convergence approach relies spectrum eigenvalues within distance maximal eigenvalue makes use entire spectral information obtain local quadratic convergence restricted bundle size quadratic convergence reach algorithm first order method principle convergence follows traditional approach see eg 21 include proof completeness also present primaldual interior point code solving quadratic semidefinite programming problems associated semidefinite cutting plane models discuss efficiency aspects properties algorithm illustrated several combinatorial examples x2 basic properties semidefinite programs stated transform semidefinite programs eigenvalue optimization problems section 3 introduces bundle method algorithm proof convergence given x4 quadratic semidefinite subproblems arising bundle method solved interior point methods explained x5 section 6 gives outline implementation briefly discusses computation maximal eigenvalue associated eigenvector numerical examples combinatorial problems presented x7 conclude paper summary possible extensions improvements x8 convenience reader appendix explaining notation symmetric kronecker product included end paper 2 semidefinite programs eigenvalue optimization denote set symmetric matrices order n sn regard space isomorphic r n1 scalar product b 2 sn general use trace sum diagonal elements square matrix often use symbol canonical scalar product vectors b appropriate space clear context subset positive semidefinite matrices n fulldimensional non polyhedral convex cone sn defines partial order symmetric matrices b n positive definite matrices denoted n 0 consider standard primaldual pair semidefinite programs z 0 linear operator adjoint operator defined hax yi ff x 2 sn form 2 sn cost matrix b 2 r righthandside vector assume constraint qualification hold problems satisfy strong duality sense optimal solution x p optimal solution z following assumption allows simple reformulation dual eigenvalue optimization problem assume constant 0 case add tr redundant constraint primal problem obtain following dual equivalent 0 implies x 6 0 optimum hence optimal z dual singular therefore dual optimal solutions z satisfy leading thus shown equivalent min amax convenience assume deal following problem eigenvalue problem e convex nonsmooth optimization problem well studied literature recall basic facts function differentiable maximal eigenvalue multiplicity one optimizing eigenvalue functions optimum generically attained matrices whose maximal eigenvalue multiplicity larger one case one consider subdifferential max x see eg 35 particular v 2 r n belonging eigenspace maximal eigenvalue x contained subdifferential max x function interest spectral bundle method 5 subdifferential f derived standard rules see 17 observe set subgradients bounded remark 21 even though assumption 22 might look artificial hold sdp arising quadratic 01 optimization also holds many sdp derived relaxations combinatorial optimization problems see instance 1 12 24 3 bundle method section develop new method minimizing f use two classical ingredients proximal point idea bundle concept new contribution lies way derive new iterate bundle subgradient information collected previous iterates since approach builds several subtle ideas proceed small steps explain first derive minorant f local information 31 minorizing f f first goal obtain minorant f f approximates f neighborhood current iterates reasonably well easier handle f introducing function express fy formulation shows lower approximations f obtained constraining w subset semidefinite matrices tr propose following choice subset let p n theta r p n tr two matrices restrict w contained set c f defined p w reads wg definition fy fy 8y w eigenvector v fy eg case v column p v contained range space p intuitive idea behind specific choice c w follows matrix p contains subgradient information current point perhaps previous iterates explain detail propose select update matrix p computational efficiency would like keep number r columns p small independent multiplicity largest eigenvalue therefore collect indispensable subgradient information removed p aggregate subgradient aggregation final ingredient local model f matrix w plays role aggregate subgradient discuss low w updated algorithm main point instead optimizing semidefinite matrices w constrain small subset remark 31 set use matrix p set eigenvectors r largest eigenvalues would end model closely related approach 6 case would important select r least large multiplicity largest eigenvalue present approach necessary 6 c helmberg f rendl 32 proximal point idea next goal minimize f instead f since f built local information previous iterates model function unlikely reliable points far current iterate therefore use proximal point idea add penalty term displacement current point thus determine new candidate current iterate solving following convex problem referred augmented model u 0 fixed real weight min note minimization problem corresponds lagrangian relaxation thus replace original function f minorant f minimize locally around weight u controls indirectly radius sphere around minimize substituting definition f problem min problem simplified unconstrained note therefore obtain min w2c w bgammaaw uygammay0 w2c first equality follows interchanging min max see corollary 3732 41 using first order optimality inner minimization respect final problem semidefinite program concave quadratic cost function discuss x5 problems kind solved efficiently optimal solution w k1 gives new trial point 33 remark 32 choice weight u somewhat art several clever update strategies published literature see instance 21 42 33 one iteration algorithm main ingredients approach explained give formal description general iteration k algorithm consistent notation algorithm given x4 let us denote x k called x32 algorithm may compute several trial points k1 keeping x progress considered satisfactory null step k1 function evaluated subgradient eigenvector computed information added c w k form improved model c w k1 therefore assume current bundle spectral bundle method 7 contains eigenvector span k may may equal x k p need minorant f iteration k denoted c represents current approximation set semidefinite matrices trace one see 31 convenient introduce also regularized version new trial point k1 obtained minimizing f k respect described done follows first solve interior point methods see x5 yielding necessarily unique maximizer w use 33 compute finish iteration decide whether enough progress made perform serious step ie whether going set x update p k w k p k yet use maximum number columns allowed update process simple orthogonalize new eigenvector respect p k add new column form p k1 continue general however p k already use maximum number columns make room new subgradient information instead simply eliminating columns p k better exploiting information available ff v let qlambdaq eigenvalue decomposition v important part spectrum w k1 important subspace within space spanned p k spanned eigenvectors associated large eigenvalues v thus split eigenvectors q two parts corresponding spectra 1 2 containing columns eigenvectors associated large eigenvalues v q 2 containing remaining columns next p k1 computed contain p k q 1 least one eigenvector v k1 maximal eigenvalue operator orth indicates take orthonormal basis p k next aggregate matrix built way w k1 2 c contains important part p k given p k q 1 include remaining part p k given p k q 2 w k1 ff w k note w k1 scaled trace equal one proposition 33 update rules 37 38 ensure w k1 2 c w k1 proof let w k1 form 36 37 orthonormal matrix p k1 w k1 summarize easy facts used convergence analysis algorithm since k1 minimizer f k f k next let using definition k1 35 follows easily augmented model next iteration satisfy 8y remark 34 choice update p k fairly natural could use update formulas w main properties guiding update w k1 2 c ensuring 311 k1 model supported subgradient f pushing model towards f vicinity last minimizer 4 algorithm convergence analysis previous section focused question one iteration bundle method provide formal description method point except choice bundle nature subproblem minor changes parameters algorithm proof identical algorithm kiwiel presented 21 keep paper selfcontained present analyze simplified variant fixed u refer reader 21 algorithm variable choice u algorithm 41 input initial point maximal eigenvalue c gamma 0 0 termination improvement parameter ml 2 upper bound r 1 number columns p 1 2 direction finding solve 34 get k1 35 decompose v using 38 3 evaluation compute eigenvector v k1 compute p k1 37 4 termination fx k spectral bundle method 9 5 serious step set x continue step 7 otherwise continue step 6 6 null step set x 7 increase k 1 go step 2 prove convergence algorithm algorithm stops finite number iterations thus 35 0 2 fx k x k optimal assume following algorithm stop first consider case null steps occur iteration k lemma 42 k 0 41 violated k k proof convenience set using relations 310 39 obtain k k therefore f k k1 converge f fx computed gradient f k1 observe linearization f f k1 ff w k1 thus gammaomega ff convergence f k k1 boundedness gradients fact imply last term goes zero k 1 2 n k follows 41 violated k k thus sequences fy k1 converge fx k1 minimizer regularized function f k one hand implies k1 x hand 0 must contained subgradient f k k1 therefore sequence h k 2 subgradients converging zero converge fx k1 converge x hence zero must contained fx may concentrate serious steps following order simplify notation speak x k sequence generated serious steps duplicates eliminated f k corresponding refer function whose minimization gives rise x k1 next lemma investigates case fx k remain value fx fixed x lemma 43 fixed k x k converge minimizer f proof first prove boundedness x k end denote g k1 2 subgradient arising optimal solution minimization problem observe 33 therefore distance x k1 x bounded 2omega ff 2omega ff k k recursive application bound yields ux 41 progress algorithm serious step least ml fx k together 42 obtainx therefore sequence x k remains bounded accumulation point x replacing x x 44 choosing k sufficiently large remaining sum made smaller arbitrary small ffi 0 thus proving convergence x k x x k1 converge x g k1 converge zero 43 since sequence fx k converge zero well conclude x minimizer f lemma also implies fx k minimizers summarize discussion following theorem theorem 44 21 set minimizers f empty x k converge minimizer f case fx k spectral bundle method 11 remark 45 seen bundle algorithm works correctly even p contains one column case use aggregate subgradient crucial achieve correctness bundle algorithm without aggregate subgradients suffices store p subspace spanning eigenvectors corresponding nonzero eigenvalues optimal solution w k1 32 using bound 36 difficult show case maximal number columns one provide largest plus number eigenvectors added iteration least one computational experiments found upper bound hardly ever reached fact typical values maximal rank around half upper bound 5 solving subproblem section concentrate minimizer f k computed efficiently already seen x3 task equivalent solving quadratic semidefinite program 34 problems kind solved interior point methods see eg 7 23 dropping iteration index k constants 34 obtain ff ff 0 v 0 expanding cost function yields ff ff ff 0 v 0 using svecoperator see appendix definition important properties svec symmetric kronecker productomega expand symmetric matrices r column vectors length obtain quadratic program recall svecv technical linear algebra ff ff omega c w point advisable spend thought w algorithm designed large sparse cost matrices c w size c initially might possible exploit low rank structure w efficient representations algorithm proceeds rank w grows inevitably thus impossible store information w however see 52 56 suffices available vector aw 2 r scalaromega c w ff construct quadratic program furthermore linearity adelta hc deltai values easily updated whenever w changed solve 51 employ primaldual interior point strategy formulate defining equations central path introduce lagrange multiplier equality constraint dual slack matrix u 0 complementary variable v dual slack scalar fi 0 complementary variable ff barrier parameter 0 system reads ts gamma svecv step direction deltaff deltafi deltau deltav deltat determined via linearized system svecdeltav current context prefer linearization makes system easy solve deltav relatively little computational work per iteration final system deltav reads ff difficult see system matrix positive definite suffices show q using 0 main work per iteration factorization matrix v 2 r possible much better since q 11 inverted point strong dominance factorization pays employ predictor corrector approach delve strictly feasible primal starting point strictly feasible dual starting point constructed choosing 0 sufficiently negative spectral bundle method 13 starting strictly feasible primaldual pair compute first compute step direction deltaff deltafi deltau deltav deltat indicated perform line search line search parameter strictly feasible move new point compute new ae oe iterate stop hu 6 implementation implementation algorithm largely follow rules outlined 21 particular u adapted algorithm first guess u equal norm first subgradient determined v 0 scheme adapting u 21 except changes parameters example parameter ml accepting step serious set parameter mr indicating model good progress serious step larger mr fx k u decreased set stopping criterion formulated relative precision implementation choice upper bound r number columns r p selection subspace merits additional remarks observe remark 45 highly unlikely r violates bound even number columns p restricted also order system matrix 57 usually considerably smaller size system matrix traditional interior point codes semidefinite programming order furthermore order matrix variables r compared n traditional interior point codes thus number constraints roughly size n matrix order still considered factorizable running algorithm without bounding number columns p may turn considerably faster running interior point method observed practice see x7 huge n primaldual interior point methods applicable x z gamma1 system matrix dense case proposed bundle approach allows apply powerful interior point approach least important subspace problem correct identification relevant subspace v facilitated availability complementary variable u u helps discern small eigenvalues v interior point approach v 0 eigenvectors v v importance optimal solution subproblem large value v u v whereas eigenvectors ambiguous small eigenvalue v v v small value v u v practice restrict number columns p 25 provide room least five new vectors iteration see eigenvectors v correspond small important eigenvalues v added w important eigenvectors added w room needed new vectors large computation 52 56 quite involved central object appearing constants projection constraint space spanned 14 c helmberg f rendl p p p since size x assume huge important exploit whatever structure present compute projection efficiently combinatorial applications form vv v sparse projection computed efficiently projection step particular forming q 11 size r strong influence neglect computation computation q 11 still requires 2m flops indeed large small r construction q 11 takes longer solving associated quadratic semidefinite program large computational costs involved construction solution semidefinite subproblems may lead conviction model may worth trouble however evaluation eigenvaluefunction fact much expensive considerable work computing eigenvalues huge sparse matrices see eg 9 references therein extremal eigenvalues symmetric matrices seems general consensus lanczos type methods work best iterative methods run difficulties eigenvalues well separated context expected course algorithm largest eigenvalues get closer closer till identical optimum reasonable convergence block lanczos algorithms blocksize corresponding largest multiplicity eigenvalues employed first ten iterations largest eigenvalue usually well separated algorithm fast soon eigenvalues start cluster larger larger blocksizes used eigenvalue problem gets difficult solve order reduce number evaluations seems worth employ powerful methods cutting plane model increase computation time required solve subproblem goes hand hand difficulty eigenvalue problem correspondence rank p number clustered eigenvalues iterative methods computing maximal eigenvectors generically offer approximate eigenvectors several large eigenvalues well space spanned approximate eigenvectors likely good approximation true eigenspace maximal number columns p yet attained may worth include several approximate eigenvectors well algorithm use block lanczos code based fortran code hua guess hua dai 47 works complete orthogonalization employs chebyshev iterations acceleration choice blocksize based approximate eigenvalues produced previous evaluations 30 four block lanczos steps followed twenty chebyshev iterations scheme repeated till maximal eigenvalue found required relative precision relative precision depends distance maximal second largest eigenvalue bounded 10 gamma6 starting vectors use complete block eigenvectors lanczosvectors previous evaluation 7 combinatorial applications combinatorial problem investigate quadratic programming fgamma1 1g variables case c laplace matrix possible weighted graph problem known equivalent maxcut problem standard semidefinite relaxation based identity x cx ff fgamma1 1g n vectors xx positive semidefinite matrix diagonal elements equal one relax xx x 0 obtain following spectral bundle method 15 primaldual pair semidefinite programs z 0 nonnegatively weighted graphs celebrated result goemans williamson 8 says always cut within 878 optimal value relaxation one first attempts approximate dmc using eigenvalue optimization contained 39 authors use bundle code schramm zowe 42 limited number bundle iterations solve dmc exactly far practical algorithms computing optimal value primaldual interior point algorithms however able exploit sparsity cost function cope dense matrices x z gamma1 alternative approach based combination power method generic optimization scheme plotkin shmoys tardos 37 proposed 22 seems purely theoretical table 71 compare proposed bundle method semidefinite primaldual interior point code 14 called pdip sequel graphs nodes generated rudy machine independent graph generator written g rinaldi table 77 contains command lines specifying graphs graphs g 1 g 5 unweighted random graphs density 6 approx 19000 edges g 6 g 10 graphs random edge weights fgamma1 1g g 11 g 13 toroidal grids random edge weights fgamma1 1g 1600 edges g 14 g 17 unweighted almost planar graphs edge set union two almost maximal planar graphs approx 4500 edges g g 21 almost planar graphs random edge weights fgamma1 1g cases cost matrix c laplace matrix graph divided 4 ie let denote weighted adjacency matrix g description code pdip see 14 termination criterion requires gap primal dual optimal solution closed relative accuracy bundle algorithm dmc transformed eigenvalue optimization problem described x2 addition diagonal c removed fact algorithm works problem min change problem pmc diagonal elements x fixed one offset 1e ae gamma diaga added output influence algorithm whatsoever particular influence stopping criterion starting vector 0 choose zero vector parameters described x6 computation times interior point code pdip well bundle code refer machine sun sparc ultra 1 model 140 ultrasparc cpu 64 mb ram time measured user time given leading zeros dropped first column table 71 identifies graphs second third refer pdip contain optimal objective value produced regarded highly accurate solutions computation time fourth fifth column give numbers bundle code examples bundle code superior pdip although examples belong favorable class instances small relatively large n difference computation time astonishing note termination criterion used bundle code quite accurate except g 11 seems difficult problem bundle method deviation accuracy caused cancellations connection offset difficulty example seem depend number nonzeros rather shape objective function toroidal grid graphs maximum cut likely unique thus objective function rather flat flatness effect distribution eigenvalues optimal solution indeed g 11 eigenvalues cluster around maximal eigenvalue problems illustrate table 72 gives largest eigenvalues solution termination problems g 1 g 6 g 11 g 14 g table comparison interior point pdip bundle b approach sol gives computed solution value time gives computation time pdipsol pdiptime bsol btime g2 1208943 11914 1208945 519 g6 265616 12453 265618 357 g 11 62916 12841 62921 4526 g g 17 g g 19 table 73 provides additional information performance bundle algorithm examples table 71 second column gives accumulated time spent eigenvalue computation accounts roughly 90 computation time serious displays number serious steps iter gives total number iterations including serious null steps kgk norm subgradient arising last optimal w k1 termination g 11 norm considerably higher examples since desired accuracy achieved g 11 standard stopping criterion may worth consider alternative stopping criterion taking account norm subgradient well column maxr gives maximal rank p attained iterations rank p would spectral bundle method 17 table maximal eigenvalues termination examples g 1 g6 g11 g14 g18 13 31190 32239 07651 10557 14047 14 31135 32181 07650 10515 14007 19 27214 27716 07647 10398 13725 22 26834 26756 07644 10341 13583 26 26274 18722 07636 10239 13480 28 25137 17974 07633 10211 13397 29 24840 14859 07630 10180 13345 bounded 25 bound never came effect examples aggregation necessary observe theoretic bound allows r 39 yet maximal rank half number last column gives time objective value first within 10 gamma3 optimum combinatorial applications high accuracy optimal solution minor importance algorithm deliver reasonable bound fast solution provide hint good feasible solution constructed bundle algorithm offers respect computation time bundle algorithm displays usual behavior subgradient algorithms initially progress fast bound approaches optimum strong tailing effect illustrate giving objective values computation times serious steps example g 6 diagonal offset 77 example table 74 one minute bound within 01 optimum examples see last column table 73 respect primal feasible solution observe p k v k successively better better approximation primal optimal solution x case much information stored aggregate vector aw k remember advisable store w k p k may enriched additional lanczosvectors eigenvalue computation solution enlarged quadratic semidefinite subproblem acceptable approximation x necessary table additional information bundle algorithm examples table 71 time gives total amount time spent computing eigenvalues eigenvectors serious gives number serious steps iter total number iterations including null steps kgk refers norm gradient resulting optimal solution last semidefinite subproblem maxr maximum number columns used p limit would 25 01time gives time bound within 10 gamma3 optimum relative precision time serious iter kgk maxr 01time g1 312 22 33 01639 g4 238 19 27 008235 19 54 g 9 259 g13 1724 43 78 0218 15 617 g g 19 1124 41 71 01571 15 334 construct whole matrix x fact factorized form p k much convenient work example approximation algorithm goemans williamson 8 requires precisely factorization particular x ij element x easily computed inner product row j n theta r principle opens door branch cut approaches improve initial relaxation subject work table 75 gives similar set examples last set examples devoted lovasz function 27 yields upper bound cardinality maximal independent stable set graph implementational convenience use formulation within quadratic fgamma1 1g programming setting see 24 graph k nodes h edges obtain semidefinite program matrix variables order constraints examples going consider one thousand nodes six thousand edges examples interior point methods applicable memory requirements clear examples table 75 also little hope bundle method terminate within reasonable time however significant progress achieved beginning bundle method memory consumption problem run examples time limit five hours precisely algorithm terminated first serious step occurs five hours computation time graph instances type computational results displayed table 76 new columns n give order matrix variable number constraints respectively observe toroidal grid graphs spectral bundle method 19 table detailed account serious steps example g 6 iter value time kgk maxr 8 267010 28 5992 15 9 266607 34 4173 14 265631 157 0431 19 265619 333 007243 table examples bsol btime time serious iter kgk maxr g22 1413598 3811 2800 26 52 00781 23 g26 1413293 3445 2637 31 48 03066 23 g 28 410081 2941 2108 23 g g g 36 800604 25610 23109 62 115 02634 24 g 37 g 38 801501 40353 33924 58 155 01937 22 g g 48 g 49 perfect independence number 1500 independence number g 50 1440 g 50 perfect know independence number graphs except g 48 g 49 g 48 perfectness hard judge quality solutions tracing development bounds last serious steps examples g 43 g 47 g 51 g 54 still produced improvements 05 1 rather large norm subgradient table upper bound function five hours computation time serious iter kgk maxr g 43 g 44 1001 10991 31013 50631 31425 g48 3001 9001 152653 51131 45957 54 94 04062 15 g g 50 3001 9001 153612 51751 50125 50 124 04728 15 g 53 1001 6915 46386 50836 43634 41 104 2593 25 g 51 g 54 indicate values cannot expected good approximations function also note size subspace required g 48 g 50 still well 25 examples g 51 g 54 value ff almost negligible g 43 g 47 value ff roughly 13 termination thus examples restriction 25 columns became relevant computational results table 76 demonstrate algorithm limits nonetheless bounds obtained still useful primal approximation corresponding subgradient reasonable starting point primal heuristics 8 conclusions extensions proposed proximal bundle method solving semidefinite programs large sparse strongly structured coefficient matrices semidefinite constraint lifted objective function means lagrange multiplier whose correct value known general except problems fixed primal trace latter case precisely value trace approach differs previous bundle methods subproblem tailored semidefinite programming fact whole approach interpreted semidefinite programming subspaces subspace successively corrected improved till optimal subspace identified set subgradients modeled semidefinite subproblem superset subgradients used traditional polyhedral cutting plane model therefore convergence new method direct consequence previous proofs traditional bundle methods yet clear whether specialized model admits stronger convergence results choice u still much open problem high practical importance constrained quadratic fgamma1 1gprogramming method offers good bound within reasonable time allows construct approximate primal optimal solution relaxation compact representation improve bound cutting plane approach algorithm must able deal sign constraints yvariables principle difficult model sign constraints semidefinite subproblem however consequence influence sign constrained variables cost coefficients quadratic subproblem cannot eliminated longer rendering method impractical even moderate number cutting planes alternatively one might consider active set methods entail danger destroying convergence together kc kiwiel currently working alternative methods incorporating sign constraints 13 backbone method efficient routine computing maximal eigenvalue huge structured symmetric matrices although implementation spectral bundle method 21 table arguments generating graphs graph generator rudy g1 rnd graph 800 6 8001 g2 rnd graph 800 6 8002 g3 rnd graph 800 6 8003 g4 rnd graph 800 6 8004 g5 rnd graph 800 6 8005 g6 rnd graph 800 6 8001 random g7 rnd graph 800 6 8002 random g8 rnd graph 800 6 8003 random g9 rnd graph 800 6 8004 random g12 toroidal grid 2d 50 g13 toroidal grid 2d 25 g14 planar 800 99 8001 planar 800 g17 planar 800 99 8007 planar 800 g22 rnd graph 2000 1 20001 g23 rnd graph 2000 1 20002 g24 rnd graph 2000 1 g25 rnd graph 2000 1 20004 g26 rnd graph 2000 1 20005 g27 rnd graph 2000 1 20001 random g28 rnd graph 2000 1 20002 random g29 rnd graph 2000 1 times g31 rnd graph 2000 1 20005 random g32 toroidal grid 2d 100 20 random 0 1 times g35 planar 2000 99 20001 planar 2000 g36 planar 2000 99 planar 2000 g37 planar 2000 99 20005 planar 2000 g38 planar 2000 99 20007 planar 2000 planar 2000 99 20001 planar 2000 g40 planar 2000 99 planar 2000 planar 2000 99 20005 planar 2000 times 2 plus g42 planar 2000 99 20007 planar 2000 g43 rnd graph 1000 2 10001 g44 rnd graph 1000 2 10002 g45 rnd graph 1000 2 10003 g46 rnd graph 1000 2 10004 g47 rnd graph 1000 2 10005 g48 toroidal grid 2d 50 toroidal grid 2d g50 toroidal grid 2d 25 120 g52 planar 1000 100 10003 planar 1000 100 10004 g53 planar 1000 100 10005 planar 1000 100 10006 g54 planar 1000 100 10007 planar 1000 100 10008 based code hua seems work sufficiently stable certainly much room improvement straight forward approach achieve serious speedups implement algorithm parallel machines see instance 43 rather recently interest lanczos method risen see 25 3 5 10 30 references therein papers based concept implicit restart proposed 44 polynomial acceleration approach require additional matrix vector multiplications interesting test new ideas within bundle framework thank kc kiwiel fruitful discussions c lemarechal anony 22 c helmberg f rendl mous referee constructive critisism helped improve presentation appendix notation r real column vector dimension n real matrices real matrices positive definite matrices positive semidefinite matrices 0 positive definite 0 positive semidefinite n identity appropriate size size n e vector ones appropriate dimension maximal eigenvalue tr trace 2 nn tr product mmn dimensional vector representation 2 sn kronecker product 2 mn diaga diagonal 2 mn column vector diagonal matrix v main diagonal sn isomorphic r via map sveca defined stacking columns lower triangle top multiplying offdiagonal elements 2 factor offdiagonal elements ensures symmetric kronecker productomega defined arbitrary square matrices nn action vector svecc symmetric matrix c 2 sn concepts first introduced 2 use notation introduced 45 latter paper also cite properties symmetric kronecker product convenience reader 1 bomega 2 aomega b 3 aomega symmetric 4 5 aomega 6 0 b 0 aomega b 0 7 r interior point methods semidefinite programming applications combinatorial optimization iterative methods computation eigenvalues large symmetric matrix solving largescale sparse semidefinite programs combinatorial optimization implicitly restarted lanczos method large symmetric eigenvalue problems minimization certain nondifferentiable sums eigenvalues symmetric matrices improved approximation algorithms maximum cut satisfiability problems using semidefinite programming matrix computations shifted block lanczos algorithm solving sparse symmetric generalized eigenproblems geometric algorithms combinatorial optimization fixing variables semidefinite relaxations incorporating inequality constraints spectral bundle method interiorpoint method semidefinite programming quadratic knapsack relaxations using cutting planes semidefinite programming convex analysis minimization algorithms interiorpoint method minimizing maximum eigenvalue linear combination matrices solving graph bisection problems semidefinite programming proximity control bundle methods convex nondifferentiable minimization efficient approximation algorithms semidefinite programs arising maxcut coloring connections semidefinite relaxations maxcut stable set problems tr ctr jiahai wang letters improved discrete hopfield neural network maxcut problems neurocomputing v69 n1315 p16651669 august 2006 samuel burer renato c monteiro yin zhang interiorpoint algorithms semidefinite programming based nonlinear formulation computational optimization applications v22 n1 p4979 april 2002 abraham duarte ngel snchez felipe fernndez ral cabido lowlevel hybridization memetic algorithm vns maxcut problem proceedings 2005 conference genetic evolutionary computation june 2529 2005 washington dc usa gerald gruber franz rendl bundle method hard combinatorial optimization problems combinatorial optimization eureka shrink springerverlag new york inc new york ny tijl de bie nello cristianini fast sdp relaxations graph cut clustering transduction combinatorial problems journal machine learning research 7 p14091436 1212006 kazuhide nakata makoto yamashita katsuki fujisawa masakazu kojima parallel primaldual interiorpoint method semidefinite programs using positive definite matrix completion parallel computing v32 n1 p2443 january 2006 hernn alperin ivo nowak lagrangian smoothing heuristics maxcut journal heuristics v11 n56 p447463 december 2005 alper yildirim xiaofei fanorzechowski extracting maximum stable sets perfect graphs using lovszs theta function computational optimization applications v33 n23 p229247 march 2006 stephen braun john e mitchell semidefinite programming heuristic quadratic programming problems complementarity constraints computational optimization applications v31 n1 p529 may 2005 henry wolkowicz miguel f anjos semidefinite programming discrete optimization matrix completion problems discrete applied mathematics v123 n13 p513577 15 november 2002