advantages polar logpolar mapping direct estimation timetoimpact optical flow application anthropomorphic retinalike visual sensor advantages polar logpolar mapping visual navigation investigated demonstrated motion equations relate egomotion andor motion objects scene optical flow considerably simplified velocity represented polar logpolar coordinate system opposed cartesian representation analysis conducted tracking egomotion generalized arbitrary sensor object motion main result stems abundance equations written directly relate polar logpolar optical flow time impact experiments performed images acquired real scenes presented b introduction autonomous systems able control movements environment adapt unexpected events possible robot capability sense environment among sensors used robots visual sensors ones require greatest computational power process acquired data also generate greatest amount information despite fact many researchers addressed problem process visual data less attention given definition devices strategies image acquisition could possibly reduce amount data processed algorithmic complexity extraction useful information ballard et al also sandini tistarelli among others investigated tracking motion strategy greatly simplifies problem visual navigation extent major effort done far try reduce amount information flowing visual sensors processing part artificial visual systems concentrated image approach certainly justified situations images acquired pas sively transmitted distant station analyzed eg satellites data reduction strategy solves case communication problem different strategies exploited data reduction achieved order solve understanding behavioural problems case main goal data reduction strategy avoid overloading system useless information hardest part define term useless unless meaning directly tied task performed useful information information necessary carry task also case think terms images impossible extract use ful information unless least rough processing images performed example strategy related extraction edges processing applyed overall image even task hand small portion image would sufficient solutions problem proposed past use multiple windows system achieved results certainly interesting approach studying developing smart sensing devices buitin data compression capabilities spacevariant sensor produces logpolar transformation described paper example may possible reduce information amount directly sensor level using foveated sensor ie sensor small high resolution part surrounded lower resolution area poses problem look ie position central high resolution part visual field solve problem two approaches possible first base selection focus attention results interpretation pro cedure second take advantage task carried first approach mandatory static systems second approach advantage less dependent visual information processing fact following second approach selection focus attention done preattentively ie visual processing part even started case spacevariant sensors kind described paper selection focus attention goes along selection direction gaze fact direction gaze defines part scene analyzed highest detail conversely region around focus attention sampled highest resolution stemming consideration strategies gaze control seen way reduce amount information processed moreover strategies made dependent upon task alone data reduction carried interpretation visual images examples mean visuomotor strategies performed humans execution specific behaviours example locomotion direction gaze either pointed straight ahead toward direction motion provide gross orientation information ahead actual position detect unexpected obstacles course safety level processes running simultaneously major feature possibility defining focus attention consequence direction gaze function task alone examples presented field manipulation final stage grasping example focusofattention vicinity contact area close endeffector also case position hand space known direcition gaze controlled first knowing trajectory endeffector secondly tracking obvious even case image processing performed eg tracking point space important observation procedures independent content images words data driven one major goal paper demonstrate data driven processes performed even better using spacevariant sensor using conventional device 1 worth noting however reality problem need solved also conventional sensors fact location fixation point stereo systems convergent optical axis defined well focussing procedures feasible focus everywhere result poposed approach usefulf limiting amount information processed hardware focusofattention fovea geometrical structure advantageous control position focusofattention active tracking motion basis analysis conducted pa per related properties retinalike sensor defining depth frommotion equations logpolar mapping due particular topology retinalike sensor incorporates many advantages dynamic image processing shape recognition potential considerably augmented making sensor smart eg defining set visual tasks performed directly sensor without need external input dramatically reducing time required processing computation optical flow image sequence tracking moving targets examples visual tasks defined paper derive optical flow equations retinal ccd sensor flow field computed logpolar plane used estimate timetoimpact analysis performed considering optical flow equations due tracking egomotion retinal cartesian plane relating computed velocity field logpolar plane even though quantitative results obtained natural way approaching problem jain pointed advantages processing optical flow due camera translation using logpolar complex mapping images choosing position foe center representation observer rotates translation fixate static target space conversely rotates without translating tracking moving target possible intuitively understand time fly reaching obstacles proportional rate expansion projection objects shape retinal plane demonstrate fact radial component optical flow represented polar plane depends timetoimpact polar flow representation analysed number simple relations derived allow us directly compute timetoimpact arbitrary ego andor eco motion conformal mapping human visual system receptors retina distributed space increasing density toward center visual field fovea decreasing density fovea toward periphery topology simulated proposed sandini tagliasco means discrete distribution elements whose sampling distance distance neighbouring sampling points increases linearly eccentricity fovea interesting feature spacevariant sampling topological transformation retinal image cortical projection 2 transformation described conformal mapping points polar retinal plane ae log ae values ae j obtained mapping cartesian coordinates x pixel corresponding polar coordinates resulting cortical projection certain conditions invariant linear scalings rotations retinal image complex transformations reduced simple translations along coordinate axes cortical image property valid scene andor sensor move along scaling around rotation optical axis properties hold case simple polar mapping im age linear dialation around fovea transformed linear shift along radial coordinate ae plane meanwhile logpolar transformation produces constant shift along radial coordinate cortical projection beyond geometric properties polar logpolar mapping referred following sections logpolar transformation performs relevant data reduction image equally sampled throughout field view preserving high resolution around fovea thus providing good compromise resolution bandlimiting needs property turns effective wish focus attention particular object feature track moving target ie stabilize image target fovea therefore properties topological logpolar mapping found interesting applications object specifically shape recognition object tracking main focus paper fact stress peculiarity transformation computation dynamic measures particular main observation tracking moving object mapping 2 terms retinal cortical derive observation conformal mapping described similar mapping retinal image onto visual cortex humans stabilized retinal image onto cortical image deforms way easier compute behavioral variables usefulf control position focusofattention fact object perfectly stabilized retina shape change due perspective changes good approximation images sampled closely time component velocity field along log ae axis alone measures rate dilation retinal image object measure used compute timetocrash paper illustrate logpolar transformation well simple polar mapping dramatically simplify recovery structural information image sequences allowing direct estimation time toimpact optical flow 21 structure retinalike sensor prototype retinalike visual sensor designed within collaborative project involving several partners 3 paper refer physical characteristics prototype sensor dealing logpolar transformation results could easily generalised particular logpolar mapping modifying constant parameters involved transformation retinocortical mapping implemented circular ccd array using polar scan spacevariant sampling structure characterized sampling period eccentricity distance center sensor spatial geometry receptors obtained square tassellation sampling grid formed concentric circles prototype ccd sensor depicted fig 1 divided 3 concentric areas consisting 10 circular rows central fovea circular row consists 64 light sensitive elements central fovea covered square array 102 light sensitive elements 4 experiments information coming fovea used extrafoveal part sensor retinocortical transformation 3 institutions involved design fabrication retinal ccd sensor dist university genoa italy university pennsylvania dept electrical engineering italy actual fabrication chip done imec leuven belgium 4 currently performances ccd sensor evaluated first real image recently acquired prototype camera built experiments reported paper carried resampling standard tv images following geometry sampling structure sensor defined log ae gamma p 1 polar coordinates point retinal plane constants determined physical layout ccd sensor tracking egomotion optical flow ability quickly detect obstacles evaluate timetoimpact react order avoid vital importance animates passive vision techniques beneficially adopted active movements performed dynamic spatiotemporal representation scene timetoimpact objects computed optical flow extracted monocular image sequences acquired tracking movements sensor image velocity described function camera parameters split two terms depending rotational translational components camera velocity respectively f 2 z distances camera fixation point two successive instants time oe rotations camera referred coordinate axes shown fig 2 z distance world point image plane interested computing timetoimpact wz z v must derived total optical flow rotational part flow field v r computed proprioceptive data eg camera rotational angles focal length global optic flow v computed v determined subtracting adopting constrained tracking egomotion rotational angles known correspond motor control generated fixationtracking system image velocity computed applying algorithm estimation optical flow sequence cortical images fig 3 first last images sequence 16 shown images acquired conventional ccd camera digitized 256x256 pixels 8 bits per pixel motion sensor translation plus rotation oe around horizontal axis x movement direction gaze fixed point ground far behind sensor fig 4 result retinal sampling applied first image fig 3 4b simulated output retinal sensor obtained following characteristics chip shown resulting images 30x64 pixels evaluating presented experimental results extremely low number pixels 1920 considered optical flow computed solving overdetermined system linear equations unknown terms u equations impose constancy image brightness time stationarity image motion field represents image intensity point x time least squares solution 3 computed point cortical plane fl represent point coordinates cortical plane fig 5 optical flow sequence fig 4b shown 31 first step relating motion equations logpolar mapping 2 defined relation camera velocity optical flow develop equations velocity field transformed onto cortical plane goal find expression relates cortical velocity rotational flow v r derive translational flow compute timetoimpact firstly derive motion equations cortical plane ae ae log e e natural logarithmic base q constants related eccentricity density receptive fields retinal sensor retinal velocity expressed function retinal coordinates relative cartesian reference system x retinal velocity image point relative cartesian reference system centered fovea substituting 6 5 yields log developing 7 making explicit retinal velocity gammay x log e substituting expression v r 2 8 obtain translational flow referred retinal plane log e gamma f log e fl velocity field computed sequence cortical images worth noting expressing cartesian coordinates retinal sampling element microns focal length retinal velocity also expressed units rotational velocity camera tracking avaliable v computed timetoimpact image points retinal plane recovered using wellknown relation f displacement considered point focus translational field image plane wz translational component sensor velocity along optical z axis ratio lefthand side represents timetoimpact respect considered world point location foe estimated computing least squares fitting pseudo intersection set straight lines determined velocity vectors v formulation direct exploit completely implications advantages logpolar mapping also many external parameters related camera motion required compute timetoimpact focal length rotational motion camera must known order estimate translational component velocity foe must computed v estimate timetoimpact moreover schema used presence independently moving objects since basic assumption environment completely static summary algorithm applied rotational part camera motion known intrinsic camera parameters least focal length known objects scene move independently exploiting advantages peculiarities polar logpolar mapping requirements constraints gradu ally relaxed analysis performed remainder paper final aim estimation time impact using imagederived parameters case camera object motion even though algorithm requires many constraints still limit generality possible applications successfully ap plied example locate obstacles detect corridors free space robot navigation accuracy measurements depends resolution input images retinal sensor low nevertheless hazard map computed method still exploited qualitative properties visual navigation fig 6a shows timetoimpact objects hazard map scene fig 4a fig 6b associated uncertainty appendix quantitative measure error estimated timetoimpact derived 32 exploiting polar logpolar mapping optical flow possible generalise logarithmicpolar complex mapping property transforming objects dialation translation along radial coordinate general complex kind motions generally expansion image object due either motion camera object produce radial component velocity retinal plane intuitive observation also stated following way timetoimpact point retinal plane effects radial component optical flow formally prove assertion remainder paper observation lead us adopt approach different one pursued previous section order represent optical flow cortical plane turns convenient way representing analysing velocity terms radial angular components respect fovea let us consider moment general motion camera rotational translational later consider special case tracking egomotion explaining simplifies analyisis finally dealing also object motion velocity image plane along radial angular coordinates ae retinal velocity respect cartesian coordinate system centered fovea plugging motion equations small angular rotations 2 f f sin j ae f f sin j substituting values ae hi wx sin z retinal sensor performs logarithmic mapping obtain ae ae log e z ae ae oe sin fl log e ae hi wx sin z cos equations simply show components optical flow depend upon depth z objects space radial component depends upon timetoimpact z wz moreover angular component depends upon rotations around optical axis radial component invariant respect notice made hypothesis motion sensor therefore equations certainly hold kind camera motion even though analysis conducted moving camera static environment result obtained 13 holds combination object camera motion motion parameters expressed terms translational velocities space w x rotational velocities referred cameracentered cartesian coordinate system oe velocities absolute velocities represent relative motion camera respect objects sum two velocities equations 13 developed case tracking egomo tion imposing general optical flow equations motion constraint expressed 2 distance fixation point retinal plane measured frame time following one optical flow computed developing equation 13 using values w x w given 14 obtain ae z oe sin fl log e ae z sin fl structure two equations similar rotational velocity camera known possible substitute values oe 15 directly obtain z wz radial component z f oe cos fl f oe cos fl z log e f oe sin fl also focal length f must known q constant values related physical structure ccd sensor worth noting importance z z wz fact represent relative measurements depth primal importance humans animals relate environment interesting compare last equation equations 9 10 noticed equation 16 depend position foe therefore necessary differentiate optical flow respect rotational component v r estimate translational component option try compute timetoimpact partial derivatives velocity components ae oe sin fl combining equation expression equation 15 obtain log e z f oe sin fl z w z log e ae f oe sin fl also case timetoimpact computed directly substituting values rotational angles note case equation depend rotation around optical axis first important result makes computation timetoimpact independent one motion parameter develop alternative methods allow us release parameters also easily computed images let us consider first derivative f f ae z oe sin notice ae q result suggests another possibility using first derivative obtain relation similar 18 z w z log e f oe sin fl considering neighbouring pixels eccentricity possible formulate two equations two unknown terms form z w z f oe sin setting z w z obtained equation timetoimpact involves imagederived parameters like velocity derivative image coor dinates without requiring knowledge motion parameters also worth noting formulations derived section optical flow differentiated recover translational flow like approaches foe position computed important features method computation timetoimpact fact rotational velocity foe computed indirectly optical flow therefore subject errors even though equation 22 directly applied points along ray used improve robustness algorithm case overconstrained system equations used solved using standard least squares techniques w z z b z underlying assumption constant depth requires use small neighbourhood ae depth discontinuities fact introduce artifacts errors processing images cluttered scenes another equation simpler timetoimpact obtained combining equation 17 19 w z z log e ae f oe sin fl w z z log e f oe sin fl combining two equations obtain z w z log e gamma equation 25 allows direct computation timetoimpact images notice first order derivatives optical flow required pixel position appear parameters q calibrated constants ccd sensor interesting relate result divergence approach proposed thompson also recently nelson aloimonos equation 25 regarded formulation oriented divergence tracking motion modified take account fact sensor planar spherical fig 7a first last image sequence 10 shown images acquired resolution 256x256 pixels resampled performing logpolar polar mapping motion camera translation plus rotation around vertical axis direction gaze controlled keep fixation apple center basket object nearest observer inverse timetoimpact wz z computed applying equation 25 optical flow fig 8a shown fig 9a despite low resolution closest object correctly located last equation timetoimpact obtained computing second order partial derivative f ae z oe sin fl log e 26 z w z log e gamma log e equation clearly states timetoimpact computed using radial component velocity respect fovea formulation depend motion fixated target motion parameters involved also represent relative motion equation applied image point method still valid scenes containing many independently moving objects fig 9b inverse timetoimpact scene fig 7 computed applying equation 31 optical flow fig 8b shown analysis performed section developed ccd retinal sensor many results still valid conventional raster sensors advantages still obtained using retinal sensor let us consider equation 12 apply tracking constraint without making complex logarithmic mapping z ae z partial derivative respect j changes constant factor partial derivatives ae respect ae ae ae z f ae first formulation timetoimpact involves second order partial derivatives radial component ae optical flow z w z ae ae ae equation timetoimpact containing first order partial derivatives velocity obtained considering also angular component ae combining equation 28 29 obtain z w z ae ae ae ae conclude abundance equations exist compute timetoimpact case tracking egomotion proper coordinate system chosen polar approach shown polar velocity representation seems best suited recover timetoimpact image sequences even though technology conservative producing raster ccd arrays imaging devices therefore strongly linked cartesian coordinate system polar transformation performed realtime using commercially avaliable hardware enforcing feasibility proposed methodology hand retinal ccd sensor naturally incorporates polar representation also introduces logarithmic scaling effect makes equations simpler depend radial coordinate point ae fig 10 measurements inverse timetoimpact sequence fig 7 computed using 10a equation 31 10b equation 33 shown order demonstrate applicability method case independently moving objects equation 33 applied sequence images camera moving along trajectory parallel optical axis object moving along collision course toward camera along direction camera optical axis slightly rotating well first last images shown fig 11a output polar mapping cartesian plane shown fig 11b 11c polar mapping ae plane hazard maps scene relative 2 successive frames shown fig 12 appendix b comparative analysis accuracy formulations derived timetoimpact case polar logpolar mapping performed 4 analysing general motion stage possible relax tracking constraint consider general motion camera andor objects scene already pointed equations still hold relative motions compute timetoimpact case general motion equation 25 still applied computing partial derivatives optical flow stated equation 13 obtain ae z ae oe sin fl ae hi wx cos sin combining expressions optical flow partial derivatives obtain log e f oe sin fl log e f oe sin fl combining two equations obtain z w z log e gamma exactly equation 25 similarly also demonstrated equations developed recover timetoimpact hold general motion important result enforces relevance polar representation optical flow furthermore extends validity arbitrary motion already pointed equations obtained timetoimpact similar divergence operator wz z factor directly estimated equations corresponds divergence mea surement divergence theory finds mathematical proof assuming spherical geometry imaging device proposed method holds exactly kind planar sensor best way implement schema using conventional raster sensor map sampled image polar representation compute optical flow directly polar images estimated optical flow already represented polar ae plane matter fact computing mapping optical flow obtained original images cartesian plane efficient considering beginning visual process steady state initial time delay passed mapping original images one frame processed time instead two components optical flow moreover mapping original images results accurate evaluation retinal flow mapping implemented efficently using two lookup tables directly address pixels polar cartesian coordinate system 5 conclusion application retinalike anthropomorphic visual sensor implications polar logpolar image mapping dynamic image analysis investigated particular case moving observer undertaking active movements considered starting point directly estimate timetoimpact optical flow main advantages obtained logpolar retinalike sensor related spacevariant sampling structure characterized image scaling rotation invariance variable resolution due topology amount incoming data considerably reduced high resolution preserved part image corresponding focus attention also part image higher resolution computation velocity necessary adopting tracking egomotion strategy computation optical flow timetoimpact simplified moreover amplitude image displacements increases fovea periphery retinal image almost computational accuracy achieved throughout visual field minimizing number pixels processed polar mapping introduces considerable simplification motion equations allow direct computation timetoimpact polar logpolar representation optical flow certainly best suited computation scene structure mainly three reasons number simple equations written relate optical flow derivatives second order one case timetoimpact relative depth easily computed image parameters either relative observer velocity z wz distance fixation point space z dependence depth decoupled radial angular component optical flow radial component proportional timetoimpact derived equations easily applied also images acquired conventional raster ccd sensor polar mapping efficently implemented using general purpose hardware obtaining realtime performances complete system implemented sun sparc station1 computes optical flow timetoimpact less one minute estimation timetoimpact seems important process animals avoid obstacles catch prey importance timetoimpact qualitative feature already pointed timetoimpact inverse measurement effectively used detect avoid obstacles without requiring exact recovery ob jects surface therefore even though optical flow derivatives computed unfortunately low accuracy critical accuracy mandatory qualitative estimates fact also implies accurate camera calibration needed accomplish visual navigation human beings lowlevel visual processes directly performed retina early stages visual system simple image processes like filtering edge motion detection must performed quickly minimal delay acquisition stage vital importance survival example detect static moving obstacles computation timetoimpact represents simple process sort building block could implemented directly sensor local even analogic using electrical charge output sensitive elements parallel operations avoiding delay decoding transmitting data external devices 5 appendix quantitative estimation error recovering timetoimpact estimation timetoimpact modeled stochastic process parameters involved computation uncorrelated probabilistic variables assuming process gaussian set variables whose mean values equal measured ones oe acknowledgements authors thank f bosero f bottino ceccherini help developing computer simulation environment ccd retinal sensor also gratefull frank susan donoghue carefully proofread text corrected english research supported special project robotics italian national council research x position point retinal plane reference cartesian coordinate system centered fovea estimated retinal velocity coordinates focus expansion retinal plane oe rotation angles undertaken camera tracking motion j jacobian ti function diagonal matrix variances independent variables x computing partial derivatives 10 obtain oe oe variances oe x associated position image point set equal radius sensitive cell ccd array depends spatial position point within field view variances rotational angles set equal constant determined positional error driving motors experiments performed values set 01 01 01 degrees reasonable error standard dc servomotors variances foe position oe fx determined least squares fit error variances optical flow components fl cortical plane directly determined differentiating least squares solution 4 variance computed image derivatives used compute variances optical flow estimated assuming uniform distribution unitary quantization step gray levels weights derivative operator 5 point frames time derivative support value oe 2 equal 00753 first derivative oe ii 2 equal 08182 second derivative obtained variances velocity vectors retinal plane obtained differentiating 8 respect fl oe oe variances optical flow computed cortical plane grouping similar terms 38 obtain oe expression within brackets appearing last two rows represents relevant term multiplyed f consequently higher errors due computation image velocity estimation rotational angles cameras conversely positioning error driving motors terms containing errors rotational angles also quadratic image coordinates hence periphery visual field area spatial coordinates pixels reach greatest values affected foeva nevertheless terms divided modulus velocity raised sixth power therefore amplitude image displacement sufficiently large errors drop quickly matter fact amplitude optical flow crucial importance reducing uncertainty estimation timetoimpact appendix b comparative analysis accuracy recovering time impact case polar logpolar mapping beyond aim paper perform exhaustive error analysis derived equations interesting compare analitically results obtained polar logpolar mapping analogy analysis conducted equation 10 possible model computation timetoimpact gaussian stochastic process parameters involved computation uncorrelated probabilistic variables variance timetoimpact equation oe log e 2 oe ffi fl similarly variance timetoimpact equation 33 oe ae ae ae ae ffi j let us assume variances optical flow partial derivatives equation 41 42 assumption justified fact formulation used estimate optical flow considering unitary sampling step polar mapping oe ae 2 1 possible compare two variances inequality ae 4 oe 2 variance radial component optical flow first notice value term left hand side depends value ae term right hand side constant therefore first term left hand side neglected good approximation constitutes higher order infinitesimal term respect 1 variance oe 2 close unit sufficient evaluate given value 10945543 trivial find value variance equation 41 lower equation 42 radial coordinate ae less 22 analysis made comparing equation 27 31 expressions variance timetoimpact two equations oe log e 2 oe 44a oe ae ae ae3 ae let us assume variances oe aeof independent parameters small least bounded integer value velocity field smooth also value ae bounded higher terms turn relative error second derivative optical flow compare equation 44a 44b analysing inequality ae assuming variances oe ae 2 almost equal inequality simply represents intersection parabola function radial coordinate ae horizontal straight line meaurement time impact performed using equation 27 turns accurate obtained using equation 31 sense minimum variance radial coordinate considered point greater log e result nicely complements constraint found comparing equation 25 33 considering point image varying radial coordinate ae whereas equation 31 allows accurate estimation equation 27 equation 25 lower variance equation 33 thus balancing performances two formulations involving polar logpolar mapping throughout entire field view r simulated output retinal ccd sensor time impact fig applied first image cartesian x logpolar equation 31 optical flow fig cartesian x 8th 10th b frame sequence fig tr edge detection motion stereo using egomotion complex logarithmic mapping active vision integration fixed mobile cameras motor spatial aspects artificial vision obstacle avoidance using flow field divergence active tracking strategy monocular depth inference multiple frames extending oriented smoothness constraint temporal domain estimation derivatives optical flow estimation depth motion using anthropomorphic visual sensor active vision based spacevariant sensing measurement visual motion robot vision computer vision ctr konrad schindler geometry construction straight lines logpolar images computer vision image understanding v103 n3 p196207 september 2006 mohammed yeasin optical flow logmapped image planea new approach ieee transactions pattern analysis machine intelligence v24 n1 p125131 january 2002 jos martnez leopoldo altamirano new foveal cartesian geometry approach used object tracking proceedings 24th iasted international conference signal processing pattern recognition applications p133139 february 1517 2006 innsbruck austria v javier traver filiberto pla similarity motion estimation active tracking spatialdomain projections logpolar images computer vision image understanding v97 n2 p209241 february 2005 sovira tan jason l dale alan johnston performance three recursive algorithms fast spacevariant gaussian filtering realtime imaging v9 n3 p215228 june nattel yehezkel yeshurun direct feature extraction foveated environment pattern recognition letters v23 n13 p15371548 november 2002 didi sazbon hctor rotstein ehud rivlin finding focus expansion estimating range using optical flow images matched filter machine vision applications v15 n4 p229236 october 2004 pierre chalimbaud franois berry embedded active vision system based fpga architecture eurasip journal embedded systems v2007 n1 p2626 january 2007 swarup reddi george loizou analysis camera behavior tracking ieee transactions pattern analysis machine intelligence v17 n8 p765778 august 1995 phillipe burlina rama chellappa analyzing looming motion components spatiotemporal spectral signature ieee transactions pattern analysis machine intelligence v18 n10 p10291033 october 1996 philippe burlina rama chellappa temporal analysis motion video sequences predictive operators international journal computer vision v28 n2 p175192 june 1998 frank tong zenian li reciprocalwedge transform spacevariant sensing ieee transactions pattern analysis machine intelligence v17 n5 p500511 may 1995 pelegrn camacho fabin arrebola francisco sandoval multiresolution vision autonomous systems autonomous robotic systems soft computing hard computing methodologies applications physicaverlag gmbh heidelberg germany zoran duric azriel rosenfeld james duncan applicability greens theorem computation rate approach international journal computer vision v31 n1 p8398 feb 1999 jose antonio boluda fernando pardo reconfigurable architecture autonomous visualnavigation machine vision applications v13 n56 p322331 march f wrgtter cozzi v gerdes parallel noiserobust algorithm recover depth information radial flow fields neural computation v11 n2 p381416 feb 15 1999 c capurro f panerai g sandini dynamic vergence using logpolar images international journal computer vision v24 n1 p7994 aug 1997