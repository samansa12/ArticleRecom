machine learning approach pos tagging applied inductive learning statistical decision trees relaxation labeling natural language processing nlp task morphosyntactic disambiguation part speech tagging learning process supervised obtains language model oriented resolve pos ambiguities consisting set statistical decision trees expressing distribution tags words relevant contexts acquired decision trees directly used tagger relatively simple fast tested evaluated wall street journal wsj corpus competitive accuracy however better results obtained translating trees rules feed flexible relaxation labeling based tagger direction describe tagger able use information kind ngrams automatically acquired constraints linguistically motivated manually written constraints etc particular incorporate machinelearned decision trees simultaneously address problem tagging limited training material available crucial process constructing scratch annotated corpus show high levels accuracy achieved system situation report results obtained using develop 55 million words spanish corpus scratch b introduction part speech pos tagging basic well known natural language processing nlp problem consists assigning word text proper morphosyntactic tag context appearance useful number nlp applications preprocessing step syntactic parsing information extraction retrieval eg document classification internet searchers text speech systems corpus linguistics etc base pos tagging many words ambiguous regarding pos cases completely disambiguated taking account adequate context instance sample sentence presented table 1 word shot disambiguated past participle preceded auxiliary although case word disambiguated simply looking preceding tag must taken account preceding word could ambiguous necessary context could much complicated merely preceding word furthermore even cases ambiguity nonresolvable using morphosyntactic features context require semantic andor pragmatic knowledge table 1 sentence pos ambiguity appearing tags penn treebank corpus described appendix dt first jj time nn prp vbd shot vbn dt hand nn prp chased vbd dt robbers nns outside rb first time shot hand chased outside jj nn nn nn jj vbn rp vbn nn 11 existing approaches pos tagging starting pioneer tagger taggit greene rubin 1971 used initial tagging brown corpus bc lot effort devoted improving quality tagging process terms accuracy efficiency existing taggers classified three main groups according kind knowledge use linguistic statistic machinelearning family course taggers difficult classify classes hybrid approaches must considered within linguistic approach systems codify knowledge involved set rules constraints written linguists linguistic models range hundreds several thousand rules usually require years labor work tosca group oostdijk 1991 recently development constraint grammars helsinki university karlsson et al 1995 considered important direction machine learning approach pos tagging 3 extended approach nowadays statistical family obviously due limited amount human effort involved basically consists building statistical model language using model disambiguate word sequence language model coded set cooccurrence frequencies different kinds linguistic phenomena statistical acquisition usually found form ngram collection probability certain sequence length n estimated occurrences training corpus case pos tagging usual models consist tag bigrams trigrams possible sequences two three consecutive tags respectively ngram probabilities estimated new examples tagged selecting tag sequence highest probability roughly technique followed widespread hidden markov model taggers although form model way determining sequence modeled also tackled several ways systems reduce model unigrams bigrams trigrams seminal work direction claws system garside et al 1987 used bigram information probabilistic version taggit later improved derose 1988 using dynamic programming tagger church 1988 used trigram model taggers try reduce amount training data needed estimate model use baumwelch reestimation algorithm baum 1972 iteratively refine initial model obtained small handtagged corpus case xerox tagger cutting et al 1992 successors interested subject find excellent overview merialdo 1994 works placed statistical family schmid 1994a performs energyfunction optimization using neural nets comparisons linguistic statistic taggers found chanod tapanainen 1995 tasks also approached statistical methods speech recognition field productive issue actually ngram modelling used speech recognition used pos tagging recent works field try limit model fixed order ngram combining different order ngrams morphological information longdistance ngrams triggering pairs rosenfeld 1994 ristad thomas 1996 saul pereira 1997 approaches may see incorporated pos tagging tasks short term although statistic approach involves kind learning supervised un supervised parameters model training corpus place machinelearning family systems include sophisticated information ngram model brills tagger brill 1992 brill 1995 automatically learns set transformation rules best repair errors committed mostfrequenttag tagger samuelsson et al 1996 acquire constraint grammar rules tagged corpora daelemans et al 1996 apply instancebased learning finally work present based marquez rodriguez 1997 uses decision trees induced tagged corpora combines learned knowledge hybrid approach consisting applying arquez llu padr horacio rodr iguez relaxation techniques set constraints involving statistical linguistic machinelearned information padro 1996 padro 1998 accuracy reported statistic taggers surpasses 9697 linguistic constraint grammars surpass 99 allowing residual ambiguity 1026 tags per word accuracy values usually computed test corpus used training phase corpora commonly used test benches brown corpus wall street journal wsj corpus british national corpus bnc 12 motivation goals taking accuracy figures account one may think pos tagging solved closed problem accuracy perfectly acceptable systems waste time designing yet another tagger increase 03 accuracy really mean several reasons thinking still work field automatic pos tagging processing huge running texts considering average length per sentence 2530 words admit error rate 34 follows average sentence contains one error since pos tagging basic task nlp understanding systems starting error sentence could severe drawback especially considering propagation errors could grow linearly nlp tasks sensitive pos disambiguation errors found domain word sense disambiguation wilks stevenson 1997 information retrieval krovetz 1997 another issue refers need adapting tuning taggers acquired learned parameters specific corpus onto another one may contain texts domains trying minimize cost transportation accuracy taggers usually measured test corpora characteristics corpus used training nevertheless serious attempts made evaluate accuracy taggers corpora different charac teristics even domainspecific finally specific problems must addressed applying taggers languages english addition problems derived richer morphology particular languages general problem consisting lack large manually annotated corpora training although bootstrapping approach carried using lowaccurate tagger producing annotated text could used retraining tagger learning accurate model usefulness approach highly relies quality retraining material want guarantee low noisy retraining corpora provide methods able achieve high accuracy known unknown words using small highquality training corpus direction involved project tagging spanish catalan corpora 5m words limited linguistic resources departing machine learning approach pos tagging 5 manually tagged core size around 70000 words sake comparability included experiments performed reference corpus english however also report results obtained applying presented techniques annotate lexesp spanish corpus proving good accuracy achieved fairly low human cost paper organized follows section 2 describe application domain language model learning algorithm model evaluation sections 3 4 describe language model application two taggers decision tree based tagger relaxation labelling based tagger respectively comparative results special case using small training corpus joint use taggers annotate spanish corpus reported section 5 finally main conclusions overview future work found section 6 2 language model acquisition enable computer system process natural language required language modeled way phenomena occurring language characterized captured way used predict recognize future uses language rosenfeld 1994 defines language modeling attempt characterize capture exploit regularities natural language states need language modeling arises great deal variability uncertainty present natural language described section 1 language models handwritten statistically derived machinelearned paper present use machinelearned model combined statistically acquired models testimonial use handwritten models also included 21 description training corpus word form lexicon used portion 1 170 000 words wsj tagged according penn treebank tag set train test system relevant features following tag set contains 45 different tags 1 365 words corpus ambiguous ambiguity ratio 244 tagsword ambiguous words 152 overall corpus contains 243 different ambiguity classes equally important fact 40 frequent ambiguity classes cover 8395 occurrences corpus 194 frequent cover almost 9950 training corpus also used create word form lexicon 49206 entries associated lexical probabilities word probabilities estimated simply counting number times word appears corpus different tag simple information provides heuristic arquez llu padr horacio rodr iguez naive disambiguation algorithm consists choosing word probable tag according lexical probability note tagger use contextual information frequencies isolated words figure 1 shows performance mostfrequenttag tagger mft wsj domain different sizes training corpus reported figures refer ambiguous words taken lower bound tagger particularly clear training corpus bigger 400000 words accuracy obtained around 8183 however reasonable think could significantly raised simply adding training corpus order estimate lexical probabilities effectively657585 training accuracy mft tagger figure 1 performance frequent tag heuristic related training set size due errors corpus annotation resulting lexicon certain amount noise order partially reduce noise lexicon filtered manually checking entries frequent 200 words corpus note 200 frequent words corpus represent half instance original lexicon entry numbers indicate frequencies training corpus common word since appears corpus six different tags cd cardinal dt de proper noun vbp verbpersonal form obvious correct reading determiner 22 learnig algorithm set possible tags choosing proper syntactic tag word particular context stated problem classification case classes identified tags decision trees recently used several nlp tasks speech recognition bahl 1989 pos tagging schmid 1994b marquez rodriguez 1995 daelemans et al 1996 parsing mccarthy lehnert 1995 magerman 1996 machine learning approach pos tagging 7 sense disambiguation mooney 1996 information extraction cardie 1994 suitable performing task 221 ambiguity classes statistical decision trees possible group words appearing corpus according set possible tags ie adjectivenoun adjectivenounverb adverbpreposition etc call sets ambiguity classes obvious inclusion relation classes ie words adjective noun verb particular adjective noun whole set ambiguity classes viewed taxonomy dag structure figure 2 represents part taxonomy together inclusion relation extracted wsj 2ambiguity 4ambiguity jjnnrbvb jjnnrbrpvb injjnnrb jjnnrb jjnnnprb jjnnrbuh figure 2 part ambiguityclass taxonomy wsj corpus way split general pos tagging problem one classification problem ambiguity class identify remarkable features domain comparing common classification domains machine learning field firstly large number training examples 60000 examples single tree secondly quite significant noise training test data wsj corpus contains 23 mistagged words main consequence characteristics together fact simple context conditions cannot explain ambiguities voutilainen 1994 possible obtain trees completely classify training examples stead aspire obtain adjusted probability distributions words possible tags conditioned particular contexts appearance use statistical decision trees instead common decision trees representing information algorithm used construct statistical decision trees nonincremental supervised learningfromexamples algorithm tdidt top induction decision trees family constructs trees topdown way guided distributional information examples quinlan 1993 arquez llu padr horacio rodr iguez 222 training set attributes ambiguity class set examples built selecting training corpus occurrences words belonging ambiguity class set attributes describe example refer partofspeech tags neighbour words orthography characteristics word disambiguated discrete attributes common ambiguity classes set attributes consists window covering 3 tags left 2 tags right size well final set attributes determined empirical basis wordform table 2 shows real examples training set words preposition adverb inrb ambiguity class table 2 training examples prepositionadverb ambiguity class tag gamma3 tag gamma2 tag gamma1 wordtag tag 1 tag 2 rb vbd afterin dt nns jj nn nns belowrb vbp dt new set orthographic features incorporated order deal particular ambiguity class namely unknown words introduced following sections see table 3 description whole set attributes table 3 list considered attributes attribute values number values tag gamma3 tag penn treebank 45 tag tag tag tag word form word ambiguity class 847 first character printable ascii character 190 last character capital attributes many values ie wordform presuffix attributes used dealing unknown words treated dynamically adjusting number values n frequent joining rest new otherwise value maximum number values fixed 45 number different tags order homogeneous attributes machine learning approach pos tagging 9 223 attribute selection function testing several attribute selection functions including quinlans gain ratio quinlan 1986 gini diversity index breiman et al 1984 relieff kononenko 1994 2 test symmetrical tau 1991 significant differences used attribute selection function proposed lopez de mantaras 1991 belonging informationtheorybased family showed slightly higher stability others proved biased towards attributes many values capable generating smaller trees loss accuracy compared quinlans gain ratio lopez de mantaras et al 1996 roughly speaking defines distance measurement partitions selects branching attribute generates closest partition correct partition namely one perfectly classifies training data let x set examples c set classes p c x partition x according values c selected attribute one generates closest partition x p c x need define distance measurement partitions let pa x partition x induced values attribute average information partition defined follows px probability element x belonging set subset x whose examples certain value attribute estimated ratio jx aj jxj average information measurement reflects randomness distribution elements x classes partition induced consider intersection two different partitions induced attributes b obtain conditioned information pb x given pa x ipb xjpa pxa easy show measurement distance normalizing obtain values 01 finally selected attribute one minimizes normalized distance dn p c x pa x arquez llu padr horacio rodr iguez 224 branching strategy dealing discrete attributes usual tdidt algorithms consider branch value selected attribute however possibilities instance systems perform previous recasting attributes order binaryvalued attributes magerman 1996 motivation could efficiency dealing binary trees certain advan tages avoiding excessive data fragmentation large number values although transformation attributes always possible resulting attributes lose intuition direct interpretation explode number chosen mixed approach consists splitting values subsequently joining resulting subsets groups insufficient statistical evidence different distributions statistical evidence tested 2 test 95 confidence level previous smoothing data order avoid zero probabilities 225 pruning tree order decrease effect overfitting implemented post pruning technique first step tree completely expanded afterwards pruned following minimal costcomplexity criterion breiman et al 1984 using comparatively small fresh part training set alternative smoothing conditional probability distributions leaves using fresh corpus magerman 1996 left also wanted reduce size trees experimental tests shown domain pruning process reduces tree sizes 50 improves accuracy 25 226 example finally present real example decision tree branch learned class inrb clear linguistic interpretation word form 1st right tag 2nd right tag others others others figure 3 example decision tree branch observe figure 3 node path root leaf contains question concrete attribute probability distribution root prior probability distribution class nodes represents probability distribution conditioned answers questions preceding node example second node says word commonly preposition adverb leaf says word machine learning approach pos tagging 11 almost certainly adverb occurs immediately another adverb preposition case much well soon etc 3 treetagger treebased tagger using model described previous section implemented reductionistic tagger sense constraint grammars karlsson et al 1995 initial step wordform frequency dictionary constructed training corpus provides input word possible tags associated lexical probability iterative process reduces ambiguity discarding low probable tags step certain stopping criterion satisfied whole process represented figure 4 see also table 4 real process disambiguation part sentence presented table 1 raw text classify update filter tagging algorithm tree base tagged text tokenizer frequency lexicon language model figure 4 architecture treetagger particularly step ambiguous word work done parallel 1 classify word using corresponding decision tree ambiguity context either left right classification may generate multiple answers questions nodes case paths followed result taken weighted average results possible paths 2 use resulting probability distribution update probability distribution word updating probabilities done simply multiplying previous probabilities per new probabilities coming tree 3 discard tags almost zero probability probabilities lower certain discard boundary parameter stopping criterion satisfied words could still remain ambigu ous two possibilities 1 choose probable tag stillambiguous word completely disambiguate text 2 accept residual ambiguity successive treatment note unique iteration forcing complete disambiguation equivalent use trees directly classifiers results efficient tagger arquez llu padr horacio rodr iguez table 4 example disambiguation chased robbers outside it1 in096 prp1 vbd097 dt1 nns1 in001 1 it2 in1 prp1 vbd1 dt1 nns1 rb1 1 stop performing several steps progressively reduces efficiency takes advantage statistical nature trees get accurate results another important point determine appropriate stopping criterion since procedure heuristics convergence guaranteed however case experiments first experiments seem indicate performance increases unique maximum softly decreases number iterations increases phenomenon studied padro 1998 noise training test sets suggested major cause sake sim plicity experiments reported following sections number iterations experimentally fixed three although might seem arbitrary decision broadranging experiments performed seem indicate value results good average tagging performance terms accuracy efficiency 31 using treetagger divided wsj corpus two parts words used train ingpruning set 50 000 words fresh test set used lexicon described section 21 derived training corpus containing possible tags word well lexical probabilities words test corpus appearing training set stored tags words test corpus lexical probability ie assigning uniform distribution approach corresponds assumption morphological analyzer provides possible tags unknown words following experiments treat unknown words less informed way 243 ambiguity classes acquisition algorithm learned base 194 trees covering 995 ambiguous words requiring 500 kb storage learning algorithm common lisp implementation took cpuhours running sun sparcstation10 64mb primary memory first four columns table 5 contain information trees learned ten representative ambiguity classes present figures number examples used learning tree number nodes estimation error rate tested sample new examples last figure could machine learning approach pos tagging 13 taken rough estimation error trees used treetagger though exactly true since learning examples fully disambiguated context tagging contexts left right ambiguous table 5 tree information number percentages error difficult ambiguity classes amb class exs nodes error tterrors mfterrors jjvbdvbn 11346 761 1875 95 1670 180 3164 jjnn 16922 680 1630 122 1401 144 1654 nnsvbz 15233 688 437 44 619 81 1140 jjrb 8650 854 1120 48 1084 73 1649 total 179601 5871 787 1806 tagging algorithm running sun ultrasparc2 processed test set speed 300 wordssec results obtained seen different levels granularity ffl performance learned trees shown last two columns table 5 corresponding ambiguity classes concentrate 625 errors committed mostfrequenttag tagger mft column tt column shows number percentage errors committed tagger one hand observe remarkable reduction number errors 564 hand useful identify problematic cases instance jjnn seems difficult ambiguity class since associated tree obtains slight error reduction mft baseline tagger 153 surprising since semantic knowledge necessary fully disambiguate noun adjective results dtinrbwdt ambiguity reflect overestimation generalization performance tree predicted error rate 607 much lower real 1208 may indicating problem pruning ffl global results following forcing complete disambiguation resulting accuracy 9729 accepting residual ambiguity accuracy rate increased 9822 ambiguity ratio 108 tagsword ambiguous words 1026 tagsword overall words 275 words remained ambiguous 96 retaining 2 tags marquez rodriguez 1997 shown results good better cases results number nonlinguistically motivated stateoftheart taggers arquez llu padr horacio rodr iguez addition present figure 5 performance achieved tagger increasing sizes training corpus results accuracy computed words figure includes mft results seen lower bound9092949698 accuracy treetagger mft tagger figure 5 performance tagger related training set size following intuition see performance grows training set size grows maximum 9729 previously indicated one way easily evaluate quality classprobability estimates given classifier calculate rejection curve plot curve showing percentage correctly classified test cases whose confidence level exceeds given value case statistical decision trees confidence level straightforwardly computed class probabilities given leaves trees case calculate confidence level difference probability two probable cases difference large chosen class clearly much better others difference small chosen class nearly tied another class rejection curve increases smoothly indicates confidence level produced classifier transformed accurate probability measurement rejection curve classifier included figure 6 increases fairly smoothly giving idea acquired statistical decision trees provide good confidence estimates close connection aforementioned positive results tagger disambiguation lowconfidence cases required 32 unknown words unknown words words present lexicon ie case words present training corpus previous experiments considered possibility unknown words instead assumed morphological analyzer providing set possible tags uniform probability dis tribution however realistic scenario firstly morphological analyzer always present due morphological simplicity treated machine learning approach pos rejection accuracy figure 6 rejection curve trees acquired full training set language existence efficiency requirements simply lack sources secondly available probably certain error rate makes necessary considered noise introduces seems clear deal unknown words order obtain realistic figures real performance tagger several approaches dealing unknown words one hand one assume unknown words may potentially take tag excluding tags corresponding closed categories preposition determiner etc try disambiguate hand approaches include preprocess tries guess set candidate tags unknown word feed tagger information see padro 1998 detailed explanation methods case consider unknown words words belonging ambiguity class containing possible tags corresponding open categories ie noun proper noun verb adjective adverb cardinal etc number candidate tags come 20 state classification problem 20 different classes estimated proportion tags appearing naturally wsj unknown words collected examples training corpus according proportions frequent tag nnp proper noun represents almost 30 sample fact establishes lower bound accuracy 30 domain ie performance mostfrequenttag tagger would obtain used simple information orthography context unknown words order improve results particular initial set 17 potential attributes empirically decided relevant turned following 1 reference word form first letter last three letters four binaryvalued attributes accounting capitalization whether word multiword existence numeric characters word 2 reference context preceding following pos tags set attributes fully described table 3 arquez llu padr horacio rodr iguez table 6 shows generalization performance trees learned training sets increasing sizes 50000 words order compare figures close approach implemented igtree system daelemans et al 1996 tested performance exactly conditions igtree system memorybased pos tagger stores memory whole set training examples predicts part speech tags new words particular contexts extrapolation similar cases held memory knearest neighbour retrieval algorithm main connection point work presented huge example bases indexed using treebased formalism retrieval algorithm performed using generated trees classi fiers additionally trees constructed base previous weighting attributes contextual orthographic attributes used disambiguating similar using quinlans information ratio quinlan 1986 note final pruning step applied igtree increase compression factor even also implemented version results igtree also included table 6 figures 7 8 contain plots corresponding results table 6 generalization performance trees unknown words treetagger igtree exs accuracynodes accuracynodes 2000 7753 224 7036 627 5000 8090 520 7633 1438 10000 8330 1112 7918 2664 20000 8582 1644 8230 4783 30000 8732 2476 8511 6477 50000 8812 4056 8714 9554 observe system produces better quality trees igtree measure quality terms generalization performance well trees fit new examples size number nodes one hand see figure 7 generalization performance better hand figure 8 seems indicate growing factor number nodes linear cases clearly lower important aspects contributing lower size merging attribute values post pruning process applied algorithm experimental results showed tree size reduced 50 average without loss accuracy marquez 1998 better performance probably due fact igtrees actually decision trees sense trees acquired supervised algorithm topdown induction use certain attribute selection function decide step attribute best contributes discriminate current set examples treebased compression base examples inside machine learning approach pos tagging accuracy treetagger igtree figure 7 accuracy vs training set size unknown words kind weighted nearestneighbour retrieval algorithm representation weighting attributes allows us think igtrees decision trees would obtained applying usual topdown induction algorithm naive attribute selection function consisting making previous unique ranking attributes using quinlans information ratio examples later selecting attributes according ordering experimental results show better reconsider selection attributes step decide priori fixed order marquez 19982006001000 nodes treetagger igtree figure 8 number nodes trees unknown words course conclusions taken domain small training sets plot figure 8 suggests difference two methods decreases training set size increases using bigger corpora training arquez llu padr horacio rodr iguez might improve performance significantly instance daelemans et al 1996 report accuracy rate 906 unknown words training whole wsj 2 million words results considered better sense system needs less resources achieving performance note result holds using whole training set daelemans et al report tagging accuracy 964 training 2mwords training set results slightly 97 achieved using 12mwords 2 4 relax relaxation labelling based tagger described decisiontree acquisition algorithm used automatically obtain language model pos tagging classification algorithm uses obtained model disambiguate fresh texts language model acquired would useful could used different systems extended new knowledge section describe flexible tagger based relaxation labelling methods enables use models coming different sources well combination cooperation algorithm bigrams trigrams manually constraints tagged corpus labelling constraints treebased relaxation raw corpus lexicon language model tagging algorithm figure 9 architecture relax tagger tagger present architecture described figure 9 unique algorithm uses language model consisting constraints obtained different knowledge sources relaxation generic name family iterative algorithms perform function optimization based local information closely related neural nets torras 1989 gradient step larrosa meseguer 1995b although relaxation operations long used engineering fields solve systems equations southwell 1940 achieve breakthrough success relaxation labelling extension symbolic domain applied waltz 1975 rosenfeld et al 1976 constraint propagation field especially lowlevel vision problems relaxation labelling technique used solve consistent labelling problems clps see larrosa meseguer 1995a consistent labelling proba lem consists given set variables assigning variable value compatible values ones satisfying maximum possible extent set compatibility constraints artificial intelligence field relaxation mainly used computer vision since first used address problems corner edge recognition line image smoothing richards et al 1981 lloyd 1983 nevertheless many traditional ai problems stated labelling prob lem traveling salesman problem nqueens combinatorial problem aarts korst 1987 utility algorithm perform nlp tasks pointed work pelillo refice 1994 pelillo maffione 1994 pos tagging used toy problem test methods improve computation constraint compatibility coefficients relaxation processes nevertheless first application real nlp problems unrestricted text work presented padro 1996 voutilainen padro 1997 marquez padro 1997 padro 1998 point view remarkable feature algorithm since deals context constraints model uses improved writing constraint formalism available knowledge constraints used may come different sources statistical acquisition machinelearned models hand coding additional advantage tagging algorithm independent complexity model 41 algorithm although section relaxation algorithm described general point view application pos tagging straightforwardly performed considering word variable possible pos tags label set variables words g set possible labels pos tags variable v number different labels possible v let c set constraints labels variables constraint compatibility value combination pairs variablelabel binary constraint eg bigram ternary constraint eg trigram first constraint states combination variable v 1 label variable v 3 label b compatibility value 053 similarly second constraint states compatibility value three pairs variablevalue contains constraints order define compatibility value combinations number variables aim algorithm find weighted labelling global consistency maximized weighted labelling weight assignment possible label variable arquez llu padr horacio rodr iguez vector containing weight possible label v p since relaxation iterative process weights vary time note weight label j variable time step n p j n simply p j time step relevant maximizing global consistency defined maximizing variable v 1 average support variable defined weighted sum support received possible labels ij support received pair context support pair variablelabel compatible assignation label j variable labels neighbouring variables according constraint set although several support functions may used chose following one defines support sum influence every constraint label infr defined follows r ij set constraints label j variable ie constraints formed combination variablelabel pairs includes pair v k1 kd product current weights labels appearing constraint except v representing applicable constraint current context multiplied c r constraint compatibility value stating compatible pair context although c r compatibility values constraint may computed different ways performed experiments padro 1996 padro 1998 point best results case obtained computing compatibilities mutual information tag context cover thomas 1991 mutual information measures informative event respect another computed b independent events conditional probability given b equal marginal probability measurement zero conditional probability larger means two events tend appear together often would chance measurement yields positive number inversely conditional occurrence scarcer chance measurement negative although mutual information simple useful way assign compatibility values constraints promising possibility still explored assigning maximum entropy estimation rosenfeld 1994 pseudocode relaxation algorithm found table 7 consists following steps machine learning approach pos tagging 21 1 start random labelling p 0 case select betterinformed starting point lexical probabilities word tag 2 variable compute support label receives current weights variable labels ie see compatible current weighting current weightings variables given set constraints 3 update weight variable label according support obtained increase weight labels high support greater zero decrease weight low support less zero chosen updating function case 4 iterate process convergence criterion met usual criterion wait significant changes support computing weight changing must performed parallel avoid changing weight label would affect support computation others could summarize algorithm saying timestep variable changes label weights depending compatible label labels variables timestep constraints consistent process converges state variable weight 1 one labels weight 0 others performed global consistency maximization vector optimization maximize one might think sum supports variables finds weighted labelling choice would increase support variable given course labelling exists labelling exist algorithm end local maximum note global consistency idea makes algorithm robust problem mutually incompatible constraints combination label assignations satisfies constraints solved relaxation necessarily find exclusive combination labels ie unique label variable weight possible label constraints satisfied maximum possible degree especially useful case since constraints automatically acquired different knowledge sources combined constraints might fully consistent advantages algorithm ffl highly local character variable compute new label weights given state previous timestep makes algorithm highly parallelizable could processor compute new label weights 22 llu arquez llu padr horacio rodr iguez 1 2 repeat 3 variable v 4 j possible label v infr 6 end 7 j possible label v 8 9 end 10 end 11 changes table 7 pseudo code relaxation labelling algorithm variable even processor compute weight label variable expressiveness since state problem terms constraints variable labels case enables us use binary bigram ternary trigram constraints well sophisticated constraints decision tree branches handwritten constraints flexibility check absolute consistency constraints robustness since give answer problems without exact solution incompatible constraints insufficient data ffl ability find localoptima solutions np problems nonexponential time upper bound number iterations ie convergence fast algorithm stopped fixed number iterations drawbacks algorithm cost n number variables v average number possible labels per variable c average number constraints per label average number iterations convergence average cost n theta v theta c theta depends linearly n problem many labels constraints convergence quickly achieved multiplying terms might much bigger n application pos tagging bottleneck machine learning approach pos tagging 23 number constraints may several thousand average number tags per ambiguous word 25 average sentence contains 10 ambiguous words ffl since acts approximation gradient step algorithms typical convergence problems found optima local convergence guaran teed since chosen step might large function optimize ffl general relaxation labelling applications constraints would written manu ally since modeling problem good easytomodel domains reduced constraintset problems case pos tagging constraints many complicated easily written hand ffl difficulty stating hand compatibility value con straint deal combinatorial problems exact solution eg traveling salesman constraints either fully compatible eg stating possible go city fully incompatible eg stating possible twice city value straightforwardly derived distance cities try model sophisticated less exact problems pos tagging establish way assigning graded compatibility values constraints mentioned using mutual information ffl difficulty choosing suitable support updating functions particular problem 42 using machinelearned constraints order feed relax tagger language model acquired decisiontree learning algorithm group 44 representative trees covering 8395 examples translated set weighted context constraints relax fed constraints also bitri gram information constraint grammars formalism karlsson et al 1995 used code tree branches cg widespread formalism used write context constraints since able represent kind context pattern use represent constraints ngram patterns handwritten constraints decisiontree branches since cg formalism intended linguistic uses statistical contribution place constraints state full compatibility constraints select particular reading full incompatibility constraints remove particular reading thus slightly extended formalism enable use realvalued compatibilities way constraints assigned removeselect command real number indicating constraint compatibility value described section 41 computed mutual information focus tag context arquez llu padr horacio rodr iguez translation bitrigrams context constraints straightforward left prediction bigram right prediction counterpart would training corpus contains 1404 different bigrams since used left right prediction converted 2808 binary constraints trigram may used three possible ways ie abc trigram pattern generates constraints c given preceded ab given followed bc b given preceded followed c 216 vb 154 nn 182 dt 17387 trigram patterns training corpus produce 52161 ternary constraints usual way expressing trees set rules used construct context constraints instance tree branch represented figure 3 translated two following constraints gamma581 2366 rb 0 0 express compatibility either positive negative tag first line given context ie focus word first word right tag rb second tag decision trees acquired 44 frequent ambiguity classes result set 8473 constraints main advantage relax ability deal constraints kind enables us combine statistical ngrams written form constraints learned decision tree models even linguistically motivated handwritten constraints following states high compatibility value vbn participle tag preceded auxiliary verb provided participle adjective phrase change since cost algorithm depends linearly number constraints use trigram constraints either alone combined others makes disambiguation six times slower using bc 20 times slower using b machine learning approach pos tagging 25 obtained results different knowledge combination shown table 8 results produced two baseline taggers mft mostfrequenttag tag ger hmm bigram hidden markov model tagger elworthy 1993 also reported b stands bigrams trigrams c constraints acquired decision tree learning algorithm results using sample 20 linguisticallymotivated constraints h found table 9 results show addition automatically acquired context constraints led improvement accuracy tagger overcoming bitri gram models properly cooperating see marquez padro 1997 details experiments comparisons current taggers table 8 results baseline tagger relax tagger using every combination constraint kinds ambiguous 8531 9175 9135 9182 9192 9196 9272 9282 9255 overall 9466 9700 9686 9703 9706 9708 9736 9739 9729 table 9 results tagger using every combination constraint kinds plus hand written constraints h bh th bth ch bch tch btch ambiguous 8641 9188 9204 9232 9197 9276 9298 9271 overall 9506 9705 9711 9721 9708 9737 9745 9735 figure shows 95 confidence intervals results table 8 main conclusions drawn data described ffl relax slightly worse hmm tagger using information bigrams may due higher sensitivity noise training corpus ffl two significantly distinct groups using statistical infor mation using statistical information plus decision trees model ngram models learned model belong first group combination statistical model acquired constraint belongs second group ffl although handwritten constraints improve accuracy model size linguistic constraint set small make improvement statistically significant ffl combination two kinds model produces significantly better results separate use indicates model contains information included relaxation labelling combines properly 26 llu arquez llu padr horacio rodr iguezhmmbtbtcbctcbtctch 9650 9700 9750 9700 9750 figure 10 95 confidence intervals relax tagger results 5 using small training sets section discuss results obtained using two taggers described apply language models learned small training corpus motivation analysis need determining behavior taggers used language models coming scarce training data order best exploit development spanish catalan tagged corpora starting scratch 51 testing performance wsj particular used 50000 words wsj corpus automatically derive set decision trees collect bigram statistics trigram statistics considered since size training corpus large enough reasonably estimate big number parameters model note 45tag tag set produces trigram model 90000 parameters obviously cannot estimated set 50000 occurrences using training set learning algorithm able reliably acquire trees representing frequent ambiguity classes note training data insufficient learning sensible trees 150 ambiguity classes following formalism described previous section translated trees set 4000 constraints feed relaxation labelling algorithm results table 10 computed average ten experiments using randomly chosen training sets 50000 words b stands bigram machine learning approach pos tagging 27 table 10 comparative results using different models acquired small training corpus mft treetagger relaxc relaxb relaxbc ambiguous 7535 8729 8629 8750 8856 overall 9164 9569 9535 9576 9612treebased crelax crelax brelax bc figure 11 95 confidence intervals tagger results model c learned decision tree either form trees translated constraints corresponding confidence intervals found figure 11 presented figures point following conclusions ffl think result quite accurate order corroborate statement compare accuracy 9612 960 reported daelemans et al 1996 igtree tagger trained double size corpus 100 kw ffl treetagger yields higher performance relax tagger use c model caused fact due scarceness data significant amount test cases match complete tree branch thus treetagger uses intermediate node probabilities since complete branches translated constrains partial branches used avoid excessive growth number constraints tagger use intermediate node information produces lower results exhaustive translation tree information constraints issue studied short run ffl relax tagger using b model produces better results taggers using c model alone cause related aforementioned problem estimating big number parameters small sample since model consists six features number parameters 28 llu arquez llu padr horacio rodr iguez learned still larger case trigrams thus estimation complete could ffl relax tagger using bc model produces better results statistically significant 95 confidence level combination suggests although tree model complete enough contains different information bigram model moreover information proved useful combined b model relax 52 tagging lexesp spanish corpus lexesp project multidisciplinary effort headed psychology department university oviedo aims create large database language usage order enable encourage research activities wide range fields linguistics medicine psychology artificial intelligence among others one main issues database linguistic resources lexesp corpus contains 55 mw written material including general news sports news literature scientific articles etc corpus morphologically analyzed maco system fast broadcoverage analyzer carmona et al 1998 tagset contains 62 tags percentage ambiguous words 3926 average ambiguity ratio 263 tagsword ambiguous words 164 overall material 95 kw handdisambiguated get initial training set 70 kw test set 25 kw automatically disambiguate rest corpus applied bootstrapping method taking advantage use taggers procedure applied starts using small handtagged portion corpus initial training set taggers used disambiguate fresh material tagger agreement cases material used enlarge language model incorporating training set retraining taggers procedure could iterated obtain progressively better language models point cases taggers coincide present higher accuracy thus used new retraining set lower error rate obtained using single tagger instance using single tagger trained handdisambiguated training set tag 200000 fresh words use retrain tagger case best tagger would tag new set 974 accuracy merging result previous handdisambiguated set would obtain 270kw corpus error rate 19 hand given taggers agree 975 cases 200kw set 984 cases correctly tagged get new corpus 195kw error rate 16 add manually tagged 70kw get 265kw corpus 12 error rate significantly lower 19 main results obtained approach summarized starting manually tagged training corpus best tagger combination achieved accuracy 931 ambiguous words 974 overall one bootstrapping machine learning approach pos tagging 29 iteration using coincidence cases fresh set 800 kw accuracy increased 942 ambiguous words 978 overall important note improvement statistically significant achieved completely automatic reestimation process domain iterations result new significant improvements detailed description refer reader marquez et al 1998 experiments using different sizes retraining corpus reported well different combination techniques weighted interpolation andor previous hand checking tagger disagreement cases aforementioned results emphasize following conclusions ffl 70 kw manuallydisambiguated training set provides enough evidence allow taggers get fairly good results absolute terms results obtained lexesp spanish corpus better obtained wsj english corpus one reasons contributing fact may less noisy training corpus however investigated part speech ambiguity cases spanish simpler average ffl combination two taggers seems useful obtain larger training corpora reduced error rate enable learning procedures build accurate taggers building tagger proposes single tag taggers coincide two tags disagree depending user needs might worthwhile accept higher remaining ambiguity favour higher recall models acquired best training corpus get tagger recall 983 remaining ambiguity 1009 tagsword 991 words fully disambiguated remaining 09 keep two tags 6 conclusions work presented evaluated machinelearning based algorithm obtaining statistical language models oriented pos tagging directly applied acquired models simple fast treebased tagger obtaining fairly good results also combined models ngram statistics flexible relaxationlabelling based tagger reported figures show models properly collaborate order improve results model learning testing performed wsj corpus english comparison results obtained using large training corpora see section 42 obtained fairly small training sets see section 5 points best policy cases combination learned treebased model best ngram model using large training corpora reported accuracy 9736 better least good number current nonlinguistic based taggers arquez llu padr horacio rodr iguez see marquez padro 1997 details using small training corpora promising 9612 obtained english deeper application techniques together collaboration taggers voting approach used develop scratch 55mw annotated corpus lexesp estimated accuracy 978 result confirms validity proposed method shows high accuracy possible spanish tagging relatively low manual effort details issue found marquez et al 1998 however work still done several directions referring language model learning algorithm interested testing informed attribute selection functions considering complex questions nodes finding good smoothing procedure dealing small ambiguity classes see marquez rodriguez 1997 first approach reference information algorithm uses would like explore inclusion morphological semantic information well complex context features nonlimited distance barrier rules style samuelsson et al 1996 also specially interested extending experiments involving combinations two taggers double direction first obtain less noisy corpora retraining steps bootstrapping processes second construct ensembles classifiers increase global tagging accuracy plan apply techniques develop taggers annotated corpora catalan language near future conclude saying carried first attempts padro 1998 using techniques tackle another classification problem nlp area namely word sense disambiguation wsd believe authors take advantage treating problems jointly acknowledgments research partially funded spanish research department ci cyts item project tic961243c0302 eu commission eurowordnet catalan research department cirits quality research group 1995sgr 00566 machine learning approach pos tagging 31 appendix list description penn treebank tag set used tagging wsj corpus complete description corpus see marcus et al 1993 cc coordinating conjunction cd cardinal number dt determiner ex existential fw foreign word preposition jj adjective jjr adjective comparative jjs adjective superlative ls list item marker md modal nn noun singular nnp proper noun singular nns noun plural nnps proper noun plural pos possessive ending prp personal pronoun possessive pronoun rb adverb rbr adverb comparative rbs adverb superlative rp particle uh interjection vb verb base form vbd verb past tense vbn verb past participle vbp verb non3rd ps sing present vbz verb 3rd ps sing present wdt whdeterminer wp whpronoun wp possessive whpronoun wrb whadverb end sentence comma straight double quote left open single quote left open double quote right close single quote right close double quote notes 1 size tag sets differ greatly one domain another depending contents complexity level annotation move 3040 several hundred different tags course differences important effects performance rates reported different systems imply difficulties comparing see krenn samuelsson 1996 detailed discussion issue 2 nevertheless recent studies tagger evaluation comparison padro marquez 1998 show noise test corpora case wsj may significantly distort evaluation comparison tagger accuracies may invalidate even improvement one reported test conditions taggers exactly r boltzmann machines applications inequality associated maximization technique statistical estimation probabilistic functions markov process classification regression trees unsupervised learning disambiguation rules partofspeech tagging domain specific knowledge acquisition conceptual sentence analysis phd thesis turmo j tagging french comparing statistical constraintbased method stochastic parts program noun phrase parser unrestricted text elements information theory practical partofspeech tagger grammatical category disambiguation statistical optimization mtb memorybased partofspeech tagger generator computational analysis english automatic grammatical tagging english constraint grammar estimating attributes analysis extensions relief linguists guide statistics constraint satisfaction global optimization optimizationbased heuristic maximal constraint satisfaction optimization approach relaxation labelling algorithms learning grammatical structure using statistical decisiontrees building large annotated corpus english penn treebank towards learning constraint grammar annotated corpora using decision trees flexible pos tagger using automatically acquired language model experiments automatic acquisition language model pos tagging using decision trees using decision trees coreference resolution tagging english text probabilistic model comparative experiments disambiguating word senses illustration role bias machine learning corpus linguistic automatic analysis english hybrid environment syntaxsemantic tagging llenguatges sistemes informatics evaluation comparison taggers effect noise testing corpora learning compatibility coefficients relaxation labeling processes using simulated annealing train relaxation labelling processes induction decision trees simple introduction maximum entropy models natural language processing accuracy pixel relaxation labelling models maximum entropy modeling natural language scene labelling relaxation operations adaptive statistical language modeling maximum entropy approach phd thesis inducing constraint grammars comparing linguistic stochastic tagger aggregate mixedorder markov models statistical language processing probabilistic partofspeech tagging using decision trees relaxation methods engineering science relaxation neural learning points convergence divergence journal parallel distributed computing 6 three studies grammarbased surface parsing unrestricted english text developing hybrid np parser understanding line drawings scenes shadows psychology computer vision combining independent knowledge sources word sense disambiguation statisticalheuristic feature selection criterion decision tree induction contributing authors lluis marquez tr ctr wenjie li kamfai wong guihong cao chunfa yuan applying machine learning chinese temporal relation resolution proceedings 42nd annual meeting association computational linguistics p582es july 2126 2004 barcelona spain ferran pla antonio molina improving partofspeech tagging using lexicalized hmms natural language engineering v10 n2 p167189 june 2004