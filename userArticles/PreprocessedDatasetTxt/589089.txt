feasible bfgs interior point algorithm solving convex minimization problems propose bfgs primaldual interior point method minimizing convex function convex set defined equality inequality constraints algorithm generates feasible iterates consists computing approximate solutions optimality conditions perturbed sequence positive parameters mu converging zero prove converges qsuperlinearly fixed mu also show globally convergent analytic center primaldual optimal set mu tends 0 strict complementarity holds b introduction consider problem minimizing smooth convex function convex set defined inequality constraints problem written r function minimize cx 0 means component c must nonnegative solution simplify presentation avoid complicated notation case linear equality constraints present discussed end paper since assume components c concave feasible set problem convex algorithm proposed paper convergence analysis require f c dierentiable least one functions f c 1 c strongly convex reason latter hypothesis clarified since algorithm belongs class interior point ip methods may well suited problems many inequality constraints also ecient number variables remains small medium say fewer 500 updates n n matrices quasinewton qn formula problems variables limited memory bfgs updates 39 used consider issue paper motivation based practical considerations last 15 years much progress realized ip methods solving linear convex minimization problems see monographs 29 10 38 44 23 42 47 49 nonlinear convex problems algorithms assume second derivatives functions used define problem available see 43 35 36 12 38 26 practice received editors september 15 1998 accepted publication revised form january 26 2000 published electronically august 3 2000 httpwwwsiamorgjournalssiopt11134472html des sciences 123 av thomas 87060 limoges cedex france paularmand unilimfr inria rocquencourt bp 105 78153 le chesnay cedex france jeancharlesgilbertinriafr mip ufr mig universite paul sabatier 118 route de narbonne 31062 toulouse cedex 4 france janmipupstlsefr 200 p armand j ch gilbert janj egou ever uncommon find situations requirement cannot satis fied particular large scale engineering problems see 27 example partly motivates study deals estimation parameters three phase flow porous medium despite possible use computational dieren tiation techniques 8 19 3 28 computing time needed evaluate hessians hessianvector products may large ip algorithms using second derivatives may unattractive situation familiar unconstrained optimization case qn tech niques use first derivatives proved ecient even millions variables see 32 20 9 example meteorology fact motivates present paper explore possibility combining ip approach qn techniques ambition remains modest however since confine question whether elegant bfgs theory unconstrained convex optimization 41 6 still valid inequality constraints present applications would desirable qnip algorithm case f c nonlinear necessarily convex postpone dicult subject future research see 21 48 possible approaches provided constraints satisfy qualification assumptions karush kuhntucker kkt optimality conditions problem 11 written see 17 example follows exists vector multipliers r fx gradient f x euclidean scalar product cx matrix whose columns gradients c x diagonal matrix whose diagonal elements components c lagrangian function associated problem 11 defined r n since f convex component c concave fixed 0 convex function r n r f c twice dierentiable gradient hessian respect x given primaldual ip approach rather standard see 24 36 35 11 12 1 26 25 15 7 5 computes iteratively approximate solutions perturbed optimality system sequence parameters 0 converging zero 12 vector ones whose dimension clear context last inequality means components cx must positive perturbing complementarity equation kkt conditions parameter bfgs interior point algorithm 201 combinatorial aspect problem inherent determination active constraints zero multipliers avoided use word inner qualify iterations used find approximate solution 12 fixed outer iteration collection inner iterations corresponding value newton step solving first two equations 12 fixed solution linear system fx cx xx x direction sometimes called primaldual step since obtained linearizing primaldual system 12 primal step newton direction minimizing primal variable x barrier function log c x associated 11 algorithms 16 33 4 spirit two problems related since elimination 12 represents optimality conditions unconstrained barrier problem cx 0 result approximate solution 12 also approximate minimizer barrier problem 14 however algorithms using primaldual direction shown present better numerical eciency see example 46 algorithm solving 12 14 approximately search direction computed solution 13 positive definite symmetric matrix approximating 2 xx x updated bfgs formula see 14 17 material qn techniques eliminating 13 obtain since iterates forced remain strictly feasible ie cx 0 positive definiteness implies x descent direction x therefore force convergence inner iterates possibility could force decrease iteration however since algorithm also generates dual variables prefer add function see 45 1 18 log c x control change function also used 30 31 potential function nonlinear complementarity problems even though map x vx necessarily convex show unique minimizer solution 12 decreases along direction primaldual merit function used force convergence pairs x solution 12 using linesearches shown additional p armand j ch gilbert janj egou function v prevent unit stepsizes accepted asymptotically important point eciency algorithm let us stress fact algorithm standard bfgs algorithm solving barrier problem 14 since hessian lagrangian approximated updated matrix hessian motivated following arguments first dierence 2 involves first derivatives since derivatives considered available need approximated second hessian 2 xx approximated independent become illconditioned goes zero third approximation 2 obtained end outer iteration used starting matrix next outer iteration looks attractive also inconvenience restricting approach strongly convex functions explain computation new iterates stepsize given linesearch matrix updated bfgs formula using two vectors since want new matrix approximation 2 satisfies qn equation property bfgs formula makes sense define formula well defined generates stable positive definite matrices provided vectors satisfy 0 inequality known curvature condition expresses strict monotonicity gradient lagrangian two successive iterates unconstrained optimization always satisfied using wolfe linesearch provided function minimize bounded reasonable assumption unconstrained optimization longer case constraints present since optimization problem may perfectly well defined even unbounded assuming hypothesis boundedness would less restrictive assuming strong convexity satisfactory indeed bounded lagrangian curvature condition satisfied wolfe linesearch unconstrained optimization near solution information 2 collected matrix could come region far optimal point would prevent qsuperlinear convergence erates observation assume f one functions c strongly convex lagrangian becomes strongly convex function x fixed 0 assumption curvature condition satisfied independently kind linesearch techniques actually used algorithm question whether present theory adapted convex problems hence including linear programming puzzling come back issue discussion section large part paper devoted analysis qn algorithm solving perturbed kkt conditions 12 fixed algorithm detailed next section convergence speed analyzed sections 3 4 particular shown fixed 0 primaldual pairs x converge qsuperlinearly toward solution 12 tools used prove convergence essentially bfgs interior point algorithm 203 bfgs theory 6 13 40 section 5 overall algorithm presented shown sequence outer iterates globally convergent sense bounded accumulation points primaldual solutions problem 11 addition strict complementarity holds whole sequence outer iterates converges analytic center primaldual optimal set 2 algorithm solving barrier problem euclidean 2 norm denoted recall function r n r said strongly convex modulus 0 x r n equivalent definitions see example 22 chapter iv minimal assumptions following assumption 21 functions f c 1 convex dierentiable r n r least one functions f c 1 c strongly convex ii set strictly feasible points problem 11 nonempty ie exists x r n cx 0 assumption 21i motivated section 1 assumption 21ii also called strong slater condition necessary wellposedness feasible interior point method convexity assumption equivalent fact set multipliers associated given solution nonempty compact see 22 theorem vii232 example assumptions following clear consequence lemma 22 suppose assumption 21 holds solution set problem 11 nonempty bounded lemma 22 level sets logarithmic barrier function compact fact used frequently consequence 16 lemma 12 recall completeness lemma 23 let f r n r convex continuous function c r n continuous function concave components suppose set x r cx 0 nonempty solution set problem 11 nonempty bounded r 0 set log c x compact possibly empty let x 1 first iterate feasible ip algorithm hence satisfying cx 1 0 define level set lemma 24 suppose assumption 21 holds barrier problem 14 unique solution denoted x proof assumption 21 lemma 22 lemma 23 l p 1 nonempty compact barrier problem 14 least one solution solution also unique since strictly convex x r indeed assumption 21i 2 x given 16 positive definite simplify notation denote z x typical pair primaldual variables z set strictly feasible zs 204 p armand j ch gilbert janj egou algorithm generates sequence pairs z z z positive definite symmetric matrix given pair z next one z obtained follows first stepsize unique solution 13 uniqueness comes positivity cx positive definiteness unicity x use 15 next matrix updated given formula gives symmetric positive definite matrix provided symmetric positive definite 0 see 14 17 latter condition satisfied strong convexity assumption indeed since least one functions f c strongly convex fixed 0 function x x strongly convex exists constant 0 since sizes displacement x merit function used estimate progress solution must depend x follow idea anstreicher vial 1 add function forcing take value cx merit function defined z x z log c x note using merit function reasonable provided problem z z bfgs interior point algorithm 205 unique solution solution 12 direction descent direction check lemmas 25 26 lemma 25 suppose assumption 21 holds problem 24 unique solution z x x unique solution barrier problem ith component defined c x furthermore stationary point z proof optimality unique solution x barrier problem 14 x cx 0 hand since log minimized c x index z z adding preceding two inequalities gives z z z z hence z solution 24 remains show z unique stationary point z stationary satisfies canceling first equality gives fx cxcx thus x unique minimizer convex function second equation system lemma 26 suppose z z symmetric positive definite let solution 13 descent direction point z z meaning z 0 proof z using 15 nonpositive hand satisfies second equation 13 one see 1 also nonpositive formula z given statement lemma follows calculation furthermore z 0 z z state precisely one iteration algorithm used solve perturbed kkt system 12 constants 0 1 0 1 given independently iteration index 206 p armand j ch gilbert janj egou algorithm solving 12 one iteration beginning iteration current iterate supposed available well positive definite matrix approximating hessian lagrangian 2 xx x 1 compute x solution linear system 13 2 compute stepsize means backtracking line search 20 21 test sucient decrease condition 22 25 satisfied choose new trial stepsize go step 21 25 satisfied set z 3 update bfgs formula 21 given 22 lemma 26 descent direction z stepsize 0 satisfying 25 found linesearch implicitly assumed 25 satisfied z holds new iterate z conclude section result gives contribution linesearch convergence sequence generated algorithm spirit similar result given zoutendijk 50 proof see 6 say function c 11 lipschitz continuous first derivatives denote level set determined first iterate z lemma 27 c 11 open convex neighborhood level set l pd positive constant k z l pd determined linesearch step 2 algorithm one following two inequalities holds important mention result holds even though may defined positive stepsizes along linesearch may reduce stepsize first stage enforce feasibility 3 global rlinear convergence algorithm convergence analysis bfgs path qsuperlinear convergence traditionally leads rlinear convergence see 41 6 section show iterates generated algorithm converge z x solution 12 convergence speed use notation first result shows iterates x remain level set l pd sequence cx bounded bounded away zero bfgs interior point algorithm 207 lemma 31 suppose assumption 21 holds level set l pd 1 compact exist positive constants k 1 k 2 1 proof since cx log c x bounded m1 log constant k 1 assumption 21 lemma 23 level set l 1 compact continuity cl also compact cx bounded bounded away zero z l pd 1 proven implies 1 bounded constant k 1 hence components zs l pd bounded bounded away zero shown l pd 1 included compact set compact continuity next proposition crucial technique use prove global convergence see 6 claims proximity point z unique solution 24 measured value z norm gradient z unconstrained optimization corresponding result direct consequence strong convexity necessarily convex result still established using lemma 25 lemma 31 function nonconvex example minimized halfline nonnegative real numbers proposition 32 suppose assumption 21 holds constant 0 z l pdaz z 2 proof let us show strongly convex neighborhood z using 23 fact e hessian z written assumption 21 fixed 0 lagrangian strongly convex function variable x follows hessian respect x positive definite us show matrix also positive definite multiplying matrix sides vector u v r n positive definite cx 0 quantity nonnegative vanishes one deduces next positive definite let us prove local version proposition exist constant 0 open neighborhood n z z z z 2 208 p armand j ch gilbert janj egou inequality left comes fact z strong convexity near z inequality right first use local convexity arbitrary z near z z z z cauchyschwarz inequality inequality left 32 one gets 1simplifying squaring give inequality right 32 extend validity 32 z l pd suces note virtue lemma 25 ratios z z well defined continuous compact set l pd z unique minimizer l pd 25 ratios respectively bounded away zero bounded l pd 1 n positive constants k 1 k 2 conclusion proposition follows taking proof rlinear convergence rests following lemma part theory bfgs updates stated independently present context see byrd nocedal 6 denote k angle k k roundup operator lemma 33 let k positive definite matrices generated bfgs formula using pairs vectors k k k1 satisfying k 1 1 0 2 0 independent k r 0 1 exist positive constants b 1 b 2 b 3 index k 1 least rk indices j 1 k assumptions 33 made k k lemma satisfied context first one due strong convexity one functions f c 1 c fact bounded away zero lemma 31 f c c 11 second one deduced lipschitz inequality boundedness lemma 31 first inequality 33 theorem 34 suppose assumption 21 holds f c c 11 functions algorithm generates sequence z k converging z rlinearly meaning lim sup kz k z 1k 1 particular z proof denote k positive constants independent iteration index also use notation bfgs interior point algorithm 209 bounds cx given lemma 31 fact f c c 11 imply c 11 open convex neighborhood level set l pd 1 example open bounded convex set containing l pd 1 set used c bounded given neighborhood therefore linesearch lemma 27 positive constant k 1 either let us apply lemma 33 fix r 0 1 denote j set indices j 34 holds using lemma 26 bounds lemma 31 one let us denote k 4 positive constant cx k 4 x l pd 1 using 23 15 inequality also 13 x p armand j ch gilbert janj egou combining inequalities 35 36 gives positive constant k j j end proof standard see 41 6 using proposition 32 j j 0 1 hand linesearch z k1 1 lemma 33 1 k j rk rk last inequality gives k 1 k 7 positive constant using inequality left 31 one k 1 rlinear convergence z k follows 4 qsuperlinear convergence algorithm rlinear convergence result previous section ready establish qsuperlinear convergence sequence z k generated algorithm definition z k converges qsuperlinearly z following estimate holds z z means z k1 z z k z 0 assuming z k z get result f c little bit smoother namely twice continuously dierentiable near x use notation start showing unit stepsize accepted asymptotically linesearch condition 25 provided updated matrix k becomes good suciently large sense specified inequality 41 provided iterate z k suciently close solution z given two sequences vectors u k v k normed spaces positive number write u k ov k exists sequence k r k 0 u k k v k k proposition 41 suppose assumption 21 holds f c twice continuously dierentiable near x suppose also sequence z k generated algorithm converges z positive definite matrices k satisfy estimate x k sucient decrease condition 25 satisfied k suciently large provided 1 bfgs interior point algorithm 211 proof observe first positive definiteness 41 implies x positive constant k suciently large k observe also k 0 x use 15 42 therefore k large enough z k z k near z one expand z k second order expansion gives lefthand side 25 want show quantity negative k large first aim show z k smaller term order od k 2 purpose one computes hand using one gets lemma 26 k estimates 41 fact 2 lemma 31 boundedness c k 43 becomes x clear result proven show positive constant k k large z k k kd k 2 show use 212 p armand j ch gilbert janj egou last expression z k k upper bound x k obtained cauchyschwartz inequality x k follows x k therefore using 42 lemma 31 one gets positive constant k k large proposition 41 shows particular function v added get merit function right curvature around z unit stepsize x accepted linesearch following proposition establish necessary sucient condition qsuperlinear convergence dennis 13 type analysis assumes unit stepsize taken updated matrix k suciently good asymptotically manner given estimate 45 slightly dierent 41 proposition 42 suppose assumption 21 holds f c twice dierentiable x suppose sequence z k generated algorithm converges z k suciently large unit stepsize accepted linesearch z k converges qsuperlinearly towards z proof let us denote nonsingular jacobian matrix perturbed kkt conditions 12 solution z x first order expansion righthand side 13 z identities e give subtracting md k sides assuming unit stepsize obtain z bfgs interior point algorithm 213 suppose z k converges qsuperlinearly righthand side 46 order oz k z 45 follows fact qsuperlinear convergence z k z k let us prove converse 45 lefthand side 46 od k due nonsingularity 46 gives z k1 z unit stepsize z z k z finally get z k1 z proving qsuperlinear convergence sequence z k need following result bfgs theory see 40 theorem 3 6 lemma 43 let k sequence matrices generated bfgs formula given symmetric positive definite matrix 1 pairs k k vectors verifying symmetric positive definite matrix sequences k k bounded using lemma see bfgs formula gives estimate note estimate implies 45 qsuperlinear convergence z k follow function twice dierentiable neighborhood point x r n said locally radially lipschitzian hessian x exists positive constant l x near x one theorem 44 suppose assumption 21 holds f c c 11 functions twice continuously dierentiable near x locally radially lipschitzian hessians x suppose linesearch algorithm uses constant 2 sequence z k generated algorithm converges z x qsuperlinearly k suciently large unit stepsize accepted linesearch proof let us start showing lemma 43 applied first k k 0 already discussed lemma 33 convergence series 47 use taylor expansion assuming k large enough f c c 2 near 214 p armand j ch gilbert janj egou local radial lipschitz continuity 2 f 2 c x boundedness k1 exist positive constants k k z hence series 47 converges theorem 34 therefore 48 fact k parallel x estimate 49 proposition 41 unit stepsize accepted k large enough qsuperlinear convergence z k follows proposition 42 5 overall primaldual algorithm section consider overall algorithm solving problem 11 recall lemma 22 set primal solutions problem nonempty bounded slater condition sumption 21ii set dual solutions also nonempty bounded let us denote primaldual solution problem 11 also solution necessary sucient conditions optimality overall algorithm solving 11 51 called algorithm consists computing approximate solutions perturbed optimality conditions 12 sequence converging zero primaldual algorithm used find approximate solution 12 done socalled inner iterations next decreased process solving 12 new value repeated call outer iteration collection inner iterations solving 12 fixed value index outer iterations superscripts j n0 algorithm solving problem 11 one outer iteration beginning jth outer iteration approximation z j z solution z 51 supposed available well positive 1 approximating hessian lagrangian value given well precision threshold j 0 1 starting z j use algorithm z j 2 choose new starting iterate z j1 next outer iteration well positive definite matrix j1 1 set new parameters j1 0 j1 0 j j converge zero j bfgs interior point algorithm 215 start j1th outer iteration possibility take z j1 updated matrix obtained end jth outer iteration far global convergence concerned z determined important therefore point algorithm leaves user much freedom maneuver theorem 51 gives us global convergence result general algorithm theorem 51 suppose assumption 21 holds f c c 11 functions algorithm generates bounded sequence z j limit point z j primaldual solution problem 11 proof theorem 34 outer iteration algorithm terminates iterate z j satisfying stopping criteria step 1 therefore algorithm generates sequence z j since sequences j j converge zero limit point z j solution problem 11 remains show z j bounded let us first prove boundedness x j convexity lagrangian implies using positivity j cx 1 next stopping criteria algorithm follows x j unbounded setting j x j j x j one choose subsequence j lim last inequality deduce moreover since cx j 0 c follows see example 22 proposition iv325 2 formula 1 therefore solution set problem 11 would unbounded contradiction claimed lemma 22 prove boundedness multipliers suppose algorithm generates unbounded sequence positive vectors j subsequence j sequence jj bounded thus least one limit point say dividing two inequalities 52 j taking limits j deduce 0 cx using concavity components c one inequality right follows strict feasibility first iterate multiplying deduce cx 1 contradiction p armand j ch gilbert janj egou rest section give conditions whole sequence converges particular point called analytic center primaldual optimal set actually occurs following two conditions hold strict complementarity proper choice forcing sequence j algorithm satisfy estimate meaning j j let us first recall notion analytic center optimal sets assumption 21 uniquely defined see monteiro zhou 37 related results denote optp optd sets primal dual solutions problem 11 analytic center optp defined follows optp reduced single point analytic center precisely point otherwise optp convex set one point case f strongly convex assumption 21i least one constraint functions c 0 say strongly convex follows index set nonempty contains 0 analytic center optp defined unique solution following problem log c x fact problem well defined unique solution matter lemma 52 similarly optd reduced single point analytic center point case multiple dual solutions index set nonempty otherwise optd would reduced 0 analytic center optd defined unique solution following problem log lemma 52 suppose assumption 21 holds optp resp optd reduced singleton problem 53 resp 54 unique solution proof consider first problem 53 suppose optp singleton seen b nonempty convexity set optp concavity functions c exists therefore feasible set 53 nonempty hand let x 0 point satisfying constraints 53 set log c x ib log c x nonempty bounded lemma 22 closed therefore problem 53 solution finally assumption 21i know index bfgs interior point algorithm 217 c 0 strongly convex follows objective 53 strongly concave problem 53 unique solution similar arguments fact objective function 54 strictly concave follows problem 54 unique solution complementarity ie cx convexity problem 11 index sets b n intersect may indices neither b n said problem 11 strict complementarity property 1 n equivalent existence primaldual solution satisfying strict complementarity theorem 53 suppose assumption 21 holds f c c 11 functions suppose also problem 11 strict complementarity property sequence j algorithm satisfies estimate sequence z j generated algorithm converges point z x 0 analytic center primal optimal set 0 analytic center dual optimal set proof let x arbitrary primaldual solution 11 x minimizes using convexity j stopping criterion 52 inner iterations algorithm one x theorem 51 constant c 1 1 adding corresponding sides two inequalities leads pursue adapting idea used mclinden 34 give properties limit points path x let us define indices c c substituting 55 dividing j give c x c assumptions supposing x 0 0 limit point x j j taking limit preceding estimate provide c x p armand j ch gilbert janj egou necessarily strict complementarity exactly terms lefthand side preceding inequality hence arithmeticgeometric mean inequality c x c x one take inequality c x c x 0 shows x 0 solution 53 0 solution 54 since problems 53 54 unique solutions sequence x j converges x 0 sequence j converges 0 6 discussion way conclusion discuss results obtained paper give remarks raise open questions problems linear constraints algorithm presented convex inequality constraints also used linear constraints present consider problem min fx obtained adding linear constraints problem 11 61 p n matrix p n b r p given range space problem 61 reduced problem 11 using basis null space matrix indeed let x 1 first iterate supposed strictly feasible sense let us denote z n q matrix whose columns form basis null space point satisfying linear constraints 61 written notation problem 61 rewritten problem u r form 11 thanks transformation deduce assumption 21 minimal assumptions algorithm solving problem 62 equivalently problem 61 converge bfgs interior point algorithm 219 assumption 61 realvalued functions f c 1 convex dierentiable ane subspace x b least one functions f c 1 c strongly convex x ii exists x r n assumptions previous results apply particular algorithm converges rlinearly f c also c 11 qsuperlinearly f c also c 11 twice continuously dierentiable near x locally radially lipschitzian hessian similarly conclusions theorem 51 apply f c also c 11 feasible algorithms qn techniques framework qn methods property generate feasible iterates viewed restriction limiting applicability feasible algorithm indeed case problem 62 sometimes dicult find strictly feasible initial iterate matrix update solving problem order q instead order n infeasible algorithm solving problem 61 directly q n qn updates approach reduced hessian lagrangian z 2 z rapidly full hessian 2 feasible algorithm likely converge rapidly strong convexity hypothesis another issue concerns extension present theory convex problems without strong convexity assumption assumption 21i hypothesis class problems consider encompasses linear programming f c ane clear dealing properly linear programs algorithm needs modifications since bfgs formula longer defined course would ineective solve linear programs qn techniques proposed paper desired matrix problems almost linear near solution may encountered technique dealing situation k k interest accept look limit bfgs formula 21 possible update formula could updated matrix satisfies k1 positive semidefinite provided already positive semidefinite fact k1 may singular raises diculties however example search direction x may longer defined see formula 15 matrix cxcx 1 cx singular therefore present theory cannot extended straightforward manner hand strong convexity assumption may viewed important restriction fictive strongly convex constraint always added obvious example fictive constraint x x k constant k large enough constraint inactive solution solution original problem altered new constraint present theory applies better control outer iterations last least global convergence result section 5 independent update rule parameters j practice however choice decreasing values j j essential eciency algorithm would deserve detailed numerical study theoretical viewpoint would highly desirable update rule would allow outer iterates algorithm converge qsuperlinearly along p armand j ch gilbert janj egou lines interesting problem design algorithm barrier parameter would updated every step qsuperlinear convergence iterates extensions would involve dicult issues global convergence result proved paper gives us reasons believe unreasonable tackle open questions acknowledgments would like thank referees valuable com ments one shown us direct argument last part proof proposition 32 one finally chosen give paper referee brought mclindens paper attention led us theorem 53 r convergence infeasible primaldual interiorpoint method convex programming asymptotic analysis penalty barrier methods convex linear programming computational di trust region interior point algorithm linearly constrained optimization trust region method based interior point techniques tool analysis quasinewton methods application unconstrained minimization primaldual algorithm minimizing nonconvex function subject bound linear equality constraints principles techniques algorithmic di interior point approach linear potential reduction method class smooth convex programming problems classical logarithmic barrier function method class smooth convex programming problems numerical methods unconstrained optimization nonlinear equations formulation theory newton interiorpoint method nonlinear programming sequential unconstrained minimization techniques practical methods optimization feasible direction interiorpoint technique nonlinear optimization interior point techniques optimizationcomplementarity method analytic centers solving smooth convex problems practical interiorpoint method convex programming unified approach interior point algorithms linear complementarity problems new continuation method complementarity problems uniform p limiting behavior trajectories generated continuation method monotone complementarity problems limited memory bfgs method large scale optimization projective sumt method convex programming problems analogue moreaus proximation theorem interior point algorithm solving smooth convex programs based newtons method extension karmarkartype algorithms class convex separable programming problems global linear rate convergence existence convergence central path convex programming duality results updating quasinewton matrices limited storage convergence variable metric algorithm global convergence properties variable metric algorithm minimization without exact line searches theory algorithms linear optimizationan interior point approach analytical center interior point methods mathematical programming computational experience primaldual interiorpoint method smooth convex programming pure primal newton barrier step may infeasible superlinear quadratic convergence primaldual interior point methods constrained optimization interior point algorithmstheory analysis integer nonlinear pro gramming tr ctr richard h byrd jorge nocedal richard waltz feasible interior methods using slacks nonlinear optimization computational optimization applications v26 n1 p3561 october paul armand quasinewton penalty barrier method convex minimization problems computational optimization applications v26 n1 p534 october dingguo pu weiwen tian revised dfp algorithm without exact line search journal computational applied mathematics v154 n2 p319339 15 may