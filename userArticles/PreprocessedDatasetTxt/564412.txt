document clustering committees document clustering useful many information retrieval tasks document browsing organization viewing retrieval results generation yahoolike hierarchies documents etc general goal clustering group data elements intragroup similarities high intergroup similarities low present clustering algorithm called cbc clustering committee shown produce higher quality clusters document clustering tasks compared several well known clustering algorithms initially discovers set tight clusters high intragroup similarity called committees well scattered similarity space low intergroup similarity union committees subset elements algorithm proceeds assigning elements similar committee evaluating cluster quality always difficult task present new evaluation methodology based editing distance output clusters manually constructed classes answer key evaluation measure intuitive easier interpret previous evaluation measures b introduction document clustering initially proposed improving precision recall information retrieval systems 18 clustering often slow large corpora indifferent performance 8 document clustering used recently document browsing 3 improve organization viewing retrieval results 6 accelerate nearestneighbor search 1 generate yahoolike hierarchies 12 common characteristics document clustering include large number documents clustered number output clusters may large document large number features eg features may include terms document feature space union features documents even larger paper propose clustering algorithm cbc clustering committee produces higher quality clusters document clustering tasks compared several well known clustering algorithms many clustering algorithms represent cluster centroid members eg kmeans 13 representative element eg kmedoids 10 averaging elements cluster centroid cluster may unduly influenced elements marginally belong cluster elements also belong clusters illustrate point consider task clustering words use contexts words features group together words tend appear similar contexts instance us state names clustered way tend appear following contexts drivers license illegal outlaws sth primary sales tax senator create centroid state names centroid also contain features list b airport archbishop business district fly mayor mayor subway outskirts state names like new york washington also names cities permission make digital hard copies part work personal classroom use granted without fee provided copies made distributed profit commercial advantage copies bear notice full citation first page copy otherwise republish post servers redistribute lists requires prior specific permission andor fee sigir02 august 1115 2002 tampere finland using single representative cluster may problematic individual element idiosyncrasies may shared members cluster cbc constructs centroid cluster averaging feature vectors subset cluster members subset viewed committee determines elements belong cluster carefully choosing committee members features centroid tend typical features target class example system chose following committee members compute centroid state cluster illinois michigan minnesota iowa wisconsin indiana nebraska vermont result centroid contains features like list evaluating clustering results difficult task introduce new methodology based editing distance clustering results manually constructed classes answer argue easier interpret results evaluation measure results previous measures remainder paper organized follows next section review related clustering algorithms commonly used document clustering section 3 describes representational model section 4 present cbc algorithm evaluation methodology experimental results presented sections 5 6 section 7 show example application cbc finally conclude discussion future work 2 related work generally clustering algorithms categorized hierarchical partitional hierarchical agglomerative algorithms clusters constructed iteratively merging similar clusters algorithms differ compute cluster similarity singlelink clustering 16 similarity two clusters similarity similar members completelink clustering 11 uses similarity least similar members averagelink clustering 5 computes similarity average similarity pairs elements across clusters complexity algorithms 2 logn n number elements clustered 7 algorithms inefficient document clustering tasks deal large numbers documents experiments one corpora used small enough 2745 documents allow us compare cbc hierarchical algorithms chameleon hierarchical algorithm employs dynamic modeling improve clustering quality 9 merging two clusters one might consider sum similarities pairs elements across clusters eg averagelink clustering drawback approach existence single pair similar elements might unduly cause merger two clusters alternative considers number pairs elements whose similarity exceeds certain threshold 4 however may cause undesirable mergers large number pairs whose similarities barely exceed threshold chameleon clustering combines two approaches often document clustering employs kmeans clustering since complexity linear n number elements clustered kmeans family partitional clustering algorithms following steps outline basic algorithm generating set k clusters 1 randomly select k elements initial centroids clusters 2 assign element cluster according centroid closest 3 recompute centroid cluster average clusters elements 4 repeat steps 23 iterations centroids change predetermined constant kmeans complexity oktn efficient many document clustering tasks random selection initial centroids resulting clusters vary quality sets initial centroids lead poor convergence rates poor cluster quality bisecting kmeans 17 variation kmeans begins set containing one large cluster consisting every element iteratively picks largest cluster set splits two clusters replaces split clusters splitting cluster consists applying basic kmeans algorithm times k2 keeping split highest average element centroid similarity hybrid clustering algorithms combine hierarchical partitional algorithms attempt high quality hierarchical algorithms efficiency partitional algorithms buckshot 3 addresses problem randomly selecting initial centroids kmeans combining averagelink clustering cutting et al claim clusters comparable quality hierarchical algorithms lower complexity buckshot first applies averagelink random sample n elements generate k clusters uses centroids clusters initial k centroids kmeans clustering complexity buckshot nlogn parameters k usually considered small numbers since dealing large number clusters buckshot kmeans become inefficient practice furthermore buckshot always suitable suppose one wishes cluster 100000 documents 1000 newsgroup topics buckshot could generate 316 initial centroids 3 representation cbc represents elements feature vectors features document terms usually stemmed words occur within value feature statistic term example statistic simply terms frequency tf within document order discount terms low discriminating power tf usually combined terms inverse document frequency idf inverse percentage documents term occurs measure referred tfidf 15 use mutual information 2 element document features terms algorithm element e construct frequency count vector em total number features c ef frequency count feature f occurring element e document clustering e document c ef term frequency f e construct mutual information vector em mi ef mutual information element e feature f defined c c c log c total frequency count features elements compute similarity two elements using cosine coefficient 15 mutual information vectors f e f f e f f e e mi mi mi mi e e sim2 4 algorithm cbc consists three phases phase compute elements topk similar elements experiments used 20 phase ii construct collection tight clusters elements cluster form committee algorithm tries form many committees possible condition newly formed committee similar existing committee condition violated committee simply discarded final phase algorithm element assigned similar cluster 41 phase find topsimilar elements computing complete similarity matrix pairs elements obviously quadratic however one dramatically reduce running time taking advantage fact feature vector sparse indexing features one retrieve set elements given feature compute top similar elements element e first sort mutual information vector mie consider subset features highest mutual information finally compute similarity e elements share feature subset since high mutual information features tend occur many elements need compute fraction possible pairwise combinations 18828 elements phase completes 38 minutes using heuristic similar words share low mutual information features missed algorithm however experiments visible impact cluster quality 42 phase ii find committees second phase clustering algorithm recursively finds tight clusters scattered similarity space recursive step algorithm finds set tight clusters called committees identifies residue elements covered committee say committee covers element elements similarity centroid committee exceeds high similarity threshold algorithm recursively attempts find committees among residue elements output algorithm union committees found recursive step details phase ii presented figure 1 step 1 score reflects preference bigger tighter clusters step 2 gives preference higher quality clusters step 3 cluster kept similarity previously kept clusters fixed threshold experiments set terminates recursion committee found previous step residue elements identified step 5 residues found algorithm terminates otherwise recursively apply algorithm residue elements committee discovered phase defines one final output clusters algorithm 43 phase iii assign elements clusters phase iii every element assigned cluster containing committee similar phase resembles kmeans every element assigned closest centroid input list elements e clustered similarity database phase thresholds 1 2 step 1 element e e cluster top similar elements e using averagelink clustering cluster discovered c compute following score c avgsimc c number elements c avgsimc average similarity elements c store highestscoring cluster list l step 2 sort clusters l descending order scores step 3 let c list committees initially empty cluster c l sorted order compute centroid c averaging frequency vectors elements computing mutual information vector centroid way individual elements cs similarity centroid committee previously added c threshold 1 add c c step 4 c empty done return c step 5 element e e es similarity every committee c threshold 2 add e list residues r step empty done return c otherwise return union c output recursive call phase ii using input except replacing e r output list committees figure 1 phase ii cbc unlike kmeans number clusters fixed centroids change ie element added cluster added committee cluster 5 evaluation methodology many cluster evaluation schemes proposed generally fall two categories comparing cluster outputs manually generated answer keys hereon referred classes embedding clusters application eg information retrieval using evaluation measure one approach considers average entropy clusters measures purity clusters 17 however maximum purity trivially achieved element forms cluster given partitioned set n elements n n 1 2 pairs elements either partition partition implies n n 1 2 decisions another way evaluate clusters compute percentage decisions agreement clusters classes 19 measure sometimes gives unintuitive results suppose answer key consists 20 equally sized classes 1000 elements treating element cluster gets misleadingly high score 95 evaluation document clustering algorithms information retrieval often uses embedded approach 6 suppose cluster documents returned search engine assuming user able pick relevant cluster performance clustering algorithm measured average precision chosen cluster scheme best cluster matters entropy pairwise decision schemes measure specific property clusters however properties directly related applicationlevel goals clustering information retrieval scheme goaloriented however measures quality best cluster propose evaluation methodology strikes balance generality goalorientation like entropy pairwise decision schemes assume answer key defines elements supposed clustered let c set clusters answer key define editing distance distc number operations required transform c allow three editing operations merge two clusters move element one cluster another copy element one cluster another let b baseline clustering element cluster define quality cluster c follows dist dist measure interpreted percentage savings using clustering result construct answer key versus constructing scratch ie baseline make assumption element belongs exactly one cluster transformation procedure follows 1 suppose classes answer key start list empty sets labeled class answer key 2 cluster merge set whose class largest number elements cluster tie broken arbitrarily 3 element set whose class one elements classes move element set belongs 4 element belongs one target class copy element sets corresponding target classes except one already belongs distc number operations performed using transformation rules c figure 2 shows example cluster containing e could merged either set arbitrarily chose second total number operations 5 6 experimental results section describe test data present evaluation system compare cbc clustering algorithms presented section 2 provide detailed analysis kmeans buckshot proceed studying effect different clustering parameters cbc e c e c c e c e c e e c e figure 2 example applying transformation rules three clusters classes answer key b clusters transformed c sets used reconstruct classes rule 1 sets three merge operations step e sets one move operation step 3 f sets one copy operation step 4 61 test data conducted documentclustering experiments two data sets reuters21578 v12 1 20news18828 2 see table 1 reuters corpus selected documents 1 assigned one topics 2 attribute lewissplittest 3 tags 2745 documents 20news18828 data set contains 18828 newsgroup articles partitioned nearly evenly across 20 different newsgroups 62 cluster evaluation clustered data sets using cbc clustering algorithms section 2 applied evaluation methodology previous section table 2 shows results columns editing distance based evaluation measure cbc outperforms kmeans k1000 414 20news data set implementation chameleon unable complete reasonable time 20news corpus cbc spends vast majority time finding top similar documents 38 minutes computing similarity documents committee centroids 119 minutes rest computation includes clustering top20 similar documents every one 18828 documents sorting clusters took less 5 minutes used pentium iii 750mhz processor 1gb memory 63 kmeans buckshot figure 3 figure 4 show cluster quality different ks 20news data set plotted eight iterations kmeans buckshot algorithms respectively cluster quality kmeans clearly increases k reaches 1000 although increase quality slows k60 k1000 buckshot similar performance kmeans reuters corpus however performs much worse 20news corpus kmeans performs well data set k large eg k1000 whereas buckshot cannot k higher reuters corpus best clusters kmeans obtained buckshot k large 52 however k approaches 52 buckshot degenerates kmeans algorithm explains buckshot similar performance kmeans figure 5 compares cluster quality kmeans buckshot different values k 20news data set table 1 number classes test data set number elements largest smallest classes classes class class reuters 2745 92 1045 1 20news table 2 cluster quality several algorithms reuters 20news data sets reuters 20news kmeans 6238 7004 buckshot 6203 6596 bisecting kmeans 6080 5852 chameleon 5867 na averagelink 6300 7043 completelink 4622 6423 figure 3 kmeans cluster quality 20news data set different values k plotted eight iterations0204061 2 3 4 5 6 7 8 itera tions quality figure 4 buckshot cluster quality 20news data set different values k plotted eight iterations0204061 2 3 4 5 6 7 8 itera tions quality buckshot first applies averagelink clustering random sample n elements n number elements clustered sample size counterbalances quadratic running time averagelink make buckshot linear experimented larger sample sizes see buckshot performs better 20news data set clustering 137 elements using averagelink fast afford cluster larger sample figure 6 illustrates results k150 20news data set f indicates forced sample size fsqrt original buckshot algorithm described section 2 since k 137 fsqrt kmeans algorithm always sample least k elements buckshot better performance kmeans long sample size significantly bigger k values f 500 converged two iterations fsqrt took four iterations converge 64 clustering parameters experimented different clustering parameters describe parameter possible values 1 vector space model described section 3 mi mutualinformation model termfrequency model c tfidf1 tfidf model tfidf2 tfidf model using following 2 stemming terms stemmed stemmed using porters stemmer 14 3 stop words w stop words used features terms used 4 filtering filtering performed terms mi05 deleted filtering feature vectors become smaller similarity computations become much faster refer experiment using string first position corresponds stemming parameter second position corresponds stop words parameter third position corresponds filtering parameter example experiment swf means terms stemmed stop words ignored filtering performed vector space model parameter always explicitly given figure 7 illustrates quality clusters generated cbc reuters corpus varying clustering parameters document clustering systems use tfidf1 vector space model however mi model outperforms model including tfidf1 furthermore varying parameters mi model makes significant difference cluster quality making mi robust tf performs worse since terms low discriminating power eg furthermore discounted although tfidf2 slightly outperforms tfidf1 experiment swf clearly robust except tf model stemming terms always produced better quality clusters 7 example collected titles abstracts 46 papers presented sigir2001 clustered using cbc paper used part filename session name presented conference number representing order appears proceedings example cat017 refers paper presented categorization session 17 th paper proceedings results shown table 3 features many automatically generated clusters clearly correspond sigir2001 session topics eg clusters 102061 quality kmeans buckshot figure 5 comparison cluster quality kmeans buckshot different k 20news data set050607 iterations quality fsqrt f500 f1000 f1500 f2000 f2500 f3000 f3500 f4000 figure 6 buckshot cluster quality k150 varying sample size f 20news data set plotted five iterations 4 applying evaluation methodology section 5 gives score 3260 score fairly low following reasons documents could potentially belong one session example lrn037 clustered categorization cluster 1 deals learning text categorization titled metalearning approach text categorization using sessions answer key lrn037 counted incorrect cbc generates clusters correspond session topic example papers cluster 6 news stories application domain papers cluster 5 deal search engines table 3 output cbc applied 46 papers presented sigir2001 left column shows clustered documents right column shows top7 features stemmed terms forming cluster centroids ms035 eval011 ms036 sys006 cat018 threshold score term base distribut optim scheme 3 lm015 lm041 lm016 cl012 cl013 cl014 model languag translat expans estim improv framework user result use imag system search index 5 web030 sys007 ms033 eval010 search engin page web link best 6 sum002 lrn039 lm042 stori new event time process applic content 8 lrn040 sys005 level space framework vector comput recent 9 qa046 qa044 qa045 ms034 answer question perform task passag larg give 11 web032 sum026 web031 link algorithm method hyperlink web analyz identifi catcategorization clcrosslingual evalevaluation lmlanguagemodels lrnlearning msmetasearch qaquestionanswering rmretrievalmodels sumsummarization syssystems ususerstudies webweb figure 7 cbc evaluation cluster quality using different clustering parameters reuters corpus01030507 mi tf tfidf1 tfidf2 vector space mode ls quality 8 conclusion document clustering important tool information retrieval presented clustering algorithm cbc handle large number documents large number output clusters large sparse feature space discovers clusters using well scattered tight clusters called committees experiments document clustering showed cbc outperforms several wellknown hierarchical partitional hybrid clustering algorithms cluster quality example one experiment outperforms kmeans 414 evaluating cluster quality always difficult task presented new evaluation methodology based editing distance output clusters manually constructed classes answer key evaluation measure intuitive easier interpret previous evaluation measures cbc may applied clustering tasks word clustering since many words multiple senses modify phase iii cbc allow element belong multiple clusters element e find similar cluster assign e remove features e shared centroid cluster recursively find es next similar cluster repeat feature removal process continues es similarity similar cluster threshold total mutual information residue features e fraction total mutual information original features polysymous word cbc potentially discover clusters correspond senses preliminary experiments clustering words using trec collection 3gb proprietary collection 2gb grade school readings educational testing service gave following automatically discovered word senses word bass clarinet saxophone cello trombone alliedlyons grand metropolitan united biscuits cadbury schweppes contralto baritone mezzo soprano steinbach gallego felder uribe halibut mackerel sea bass whitefish kohlberg kravis kohlberg bass group american word china russia china soviet union japan earthenware pewter terra cotta porcelain word senses represented four committee members cluster 9 acknowledgements authors wish thank reviewers helpful comments research partly supported natural sciences engineering research council canada grant ogp121338 scholarship pgsb207797 10 r optimization inverted vector searches word association norms scattergather clusterbased approach browsing large document collections rock robust clustering algorithm categorical attributes data mining concepts techniques reexamining cluster hypothesis scattergather retrieval results data clustering review use hierarchical clustering information retrieval chameleon hierarchical clustering algorithm using dynamic modeling clustering means medoids hierarchically classifying documents using words methods classification analysis multivariate observations algorithm suffix stripping introduction modern information retrieval numerical taxonomy principles practice numerical classification comparison document clustering techniques information retrieval clustering instancelevel constraints tr scattergather clusterbased approach browsing large document collections reexamining cluster hypothesis optimization inverted vector searches data clustering data mining information retrieval introduction modern information retrieval chameleon hierarchically classifying documents using words clustering instancelevel constraints ctr dolf trieschnigg wessel kraaij scalable hierarchical topic detection exploring sample based approach proceedings 28th annual international acm sigir conference research development information retrieval august 1519 2005 salvador brazil han eren manavoglu hongyuan zha kostas tsioutsiouliklis c lee giles xiangmin zhang rulebased word clustering document metadata extraction proceedings 2005 acm symposium applied computing march 1317 2005 santa fe new mexico bhushan mandhani sachindra joshi krishna kummamuru matrix density based algorithm hierarchically cocluster documents words proceedings 12th international conference world wide web may 2024 2003 budapest hungary reid swanson andrew gordon comparison alternative parse tree paths labeling semantic roles proceedings colingacl main conference poster sessions p811818 july 1718 2006 sydney australia zhengyu niu donghong ji chewlim tan document clustering based cluster validation proceedings thirteenth acm international conference information knowledge management november 0813 2004 washington dc usa zhengyu niu donghong ji chew lim tan using cluster validation criterion identify optimal feature subset cluster number document clustering information processing management international journal v43 n3 p730739 may 2007 chihping wei chinsheng yang hanwei hsiao tsanghsiang cheng combining preference contentbased approaches improving document clustering effectiveness information processing management international journal v42 n2 p350372 march 2006 sanjuan fidelia ibekwesanjuan text mining without document context information processing management international journal v42 n6 p15321552 december 2006 khaled hammouda mohamed kamel efficient phrasebased document indexing web document clustering ieee transactions knowledge data engineering v16 n10 p12791296 october 2004