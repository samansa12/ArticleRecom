demonstrating scalability molecular dynamics application petaflop computer ibm blue gene project endeavored development cellular architecture computer millions concurrent threads execution one major challenges project demonstrating applications successfully exploit massive amount parallelism starting sequential version well known molecular dynamics code developed new application exploits multiple levels parallelism blue gene cellular architecture perform analytical simulation studies behavior application executed large number threads result demonstrate class applications execute efficiently large cellular machine b introduction several teraflopscale machines deployed various industrial governmental academic sites high performance computing community starting look next big step petaflopscale machines least two different approaches advocated development first generation machines one hand projects like htmt 10 propose use thousands high speed processors hun dreds gigahertz hand projects like ibms blue gene 2 advance idea using large number millions modest speed processors hundreds megahertz two approaches seen extremes wide spectrum choices particularly interested analyzing feasibility latter blue gene style approach massively parallel machines built today relatively straightforward way adopting cellular architecture basic buildingblock containing processors memory interconnect support preferably implemented single silicon chip replicated many times following regular pattern combined logic memory microelectronics processes soon deliver chips hundreds millions transistors several research groups advanced processorinmemory designs rely technol ogy used building blocks cellular machines examples include illinois flexram 5 12 berkeley projects nevertheless sheer size building cellular machine deliver petaflops 10 15 floatingpoint operations per second quite challenge enterprise justified demonstrated applications execute efficiently machine paper report results analytical simulationbased studies behavior computational molecular dynamics application analyze behavior application cellular architecture millions concurrent threads ex ecution design ibms blue gene project one example class applications indeed exploit multiple levels massive parallelism offered petaflopscale cellular machine discussed 11 parallel application derived serial version molecular dynamics code developed university pennsylvania 6 application rewritten new partitioning techniques take advantage multiple levels parallelism developed analytical model performance application compared direct simulation measurements quantitative results molecular dynamics code demonstrate class applications successfully exploit petaflop scale cellular machine terms absolute performance simulations indicate achieve 014 petaflops 014 10 15 floatingpoint operations per second 087 petaops 087 operations per second sustained performance terms speed molecular dynamics computation integrate equations motion typical problem 32000 atoms rate one timestep every 375s problem using sequential version code solved power 3 workstation peak performance 800 mflops 140 seconds per time step thus machine 1250000 times total peak floatingpoint performance uniprocessor achieve speedup 368000 rest paper organized follows section 2 gives brief overview cellular machine considered level necessary understand molecular dynamics application parallelized efficient execution section 3 introduces parallel molecular dynamics algorithm used also presents analytical performance model execution algorithm cellular architecture section 4 describes simulation experimental infrastructure use infrastructure deriving experimental performance results presented together results analytical model section 5 section 6 discusses changes blue gene architecture motivated work finally section 7 presents conclusions 2 petaflop cellular machine interested investigating machine like ibms blue gene would perform class molecular dynamics applica tions fundamental premise architecture blue gene performance obtained exploiting massive amounts parallelism rather fast execution particular thread control premise significant technological architectural impacts first individual processors kept simple facilitate design large scale replication single silicon chip instead spending transistors watts enhance singlethread performance real estate power budgets used add processors architecture also characterized memory centric enough threads execution provided order fully consume memory bandwidth tolerating latency individual loadstore operations single silicon chip consists many replicated units naturally fault toler ant replication approach also improves yield fabrica tion even large chips used thus reducing cost moreover allows machine gracefully degrade continue operate individual units fail building block cellular architecture node node implemented single silicon chip contains memory processing elements interconnection elements node viewed singlechip sharedmemory multiprocessor de sign node contains 16 mb shared memory 256 instruction units instruction unit associated one thread exe cution giving 256 simultaneous threads execution one node executes instructions strictly order group 8 threads shares one data cache one floatingpoint unit data cache configuration blue gene 16 kb 8way set associative call group threads floatingpoint unit cache proces sor simplifications necessary keep instruction units simple resulting large numbers die floatingpoint units 32 node pipelined complete multiply add every cycle 500 mhz clock cycle translates 1 gflops peak performance per floatingpoint unit gflops peak performance per node node also six channels communication direct interconnection six nodes 16bit wide channels operating 500 mhz communication bandwidth 1 gbs per channel direction achieved nodes communicate streaming data channels larger systems built interconnecting multiple nodes regular pattern system design use six communication channels node interconnect threedimensional mesh configuration node connected two neighbors along axis x z nodes faces along edges mesh fewer connections use mesh topology regularity built without additional hardware directly connect communication channels node communication channels neighbors threedimensional mesh nodes build system 32768 nodes since node attains peak computation rate gflops entire system delivers peak computation rate approximately 1 petaflops see figure 1 application running petaflop machine must exploit inter intranode parallelism first application decomposed multiple tasks task assigned particular node discussed previously tasks communicate messages second task decomposed multiple threads thread operating subset problem assigned task threads task interact sharedmemory 3 molecular dynamics algorith goal molecular dynamics algorithm determine state molecular system evolves time given molecular system set n atoms state atom time described mass charge q position x velocity v evolution system governed equation motion subject initial conditions x atom f force acting atom x 0 initial position velocity respectively atom notation represents set positions atoms time equation 1 integrated numerically particular choice time step positions atoms fx j tg time used compute forces f time forces used compute accelerations time velocities accelerations time finally used compute new positions velocities time respectively particular molecular dynamics approach chose system simulated shaped form box replicated indefinitely three dimen sions giving rise periodic system illustrated two dimensions figure 2 r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r figure 2 molecular system simulate periodic structure principle need consider interactions among infinite number atoms threads 500 mhz 32 chips 4 boards 256 racks 32 processors system rack processor 1 petaflop gigaflops board chip figure 1 blue genes cellular architecture hierarchy entire system consists 32768 nodes 1 million processors connected threedimensional mesh 31 molecular dynamics algorithm force applied atom time vector sum pairwise interactions atom atoms system pairwise interactions take several forms described 1 divide two main groups intramolecular forces intermolecular forces intramolecular forces occur adjacent closeby atoms molecule model use three different types intramolecular forces bonds bends torsions intermolecular forces occur pair atoms take two forms lennardjones van der walls forces electrostatic coulombic forces lennardjones forces decay rapidly distance therefore given atom enough consider lennardjones interactions atoms inside sphere centered atom radius sphere called cutoff radius note sphere atom may include atoms neighboring replicated box computing electrostatic forces one take account periodic nature molecular system one commonly used approaches ewald summation 4 method computation electrostatic forces divided two fast converging sums real space part reciprocal space also called kspace part f r real space part f r interactions computed atom atoms inside sphere centered atom lennardjones forces although cutoff radius could dif ferent pairs atoms whose distance within cutoff commonly stored linked list also known verlet list generation list usually performed every several timesteps atoms move slowly frequency generation optimization problem since computational cost amortized several timesteps discuss generation verlet lists paper computation reciprocal space first set k reciprocal space vectors computed k 2 k compute k fourier transform point charge distribution structure factor particular value k n number atoms q x charge position atom respectively k terms also called kfactors paper contribution reciprocal space part electrostatic force acting atom computed summation values k also called kforces paper exact formula function f k important structuring paral lelization precise expression specified 1 note atom need perform summation k 2 k value k depends position atoms system 311 internode parallelization efficient parallel molecular dynamics algorithm requires various forces distributed evenly among compute nodes molecular dynamics application uses extension decomposition intermolecular forces described 8 new parallel formulation kspace decomposition although cellular machine threedimensional mesh nodes view p 3 nodes logical twodimensional mesh c columns r rows particular use c two threedimensional mesh embedding use described later consider twodimensional logical mesh partition set atoms two ways target partition r sets ft0 1g size l jaj r source partition c sets fs0 l jaj c replicate target sets across rows twodimensional every node row contains copy atoms target set ti similarly replicate source sets across columns every node column j contains copy atoms source set sj configuration node nij row column j logical mesh assigned source set sj target set ti shown figure 3 k0 k1 k0 k1 k0 k1 figure 3 force decomposition adopted molecular dynamics applicationeach target set ti replicated along nodes row source set sj reciprocal space vector set kj replicated along nodes column j using decomposition described node nij compute locally communication lennardjones real space electrostatic forces exerted atoms source set sj atoms target set ti previously mentioned interactions atoms closer certain cutoff radius actually computed words let f lj lm lennardjones force atoms l let f r lm real part electrostatic force atoms l node nij computes f lj lm f r distance atoms l less cutoff radius lennardjones electrostatic forces respectively node nij also computes partial vector sum lennardjones real part electrostatic forces act upon atoms target set f lj 8m2s j f lj f r 8m2s j f r compute reciprocal space electrostatic forces set k reciprocal space vectors partitioned c sets g size l jkj c replicate set kj along nodes column j logical mesh also shown figure 3 actual computation performed three phases kfactors phase every node nij computes contributions atoms ti k k 2 kj node nij computes 8l2t computing sum contributions along columns twodimensional mesh constitutes second phase sum computed allreduce operation every node obtains value sum contributions reduction every node nij value k k number items kfactors reduction computed along column j equal size reciprocal space vector set kj third final phase also called kforces phase k values used node compute contribution described equation 4 atoms set ti node nij computes intermolecular forces computed perform reduction forces along rows twodimensional logical mesh nodes nij row obtain value f r f lj result every node row obtains resulting forces atoms ti note number items force vectors reduction computed along row equal size number atoms target set ti computation intramolecular forces bonds bends torsions replicated every column logical mesh note every column entire set atoms formed union sets partition atoms among ti sets adjacent atoms molecule assigned adjacent target sets communication required compute intramolecular forces update positions atoms molecule split adjacent rows twodimensional logical mesh computed forces atoms ti compute new positions velocities finally update positions atoms sj atoms ti performing broadcast operation along columns twodimensional mesh broadcast phase node nij sends positions atoms ti sj nodes column j correspondingly receives positions atoms sj ti sj nodes column interactions atoms close together need evaluated often interactions atoms far apart reason use multitimestep algorithm similar respa 13 14 reduce frequency intermolecular forces computation illustrate concept figure 4 computational time step divided sequence short intermediate long steps precisely configuration computation time step consists ten components four short steps followed intermediate step followed four short steps followed long step short step intramolecular bonds bends forces computed intermediate step intramolecular forces bonds bends torsions computed together lennardjones real space electrostatic forces within short cutoff radius computation intermolecular forces requires reduction along rows finally long step intra intermolecular forces including kspace electrostatic computed long step requires reduction along columns kfactors along rows intermolecular forces within steps computation threads node wait barrier indicated b figure 4 positions atoms received nodes updated threads compute forces required step parallel synchronize update shared vector forces using critical regions threads wait second barrier forces step computed updating positions velocities atoms figure 4 also presents summary performance numbers discussed section 5 focus embedding twodimensional logical mesh threedimensional mesh consider twodimensional mesh 128 rows 256 columns column logical mesh 128 nodes mapped onto one physical plane threedimensional cellular machine shown figure 5 note 8 logical columns embedded one physical plane broadcast positions reduction kfactors reciprocal space column logical mesh interfere broadcasts reductions logical columns use x dimensions threedimensional mesh reduction forces along row twodimensional mesh performed adjacent planes threedimensional physical mesh uses disjoint wires z dimension nodes logical row assigned plane also need perform reduction one wire needed row along x dimension 312 intranode parallelization intranode parallelism molecular dynamics code exploited multithreading simplicity set forces bonds bends torsions lennardjones electrostatic computed node statically partitioned among threads node n force forces certain type computed n thread compute threads node thread computes l n force thread forces movement atoms partitioned way natom atoms target set node thread responsible moving l n atom thread atoms force acts atom target set node computed added force accumulator atom lennardjones real electrostatic force computed acts one target atom bond bend torsion intramolecular forces computed however acts 2 3 4 atoms target set respec tively update force accumulator performed thread thread needs acquire lock accumula tor therefore lennardjones real electrostatic force computed thread acquire one lock bond bend torsion computed thread needs acquire 2 3 4 locks respectively parallelization kspace computations different performed four steps inside node start first computing contribution atom target set kfactors assigned node computation statically scheduled thread performs l n atom thread computations evaluate kfactors sum contribution atom done binary reduction tree third step completes computation kfactors within node preparation reduction across columns finally column reduction performed kforces atom computed computations statically scheduled thread computes l n atom thread forces form 32 performance model molecular dynamics computation based previous discussion molecular dynamics algo rithm section derive analytical performance model behavior algorithm cellular architecture like ibms blue gene section 5 compare results performance model direct measurements simulations also use model evaluate impact certain architectural features let ts execution time short step let execution time intermediate step let l execution time long step total execution time one computational time step shown figure 4 computed number short steps one computational time step decompose time step two parts computation time exposed communication time sil exposed communication time part communication time overlapped computation 321 computation time modeling let n bond n bend total number bonds bends spectively need computed node short step let n thread number computation threads available node let bond bend time takes compute one bond one bend respectively let tmove time needed update atoms position velocity let natom number target atoms assigned node finally let barrier n time takes perform barrier operation n threads time short step expressed p shown equation 12 figure 6 let n torsion number torsions need computed node intermediate step let n ljshort n esshort number lennardjones real part electrostatic forces within short cutoff respectively need computed node intermediate step let lj es time takes compute one lennardjones one real part electrostatic force respectively time intermediate step expressed shown equation 13 figure 6 let n ljlong n eslong number lennardjones real part electrostatic forces within long cutoff respectively need computed node long step let n kfactor number k terms node size k set node let eta n kfactor time takes compute one atoms contribution k terms node let redux n kfactor broadcast positions put adjacent bonds bends move atoms 4 times put adjacent bonds bends move atoms 4 times broadcast positions move atoms move atoms bytes transmitted bonds bends real elect short cutoff lennardjones short cutoff torsions force reduction bends bonds lennardjones long cutoff long cutoff real elect force reduction kspace forces torsions kfactors reduce kfactors 250 compute threads computation instthread computation instthread 19000 long step short step short step repeat done intermediate step figure 4 flowchart molecular dynamics computation complete long time step computational performance molecular dynamics application4128 z threedimensional physical mesh0000000000000000000000000000000011111111111111111111111111111111110000000000000000111111111111111111 2d logical mesh 128 rows x 256 cols vertical broadcast horizontal reduction figure 5 mapping columns logical mesh physical machine left communication patterns vertical broadcast positions horizontal reduction forces right time takes add two sets contributions k terms let kfactor n kfactor time takes finalize computing k terms node finally let kforce time takes compute kforce one atom time long step expressed l shown equation 14 figure 6 322 communication time modeling dominant communication operations molecular dynamics code force reductions along rows kfactor reductions position broadcasts along columns twodimensional logical mesh reductions broadcasts implemented cellular machine organizing nodes within logical row logical column according spanning binary tree shown figure 7 figures root tree marked black circle referring figures 5 7 b note 128node column logical mesh consists four adjacent strands 32nodes physical mesh referring figures 5 7 c note 256node row logical mesh consists eight strands 32nodes physical mesh strands 4 nodes apart communications atom positions evaluate intramolecular forces make small contribution total communication always performed neighboring nodes along columns logical mesh want compute time four communication operations operate vectors elements let positions time broadcast positions along columns kfactors redux time reduce kfactors along columns forces time reduce forces along rows put time put new position neighboring row time operations decomposed latency time transfer time latency time time complete operation first element vector transfer time rate element processed times thread thread natom thread thread thread natom thread tmove thread thread thread thread thread natom thread tmove thread n ljlong thread n eslong thread natom thread thread natom thread kforce n kfactor figure analytical expressions computation time short intermediate long steps r nodes r nodes 4 nodes r nodes r nodes 8 nodes column fanin tree b column fanout tree c row fanin tree row fanout tree figure 7 fanin fanout trees reduction broadcast operations along columns b reduction operations along rows c number elements vector equation form latency positions kfactors latency kfactors redux latency latency n source n kfactor number source atoms kfactors assigned column respectively n target number target atoms assigned row n put number puts neighbors node perform triplet complex time transfer one triplet force position one complex number kfactor respectively position force bytes long three doubleprecision floatingpoint numbers kfactor long two doubleprecision floatingpoint hop c number hops nodes node column root fanin fanout trees intracolumn operations let hop time go hop node cellular machine interconnect latency positions hop c hop 19 factor two accounts round trip let n add c number additions need performed reduce item originating node column gets root fanin tree let add time add one item latency kfactors hop c hop add c add 20 analysis force reduction along rows similar let hop r number hops node row root fanin fanout trees intrarow operation let n add r number additions need performed reduce item originating node row gets root fanin tree latency hop r hop add r add 21 derive worst case estimates exposed communications times si case overlap computation communication upper bound exposed communication time factor 8 comes 8 short steps within computational time step 4 simulation environment complement analytical modeling described executed molecular dynamics application instructionlevel simulator blue gene machine proprietary instruction set typical risc loadstore architecture three register instructions model application coded c compiled gcc version 2952 compiler properly modified generate code instruction set thread creation management inside node performed application level using calls pthreadcompatible library internode communication accomplished proprietary communication library implements put onesided communication broadcast reduce operations node machine runs resident system kernel executes supervisor privileges purpose kernel twofold first protect machine resources threads memory communication channels accidental corruption misbehaving application program resources used reliably error detection debugging performance monitoring second isolate application libraries details underlying hardware communications protocols minimize impact evolutionary changes areas actual application runs user privileges invokes kernel services inputoutput internode communication instructionlevel simulator architecturally accurate executing kernel application code models features node communication nodes instance pro cess simulator models one node multiple instances used simulate system multiple nodes internally multiple simulator instances communicate mpi result executing application instructionlevel simulator produces detailed traces instructions executed also produces histograms instruction mix one trace one histogram produced every node simulated instructionlevel simulator timing information therefore directly produce performance estimates instead use traces produced simulator feed two performance tools one tools cache simulator event visualizer provides measurements cache behavior tool trace analyzer detects dependences conflicts instruction trace producing estimate actual machine cycles necessary execute code trace analyzer execute instructions models resource usage instructions trace analyzer instruction predefined execution latency instructions compete shared resources threads node execute instructions program order thus resources instruction available thread tries issue instruction issue delayed thread stalls resources become available memory operations trace analyzer uses probabilistic cache model 90 hit rate value validated running trace cache simulator performance parameters simulated architecture shown table 1 parameters early estimates may change lowlevel logic design completed table interpreted follows delay instruction decomposed two parts execution cycles latency cycles execution unit kept busy number execution cycles issue another instruction result available number executionlatency cycles resources utilized instructions latency period example floating point add execution 1 cycle latency 5 cycles means fpu busy 1 cycle executing instruction instructions may already pipeline next cycle execute another instruction however result addition available 6 15 cycles another example integer division takes 33 execution cycles 0 latency cycles means occupies exclusively integer unit 33 cycles result available immediately execution period completes threads issue instruction every cycle unless stall dependence result previous instruction memory operations latency operations depend deep memory hierarchy go fetch result memory distributed within chip accessing local memory faster accessing memory across chip memory shared threads within chip address others local memories latency operation depends physical location memory memory accesses determined local global based effective address however blue gene processors use scrambling global access may local memory model scrambling probabilis tically 1p probability global access local memory p number processors chip configuration simulating petaflop machine easy task straightforward simulation entire machine would require 32768 simulation processes one simulated node however molecular dynamics application structure allows us simulate completely one node entire machine extrapolate performance results machine presented previously molecular dynamics code runs twodimensional logical mesh nodes node simulates atomic interactions two sets atoms target set source set particular decomposition method used communication nodes occurs intrarow intracolumn thus simulation one row provides information behavior rows logical twodimensional mesh similarly simulation one column provides information behavior columns logical twodimensional mesh reduces number nodes need simulate 383 256 one row j plus 128 one column minus one instance node j illustrated figure 8 still need provide correct values incoming messages boundary subsystem simulate modified communication layer fake communication except nodes row j column balanced work distribution node j performs set operations similar performed nodes system therefore performance node j extrapolated table 1 estimates instruction performance parameters blue gene isa type instruction characterized number cycles keeps execution unit busy column execution latency complete column latency instruction type execution latency branches integer multiply divide 33 0 floating point add multiply conversions 1 5 floating point divide square root 54 0 floating point prefetching memory operation cache hit 1 2 memory operation shared local 1 20 memory operation shared remote 1 27 operations 1 0 obtain performance entire system use instruction level simulator simulate nodes column row j collect analyze trace information node j011001111 j nodes256 nodes chips simulate information detailed figure 8 strategy performance estimation simulating one row column logical mesh 5 experimental results test case code assembled molecular system human carbonic anhydrase hca enzyme 9 plays important role disease glaucoma hca enzyme solvated 9000 water molecules total 32000 atoms system used molecular dynamics code running simulator described compute evolution system starting experimental coordinates taken nmr structure carbonic anhydrase described 9 molecular system prepared taking crystallographic configuration hca enzyme protein data bank identification label 1am6 httpwwwrcsborgpdb solvating box water size 70 charmm22 3 force field used treat interactions atoms proteinwater system newtonian dynamics generated standard conditions room temperature 1 atm pressure use standard verlet algorithm solver 15 table 2 lists values various parameters used analytical performance model section 3 table shows number forces computed per computational time step serial version code also shows number forces computed single node participating parallel execution 256 128 mesh scaling factor column shows ratio two values table 3 lists number instructions required force computation forces require use locks discussed section 312 number instructions acquire release locks shown table 3 number instructions treebased multithreaded barrier function number threads participating barrier shown figure 9 data tables 2 3 figure 9 provide information needed estimating performance analytical performance model section 3 table 4a summarizes values various primitive communications parameters obtained intrinsic characteristics architecture application table 4b contains resulting values derived parameters obtained equations 30050150250number computation threads instruction cycles barrier time figure 9 cost barrier function number threads participating table 5 summarizes total instruction counts per node various thread configurations obtained architectural simulator configurations differ number threads allocated perform computations number floatingpoint units allocated computation l n threadm additional threads included numbers allocated perform communication system services table lists total number loads stores branches integer instructions logical instructions system calls floatingpoint instructions executed threads node case floatingpoint instructions detail number multiplyadd fmad multiply table 2 number forces computed node one time step simulation hca protein 14 long real cutoff 7 short cutoff show results single node execution 256 columns 128 rows logical mesh single node one node 256 128 mesh items items scaling factor target atoms 31747 249 128 electrostatic forces real part long cutoff 36157720 1297 27878 lennardjones forces long cutoff 36241834 1313 27602 electrostatic forces real part short cutoff 4430701 127 34887 lennardjones forces short cutoff 4440206 127 34962 kfactors 8139 23 354 sines cosines kspace 12652290 494 25612 bonds 22592 259 87 bends 16753 456 37 torsions 11835 748 table 3 measured parameters computation various forces parameter instructions description force locks total tmove 50 50 compute new position velocity atom computation bond force 2 atoms bend 250 90 340 computation bend force 3 atoms computation torsion force 4 atoms computation lennardjones force computation realpart electrostatic force eta 2000 2000 computation one atom contribution kfactor redux 500 500 computation one reduction step kfactor kfactor 1000 1000 final stage computing kfactors kforce 3000 3000 computation fourierspace electrostatic force subtract instructions instructions performs two floatingpoint operations total number floatingpoint operations shown row flops total number instructions row total note expected number floatingpoint instructions change significantly number threads hand number loads branches increase significantly number threads threads spend time waiting barriers locks table 6 identifies major sources overhead note additional instructions executed multithreaded belong two operations barrier lock additional lock instructions represent contention locks increase larger number threads increase number instructions spent barriers two sources first number threads increases total time barrier increases illustrated figure 9 addition larger number threads typically makes load balancing difficult faster threads wait time barrier slower threads total overhead row table 6 equal difference number instructions multithreaded singlethreaded execution effect overall performance achieved barrier overhead andor lock overhead avoided discussed section 6 table 7 summarizes additional performance results configurations different numbers computational threads tested configuration list number instructionsthread short step intermediate step long step see figure 4 show number instructionsthread kfactor k force components long step also show total number instructionsthread computation cycles per time step determined architectural simulator trace analyzer cpi clocks per instruction computed ratio two last numbers cpf clocks per floatingpoint instruction measure average number clocks per floatingpoint instruction perspective floatingpoint units compute cycles dn thread 8e n float total number floatingpoint instructions n cycles number computation cycles dn thread 8e number floatingpoint units utilized 8 threads share one floatingpoint unit number machine cycles internode communication table 7 obtained equation 25 independent number threads total number cycles per time step obtained adding computation communication cycles compute multithreaded speedup ratio total cycles single multithreaded execution finally efficiency computed ratio speedup relative singlethreaded exe cution number threads low cpihigh cpf numbers one thread indicates good thread unit utilization low floatingpoint unit utilization configurations eight active threads per floatingpoint unit perform roughly two times work per floatingpoint unit single thread table 4 summary communication cost parameters primitive parameters parameter value description source 125 number source atoms assigned column target 249 number target atoms assigned row kfactor 23 number kfactors assigned column number puts nearest neighbor machine cycles transfer 24 bytes interconnect machine cycles transfer 16 bytes interconnect hop 6 machine cycles cross one node cellular interconnect machine cycles complete one floatingpoint addition hop c 20 maximum number hops inside column add c 28 maximum number adds inside column hop r 48 maximum number hops inside row add r 38 maximum number adds inside row b derived parameters parameter machine cycles description latency positions 240 latency broadcast first position latency kfactors redux 464 latency reduce first kfactor latency forces 880 latency reduce first force latency put latency memory nearest neighbor positions 1740 total time broadcast positions kfactors redux 648 total time reduce kfactors forces 3838 total time reduce forces put 186 total time put positions nearest neighbor l 6256 communication time long step 186 communication time short step upper bound exposed communication time per time step curves figures 10 11 indicate number instruc tionsthread derived analytical performance model execution different components molecular dynamics ap plication markers figures indicate measurements made simulator values plotted accumulated value component added previous components therefore top curve figure 10 also represents total number instruction cycles computational time step top curve figure 11 represents total number instruction cycle kspace computation figure 10 shows contribution short intermediate reciprocal space kspace finally long range real part electrostatic lennardjones forces total computational time step figure 11 shows contribution kfactor kforce components total kspace computation good fit analysis simulation indicating analytical models indeed capture behavior application time charts bottom figure 4 summarize computational performance molecular dynamics application first two time lines show number instructionsthread 250thread execution entire computational time step takes approximately 39000 instructionsthread short step takes 1700 instructions intermediate step takes 5000 instructions long step takes 19000 instructions next time line summarizes communication behavior obtained simulator showing total number bytes transmitted simulated node along axes physical threedimensional mesh note much data transmitted along x z axes along axis result particular embedding twodimensional logical mesh threedimensional physical mesh bottom line shows total number cycles estimated time one iteration step results cache simulator shown figure 12 plot average cache miss rate different cache sizes set associativity values configuration 250 compute threads results show probabilistic model 90 data cache hit rate valid 4 8way setassociative caches size 8 kb associativity 16 kb larger cache results section indicate molecular dynamic simulation 32000 atoms run full petaflop cellular machine good performance possible exploit 32768 nodes 250 threads run full computational time step figure 4 375 187000 cycles 2 nscycle code runs 087 petaops 014 petaflops however state results presented approximate simulator detailed cycle level simulator feasible slow performance simulator lack detailed logic design machine rather trace analyzer uses estimated information depth various pipelines uses queuing model congestion key shared resources expect changes software algorithms mathematical table 5 instruction counts sample node j instruction class 1 thread 50 threads 100 threads 150 threads 180 threads 200 threads 250 threads loads 932617 1380652 2013751 2367753 3000583 3253284 3277406 stores 314545 350290 386740 423190 445060 459640 495778 branches 392998 810678 1412827 1735879 2350139 2590460 2584124 integer ops 779227 815525 852525 889452 911562 926302 962535 logical ops 910489 1006189 1080147 1164083 1208168 1247712 1334017 system 46835 51686 56636 61586 64556 66536 71486 floatingpoint ops 1206253 1211545 1216945 1222345 1225585 1227745 1232891 fmad 288217 288805 289405 290005 290365 290605 291169 flops 1537291 1543171 1549171 1555171 1558771 1561171 1566881 total 4582964 5626565 7019571 7864288 9205653 9771679 9958237 instructions 26 22 17 15 13 13 12 table instructions sample node j function 1 thread 50 threads 100 threads 150 threads 180 threads 200 threads 250 threads barrier lock 52874 190280 274926 347642 363564 391848 446194 barrierlock 52874 715610 1755842 2251036 3391990 3800344 3657628 total overhead 0 1043601 2436607 3281324 4622689 5188715 5375273 50 100 150 200 250 3001030507090110number computation threads instruction cycles thousands node performance 32000 atom problem 256x128 mesh short intermediate kspace long total figure 10 performance components molecular dynamics code solid lines represent values analytical model markers measurements simulation methods significantly improve performance code simulated hand also expect many surprises challenges proceed simulations actual system 6 architectural impact since completed work described paper blue gene architecture evolved changes motivated silicon real estate constraints example number threads per floatingpoint unit reduced 8 4 total 128 threadsnode memory also reduced 16 mb 8 mbnode important changes however motivated 100 150 200 250 30039152127number computation threads instruction cycles thousands node performance 32000 atom problem 256x128 mesh kfactor kforce kspace total figure 11 performance components kspace computa tion solid lines represent values analytical model markers measurements simulation results performance evaluation work discuss changes table 3 figure 9 show direct cost locks barriers respectively evaluate impact final performance molecular dynamics code measured would performance code free locks barriers ob viously code would execute correctly results shown figure 13 shows large number threads 250 instruction count could reduced 10000 25 locks barriers result blue gene architecture includes fast hardware barriers atomic mem table 7 instruction cycle counts one compute thread sample node j cpi average number machine cycles per instruction cpf number machine cycles per floatingpoint instruction thread 50 threads 100 threads 150 threads 180 threads 200 threads 250 threads short intermediate 656383 15191 9226 6533 6409 5683 4703 long 2460789 56954 34867 26061 25221 23713 18791 kfactor 332654 11704 9368 9002 8625 8608 7826 kforce 661631 14024 8491 5738 5738 5738 2987 instructionsthread 4582740 111975 69585 51790 50502 48228 39162 computation cycles 10847196 cpi 237 541 499 480 453 444 444 internode communication cycles 13352 13352 13352 13352 13352 13352 13352 total cycles 10860548 620882 360149 262354 242020 227505 187248 efficiency 100 035 cache size bytes miss rate cache miss rate x cache size associativity 32 bytesline figure 12 data cache simulation results traces collected executing molecular dynamics code ory updates hardware barriers allow synchronization threads node less 10 machine cycles another architectural enhancement motivated work bswitch shown table 7 overall impact communication cycles small less 10 total however achieve goal necessary perform reductions broadcasts minimal software overhead motivated development bswitch microprogrammed data streaming switch operates double word 64bit level cycle b switch route data input combination outputs also perform floatingpoint operations streams direct streams data tofrom memory node diagram bswitch shown figure 14 7 conclusions estimated execution molecular dynamic code system 32000 atoms full petaflop cellular system 50 100 150 200 250 3001030507090110number computation threads instruction cycles thousands node performance 32000 atom problem 256x128 mesh locks barriers locks barriers locks barriers locks barriers figure 13 impact barriers locks performance molecular dynamics code different lines represent different configurations locks barriers scale envisioned ibms blue gene project sequential version application executed 140 stime step 800 mflops workstation parallel version executed 375stime step petaflop machine corresponds parallel speedup 368000 machine 1250000 times faster efficiency 30 result agreement estimates derived 11 exercise demonstrates class molecular dynamics applications exhibits enough parallelism exploit millions concurrent threads execution reasonable efficiency demon strates broad lines validity massively parallel cellular system design one approach achieving one petaflop computing power also provides us clear understanding representative molecular dynamic application code accurate performance model still much work refine improve results fpu input fifo 4 input fifo 5 output fifo 0 output fifo 1 output fifo 2 output fifo 3 output fifo 4 output fifo 5 memory fifo memory fifo control input fifo 0 input fifo 1 input fifo 2 input fifo 3 register file bswitch figure 14 bswitch device streaming data input ports memory output ports memory bswitch incorporates fpu perform arithmetic logic operations directly data stream presented simulators upgraded represent detailed hardware design performance models need upgraded validated cycle faithful simulations communication analysis needs reflect overlap computation communication node failures machine scale addressed algorithms methods continue improved 8 r computer simulation liquids blue gene project program macromolecular energy minimization die berechnung optischer und elektrostatischer gitterpotentiale flexram toward advanced intelligent memory system cm3d cmm md code case intelligent ram iram fast parallel algorithms shortrange molecular dynamics novel binding mode hydroxamate inhibitors human carbonic anhydrase ii hybrid technology multithreaded parallel molecular dynamics implications massively parallel machines toward costeffective dsm organization exploits processormemory integration molecular dynamics systems multiple time scales reversible multiple time scale molecular dynamics computer experiments classical fluids tr computer simulation liquids fast parallel algorithms shortrange molecular dynamics parallel molecular dynamics ctr jeffrey vetter frank mueller communication characteristics largescale scientific applications contemporary cluster architectures journal parallel distributed computing v63 n9 p853865 september jeffrey vetter dynamic statistical profiling communication activity distributed applications acm sigmetrics performance evaluation review v30 n1 june 2002 chingtien ho larry stockmeyer new approach faulttolerant wormhole routing meshconnected parallel computers ieee transactions computers v53 n4 p427439 april 2004 george almsi clin cacaval jos g castaos monty denneau derek lieber jos e moreira henry warren jr dissecting cyclops detailed analysis multithreaded architecture acm sigarch computer architecture news v31 n1 march kumar c huang g zheng e bohm bhatele j c phillips h yu l v kal scalable molecular dynamics namd ibm blue genel system ibm journal research development v52 n1 p177188 january 2008