effects communication parameters end performance shared virtual memory clusters recently lot effort providing costeffective shared memory systems employing software solutions clusters highend workstations coupled highbandwidth lowlatency commodity networks much work far focused improving protocols work restructuring applications perform better svm systems result progress promise good performance range applications least 1632 processor range new system area networks network interfaces provide significantly lower overhead lower latency higher bandwidth communication clusters inexpensive smps become common nodes clusters svm protocols quite mature progress useful examine important system bottlenecks stand way effective parallel performance particular parameters communication architecture important improve relative processor speed ones already adequate modern systems applications change technology future information assist system designers determining focus energies improving performance users determining system characteristics appropriate applicationswe find important system cost improve overhead generating delivering interrupts improving network interface io bus bandwidth relative processor speed helps bandwidthbound applications currently available ratios bandwidth processor speed already adequate many others surprisingly neither processor overhead handling messages occupancy communication interface preparing pushing packets network appear require much improvement b introduction success hardware cachecoherent distributed shared memory dsm lot effort made support programming model coherent shared address space using commodity oriented communication architectures addition commodity nodes techniques communication architecture range using less customized integrated controllers 17 2 supporting shared virtual memory svm page level operating system 12 8 techniques reduce cost unfortunately usually lower performance well great deal research effort made improve systems large classes applications focus paper svm systems last years much improvement svm protocols systems several applications restructured improve performance 12 8 25 14 progress interesting examine important system bottlenecks stand way effective parallel performance particular parameters communication architecture important improve relative processor speed already adequate modern systems applications change technology future studies hopefully assist system designers determining focus energies improving performance users determining system characteristics appropriate applications paper examines questions detailed architectural simulation using applications widely different behavior simulate cluster architecture smp nodes fast system area interconnect programmable network interface ie myrinet use homebased svm protocol demonstrated comparable better performance families svm protocols base case protocol called homebased lazy release consistency hlrc require additional hardware support later examine variant called automatic update release consistency aurc uses automatic hardware propagation writes remote nodes perform updates shared data also extend analysis use uniprocessor nodes useful major performance parameters consider host processor overhead send message network interface occupancy prepare transfer packet nodetonetwork bandwidth often limited io bus bandwidth interrupt cost consider network link latency since small usually constant part endtoend latency system area networks san dealing performance parameters also briefly examine impact key granularity parameters communication architecture page size granularity coherence number processors per node assuming realistic system quite easily implemented today range applications well optimized svm systems 10 see figure 1 applications protocol communication overheads substantial speedups obtained realistic implementation much lower ideal case communication costs zero motivates current research whose goal twofold first want understand performance changes fft lu ocean contiguous water nsquared water spatial radix volrend raytrace barnes rebuild barnes space26101418speedup figure 1 ideal realistic speedups application ideal speedup computed ratio uniprocessor execution time divided sum compute local cache stall time parallel execution ie ignoring communication synchronization costs realistic speedup corresponds realistic set values see section communication architecture parameters today configuration four processors per node parameters communication architecture varied relative processor speed see invest systems energy understand likely evolution system performance technology evolves use wide range values parameter second focus three specific points parameter space first point system generally achieves best performance within ranges parameter values examine performance application point called best performance second point aggressive set values communication parameters current nearfuture systems especially certain operating system features well optimized performance point space called achievable performance third point ideal point represents hypothetical system incurs communication synchronization overheads taking consideration compute time stall time local data accesses goal understand gaps achievable best ideal performance identifying parameters contribute performance differences leads us primary communication parameters need improved close gaps find somewhat surprisingly host overhead send messages perpacket network interface occupancy critical application performance cases interrupt cost far dominant performance bottleneck even though protocol designed aggressive reducing occurrence interrupts nodetonetwork bandwidth typically limited io bus also significant applications interrupt cost important applications study results suggest system designers focus reducing interrupt costs support svm well svm protocol designers try avoid interrupts possible perhaps using polling using programmable communication assist run part protocol avoiding need interrupt main processor results show relative effect parameters ie relative processor speed f f f processor e r processor io first level cache buffer write second level cache network interface e r snooping device figure 2 simulated node architecture one another absolute parameter values used achievable set match consider achievable aggressive current nearfuture systems viewing parameters relative processor speed allows us understand behavior technology trends evolve instance ratio bandwidth processor speed changes use results reason system performance section 2 presents architectural simulator use work section 3 discuss parameters use methodology study section 4 presents applications suite sections 5 6 present results communication performance parameters section 5 examine effects parameter system performance section 6 discuss application parameters limit performance section 7 presents effects page size degree clustering system performance discuss related work section 8 finally discuss future work directions conclusions sections 9 10 respectively simulation environment simulation environment use built top augmint 18 execution driven simulator using x86 instruction set runs x86 systems section present architectural parameters vary simulated architecture figure 2 assumes cluster cprocessor smps connected commodity interconnect like myrinet 3 contention modeled levels except network links switches processor p6like instruction set assumed 1 ipc pro cessor data cache hierarchy consists 8 kbytes firstlevel direct mapped writethrough cache 512 kbytes secondlevel twoway set associative cache line size 32 bytes buffer 19 26 entries 1 cache line wide retireat4 policy write buffer stalls simulated read hit cost one cycle satisfied write buffer first level cache 10 cycles satisfied secondlevel cache memory subsystem fully pipelined network interface ni two 1 mbyte memory queues hold incoming outgoing packets size queues constitute bottleneck communication subsystem network queues fill ni interrupts main processor delays allow queues drain network links operate processor speed 16 bits wide assume fast messaging system 5 16 4 basic communication library memory bus splittransaction 64 bits wide clock cycle four times slower processor clock arbitration takes one bus cycle priorities decreasing order outgoing network path ni second level cache write buffer memory incoming path ni io bus bits wide relative bus bandwidth processor speed match modern systems assume processor 200 mhz clock memory bus 400 mbytess protocol handlers cost variable number cycles code protocol handlers simulated since simulator multithreaded use handler estimate cost code sequence cost access tlb handler running kernel 50 processor cycles cost creating applying diff 10 cycles every word needs compared 10 additional cycles word actually included diff protocols use two versions homebased protocol hlrc aurc 8 25 protocols either use hardware support automatic write propagation aurc traditional software diffs hlrc propagate updates home node page release point necessary pages invalidated acquire points according lazy release consistency lrc subsequent page fault whole page fetched home guaranteed date according lazy release consistency 8 protocol smp nodes attempts utilize hardware sharing synchronization within smp much possible reducing software involvement 1 optimizations used include use hierarchical barriers avoidance interrupts much possible interrupts used remote requests pages locks arrive node requests synchronous rpc like avoid interrupts replies arrive requesting node barriers implemented synchronous messages interrupts interrupts delivered processor 0 node complicated schemes ie round robin random assignment result better load balance interrupt handling used operating system provides necessary support schemes however may increase cost delivering interrupts paper also examine round robin scheme mentioned earlier focus following performance parameters communication archi tecture host overhead io bus bandwidth network interface occupancy interrupt cost examine network link latency since small usually constant part endtoend latency system area networks san parameters describe basic features communication sub system rest parameters system example cache memory configuration total number processors etc remain constant message exchanged two hosts put post queue network interface asynchronous send operation assume sender free continue useful work network interface processes request prepares packets queues outgoing network queue incurring occupancy per packet transmission packet enters incoming network queue receiver processed network interface deposited directly host memory without causing interrupt 2 4 thus interrupt cost overhead related much data transfer processing requests examine range values parameter varying parameter usually keep others fixed set achievable values recall values might consider achievable currently systems provide optimized operating system support interrupts choose relatively aggressive fixed values effects parameter varied observed detail host overhead time host processor busy sending message range parameter cycles post send systems support asynchronous sends time needed transfer message data host memory network interface synchronous sends used asynchronous sends available achievable value host overhead hundred processor cycles recall processor overhead data transfer destination end range values consider 0 almost cycles 10000 processor cycles 50s 5ns processor clock systems support asynchronous sends probably closer smaller values systems synchronous sends closer higher values depending message size achievable value use overhead 600 processor cycles per message ffl io bus bandwidth determines host network bandwidth relative processor speed contemporary systems limiting hardware component available nodetonetwork network links memory buses tend much faster range values io bus bandwidth 025 mbytes per processor clock mhz 2 mbytes per processor clock mhz 50 mbytess 400 mbytess assuming 200 mhz processor clock achievable value 05 mbytesmhz 100 mbytess assuming 200 mhz processor clock ffl network interface occupancy time spent network interface preparing packet network interfaces employ either custom state machines network processors general purpose custom designs perform processing thus processing costs network interface vary widely vary occupancy network interface almost 0 10000 processor cycles 50s 5ns processor clock per packet achievable value use 1000 main processor cycles 5s assuming 200 mhz processor clock value realistic currently available programmable nis given programmable communication assist ni usually much slower main processor ffl interrupt cost cost issue interrupt two processors smp node cost interrupt processor network interface includes cost context switches operating system processing although interrupt cost parameter communication subsystem important aspect svm systems interrupt cost depends operating system used vary greatly system system affecting performance portability svm across different platforms therefore vary interrupt cost free interrupts 0 processor cycles 50000 processor cycles issuing delivering interrupt total 100000 processor cycles 500 5ns processor clock achievable value use 500 processor cycles results cost 1000 cycles null interrupt choice significantly aggressive current operating systems provide however achievable fast interrupt technology 21 use achievable value varying parameters ensure interrupt cost swamp effects varying parameters capture effects parameter separately keep parameters fixed achievable values necessary also perform additional guided simulations clarify results addition results obtained varying parameters results obtained achievable parameter values interesting result speedup obtained using best value range parameter limits performance obtained improving communication architecture within range parameters parameter values best configuration host overhead 0 processor cycles io bus bandwidth equal memory bus bandwidth network interface occupancy per packet 200 processor cycles total interrupt cost 0 processor cycles best con figuration contention still modeled since values system parameters still nonzero table 1 summarizes values parameter 200 mhz processor achievable set values discussed assumes parameter values host overhead 600 processor cycles memory bus bandwidth 400 mbytess io bus bandwidth 100 mbytess network interface occupancy per packet 1000 processor cycles total interrupt cost 1000 processor cycles parameter range achievable best host overhead cycles 010000 600 0 io bus bandwidth mbytesmhz 0252 05 2 ni occupancy cycles 010000 1000 200 table 1 ranges achievable best values communication parameters consideration applications use splash2 22 application suite section briefly describes basic characteristics application relevant study detailed classification description application behavior svm systems uniprocessor nodes provided context aurc lrc 9 applications divided two groups regular irregular 41 regular applications applications category fft lu ocean common characteristic optimized singlewriter applications given word data written processor assigned given appropriate data structures singlewriter page granularity well pages allocated among nodes writes shared data almost local hlrc need compute diffs aurc need use write cache policy protocol action required fetch pages applications different inherent induced communication patterns 22 9 affect performance impact smp nodes application page faults page fetches local lock acquires remote lock acquires barriers waternsquared 512 6919 2206 804 6826 1901 729 waterspatial 512 9786 2142 923 9381 1773 604 001 183 260 394 216 139 419 volrend head 10509 4406 3449 10478 2935 653 000 2934 4380 4434 1764 397 161 raytrace car 8980 2564 683 8979 2557 676 003 221 396 489 326 134 010 barnesrebuild barnesspace table 2 number page faults page fetches local remote lock acquires barriers per processor per 10 7 cycles application 14 8 processors per node fft alltoall readbased communication fft essentially transposition matrix complex numbers use two problem sizes 256k512x512 1m1024x1024 elements fft high inherent communication computation ratio lu use contiguous version lu allocates page data assigned one processor lu exhibits small communication computation ratio inherently imbalanced used 512x512 matrix ocean communication pattern ocean application largely nearestneighbor iterative regular grid run contiguous 4d array version ocean 514x514 grid error tolerance 0001 42 irregular applications irregular applications suite barnes radix raytrace volrend water barnes ran experiments different data set sizes present results 8k particles access patterns barnes irregular finegrained use two versions barnes differ manner build shared tree time step first version barnesrebuild one splash2 processors load particles assigned force calculation directly shared tree locking frequently necessary second version barnesspace 10 optimized svm avoids locking much possible uses different treebuilding algorithm disjoint subspaces match tree cells assigned different processors subspaces include particles particles assigned processors force calculation processor builds partial tree partial trees merged global tree without locking radix radix sorts series integer keys irregular application highly scattered writes remotely allocated data high inherent communication computation ratio use unmodified splash2 version fft lu ocean contiguous water nsquared water spatial radix volrend raytrace barnes rebuild barnes space200600100014001800normalized number messages sent figure 3 number messages sent per processor per 10 7 compute cycles application 14 8 processors per node raytrace raytrace renders complex scenes computer graphics version use modified splash2 version run efficiently svm systems global lock necessary removed task queues implemented better svm smp 10 inherent communication small volrend version use slightly modified splash2 version provide better initial assignment tasks processes stealing 10 improves svm performance greatly inherent communication volume small water use versions water splash2 waternsquared waterspatial water nsquared categorized regular application put ease comparison waterspatial versions updates water molecules positions velocities first accumulated locally processors performed shared data end iteration inherent communication computation ratio small use data set size 512 molecules table 2 figures 3 4 used characterize applications table 2 presents counts protocol events application 1 4 8 processors per node 16 processors total cases figures 3 4 show numbers messages mbytes data application protocol sent processor system characteristics measured per 10 7 cycles application compute time per processor averaged processors system use categorize applications terms communication exhibit number messages mbytes data exchanged important performance use geometric mean properties captures multiplicative effect metric divide applications fft lu ocean contiguous water nsquared water spatial radix volrend raytrace barnes rebuild barnes normalized mbytes sent figure 4 number mbytes sent per processor per 10 7 compute cycles application 14 8 processors per node three groups first group barnesrebuild fft radix exhibit lot communication second group belong waternsquared volrend exhibit less communication third group rest applications lu ocean waterspatial raytrace barnesspace exhibit little communication important note categorization holds 4processor per node configuration changing number processors node dramatically change behavior applications picture different instance ocean exhibits high communication 1 processor per node 5 effects communication parameters section present effects parameter performance allsoftware hlrc protocol range values table 3 presents maximum slowdowns application parameters consideration maximum slowdown computed speedups smallest biggest values considered parameter keeping parameters achievable val ues negative numbers indicate speedups rest section discusses parameters one one parameter also identify application characteristics closely predict effect parameter next section take different cut looking bottlenecks perapplication rather perparameter basis end section also present results aurc host overhead figure 5 shows slowdown due host overhead generally low especially realistic values asynchronous message overheads however varies among applications less 10 barnesspace oceancontiguous raytrace 35 volrend radix barnesrebuild across entire range values general applications send messages exhibit higher dependency host overhead seen figure 6 shows two application host overhead ni occupancy io bus bandwidth interrupt cost page size procsnode fft 226 119 408 866 726 138 lucontiguous 179 75 159 708 344 353 oceancontiguous 45 28 65 352 196 632 waternsquared 324 166 108 832 622 871 waterspatial 237 85 89 679 510 875 radix 358 318 776 587 3682 6994 volrend 347 128 157 913 639 681 raytrace 82 29 89 523 91 161 barnesrebuild 407 218 448 803 715 3834 barnesspace 44 06 275 590 1096 494 table 3 maximum slowdowns respect various communication parameters range values experiment negative numbers indicate speedups curves one slowdown application smallest highest host overheads simulate normalized biggest slowdowns second curve number messages sent processor per 10 6 compute cycles normalized biggest numbers messages note asynchronous messages host overheads low side range conclude host overhead sending messages major performance factor coarse grain svm systems unlikely become near future network interface occupancy figure 7 shows network interface occupancy even smaller effect host overhead performance realistic occupancies applications insensitive exception couple applications send large number messages applications slowdowns 22 observed highest occupancy values speedup observed radix reality caused timing issues contention bottleneck radix io bus bandwidth figure 8 shows effect io bandwidth application performance reducing bandwidth results slowdowns 82 4 11 applications exhibiting slowdowns 40 however many applications dependent bandwidth fft radix barnesrebuild benefit much increasing io bus bandwidth beyond achievable relationships processor speed today course mean important worry improving bandwidth processor speed increases bandwidth trends keep quickly find relationship reflected lower bandwidth case examine even worse mean bandwidth keeps processor speed likely major limitation svm systems applications figure 9 shows dependency bandwidth number bytes sent per processor application units normalized maximum numbers presented curve applications exchange lot data necessarily lot messages need higher bandwidth shows interrupt cost important parameter system unlike bandwidth affects performance applications dramatically many cases relatively fft lucontiguous oceancontiguous waternsquared waterspatial radix volrend raytrace barnesspace barnesrebuild figure 5 effects host overhead application performance data points application correspond host overhead 0 600 1000 5000 10000 processor cycles small increase interrupt cost leads big performance degradation applications interrupt costs 2000 processor cycles initiation delivery seem hurt much however commercial systems typically much higher interrupt costs increasing interrupt cost beyond point begins hurt sharply applications slowdown 50 interrupt cost varies 0 50000 processor cycles except oceancontiguous exhibits anomaly since way pages distributed among processors changes interrupt cost suggests architectures operating systems work harder improving interrupt costs support svm well svm protocols try avoid interrupts much possible figure 11 shows slowdown due interrupt cost closely related number protocol events cause interruptspage fetches remote lock acquires smp nodes many options interrupts may handled within node protocol uses one particular method systems uniprocessor nodes less options experimented configurations well found interrupt cost important case well difference system seems little less sensitive interrupt costs 2500 5000 cycles range performance degrades quickly smp configuration also experimented round robin interrupt delivery results look similar case interrupts delivered fixed processor smp overall performance seems increase slightly compared static interrupt scheme static scheme degrades quickly cost increases moreover implementing scheme real system may complicated may incur additional costs lucontiguous oceancontiguous radix raytrace volrend waternsquared waterspatial0103050709normalized units slowdown due host overhead normalized largest slowdown number messages sentprocessor1m compute cycles normalized largest figure 6 relation slowdown due host overhead number messages sent aurc mentioned introduction besides hlrc also used aurc study effect communication parameters using hardware support automatic write propagation instead software diffs results look similar hlrc exception network interface occupancy much important aurc automatic update mechanism may generate traffic network interface new values data may sent multiple times home node release importantly number packets may increase significantly since updates sent much finer granularity apart space time may coalesced well packets figure 12 shows performance changes ni overhead increases regular irregular applications 6 limitations application performance section examine difference performance best configuration ideal system speedup computed compute local stall times ignoring communication synchronization costs difference performance achievable best configuration per application basis recall best stands configuration communication parameters assume best value achievable stands configuration communication parameters assume achievable values goal identify application properties architectural parameters responsible difference best fft lucontiguous oceancontiguous waternsquared waterspatial radix volrend raytrace barnesspace barnesrebuild figure 7 effects network interface occupancy application performance data points application correspond network occupancy 50 250 500 1000 2000 10000 processor cycles ideal performance parameters responsible difference achievable best performance speedups configuration called ideal best achievable respectively table 4 shows speedups applications many cases achievable speedup close best speedup however cases fft radix barnes remains gap performance best configuration often quite far ideal speedup understand effects let us examine application separately fft best speedup fft 135 difference ideal speedup 162 comes data wait time page faults cost even best configuration despite high bandwidth zerocost interrupts achievable speedup 77 two major parameters responsible drop performance cost interrupts bandwidth io bus making interrupt cost 0 results speedup 11 increasing io bus bandwidth memory bus bandwidth gives speedup 10 modifying parameters time gives speedup almost best speedup lu best speedup 137 difference ideal speedup due load imbalances communication due barrier cost achievable speedup lu best speedup since application low communication computation ratio communication problem ocean best speedup ocean 1055 reason interrupt cost 0 anomaly observed first touch page allocation speedup low due large number mbytesmhz13579111315speedup fft lucontiguous oceancontiguous waternsquared waterspatial radix volrend raytrace barnesspace barnesrebuild figure 8 effects io bandwidth application performance data points application correspond io bandwidth 2 1 05 025 mbytes per processor clock mhz 400 200 100 50 mbytess assuming 200 mhz processor page faults achievable speedup 130 main cost barrier synchronization worth noting speedups ocean artificially high local cache effects processors working set fit cache uniprocessor fit cache processors thus sequential version performs poorly due high cache stall time barnesrebuild best speedup barnesrebuild 590 difference ideal page faults large number critical sections locks achievable speedup 39 difference best achievable speedups presence page faults synchronization wait time even higher due increased protocol costs increased costs mostly host overhead loss 1 speedup ni occupancy 08 verify disabled remote page fetches simulator page faults appear local speedup becomes 1464 best 1062 achievable cases respectively gap best achievable speedups due host ni overheads barnesspace second version barnes run improved version minimal locking 10 best speedup 145 close ideal achievable speedup 125 difference two mainly lower available io bandwidth achievable case increases data wait time imbalanced way waternsquared best speedup waternsquared 99 achievable speedup 9 reason high best speedup page faults occur contended critical sections lucontiguous oceancontiguous radix raytrace volrend waternsquared waterspatial0103050709normalized units slowdown due io bus bandwidth normalized largest slowdown number bytes sentprocessor1m compute cycles normalized largest figure 9 relation slowdown due io bus bandwidth number bytes transferred greatly increasing serialization locks artificially disable remote page faults best speedup increases 99 141 cost locks artificial case small nonideal speedup due imbalances computation waterspatial best speedup 1375 difference ideal mainly due small imbalances computation lock wait time data wait time small achievable speedup 133 radix best speedup radix 7 difference ideal speedup 161 due data wait time exaggerated contention even best parameter values resulting imbalances among processors lead high synchronization time imbalances observed due contention network interface achievable speedup 3 difference best speedup due factors data wait time much higher much imbalanced due much greater contention effects main parameter responsible io bus bandwidth instance quadruple io bus bandwidth achievable speedup radix becomes 7 like best speedup raytrace raytrace performs well best speedup 1564 achievable speedup 1480 fft lucontiguous oceancontiguous waternsquared waterspatial radix volrend raytrace barnesspace barnesrebuild figure 10 effects interrupt cost application performance six bars application correspond interrupt cost 0 500 1000 2500 5000 10000 50000 processor cycles volrend best speedup 1095 reason low number imbalances computation due cost task stealing large lock wait times due page faults critical sections artificially eliminate remote page faults computation perfectly balanced synchronization costs negligible speedup 149 fictional case achievable speedup 940 close best speedup see difference ideal best performance due page faults occur critical sections io bandwidth limitations imbalances communication computation difference best achievable performance primarily due interrupt cost io bandwidth limitations less due host overhead overall application performance svm systems today appears limited primarily interrupt cost next io bus bandwidth host overhead ni occupancy per packet substantially less significant order 7 page size degree clustering addition performance parameters communication architecture discussed granularities coherence data transferie page sizeand number processors per node two important parameters affect behavior system play important role determining amount communication takes place system cost determined performance parameters page size page size system important many reasons defines size transfers since software protocols data fetches performed page sizes also affects lucontiguous oceancontiguous radix raytrace volrend waternsquared waterspatial0103050709normalized units slowdown due interrupt cost normalized largest slowdown number page fetches remote lock acquires normalized largest figure 11 relation slowdown due interrupt cost number page fetches remote lock acquires amount false sharing system important svm two aspects page size conflict bigger pages reduce number messages system spatial locality well exploited communication increase amount false sharing vice versa moreover different page sizes lead different amounts fragmentation memory may result wasted resources figure 13 shows effects page size applications vary lot applications seem favor smaller page sizes exception radix benefits lot bigger pages vary page size 2 kbytes 32 kbytes pages systems today support either 4 kbytes 8 kbytes pages note two caveats study respect page size first tune applications specifically different page sizes second effects page size often related problem sizes used applications amount false sharing fragmentation ie granularity access interleaving memory different processors changes problem size larger problems run real systems may benefit larger pages ie fft size degree clustering number processors per node figure 14 shows applications greater clustering helps even memory configuration bandwidths kept 1 use cluster sizes 1 4 8 16 processors always keeping total number processors assumption keeping memory subsystem increasing number processors per node realistic since systems higher degrees clustering usually aggressive memory subsystem well fft lucontiguous oceancontiguous waterspatial volrend raytrace barnesrebuild figure 12 effects network interface occupancy application performance aurc data points application correspond network occupancy 50 250 500 1000 2000 10000 processor cycles system 16 configurations cover range uniprocessor node configuration cachecoherent busbased multiprocessor typical svm systems today use either uniprocessor 4way nodes couple interesting points emerge first unlike applications oceancontiguous optimal clustering four processors per node reason oceancontiguous generates lot local traffic memory bus due capacity conflict misses processors bus exacerbate problem hand oceancontiguous benefits lot clustering communication pattern thus four processors per node used performance improvement one processor per node comes sharing system four processors per node memory bus saturated although system benefits sharing performance degrading memory bus contention radix fft also put greatly increased pressure shared bus crossnode svm communication however high reduction via increased spatial locality page grain due clustering outweighs problem second important point applications perform poorly svm well shared bus system scale reason applications either exhibit lot synchronization make fine grain accesses much cheaper hardwarecoherent shared bus architecture example applications problem svm page faults within critical sections ie barnes rebuild perform much better architecture results show bus bandwidth significant problem applications scale use hardware coherence synchronization outweighs problems sharing bus likely provide greater nodetonetwork bandwidth application best achievable ideal fft 135 77 162 ocean 105 130 160 waternsquared 99 90 158 waterspatial 137 133 158 radix 70 30 161 volrend 109 940 154 raytrace 156 148 164 barnesrebuild 59 39 154 barnesspace 145 125 156 table 4 best achievable speedups application 8 related work work similar spirit earlier studies conducted 15 7 different context 15 authors examine impact communication parameters end performance network workstations applications written splitc top generic active messages find application performance demonstrates linear dependence host overhead gap transmissions fine grain messages svm find parameters important since cost usually amortized page granularity applications found quite tolerant latency bulk transfer bandwidth splitc study well 7 holt et al find occupancy communication controller critical good performance dsm machines provide communication coherence cache line granularity overhead significant unlike 15 since small 11 karlsson et al find latency bandwidth atm switch acceptable clustered svm architecture 13 lazy release consistency protocol hardware cachecoherence presented different context find applications sensitive bandwidth latency component communication several studies also examined performance different svm systems across multiprocessor nodes compared performance configurations uniprocessor nodes erlichson et al 6 find clustering helps shared memory applications yeung et al 23 find true svm systems node hardware coherent dsm machine 1 find true general software svm systems svm systems support automatic write propagation 9 discussion future work work shows room improving svm cluster performance various directions ffl interrupts since reducing cost interrupts system improve performance signifi cantly important direction future work design svm systems reduce frequency fft lucontiguous oceancontiguous waternsquared waterspatial radix volrend raytrace barnesspace barnesrebuild figure 13 effects page size application performance data points application correspond page size 2 kbytes 4 kbytes 8 kbytes 16 kbytes kbytes andor cost interrupts polling better operating system support support remote fetches involve remote processor mechanisms help direction operating system architectural support inexpensive interrupts would improve system performance unfortunately always achieved especially commercial systems cases protocol modifications using noninterrupting remote fetch operations implementation optimizations using polling instead interrupts improve system performance lead predictable portable performance across different architectures operating systems polling done either instrumenting applications smp systems reserving one processor protocol processing recent results interrupts versus polling svm systems vary one study finds polling may add significant overhead leading inferior performance interrupts page grain svm systems 24 hand stets et al find polling gives generally better results interrupts 20 believe research needed modern systems understand role polling another interesting direction exploring moving protocol processing network processor found programmable network interfaces like myrinet thus reducing need interrupting main processor ffl system bandwidth providing high bandwidth also important keep increasing processor speeds although fast system interconnects available software performance practice rarely close hardware provides low level communication libraries fail deliver close raw hardware performance many cases work low level communication interfaces may also helpful providing lowcost highperformance svm systems multiple network interfaces per node another approach increase available bandwidth case protocol changes may necessary ensure proper event ordering fft lucontiguous oceancontiguous waternsquared waterspatial radix volrend raytrace barnesspace barnesrebuild figure 14 effects cluster size application performance data points application correspond cluster size 1 4 8 16 processors per node ffl clustering scale examined adding processors per node helps almost cases applications performance increase quickly cluster size scaling system parameters memory bus io bandwidth desirable effects ffl applications work found restructuring applications area make big difference understanding application behaves restructuring properly dramatically improve performance far beyond improvement system parameters protocols 10 however always easy unfortunately many tools available parallel systems help easily discover cause bottlenecks obtain insight application restructuring needs especially contention major problem often commoditybased communication architectures architectural simulators one tools currently used understand application behaves detail point work limited certain family homebased svm protocols systemsfor instance fine grain svm systemsmay exhibit different behavior dependencies communication parameters similar studies protocols architectures help us understand better differences similarities among svm systems work based 16 processor system address question happens bigger systems run experiments processor configuration compared number protocol events two configurations table 5 shows ratios protocol events communication traffic 32 16 processor configuration cases event counts scale proportionally size system leads us believe results presented far hold bigger configurations well least 32 processors moreover larger problem sizes problems related communication architecture usually alleviated however sophisticated application page faults page fetches remote lock acquires local lock acquires barriers mbytes sent messages sent lu 194 253 186 200 190 1290 2 366 ocean 075 053 277 157 199 250 195 waternsquared 289 263 140 250 199 280 237 waterspatial 185 205 168 226 198 200 208 radix 183 243 270 410 199 219 238 volrend raytrace 208 208 133 240 200 208 183 table 5 ratios protocol events 32 16 processor configuration 4 processor per node scaling models take account problem size may necessary detailed accurate predictions another important question communication parameters going scale time seems parameters closely follow hardware performance host overhead network interface occupancy bandwidth potential getting better relative processor speeds interrupt cost depends operating system special architectural support conclusions examined effects communication parameters family svm protocols detailed architectural simulations cluster smps variety applications find applications sensitive interrupt cost would benefit improvements bandwidth relative processor speed well unbalanced systems relatively high interrupt costs low io bandwidth result substantial losses application performance cases observe slowdowns 90 factor 10 longer execution time however applications sensitive host overhead network interface occupancy regular applications achieve good svm performance best configuration parameters irregular applications though even best performance low mainly due serialization effects critical sections ie due page faults incurred inside critical sections dilate critical sections increase serialization example reducing amount locking using different algorithm parallel tree building performance barnes improves factor 23 overall achievable application performance today limited primarily interrupt cost node network bandwidth host overhead ni occupancy appear less important improve relative processor speed interrupts free bandwidth high relative processor speed achievable performance approaches best performance cases acknowledgments thank hongzhang making available us improved version barnes anonymous reviewers comments feedback r comparison shared virtual memory across uniprocessor smp nodes virtual memory mapped network interface shrimp multicomputer gigabitpersecond local area network design implementation virtual memorymapped communication myrinet active messages mechanism integrated communication computation benefits clustering shared address space multiprocessors applicationsdriven investigation effects latency improving releaseconsistent shared virtual memory using automatic update understanding application performance shared virtual memory application restructuring performance portability shared virtual memory hardwarecoherent multiprocessors performance evaluation clusterbased multiprocessor built atm switches busbased multiprocessor servers distributed shared memory standard workstations operating systems lazy release consistency hardwarecoherent multi processors effect communication latency fast messages fm 20 streaming interface tempest typhoon userlevel shared memory augmint multiprocessor simulation environment intel x86 architectures design issues tradeoffs write buffers fast interrupt priority management operating system kernels methodological considerations characterization splash2 parallel application suite mgs multigrain shared memory system relaxed consistency coherence granularity dsm systems performance evaluation performance evaluation two homebased lazy release consistency protocols shared virtual memory systems tr active messages virtual memory mapped network interface shrimp multicomputer tempest typhoon benefits clustering shared address space multiprocessors lazy release consistency hardwarecoherent multiprocessors mgs understanding application performance shared virtual memory systems performance evaluation two homebased lazy release consistency protocols shared virtual memory systems application restructuring performance portability shared virtual memory hardwarecoherent multiprocessors vmbased shared memory lowlatency remotememoryaccess networks myrinet design implementation virtual memorymapped communication myrinet fast interrupt priority management operating system kernels improving releaseconsistent shared virtual memory using automatic update performance evaluation clusterbased multiprocessor built atm switches busbased multiprocessor servers design issues tradeoffs write buffers effects latency occupancy bandwidth distributed shared memory multiprocessors effect communication latency overhead bandwidth cluster ctr mainak chaudhuri mark heinrich chris holt jaswinder pal singh edward rothberg john hennessy latency occupancy bandwidth dsm multiprocessors performance evaluation ieee transactions computers v52 n7 p862880 july cheng liao dongming jiang liviu iftode margaret martonosi douglas w clark monitoring shared virtual memory performance myrinetbased pc cluster proceedings 12th international conference supercomputing p251258 july 1998 melbourne australia soichiro araki angelos bilas cezary dubnicki jan edler koichi konishi james philbin userspace communication quantitative study proceedings 1998 acmieee conference supercomputing cdrom p116 november 0713 1998 san jose ca angelos bilas courtney r gibson reza azimi rosalia christodoulopoulou peter jamieson using system emulation model nextgeneration shared virtual memory clusters cluster computing v6 n4 p325338 october angelos bilas liviu iftode jaswinder pal singh evaluation hardware write propagation support nextgeneration shared virtual memory clusters proceedings 12th international conference supercomputing p274281 july 1998 melbourne australia angelos bilas dongming jiang jaswinder pal singh accelerating shared virtual memory via generalpurpose network interface support acm transactions computer systems tocs v19 n1 p135 feb 2001 sanjeev kumar yitzhak mandelbaum xiang yu kai li esp language programmable devices acm sigplan notices v36 n5 p309320 may 2001 zoran radovi erik hagersten removing overhead softwarebased shared memory proceedings 2001 acmieee conference supercomputing cdrom p5656 november 1016 2001 denver colorado angelos bilas cheng liao jaswinder pal singh using network interface support avoid asynchronous protocol processing shared virtual memory systems acm sigarch computer architecture news v27 n2 p282293 may 1999 salvador petit julio sahuquillo ana pont david kaeli addressing workload characterization study design consistency protocols journal supercomputing v38 n1 p4972 october 2006