maximizing theory accuracy selective reinterpretation existing methods exploiting flawed domain theories depend use sufficiently large set training examples diagnosing repairing flaws theory paper offer method theory reinterpretation makes marginal use training examples idea follows often small number flaws theory completely destroy theorys classification accuracy yet clear valuable information available even flawed theories example instance severalindependent proofs slightly flawed theory certainly likely correctly classified positive instance single proofthis idea generalized numerical notion degree provedness measures robustness proofs refutations given instance degree provedness easily computed using soft interpretation theory given ranking instances based values obtained required classify instances determine cutoff threshold instances classified positive threshold determined basis small set training examplesfor theories localized flaws improve method rehardening interpreting parts theory softly interpreting rest theory usual manner isolating parts theory interpreted softly done basis small number training examplessoftening without rehardening used quick way handling theories suspected flaws training examples available additionally softening rehardening used conjunction methods metaalgorithm determining theory revision methods appropriate given theory b introduction central concern machine learning research use prior knowledge effectively provide useful learning bias important type prior knowledge may thus used flawed domain theory obtained domain expert knowledge engineering one main methods using theory attempt revise order improve classification accuracy 1994a ourston mooney 1994 although idea great intuitive appeal revision always best way use given theory another class methods attempt repair given theory reinterpret profitable manner done using theory resource constructive induction pazzani kibler 1992 donoho rendell 1995 ortega fisher 1995 koppel engelson 1996 numerical refinement probabilistic theories mahoney mooney 1994 mahoney 1996 buntine 1991 lam bacchus 1994 russell binder koller kanazawa 1995 ramachandran mooney 1998 relevant paper interpreting logical theory probabilistic manner towell shavlik 1993 koppel feldman methods depend use sufficiently large set training examples diagnosing repairing flaws theory paper offer universal method theory reinterpretation makes marginal use training examples simplest version method examples required order approximate number positive negative instances sophisticated version method selectively reinterprets theory basis training examples empirical evidence indicates small training set sufficient idea squeeze much reliable information possible unreliable theory prior invoking information contained training examples central observation follows often small number flaws theory completely destroy theorys classification accuracy example one easily satisfied extra clause near root theory render instances positive ostensibly destroying theory yet clear valuable information available even flawed theories example instance several independent proofs theory certainly likely correctly classified positive instance single proof idea generalized easily computed numerical notion degree provedness measures robustness proofs refutations given instance instead interpreting theory usual boolean manner interpret softly assigning instance degree provedness value 0 1 given ranking instances based values obtained required classify determine cutoff threshold instances classified positive threshold determined basis small set training examples fact might even enough purpose know approximate number positive examples set examples without actually knowing correct classification single example see interpreting theory softly remarkably effective method classifying examples despite presence flaws moreover method benign case unflawed theory harm theories localized flaws improve method rehardening interpreting parts theory softly interpreting rest theory usual manner isolating parts theory interpreted softly done basis small number training examples softening without rehardening used quick way handling theories suspected flaws training examples available additionally softening rehardening may used conjunction methods metaalgorithm determining theory revision methods appropriate given theory particular method used determine whether theory localized flaws revised distributed flaws requiring reinterpretation whether theory contains useful information used learning bias revision deemed appropriate rehardening offer suggestions components theory ought focus repair outline paper follows section 2 explain justify soft interpretation theories section 3 show use softening classify instances section 4 explain justify technique rehardening section 5 illustrate methods work several wellknown theories section 6 give results tests methods large testbed synthetically generated flawed theories appendix offer proofs analytic claims concerning connection measure degree provedness actual robustness proofs refutations softening logical theories 21 logical provedness consider case propositional theories expressed definiteclause form negation asfailure clauses head positive literal body conjunction positive negative literals assume concept learned represented unique root proposition appear body clause section review theory probabilization method described koppel et al 1994b serves basis current work first review standard method computing function 1 example e proved propositional theory gamma 0 otherwise next section extend function take values 0 1 measuring relative notion example provedness observable proposition p define p false e p true e clause c antecedents l similarly nonobservable proposition p head clauses c finally negated proposition p let formulae simply arithmetic forms boolean functions respectively formulation simplified reformulating theory terms nand relations define children proposition clauses head children clause antecedent literals children negated proposition unnegated proposition primitive proposition k define ue k gamma component proposition clause negative literal k children c since ands ors strictly alternate every proposition p ue every example e particular r root proposition gamma ue e proved gamma exactly ue 22 soft provedness defined ue assume values 0 1 gamma either proves refutes r given e however since gamma assumed flawed would like evaluate precisely degree example proved theory consider example theory r r b c r e three examples proved theory although intuitive degree provedness varies considered proved greater degree since three proofs e 2 one furthermore although e 2 e 3 one proof e 2 also two near proofs thought proved greater degree e 3 reason believe theory might slightly flawed classification positive examples might therefore mistaken suspicion ought fall readily e 3 since reclassifying e 3 would require least violence theory want therefore relative measure degree proof extend definition u way assume values 0 1 correspond intuitive notion degree proof effect softening gammas classifications let ffl small value greater 0 similar development observable proposition p define softening function u ffl p false e p true e component k gamma children c term 1gammaffl thought introducing uncertainty theory placing probability measure subtheories gamma component independent probability ffl deleted thus asymmetry u ffl e deleting false proposition may cause clause become true vice versa computation u ffl e approximates expected classification e measure see appendix note component weights represent metatheory concept giving probability measure possible theories pearls bayesian networks pearl 1988 conditional probabilities results given premises way u ffl e provides useful measure resilience es classification changes theory particular discussed section 44 sufficiently small ffl u ffl e reflects minimal number components gamma would need revised order change es classification es revision distance defined precisely example e 1 highest revision distance 3 whereas e 2 e 3 revision distance 1 reflecting intuitive notion e 1 strongly classified positive theory two examples finegrained measure given u ffl however indeed u 01 given e 2 proved given e 2 given e 3 completely reflecting intuitions relative degreeofproof three examples value example rank positive negative figure 1 softened truth values u 01 106 examples promoter theory rank order optimal classification threshold examples depicted vertical line experiments reported paper set tuning although results date show little sensitivity choice properly tuning ffl matter investigation classification given flawed theory gamma concept r consider use softening function u ffl classify examples idea examples proved greater degree according likely truly positive vice versa regardless whether example actually proved gamma thus rank set unclassified examples e according u ffl choosing good threshold classify example e positive u ffl e negative u ffl e example consider wellknown domain theory identifying e coli promoter gene sequences promoter theory merz murphy aha 1996 theory consists 10 rules single toplevel proposition indicating whether particular gene sequence promoter theory given classification accuracy 50 every example classified negative fact half however sort u ffl distinguish nearly perfectly positive negative examples shown figure 1 fact choosing optimal threshold u ffl examples scoring threshold taken proved scoring unproved get classification accuracy 934 example illustrates softening theory may dramatically improve classification accuracy 50 934 without revision whatsoever merely confirms earlier results ortega 1995 indicate simple numerical generalization strategies effective particular theory surprising though simple scheme achieves good better results theory many theory revision techniques towell shavlik 1993 koppel et al 1994a ourston mooney 1994 naturally classification threshold must chosen properly practice right threshold estimated small set preclassified training examples see precisely given theory gamma root r training set e softening function u ffl threshold fraction examples e accurately classified using classification threshold u ffl classify new example e using algorithm softclassify accuracy training softening ptr either kbann figure 2 softening promoter theory accuracy original theory learning curves c45 softening ptr either kbann 1 let threshold maximizing accgammae u ffl 2 u ffl e classify positive 3 else classify negative promoter theory requires little training reach respectable classification accuracy using softclassify example choosing optimal threshold based 20 training examples get average classification accuracy 91 using 5fold crossvalidation withholding 20 training examples time figure shows corresponding learning curves softening several learning tech niques performed 5fold crossvalidation withholding different amounts training generate data point compare softening accuracy original theory examplebased learning system c45 quinlan 1993 theory revision systems 1 ptr koppel et al 1994a either ourston mooney 1994 kbann towell shavlik 1993 rapture mahoney mooney 1994 figure shows softening promoter theory better alternatives 2 alternatives competitive softening kbann rapture theory revision systems use numerical representations theory course revision fact results rapture virtually identical softclassify interesting note softening still performs well systems despite fact learning estimate single threshold theory revision results presented koppel et al 1994a unfortunately original data revision system experiments could compute statistical significance results illegal samelocabcd adjbf illegal samelocabef illegal sameloccdef illegal kingattackking illegal rookattackking kingattackking adjae adjbf kingattackking adjae bf kingattackking ae adjbf kingattackking knightmoveabef rookattackking ce kingnotbfile rookattackking df kingnotbrank kingnotbrank bd kingnotbrank bd betweencae kingnotbfile ac kingnotbfile ac betweendbf figure 3 flawed version chess endgame theory added antecedents clauses shown boldface deleted components actually flawed theory shown italics lowlevel propositions ab defined terms primitive attributes f takes values 1 8 shown 4 partial rehardening 41 problem softening although softening works remarkably well theory like promoter errors distributed throughout theory koppel et al 1994b ortega 1995 expect work well theory flaws highly localized softening treats components theory way since flawed correct components softened equally softclassify cannot always distinguish correctly classified examples incorrectly classified examples softening parts theory correct cannot expected improve classification accuracy consider example flawed domain theory categorizing kingrookking chess endgames merz et al 1996 depicted figure 3 root illegal theory softening neither improves harms classification accuracy shown figure 4 using number training examples 10 100 softclassify gives accuracy 77 separate set 200 test examples 34 positive 66 negative theorys raw accuracy soft classification doesnt help chess endgame theory theory softened illustrate point consider two examples theory e 1 adjbf true samelocabcd true examples otherwise identical proofs theory given since adjbf incorrectly added first illegal clause e 2 truly positive despite u ffl gamma flawed theory greater u ffl greater number occurrences adjbf theory surprisingly though find fact softening theory reduce classification accuracy fluke whole even locally flawed theories softening almost never harm often improves classification accuracy significantly nevertheless shall see theories localized flaws generally obtain improved classification accuracy value example rank positive negative figure 4 sorting graph per figure 1 100 randomly chosen examples softened chess endgame theory vertical dashed line shows optimal classification threshold softening selective fashion 42 partially rehardened theories example illustrates given theory may contain regions interpreted soft manner ie like promoter regions interpreted nonsoft manner ie correct theories localized flaws chess endgame theory classification softclassify algorithm would greatly improve could somehow soften flawed portions theory section describe simple algorithm finds components theory interpreted soft manner first though define precisely means interpret theory partially soft ie components defined hard note consider appearance proposition antecedent literal separate component theory one appearance proposition hard another soft formally given set h theory components defined hard component k children whereas appearance l primitive proposition p intuitively ffl introduces uncertainty soft components theory others assumed correct example evaluating example theory components hardened simply gives 1 0 according example positive negative theory 43 algorithm wish exploit given set training examples order determine components hardened softened idea harden components whose hardening improves classification accuracy using softclassify use softclassify classify new examples using partially rehardened theory thus obtained idea iteratively harden components theory time evaluating optimal accuracy theory training set thus given theory gamma training set e softening function u ffl define soft accuracy gamma reharden algorithm greedily hardens components gamma would reduce accuracy theory training set e ie ffl note evaluating theory n components given set hardened components takes onje time since evaluation performed n times component hardened worst case straightforward implementation reharden takes 3 je time method usually much faster practice efficiency improved caching intermediate results 1 h 2 evaluate 3 evaluate every component c 2 gammanh hardening accuracy 4 let c component closest root whose accuracy 0 breaking ties arbitrarily 5 component exists h b goto step 2 6 else let c component closest root whose hardening accuracy breaking ties arbitrarily 7 component exists h b goto step 2 8 else return h 44 justification rehardening revision distance reason rehardening procedure works hardening flawed components tends reduce accuracy softclassify ii hardening unflawed components theory tends increase accuracy softclassify since hardening flawed components usually reduces accuracy greedy rehardening algorithm general harden unflawed components turn tend increase softclassifys accuracy using theory connection choice components hardened resulting accuracy softclassify consequence fundamental property u h ffl function forms basis softclassify property u h ffl sorts examples primarily many revisions deletions nonhardened components theory would suffice change examples classifications examples revision distance classifying using revision distance obviously correlated hardening unflawed components hardening unflawed components causes revision distance correctly classified examples increase number possible revision sites decreases incorrectly classified examples however always sufficient revise flawed components thus flawed components remain unhardened revision distance incorrectly classified example cannot increase beyond minimum number flawed components need revised order change examples class make intuitive notion precise considering subtheories given theory gamma obtained deleting gammas nonhard components ie components h distance subtheory gamma 0 gamma distgamma gamma 0 number components deleted order quantify robust example es classification respect possible flaws gamma measure distance nearest subtheory gamma 0 classifies e differently gamma define revision distance h gamma e e gamma respect set hardened components h number deletions required fix example particular ffl positive revision distance gives number component deletions needed make unproved example proved ffl negative revision distance gives number needed make proved example unproved theory without negation former deletions antecedents latter clauses key idea exists close relationship u h ffl revision distance case treestructured theories nonprimitive proposition appears possibly negated antecedent one clause relationship neatly formulated follows theorem 1 given treestructured theory gamma root r set h components gamma examples gamma h gamma sufficiently small ffl theorem states limit larger values h lead smaller values u h ffl sorting according u h ffl consistent sorting according much theory would change order change examples classification sorting u h ffl however provides finegrained measure gives useful information even revision distances identical example components theory soft revision distance nearly useless since examples classification changed revising components root revision distance becomes meaningful however components theory hardened components hardened precise measure becomes distinguishing degree examples proved see appendix formal treatment ideas proof theorem 5 illustrations softening rehardening section following show results systematic set experiments designed test hypotheses effectiveness softening rehardening flawed theories getting though let us consider illustrations method familiar theories illegal samelocabcd adjbf illegal samelocabef illegal sameloccdef illegal kingattackking illegal rookattackking kingattackking adjae adjbf kingattackking adjae bf kingattackking ae kingattackking knightmoveabef rookattackking df kingnotbrank kingnotbrank bd kingnotbrank bd betweencae value example rank positive negative b figure 5 results rehardening hardened version chessendgame theory softened clauses boldface softened antecedents underlined b sorting graph rehardened theory using 100 examples figure 4 vertical dashed line shows optimal classification threshold accuracy training softening acc rehardened acc figure rehardening chess endgame theory averaged learning curves c45 softclassify rehardening 51 rehardening chess theory let us reconsider gamma flawed version chess endgame theory shown figure 5a shows results rehardening theory training set e examples note components remain soft almost exactly contain flaws theory figure 5b shows softclassify sorts 100 test examples e using rehardened theory shown rehardening softaccgammae u h increases 90 opposed softaccgammae u ffl 79 without rehardening figure 6 compare accuracies obtained test set using c45 softening alone rehardening respectively varying amounts training data evaluated methods 5 trials trial trained 10 100 examples testing accuracy disjoint 200 example test set averaged accuracies 5 trials evident rehardening significantly improves softening remains equal original theory accuracy entire range furthermore training examples less 90 rehardening better learning directly examples however rehardening curve flattens quickly even c45 continues improve since rehardening somewhat crude example cannot finetune theories adding components potential limited thus sufficient examples available may preferable use inductive methods 52 comparing types flawed theories examine proposed explanation difference softening performance promoter chess endgame theories addition locally flawed theory presented therefore also created flawed theory synthetic distributed flaws show distinguish types theories based performance softening rehardening theory distributed flaws many components theory flawed flaw change meaning theory much example antecedents two clauses proposition p randomly redistributed clauses many flaws illegal kingattackking rookattackking illegal kingattackking sameloccdef illegal kingattackking samelocabef illegal rookattackking samelocabef samelocabcd rookattackking kingnotbfile adjae df rookattackking df rookattackking kingnotbrank df kingnotbfile ce kingnotbfile adjbf adjae betweendbf kingnotbfile ce df betweendbf ac kingnotbfile betweendbf ac kingnotbfile ae adjae ac kingnotbfile df ce ac kingnotbrank betweencae bd kingnotbrank betweencae bd kingattackking adjbf ae bf adjae kingattackking bf adjbf adjae illegal intb bd illegal ac adjae figure 7 chess2 chess theory distributed flaws chess3 random chess theory chess1 chess2 chess3 theory 77 35 65 softening 77 87 68 rehardening 89 94 72 table 1 accuracies different interpretation methods chess theories different types flaws last line shows fraction total example set classified theory positive chess1 localized flaws chess2 distributed flaws chess3 random introduced antecedent wrongly placed still set components influences truth p flawed theory thus consider three flawed versions chess endgame theory 1 theory considered four localized flaws chess1 2 theory distributed flaws created repeatedly merging randomly splitting antecedent sets clauses theory chess2 figure 7 3 randomly generated theory primitives correct theory chess3 figure 7 performed set trials using different 40example training set 200example testing set trial theory evaluated accuracy test set original flawed theory b softened theory using threshold evaluated training set c theory rehardened based training set averaged results 5 random trials results shown table 1 table 1 shows initial accuracies results using softening partial rehardening three test theories note using classification accuracy tells us nothing relative merits theories fact random chess3 better chess2 distributed flaws nevertheless see accuracy random theory using softclassify even rehardening little better raw accuracy significantly little better simply classifying examples negative indicating clearly theory essentially useless distinguishing positive negative examples thus discarded hand chess1 chess2 show significant improvement accuracy using softclassify rehardening theories distinguishable however respective differences gap softening rehardening rehardening adds less effect softening chess2 suggests flaws nonlocalized rehardening improves chess1 quite noticeably softening suggests chess1s flaws localized information could useful deciding handle one three theories probably revise theory localized flaws interpret theory distributed flaws probabilistically throw random theory indeed c45 performs significantly better 81 chess3 even rehardening whereas chess1 chess2 show improvement c45 rehardening small training set generally results suggest might decide proper way use given theory based training set first check theory contains useful information ie positive examples proved greater degree negative examples specifically need check accuracy obtained softened theory significantly better accuracy expected optimally partitioning random ordering positive negative examples roughly speak ing expected accuracy slightly exceeds maxposneg pos neg respective 1 nopaymentdue deferment 2 nopaymentdue continuous 3 deferment disabilitydeferment 4 deferment studentdeferment 5 deferment financialdeferment 6 deferment peacecorpsdeferment 7 deferment militarydeferment 8 continuous enrolledfiveyears neverleft 9 militarydeferment armedforcesenlist 10 peacecorpsdeferment peacecorpsenlist 11 financialdeferment unemployed 12 financialdeferment filedforbankruptcy 13 studentdeferment enrolledelevenyears 14 disabilitydeferment disabled figure 8 correct studentloan theory percentages positive negative examples training set thus example chess theory 66 training examples negative softened random theory correctly classes 68 easily achieved choosing threshold near 10 ie classing almost examples negative case theory revised contains localized errors ie rehardening obtains significantly better classification training data softening could done given small training set evaluating expected accuracy softening rehardening crossvalidation softening increases accuracy greatly perhaps theory used however softening little rehardening helps theory probably revised neither softening rehardening helps theory discarded pure inductive techniques applied next section see though method completely reliable general rehardening often helps significantly theories nonlocalized flaws helping theories localized flaws 53 rehardening flawed studentloan theories take closer look rehardened flawed theories order compare set flawed components set components left soft see although two sets similar quite identical obviously flaws adversely affect classification training examples left soft additionally turns surprisingly subtle ways hardening flawed components leaving related components soft actually leads better results leaving flawed components soft order illustrate related phenomena consider three arbitrarily chosen flawed versions studentloan theory shown figure 8 used determining whether student must pay back student loan pazzani brunk 1991 performed five independent trials theory using disjoint 100example training test sets compare flawed theories sets hardened components well examining accuracies initial theory theory softening rehardening flawed components left soft simulating ideal rehardening although set rehardened components flawed theory varies given training theory flaws soft components sl 1 add militarydeferment filedforbankruptcydisabled 2 clause 8 add antecedent financialdeferment financialdeferment clause 8 3 clause 12 add antecedents militarydeferment studentdeferment 4 clause 14 add antecedent continuous continuous clauses 2 14 sl ii 1 delete clause 7 2 add deferment filedforbankruptcy financialdefermentstudentdeferment disabilitydeferment 3 clause 1 add antecedent enrolledeleven enrolledeleven clause 1 4 clause 10 add antecedent continuous continuous clause 10 5 clause 14 add antecedent continuous continuous clause 14 continuous clause 2 sl iii 1 delete clause 5 2 add peacecorpsdeferment peacecorpsdeferment firedepartmentenlist neverleft firedepartmentenlist added clause 3 clause 8 delete antecedent neverleft clause 2 continuous clause 2 4 add militarydeferment disabled studentdeferment foreignlegionenlist table 2 rehardening results three flawed studentloan theories table shows flaws introduced correct theory figure 8 components left soft rehard ening component whose softening compensates particular flaw placed near flaw table examples case one rehardened theory appeared majority independent trials rehardened theories refer flawed theories compared results rehardening table 2 3 theory sl flaw 1 let negative examples fact lets positive ones incorrectly blocked flaw 3 therefore left soft flaws 2 4 captured directly easy theory softening handle since flaws additions rather deletions rehardened theory perfectly classifies test examples theory sl ii flaws 34 5 captured directly flaw 2 let negative examples deleted clause directly compensated result rehardened theory 94 accurate seen table 3 even precisely flawed components left soft ie antecedent deferment rendered overly specific deletion clause 7 also left soft rehardened accuracy exceed 94 theory sl iii interesting three flawed theories considered effect added clause flaw 2 diminished softened parent clause softened softening firedepartmentenlist softened added clause boosts relative effect antecedent neverleft deleted clause 8 flaw 3 softening clause 2 diminishes influence clause 6 lets negative examples result flaw 3 finally softening antecedent continuous clause 2 diminishes influence softening clause flaw 4 let negative examples left sl sl ii sl iii initial 75 74 75 softened 98 86 87 rehardened 100 94 94 flaws soft 100 94 90 table 3 results softening rehardening three flawed studentloan theories shown initial accuracy flawed theory well accuracies obtained softening rehardening hardening components except actually flawed soft seen table 3 results accurate rehardened theory 94 one precisely flawed components left soft 90 6 experimental results 61 theory accuracy anecdotal results previous section turn several systematic experiments synthetically generated five different random propositional theories number distinct propositions theories ranged 15 21 number clauses theories ranged 14 22 average 25 antecedents per clause theory 20 flawed theories five local flaws 10 five distributed flaws follows local flaw generator chooses theory component random inserts random flaw adding clause 24 random antecedents proposition deleting antecedent either adding random antecedent clause deleting clause equal probability distributed flaw generator chooses proposition random replaces two random clauses proposition new clause c union two clauses antecedents splits c randomly two new clauses randomly assigning antecedent literals one new clauses two types flaws allow us experimentally evaluate explanation difference promoter chess endgame theories described five correct theories 100 examples randomly generated divided two equal sets nested subsets various sizes selected one sets training examples corresponding flawed theory results tested examples set roles two sets switched flawed theory number training examples two data points generated averaged accompanying scatter plots plot initial accuracy fifty locallyflawed theories fifty distributedflawed theories softened accuracy hardened accuracy show results 20 40 training examples figures 9 10 average results given table 4 tested statistical significance improvement softening original theory b rehardening softening performing ttests paired data seven eight cases accuracy improvement proved significant 0002 case distributed flaws 20 training examples rehardening accuracy indistinguishable softening accu racy accords hypothesis lessened effect rehardening theories distributed flaws accuracy original acc flawed theory training hardened acc testing original acc flawed theory training accuracy original acc flawed theory training hardened accuracy original acc flawed theory training rehardening figure 9 scatter plots experiments synthetic theories local flaws using 20 40 training examples training 40 training flaw type original softening rehardening softening rehardening distributed 690 842 849 852 881 table 4 average results experiments synthetic theories local distributed flaws shown average original accuracies theories accuracies softening hardening 20 40 training examples accuracy original acc flawed theory training hardened acc testing original acc flawed theory training accuracy original acc flawed theory training hardened accuracy original acc flawed theory training rehardening figure 10 scatter plots experiments synthetic theories distributed flaws using 20 hardened accuracy accuracy training hardened accuracy accuracy training distributed figure scatter plots experiments synthetic theories local distributed flaws using training examples comparing rehardening softening accuracy evident plots softening astonishingly effective even using 20 training examples almost never harm rehardening used focus softening every single one 100 flawed theories improved 40 training examples used even twenty training examples used two one hundred rehardened theories classify less accurately original theories tiny margin interesting note though contrary expectations difference effectiveness softening versus rehardening local versus distributed flaws although detectable table 4 significant softening works well rehardening generally slightly improves greater improvement locally flawed theories small difference seen however figure 11 plots accuracy rehardening function accuracy softening 62 finding single flaws saw theories multiple flaws rehardening often works effectively leaving soft theory components actually flawed case theories single flaws would hope rehardening would able isolate flaws precisely extent rehardening successful isolating flaws way could effectively used conjunction theory revision algorithms repair isolated flaws accordingly ran following experiment test ability rehardening isolate individual theory flaws clauses average 2944 components per theory theory generated 10 flawed versions single random flaw local flaws correct theory also generated 10 random training sets 20 examples use rehardening flawed theories rehardened flawed theory using training set theory giving us 1000 data points rehardening flawed theory training set evaluated components soft compared actual location theory flaw possible outcomes trial exact oversoft undersoft nearby failed number trials accuracy class figure 12 results 1000 trials singleerror theories see text explanation exact flawed component one left soft oversoft flawed component one several left soft undersoft components left soft nearby flawed component left soft ancestordescendant possibly components well failed flawed component left soft entirely different components figure 12 shows results experiment shown flaw left soft 55 trials 69 trials either flaw one ancestors descendants left soft average 128 components left soft theory 70 cases 7 components unrelated flaw left soft 470 cases exactly one component left soft 381 cases 81 single soft component flawed component noted 239 cases component left soft 100 unavoidable since flawed theory perfect training set expected since training contained 20 random examples conclusions introduced notion degree provedness believe fundamental distilling reliable information unreliable theories particular found softening extremely effective method theory reinterpretation increases classification accuracy almost flawed theories even handful training examples available moreover softening adds computational expense ordinary logical methods classification using theories hence whenever reliability given propositional theory doubt sufficient information choosing threshold eg small number examples available recommended theory interpreted softly shown formally rehardening unflawed components theory typically improve soft classification shown empirically rehardening based small training sets fact improve softening sometimes performing even better rehardening actual flawed components however necessarily case improvement classification accuracy obtainable rehardening always justifies computational expense methods improving efficiency efficacy rehardening process remain explored softening rehardening crude methods reinterpret theories revise thus potential increasing accuracy limited especially theory components deleted strength methods lies primarily requiring large amounts training examples training examples abound inductive methods may prove superior methods suggested ways rehardening used metaalgorithm theory revision determining whether flawed theory contains useful information whether flaws localized distributed throughout theory localized flaws located information used decide whether theory candidate revision whether patched reinterpreted however results bear relative effectiveness softening versus rehardening reliably distinguish theories localized flaws type distributed flaws considered believe weakness results artifact method generating distributed flaws precise definition distributed flaws methods generating important topic future research one issue remains open interaction various parameters softening experiments always set 01 theories used 25 150 components 5 20 flaws parameters found 20 training examples sufficient softening 40 sufficient rehardening relationship ffl theory size number type flaws number examples required varying accuracy levels merits investigation r theory refinement bayesian networks grammatically biased learning learning logic programs using explicit antecedent description language rerepresenting restructuring domain theories constructive induction approach integrating multiple classifiers finding areas expertise journal intl conference machine learning combining symbolic connectionist learning revise certaintyfactor rule bases comparing methods refining certaintyfactor rule bases uci repository machine learning databases informativeness dna promoter sequences domain theory flexibly exploiting prior knowledge empirical learning intl conference machine learning theory refinement combining analytic empirical methods detecting correcting errors rulebased expert systems integration empirical explanationbased learning utility prior knowledge inductive learning probabilistic reasoning intelligent systems networks plausible inference theory refinement bayesian networks hidden variables local learning probabilistic networks hidden variables multistrategy learning theory revision extracting refined rules knowledgebased neural networks tr