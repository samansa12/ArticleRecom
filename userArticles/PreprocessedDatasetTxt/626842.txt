performance evaluation hierarchical ringbased shared memory multiprocessors investigates performance wordpacket slotted unidirectional ringbased hierarchical direct networks context largescale shared memory multiprocessors slotted unidirectional rings attractive electrical characteristics simple interfaces allow fast cycle times large bandwidths largescale systems necessary use multiple rings increased aggregate bandwidth hierarchies attractive topology ensures unique paths nodes simple node interfaces simple interring connections ensure realistic region design space examined architecture network used hector prototype adopted initial design point simulator architecture developed validated measurements prototype system workload parameterization reflects conditions expected near future results study shows importance system balance performance b introduction paper study performance largescale sharedmemory multiprocessors use wordpacket ringbased hierarchical network class architectures interest several reasons first distributing shared memory among processor modules associating caches processor module locating processor modules nodes network using direct network communication locality exploited reduce network traffic memory latency second bitparallel unidirectional slotted rings found effective maximizing link bandwidth direct networks 26 advantages unidirectional rings include 1 pointtopoint connections run high clock speeds 2 easy make full use bandwidth provide natural broadcast mechanism 4 allow easy addition extra nodes third single slotted ring scale well multiple rings need interconnected hierarchical ring interconnection attractive since allows simple node interfaces interring connections node need interface two ring neighbors interring connections regardless system size implemented using twobytwo crossbar switch moreover hierarchy provides unique path two nodes useful implementation cache consistency protocols 13 disadvantage hierarchy limited bandwidth near root however disadvantage mitigated sufficient communication locality interest lies performance type network within context shared memory multiprocessor isolation consequently evaluating overall system performance effects memory cycle time memory utilization aspects processor design effect request rate need considered specific issues interest effectiveness techniques hide memory latency multiple outstanding transactions nonblocking reads use memory banks ring topology communication locality hot spots relative speeds processors memories rings transactions consider memory reads writes single words block transfers approach evaluating issues accurately simulate existing system using detailed packetlevel simulator validated existing system purpose use hector prototype multiprocessor class architecture since interested performance systems order 1024 processors since clear extrapolate results small systems found necessary use synthetic workloads simulating instruction execution tango 12 large system would take prohibitively long using address traces systems highly questionable results study show importance system balance performance largescale systems inherently large communication delays distant accesses processor efficiency low unless processors operate multiple outstanding transactions using techniques prefetching asynchronous writes multiple hardware contexts however multiple outstanding transactions one memory bank per processing module memory quickly saturates memory saturation alleviated multiple memory banks per processing module shifts bottleneck ring subsystem topology ring hierarchy affects performance show topologies similar branching factor levels except possibly slightly smaller rings root hierarchy tend perform best ring subsystem inherently limit throughput system hence increasing number outstanding transactions per processor beyond certain point limiting effect perfor mance since causes rings become congested adaptive maximum number outstanding transactions appears necessary adjust appropriate tradeoff concurrency contention communication locality changes show relationships processor ring memory speeds effects performance next section describe detail systems examining simulation methodology system parameters workload parameters experimental results reported section three conclude section four together discussion related work 2 system workload description chosen hector architecture developed university toronto initial design point study designed specifically ringbase hierarchies implemented successfully choose start design actually implemented two reasons first basing study implementation helps ensure performance system modules interactions correctly captured abstract system model might miss second restricting study designs related carefully thought implementation focusing attention realistic section design space one believe relatively promising respect scalability briefly describe hector detailed presentation given 29 27 comment simulator system parameters workload parameters 21 hector architecture hector sharedmemory multiprocessor consisting set stations interconnected hierarchy unidirectional rings station contains collection processor modules containing processor local cache part main memory station connected lowest level local ring hector provides flat global physical address space station assigned unique contiguous portion address space determined location processors transparently access memory locations system information transfer takes place means fixedsize packets transferred bitparallel format along unique path ring hierarchy two types ring interfaces control packet transfers simple realize station controllers control onstation traffic well local ring traffic station gate incoming packets ring onto station outgoing packets station onto ring continuing packets previous ring interface next ring interface packets ring priority packets station minimize time packets buffered station controllers interring interfaces control traffic two rings logically interring interface corresponds 2 theta 2 crossbar switch fifo buffers fifos needed order able store packets collisions occur happen given cycle input packets rings routed output order minimize remaining delay packets descending hierarchy packets higherlevel ring priority packets lowerlevel ring figure 1 depicts hector configuration two levels ring hierarchy global ring connects several local rings turn connect multiple stations example corresponds prototype bus connects several processing modules local ring interface communication hector occurs synchronously given clock cycle packet transferred two adjacent ring interfaces station controller onto station station onto ring request access nonlocal memory location initiates station station controller interring interface figure 1 structure hector two levels ring hierarchy packet transfer across network following terminology scalable coherent interface protocol 22 transaction involves request response subtransaction subtransaction typically entails transmission single packet event collisions timeouts several packets may used example access remote memory location request packet containing address target memory formed processing module transferred ring via local station controller packet travels around ring visiting one segment ring cycle packet reaches first interring interface path either switched onto higher level ring passed next station controller ring depending destination address packet first travels hierarchy level needed reach target station descends hierarchy target station removed local ring target station sends response packet back requesting pm along similar path case read transaction response packet contains requested data case write transaction request packet contains data addition addressing information response packet contains acknowledgment writes response packet sent back requesting station soon write queued target memory latency actual memory operation hidden possible request packet cannot successfully delivered target memory happen example congestion target memory cannot accept request packet arrives case target station generates negative acknowledgement packet sent back requesting station retry operation later time retransmitting request packet 22 simulator constructed simulator reflects behavior packets cyclebycycle basis simulator written using smpl 19 simulation library batch means method output analysis used first batch discarded account initialization bias batch means method single long run divided subruns called batches separate sample mean computed batch batch means used compute grand mean confidence interval 19 batch termination criterion processor complete least minimum number requests early experiments showed using total number requests completed entire system batch termination criterion substantially underestimate mean response times since requests long response times underrepresented using several workloads validated simulator measurements collected hector prototype base simulator extended model features present prototype arbitrary number ring levels ring cycle times different processor cycle time 23 system parameters hierarchical ringbased system characterized following parameters system size processors relative processor memory ring cycle times maximum number type transactions processor may outstanding whether processors blocks read completes ring topology number banks memory module transaction latency refers entire time request packet transaction issued processor transaction completes response packet returns processor 1 base contentionfree fraction transaction latency number cycles required traverse network twice reads time actually execute memory operation absence contention remaining fraction transaction latency number additional cycles due contention exposed transaction latency fraction transaction latency processor blocked waiting transaction complete thus memory cycle time may imply transaction latency 100 processor cycles large system target memory far source processor processor efficiency fraction time processor spends useful work averaged processors useful work includes delay references cache hits assume processor local cache include additional delay cache misses processor blocks read write completes large transaction latency relative processor cycle time implies low processor efficiency number techniques proposed increase processor utilization base case including relaxed memory consistency models prefetching multiple hardware contexts 16 instead assuming one technique versus another characterize effects varying maximum number outstanding transactions processor may blocking considering whether reads block thus processor block either number outstanding transactions exceeded maximum reads block read cache miss occurs goal multiple outstanding transactions nonblocking reads hide exposed transaction latency one effective methods allowing nonblocking reads multiple outstanding transactions use multiple hardware contexts consider case multiple outstanding transactions intentionally take account cycles lost due hardware context switches assumption based recent work suggests context switches scheduled avoid lost cycles 7 prevent network saturation consider implications alternative ring topologies system topology specified branching factor level hierarchy starting 1 writes response packet returns upon queueing request target memory possible target memory still processing write request transaction completes sense number stations local level 1 ring last branching factor number ring directly attached root ring thus topology refers topology 2 stations per level 1 ring 3 level 1 rings per level 2 ring 4 level 2 rings connected root ring throughout paper assume one processor per station prevent memory saturation consider use multiple memory banks per memory module assume transaction particular target memory equally likely access banks memory 24 workload parameters detailed system simulator needed order ensure important features architecture studied captured simulating large system also important since extrapolating results small system questionable key satisfying concerns use synthetic workload model simulating instruction execution large system tango would take prohibitively long using address traces smaller systems highly questionable contrast synthetic workload model number transactions need issued processor order obtain system performance measures dramatically smaller number generated simulating execution actual application programs moreover use synthetic workload sometimes allows clearer understanding significance different workload parameters course concern addressed realism workload model approach characterize workload mean time cache misses given nonblocked processor equivalently request rate inverse mean time probability cache miss read communication locality read cache miss probability assume 07 throughout study consistent empirical statistics 15 request rate consider rate 001 005 cache misses per processor cycle equiv alently 20 100 cycles cache misses range choice supported recent study number application programs observed mean number processor cycles 6 137 shared data reads 7 assume code private data references stack always hit local cache factoring shared data writes shared data cache hits yields realistic mean number processor cycles cache misses least 20 chose workload characterization avoid accounting least explicitly cache coherence traffic cache coherence traffic could included within lowlevel workload model providing translation highlevel workload model lowlevel workload model preliminary study effect translation case software cache coherence using approach developed adve etal 1 resulting ranges lowlevel workload parameters consistent ranges consider 2 results presented paper assume one word transfers transfers larger word arise several sources page migrations replications cache line transfers prefetching regard page migrations replications parameter values greatly effect results since clear value ranges use chose ignore cache line transfers prefetching adjacent words cache miss consider use pagemode dram access transfer blocks words thus example upon memory access first word provided memory say processor cycles successive words might provided intervals 5 10 processor cycles 2 lowlevel software cache coherence model includes traffic due posts invalidates cache lines assuming single word cache lines possible include traffic within read write parameters 25 communication locality hot spots direct network communication locality hot spots greatly affect performance clusters locality model attempts model communication locality intuitively model processor logically organizes processors clusters around processor independent network topology first cluster typically contains processor second cluster contains nearby processors additional clusters contain processors away case view processors leaves tree defined ring hierarchy number left right clusters defined terms distance two processors absolute difference modulo size system two processor numbers example two cluster case cluster 0 may source processor module cluster 1 remaining processor modules three cluster case cluster 0 may source processor module cluster 1 source processor modules set closest neighbors cluster 2 remaining processors modules defining clusters manner reasonable since likely applications programmed manner independent particular branching factors present certain ring topology probability target memory transaction processor module particular cluster depends cluster given target memory particular cluster probability processor module within cluster containing target memory uniformly distributed communication locality model specifies number clusters number cluster probability transactions target cluster thus 1024 processors system means three clusters cluster 1 size 1 probability 09 target cluster 2 4 closest processing modules probability 08 target given target cluster 0 cluster 3 remaining 1019 processing modules probability 10 containing target given target cluster 1 cluster 2 clusters locality model adopted similar simpler models shown effective studies direct networks 25 2 3 model exhibits memory access patterns similar scientific applications examined hector especially true applications optimized numa systems migrate replicate data objects improve locality including migration replication done operating system transparent application study extent real shared memory programs conform model allowing application restructuring hardware software dynamic page cache block placement interest outside scope paper major type nonuniform traffic hot spot single memory unusually high probability accessed many processors early papers 23 30 identified hot spot memory modules major cause performance degradation sharedmemory interconnection networks degradation exacerbated tree saturation 23 even obstructs memory traffic nonhot spots locations significant progress made reducing hot spot traffic especially hot spot traffic due synchronization techniques include separate synchronization networks possibly combining 18 hotspotfree software algorithms use distributed data structures 20 furthermore flow control mechanisms may useful especially hot senders processors usually high request rates high favoritism hot spot factor evaluating alternative techniques reducing hot spot traffic outside scope paper instead investigate significance effects due hot spots performance type architecture parameter value description n 1024 number processors memories system number memory banks branching factor n level rxmy r1m10 r4m60 ratio ring memory cycles processor cycles 18 maximum number outstanding transactions request rate r 07 probability cache miss read 09 10 cluster probability clusters 1 1023 cluster size clusters f 000025 favorite memory probability table 1 system workload parameters used simulations value ranges section present results experiments primary performance metric used processor efficiency since reflects overall system performance understand differences identified processor efficiency number secondary metrics examined include mean transaction latency mean remote transaction latency accesses directed nonlocal memory mean local transaction latency relatively constant network traversal contention avoided local transactions priority remote transactions also consider memory ring utilizations ring utilization report average utilization rings level largest average utilization since ring level dominant performance effect ranges input parameter values shown table 1 systems con sideration system size 1024 processors probability transaction read 07 size cluster 1 1 1 system parameters studied system topology maximum number outstanding transactions per processor use blocking versus nonblocking reads number memory banks per memory module relative speed processor ring memory refer latter cycle ratio specify rxmy means ring cycle x times slow processor cycle memory requires processor cycles service one memory access workload parameters studied request rate communication locality presence hot spots simulation results reported section confidence interval halfwidths 1 less 90 confidence level except near saturation confidence interval halfwidths sometimes increase percent section 41 describes base system performance different request rates degrees communication locality following sections examine issues mentioned section 1 section 42 consider effects increasing maximum number outstanding transactions whether reads block section 43 presents results use multiple memory banks effect variations ring topology considered section 44 section 45 considers hot spot traffic finally section 46 considers changes relative speeds processors memories rings 31 base system performance base system variable parameters fixed follows system topology 16 4 4 2 2 one memory bank per memory module 1 one outstanding transaction per processor transactions including reads block cycle ratio r2m30 ring cycle time twice long processor memory requires processor cycles per access cycle ratio chosen based nearterm expected timings particular highperformance processors obtaining cycles times order 5ns dec alpha 11 although ieee sci standard specifies ring cycle time 2ns assume ring cycle time 10ns define ring cycle time time required packet move input one station input next station transfer need occur single ring cycle example recent performance study sci ring 26 assumes contention four ring cycles packet traverse station link next station assumption memory cycle takes cycles follows values used recent studies 10 figure 2a shows efficiency varies request rate base system cluster probabilities varied request rate increases efficiency drops sharply less 40 request rate 005 even 95 accesses go local memory high degree locality cluster 1 probability 90 higher memory utilization maximum ring utilization far saturation hence noncontention component transaction latency primary cause decline efficiency words efficiency low transaction latency exposed cluster 1 probabilities less 90 ring contention level 4 rings one level root ring becomes substantial enough effect latency thus decrease efficiency next considered effect cluster size base system several different cluster 1 probabilities 08 figure 2b plots efficiency function request rate case p 09 increasing cluster 2 size 4 16 32 thus spreading nonlocal accesses wider range minimal effect efficiency cluster 1 probabilities considered one reason invariance base case level 1 ring contains transaction target memory level 1 ring source processor imposes load regardless logical distance target memorys processor source processor consequently primary cause increased load increasing cluster 2 size 4 16 due increased traffic adjacent level 1 rings hand increasing cluster 2 size 1023 causing nonlocal transactions uniformly distributed across machine major effect efficiency due ring saturation primarily level 3 rings even high locality request rate causes maximum ring utilization 90 080 maximum ring utilization 100 even request rate 001 conclude base system processor efficiency quite low high request rates cluster 1 probabilities 90 cluster 2 sizes order 4 32 processors low efficiency due long contentionfree latency exposed stems limit one outstanding transaction per processor cluster 1 probabilities 90 cluster 2 size order system size considered cluster 1 probability ring contention becomes factor increasing transaction latency examine techniques address causes low efficiency vary cluster probability theta theta theta theta f f c e request rate b vary cluster2 size theta theta theta theta figure 2 base system experiments figure figure 2b 32 maximum outstanding transactions nonblocking reads next examine two techniques increasing efficiency presence long exposed latency increasing maximum number outstanding transactions allowing nonblocking reads assumptions base system cluster sizes cluster probabilities 10 effects allowing maximum number outstanding transactions 1 2 4 6 8 without reads blocking described first experiment shown varied request rate different values assuming blocking reads might reflect example single context using relaxed memory consistency model increasing 1 2 causes small increase efficiency 1 absolute change increases significant effect given 70 transactions reads surprising increasing limited effect remainder study therefore considers nonblocking reads figure 3a plots efficiency versus request rate different values assuming nonblocking reads increasing effective substantially improving efficiency approximately 75 t4 request rate 005 however increases traffic hence contention turn increases latency thus reducing effectiveness increasing reduce exposed latency cases t6 t8 simulations complete high request rates excessive number packets system arising severe contention figure 3b shows mean transaction latency processor cycles increases different values request rate increases figure 4 shows excessive memory utilization primary cause contention increase ring utilizations higher request rates indirect result memory contention increased packet traffic arises increase packet retries turn due requests theta theta theta theta l c l e request rate b theta theta figure 3 varying nonblocking reads turned back saturated memories retries doubling request rate increase ring traffic 10 conclude increasing maximum number outstanding transactions given nonblocking reads effective increasing efficiency communication locality considered memory saturation limits efficiency improvements 33 multiple memory banks next consider dividing memory module multiple memory banks means decreasing memory utilization system considered initially considered previous subsection varying number memory banks 8 first series experiments varies number memory banks case fixed number outstanding 4 varied communication locality considering case considering case shown going 1 2 memory banks effect processor efficiency efficiency request rate 005 increases 56 64 adding memory banks little effect efficiency since exposed part contentionfree transaction latency limiting factor efficiency memory contention 4 hand enough concurrency contentionfree transaction latency hidden memory contention limiting factor ranges considered shown figure 5a displays efficiency versus request rate different numbers memory banks efficiency increased request rate 005 90 4 memory banks 4 cause improvement reduction memory utilization shown e u l request rate a33 theta theta theta x r u l request rate b theta theta theta theta figure 4 varying nonblocking reads figure 5b results similar efficiency somewhat higher 95 request rate memory maximum ring utilizations change percent figure 5c plots maximum ring utilization function request rate ring utilization still low enough little effect transaction latency higher request rates approaching levels effects 1 2 memory banks ring utilization constrained higher request rates memory contention higher number memory banks increases processor efficiency removing memory bottleneck turn increases offered load network thus ring utilization however increase ring utilization due packet retries seen figure 4b longer present increase ring utilization shown figure 5c identifies fundamental tradeoff maintaining small value improving processor efficiency increasing potential increasing contention increasing memory ring utilization consequently increasing order reduce exposed latency point provide minimal improvement thus tradeoff choose value small possible still achieving almost possible improvement processor efficiency experiments indicate 095 08 10 value would good balance tradeoff investigate tradeoff point sensitivity degree communication locality examined results shown indicate tradeoff point highly sensitive request rates 004 002 higher cause almost 100 maximum ring utilization request rates 002 001 higher cause 100 maximum ring utilization cases level 4 rings always f f c e request rate b6 theta theta theta theta theta u l request rate b33 theta theta theta theta x r u l request rate c3 theta theta theta figure 5 varying b highest utilization clearly degrees communication locality lower needed given sensitivity relative communication locality adaptive level algorithm clearly desirable one decentralized approach processor decrease observes latency transactions higher expected indication high network memory contention likewise processor increase level actual latency transactions low contentionfree latency would alternative approach using centralized scheme allows level adjusted systemwide avoids potential fairness problems decentralized approach introducing coordination overhead discussion shows memory saturation major limiting factor increased processor efficiency use multiple memory banks effective technique remove bottleneck increase offered load due multiple outstanding transactions removal memory saturation causes ring utilization increase saturate reasonable traffic patterns appropriate number outstanding transactions tradeoff concurrency contention highly sensitive degree communication locality adaptive maximum number outstanding transactions may useful adjusting tradeoff communication locality changes 34 ring topology ring utilization also affected topology ring hierarchy evaluate significance ring topology first analyzed maximum transaction latency ring hierarchy access goes distant memory request response packets must traverse levels going source target memory back request response subtransactions considered together traversed ring completely traversed exactly complete traversal ring branching factor l takes l 1 ring cycles ring root ring one cycle l child interring interface station plus one parent interring interface complete traversal root ring branching factor l takes ring cycles thus contentionfree maximum transaction latency ml configuration r c l e number levels 1 level 1054 figure latency ring cycles optimal topology number ring levels mem 3006081 l z e number levels theta theta theta theta theta theta theta theta figure 7 comparison normalized contentionfree maximum transaction latency normalized mean remote transaction latency traffic pattern mem memory cycle time ring cycles computed ml number different topologies topologies number levels chose one lowest ml value plotted figure 6 although memory latency 30 assumed order consistent r2m30 cycle ratios used simulations mem simply causes constant shift optimum topology 5 levels 4 given number levels topologies best ml values minimize large branching factors except possibly larger branching factor root since root ring traversed relative performance topologies different numbers levels determined balancing two factors 1 larger branching factors disadvantage requiring packet traverse extra links reach next level 2 larger number levels disadvantage increasing total number interring interfaces system although analysis shows balanced topologies best analysis ignores realistic traffic patterns contention analysis represents worst case accesses memories contentionfree transaction latency maximum traffic pattern cause ring saturation unless request rate extremely low unrealistic next set experiments consider traffic patterns realistic previous experiments indicate probably cause network heavily utilized saturated preliminary comparison considered topologies best ml value level used table 2 ring topologies considered different communication localities index denotes topology figure 8 traffic pattern 4 request rate normalized results worst mean remote transaction latency topology plotted figure 7 qualitative behavior alternative topologies similar normalized ml values topologies 4 6 levels tend superior figure 7 shows high traffic loads levels hurt performance substantially low traffic loads many levels also hurts performance request rate 005 large number levels degrade performance invariance due root ring contention best topologies ml value large branching factor root topologies root ring contention masks performance changes due large number levels topologies 4 6 levels appear perform better examined detail performance alternative topologies many levels three traffic patterns three traffic patterns considered 1 2 3 besides topologies ml analysis indicated optimal considered several others somewhat larger branching factors lower ring levels rationale previous experiments indicated higher ring levels highest utilization consequently reducing branching factor higher ring levels might advantageous table 2 lists topologies considered index associated topology table 2 denotes topology figure 8 topologies indices 13 best number levels ml values figure 8 plots maximum ring utilization investigated topologies three traffic patterns best topologies measured ml perform especially well baseline topology 16 4 4 2 2 using earlier experiments performs well relative performance depends traffic pattern topologies numbered seem acceptable choice none clearly superior x r u l topologies a2060100 x r u l topologies b2060100 x r u l topologies c figure 8 comparison maximum ring utilizations alternative topologies three traffic patterns plots topologies patterns listed table 2 conclude best contentionfree topology measured maximum transaction latency one least sensitive contention 4 6 levels best topologies tend uniform branching factors different levels except somewhat larger branching factors lower levels also well relative performance best topologies depends traffic patterns also performance measure mean remote transaction latency maximum ring utilization 35 hot spot traffic results presented far based communication locality model traffic within cluster uniformly distributed subsection examine behavior system presence hot spot perform analysis assume transaction processor probability f addressing hot spot memory remaining transactions distributed according standard communication locality model moreover assume 095 08 10 experiments reported far simulated system memory queue length 9 hot spot traffic four memory banks queue length sufficient queue overflow extremely unlikely traffic loads saturate memory possible hot spot longer memory queue needed consequently section consider queue lengths 9 36 figure 9a evaluates effect hot spot overall system performance plotting mean remote transaction latency remote transactions system versus request rate different favorite memory probabilities memory queue length 9 request rate needed cause latency significantly increase depends greatly favorite memory probability clear favority memory probabilities 1 supportable reasonable request rates respect overall system performance figure 9b uses queue length 36 change queue length effect qualitative behavior degradation performance figure 9 large critical request rate reached negative feedback effect packet retries favoritism hot spot memory imposes additional load three resources hot spot memory queue hot spot memory network near hot spot memory contention hotspot memory increases access request packets destined hot spot memory negatively acknowledged resent experiments indicate three resources contribute performance degradation hot spot memory clearly factor due high utilization actually saturate queue lengths 9 36 workloads cause mean remote latencies hundreds hot memory utilizations 85 98 memory queue overflow factor since memory queue 36 lower mean remote latencies higher hot memory utilizations memory queue 36 network saturation rings near hot memory factor measurements queues iris near hot memory show long queues mentioned section 2 goal respect hot spots understand significance within class systems instead evaluating alternative solutions without techniques alleviate hot spots experiments indicate systems become unstable favorite memory probabilities order 1 2 reasonable request rates memory queues inadequate length significantly lower favorite memory probabilities cause insta bility techniques proposed synchronization literature separate synchronization networks combining software algorithms using distributed data structures 20 18 may well reduce likelihood hot spots simulated system provide flow control number cycles source processing module submits retry function number retries previously sent 27 sophisticated flow control mechanisms could considered one possibility destination module could return negative acknowledgement indication congested source module could use information choose wait period 36 relative speed far assumed fixed ratio processor memory ring speeds namely r2m30 section examine effect varying ratio conclusions drawn previous sections since processor speeds seem increasing faster memory speeds three cases considered processor speed increased 100 memory cycle remains unchanged thus cycles first case assumes ring cycle time remains unchanged 4 processor cycles second case assumes 50 reduction third case assumes 75 reduction ring cycle time cases denoted r4m60 r2m60 r1m60 respectively assume figure 10a plots efficiency versus request rate three cases well r2m30 base case faster processor causes efficiency drop somewhat comparison r2m30 surprisingly drop essentially identical three cases noting high efficiency r2m30 important remember relatively speaking processor case executing instructions half speed cases figure 10b plots maximum ring utilization versus request rate cases shows similar behavior respect efficiency masks major differences respect ring utilization little difference among cases respect memory utilization fact ring saturation r4m60 suggests processor efficiency case start dropping offered load increased test hypothesis redid experiment figure 11a figure 11b plots efficiency maximum ring utilization respectively versus request rate communication l c l e request rate f10 22 f15 theta l c l e request rate b f10 22 f15 theta theta figure 9 hot spot large request rate supported different favorite memory probabilities overall system performance degrades measured mean remote transaction latency figure 9a memory queue 9 deep figure 9b memory queue 36 deep10305070900 f f c e request rate theta theta theta theta10305070900 001 x r u l request rate b33 theta theta theta theta figure 10 effect different processor ring speeds theta theta theta theta10305070900 001 x r u l request rate b33 theta theta theta theta figure 11 effect different processor ring speeds 10 simulation r4m60 aborts due saturation efficiency maximum ring utilization points reported case average batches completed locality results confirm hypothesis sharp drop processor efficiency r4m60 request rate 003 collapse performance increasing request rate beyond 002 sharp request rate 003 simulation complete point reported mean batches simulation aborts explicitly considered faster ring cycle time experiments r1m60 results figure 10 figure 11 plots make clear faster ring cycle time would little effect since r1 memory limiting factor ring utilization 3 returning base case considered effect assumption memory cycle equals cycles one conclusions increased hide exposed transaction latency memory saturation became limiting factor used multiple memory banks allow higher processor efficiency return one memory bank see sensitive results memory cycle time assume 095 08 10 ring cycle equals 2 processor cycles vary memory cycle time 10 40 processor cycles figure 12a plots efficiency versus request rate memory cycle time varied increases efficiency drops partially due increase base transaction latency partially due increased contention understand degree contention contributes drop efficiency plot memory maximum ring utilization versus request rate figure 12b figure 12c respectively memory cycle time increases memory utilization increases maximum ring utilization decreases longer memory cycle times high 3 ringbased systems ksr1 8 6 ring cycle time faster processor cycle time often systems ksr1 require several ring cycles transaction pass ring node m40 theta theta theta theta u l z request rate b33 theta theta theta x r u l request rate theta theta theta figure 12 effect different memory cycle times request rates memory saturates maximum ring utilization constrained memory saturation case significantly differs larger values memory utilization less maximum ring utilization appears significant transition increases 10 involves whether memory utilization becomes bottleneck rings become bottleneck higher values multiple memory banks useful increasing efficiency lower values multiple memory banks useful increasing efficiency since ring saturation dominates since little room improvement efficiency given raised adequately efficiency close 100 figure 12 37 block transfers increase ratio memory cycle time processor cycle time motivated development innovative memory technologies 24 21 one approach examine pagemode access pagemode access fetching first word memory requires raising rowaccess strobe line columnaccess strobe line however subsequent words row memory retrieved changing column address reraising column access strobe line thus access time first word unchanged additional access time subsequent words row sharply reduced pagemode access natural mechanism support block transfer result pagemode access increase memory bandwidth reduce average latency measured words fetched memory assume first word fetched cache miss subsequent words prefetches use pagemode access advantageous extent cache hit ratio increased spatial locality prefetching pagemode access disadvantages memory network utilization increase increased potential cache pollution extent prefetched words referenced cause replacement words referenced addition processor stalls last word fetched reaches processor instead first word fetched stall time cache misses increases extent use pagemode access increases cache hit ratio highly program dependent scope study however determine extent cache hit ratio needs increase order compensate disadvantages pagemode access conducted several experiments determine needed hit ratio increase request rates 001 005 experiments fixed ring topology ratio processor ring memory speeds base case respectively assumed four memory banks hot spot traffic word block first word takes 5 processor cycles considered four cases communication locality 08 four communication localities considered four outstanding transactions also considered one outstanding transaction considered four block sizes 1 2 4 8 words block sizes assumed processor stall ends first word reaches processor communication localities maximum numbers outstanding transactions consid ered increasing block size degrades performance request rates measures processor efficiency mean request latency measured processor stall ends memory utilization maximum ring utilization percentage degradation depends measure memory maximum ring utilizations fundamental sense proximity saturation determine contention components processor efficiency request latency since maximum ring utilizations substantially higher memory utilizations experiments chose report change maximum ring utilization characterize change percentage mean number processor cycles cache misses must increase order maximum ring utilization block size b equal maximum ring utilization block size 1 thus implicitly identifying necessary change cache hit ratio results communication locality plotted figure 13 behavior surprising needed increase number processor cycle cache misses increasingly close linearly independent request rate maximum number outstanding transactions behavior shown three communication localities similar percentage increases also quite large example experiments one case block size two words requires increase 13 cases block size two words requires increase 33 90 whether increase block size two words would cause improvement depends program cache characteristics seems doubtful increasing block size two words would require much additional improvement seems even doubtful experiments assume processor stall ends first word block reaches processor 8 word block size repeated experiments considering case processor stall ends last word block reaches processor comparison 8 word block size processor stall ending first word returns change minor memory maximum ring utilizations decrease percentage points expected mean remote request latency increases extra length remaining words block plus contention factor processor efficiency drops percent conclude using block transfers pagemode dram access appear promising across many communication localities request rates maximum numbers outstanding transactions block sizes extra traffic resulting fetching extra words significantly raises network utilization measured maximum ring utilization improvement cache hit ratio due larger block size program cache dependent however improvement c l e e e block size t4 theta theta theta c l e e e block size b t1 theta theta theta figure 13 effect increasing block size pagemode dram access effect indicated percentage mean number processor cycles cache misses must increase order maximum ring utilization block size b equal maximum ring utilization block size 1 communication locality cache hit ratio needed compensate increase maximum ring utilization large enough doubtful improvement achieved result specific large systems type network might apply smaller systems systems types networks conclusions 41 related work beside hector several architectures based slotted rings proposed including cdc cyberplus 14 express ring 5 ieee sci scalable coherent interface standard 17 22 ksr1 kendall square research 8 6 performance studies single slotted rings ringbased hierarchies previous studies sharedmemory architectures system models level detail simulator tended examine small systems 100 processors less 31 9 performance branching factor topologies considered bus hierarchies experiments vernon jog sohi 28 indicate best topologies large branching factors low levels small branching factors near root contrast contentionbased experiments indicate ringbased systems topologies close balanced branching factors best somewhat larger branching factors lower ring levels sometimes well large branching factors low ring levels perform poorly recent analytical study agarwal examines effectiveness multithreading increasing processor efficiency 4 relevant since multithreading one main approaches allowing multiple outstanding transactions however system workload characteristics agarwal considers significantly different network kary ncube memory access time 10 processor cycles traffic uniform memory equally likely target memory cache miss cache miss ratio function number threads context switch overhead nonzero largest difference role memory agarwal model take memory contention account determining transaction latency even model significance memory saturation occurring network saturation thus importance techniques reduce memory utilization would observed due memory access time 10 cycles see figure 12 42 summary paper presented results simulation study assess performance sharedmemory multiprocessor based hierarchical network rings ring based systems interest run fast rates due use short unidirectional pointto point connections use simple node interfaces interring connections hierarchical systems interest scalable ensure systems simulated realistic realizable based specific system hector ensure simulator correct captures subtle system interactions simulator validated using measurements prototype system tracedriven simulation would produce questionable results executiondriven simulation using complete applications would take prohibitively long simulating system large level detail desired realism consequently introduced synthetic workload model using parameter value ranges based experience applications workload model shown systems order 1024 processors evaluated even network memory simulation detailed level main results study ffl without high degree locality data accesses applications running type system ring contention cause memory access latency increase significantly locality achieved proper data placement migrating replicating data objects ffl processors support multiple outstanding transactions prefetching multiple hardware contexts release consistency processor efficiency low due long memory access transaction latency even contention ffl multiple outstanding transactions memory quickly saturate one memory bank per processing module using multiple memory banks per processing module effective way reduce memory contention ffl necessary limit number multiple outstanding transactions per processor order limit network contention appropriate number tradeoff concurrency contention found sensitive degree communication locality proposed adaptive approach adjust tradeoff communication locality changes ffl respect topology found 1024 processor systems 4 6 levels hierarchy tend perform better although one topology best also found wellbalanced systems similar branching factor levels tend perform best slightly smaller rings root hierarchy reduce potential congestion point ffl saturation hot spot memories causes substantial degradation overall system performance 1 memory traffic targeted single memory ffl doubling processor speed keeping memory cycle time constant causes drop processor efficiency due increased relative transaction latency traffic patterns considered changes ring speed major effect maximum ring utilization effect processor efficiency sensitive communication locality ffl varying memory cycle time significantly effects processor efficiency memory utilization maximum ring utilization notably one memory bank presence memory saturation offered loads cause ring saturation occurs memory cycle time 20 processor cycles longer ffl prefetching use block transfers imposes additional load network improvement cache hit ratio needed compensate increase maximum ring utilization large enough make doubtful prefetching advantageous issues warrant investigation include effect synchronization flow control mechanisms hot spot traffic effectiveness proposed adaptive maximum number outstanding transactions use dram techniques compensate long memory access times including traffic pattern page migration replication transactions acknowledgements thank darrell kindred work early version simulator keith farkas thorough comments draft paper r comparison hardware software cache coherence schemes performance analysis multiprocessor mesh interconnection networks wormhole routing limits interconnection network performance performance tradeoffs multithreaded processors cache coherence slotted ring ultracomputers teraflop time improved multithreaded techniques hiding communication latency multiprocessors overview ksr 1 computer system impact synchronization granularity parallel systems reducing memory latency via nonblocking prefetching caches dec developed alpha multiprocessor simulation tracing using tango cache consistency hierarchicalringbased multi processors cyberplus map v interprocessor communications parallel array processor systems hiding memory latency using dynamic scheduling sharedmemory multiprocessors comparative evaluation latency reducing tolerating techniques scalable coherent interface related standards projects effective synchronization network hot spot accesses simulating computer systems techniques tools synchronization without contention fast computer memories ballot review committee ieee microprocessor standards committee hot spot semiconductor memories multicomputer networks messagebased parallel processing performance sci ring experiences hector multiprocessor performance analysis hierarchical cacheconsistent multiprocessors hector hierarchically structured sharedmemory multiprocessor distributing hotspot addressing largescale multipro cessors performance study memory consistency models tr simulating computer systems techniques tools distributing hotspot addressing largescale multiprocessors multicomputer networks messagebased parallel processing performance analysis hierarchical cacheconsistent multiprocessors hector synchronization without contention comparative evaluation latency reducing tolerating techniques comparison hardware software cache coherence schemes ultracomputers teraflop time dec developed alpha performance study memory consistency models hiding memory latency using dynamic scheduling sharedmemory multiprocessors improved multithreading techniques hiding communication latency multiprocessors performance sci ring reducing memory latency via nonblocking prefetching caches effective synchronization network hotspot accesses cache consistency hierarchicalringbased multiprocessors impact synchronization granularity parallel systems scalable coherent interface related standards projects limits interconnection network performance performance tradeoffs multithreaded processors ctr v carl hamacher hong jiang hierarchical ring network configuration performance modeling ieee transactions computers v50 n1 p112 january 2001 jong wook kwak chu shik jhon torus ring improving performance interconnection network modifying hierarchical ring parallel computing v33 n1 p220 february 2007 karim harzallah kenneth c sevcik predicting application behavior large scale sharedmemory multiprocessors proceedings 1995 acmieee conference supercomputing cdrom p53es december 0408 1995 san diego california united states fadi n sibai performance hyperring multicomputer proceedings 1998 acm symposium applied computing p598606 february 27march 01 1998 atlanta georgia united states gang han robert h klenke james h aylor performance modeling hierarchical crossbarbased multicomputer systems ieee transactions computers v50 n9 p877890 september 2001 fadi n sibai optimal clustering hierarchical hyperring multicomputers journal supercomputing v14 n1 p5376 july 1999