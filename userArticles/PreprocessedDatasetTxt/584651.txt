learning recursive bayesian multinets data clustering means constructive induction paper introduces evaluates new class knowledge model recursive bayesian multinet rbmn encodes joint probability distribution given database rbmns extend bayesian networks bns well partitional clustering systems briefly rbmn decision tree component bns leaves rbmn learnt using greedy heuristic approach akin used many supervised decision tree learners bns learnt leaves using constructive induction key idea treat expected data real data allows us complete database take advantage closed form marginal likelihood expected complete data factorizes separate marginal likelihoods family node parents approach evaluated synthetic realworld databases b introduction one main problems arises great variety elds including pattern recognition machine learning statistics socalled data clustering problem 1 3 7 14 15 22 25 data clustering viewed datapartitioning problem partition data dierent clusters based quality similarity criterion eg kmeans 30 alternatively data clustering one way representing joint probability distribution database assume addition observed predictive attributes hidden variable unobserved variable ects cluster membership every case database therefore data clustering problem also example learning incomplete data due existence hidden variable incomplete data represents special case missing data missing entries concentrated single hidden variable refer given database incomplete classication given parameter estimation model comparison classical bayesian statistics provide solution data clustering problem frequently used approaches include mixture density models eg gaussian mixture models 3 bayesian networks eg autoclass 8 aim automatically recover joint probability distribution given incomplete database learning recursive bayesian multinets rbmns roughly recursive bayesian multinet decision tree 4 44 decision path ie conjunction predictive attributevalue pairs ends alternate component bayesian network bn 6 24 29 38 rbmns natural extension bns conditional independencies encoded bn contextnonspecic conditional independencies rbmns allow us work contextspecic conditional independencies 21 49 dier decision path decision path heuristic approach learning rbmns requires learning component bns incomplete data last years several methods learning bns arisen 5 12 23 37 48 learn incomplete data 9 17 33 39 40 49 describe bayesian heuristic algorithm learning bns data clustering developed pena et al 39 extended learn rbmns key step bayesian approach learning graphical models general bns particular computation marginal likelihood data given model quantity ordinary likelihood data averaged parameters respect prior distribution dealing incomplete data exact calculation marginal likelihood typically intractable 12 thus computation approximated 11 existing methods rather inecient purpose eliciting rbmn incomplete database since factorize scores families ie nodes parents hence would recompute score whole structure anew although factors families changed avoid problem use algorithm developed 39 based upon work done 49 search parameter values initial structure means em algorithm 13 31 means bcem method 40 allows us complete database using current model treating expected data real data results possibility using score criterion closed form remainder paper organized follows section 2 describe bns bayesian multinets bmns rbmns data clustering section 3 dedicated heuristic algorithm learning component bns incomplete data section 4 describe algorithm learning rbmns data clustering finally section 5 present experimental results paper nishes section 6 draw conclusions outline lines research bns bmns rbmns data clustering 21 notation follow usual convention denoting variables uppercase letters states letters lowercase use letter letters boldface uppercase designate set variables boldface lowercase letter letters denote assignment state variable given set jxj used refer number states variable x use px j denote probability also use px j denote conditional probability distribution mass function restrict discussion case variables discrete x given whether px j refers probability conditional probability distribution clear context mentioned facing data clustering problem assume existence ndimensional random variable x partitioned n 1dimensional random variable predictive attributes unidimensional hidden variable c cluster variable 22 bns data clustering given ndimensional variable c bn 6 24 29 38 x graphical factorization joint probability distribution x bn dened directed acyclic graph b model structure determining conditional independencies among variables x set local probability distributions b contains arc variable x j variable x x j referred parent x denote pab set parents variable x b structure lends factorization joint probability distribution x follows pab denotes state parents x pab consistent x local probability distributions bn equation 1 assume depend nite set parameters b 2 b therefore equation 1 rewritten follows b h denotes hypothesis conditional independence assertions implied b hold true joint probability distribution x obtain equation 2 according partition x equation 3 rewritten follows prepab denotes state parents correspond predictive attributes consistent thus bn completely dened pair b b rst two components model structure second component set parameters local probability distributions corresponding b see figure 1 example bn structure data clustering predictive attributes 23 bmns data clustering conditional independencies determined structure bn called context nonspecic conditional independencies 49 also known symmetric conditional independencies 21 structure implies two sets variables independent given conguration state third set variables two rst sets also independent given every conguration third set figure 1 example structure bn data clustering c follows gure joint probability distribution factorizes variables bmn 21 generalization bn model able encode contextspecic conditional independencies 49 also known asymmetric conditional independencies 21 therefore bmn structure may imply two sets variables independent given conguration third set dependent given another conguration third set formally bmn distinguished variable g 2 graphical factorization joint probability distribution x bmn dened probability distribution g set component bns xnfgg encodes joint probability distribution xnfgg given state g structure component bn may vary bmn encode contextspecic conditional independence assertions paper limit distinguished variable g one original predictive attributes however 49 allows distinguished variable either one predictive attributes hidden cluster variable c last happens leaf represents single cluster models called mixtures bns according 49 figure 2 shows structure bmn data clustering distinguished variable g two values let denote structure parameters bmn x distinguished variable g addition let us suppose b g g denote structure parameters gth component bn bmn also let h denote hypothesis contextspecic conditional independencies implied hold true joint probability distribution x distinguished variable g therefore joint probability distribution x encoded bmn given denotes parameters bmn b h g shorthand conjunction h last term previous equation factorized according structure gth component bn bmn equation 4 thus bmn completely dened pair rst two components structure bmn second component set parameters may see bmn depthone decision tree 4 44 distinguished variable root branch states end y5 52y figure 2 example structure bmn data clustering distinguished variable two component bns distinguished variable dichotomic jy 5 j2 dotted lines correspond distinguished variable 5 branches leaf component bn thus helpful see dotted lines figure 2 conforming decision tree component bns leaves 24 rbmns data clustering let us follow view bmn depthone decision tree leaves component bns propose use deeper decision trees leaves still component bns denition every component bmn limited bn rbmn allows every component either bn leaf recursively rbmn rbmns extend bns bmns rbmns also extend partitional clustering systems 16 rbmns considered extensions bns like bmns mixtures bns rbmns allow us encode contextspecic conditional independencies thus constitute exible tool bns provide user structured specialized domain knowledge alternative component bns learnt every decision path moreover rbmns generalize idea behind bmns oering possibility decision paths conjunctions many predictive attributevalue pairs want constraint decision paths must represented decision tree additionally rbmns extend traditional partitional clustering systems previous work aim 16 fisher hapanyengwi propose perform data clustering based upon decision tree measure used select divisive attribute node decision tree construction consists computation sum information gains attributes supervised paradigm measure limited information gain single specied class attribute natural generalization works supervised learning performance task comprises prediction one attribute knowledge many whereas generic performance task unsupervised learning prediction many attributes bn bn bn bn 32distinguished decision tree bns component figure 3 example structure 2levels rbmn data clustering distinguished decision tree rbmn two component bmns two component bns assuming variables distinguished decision tree dichotomic dotted lines correspond distinguished decision tree knowledge many thus rbmns work fisher hapanyengwi aim learn decision tree knowledge leaves sucient making inference along many attributes implies paradigms considered extensions traditional partitional clustering systems concerned characterizing clusters observations rather partitioning dene rbmn according intuitive idea decision tree component bns leaves let decision tree referred distinguished decision tree every internal node represents variable ii every internal node many children branches coming states variable represented node iii leaves level iv troot l set variables decision path root leaf l distinguished decision tree repeated variables troot l condition iii imposed simplify understanding rbmns learning constraint removed practice let us dene xntroot l set variables x except decision path root leaf l distinguished decision tree thus rbmn distinguished decision tree graphical factorization joint probability distribution x rbmn dened probability distribution leaves set component bns encodes joint probability distribution xntroot l given lth leaf thus component bn every leaf distinguished decision tree consider attributes involved tests decision path leading leaf obviously bmns special case rbmns distinguished decision tree one internal node distinguished variable moreover could assume bns also special case rbmns distinguished decision tree contains internal nodes figure 3 helps us illustrate structure rbmn data clustering decision tree internal node predictive attribute branches node states variable every leaf l component bn consider attributes troot l induction component bns simplied since every internal node predictive attribute hidden variable c appears every component bn fact implies component bn leaf represent one cluster fisher hapanyengwi propose 16 context specic data clustering data clustering encoded component bn totally unrelated data clusterings encoded rest means probabilistic clusters identied component bn correspondence identied rest component bns due fact c acts contextspecic local hidden cluster variable every component bn exact every variable component bn contextspecic variable interact variables component bn since elicitation every component bn totally independent rest explicitly ected notation every branch identies unambiguously component bn variables additionally avoids complex notation reasoning also applied bmns special case rbmns let denote structure parameters rbmn x distinguished decision tree addition let us suppose b l l denote structure parameters lth component bn rbmn also let h denote hypothesis contextspecic conditional independencies implied hold true joint probability distribution x distinguished decision tree therefore joint probability distribution x encoded rbmn given l leaf l one makes x consistent troot l denotes parameters rbmn l number leaves b h l shorthand conjunction h troot last term previous equation factorized according structure lth component bn rbmn equation 4 thus rbmn completely dened pair rst two components structure rbmn second component set parameters paper limit discussion case component bns dened multinomial distributions variables nite discrete variables local distributions variable component bns consist set multinomial distributions one conguration parents addition assume proportions probabilities data covered leaves follow also multinomial distribution stated rbmns extend bns due ability encode contextspecic conditional independencies increases expressive power rbmns bns decision tree eectively identies subsets original database dierent component bns result better exible way data works supervised induction identify instance subspaces local component models kohavi 27 links naive bayes nb classiers decision tree learning hand work done zheng webb 50 combines previous work kohavi lazy learning algorithm build bayesian rules antecedent conjunction predictive attributevalue pairs consequent nb classier thus works share fact use conjunctions predictive attributevalue pairs dene instance subspaces described nb classiers zheng webb 50 give extensive experimental comparison two approaches supervised learning wellknown domains langley 28 proposes identify instance subspaces independence assumptions made nb classier hold work based upon recursive split original database using decision trees nodes nb classiers leaves sets cases belonging one class illustrate rbmns structure clustering given database use realworld domain data clustering successfully performed means probabilistic graphical models 41 aim improving knowledge geographical distribution malignant tumors geographical clustering towns autonomous community basque country north spain performed every town described agestandardized cancer incidence rates six frequent cancer types patients sex 1986 1994 authors obtained geographical clustering male patients geographical clustering female patients dierences geographical patterns malignant tumors patients sex wellknown experts clusterings achieved means learning bn nal clusterings presented using colored maps partition towns way town assigned probable cluster according learnt bn ie town assigned cluster highest posterior probability due dierent geographical patterns male female patients seems quite reasonable assume rbmn would eective automatic tool face referred realworld problem without relying human expertise learning rbmn would able automatically identify instance subspace male patients encodes underlying model dierent one encoded instance subspace female patients however authors relied human expertise divide original database treat separately male female cases figure 4 shows rbmn ideally would learnt structured clustering obtained model easy see clusterings obtained male female patients dierent well contextspecic furthermore 41 reports characterization cluster completely dierent male female patients dierences geographical patterns captured learning bns original joint database example figure 4 also bmn since distinguished decision tree contains one predictive attribute however easy see rbmn might represent complex decision tree represented specialized clustering example might expect dierent component bns four conjunctions example could encoded rbmn 2levels distinguished decision tree key idea approach learning rbmn data clustering decompose problem learning component bns incomplete data component bn corresponding leaf l learnt incomplete database subset original incomplete database subset contains cases original database consistent troot l therefore still exists hidden variable learning every component bn problem learning rbmn data clustering largely problem learning component bns incomplete data thus following section present heuristic algorithm learning bn incomplete database sexmale sexfemale bn male bn female figure 4 scheme structure rbmn ideally would learnt realworld domain described 41 additionally clusterings encoded component bns shown colored maps white towns excluded study 3 learning bns incomplete data constructive induction section describe heuristic algorithm elicit component bns incomplete data use heuristic algorithm part algorithm learning rbmns data clustering present following section 31 component bn structure due diculty involved learning densely connected bns painfully slow probabilistic inference working desirable develop methods learning simplest bns data adequately examples trade cost learning process quality learnt models nb models 14 43 extended naive bayes enb models 36 37 39 42 tree augmented naive bayes models 18 19 20 26 32 40 despite wide recognition models weaker representation domains general bns expressive power models often acceptable moreover models appeal human intuition learnt relatively quickly sake brevity class compromise bns propose learn component bns referred enb 42 enb models introduced pazzani 36 37 bayesian classiers later used pena et al 39 42 data clustering enb models considered intermediate place nb models nb model fully correlated model enb model 3 5 figure 5 component bn enb model structure propose learn seen place nb models fully correlated models applied data clustering problem models predictive attributes fully correlated see figure 5 thus keep main features extremes simplicity nb models better performance fully correlated models enb models similar nb models since attributes independent given cluster variable dierence nb models number nodes structure enb model shorter original number attributes database reasons selection attributes included models performed ii attributes grouped together node fully correlated attributes refer nodes supernodes 1 therefore class enb models ensures better performance nb models maintains simplicity consider attributes relevant data clustering task perform attribute selection proposed pazzani structure enb model data clustering lends factorization joint probability distribution x follows r partition r number nodes including special nodes referred supernodes z set values original predictive attributes grouped together supernode z value predictive attribute z 32 algorithm learning enb models incomplete data log marginal likelihood often used bayesian criterion guide search best model structure important feature log marginal likelihood reasonable assumptions factorizes scores families criterion factorable search ecient since need reevaluate criterion whole structure factors families changed important feature working heuristic search algorithms iteratively transform model structure choosing transformation improves score usually transformation aect families 1 remainder paper refer set nodes supernodes enb model simply nodes 1choose initial structure initial set parameter values initial structure 2parameter search step 3probabilistic inference complete database 4calculate sufficient statistics compute log pd j b h 5structure search step 6reestimate parameter values new structure 7if change structure done stop else interleaving parameter search step go 2 else go 3 figure schematic algorithm learning component bns enb models incomplete data variable want classify hidden exact calculation log marginal likelihood typically intractable 12 thus approximate computation 11 however existing methods rather inecient eliciting component bns enb models incomplete databases factorize scores families avoid problem use heuristic algorithm presented 39 shown figure 6 first algorithm chooses initial structure parameter values performs parameter search step improve set parameters current model structure parameter values used complete database key idea approach treat expected data real data hidden variable completion means probabilistic inference current model hence log marginal likelihood expected complete data log pd j b h calculated 12 closed form furthermore factorability log marginal likelihood scores families allows performance ecient structure search step structure search algorithm reestimates parameters new structure nds maximum likelihood parameters given complete database finally probabilistic inference process complete database structure search iterated change structure figure 6 shows possibility interleaving parameter search step structural change though interleave parameter structure search experiments follow reasons cost another key point penalty term built log marginal likelihood guard overly complex models 33 similar use builtin penalty term found remainder section describe parameter search step structure search step detail 321 parameter search seen figure 6 heuristic algorithm use considers possibility interleaving parameter structure search steps concretely interleaving process 1for every case database acalculate posterior probability distribution pc j blet p max maximum pc j reached fixing probability threshold assign case cluster c max 2run bc method abound bcollapse 3set parameter values current bn bcs output parameter values 4run em algorithm convergence 5if bcem convergence stop else go 1 figure 7 schematic bcem method done least rst iteration algorithm ensure good set initial parameter values remaining iterations decide whether interleave parameter structure search steps although parameter search procedure considered perform parameter search step currently propose two alternative techniques wellknown em algorithm 13 31 bcem method 40 according 40 bcem method exhibits faster convergence rate eective robust behavior em algorithm bcem method used experimental evaluation rbmns basically bcem method alternates bound collapse bc method 45 46 em algorithm bc method deterministic method estimate conditional probabilities databases missing entries bounds set possible estimates consistent available information computing minimum maximum estimate would obtained possible completions database bounds collapsed unique value via convex combination extreme points weights depending assumed pattern missing data method presents advantages deterministic method dramatic gain eciency compared em algorithm 47 bc method used presence missing data useful hidden variable data clustering problem reason probability intervals returned bc method would large poorly inform missing entries single hidden variable bcem method overcomes problem performing partial completion database step see figure 7 schematic bcem method every case database bcem method uses current parameter values evaluate posterior probability distribution cluster variable c given assigns case cluster highest posterior probability posterior probability greater threshold xing probability threshold user must determine case remains incomplete cluster 1consider joining pair attributes 2if improvement log pd j b h make joint improves score else return current case representation figure 8 template forward structure search step 1consider splitting attribute possible point 2if improvement log pd j b h make split improves score else return current case representation figure 9 template backward structure search step posterior probability greater threshold entries hidden variable completed process hope informative probability intervals running bc method em algorithm executed improve parameter values bc method returned process repeated convergence 322 structure search 10 chickering shows nding bn structure highest log marginal likelihood set bn structures node k parents nphard k 1 therefore clear heuristic methods needed particular choice based upon work done pazzani 36 37 pazzani presents algorithms learning augmented nb classiers enb models searching dependencies among attributes backward sequential elimination joining bsej algorithm forward sequential selection joining fssj algorithm nd attribute dependencies algorithms perform constructive induction 2 35 process changing representation cases database creating new attributes supernodes existing attributes result violation conditional independence assumptions made nb models detected dependencies among predictive attributes included model ideally better performance reached model obtain constructive induction process maintains simplicity nb models pazzani uses term joining refer process creating new attribute whose values cartesian product two attributes carry change representation database pazzani proposes hillclimbing search combined two operators replacing two existing attributes new attribute cartesian product two attributes either delete irrelevant attribute resulting bsej add relevant attribute resulting fssj algorithm learning component bns use figure starts one two possible initial structures nb model model variables fully correlated considering nb model initial structure heuristic algorithm performs forward search step see figure 8 hand starting fully correlated model heuristic algorithm performs backward search step see figure 9 notice taken algorithm figure 6 completed database structure search step performed consequently log marginal likelihood expected complete data factorable closed form algorithm uses factorability log marginal likelihood score every possible change model structure eciently learning rbmns data clustering section present heuristic algorithm learning rbmns data clustering algorithm performs model selection using log marginal likelihood expected complete data guide search section starts deriving factorable closed form marginal likelihood data rbmns 41 marginal likelihood criterion rbmns assumptions variables database discrete ii cases occur independently iii database complete iv prior distribution parameters given structure uniform marginal likelihood data closed form bns allows us compute eciently particular n number variables r number states variable x q number states parent set x n ijk number cases database x kth value parent set x jth value see 12 derivation important result extended bmns 49 follows let ig denote set parameter variables associated local probability distribution ith variable belonging xnfgg gth component bn also let denote set parameter variables corresponding weights mixture component bns parameters variables mutually independent given h parameter independence ii parameter priors p ig j h conjugate g iii data complete marginal likelihood data factorable closed form bmns particular log pd j h log pd xg j b h g data restricted distinguished variable g xg data restricted variables xnfgg cases g term pd g marginal likelihood trivial bn single node g terms sum log marginal likelihoods component bns bmn furthermore observation extends rbmns follows let il denote set parameter variables associated local probability distribution ith variable belonging xntroot l lth component bn also let l denote number leaves denote number levels depth let designate set 1start empty tree l 2while stopping conditionfalse search leafl search leafl 1if l empty tree l leaf extensionl every child ch l search leafch extensionl follows 1for every variable yntroot l bfor every state ik ilet extk database restricted variables ext cases database consistent troot l ik iilearn component bn extk variables ext means constructive induction cscore candidate bmn 2choose extension candidate bmn highest score figure 10 schematic algorithm learning rbmns data clustering parameter variables corresponding weights mixture component bns parameters variables mutually independent given h parameter independence ii parameter priors conjugate l iii data complete marginal likelihood data factorable closed form rbmns particular log pd j h log pd trootl l trootl database restricted variables troot l cases consistent troot l xtrootl database restricted variables xntroot l cases consistent troot l sum rst terms easily calculated log marginal likelihood trivial bn single node many states leaves distinguished decision tree second terms sum log marginal likelihoods component bns rbmn thus assumptions referred factorable closed form calculate 12 therefore log marginal likelihood data closed form rbmns calculated log marginal likelihoods component bns fact allows us decompose problem learning rbmn learning component bns y5 52 11 figure 11 example structure 2levels rbmn data clustering distinguished decision tree dotted lines correspond distinguished decision tree component bn leaf l obtained result improving constructive induction nb model variables xntroot l 42 algorithm learning rbmns incomplete data heuristic algorithm present section performs data clustering learning incomplete data rbmns dened section 24 algorithm starts empty distinguished decision tree iter ation enlarges tree one level stopping condition veried stopping might occur userspecied depth improvement log marginal likelihood expected complete data current model equation 10 observed enlarge current tree every leaf component bn extended extension leaf l consists learning best bmn xntroot l distinguished variable 2 yntroot l bmn replaces leaf l learning component bn bmn use algorithm presented figure 6 figure 11 shows example 2levels rbmn structure could output algorithm present figure 10 last gure see learning algorithm replaces every leaf l best bmn xntroot l distinguished variable done follows let variable yntroot l every state ik algorithm learns component bn b k variables ext incomplete database extk cluster variable still hidden g learning carried heuristic algorithm presented figure 6 database extk subset original database instance subspace fact original database restricted variables ext cases consistent decision path troot l ik process candidate bmn distinguished variable possible extension leaf l given equation 9 provides us closed form log marginal likelihood bmns use score candidate bmn follows log pd xtrootl j h log pd extk j b h xtrootl dened trootl database restricted predictive attribute cases consistent troot l extk dened b h k kth component bmn rst term calculated log marginal likelihood trivial bn single node terms sum calculated using equation 8 possible candidate bmns extending leaf l scored algorithm performs extension highest score 5 experimental results section devoted experimental evaluation algorithm learning rbmns data clustering using synthetic realworld data variables domains considered discrete local probability distributions multinomial distributions experiments assumed real number clusters known thus perform search identify number clusters databases already mentioned currently algorithm learning rbmns incomplete data considers 2 alternative techniques perform parameter search component bns em algorithm bcem method according 40 bcem method exhibits desirable behavior em algorithm faster convergence rate eective robust behavior thus bcem method one used experimental evaluation although aware alternative techniques exist convergence criterion bcem method satised either relative dierence successive values log marginal likelihood model structure less 10 6 150 iterations reached following 40 used fixing probability threshold equal 051 shown algorithm learning rbmns runs algorithm learning component bns large number times runtime latter algorithm kept short possible thus throughout experimental evaluation consider interleaving parameter search step structural change figure 6 though open question whether interleaving parameter structure search would yield better results prior experiments 39 suggest interleaved search domains however yield better results reason considered forward structure search step figure 8 thus initial structure component bn always nb model decisions made based upon results work done 39 51 performance criteria section describe criteria table 1 use compare learnt models evaluate learning algorithm log marginal likelihood criterion used select best model structure use score compare learnt models well addition consider runtime valuable information also pay special attention performance learnt models predictive tasks expression comment sc initial n mean standard deviation log marginal likelihood initial model sc nal n mean standard deviation log marginal likelihood learnt model standard deviation predictive ability learnt model 10fold crossvalidation times n mean standard deviation runtime learning process seconds table 1 performance criteria predictive ability predictive ability measured setting aside test set following learning log likelihood test set measured given learnt model experiments run pentium 366 mhz computer results reported performance criteria averages 5 independent runs 52 results synthetic data section describe experimental results synthetic data course one disadvantages using synthetic databases comparisons may realistic however seeing original goldstandard models known allow us show reliability algorithm learning rbmns incomplete data improvement achieved rbmns results scored bns constructed 4 synthetic databases 1 2 3 4 follows 1 11 predictive attributes involved 1 4valued hidden cluster variable 9 11 predictive attributes 3valued 2 remaining binary attributes obtain 1 2 simulated 2 1level rbmns models distinguished decision tree 1 binary predictive attribute thus 2 component bns original model component bns several supernodes randomly created parameters local probability distribution component bns randomly generated far dened local multinomial distribution moreover weights mixture component bns equal leaves followed uniform probability distribution 2 rbmns sampled 8000 cases resulting 1 2 respectively hand 3 4 12 predictive attributes involved 4valued hidden cluster variable 9 12 predictive attributes 3valued 3 remaining binary attributes getting 3 4 simulated 2 2levels rbmns models distinguished decision tree 3 binary predictive attributes thus 4 component bns original model component bns several supernodes randomly created parameters local probability distribution component bns randomly generated far dened local multinomial distribution moreover weights mixture component bns equal 1 leaves followed uniform probability distribution 2 rbmns sampled 16000 cases resulting 3 4 respectively appendix shows structures 4 original rbmns sampled obviously discarded entries corresponding cluster variable 4 synthetic databases finally every entry corresponding supernode replaced many entries original predictive attributes grouped together database sc initial n depth sc nal n 10cv n times n table 2 performance achieved learning rbmns data clustering 4 synthetic databases results averages 5 independent runs supernode decoded cartesian product original predictive attributes every entry database corresponding supernode table 2 compares performance learnt rbmns dierent values column depth represents depth distinguished decision trees remember bns assumed special case rbmns depth distinguished decision trees equal 0 follows table algorithm learning rbmns incomplete data able discover complexity underlying model databases 1 2 models highest log marginal likelihood 1level distinguished decision tree whereas databases 3 4 learnt rbmns highest log marginal likelihood 2levels distinguished decision tree thus log marginal likelihood expected complete data appears behave eectively used guide search considered stopping condition detailed analysis rbmn learnt 5 runs 4 synthetic databases considered suggests general variables used split original databases several instance subspaces internal nodes distinguished decision trees rbmns sampled discovered runs instance runs 1 identify 1 root distinguished decision tree learnt rbmns recover average 100 true instance subspaces hand 3 5 runs 2 discover true attribute splits domain 2 instance subspaces results average 60 true instance subspaces discovered 5 runs provide us rbmn 12 root distinguished decision tree moreover 2 3 runs also identify rest true internal nodes original 2levels rbmn third 3 runs discovers 1 2 true internal nodes second level distinguished decision tree additionally 2 runs 5 3 identify 3 internal nodes distinguished decision tree original rbmn 12 1 2 2 appears root 12 1 second level distinguished decision tree 2 4 instance subspaces eectively discovered 2 runs result learnt models 3 discover average 60 2 main true instance subspaces 70 4 specic true subspaces 4 3 5 runs provide us rbmn splits original data 4 true instance subspaces remainder 2 runs provide us rbmns 12 root distinguished decision trees 2 1 2 internal nodes however fail identify 1 second node second level original distinguished decision tree thus learnt models 4 discover average 100 2 main true instance subspaces 80 4 specic true subspaces point view predictive task measured 10cv column report general learnt rbmns outperform bns databases 1 2 biggest dierence predictive ability reached learnt bns learnt rbmns depth equal 1 remember underlying original models databases 1level rbmns furthermore learnt 1level rbmns received highest sc nal exactly observed synthetic databases 3 4 biggest increase 10cv reached learnt rbmns depth equal 1 learnt rbmns 2levels distinguished decision trees note 2levels rbmns models scored highest sc nal underlying original models 2levels rbmns learnt rbmns complex distinguished decision trees improvement predictive ability decreases however general rule complex models higher predictive ability fact wellknown 10fold crossvalidation scores log likelihood test database penalize complexity model log marginal likelihood addition complexity distinguished decision tree increases instance subspaces learn component bns reduce thus uncertainty decreases order avoid complex models results show log marginal likelihood suitable score guide search best rbmn point view eciency measured runtime learning pro cess experimental results show learning rbmns implies considerable computational expense compared learning bns however expense appears justied empirical evidence rbmns behave eectively synthetic domains addition outlined advantages contextspecic conditional independencies structured clustering exibility etc 53 results real data another source data evaluation consisted 2 realworld databases uci machine learning repository 34 tictactoe database nursery database past usage tictactoe database helps classify paradigmatic domain testing constructive induction methods despite used supervised classication due presence cluster variable considered good domain evaluate performance approach cluster entries hidden furthermore past usage nursery database shows suitability testing constructive induction methods addition fact presence 5 clusters large number cases made database interesting purpose cluster entries hidden tictactoe database contains 958 cases represents legal tic tactoe endgame board case 9 3valued predictive attributes clusters nursery database consists 12960 cases representing application admission public school system case 8 predictive attributes 2 5 possible values 5 clusters obviously database sc initial n depth sc nal n 10cv n times n nursery 57026120 0 53910709 6453126 306 table 3 performance achieved learning rbmns data clustering 2 realworld databases results averages 5 independent runs databases deleted cluster entries table 3 reports results achieved learning rbmns dierent depth distinguished decision tree 2 realworld databases databases learnt rbmns outperform learnt bns terms log marginal likelihood learnt models predictive ability learnt 1level rbmns obtain highest score log marginal likelihood domains moreover learnt rbmns 1level distinguished decision trees appear predictive complex models learnt rbmns 2levels distinguished decision trees 6 conclusions future research proposed new approach perform data clustering based new class knowledge models recursive bayesian multinets rbmns models may learnt represent joint probability distribution given complete incom plete database rbmns generalization bns bmns well extensions classical partitional systems additionally described heuristic algorithm learning rbmns data clustering simplies learning elicitation component bns incomplete data also presented advantages derived use rbmns codication contextspecic conditional independencies structured specialized domain knowledge alternate clusterings able capture dierent patterns dierent instance subspaces exibility experimental results synthetic realworld domains shown learnt rbmns overcame learnt bns terms log marginal likelihood predictive ability learnt model moreover synthetic domains score guide structural search log marginal likelihood expected complete data exhibited suitable behavior instance subspaces implied underlying original models eectively discovered achieve gain obvious increase runtime learning process rbmns compared learning bns current research driven means simple data preprocessing reduce set predictive attributes considered placed distinguished decision tree reduction search space would imply huge save runtime since primary aim introduce new knowledge paradigm perform data clustering focus exploiting possibilities instance denition rbmns introduced section 24 limits modelling power rbmns since leaves level constraint imposed sake understandability new model removed practice resulting possibility obtaining natural data clusterings limitation presented heuristic algorithm learning rbmns monothetic nature single attributes considered extension distinguished decision tree currently considering possibility learning polythetic decision paths order enrich modelling power another line research investigating extension rbmns perform data clustering continuous domains case component bns would able deal continuous attributes thus would conditional gaussian networks 29 41 42 however approach would imply search best discretization attributes considered decision paths 41 example realworld continuous domain mentioned extensions rbmns continuous data could considered perform data clustering dierent patterns observed dierent instance subspaces original data extension rbmns continuous domains would decrease disrupting eects due discretization original data would necessary apply rbmns dened paper problem domain presented 41 acknowledgments jose manuel pena wishes thank dr dag wedelin interest work made visit chalmers university technology gothenburg sweden possible technical support work kindly provided department computer science chalmers university technology also authors would like thank prof douglas h fisher addition two anonymous referees useful comments addressing interesting readings related work work supported spanish ministerio de educacion cultura ap97 44673053 grant appendix structures original rbmns sampled order obtain synthetic databases structures original 1level 2levels rbmns sampled obtain synthetic databases rst two model structures correspond rbmns sampled generate synthetic databases 1 top 2 bottom whereas last two model structures correspond rbmns sampled get synthetic databases 3 top 4 bottom dotted lines correspond distinguished decision trees predictive attributes 3valued except 1 2 12 binary cluster variable c 4valued y111212222 yy yy yyy 6 41221212 6 r analysis applications constructive induction key design creativity operations learning graphical models analyse typologique autoclass bayesian classi bayesian classi learning bayesian networks npcomplete bayesian method induction probabilistic networks data maximum likelihood incomplete data via em algorithm pattern classi knowledge acquisition via incremental conceptual clustering database management analysis tools machine induction bayesian structural em algorithm bayesian network classi building classi bayesian network classi knowledge representation inference similarity networks bayesian multinets clustering algorithms learning bayesian net works combination knowledge statistical data introduction bayesian networks finding groups data learning augmented bayesian classi scaling accuracy naivebayes classi ers decisiontree hybrid induction recursive bayesian classi graphical models methods classi em algorithm extensions uci repository machine learning databases pattern recognition knowledgeguided computer duction constructive induction cartesian product attributes searching dependencies bayesian classi probabilistic reasoning intelligent systems learning bayesian networks clustering means constructive induction improved bayesian structural em algorithm learning bayesian networks clustering geometric implications naive bayes assumption learning bayesian networks incomplete databases parameter estimation bayesian networks incomplete databases learning conditional probabilities incomplete data experimental comparison bayesian analysis expert systems learning mixtures dag models lazy learning bayesian rules tr probabilistic reasoning intelligent systems networks plausible inference bayesian method induction probabilistic networks data c45 programs machine learning learning bayesian networks knowledge representation inference similarity networks bayesian multinets bayesian classification autoclass bayesian network classifiers efficient approximations marginal likelihood bayesian networks hidden variables learning bayesian networks clustering means constructive induction improved bayesian structural em algorithm learning bayesian networks clustering lazy learning bayesian rules clustering algorithms introduction bayesian networks expert systems probabiistic network models knowledge acquisition via incremental conceptual clustering induction recursive bayesian classifiers bayesian network classification continuous attributes ctr j pea j lozano p larraaga unsupervised learning bayesian networks via estimation distribution algorithms application gene expression data clustering international journal uncertainty fuzziness knowledgebased systems v12 nsupplement p6382 january 2004 radu stefan niculescu tom mitchell r bharat rao bayesian network learning parameter constraints journal machine learning research 7 p13571383 1212006 j pea j lozano p larraaga globally multimodal problem optimization via estimation distribution algorithm based unsupervised learning bayesian networks evolutionary computation v13 n1 p4366 january 2005