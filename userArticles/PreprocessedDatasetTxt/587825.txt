fully asynchronous multifrontal solver using distributed dynamic scheduling paper analyze main features discuss tuning algorithms direct solution sparse linear systems distributed memory computers developed context long term european research project algorithms use multifrontal approach especially designed cover large class problems problems symmetric positive definite general symmetric unsymmetric matrices possibly rank deficient provided user several formats algorithms achieve high performance exploiting parallelism coming sparsity problem available dense matrices algorithms use dynamic distributed task scheduling technique accommodate numerical pivoting allow migration computational tasks lightly loaded processors large computational tasks divided subtasks enhance parallelism asynchronous communication used throughout solution process efficiently overlap communication computationwe illustrate design choices experimental results obtained sgi origin 2000 ibm sp2 test matrices provided industrial partners parasol project b introduction consider direct solution large sparse linear systems distributed memory computers systems form n theta n symmetric positive definite general symmetric unsymmetric sparse matrix possibly rank deficient b righthand side vector x solution vector computed work presented article performed work package 21 within parasol project parasol esprit iv long term research project 20160 integrated environment parallel sparse matrix solvers main goal project started january 1996 finishes june 1999 build test portable library solving large sparse systems equations distributed memory systems final library public domain contain routines direct iterative solution symmetric unsymmetric systems context parasol produced multifrontal massively parallel solver 27 28 referred mumps remainder paper several aspects algorithms used mumps combine give approach unique among sparse direct solvers include ffl classical partial numerical pivoting numerical factorization requiring use dynamic data structures ffl ability automatically adapt computer load variations numerical phase ffl high performance exploiting independence computations due sparsity available dense matrices ffl capability solving wide range problems including symmetric unsymmetric rankdeficient systems using either lu ldl factorization address factors designed fully asynchronous algorithm based multifrontal approach distributed dynamic scheduling tasks current version package provides large range options including possibility inputting matrix assembled format either single processor distributed processors additionally matrix input elemental format currently one processor mumps also determine rank nullspace basis rankdeficient matrices return schur complement matrix contains classical pre postprocessing facilities example matrix scaling iterative refinement error analysis among work distributed memory sparse direct solvers aware 7 10 12 22 23 24 know capabilities mumps solver difficulty handling dynamic data structures efficiently distributed memory approaches perform numerical pivoting factorization phase instead based static mapping tasks data allow task migration numerical factorization numerical pivoting clearly avoided symmetric positive definite matrices unsymmetric matrices duff koster 18 19 designed algorithms permute large entries onto diagonal shown significantly reduce numerical pivoting demmel li 12 shown one preprocesses matrix using code duff koster static pivoting possibly modified diagonal values followed iterative refinement normally provide reasonably accurate solutions observed preprocessing combination appropriate scaling input matrix issue numerical stability approach rest paper organized follows first introduce main terms used multifrontal approach section 2 throughout paper study performance obtained set test problems describe section 3 discuss section 4 main parallel features approach section 5 give initial performance figures show influence ordering variables performance mumps section 6 describe work accepting input matrices elemental form section 7 briefly describes main properties algorithms used distributed assembled matrices section 8 comment memory scalability issues section 9 describe analyse distributed dynamic scheduling strategies analysed section 10 show modify assembly tree introduce parallelism present summary results section 11 results presented paper obtained 35 processor ibm sp2 located gmd bonn germany node computer 66 mhertz processor 128 mbytes physical memory 512 mbytes virtual memory sgi cray origin 2000 parallab university bergen norway also used run largest test problems parallab computer consists 64 nodes sharing 24 gbytes physically distributed memory node two r10000 mips risc 64bit processors sharing 384 mbytes local memory processor runs frequency 195 mhertz peak performance little 400 mflops per second experiments reported paper use version 40 mumps software written fortran 90 requires mpi message passing makes use blas 14 15 lapack 6 blacs 13 scalapack 9 subroutines ibm sp2 currently using nonoptimized portable local installation scalapack ibm optimized library pessl v2 available multifrontal methods intention describe details multifrontal method rather define terms used later paper refer reader earlier publications detailed description example 3 17 20 multifrontal method elimination operations take place within dense submatrices called frontal matrices frontal matrix partitioned shown figure 1 matrix pivots chosen within block f 11 schur complement matrix computed used update later rows columns overall matrix call update matrix contribution block fully summed rows partly summed rows fully summed columns partly summed columns figure 1 partitioning frontal matrix overall factorization sparse matrix using multifrontal scheme described assembly tree node corresponds computation schur complement described edge represents transfer contribution block son node father node tree father node assembles sums contribution blocks son nodes entries original matrix original matrix given assembled format complete rows columns input matrix assembled facilitate input matrix ordered according pivot order stored collection arrowheads permuted matrix entries example columns row column arrowhead list associated variable g symmetric case entries lower triangular part matrix stored say storing matrix arrowhead form arrowheads unassembled matrices complete element matrices assembled frontal matrices input matrix need preprocessed implementation assembly tree constructed symmetrized pattern matrix given sparsity ordering symmetrized pattern mean pattern matrix aa summation symbolic note allows matrix unsymmetric numerical pivoting possible variables cannot eliminated frontal matrix fully summed rows columns correspond variables added contribution block sent father node assembly fully summed rows columns frontal matrix father node means corresponding elimination operations delayed repeated elimination steps later frontal matrices introduced stable pivots delayed fully summed part delay elimination steps corresponds posteriori modification original assembly tree structure general introduces additional numerical fillin factors important aspect assembly tree operations pair nodes neither ancestor independent gives possibility obtaining parallelism tree socalled tree parallelism example work commence parallel leaf nodes tree fortunately near root node tree tree parallelism poor frontal matrices usually much larger techniques exploiting parallelism dense factorizations used example blocking use higher level blas call node parallelism discuss aspects parallelism multifrontal method later sections paper work based experience designing implementing multifrontal scheme shared virtual shared memory computers example 2 3 4 initial prototype distributed memory multifrontal version 21 describe design resulting distributed memory multifrontal algorithm rest paper 3 test problems throughout paper use set test problems illustrate performance algorithms describe set section tables 1 2 list unassembled assembled test problems respectively except one come industrial partners parasol project remaining matrix bbmat forthcoming rutherfordboeing sparse matrix collection 16 symmetric matrices show number entries lower triangular part matrix typical parasol test cases following major application areas computational fluid dynamics cfd structural mechanics modelling compound devices modelling ships mobile offshore platforms industrial processing complex nonnewtonian liquids modelling car bodies engine components test problems provided assembled format elemental format suffix rsa rse used differentiate elemental format original matrix represented sum element matrices nonzero entries rows columns correspond variables ith element element matrices may overlap number entries matrix elemental format usually larger matrix assembled compare matrices det norske veritas norway tables 1 2 typically twice number entries unassembled elemental format real symmetric elemental rse matrix name order elements entries origin t1rse 97578 5328 6882780 det norske veritas ship 001rse 34920 3431 3686133 det norske veritas ship 003rse 121728 45464 9729631 det norske veritas shipsec1rse 140874 41037 8618328 det norske veritas shipsec5rse 179860 52272 11118602 det norske veritas shipsec8rse 114919 35280 7431867 det norske veritas threadrse 29736 2176 3718704 det norske veritas x104rse 108384 6019 7065546 det norske veritas table 1 unassembled symmetric test matrices parasol partner elemental format tables 3 4 5 present statistics factorizations various test problems using mumps tables show number entries factors number floatingpoint operations flops elimination unsymmetric problems show estimated number assuming pivoting actual number numerical pivoting used statistics clearly depend ordering used two classes ordering considered paper first approximate minimum degree ordering referred amd see 1 second class based hybrid nested dissection minimum degree technique referred nd hybrid orderings generated using onmetis 26 combination graph partitioning tool scotch 29 variant amd halo amd see 30 matrices available assembled unassembled format used nested dissection based orderings provided det norske veritas denote mfr note paper intention compare packages used obtain orderings discuss influence type ordering performance mumps section 5 amd ordering algorithms tightly integrated within mumps code orderings passed mumps externally computed ordering tight integration observe table 3 analysis time smaller using amd real unsymmetric assembled rua matrix name order entries origin mixingtank 29957 1995041 polyflow sa bbmat 38744 1771722 rutherfordboeing cfd real symmetric assembled rsa matrix name order entries origin oilpan 73752 1835470 inpro b5tuer 162610 4036144 inpro crankseg 1 52804 5333507 macnealschwendler bmw7st 1 141347 3740507 macnealschwendler ship 001rsa 34920 2339575 det norske veritas ship 003rsa 121728 4103881 det norske veritas shipsec1rsa 140874 3977139 det norske veritas shipsec5rsa 179860 5146478 det norske veritas shipsec8rsa 114919 3384159 det norske veritas threadrsa 29736 2249892 det norske veritas x104rsa 108384 5138004 det norske veritas table 2 assembled test matrices parasol partners except matrix bbmat amd ordering entries flops time matrix factors theta10 6 estim actual estim actual seconds mixingtank 385 391 641 644 49 invextrusion1 303 312 343 358 46 bbmat 460 462 413 416 81 nd ordering entries flops time matrix factors theta10 6 estim actual estim actual seconds mixingtank 189 196 130 132 128 bbmat 357 358 255 257 113 table 3 statistics unsymmetric test problems ibm sp2 userdefined precomputed ordering paper nd mfr orderings addition cost computing external ordering included tables amd ordering nd ordering entries flops time entries flops matrix factors analysis factors b5tuer 26 13 15 24 12 bmw7st 1 table 4 statistics symmetric test problems ibm sp2 entries flops matrix factors ship 003 57 73 shipsec1 37 shipsec5 51 52 shipsec8 34 34 thread table 5 statistics symmetric test problems available assembled rsa unassembled rse formats mfr ordering 4 parallel implementation issues paper assume onetoone mapping processes processors distributed memory environment process thus implicitly refer unique processor say example task allocated process mean task also mapped onto corresponding processor shared memory environment 4 exploit parallelism arising sparsity tree parallelism dense factorizations kernels node parallelism avoid limitations due centralized scheduling host process charge scheduling work processes chosen distributed scheduling strategy implementation pool work tasks distributed among processes participate numerical factorization host process still used perform analysis phase identify pool work tasks distribute righthand side vector collect solution implementation allows host process participate computations factorization solution phases allows user run code single processor avoids one processor idle factorization solution phases code solves system three main steps 1 analysis host performs approximate minimum degree ordering based symmetrized matrix pattern carries symbolic factorization ordering also provided user host also computes mapping nodes assembly tree processors mapping keeps communication costs factorization solution minimum balances memory computation required processes computational cost approximated number floatingpoint operations assuming pivoting performed storage cost number entries factors computing mapping host sends symbolic information processes using information process estimates work space required part factorization solution estimated work space large enough handle computational tasks assigned process analysis time plus possible tasks may receive dynamically factorization assuming excessive amount unexpected fillin occurs due numerical pivoting 2 factorization original matrix first preprocessed example converted arrowhead format matrix assembled distributed processes participate numerical factorization process allocates array contribution blocks factors numerical factorization frontal matrix performed process determined analysis phase potentially one processes determined dynamically factors must kept solution phase 3 solution righthand side vector b broadcast host processes compute solution vector x using distributed factors computed factorization phase solution vector assembled host 41 sources parallelism consider condensed assembly tree figure 2 leaves represent subtrees assembly tree subtrees type 2 type 3 type 2 type 2 type 1 figure 2 distribution computations multifrontal assembly tree four processors p0 p1 p2 p3 consider tree parallelism transfer contribution block node assembly tree father node requires local data movement nodes assigned process communication required nodes assigned different processes reduce amount communication factorization solution phases mapping computed analysis phase assigns subtree assembly tree single process general mapping algorithm chooses leaf subtrees processes mapping subtrees carefully onto processes achieve good overall load balance computation bottom tree described detail 5 however exploit tree parallelism speedups disappointing obviously depends problem typically maximum speedup 3 5 illustrated table 6 poor performance caused fact tree parallelism decreases going towards root tree moreover observed see example 4 often 75 computations performed top three levels assembly tree thus necessary obtain parallelism within large nodes near root tree additional parallelism based parallel blocked versions algorithms used factorization frontal matrices nodes assembly tree treated one process referred nodes type 1 parallelism assembly tree referred type 1 parallelism parallelism obtained onedimensional 1d block partitioning rows frontal matrix nodes large contribution block see figure 2 nodes referred nodes type 2 corresponding parallelism type 2 parallelism finally frontal matrix root node large enough partition twodimensional 2d block cyclic way parallel root node referred node type 3 corresponding parallelism type 3 parallelism 42 type 2 parallelism analysis phase node determined type 2 number rows contribution block sufficiently large node type 2 one process called master holds fully summed rows performs pivoting factorization block processes called slaves perform updates partly summed rows see figure 1 slaves determined dynamically factorization process may selected able assemble original matrix entries quickly frontal matrix type 2 node duplicate corresponding original matrix entries stored arrowheads element matrices onto processes factorization way master slave processes type 2 node immediate access entries need assembled local part frontal matrix duplication original data enables efficient dynamic scheduling computational tasks requires extra storage studied detail section 8 note type 1 node original matrix entries need present process handling node execution time master type 2 node first receives symbolic information describing structure contribution blocks son nodes tree information sent master processes handling sons based information master determines exact structure frontal matrix decides slave processes participate factorization node sends information processes handling sons enable send entries contribution blocks directly appropriate processes involved type 2 node assemblies node subsequently performed parallel master slave processes perform elimination operations frontal matrix parallel macropipelining based blocked factorization fully summed rows used overlap communication computation efficiency algorithm thus depends block size used factor fully summed rows number rows allocated slave process details differences implementations symmetric unsymmetric matrices described 5 43 type 3 parallelism root node must factorize dense matrix use standard codes scalability reasons use 2d block cyclic distribution root node use scalapack 9 vendor equivalent implementation routine pdgetrf general matrices routine pdpotrf symmetric positive definite matrices actual factorization currently maximum one root node chosen analysis processed parallel node chosen largest root provided size larger computer dependent parameter otherwise factorized one processor one process also called master holds indices describing structure root frontal matrix call root node determined analysis phase estimated root node factorization structure frontal matrix estimated root node statically mapped onto 2d grid processes mapping fully determines process entry estimated root node assigned hence assembly original matrix entries contribution blocks processes holding information easily compute exactly processes must send data factorization phase original matrix entries part contribution blocks sons corresponding estimated root assembled soon available master root node collects index information delayed variables due numerical pivoting sons builds final structure root frontal matrix symbolic information broadcast processes participate factorization contributions corresponding delayed variables sent sons appropriate processes 2d grid assembly contributions directly assembled locally destination process note requirements scalapack local copying root node required since leading dimension change delayed pivots 44 parallel triangular solution solution phase also performed parallel uses asynchronous communications forward elimination back substitution case forward elimination tree processed leaves root similar factorization back substitution requires different algorithm processes tree root leaves pool readytobeactivated tasks used change distribution factors generated factorization phase hence type 2 3 parallelism also used solution phase root node use scalapack routine pdgetrs general matrices routine pdpotrs symmetric positive definite matrices 5 basic performance influence ordering earlier studies example 25 know ordering may seriously impact uniprocessor time parallel behaviour method illustrate report table 6 performance obtained using type 1 parallelism results show using type 1 parallelism produce good speedups results also show see columns speedup usually get better parallelism nested dissection based orderings minimum degree based orderings thus gain using nested dissection reduction number floatingpoint operations see tables 3 4 better balanced assembly tree discuss performance obtained mumps matrices assembled format used reference paper performance obtained matrices provided elemental format discussed section 6 tables 7 8 show performance mumps using nested dissection minimum degree orderings ibm sp2 sgi origin 2000 respectively note speedups difficult compute ibm sp2 memory paging often occurs small number processors hence better performance nested dissection orderings small number processors ibm sp2 due part reduction memory required processor since less entries factors get better idea true algorithmic speedups without memory paging effects give table 7 uniprocessor cpu time one processor instead elapsed time matrix time speedup amd nd amd nd oilpan 126 73 291 445 bmw7st 1 556 213 255 487 bbmat 784 494 408 400 b5tuer 334 255 347 422 table influence ordering time seconds speedup factorization phase using type 1 parallelism 32 processors ibm sp2 memory large enough run one processor estimate megaflop rate used compute uniprocessor cpu time estimate also used necessary compute speedups table 6 small number processors still memory paging effect may significantly increase elapsed time however speedup elapsed time one processor given considerable matrix ordering number processors oilpan amd 37 136 90 68 59 58 b5tuer amd 116 1555 241 168 161 131 crankseg 1 amd 456 5083 1624 784 633 bmw7st 1 amd 142 1534 465 213 184 167 nd 104 1057 367 202 129 117 mixingtank amd 495 2885 707 645 613 nd 104 3280 261 174 144 148 bbmat amd 320 2764 683 478 440 398 nd 198 1064 767 352 346 309 table 7 impact ordering time seconds factorization ibm estimated cpu time one processor means enough memory table 8 also shows elapsed time solution phase observe speedups phase quite good remainder paper use nested dissection based orderings unless stated otherwise factorization phase matrix ordering number processors bmw7st 1 amd 857 560 282 185 151 142 nd 3066 1827 809 529 412 355 nd 1521 938 525 330 221 170 solution phase matrix ordering number processors crankseg 2 amd 68 58 44 29 24 23 nd 43 27 18 15 11 18 bmw7st 1 amd 42 24 23 19 14 16 nd 33 21 17 14 16 15 nd 83 47 27 21 18 20 nd 63 38 29 24 20 24 table 8 impact ordering time seconds factorization solve phases sgi origin 2000 6 elemental input matrix format section discuss main algorithmic changes handle efficiently problems provided elemental format assume original matrix represented sum element matrices nonzero entries rows columns correspond variables ith element usually held dense matrix matrix symmetric lower triangular part stored multifrontal approach element matrices need assembled one frontal matrix elimination process due fact frontal matrix structure contains definition variables adjacent fully summed variable front consequence element matrices need split assembly process note classical fanin fanout approaches 7 property hold since positions element matrices assembled restricted fully summed rows columns main modifications make algorithms assembled matrices accommodate unassembled matrices lie analysis distribution matrix assembly process describe detail analysis phase exploit elemental format matrix detect supervariables define supervariable set variables list adjacent elements illustrated figure 3 matrix composed two overlapping elements three supervariables note definition supervariable differs usual definition see example 11 supervariables used successfully similar context compress graphs associated assembled matrices structural engineering prior multiple minimum degree ordering 8 assembled matrices however observed 1 use supervariables combination approximate minimum degree algorithm efficient graph size matrix supervariable detection t1rse 9655992 299194 ship 003rse 7964306 204324 shipsec1rse shipsec5rse 9933236 256976 shipsec8rse 6538480 171428 threadrse 4440312 397410 x104rse 10059240 246950 table 9 impact supervariable detection length adjacency lists given ordering phase table 9 shows impact using supervariables size graph processed ordering phase amd ordering graph size length adjacency lists variablessupervariables given input ordering phase without supervariable detection initial graph variables initial matrix742678 45 graph supervariables sum two overlapping elements figure 3 supervariable detection matrices elemental format graph size twice number offdiagonal entries corresponding assembled matrix working space required analysis phase using amd ordering dominated space required ordering phase graph size plus overhead small multiple order matrix since ordering performed single processor space required compute ordering memory intensive part analysis phase supervariable detection complete uncompressed graph need built since ordering phase operate directly compressed graph table 9 shows large graphs compression reduce memory requirements analysis phase dramatically table shows impact using supervariables time complete analysis phase including graph compression ordering see reduction time due reduced time ordering significantly less time also needed building much smaller adjacency graph supervariables time analysis matrix supervariable detection t1rse 46 18 15 03 ship 003rse 74 28 32 07 shipsec1rse 60 22 26 06 shipsec5rse 101 46 39 08 shipsec8rse 57 20 26 05 threadrse 26 09 12 02 x104rse 64 35 15 03 table 10 impact supervariable detection time seconds analysis phase sgi origin 2000 time spent amd ordering parentheses overall time spent assembly process matrices elemental format differ overall time spent assembly process equivalent assembled matrix obviously matrices elemental format often significantly data assemble usually twice number entries matrix assembled format however assembly process matrices elemental format performed efficiently assembly process assembled matrices first potentially assemble larger regular structure full matrix second input data assembled near leaf nodes assembly tree two consequences assemblies performed distributed way assemblies original element matrices done type 1 nodes hence less duplication original matrix data necessary detailed analysis duplication issues linked matrices elemental format addressed section 8 experiments shown observed despite differences assembly process performance mumps assembled unassembled problems similar provided ordering used reason extra amount assemblies original data unassembled problems relatively small compared total number flops experimental results tables 11 12 obtained sgi origin 2000 show good scalability code factorization solution phases set unassembled matrices matrix number processors ship 003rse 392 242 156 120 92 shipsec1rse 174 128 shipsec5rse 281 176 114 63 43 shipsec8rse 187 127 68 36 threadrse 186 120 69 46 37 x104rse 56 34 20 table 11 time seconds factorization unassembled matrices sgi origin 2000 mfr ordering used matrix number processors t1rse 35 21 11 12 08 ship 003rse 69 36 33 25 20 shipsec1rse 38 31 21 16 15 shipsec5rse 55 42 29 22 19 shipsec8rse 38 31 20 14 13 threadrse 23 19 13 10 08 x104rse 26 19 14 10 11 table 12 time seconds solution phase unassembled matrices sgi origin 2000 mfr ordering used 7 distributed assembled matrix distribution input matrix available processors main preprocessing step numerical factorization phase step input matrix organized arrowhead format distributed according mapping provided analysis phase symmetric case first arrowhead frontal matrix also sorted enable efficient assembly 5 assembled matrix initially held centrally host observed time distribute real entries original matrix sometimes comparable time perform actual factorization example matrix oilpan time distribute input matrix 16 processors ibm sp2 average 6 seconds whereas time factorize matrix 68 seconds using amd ordering see table 7 clearly larger problems arithmetic required actual factorization time factorization dominate time redistribution distributed input matrix format expect reduce time redistribution phase parallelize reformatting sorting tasks use asynchronous alltoall instead onetoall communications furthermore expect solve larger problems since storing complete matrix one processor limits size problem solved distributed memory computer thus improve memory time scalability approach allow input matrix distributed based static mapping tasks processes computed analyis phase one priori distribute input data remapping required beginning factorization distribution referred mumps mapping limit communication duplications original matrix corresponding type 2 nodes studied section 8 show influence initial matrix distribution time redistribution compare figure 4 three ways providing input matrix 1 centralized mapping input matrix held one process host 2 mumps mapping input matrix distributed processes according static mapping computed analysis phase 3 random mapping input matrix uniformly distributed processes random manner correlation mapping computed analysis phase figure clearly shows benefit using asynchronous alltoall communications required mumps random mappings compared using onetoall communications centralized mapping even interesting observe distributing input matrix according mumps mapping significantly reduce time redistribution attribute good overlapping communication computation mainly data reformatting sorting redistribution algorithm number processors1357 distribution time seconds centralized matrix using mumps mapping random mapping figure 4 impact initial distribution matrix oilpan time redistribution ibm sp2 8 memory scalability issues section study memory requirements memory scalability algorithms figure 5 illustrates mumps balances memory load processors figure shows two matrices maximum memory required processor average processors function number processors observe varying numbers processors values quite similar number processors50150250 size total space mbytes maximum average 28 number processors100300500700size total space mbytes maximum average figure 5 total memory requirement per processor maximum average factorization nd ordering table 13 shows average size per processor main components working space used factorization matrix bmw3 2 components ffl factors space reserved factors processor know analysis phase type 2 nodes participate therefore reserves enough space able participate type 2 nodes ffl stack area space used stacking contribution blocks factors ffl initial matrix space required store initial matrix arrowhead format ffl communication buffers space allocated send receive buffers ffl size remaining workspace allocated per processor ffl total total memory required per processor lines ideal table 13 obtained dividing memory requirement one processor number processors comparing actual ideal numbers get idea mumps scales terms memory components number processors factors 423 211 107 58 ideal 211 106 53 26 stack area 502 294 172 initial matrix 69 345 173 89 50 40 35 ideal 345 173 86 43 29 22 communication buffers 0 45 34 14 6 6 5 20 20 20 20 20 20 20 total 590 394 243 135 82 69 67 ideal 295 147 74 37 25 table 13 analysis memory used factorization matrix bmw3 2 nd ordering sizes mbytes per processor see even total memory sum local workspaces increases average memory required per processor significantly decreases 24 processors also see size factors stack area much larger ideal part difference due parallelism unavoidable another part however due overestimation space required main reason mapping type 2 nodes processors known analysis processor potentially participate elimination type 2 node therefore processor allocates enough space able participate type 2 nodes working space actually used smaller large number processors could reduce estimate factors stack area example successfully factorized matrix bmw3 2 32 processors stack area 20 smaller reported table 13 average working space used communication buffers also significantly decreases 16 processors mainly due type 2 node parallelism contribution blocks split among processors minimum granularity reached therefore increase number processors decrease reaching minimum granularity size contribution blocks sent processors note larger problems average size per processor communication buffers continue decrease larger number processors see expected line scale since corresponds data arrays size need allocated process see space significantly affects difference total ideal especially larger numbers processors however relative influence fixed size area smaller large matrices 3d simulations therefore affect asymptotic scalability algorithm imperfect scalability initial matrix storage comes duplication original matrix data linked type 2 nodes assembly tree study detail remainder section want stress however user point view numbers reported context related total memory used mumps package usually dominated large problems size stack area alternative duplication data related type 2 nodes would allocate original data associated frontal matrix master process responsible matrix number processors oilpan type 2 nodes 0 total entries 1835 1845 1888 2011 2235 2521 bmw7st total entries 3740 3759 3844 4031 4308 4793 total entries 5758 5767 5832 6239 6548 7120 shipsec1rsa type 2 nodes 0 0 4 11 19 21 total entries 3977 3977 4058 4400 4936 5337 shipsec1rse type 2 nodes total entries 8618 8618 8618 8627 8636 8655 threadrsa type 2 nodes total entries 2250 2342 2901 4237 6561 8343 threadrse type 2 nodes total entries 3719 3719 3719 3719 3719 3719 table 14 amount duplication due type 2 nodes total entries sum number original matrix entries processors theta10 3 number nodes also given type 2 node assembly process master process would charge redistributing original data slave processes strategy introduces extra communication costs assembly type 2 node thus chosen approach based duplication master process responsible type 2 node flexibility choose collaborating processes dynamically since involve data migration original matrix however extra cost strategy based decision analysis nodes type 2 partial duplication original matrix must performed order keep processors busy need sufficient node parallelism near root assembly tree mumps uses heuristic increases number type 2 nodes number processors used influence number processors amount duplication shown table 14 representative subset test problems show total number type 2 nodes sum processes number original matrix entries duplicates one processor type 2 nodes used data duplicated figure 6 shows four matrices number original matrix entries duplicated processors relative total number entries original matrix since original data unassembled matrices general assembled earlier assembly tree data matrix assembled format number duplications often relatively much smaller unassembled matrices assembled matrices matrix threadrse elemental format extreme example since even 16 processors type 2 node parallelism require duplication see table 14 conclude section want point code scales well terms memory usage virtual shared memory computers total memory sum local workspaces processors required mumps sometimes excessive therefore currently investigating reduce current overestimates local stack areas number processors515percentage bmw32 threadrsa figure percentage entries original matrix duplicated processors due type 2 nodes reduce total memory required possible solution might limit dynamic scheduling type 2 node corresponding data duplication subset processors 9 dynamic scheduling strategies avoid drawback centralized scheduling distributed memory computers implemented distributed dynamic scheduling strategies remind reader type 1 nodes statically mapped processes analysis time type 2 tasks represent large part computations parallelism method involved dynamic scheduling strategy able choose dynamically processes collaborate processing type 2 node designed twophase assembly process let inode node type 2 let pmaster process inode initially mapped first phase master processes sons inode mapped send symbolic data integer lists pmaster structure frontal matrix determined pmaster decides partitioning frontal matrix chooses slave processes phase pmaster collect information concerning load processors help decision process slave processes informed new task allocated pmaster sends description distribution frontal matrix collaborative processes sons inode send contribution blocks real values pieces directly correct processes involved computation inode assembly process thus fully parallelized maximum size message sent processes reduced see section 8 pool tasks private process used implement dynamic scheduling tasks ready activated given process stored pool tasks local process process executes following algorithm algorithm 1 nodes processed local pool empty blocking receive message process message elseif message available receive process message else extract work pool process endif note algorithm gives priority message reception main reasons choice first message received might source additional work parallelism second sending process might blocked send buffer full see 5 actual implementation use routine mpi iprobe check whether message available implemented two scheduling strategies first strategy referred cyclic scheduling master type 2 node take account load processors performs simple cyclic mapping tasks processors second strategy referred dynamic flopsbased scheduling master process uses information load processors allocate type 2 tasks least loaded processors load processor defined amount work flops associated active readytobeactivated tasks process charge maintaining local information associated current load simple remote memory access procedure using example onesided communication routine mpi get included mpi2 process access load processors necessary however mpi2 available target computers overcome designed module based symmetric communication tools mpi asynchronous send receive process charge updating broadcasting local load control frequency broadcasts updated load broadcast significantly different last load broadcast initial static mapping balance work well expect dynamic flopsbased scheduling improve performance respect cyclic scheduling tables 15 16 show significant performance gains obtained using dynamic flopsbased scheduling 24 processors gains less significant test problems small keep processors busy thus lessen benefits good dynamic scheduling algorithm also expect feature improve behaviour parallel algorithm multiuser distributed memory computer another possible use dynamic scheduling improve memory usage seen section 8 size stack area overestimated dynamic scheduling based matrix number processors scheduling 28 cyclic 791 479 407 413 389 flopsbased 611 456 419 417 404 cyclic 524 318 262 292 230 flopsbased 294 278 251 253 226 table 15 comparison cyclic flopsbased schedulings time seconds factorization ibm sp2 nd ordering matrix number processors scheduling 4 8 ship 003rse cyclic 1561 1199 919 flopsbased 1403 1102 838 shipsec5rse cyclic 1135 631 428 flopsbased 999 613 370 shipsec8rse cyclic 683 363 299 flopsbased 650 350 251 table comparison cyclic flopsbased schedulings time seconds factorization sgi origin 2000 mfr ordering memory load instead computational load could used address issue type 2 tasks mapped least loaded processor terms memory used stack area memory estimation size stack area based static mapping tasks splitting nodes assembly tree processing parallel type 2 node symmetric unsymmetric case factorization pivot rows performed single processor processors help update rows contribution block using 1d decomposition presented section 4 elimination fully summed rows represent potential bottleneck scalability especially frontal matrices large fully summed block near root tree type 1 parallelism limited overcome problem subdivide nodes large fully summed blocks illustrated figure 73571 assembly tree pivot blocks contribution blocks assembly tree splitting2nfront5npiv npiv father son son figure 7 tree subdivision frontal matrix large pivot block figure consider initial node size nfront npiv pivots replace node son node size nfront npiv son pivots father node size son npiv father npiv gammanpiv son pivots note splitting node increase number operations factorization add assembly operations nevertheless expect benefit splitting increase parallelism experimented simple algorithm postprocesses tree symbolic factorization algorithm considers nodes near root tree splitting large nodes far root sufficient tree parallelism already exploited would lead additional assembly communication costs node considered splitting distance root number edges root node let inode node tree dinode distance inode root nodes inode dinode dmax apply following algorithm algorithm 2 splitting node large enough 1 compute number flops performed master inode 2 compute number flops performed slave assuming nprocs gamma 1 slaves participate 3 w master w slave 31 split inode nodes son father npiv son 32 apply algorithm 2 recursively nodes son father endif endif algorithm 2 applied node nfront npiv2 large enough want make sure son split node type 2 size contribution block son nfront npiv son node split amount work master w master large relative amount work slave w slave reduce amount splitting away root add step 3 algorithm relative factor w slave factor depends machine dependent parameter increases distance node root parameter p allows us control general amount splitting finally algorithm recursive may divide initial node two new nodes effect splitting illustrated table 17 symmetric matrix crankseg 2 unsymmetric matrix invextrusion1 ncut corresponds number type 2 nodes cut value used flag indicate splitting flopsbased dynamic scheduling used runs section best time obtained given number processors indicated bold font see significant performance improvements 40 reduction time obtained using node splitting best timings generally obtained relatively large values p splitting occurs smaller values p corresponding times change much p number processors 28 200 time 379 314 304 295 254 150 time 418 313 310 289 272 100 time 398 323 284 286 267 ncut 9 11 13 14 15 50 time 367 336 314 296 274 ncut 28 p number processors 200 time 255 167 134 121 124 150 time 249 163 135 134 124 100 time 249 162 137 131 136 50 time 249 170 135 136 166 table 17 time seconds factorization number nodes cut different values parameter p ibm sp2 nested dissection ordering flopsbased dynamic scheduling used summary tables 19 show results obtained mumps 40 using dynamic scheduling node splitting default values parameters controlling efficiency package used therefore timings always correspond fastest possible execution time comparison results presented tables 7 8 11 summarizes well benefits coming work presented sections 9 10 matrix number processors oilpan 33 111 75 52 48 46 b5tuer 108 821 519 134 131 105 bmw7st 1 104 298 137 117 113 mixingtank 104 308 216 164 147 148 bbmat 198 2554 852 348 328 309 table 18 time seconds factorization using mumps 40 default options ibm sp2 nd ordering used estimated cpu time means swapping enough memory matrix number processors bmw7st 1 62 36 ship 003rse 392 237 124 108 51 shipsec1rse 174 125 63 shipsec5rse 281 181 103 62 37 shipsec8rse threadrse 186 125 70 38 24 x104rse 56 34 19 12 11 table 19 time seconds factorization using mumps 40 default options sgi origin 2000 nd mfr ordering used largest problem solved date symmetric matrix order 943695 39 million entries number entries factors 14 theta 10 9 number operations factorization 59 theta 10 12 one processor sgi origin 2000 factorization phase required 89 hours two nondedicated processors 62 hours required total amount memory estimated reserved mumps could solve 2 processors issue addressed improve scalability globally addressable memory computers analysis performed purely distributed memory computers larger number processors possible solutions mentioned paper limited dynamic scheduling andor memory based dynamic scheduling developed future acknowledgements grateful jennifer scott john reid comments early version paper r approximate minimum degree ordering algorithm linear algebra calculations virtual shared memory computer vectorization multiprocessor multifrontal code memory management issues sparse multifrontal methods multiprocessors multifrontal parallel distributed symmetric unsymmetric solvers fanboth family columnbased distributed cholesky factorisation algorithms compressed graphs minimum degree algorithm scalapack users guide parallel solution method large sparse systems equations supernodal approach sparse partial pivoting making sparse gaussian elimination scalable static pivoting working note 94 users guide blacs v1 algorithm 679 algorithm 679 rutherfordboeing sparse matrix collection direct methods sparse matrices design use algorithms permuting large entries diagonal sparse matrices algorithms permuting large entries diagonal sparse matrix multifrontal solution indefinite sparse symmetric linear systems developpement dune approche multifrontale pour machines memoire distribuee et reseau heterogene de stations de travail sparse cholesky factorization local memory multiprocessor highly scalable parallel algorithms sparse matrix factorization parallel algorithms sparse linear systems improving runtime quality nested dissection ordering scotch 31 users guide hybridizing nested dissection halo approximate minimum degree efficient sparse matrix ordering tr ctr kai shen parallel sparse lu factorization different message passing platforms journal parallel distributed computing v66 n11 p13871403 november 2006 omer meshar dror irony sivan toledo outofcore sparse symmetricindefinite factorization method acm transactions mathematical software toms v32 n3 p445471 september 2006 patrick r amestoy iain duff jeanyves lexcellent xiaoye li impact implementation mpi pointtopoint communications performance two general sparse solvers parallel computing v29 n7 p833849 july kai shen parallel sparse lu factorization secondclass message passing platforms proceedings 19th annual international conference supercomputing june 2022 2005 cambridge massachusetts hong zhang barry smith michael sternberg peter zapol sips shiftandinvert parallel spectral transformations acm transactions mathematical software toms v33 n2 p9es june 2007 mark baertschy xiaoye li solution threebody problem quantum mechanics using sparse linear algebra parallel computers proceedings 2001 acmieee conference supercomputing cdrom p4747 november 1016 2001 denver colorado iain duff jennifer scott parallel direct solver large sparse highly unsymmetric linear systems acm transactions mathematical software toms v30 n2 p95117 june 2004 adaptive grid refinement model two confined interacting atoms applied numerical mathematics v52 n23 p235250 february 2005 abdou guermouche jeanyves lexcellent gil utard impact reordering memory multifrontal solver parallel computing v29 n9 p11911218 september vladimir rotkin sivan toledo design implementation new outofcore sparse cholesky factorization method acm transactions mathematical software toms v30 n1 p1946 march 2004 dror irony gil shklarski sivan toledo parallel fully recursive multifrontal sparse cholesky future generation computer systems v20 n3 p425440 april 2004 abdou guermouche jeanyves lexcellent constructing memoryminimizing schedules multifrontal methods acm transactions mathematical software toms v32 n1 p1732 march 2006 olaf schenk klaus grtner twolevel dynamic scheduling pardiso improved scalability shared memory multiprocessing systems parallel computing v28 n2 p187197 february 2002 patrick r amestoy abdou guermouche jeanyves lexcellent stphane pralet hybrid scheduling parallel solution linear systems parallel computing v32 n2 p136156 february 2006 patrick r amestoy iain duff jeanyves lexcellent xiaoye li analysis comparison two general sparse solvers distributed memory computers acm transactions mathematical software toms v27 n4 p388421 december 2001 xiaoye li james w demmel superludist scalable distributedmemory sparse direct solver unsymmetric linear systems acm transactions mathematical software toms v29 n2 p110140 june olaf schenk klaus grtner solving unsymmetric sparse systems linear equations pardiso future generation computer systems v20 n3 p475487 april 2004 michele benzi preconditioning techniques large linear systems survey journal computational physics v182 n2 p418477 november 2002 patrick r amestoy iain duff stphane pralet christof vmel adapting parallel sparse direct solver architectures clusters smps parallel computing v29 n1112 p16451668 novemberdecember anshul gupta recent advances direct methods solving unsymmetric sparse systems linear equations acm transactions mathematical software toms v28 n3 p301324 september 2002 timothy davis column preordering strategy unsymmetricpattern multifrontal method acm transactions mathematical software toms v30 n2 p165195 june 2004 nicholas gould jennifer scott yifan hu numerical evaluation sparse direct solvers solution large sparse symmetric linear systems equations acm transactions mathematical software toms v33 n2 p10es june 2007 n f klimowicz mihajlovi heil deployment parallel direct sparse linear solvers within parallel finite element code proceedings 24th iasted international conference parallel distributed computing networks p310315 february 1416 2006 innsbruck austria bendali boubendir fares fetilike domain decomposition method coupling finite elements boundary elements largesize problems acoustic scattering computers structures v85 n9 p526535 may 2007