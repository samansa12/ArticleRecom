pricing agent economies using multiagent qlearning paper investigates adaptive software agents may utilize reinforcement learning algorithms qlearning make economic decisions setting prices competitive marketplace single adaptive agent facing fixedstrategy opponents ordinary qlearning guaranteed find optimal policy however population agents trying adapt presence adaptive agents problem becomes nonstationary history dependent known whether global convergence obtained whether solutions optimal paper study simultaneous qlearning two competing seller agents three moderately realistic economic models simplest case interesting multiagent phenomena occur state space small enough lookup tables used represent qfunctions find despite lack theoretical guarantees simultaneous convergence selfconsistent optimal solutions obtained model least small values discount parameter cases exact approximate convergence also found even large discount parameters show qderived policies increase profitability damp eliminate cyclic price wars compared simpler policies based zero lookahead shortterm lookahead one models shopbot model sellers profit functions symmetric find qlearning produce either symmetric brokensymmetry policies depending discount parameter initial conditions b introduction reinforcement learning rl procedures established powerful practical methods solving markov decision problems one significant actively investigated rl algorithms qlearning watkins 1989 qlearning algorithm learning estimate longterm expected reward given stateaction pair nice property need model environment used online learning strong convergence qlearning exact optimal value functions policies proven lookup table representations qfunction used watkins dayan 1992 feasible small state spaces large state spaces lookup tables infeasible rl methods combined function approximators give good practical performance despite lack theoretical guarantees convergence optimal policies realworld problems fully markov nature often nonstationary historydependent andor fully observable order rl methods generally useful solving problems need extended handle nonmarkovian properties one important application domain nonmarkovian aspects paramount area multiagent systems area expected increasingly important future due potential rapid emergence agent economies consisting large populations interacting software agents engaged various forms economic activity problem multiple agents simultaneously adapting general nonmarkov agent provides effectively nonstationary environment agents hence existing convergence guarantees hold general known whether global convergence obtained whether solutions optimal progress made analyzing certain special case multiagent problems example problem teams agents share common objective function studied example crites barto 1996 likewise purely competitive case zerosum objective functions studied littman 1994 algorithm called minimaxq proposed twoplayer zerosum games shown converge optimal value function policies players sandholm crites studied simultaneous qlearning two players iterated prisoners dilemma game sandholm crites 1995 found learning procedure generally converged stationary solutions however extent solutions optimal unclear recently hu wellman proposed algorithm calculating optimal qfunctions twoplayer arbitrarysum games hu wellman 1998 algorithm important first step however yet appear useable practical problems assumes policies followed players nash equilibrium policies address equilibrium coordination problem ie multiple nash equilibria agents decide equilibrium choose suspect may serious problem since according folk theorem iterated games kreps 1990 proliferation nash equilibria sufficiently high emphasis future rewards ie large value discount parameter fl furthermore may inconsistencies assumed nash policies policies implied qfunctions calculated algorithm paper study simultaneous qlearning economically motivated twoplayer game players assumed two sellers similar identical products compete basis price time step sellers alternately take turns setting prices taking account sellers current price price set consumers respond instantaneously deterministically choosing either seller 1s product seller 2s product product based current price pair p 1 leading instantaneous reward profit r 1 given sellers 1 2 respectively assume initially sellers full knowledge expected consumer demand given price pair fact full knowledge profit functions work builds prior research reported tesauro kephart 1998 tesauro kephart 1999 papers examined effect including fore sight ie ability anticipate longerterm consequences agents current action two different algorithms agent foresight presented generalization minimax search procedure twoplayer zerosum games ii generalization policy iteration method dynamic programming players policies simultaneously improved selfconsistent policy pairs obtained optimize expected reward two time steps found including foresight agents pricing algorithms generally improved overall agent profitability usually damped eliminated pathological behavior unending cyclic price wars long episodes repeated undercutting amongst sellers alternate large jumps price price wars found rampant prior studies agent economy models kephart hanson sairamesh 1998 sairamesh kephart 1998 agents use myopically optimal myoptimal pricing algorithms optimize immediate reward anticipate longerterm consequences agents current price setting motivation studying simultaneous qlearning paper three fold first qfunctions learned simultaneously selfconsistently players policies implied qfunctions selfconsistently optimal words agent able correctly anticipate longerterm consequences actions agents actions correctly model agents equivalent capability hence classic problem infinite recursion opponent models avoided contrast approaches adaptive multiagent system issues problematic example vidal durfee 1998 propose recursive opponent modeling scheme level0 agents opponent modeling level1 agents model opponents level0 level2 agents model opponents level1 etc approaches effective way agent model agents equivalent level depth complexity second advantage qlearning solutions correspond deep lookahead principle qfunction represents expected reward looking infintely far ahead time exponentially weighted discount parameter contrast prior work tesauro kephart 1999 based shallow finite lookahead finally comparison directly modeling agent policies qfunction approach seems extensible situation large economies many competing sellers intuition approximating qfunctions nonlinear function approximators neural networks feasible approximating corresponding policies fur thermore qfunction approach agent needs maintain single qfunction whereas policy modeling approach agent needs maintain policy model every agent latter seems infeasible number sellers large remainder paper organized follows section 2 describes structure dynamics model twoseller economy presents three economicallybased models seller profit pricequality informationfiltering shopbot known prone price wars agents myopically optimize shortterm payoffs deliberately choose parameters place systems pricewar regime section 3 describe details implement qlearning model economies first step examine simple case ordinary qlearning one two sellers uses qlearning seller uses fixed pricing policy myopically opti mal myoptimal policy examine interesting novel situation simultaneous qlearning sellers finally section 5 summarizes main conclusions discusses promising directions challenges future work model agent economies real agent economies likely contain large numbers agents complex details agents behave interact multiple time scales approach toward modeling understanding complexity begin making number simplifying assumptions first consider simplest possible case two competing seller agents offering similar identical products large population consumer agents sellers compete basis price assume prices discretized lie minimumand maximum price number possible prices hundred renders state space small enough feasible use lookup tables represent agents pricing policies expected profits time simulation also discretized time step assume consumers compare current prices two sellers instantaneously deterministically choose purchase one seller hence time step possible pair seller prices deterministic reward profit given seller simulation iterate forever may may discounting factor present value future rewards worth noting consumers regarded players model consumers strategic role behave according extremely simple fixed shortterm greedy rule buy lowest priced product time step regarded merely providing stationary environment two sellers compete twoplayer game clearly simplifying first step study multiagent phenomena future work models extended include strategic adaptive behavior part consumers well change notion desirable system behavior present model desirable behavior would resemble collusion two sellers charging high prices could obtain high profits obviously desirable consumers viewpoint regarding dynamics seller price adjustments assume sellers alternately take turns adjusting prices rather simultaneously setting prices ie game extensiveform rather normalform choice alternatingturn dynamics motivated two considerations number sellers becomes large model becomes realistic seems reasonable assume sellers adjust prices different times rather time although probably take turns welldefined order b alternatingturn dynamics stay within normal qlearning framework qfunction implies deterministic optimal policy known twoplayer alternating turn games always exists deterministic policy good nondeterministic policy littman 1994 contrast games simultaneous moves example rockpaperscissors possible deterministic policy optimal existing qlearning formalism mdps would modified extended could yield nondeterministic optimal policies study qlearning three different economic models described detail elsewhere sairamesh kephart 1998 kephart hanson sairamesh 1998 greenwald kephart 1999 first model called pricequality model sairamesh kephart 1998 models sellers products distinguished different values scalar quality parameter higherquality products perceived valuable consumers consumers modeled trying obtain lowestpriced product time step subject thresholdtype constraints quality price ie consumer maximum allowable price minimum allowable qual ity similarity substitutability seller products leads potential direct price competition however vertical differentiation due differen ing quality values leads asymmetry sellers profit functions believed asymmetry responsible unending cyclic price wars emerge sellers employ myoptimal pricing strategies second model informationfiltering model described detail kephart hanson sairamesh 1998 model two competing sellers news articles somewhat overlapping categories contrast vertical differentiation pricequality model model contains horizontal differentiation differing article categories extent categories overlap direct price competition extent differ asymmetries introduced lead potential cyclic price wars third model socalled shopbot model described greenwald kephart 1999 intended model situation internet consumers may use shopbot compare prices sellers offering given product select seller lowest price model sellers products exactly identical profit functions symmetric myoptimal pricing leads sellers undercut minimum price point reached point new price war cycle launched due buyer asymmetries rather seller asymmetries fact buyers use shopbot buyers instead choose seller random means profitable seller abandon lowprice competition bargain hunters instead maximally exploit random buyers charging maximum possible price example profit function study taken pricequality model follows let p 1 p 2 represent prices charged seller 1 seller 2 respectively let q 1 q 2 represent respective quality parameters cost seller producing item quality q assuming particular model consumer behavior described sairamesh kephart 1998 one show analytically limit infinitely many consumers instantaneous profits per consumer r 1 r 2 obtained seller 1 seller 2 respectively given ae 1 ae 2 plot profit landscape seller 1 function prices p 1 p 2 given figure 1 following parameter settings q q specific parameter settings chosen known generate harmful price wars agents use myopic optimal pricing see figure myopic optimal price seller 1 function seller 2s price p p 2 obtained value p 2 sweeping across values p 1 choosing value gives highest profit see small values p 2 peak profit obtained whereas larger values p 2 eventually discontinuous shift peak follows along parabolicshaped ridge landscape analytic expression myopic optimal price seller 1 function p 2 follows defining x similarly myopic optimal price seller 2 function price set seller 1 p p 1 given following formula assuming prices discrete ffl price discretization interval also note passing similar profit landscapes sellers informationfiltering model shopbot model three fig 1 sample profit landscape seller 1 pricequality model function seller 1 price p1 seller 2 price p2 models existence multiple disconnected peaks landscapes relative heights change depending sellers price leads price wars sellers behave myopically regarding information set made available sellers made simplifying assumption first step players essentially perfect information model consumer behavior perfectly also perfect knowledge others costs profit functions hence model thus twoplayer perfectinformation deterministic game similar games like chess main differences profits model strictly zerosum terminating absorbing nodes models state space also model payoffs given players every time step whereas games chess payoffs given terminating nodes mentioned previously constrain prices set two sellers lie range minimum maximum allowable price prices discretized one create lookup tables seller profit functions furthermore optimal pricing policies seller function sellers price p p 2 p p 1 also represented form table lookups 3 singleagent qlearning first consider ordinary singleagent qlearning twoseller economic models procedure qlearning follows let qs represent discounted longterm expected reward agent taking action state discounting future rewards accomplished discount parameter fl value reward expected n time steps future discounted fl n assume qs function represented lookup table containing value every possible stateaction pair assume table entries initialized arbitrary values procedure solving qs infinitely repeat following twostep loop 1 select particular state particular action observe immediate reward r stateaction pair observe resulting state 0 2 adjust qs according following equation deltaqs ff learning rate parameter max operation represents choosing optimal action b among possible actions taken successor state 0 leading greatest qvalue wide variety methods may used select stateaction pairs step 1 provided every stateaction pair visited infinitely often stationary markov decision problem qlearning procedure guaranteed converge correct values provided ff decreased time appropriate schedule first consider using qlearning one two sellers economic models seller maintains fixed pricing policy simulations described fixed policy fact myoptimal policy p represented example pricequality model equations 3 4 pricing application distinction states actions somewhat blurred assume state seller sufficiently described sellers last price action current price decision sufficient state description history needed either determination immediate reward calculation myoptimal price fixedstrategy player also modified concepts immediate reward r nextstate 0 twoagent case define 0 state obtained starting one action qlearner response action fixedstrategy opponent likewise immediate reward defined sum two rewards obtained two actions modifications introduced state 0 would player move state possible alternative investigated include sidetomove additional information statespace description simulations reported sequence stateaction pairs selected qtable updates generated uniform random selection amongst possible table entries initial values qtables generally set immediate reward values consequently initial qderived policies corresponded myoptimal policies learning rate varied time according initial learning rate ff0 usually set 01 constant simulation time measured units n 2 size qtable n number possible prices could selected either player number different values discount parameter fl studied ranging results singleagent qlearning three models indicated qlearning worked well expected case model value discount parameter exact convergence qtable stationary optimal solution found convergence times ranged hundred sweeps table element smaller values fl thousand updates largest values fl addition qlearning converged measured expected cumulative profit policy derived qfunction ran qpolicy players myopic policy 100 random starting states 200 time steps averaged resulting cumulative profit player found case seller achieved greater profit myopic opponent using qderived policy using myopic policy true even due redefinition q updates summing two time steps case effectively corresponds twostep optimization rather onestep optimization myopic policies furthermore cumulative profit obtained qderived policy monotonically increased increasing fl expected also interesting note many cases expected profit myopic opponent also increased playing qlearner also improved monotonically increasing fl explanation rather better exploiting myopic opponent would expected zerosum game qlearner instead reduced region would participate mutually undercutting price war typically find models myopic vs myopic play largeamplitude price wars generated start high prices persist way low prices q learner competes myopic opponent still price wars starting high prices however qlearner abandons price war quickly prices decrease effect pricewar regime smaller confined higher average prices leading closer approximation cooperative collusive behavior greater expected utilites players illustrative example results singleagent qlearning shown figure 2 figure 2a plots average profit sellers shopbot model one sellers myopic qlearner model symmetric doesnt matter seller qlearner figure 2b plots myopic price curve seller 2 qderived price curve seller 1 see curves maximum price 1 minimum price approximately 058 portion curves lying vs q shopbot model myopic vs myopic average profit pfig 2 results singleagent qlearning shopbot model average profit per time step qlearner seller 1 filled circles myopic seller seller 2 open circles vs discount parameter fl dashed line indicates baseline expected profit sellers myopic b crossplot qderived price curve seller 1 vs myopic price curve seller 2 dashed line arrows indicate temporal pricepair trajectory using policies starting filled circle along diagonal indicates undercutting behavior case seller respond opponents price undercutting ffl price discretization interval system dynamics state p 1 figure 2b obtained alternately applying two pricing policies done simple iterative graphical construction given starting point one first holds moves horizontally p 1 p 2 curve one holds moves vertically p 2 p 1 curve see figure iterative graphical construction leads unending cyclic price war whose trajectory indicated dashed line note pricewar behavior begins price pair 1 1 persists price approximately 083 point seller 1 abandons price war resets price 1 leading another round undercutting amplitude price war diminished compared situation players use myopic policy case seller 1s curve would mirror image seller 2s curve price war would persist way minimum price point leading lower expected profit sellers multiagent qlearning examine interesting challenging case simultaneous training qfunctions policies sellers approach use formalism presented previous section alternately adjust random entry seller 1s qfunction followed random entry seller 2s qfunction sellers qfunction evolves sellers pricing policy correspondingly updated optimizes agents current qfunction modeling twostep payoff r seller equation 5 use opponents current policy implied current qfunction parameters experiments generally set values previous section experiments qfunctions initialized instantaneous payoff values policies corresponded myopic policies although initial conditions explored experiments vs q pq model myopic vs myopic 1 myopic vs myopic 2 average profit vs q pq model g fig 3 results simultaneous qlearning pricequality model average profit per time step seller 1 solid diamonds seller 2 open diamonds vs discount parameter fl dashed line indicates baseline myopic vs myopic expected profit note seller 2s profit higher seller 1s even though seller 2 lower quality parameter b crossplot qderived price curves fl dashed line arrows indicate sample price dynamics trajectory starting filled circle price war eliminated dynamics evolves fixed point indicated open circle simultaneous qlearning pricequality model find robust convergence unique pair pricing policies independent value fl illustrated figure 3b solution also corresponds solution found generalized minimax generalized dp tesauro kephart 1999 note repeated application pair price curves leads dynamical trajectory eventually converges fixedpoint located p 04 detailed analysis pricing policies fixedpoint solution presented tesauro kephart 1999 brief sufficiently low prices seller 2 pays seller 1 abandon price war charge high price 09 value corresponds highest price seller 2 charge without provoking undercut seller 1 based twostep lookahead calculation seller 1 undercuts seller 2 replies undercut note fixed point correspond nash equilibrium since players incentive deviate based onestep lookahead calculation conjectured tesauro kephart 1999 solution observed figure 3b corresponds subgameperfect equilibrium fudenberg tirole 1991 rather nash equilibrium cumulative profits obtained pair pricing policies plotted figure 3a interesting seller 2 lowerquality seller actually obtains significantly higher profit seller 1 higherquality seller contrast myopic vs myopic pricing seller 2 worse seller 1 vs q shopbot model myopic vs myopic average profit fig 4 results simultaneous qlearning shopbot model average profit per time step seller 1 solid diamonds seller 2 open diamonds vs discount parameter fl dashed line indicates baseline myopic vs myopic expected profit b crossplot qderived price curves solution symmetric dashed line arrows indicate sample price dynamics trajectory c crossplot qderived price curves 09 solution asymmetric shopbot model find exact convergence qfunctions value fl however cases exact convergence found find good approximate convergence qfunctions policies converged stationary solutions within small random fluctua tions different solutions obtained value fl generally find symmetric solution shapes iden tical obtained small fl whereas broken symmetry solution similar pricequality solution obtained large fl also found range fl values 01 02 either symmetric asymmetric solution could obtained depending initial conditions asymmetric solution counterintuitive us expected symmetry two sell ers profit functions would lead symmetric solution hindsight apply type reasoning pricequality model explain asymmetric solution plot expected profit sellers function fl shown figure 4a plots symmetric asymmetric solution obtained respectively shown figures 4b 4c myopic vs myopic 1 myopic vs myopic 2 average profit vs q model g05 pfig 5 results multiagent qlearning informationfiltering model average profit per time step seller 1 solid diamonds seller 2 open diamonds vs discount parameter fl data points qfunctions policies dashed lines indicates baseline expected profit sellers myopic b crossplot qderived price curves finally informationfiltering model found simultaneous qlearning produced exact good approximate convergence small values 05 large values fl convergence obtained simultaneous qlearning solutions yielded reducedamplitude price wars montonically increasing profitability sellers function fl least 05 data points examined fl 05 even though convergence qpolicies still yielded greater profit sellers myopic vs myopic case plot qderived policies system dynamics shown figure 5b expected profits players function fl plotted figure 5a conclusions examined singleagent multiagent qlearning three models twoseller economy sellers alternately take turns setting prices instantaneous profits given sellers based current price pair models fall category twoplayer alternatingturn arbitrarysum markov games rewards statespace transitions deterministic game markov state space fully observable rewards history dependent three models pricequality informationfiltering shopbot largeamplitude cyclic price wars obtained sellers myopically optimize instantaneous profits without regard longerterm impact pricing policies find three models use qlearning one sellers myopic opponent invariably results exact convergence optimal qfunction optimal policy opponent allowed values discount parameter fl use qderived policy yields greater expected profit qlearner monotonically increasing profit fl increases many cases side benefit also enhancing welfare myopic opponent comes reducing amplitude undercutting pricewar regime cases eliminating completely also studied interesting challenging situation simultaneously training qfunctions sellers difficult sellers qfunction policy change provides nonstationary environment adaptation seller convergence proofs exist simultaneous qlearning multiple agents nevertheless despite absence theoretical guarantees find generally good behavior algorithm model economies two models shopbot pricequality find exact good approximate convergence simultaneously selfconsistent qfunctions optimal policies value fl whereas informationfiltering model simultaneous convergence found fl 05 informationfiltering shopbot models monotonically increasing expected profits sellers also found small values fl pricequality model simultaneous qlearning yields asymmetric solution corresponding solution found tesauro kephart 1999 highly advantageous lesserquality seller slightly disadvantageous higherquality seller compared myopic vs myopic pricing similar asymmetric solution also found shopbot model large fl even though profit functions players symmetric model exists range discount parameter values solutions obtained simultaneous qlearning selfconsistently optimal outperform solutions obtained tesauro kephart 1999 presumably previously published methods based limited lookahead whereas qfunctions principle look ahead infinitely far appropriate discounting intruiging simultaneous qlearning works well models despite lack theoretical convergence proofs sandholm crites also found simultaneous qlearning generally converged iterated prisoners dilemma game empirical findings suggest deeper theoretical analysis simultaneous qlearning may worth investigating may underlying theoretical principles explain simultaneous qlearning works least certain classes arbitrarysum profit functions several important challenges also faced extending approach largerscale realistic simulations economic situations real world two dominant sellers general number sellers much greater situation foresee agent economies number competing sellers large case seller profits pricing functions high input dimensionality infeasible use lookup table statespace representations likely sort compact representation combined function approximation scheme necessary furthermore many sellers concept sellers taking turns adjusting prices welldefined order becomes problematic could lead additional combinatorial explosion mechanism calculating expected reward anticipate possible orderings opponent responses furthermore economic models moderate degree realism profit functions unrealistic assumptions knowledge dynamics work reported state space fully observable infinitely frequently zero cost zero propagation delays expected consumer demand given price pair instantaneous deterministic fully known players indeed players exact profit functions fully known players also assumed players would alternately take turns equally often welldefined order adjusting prices assumptions knowledge dynamics one could hope develop algorithm could calculate advance something like gametheoretic optimal pricing algorithm agent however realistic agent economies likely agents much less full knowledge state economy agents may know details agents profit functions indeed agent may know profit function extent buyer behavior unpredictable dynamics buyers sellers may also complex random unpredictable assumed may also information delays buyers sellers part economic game may involve paying cost order obtain information state economy faster frequently greater detail finally expect buyer behavior nonstationary complex coevolution buyer seller strategies realworld complexities daunting reasons believe learning approaches qlearning may play role practical solu tions advantage qlearning one need model either instantaneous payoffs statespace transitions environment one simply observe actual rewards transitions base learning theory qlearning requires exhaustive exploration state space guarantee convergence may necessary function approximators used case training function approximator relatively small number observed states may generalize well enough unobserved states give decent practical performance several recent empirical studies provided evidence tesauro 1995 crites barto 1996 zhang dietterich 1996 acknowledgements authors thank amy greenwald helpful discussions regarding shopbot model r improving elevator performance using reinforcement learning game theory shopbots pricebots multiagent reinforcement learning theoretical framework algorithm pricewar dynamics freemarket economy software agents course microeconomic theory markov games framework multiagent reinforcement learn ing dynamics price quality differentiation information computational markets multiagent qlearning semicompetitive domain temporal difference learning tdgammon foresightbased pricing algorithms economy software agents foresightbased pricing algorithms agent economies learning nested agent models information economy learning delayed rewards qlearning highperformance jobshop scheduling timedelay td network tr ctr prithviraj raj dasgupta yoshitsugu hashimoto multiattribute dynamic pricing online markets using intelligent agents proceedings third international joint conference autonomous agents multiagent systems p277284 july 1923 2004 new york new york simon parsons michael wooldridge game theory decision theory multiagent systems autonomous agents multiagent systems v5 n3 p243254 september 2002 leigh tesfatsion agentbased computational economics growing economies bottom artificial life v8 n1 p5582 march 2002 cooperative multiagent learning state art autonomous agents multiagent systems v11 n3 p387434 november 2005