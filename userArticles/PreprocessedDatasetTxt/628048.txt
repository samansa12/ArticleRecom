natural language grammatical inference recurrent neural networks abstractthis paper examines inductive inference complex grammar neural networksspecifically task considered training network classify natural language sentences grammatical ungrammatical thereby exhibiting kind discriminatory power provided principles parameters linguistic framework governmentandbinding theory neural networks trained without division learned vs innate components assumed chomsky attempt produce judgments native speakers sharply grammaticalungrammatical data recurrent neural network could possess linguistic capability properties various common recurrent neural network architectures discussed problem exhibits training behavior often present smaller grammars training initially difficult however implementing several techniques aimed improving convergence gradient descent backpropagationthroughtime training algorithm significant learning possible found certain architectures better able learn appropriate grammar operation networks training analyzed finally extraction rules form deterministic finite state automata investigated b introduction paper considers task classifying natural language sentences grammatical ungrammatical attempt train neural networks without bifurcation learned vs innate components assumed chomsky produce judgments native speakers sharply grammaticalungrammatical data recurrent neural networks investigated computational reasons computationally recurrent neural networks powerful feedforward networks recurrent architectures shown least turing equivalent 53 54 investigate properties various popular recurrent neural network architectures particular elman narendra parthasarathy np williams zipser recurrent networks also frasconigorisoda fgs locally recurrent networks find elman wz recurrent neural networks able learn appropriate grammar implementing techniques improving convergence gradient descent based backpropagationthroughtime training algorithm analyze operation networks investigate rule approximation recurrent network learned specifically extraction rules form deterministic finite state automata previous work 38 compared neural networks machine learning paradigms problem work focuses recurrent neural networks investigates additional networks analyzes operation networks training algorithm investigates rule extraction paper organized follows section 2 provides motivation task attempted section 3 provides brief introduction formal grammars grammatical inference describes data section 4 lists recurrent neural network models investigated provides details data encoding networks section 5 presents results investigation various training heuristics investigation training simulated annealing section 6 presents main results simulation details investigates operation networks extraction rules form deterministic finite state automata investigated section 7 section 8 presents discussion results conclusions motivation 21 representational power natural language traditionally handled using symbolic computation recursive processes successful stochastic language models based finitestate descriptions ngrams hidden markov models however finitestate models cannot represent hierarchical structures found natural language 1 48 past years several recurrent neural network architectures emerged 1 insideoutside reestimation algorithm extension hidden markov models intended useful learning hierarchical systems algorithm currently practical relatively small grammars 48 used grammatical inference 9 21 19 20 68 recurrent neural networks used several smaller natural language problems eg papers using elman network natural language tasks include 1 12 24 58 59 neural network models shown able account variety phenomena phonology 23 61 62 18 22 morphology 51 41 40 role assignment 42 58 induction simpler grammars addressed often eg 64 65 19 learning tomita languages 60 task considered differs grammar complex recurrent neural networks investigated paper constitute complex dynamical systems shown recurrent networks representational power required hierarchical solutions 13 turing equivalent 22 language acquisition certainly one important questions study human language people unfailingly manage acquire complex rule system system complex resisted efforts linguists date adequately describe formal system 8 couple examples kind knowledge native speakers often take granted provided section instance native speaker english knows adjective eager obligatorily takes complementizer sentential complement contains overt subject verb believe cannot moreover eager may take sentential complement nonovert ie implied understood subject believe cannot eager john believe john eager john believe john eager believe grammaticality judgments sometimes subtle unarguably form part native speakers language competence cases judgment falls acceptability aspects language competence interpretation consider reference embedded subject predicate talk following examples john stubborn mary talk john stubborn talk john stubborn talk bill first sentence clear mary subject embedded predicate every native speaker knows strong contrast coreference options understood subject second 2 conventional asterisk used indicate ungrammaticality third sentences despite surface similarity third sentence john must implied subject predicate talk contrast john understood object predicate second sentence subject arbitrary reference words sentence read john stubborn arbitrary person talk john point emphasize language faculty impressive discriminatory power sense single word seen examples result sharp differences acceptability alter interpretation sentence considerably furthermore judgments shown robust sense virtually native speakers agree data light examples fact contrasts crop english languages example stubborn contrast also holds dutch linguists chiefly chomsky 7 hypothesized reasonable knowledge partially acquired lack variation found across speakers indeed languages certain classes data suggests exists fixed component language system words innate component language faculty human mind governs language processing languages obey socalled universal principles since languages differ regard things like subjectobjectverb order principles subject parameters encoding systematic variations found particular languages innateness hypothesis language parameters plus languagespecific lexicon acquired speaker particular principles learned based assumptions study languageindependent principles become known principlesandparameters framework governmentandbinding gb theory paper investigates whether neural network made exhibit kind discriminatory power sort data gblinguists examined precisely goal train neural network scratch ie without division learned vs innate components assumed chomsky produce judgments native speakers grammaticalungrammatical pairs sort discussed instead using innate knowledge positive negative examples used second argument innateness possible learn grammar without negative examples 3 data first provide brief introduction formal grammars grammatical inference natural language thorough introduction see harrison 25 fu 17 detail dataset used experiments 31 formal grammars grammatical inference briefly grammar g four tuple fn sets terminals nonterminals comprising alphabet grammar p set production rules start symbol every exists language l set strings terminal symbols grammar generates recognizes also exist automata recognize generate grammar grammatical inference concerned mainly procedures used infer syntactic production rules unknown grammar g based finite set strings lg language generated g possibly also finite set strings complement lg 17 paper considers replacing inference algorithm neural network grammar english language simple grammar used elman 13 shown table 1 contains structures complete english grammar eg agreement verb argument structure interactions relative clauses recursion cat mary table 1 simple grammar encompassing subset english language 13 phrase full sentence chomsky hierarchy phrase structured grammars simplest grammar associated automata regular grammars finitestateautomata fsa however firmly established 6 syntactic structures natural language cannot parsimoniously described regular languages certain phenomena eg center embedding compactly described contextfree grammars recognized pushdown automata others eg crossedserial dependencies agreement better described contextsensitive grammars recognized linear bounded automata 50 32 data data used work consists 552 english positive negative examples taken introductory gblinguistics textbook lasnik uriagereka 37 examples organized minimal pairs like example eager john wini eager john win minimal nature changes involved suggests dataset may represent especially difficult task models due small sample size raw data namely words first converted using existing parser major syntactic categories assumed gbtheory table 2 summarizes parts speech used partofspeech tagging represents sole grammatical information supplied models particular sentences addition grammaticality status important refinement implemented category examples nouns n john book destruction verbs v hit sleep adjectives eager old happy prepositions p without complementizer c thought eager determiner man man adverb adv sincerely sincerely believe john want marker mrkr possessive johns mother destruction want help table 2 parts speech include subcategorization information major predicates namely nouns verbs adjectives prepositions experiments showed adding subcategorization bare category information improved performance models example intransitive verb sleep would placed different class obligatorily transitive verb hit similarly verbs take sentential complements double objects seem give persuade would representative classes 3 fleshing subcategorization requirements along lines lexical items training set resulted 9 classes verbs 4 nouns adjectives 2 prepositions examples input data shown table 3 sentence encoding grammatical status eager john n4 v2 a2 c n4 v2 adv 1 eager john n4 v2 a2 n4 v2 adv 0 eager n4 v2 a2 v2 adv 1 table 3 examples partofspeech tagging tagging done completely contextfree manner obviously word eg may part one partofspeech tagging resulted several contradictory duplicated sentences various methods 3 following classical gb theory classes synthesized thetagrids individual predicates via canonical structural realization csr mechanism pesetsky 49 tested deal cases however removed altogether results reported addition number positive negative examples equalized randomly removing examples higher frequency class training test sets order reduce effects due differing priori class probabilities number samples per class varies classes may bias towards predicting common class 3 2 4 neural network models data encoding following architectures investigated architectures 1 3 topological restrictions 4 number hidden nodes equal sense may representational capability model 4 expected frasconigorisoda fgs architecture unable perform task included primarily control case 1 frasconigorisoda locally recurrent networks 16 multilayer perceptron augmented local feedback around hidden node localoutput version used fgs network also studied 43 network called fgs paper line 63 2 narendra parthasarathy 44 recurrent network feedback connections output node hidden nodes np network architecture also studied jordan 33 34 network called np paper line 30 3 elman 13 recurrent network feedback hidden node hidden nodes training elman network backpropagationthroughtime used rather truncated version used elman ie paper elman network refers architecture used elman training algorithm 4 williams zipser 67 recurrent network nodes connected nodes diagrams architectures shown figures 1 4 input neural networks data encoded fixed length window made segments containing eight separate inputs corresponding classifications noun verb adjective etc subcategories classes linearly encoded input manner demonstrated specific values noun input noun 0 noun class class 1 linear order defined according similarity various subcategories 4 two outputs used neural networks corresponding grammatical ungrammatical classifications data input neural networks window passed sentence temporal 4 fixed length window made segments containing 23 separate inputs corresponding classifications noun class 1 noun class 2 verb class 1 etc also tested proved inferior figure 1 frasconigorisoda locally recurrent network connections shown fully figure 2 narendra parthasarathy recurrent network connections shown fully figure 3 elman recurrent network connections shown fully figure 4 williams zipser fully recurrent network connections shown fully order beginning end sentence see figure 5 size window variable one word length longest sentence note case input window small greater interest larger input window greater capability network correctly classify training data without forming grammar example input window equal longest sentence network store information simply map inputs directly classification however input window relatively small network must learn store information shown later networks implement grammar deterministic finite state automaton recognizes grammar extracted network thus interested small input window case networks required form grammar order perform well gradient descent simulated annealing learning backpropagationthroughtime 66 5 used train globally recurrent networks 6 gradient descent algorithm described authors 16 used fgs network standard gradient descent algorithms found impractical problem 7 techniques described improving convergence investigated due dependence initial parameters number simulations performed different initial weights training settest set combinations however due computational complexity task 8 possible perform many simulations 5 backpropagationthroughtime extends backpropagation include temporal aspects arbitrary connection topologies considering equivalent feedforward network created unfolding recurrent network time 6 realtime 67 recurrent learning rtrl also tested show significant convergence present problem modifying standard gradient descent algorithms possible train networks operated large temporal input window networks forced model grammar memorized interpolated training data 8 individual simulation section took average two hours complete sun figure 5 depiction neural network inputs come input window sentence window moves beginning end sentence desired standard deviation nmse values included help assess significance results table 4 shows results using using techniques listed except noted results section elman networks using two word inputs 10 hidden nodes quadratic cost function logistic sigmoid function sigmoid output activations one hidden layer learning rate schedule shown initial learning rate 02 weight initialization strategy discussed million stochastic updates target values provided end sentence standard nmse std dev variation nmse std dev update batch 0931 00036 update stochastic 0366 0035 learning rate constant 0742 0154 learning rate schedule 0394 0035 activation logistic 0387 0023 activation tanh 0405 014 sectioning 0367 0011 sectioning yes 0573 0051 cost function quadratic 0470 0078 cost function entropy 0651 00046 table 4 comparisons using using various convergence techniques parameters constant case elman networks using two word inputs ie sliding window current previous word 10 hidden nodes quadratic cost function logistic activation function sigmoid output activations one hidden layer learning rate schedule initial learning rate 02 weight initialization strategy discussed 1 million stochastic updates nmse result represents average four simulations standard deviation value given standard deviation four individual results 1 detection significant error increases nmse increases significantly training network weights restored previous epoch perturbed prevent updating point technique found increase robustness algorithm using learning rates large enough help avoid problems due local minima flat spots error surface particularly case williams zipser network 2 target outputs targets outputs 01 09 using logistic activation function 08 08 using tanh activation function helps avoid saturating sigmoid function targets set asymptotes sigmoid would tend drive weights infinity b cause outlier data produce large gradients due large weights c produce binary outputs even incorrect leading decreased reliability confidence measure 3 stochastic versus batch update stochastic update parameters updated pattern presen tation whereas true gradient descent often called batch updating gradients accumulated complete training set batch update attempts follow true gradient whereas stochastic path followed using stochastic update stochastic update often much quicker batch update especially large redundant datasets 39 additionally stochastic path may help network escape local minima however error jump around without converging unless learning rate reduced second order methods work well stochastic update stochastic update harder parallelize batch 39 batch update provides guaranteed convergence local minima works better second order techniques however slow may converge poor local minima results reported training times equalized reducing number updates batch case equal number weight updates batch update would otherwise much slower batch update often converges quicker using higher learning rate optimal rate used stochastic update 9 hence altering learning rate batch case investigated however significant convergence obtained shown table 4 4 weight initialization random weights initialized goal ensuring sigmoids start saturation small corresponding flat part error surface 26 ad dition several 20 sets random weights tested set provides best performance training data chosen experiments current problem found techniques make significant difference 5 learning rate schedules relatively high learning rates typically used order help avoid slow convergence local minima however constant learning rate results significant parameter performance fluctuation entire training cycle performance network 9 stochastic update generally tolerate high learning rate batch update due stochastic nature updates alter significantly beginning end final epoch moody darken proposed search converge learning rate schedules form 10 11 1 jt learning rate time j 0 initial learning rate constant found learning rate final epoch still results considerable parameter fluc hence added additional term reduce learning rate final epochs specific learning rate schedule found later section found use learning rate schedules improve performance considerably shown table 4 6 activation function symmetric sigmoid functions eg tanh often improve convergence standard logistic function particular problem found difference minor logistic function resulted better performance shown table 4 7 cost function relative entropy cost function 4 29 57 26 27 received particular attention natural interpretation terms learning probabilities 36 investigated using quadratic relative entropy cost functions 1 quadratic cost function defined relative entropy cost function defined 3where correspond actual desired output values k ranges outputs also patterns batch update found quadratic cost function provide better performance shown table 4 possible reason use entropy cost function leads increased variance weight updates therefore decreased robustness parameter updating 8 sectioning training data investigated dividing training data subsets initially one subsets used training 100 correct classification obtained prespecified time limit expired additional subset added working set continued working set contained entire training set data ordered terms sentence length results obtained epoch involving stochastic update misleading surprised find quite significant difference online nmse calculations compared static calculation even algorithm appears converged shortest sentences first enabled networks focus simpler data first elman suggests initial training constrains later training useful way 13 however problem use sectioning consistently decreased performance shown table 4 also investigated use simulated annealing simulated annealing global optimization method 32 35 minimizing function downhill step accepted process repeats new point uphill step may also accepted therefore possible escape local minima optimization process proceeds length steps declines algorithm converges global optimum simulated annealing makes assumptions regarding function optimized therefore quite robust respect nonquadratic error surfaces previous work shown use simulated annealing finding parameters recurrent network model improve performance 56 comparison gradient descent based algorithms use simulated annealing investigated order train exactly elman network successfully trained 100 correct training set classification using backpropagationthroughtime details section 6 significant results obtained trials 11 use simulated annealing found improve performance simard et al 56 however problem parity problem using networks four hidden units whereas networks considered paper many parameters result provides interesting comparison gradient descent backpropagationthroughtime bptt method bptt makes implicit assumption error surface amenable gradient descent optimization assumption major problem practice however although difficulty encountered bptt method significantly successful simulated annealing makes assumptions problem 6 experimental results results four neural network architectures given section results based multiple trainingtest set partitions multiple random seeds addition set japanese control data used test set consider training models japanese data large enough dataset japanese english opposite ends spectrum regard word order japanese sentence patterns different english particular japanese sentences typically sov subjectobjectverb verb less fixed arguments less available freely permute english data course svo argument permutation generally available example canonical japanese word order simply ungrammatical english hence would extremely surprising englishtrained model accepts japanese ie expected network trained 11 adaptive simulated annealing code lester ingber 31 32 used english generalize japanese data find models resulted significant generalization japanese data 50 error average five simulations performed architecture simulation took approximately four hours summarizes results obtained various networks order make number weights architecture approximately equal used single word inputs wz model two word inputs others reduction dimensionality wz network improved performance networks contained 20 hidden units full simulation details given section 6 goal train network using small temporal input window initially could done addition techniques described earlier possible train elman networks sequences last two words input give 100 correct 996 averaged 5 trials classification training data generalization test data resulted 742 correct classification average better performance obtained using networks however still quite low data quite sparse expected increased generalization performance obtained amount data increased well increased difficulty training additionally dataset handdesigned gb linguists cover range grammatical structures likely separation training test sets creates test set contains many grammatical structures covered training set williams zipser network also performed reasonably well 713 correct classification test set note test set performance observed drop significantly extended training indicating use validation set control possible overfitting would alter performance significantly train classification std dev elman 996 084 fgs 671 122 wz 917 226 english test classification std dev elman 742 382 fgs 590 152 wz 713 075 table 5 results network architecture comparison classification values reported average five individual simulations standard deviation value standard deviation five individual results complete details sample elman network follows networks differ topology except wz better results obtained using input window one word network contained three layers including input layer hidden layer contained 20 nodes hidden layer node recurrent connection hidden layer nodes network trained total 1 million stochastic updates inputs within range zero one target outputs either 01 09 bias inputs used best 20 random weight sets chosen based training set performance weights initialized shown haykin 26 weights initialized node node basis uniformly distributed random numbers range gamma24f fanin neuron logistic output activation function used quadratic cost function used search converge learning rate schedule used learning rate learning rate training epochs current training epoch c 065 training set consisted 373 noncontradictory examples described earlier english test set consisted 100 noncontradictory samples japanese test set consisted 119 noncontradictory samples take closer look operation networks error training sample network architecture shown figure 6 error point graphs nmse complete training set note nature williams zipser learning curve utility detecting correcting significant error increases 12 figure 7 shows approximation complexity error surface based first derivatives error criterion respect weight sums weights network nw total number weights value plotted epoch training note complex nature plot williams zipser network figures 8 11 show sample plots error surface various networks error surface many dimensions making visualization difficult plot sample views showing variation error two dimensions note plots indicative quantitative conclusions drawn test error plots plot shown figures respect two randomly chosen dimensions case center plot corresponds values parameters training taken together plots provide approximate indication nature error surface different network types fgs network error surface appears smoothest however results indicate solutions found perform well indicating minima found poor compared global optimum andor network capable implementing mapping low error williams zipser fully connected network greater representational capability elman architecture sense perform greater variety computations number hidden units however comparing elman wz network error surface plots observed wz network greater percentage flat spots graphs conclusive show two dimensions plotted around one point weight space however back 12 learning curve williams zipser network made smoother reducing learning rate tends promote convergence poorer local minima epoch epoch epoch epoch wz figure 6 average nmse log scale training set training top bottom frasconigorisoda elman narendra parthasarathy williams zipser hypothesis wz network performs worse error surface presents greater difficulty training method 7 automata extraction extraction symbolic knowledge trained neural networks allows exchange information connectionist symbolic knowledge representations great interest understanding neural network actually 52 addition symbolic knowledge inserted recurrent neural networks even refined training 15 47 45 ordered triple discrete markov process fstate input nextstateg extracted rnn epoch epoch epoch epoch wz figure 7 approximate complexity error surface training top bottom frasconigorisoda elman narendra parthasarathy williams zipser used form equivalent deterministic finite state automata dfa done clustering activation values recurrent state neurons 46 automata extracted process recognize regular grammars 13 however natural language 6 cannot parsimoniously described regular languages certain phenomena eg center embedding compactly described contextfree grammars others eg crossedserial dependencies agreement better described contextsensitive grammars hence networks may implementing parsimonious versions grammar unable extract technique 13 regular grammar g 4tuple start symbol n nonterminal terminal symbols respectively p represents productions form ab b 2 n 2 weight 0 04992 6 5 4 3 2 1 weight 0 04992 weight 41 007465 5 4 3 2 1 weight 162 049550708096 5 4 3 2 weight 109 00295 5 4 3 2 115 12 weight 28 001001 6 5 4 3 2 weight 43 0075290675068506950705 weight 96 0013050684 0685 0686 0687 0688 0689 0690692 0693 weight 146 01471 6 5 4 3 2 1weight 45 012570685 069 weight 81 01939 6 5 4 3 2 15 weight weight 144 01274 5 4 3 2 1 weight 67 07342066 068072 074 weight 5 4 3 2 weight 101 003919 6 5 4 3 2 weight 76 038080682 0684 0686 0688 069 weight 13 1463 5 4 3 2 1 0weight 7 06851 weight 60 0334 5 4 3 2 1 weight figure 8 error surface plots fgs network plot respect two randomly chosen dimensions case center plot corresponds values parameters training 2 1weight 0 3033 2 weight 0 3033122124126128 weight 132 1213 13 12 11 10 9 8 7 6 5 4 3 2 weight 100 7007124 126 128132 134 136 1385 4 3 2 weight 152 06957 8 7 6 5 4 3 2 1weight 47 2201 124 125 126 127 128 129 136 5 4 3 2 weight 84 01601 5 4 3 2 1 weight 152 06957122124126128 weight 24 01485 124 125 126 127 128 129131 weight 160 05467 6 5 4 3 2 weight 172 07099 124 125127 12813132 weight 105 2075 9 8 7 6 5 4 3 2 weight 13 1695 7 6 5 4 3 2 14 weight 8 10081245125512651275 weight 195 01977 3 2 1 weight 5 2464 124 126 128 13 weight weight 155 20941225 123 1235 124 1255 126 weight 163 2216 4 3 2 1 07 weight 138 1867 122 123 124 125 126 127 128 weight figure 9 error surface plots np network plot respect two randomly chosen dimensions case center plot corresponds values parameters training algorithm use automata extraction 19 works follows network trained even training apply procedure extracting network learned ie networks current conception dfa learned dfa extraction process includes following steps clustering recurrent network activation space form dfa states 2 constructing transition weight 0 3612 9 8 7 6 5 4 3 2 weight 0 36120602 06040608 0610614 0616 weight 102 07384 3 4 weight 122 832059061063065 weight 244 8597 8 7 6 5 4 3 2 1 weight 81 2907 weight weight 151 1307 weight 263 9384 10 9 8 7 6 5 4 3 2 1 weight 215 4061058 059061 062 weight 265 8874 5 4 3 2 1 weight weight 62 5243 7 6 5 4 3 2 1weight 226 1218056 057 058 059 06 weight 128 1025 14 13 12 11 10 9 8 7 6 5 4 weight 162 8527059061063 064066 weight 118 9467 weight weight 118 9467059061063065 weight 228 4723 5 4 3 2 1 0weight 8 0370705850595 06061062 weight weight 228 47230595060506150625 figure 10 error surface plots elman network plot respect two randomly chosen dimensions case center plot corresponds values parameters training weight 0 3879 2 14 weight 0 3879141514251435 weight 65 2406 15 14 13 12 11 10 9 8 7 6 5 weight 153 1014335 1434 weight weight 105 1408 weight 122 3084 15 14 13 12 11 10 9 8 7 6 5 weight 158 9731143352 143354 weight 74 8938 112 111 110 109 108 107 106 105 104 103 102 weight 204 10714351445 145146147 weight 192 958 15 14 13 12 11 10 9 8 7 6 5 weight 86 9935 weight 170 8782 weight 2 8368 weight 214 8855 5 4 3 2 1 weight 39 055991415 142 43 weight 147 3869 1 2 weight 223 5656141142 weight 210 1329 15 14 13 12 11 10 9 8 7 6 5 weight 86 993514142144146 weight 63 3889 8 7 6 5 4 3 2 13 weight 81 21651425143514451455 weight 88 8262 6 7 weight 85 9631139 14142 143145 146 figure 11 error surface plots wz network plot respect two randomly chosen dimensions case center plot corresponds values parameters training diagram connecting states together alphabet labelled arcs putting transitions together make full digraph forming loops 4 reducing digraph minimal representation hypothesis training network begins partition quantize state space fairly wellseparated distinct regions clusters represent corresponding states finite state automaton recently proved arbitrary dfas stably encoded recurrent neural networks 45 one simple way finding clusters divide neurons range q partitions equal width thus n hidden neurons exist q n possible partition states dfa constructed generating state transition diagram ie associating input symbol partition state left partition state activates initial partition state start state dfa determined initial value t0 next input symbol maps partition state value assume loop formed otherwise new state dfa formed dfa thus constructed may contain maximum q n states practice usually much less since partition states reached eventually process must terminate since finite number partitions available practice many partitions never reached derived dfa reduced minimal dfa using standard minimization algorithms 28 noted dfa extraction method may applied discretetime recurrent net regardless order hidden layers recently extraction process proven converge extract dfa learned encoded neural network 5 extracted dfas depend quantization level q extracted dfas using values q starting 3 used standard minimization techniques compare resulting automata 28 passed training test data sets extracted dfas found extracted automata correctly classified 95 training data 60 test data 7 smaller values q produced dfas lower performance larger values q produce significantly better performance sample extracted automata seen figure 12 difficult interpret extracted automata topic future research analysis extracted automata view aiding interpretation additionally important open question well extracted automata approximate grammar implemented recurrent network may regular grammar automata extraction may also useful improving performance system via iterative combination rule extraction rule insertion significant learning time improvements achieved training networks prior knowledge 46 may lead ability train larger networks encompass target grammar paper investigated use various recurrent neural network architectures fgs np elman wz classifying natural language sentences grammatical ungrammatical thereby exhibiting kind discriminatory power provided principles parameters linguistic framework governmentandbinding theory best worst performance architectures elman wz np fgs surprising elman network outperforms fgs np networks computational power elman networks shown least turing equivalent 55 np networks shown turing equivalent 54 within linear slowdown fgs networks figure 12 automata extracted elman network trained perform natural language task start state state 1 bottom left accepting state state 17 top right strings reach accepting state rejected recently shown computationally limited 14 elman networks special case wz networks fact elman wz networks top performers surprising however theoretically elman network outperformed wz network open question experimental results suggest training issue representational issue backpropagation throughtime bptt iterative algorithm guaranteed find global minima cost function error surface error surface different elman wz networks results suggest error surface wz network less suitable bptt training algorithm used however architectures learn representation grammar networks learning grammar hierarchy architectures increasing computational power given number hidden nodes give insight whether increased power used model complex structures found grammar fact powerful elman wz networks provided increased performance suggests able find structure data may possible model fgs network additionally investigation data suggests 100 correct classification training data two word inputs would possible unless networks able learn significant aspects grammar another comparison recurrent neural network architectures giles horne 30 compared various networks randomly generated 6 64state finite memory machines locally recurrent narendra parthasarathy networks proved good superior powerful networks like elman network indicating either task require increased power vanilla backpropagation throughtime learning algorithm used unable exploit paper shown elman wz recurrent neural networks able learn appropriate grammar discriminating sharply grammaticalungrammatical pairs used gblinguists however generalization limited amount nature data available expected increased difficulty encountered training models data used clear considerable difficulty scaling models considered larger problems need continue address convergence training algorithms believe improvement possible addressing nature parameter updating gradient descent however point must reached improvement gradient descent based algorithms requires consideration nature error surface related input output encodings rarely chosen specific aim controlling error surface ability parameter updates modify network behavior without destroying previously learned information method network implements structures hierarchical recursive relations acknowledgments work partially supported australian telecommunications electronics research board sl r sequential connectionist networks answering simple questions microworld comparison criterion functions linear classifiers supervised learning probability distributions neural networks dynamics discretetime computation three models description language lectures government binding knowledge language nature finite state automata simple recurrent networks note learning rate schedules stochastic optimization towards faster stochastic gradient search structured representations connectionist models distributed representations computational capabilities localfeedback recurrent networks acting finitestate machines unified integration explicit rules learning example recurrent networks local feedback multilayered networks syntactic pattern recognition applications networks learn phonology learning extracting finite state automata secondorder recurrent neural networks extracting learning unknown grammar recurrent neural networks higher order recurrent networks role similarity hungarian vowel harmony connectionist account connectionist perspective prosodic structure representing variable information simple recurrent networks introduction formal language theory neural networks introduction theory neural computation introduction automata theory learning algorithms probability distributions feedforward feedback networks giles experimental comparison recurrent neural networks fast simulated reannealing adaptive simulated annealing asa attractor dynamics parallelism connectionist sequential machine serial order parallel distributed processing approach simulated annealing information theory statistics course gb syntax lectures binding empty categories giles natural language grammatical inference comparison recurrent neural networks machine learning methods efficient learning second order methods learning past tense english verbs using recurrent neural networks language learning cues rules encoding inputoutput representations connectionist cognitive systems focused backpropagation algorithm temporal pattern recognition control dynamical systems using neural networks constructing deterministic finitestate automata recurrent neural networks extraction rules discretetime recurrent neural networks rule revision recurrent neural networks paths categories induction dynamical recognizers learning past tenses english verbs combining symbolic neural learning computation beyond turing limit computational capabilities recurrent narx neural networks computational power neural nets analysis recurrent backpropagation accelerated learning layered neural networks learning applying contextual constraints sentence comprehension learning featurebased semantics simple recurrent networks dynamic construction finitestate automata examples using hillclimbing rules maps connectionist symbol processing many maps locally recurrent globally feedforward networks critical review architectures induction finite state languages using secondorder recurrent networks induction finitestate languages using secondorder recurrent networks efficient gradientbased algorithm online training recurrent network trajec tories learning algorithm continually running fully recurrent neural networks learning finite state machines selfclustering recurrent networks tr ctr michal eransk matej makula ubica beukov organization state space simple recurrent network training recursive linguistic structures neural networks v20 n2 p236244 march 2007 marshall r mayberry iii risto miikkulainen broadcoverage parsing neural networks neural processing letters v21 n2 p121132 april 2005 peter tio ashely j mills learning beyond finite memory recurrent networks spiking neurons neural computation v18 n3 p591613 march 2006 edward kei shiu ho lai wan chan analyzing holistic parsers implications robust parsing systematicity neural computation v13 n5 p11371170 may 2001 peter c r lane james b henderson incremental syntactic parsing natural language corpora simple synchrony networks ieee transactions knowledge data engineering v13 n2 p219231 march 2001 juan c vallelisboa florencia reali hctor anastasa eduardo mizraji elman topology sigmapi units application modeling verbal hallucinations schizophrenia neural networks v18 n7 p863877 september 2005 henrik jacobsson rule extraction recurrent neural networks taxonomy review neural computation v17 n6 p12231263 june 2005