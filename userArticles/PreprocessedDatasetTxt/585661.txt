survey optimization building using probabilistic models paper summarizes research populationbased probabilistic search algorithms based modeling promising solutions estimating probability distribution using constructed model guide exploration search space settles algorithms field genetic evolutionary computation originated classifies classes according complexity models use algorithms within class briefly described strengths weaknesses discussed b introduction recently number evolutionary algorithms guide exploration search space building probabilistic models promising solutions found far proposed algorithms shown perform well wide variety problems however spite attempts field lacks global overview done research area heading purpose paper review describe basic principles recently proposed populationbased search algorithms use probabilistic modeling promising solutions guide search settles algorithms context genetic evolutionary computation classifies algorithms according complexity class models use discusses advantages disadvantages classes next section briefly introduces basic principles genetic algorithms starting point paper continues sequentially describing classes approaches classified according complexity used class models least general one section 4 approaches work string representation solutions described paper summarized concluded section 5 genetic algorithms problem decomposition building blocks simple genetic algorithms gas holland 1975 goldberg 1989 populationbased search algorithms guide exploration search space application selection genetic operators recombinationcrossover mutation usually applied problems solutions represented mapped onto fixedlength strings finite alphabet user defines problem ga attempt solve choosing length base alphabet strings representing solutions defining function discriminates string solutions according quality function usually called fitness string fitness function returns real number quantifying quality respect solved problem higher fitness better solution gas start randomly generated population solutions current population solutions better solutions selected selection operator selected solutions processed applying recombination mutation operators recombination combines multiple usually two solutions selected together exchanging parts various strategies eg onepoint uniform crossover mutation performs slight perturbation resulting solutions created solutions replace old ones process repeated termination criteria given user met selection search biased highquality solutions new regions search space explored combining mutating repeatedly selected promising solutions mutation close neighborhood original solutions explored like local hillclimbing recombination brings innovation combining pieces multiple promising solutions together gas therefore work well problems somehow decomposed subproblems bounded difficulty solving combining solutions global solution con structed overaverage solutions subproblems often called building blocks ga liter ature reproducing building blocks applying selection preserving disruption combination mixing together powerful principle solve decomposable problems harik cantupaz goldberg miller 1997 muhlenbein mahnig rodriguez 1998 however fixed problemindependent recombination operators often either break building blocks frequently mix effectively gas work well problems building blocks located tightly strings representing solutions thierens 1995 problems building blocks spread solutions simple gas experience poor performance thierens 1995 growing interest methods learn structure problem fly use information ensure proper mixing growth building blocks one approaches based probabilistic modeling promising solutions guide exploration search space instead using crossover mutation like simple gas 3 evolutionary algorithms based probabilistic modeling algorithms use probabilistic model promising solutions guide exploration search space called estimation distribution algorithms edas muhlenbein paa 1996 edas better solutions selected initially randomly generated population solutions like simple ga true probability distribution selected set solutions estimated new solutions generated according estimate new solutions added original population replacing old ones process repeated termination criteria met edas therefore simple gas except replace genetic recombination mutation operators following two steps 1 model estimate true distribution selected promising solutions constructed 2 new solutions generated according constructed model although edas process solutions different way simple gas theoretically empirically proven results similar instance simple ga uniform crossover randomly picks value position either two parents works asymptotically socalled univariate marginal distribution algorithm muhlenbein paa 1996 assumes variables independent muhlenbein 1997 harik et al 1998 pelikan muhlenbein 1999 distribution estimate capture buildingblock structure problem accurately ensure effective mixing reproduction building blocks results linear subquadratic performance edas problems muhlenbein mahnig 1998 pelikan et al 1998 fact accurate distribution estimate captures structure solved problem edas unlike simple gas perform ga theory mostly used assumptions claims however estimation true distribution far trivial task tradeoff accuracy efficiency estimate following sections describe three classes edas applied problems solutions represented fixedlength strings finite alphabet algorithms classified according complexity class models use starting methods assume variables problem string positions independent ones take account pairwise interactions methods accurately model even complex problem structure highly overlapping multivariate building blocks example model presented class models shown models displayed bayesian networks ie directed acyclic graphs nodes corresponding variables problem string positions edges corresponding probabilistic relationships covered model edge two nodes bayesian network relates two nodes value variable corresponding ending node edge depends value variable corresponding starting node 31 simplest way estimate distribution promising solutions assume variables problem independent look values variable regardless remaining solutions see figure 1 model selected promising solutions used generate new ones contains set frequencies values string positions selected set frequencies used guide search generating new string solutions position position according frequency values fashion building blocks order one reproduced mixed efficiently algorithms based principle work well linear problems variables mutually interacting muhlenbein 1997 harik et al 1997 populationbased incremental learning pbil algorithm baluja 1994 solutions represented binary strings fixed length population solutions replaced socalled probability vector initially set assign value position probability 05 generating number solutions best solutions selected probability vector shifted towards selected solutions using hebbian learning rule hertz krogh palmer 1991 pbil also referred hillclimbing learning hcwl kvasnicka pelikan pospichal 1996 incremental univariate marginal distribution algorithm iumda muhlenbein 1997 recently analysis pbil algorithm found kvasnicka et al 1996 univariate marginal distribution algorithm umda muhlenbein paa 1996 population solutions processed iteration frequencies values position selected set promising solutions computed used generate new solutions figure 1 graphical model interactions covered replace old ones new solutions replace old ones process repeated termination criteria met theory umda found muhlenbein 1997 compact genetic algorithm cga harik lobo goldberg 1998 replaces population single probability vector like pbil however unlike pbil modifies probability vector direct correspondence population represented probability vector probability vector instead shifting vector components proportionally distance either 0 1 component vector updated shifting value contribution single individual total frequency assuming particular population size using update rule theory simple genetic algorithms directly used order estimate parameters behavior cga algorithms described section perform similarly work well linear problems achieve linear subquadratic performance depending type problem fail problems strong interactions among variables information described algorithm well theoretical empirical results see cited papers algorithms take account interdependencies various bits variables fail problems strong interactions among variables without taking account algorithms mislead lot effort put extending methods use simple model cover interactions methods could solve general class problems efficiently simple pbil umda cga solve linear problems 32 pairwise interactions first algorithms assume variables problem independent could cover pairwise interactions mutualinformationmaximizing input clustering mimic algorithm de bonet isbell viola 1997 uses simple chain distribution see figure 2a maximizes socalled mutual information neighboring variables string positions fashion kullbackliebler divergence kullback leibler 1951 chain complete joint distribution minimized however construct chain equivalent ordering variables mimic uses greedy search algorithm due efficiency therefore global optimality distribution guaranteed baluja davies 1997 use dependency trees see figure 2b model promising solutions two major advantages using trees instead chains trees general chains chain tree moreover relaxing constraints model order find best model according measure decomposable terms order two polynomial maximal branching algorithm edmonds 1967 guarantees global optimality solution used hand mimic uses greedy search order learn chain distributions npcomplete algorithm needed similarly pbil population replaced probability vector contains pairwise probabilities bivariate marginal distribution algorithm bmda pelikan muhlenbein 1999 forest set mutually independent dependency trees see figure 2c used class models even general class dependency trees single tree fact set one tree measure used determine variables connected pearsons chisquare test marascuilo mcsweeney 1977 used measure also used discriminate remaining dependencies order construct final model mimic b balujadavies 1997 c bmda figure 2 graphical models pairwise interactions covered pairwise models allow covering interactions problem easy learn algorithms presented section reproduce mix building blocks order two efficiently therefore work well linear quadratic problems de bonet et al 1997 baluja davies 1997 muhlenbein 1997 pelikan muhlenbein 1999 bosman thierens 1999 latter two approaches also solve 2d spinglass problems efficiently pelikan muhlenbein 1999 33 multivariate interactions however covering pairwise interactions still shown insufficient solve problems multivariate highlyoverlapping building blockspelikan muhlenbein 1999 bosman 1999 research area continued complex models one hand using general models brought powerful algorithms capable solving decomposable problems quickly accurately reliably hand using general models also resulted necessity using complex learning algorithms require significant computational time still guarantee global optimality resulting models however spite increased computational time spent learning models number evaluations optimized function reduced significantly fashion overal time complexity reduced moreover many problems algorithms simply work without learning structure problem algorithms must either given information expert simply incapable biasing search order solve complex decomposable problems reasonable computational cost algorithms presented section use models cover multivariate interactions extended compact genetic algorithm ecga harik 1999 variables divided number intact clusters manipulated independent variables umda see figure 3a therefore cluster building block taken whole different clusters considered mutually independent discriminate models ecga uses minimum description length mdl metric mitchell 1997 prefers models allow higher compression data selected set promising solutions advantage using mdl metric penalizes complex models needed therefore resulting models overly complex find good model simple greedy algorithm used starting variables separated iteration current groups variables merged metric increases improvement possible current model used following theory umda problems separable ie decomposable nonoverlapping subproblems bounded order ecga good model perform subquadratic time question whether ecga finds good model much effort takes moreover many problems contain highly overlapping building blocks eg 2d spinglass systems accurately modeled simply dividing variables distinct classes results poor performance ecga problems factorized distribution algorithm uses factorized distribution fixed model throughout whole computation fda capable learning structure problem fly distribution factorization given expert distributions allowed contain marginal conditional probabilities updated according currently selected set solutions theoretically proven model correct fda solves decomposable problems quickly reliably accurately muhlenbein mahnig rodriguez 1998 however fda requires prior information problem form decomposition factorization unfortunately usually available solving realworld problems therefore use fda limited problems least accurately approximate structure problem bayesian optimization algorithm boa pelikan goldberg cantupaz 1998 uses general class distributions ecga incorporates methods learning bayesian networks see figure 3b uses model promising solutions generate new ones boa selecting promising solutions bayesian network models constructed constructed network used generate new solutions measure quality networks metric used eg bayesiandirichlet bd metric heckerman geiger chickering 1994 mdl metric etc recently published experiments bd scoring metric used bd metric prefer simpler models complex ones uses accuracy encoded distribution criterion space possible models reduced specifying maximal order interactions problem taken account construct network respect given metric algorithm searches domain possible bayesian networks used recent experiments greedy algorithm used due efficiency boa uses equivalent class models fda however require information problem input able discover information nevertheless prior information incorporated ratio prior information information contained set highquality solutions found far controlled user boa fill gap fda uninformed search methods also offers method efficient even without prior information pelikan et al 1998 schwarz ocenasek 1999 pelikan et al 1999 still prohibit improvement using another algorithm uses bayesian networks model promising solutions called estimation bayesian network ecga b boa figure 3 graphical models multivariate interactions covered algorithm ebna later proposed etxeberria larranaga 1999 algorithms use models capable covering multivariate interactions achieve good performance wide range decomposable problems eg 2d spinglass systems pelikan et al 1998 muhlenbein mahnig 1998 graph partitioning schwarz ocenasek 1999 communication network optimization rothlauf 1999 etc however problems decomposable terms bounded order still difficult solve overlapping subproblems mislead algorithm right solution particular subproblem found sequentially distributed across solutions eg see f 0gammapeak muhlenbein mahnig 1998 without generating initial population use problemspecific information building blocks size proportional size problem used results exponential performance algorithms brings question problems aim solve algorithms based reproduction mixing building blocks shortly discussed earlier section 2 attempt solve problems decomposed terms bounded order problems approach solve decomposable sense solved approaching problem level solutions lower order combining best construct optimal closetooptimal solution bias search total space explored algorithm substantially reduces couple orders magnitude computationally hard problems solved quickly accurately reliably 4 beyond string representation solutions algorithms described work problems defined fixedlength strings finite alpha bet however recently attempts go beyond simple representation directly tackle problems solutions represented vectors real number computer programs without mapping solutions strings approaches use simple models cover interactions problem stochastic hillclimbing learning vectors normal distributions shclvnd rudlof koppen 1996 solutions represented realvalued vectors population solutions replaced modeled vector mean values gaussian normal distribution optimized variable see figure 4a standard deviation oe stored globally variables generating number new solutions mean values shifted towards best generated solutions standard deviation oe reduced x variable 4 variable 3 variable 5 shclvnd b b b b variable 4 variable 3 z z z z 14b servet et al 1998 figure 4 probabilistic models real vectors independent variables make future exploration search space narrower various ways modifying oe parameter exploited sebag ducoulombier 1998 another implementation realcoded pbil servet travemassuyes stern 1997 variable interval number stored see figure 4b z stands probability solution right half interval initialized 05 time new solutions generated using corresponding intervals best solutions selected numbers z shifted towards z variable gets close either 0 1 interval reduced corresponding half figure 4b z mapped corresponding interval probabilistic incremental program evolution pipe algorithm salustowicz schmid huber 1997 computer programs mathematical functions evolved like genetic programming koza 1992 however pairwise crossover mutation replaced probabilistic modeling promising programs programs represented trees internal node represents functioninstruction leaves represent either input variable constant pipe algorithm probabilistic representation program trees used probabilities instruction node maximal possible tree used model promising generate new programs see figure 5 unused portions tree simply cut evaluation program fitness function initially model set trees generated random current population programs ones perform best selected used update probabilistic model process repeated termination criteria met 5 summary conclusions recently use probabilistic modeling genetic evolutionary computation become popular combining various achievements machine learning genetic evolutionary computation efficient algorithms solving broad class problems constructed recent algorithms continuously proving powerfulness efficiency offer promising approach solving problems resolved combining highquality pieces information bounded order together paper reviewed algorithms use probabilistic models promising psdasd 231 px12 psdasd 231 px12 psdasd 231 px12 psdasd 231 px12 psdasd 231 figure 5 graphical model program interactions covered used pipe solutions found far guide exploration search space algorithms classified classes according complexity class models use basic properties classes algorithms shortly discussed thorough list published papers references given 6 acknowledgments authors would like thank erick cantupaz martin butz dimitri knjazew jiri pospichal valuable discussions useful comments helped shape paper work sponsored air force office scientific research air force materiel com mand usaf grant number f496209710050 research funding project also provided grant us army research laboratory federated laboratory program cooperative agreement daal019620003 us government authorized reproduce distribute reprints governmental purposes notwithstanding copyright notation thereon views conclusions contained herein authors interpreted necessarily representing official policies endorsements either expressed implied air force scientific research us government r using optimal dependencytrees combinatorial optimization learning structure search space linkage information processing distribution estimation algorithms optimum branching genetic algorithms search linkage learning via probabilistic modeling ecga illigal report compact genetic algorithm learning bayesian networks combination knowledge statistical data technical report msrtr9409 introduction theory neural compu tation adaptation natural artificial systems genetic programming programming computers means natural selection information sufficiency hill climbing learning abstraction genetic algorithm nonparametric distributionfree methods social sciences machine learning equation response selection use prediction convergence theory applications factorized distribution algorithm boa bayesian optimization algo rithm bivariate marginal distribution algorithm communication network optimization stochastic hill climbing learning vectors normal distributions probabilistic incremental program evolution stochastic search program space extending populationbased incremental learning continuous search spaces telephone network traffic overloading diagnosis evolutionary computation techniques analysis design genetic algorithms tr adaptation natural artificial systems genetic programming genetic algorithms search optimization machine learning machine learning schemata distributions graphical models evolutionary optimization probabilistic incremental program evolution using optimal dependencytrees combinational optimization extending populationbased incremental learning continuous search spaces recombination genes estimation distributions binary parameters telephone network traffic overloading diagnosis evolutionary computation techniques fuzzy recombination breeder genetic algorithm populationbased incremental learning method integrating genetic search based function optimization competitive learning ctr radovan ondas martin pelikan kumara sastry scalability genetic programming probabilistic incremental program evolution proceedings 2005 conference genetic evolutionary computation june 2529 2005 washington dc usa martin pelikan david e goldberg hierarchy machine learning optimize nature humans complexity v8 n5 p3645 mayjune paul winward david e goldberg fluctuating crosstalk deterministic noise ga scalability proceedings 8th annual conference genetic evolutionary computation july 0812 2006 seattle washington usa j pea j lozano p larraaga unsupervised learning bayesian networks via estimation distribution algorithms application gene expression data clustering international journal uncertainty fuzziness knowledgebased systems v12 nsupplement p6382 january 2004 chaohong chen weinan liu yingping chen adaptive discretization probabilistic model building genetic algorithms proceedings 8th annual conference genetic evolutionary computation july 0812 2006 seattle washington usa juergen branke clemens lode jonathan l shapiro addressing sampling errors diversity loss umda proceedings 9th annual conference genetic evolutionary computation july 0711 2007 london england jun sakuma shigenobu kobayashi realcoded crossover role kernel density estimation proceedings 2005 conference genetic evolutionary computation june 2529 2005 washington dc usa paul winward david e goldberg fluctuating crosstalk deterministic noise ga scalability proceedings 8th annual conference genetic evolutionary computation july 0812 2006 seattle washington usa joseph reisinger risto miikkulainen selecting evolvable representations proceedings 8th annual conference genetic evolutionary computation july 0812 2006 seattle washington usa fernando g lobo cludio f lima review adaptive population sizing schemes genetic algorithms proceedings 2005 workshops genetic evolutionary computation june 2526 2005 washington dc j l shapiro drift scaling estimation distribution algorithms evolutionary computation v13 n1 p99123 january 2005 hung yingping chen hsiao wen zan characteristic determination solid state devices evolutionary computation case study proceedings 9th annual conference genetic evolutionary computation july 0711 2007 london england shigeyoshi tsutsui martin pelikan ashish ghosh edge histogram based sampling local search solving permutation problems international journal hybrid intelligent systems v3 n1 p1122 january 2006 peter n bosman jrn grahl franz rothlauf sdr better trigger adaptive variance scaling normal edas proceedings 9th annual conference genetic evolutionary computation july 0711 2007 london england jrn grahl peter bosman franz rothlauf correlationtriggered adaptive variance scaling idea proceedings 8th annual conference genetic evolutionary computation july 0812 2006 seattle washington usa dongil seo byungro moon informationtheoretic analysis interactions variables combinatorial optimization problems evolutionary computation v15 n2 p169198 summer 2007 mendiburu j miguelalonso j lozano ostra c ubide parallel edas create multivariate calibration models quantitative chemical applications journal parallel distributed computing v66 n8 p10021013 august 2006 martin pelikan kumara sastry david e goldberg sporadic model building efficiency enhancement hierarchical boa proceedings 8th annual conference genetic evolutionary computation july 0812 2006 seattle washington usa xavier llor kumara sastry david e goldberg abhimanyu gupta lalitha lakshmi combating user fatigue igas partial ordering support vector machines synthetic fitness proceedings 2005 conference genetic evolutionary computation june 2529 2005 washington dc usa marcus gallagher marcus frean populationbased continuous optimization probabilistic modelling mean shift evolutionary computation v13 n1 p2942 january 2005 yunpeng sun xiaomin jia peifa probabilistic modeling continuous eda boltzmann selection kullbackleibeler divergence proceedings 8th annual conference genetic evolutionary computation july 0812 2006 seattle washington usa steven thierens mark de berg design analysis competent selectorecombinative gas evolutionary computation v12 n2 p243267 june 2004 martin v butz martin pelikan studying xcsboa learning boolean functions structure encoding random boolean functions proceedings 8th annual conference genetic evolutionary computation july 0812 2006 seattle washington usa martin v butz david e goldberg kurian tharakunnel analysis improvement fitness exploitation xcs bounding models tournament selection bilateral accuracy evolutionary computation v11 n3 p239277 fall mark hauschild martin pelikan claudio f lima kumara sastry analyzing probabilistic models hierarchical boa traps spin glasses proceedings 9th annual conference genetic evolutionary computation july 0711 2007 london england martin v butz kumara sastry david e goldberg strong stable reliable fitness pressure xcs due tournament selection genetic programming evolvable machines v6 n1 p5377 march 2005 martin pelikan kumara sastry david e goldberg multiobjective hboa clustering scalability proceedings 2005 conference genetic evolutionary computation june 2529 2005 washington dc usa martin pelikan david e goldberg shigeyoshi tsutsui getting best worlds discrete continuous genetic evolutionary algorithms concert information sciences international journal v156 n34 p147171 15 november kumara sastry hussein abbass david e goldberg johnson substructural niching estimation distribution algorithms proceedings 2005 conference genetic evolutionary computation june 2529 2005 washington dc usa martin v butz martin pelikan xavier llor david e goldberg extracted global structure makes local building block processing effective xcs proceedings 2005 conference genetic evolutionary computation june 2529 2005 washington dc usa martin pelikan james laury jr order parallelization model building hboa affect scalability proceedings 9th annual conference genetic evolutionary computation july 0711 2007 london england martin pelikan rajiv kalapala alexander k hartmann hybrid evolutionary algorithms minimum vertex cover random graphs proceedings 9th annual conference genetic evolutionary computation july 0711 2007 london england yingping chen david e goldberg convergence time linkage learning genetic algorithm evolutionary computation v13 n3 p279302 september 2005 martin v butz martin pelikan xavier llor david e goldberg automated global structure extraction effective local building block processing xcs evolutionary computation v14 n3 p345380 september 2006 martin v butz martin pelikan xavier llor david e goldberg automated global structure extraction effective local building block processing xcs evolutionary computation v14 n3 p345380 september 2006 j pea j lozano p larraaga globally multimodal problem optimization via estimation distribution algorithm based unsupervised learning bayesian networks evolutionary computation v13 n1 p4366 january 2005 martin pelikan kumara sastry david e goldberg sporadic model building efficiency enhancement hierarchical boa genetic programming evolvable machines v9 n1 p5384 march 2008