parallel preconditioning sparse approximate inverses parallel preconditioner presented solution general sparse linear systems equations sparse approximate inverse computed explicitly applied preconditioner iterative method computation preconditioner inherently parallel application requires matrixvector product sparsity pattern approximate inverse imposed priori captured automatically keeps amount work number nonzero entries preconditioner minimum rigorous bounds clustering eigenvalues singular values derived preconditioned system proximity approximate true inverse estimated extensive set test problems scientific industrial applications provides convincing evidence effectiveness approach b introduction consider linear system equations work grote supported ibm fellowship work huckle supported research grant deutsche forschungsgemeinschaft scientific computing computational mathematics bldg 460 stanford university stanford ca 94305 grotesccmstanfordedu z institut fr angewandte mathematik und statistik universitt wurzburg d97074 wurzburg germany hucklevaxrzuniwuerzburgd400de large sparse nonsymmetric matrix due size direct solvers become prohibitively expensive amount work storage required alternative consider iterative methods gmres bcg bicgstab cg applied normal equations 6 given initial guess x 0 algorithms compute iteratively new approximations x k true solution b iterate xm accepted solution residual r general convergence guaranteed may extremely slow hence original problem 1 must transformed tractable form consider preconditioning matrix apply iterative solver either right left preconditioned system therefore chosen good approximation identity ultimate goal reduce total execution time computation matrixvector product evaluated parallel since matrixvector product must performed iteration number nonzero entries exceed successful preconditioning methods terms reducing number iterations incomplete lu factorizations ssor notoriously difficult implement parallel architecture especially unstructured matrices indeed application preconditioner iteration phase requires solution triangular systems step difficult parallelize recursive nature computation see 2 section 444 aim find inherently parallel preconditioner retains convergence properties incomplete lu natural way achieve parallelism compute approximate inverse sense evaluation easy parallelize cheap sparse although sparse approximate inverse always exist often occurs applications entries gamma1 small 5 1 instance problem results discretization partial differential equation generally meaningful look sparse approximate inverse polynomial preconditioners inherently parallel lead much improvement convergence incomplete lu see 2 section 3 different approach minimize kam gamma ik yet cannot confine whole spectrum vicinity 1 else would minimizing 1norm 2norm would expensive since solution long kam gamma ik 1 ensure fast convergence reasonable cost need cluster eigenvalues singular values 1 give outliers additional freedom achieve minimizing kam gamma ik frobenius norm moreover choice naturally leads inherent parallelism columns k computed independently one another indeed since solution 3 separates n independent least squares problems thus solve 4 parallel obtain explicit approximate inverse sparse 4 reduces n small least squares problems solved quickly 9 13 difficulty lies determining good sparsity structure approximate inverse else solution 4 yield effective preconditioner therefore seek method captures sparsity pattern main entries gamma1 automatically yet reasonable cost start given sparsity pattern diagonal augment progressively 2 norm residual small enough maximal sparsity reached key algorithm lies strategy used determine locations entries selection criterion simple cheap compute yet yields effective preconditioner without generating excessive fillin extensive set difficult test problems consider end shows algorithm produces sparse effective preconditioner computation approximate inverses based minimizing 4 proposed several authors yeremin et al compute factorized sparse approximate inverse 11 12 13 consider fixed sparsity patterns simon grote solve 4 explicitly allow banded sparsity pattern 9 10 approach cosgrove diaz griewank 4 similar differs criteria used augmenting chow saad 3 use iterative method compute approximate solution 4 method automatically generates new entries apply dropping strategy remove excessive fillin appearing section 2 introduce spai algorithm computes sparse approximate inverse section 3 derive theoretical properties spectrum preconditioned system 2 sections 4 5 present wide range numerical experiments using test matrices engineering scientific applications 2 computation sparse approximate invers shall first show compute sparse approximate inverse given sparsity structure matrix solution minimization problem 4 since columns independent one another need present algorithm one denote k let j set indices j k j 6 0 denote reduced vector unknowns k j set indices ai j identically zero enables us eliminate zero rows submatrix j denote resulting submatrix ai j similarly define e set solving 4 k equivalent solving min n 1 theta n 2 least squares problem 5 extremely small sparse matrices nonsingular submatrix must full rank thus qrdecomposition r nonsingular upper triangular n 2 theta n 2 matrix let solution 5 solve 7 yields approximate inverse minimizes kam gamma ik f given sparsity structure aim improve upon augmenting sparsity structure obtain effective preconditioner shall reduce current error reduce kam recall k optimal solution least squares problem 4 denote residual exactly kth column gamma1 cannot improved upon assume r 6 0 demonstrate augment set indices j reduce krk 2 since k sparse components r zero denote l remaining set indices r 6 0 typically l equal since r exact zero entries finite precision contain k must included l since rk equal gamma1 every 2 l corresponds index set n consists indices nonzero elements j yet potential new candidates might added j contained must select new indices j lead profitable reduction krk 2 cheap effective way consider j onedimensional minimization problem solution 10 2 11 j compute 2norm ae j new residual r given 11 2 12 least one index j r ae j 6 0 lead smaller residual 12 otherwise would imply rl zero since al full rank note j j contains column indices nonzero elements al j reduce j set profitable indices j smallest ae j add j note equation 9 also used 4 estimate reduction residual already found 8 using augmented set indices j solve sparse least squares problem 4 yields better approximation k kth column gamma1 repeat process residual satisfies prescribed tolerance maximum amount fillin reached numerical study section 4 shows iterative procedure captures main entries gamma1 extremely well every time augment set nonzero entries k solve least squares problem exactly shall demonstrate one easily update qr decomposition greatly reduce amount work recall j current set j set new indices added k denote new rows correspond nonzero rows j j contained yet 2 number indices j thus need replace 5 submatrix larger submatrix ai solve 5 use known qr decomposition update qr decomposition augmented ai requires computation qr decomposition b 2 note already contains indices nonzero entries present columns j let r using 16 rewrite 15 n 2 r procedure enables us add new indices j solve least squares problem optimal solution without recomputing full qr decomposition step generalizes updating strategy proposed 4 single entry allowing several new entries time stop process algorithm compute kth column gamma1 practice however stop process prescribed tolerance met maximal amount fillin reached present full spai algorithm spai stands sparse approximate inverse spai algorithm every column k choose initial sparsity j b compute row indices corresponding nonzero entries qr decomposition 6 ai j compute solution k least squares problem 4 residual r given 8 c set l equal set indices r 6 0 set j equal set new column indices appear l rows j e j 2 j solve minimization problem 10 f j 2 j compute ae j given 12 delete j profitable indices g determine new indices update qr decomposition using 17 solve new least squares problem compute new residual j remarks 1 initial sparsity structure arbitrary may chosen empty diagonal priori information sparsity gamma1 available yet solve sequence problems similar sparsity patterns varying entries clever initial guess initial sparsity choose sparsity previously computed approximate inverse greatly reduces computational cost since initial sparsity structure would almost optimal numerical examples initial sparsity chosen diagonal 2 addition stopping criterion krk 2 constrain loop maximal number iterations limit maximal fillin per column threshold almost never reached total number nonzero entries usually comparable amount 3 f first reduce j set indices j ae j less equal mean value ae j remaining indices keep indices smallest ae j small integer avoid excessive fillin set equal 5 numerical calculations criterion cheap compute removes useless indices effectively parameterfree since uses dynamic mean value criterion require threshold input user sophisticated weighting could applied distribution ae j control rate filled 4 update qr decomposition 17 store householder matrices resulting factorizations matrices b 2 separately never construct q explicitly 5 selection process c may restricted easily accessible rows minimize communications data flow also restricted largest elements r 6 onedimensional minimization replaced another minimization method steepest descent exact minimization problem related j fjg experience former accurate enough latter expensive 7 approximate inverse computed spai algorithm permutation invariant replaced p 1 ap 2 p 1 p 2 permutation matrices obtain p 1 instead spai algorithm may also applied compute sparse approximate left inverse used left preconditioner 2 may yield better result rows gamma1 sparse columns full case discussed section 5 iterative solver preconditioned current con verge easy improve upon using sparsity initial sparsity spai algorithm iteration proceed new preconditioner moreover since compute residual individual column k easy single difficult columns concentrate improve convergence iterative solver may prove useful connection flexible preconditioning preconditioner adapted iterative process see 17 14 3 theoretical properties shall derive rigorous bounds spectrum preconditioned matrix furthermore shall estimate difference derive conditions guarantee nonsingular let approximate inverse obtained spai algorithm let k kth column denote r k residual every k assume satisfies theorem 31 1kn fnumber nonzero elements r k note p usually much smaller n sparse thus bounds derived section tighter discussed 4 ch 8 1 directly apply computed approximate inverse preconditioned system 2 basic assumption 18 coincides stopping criterion used spai algorithm proof since immediately obtain left inequality 19 derive left inequality 20 use definition 2norm get right inequalities 19 20 using 23 holds 2norm frobenius norm kr k k 1 bounded immediately get 21if apply gershgorins theorem see eigenvalues lie inside disk radius centered 1 therefore nonsingular summarize result corollary corollary 31 generally know p advance 21 useful computed since p n see p must nonsingular gives criterion choosing given n although practice often costly run algorithm small even symmetric nonsymmetric general may appropriate use symmetrized preconditioner derive estimates begin following inequality holds 2norm frobenius norm symmetric therefore symmetrized preconditioner 2 satisfies general equation 26 pessimistic estimate much practical use spai algorithm take advantage symmetry yield symmetric approximate inverse easy however reformulate algorithm compute lower triangular part yields symmetric preconditioner algorithm loses inherent parallelism interesting alternative would compute factorized sparse approximate inverse 11 leave sparsity open like spai algorithm convergence iterative methods heavily depends distribution eigenvalues singular values preconditioned matrix 6 indeed eigenvalues clustered 1 outliers present convergence generally fast thus crucial derive estimates spectrum determine theoretical effectiveness preconditioner estimates summarized following two theorems theorem 32 let 1kn fnumber nonzero elements r k eigenvalues k clustered 1 lie inside circle radius proof let qrq schur decomposition gamma eigenvalues k clustered 1 next use gershgorins theorem 21 conclude k lie inside disk radius p centered 1 immediate consequence 28 see minimizing kam gamma ik f also reduces departure normality knk 2 schur decomposition derive bounds singular values condition number theorem 33 singular values clustered 1 lie inside interval 1 gamma furthermore condition number satisfies proof since singular values square roots eigenvalues bound 31 next apply 28 amm instead conclude singular values must clustered 1essential properties convergence iterative methods clustering eigenvalues singular values condition number departure normality preconditioned linear system section shown minimizing kam gamma ik frobenius norm produces preconditioner improves four points 4 first numerical example begin detailed study orsirr2 smallest among orsx oil reservoir simulation problems harwellboeing matrix collection convergence results orsreg1 orsirr1 presented end section brief description problems bcg cgs bicgstab gmres20 cgne table 1 convergence results orsirr2 unpreconditioned preconditioned 02 06 orsreg1 oil reservoir simulation matrix 21 theta 21 theta 5 full grid size entries orsirr1 oil reservoir simulation matrix 21 theta 21 theta 5 irregular grid orsirr2 oil reservoir simulation matrix 21 theta 21 theta 5 irregular grid rather small size orsirr2 compute true inverse eigenvalues singular values preconditioned system numerical experiments presented section computed double precision using matlab implementation initial guess always stopping criterion table 1 present convergence results variety iterative methods decreasing values algorithms various iterative methods obtained 2 convergence results without preconditioning listed denote cgne cg algorithm applied preconditioned normal equations amm reduce kam gamma ik f number iterations drops significantly iterative methods results table 1 exemplify robustness preconditioner respect table 2 orsirr2 kam gamma ik condition number different values table 2 compare different norms gamma function reduce 1norm 2norm remain nearly constant thus better solution minimization problem kam gamma ik 1 computed 02 also true 2norm long 03 indicates neither 1norm 2norm good measure proximity identity frobenius norm minimized reduce singular values cluster one demonstrated theorem 33 clearly shown table 2 since condition number ratio largest smallest singular value figure 1 verify improvement clustering eigenvalues predicted theorem 32 reduce next compare approximate bcg cgs bicgstab gmres20 gmres50 table 3 convergence results orsreg1 orsirr1 unpreconditioned preconditioned 02 02 figure 1 orsirr2 eigenvalues 02 bottom inverse true inverse gamma1 compute gamma1 discard entries whose absolute value less equal 0001 compute approximate inverse 02 quite striking well sparsity patterns matrices agree qualitatively figure 2 conclude section convergence results orsreg1 orsirr1 given table 3 relative sparsity nzmnza 094 orsreg1 088 orsirr1 5 numerical experiments section consider wide spectrum problems coming scientific industrial applications shall demonstrate effectiveness preconditioner two standard different iterative methods bicgstab gmresm restart recall former requires two matrixvector multiplications per iteration whereas latter requires one matrixvector multiply per iteration numerical calculations figure 2 orsirr2 sparsity pattern entries gamma1 larger 0001 absolute value left sparse approximate inverse 02 right initial guess x unless specified stopping criterion 33 computations done double precision fortran partly sparc10 sun station partly iris 4d35 sgi station fortran code provided 2 used gmres begin convergence results five shermanx black oil simulators set consists sherman1 black oil simulator shale barrier size sherman2 thermal simulation steam injection 6 theta 6 theta 5 grid 5 unknowns sherman3 black oil impes simulation 35 theta 11 theta 13 grid 1 unknown sherman4 black oil impes simulation 16 theta 23 theta 3 grid 1 unknown sherman5 fully implicit black oil simulator 16 theta 23 theta 3 grid 3 unknowns sh1 sh2 sh3 sh4 sh5 table 4 convergence results shermanx unpreconditioned top preconditioned bottom righthand side always provided table 4 shows preconditioning clearly improves convergence considered problems max denotes upper limit number nonzero elements per column sherman2 bicgstab gmres20 reduced relative residual 10 gamma5 4 7 steps respectively never reached gamma8 may due large condition number 964 theta 10 11 sherman2 could improved upon different implementation gmres 16 might reduce residual pursue matter figure 3 displayed original matrix approximate inverse sherman2 picture clearly shows cannot simply set equal banded matrix like 10 indeed sparsity structures totally different particular case whereas number nonzero entries matrices comparable next set examples consists larger problems poresx collections pores2 nonsymmetric matrix pores3 nonsymmetric matrix saylor3 nonsymmetric problem saylor4 nonsymmetric problem righthand side randomly chosen since pores2 generate sparse approximate inverse iteration never reached relative figure 3 sherman2 original matrix left sparse approximate inverse tolerance 10 gamma8 opted left instead right preconditioning thus case pores2 considered kma gamma ik f instead 4 yet still computed exact residual original problem check convergence approach proved efficient rows gamma1 could approximated effectively columns figure 4 compare pores2 left approximate inverse number nonzero entries similar sparsity patterns quite different pores2 gmres20 reduced relative residual 10 gamma6 106 iterations improve different implementation gmres 16 might mitigate problem saylor3 discovered columns 988 989 998 999 two independent 2 theta 2 singular submatrices ignored four columns computation simply replaced two submatrices 2 theta 2 identity matrices guarantee existence solution set righthand side times random vector orsx shermanx poresx saylorx problems taken harwellboeing matrix collection comparative study problems different iterative methods using incomplete lu preconditioning found 15 final part section shall consider several large problems righthand side always provided initial guess x tolerance 33 start four typical problems centric engineering figure 4 pores2 original matrix left sparse approximate inverse pores 2 pores 3 saylor 3 saylor 4 bicgstab 78 118 104 285 table 5 convergence results poresx saylorx unpreconditioned top preconditioned middle bottom left preconditioning used pores2 table 6 convergence results px unpreconditioned top preconditioned bottom p1 incompressible flow pressure driven pipe p2 incompressible flow pressure driven pipe p3 landing hydrofoil airplane fse model p4 slosh tank model remark used actual number nonzero elements px matrices since 03 entries zero original data files quite remarkable although px matrices rather full obtain big improvement convergence much sparser approximate inverses moreover periodic pattern figure 5 one could determine sparsity pattern first columns slide pattern diagonal lower right corner get full sparsity structure conclude series numerical experiments consider three problems coming implicit 2d euler solver unstructured grid 19 matrices order correspond initial later stages flow simulation none problems converged without preconditioning first problem t01 much easier since flow still initial stage also discussed 7 see t01 obtain considerable improvement figure 5 p1 original matrix left sparse approximate inverse convergence sparse approximate inverse ran t01 04 maximum 50 nonzero entries per column lead relative sparsity t25 t50 ran 03 maximum 100 nonzero entries per column lead relative sparsity figure 6 convergence history tx bicgstab relative residual vs number iterations t01dashdot t25dotted t50solid 6 conclusion spai algorithm computes sparse approximate inverse general sparse matrix inherently parallel since columns calculated independently one another matrix gives valuable insight gamma1 provides measure proximity gamma1 instead imposing priori sparsity pattern upon let algorithm capture automatically relevant entries inverse thus minimize number nonzero entries concentrate computational effort needed algorithm generates robust flexible preconditioner iterative solvers gives full control sparsity quality shown theoretical practical point view preconditioner generated spai algorithm effective improving convergence iterative solvers possible minimize total execution time particular problem architecture choosing optimal clear sparse preconditioner cheap may lead much improvement con vergence becomes dense becomes expensive compute optimal preconditioner lies somewhere two ex tremes problem architecture dependent parallel environment slow communication capabilities cluster workstations may advantageous compute fairly dense approximate inverse increase local floatingpoint intensive computations reduce number iterations hence amount communications required iteration inner products matrixvector products diminished implementation method parallel computer straight forward since calculations done independently yet processor must access data required solve subproblem although preconditioner invariant permutation ordering unknowns affect amount interprocessor communication involved computation may optimized particular application approach prove particularly effective linear system needs solved repeatedly implicit timemarching schemes solution nonlinear equations acknowledgements thank horst simon providing us matrices helpful comments paper written second author visiting sccm program stanford university would like thank gene golub kind hospitality r cambridge university press templates solution linear systems approximate inverse preconditioners general sparse matrices approximate inverse preconditionings sparse linear systems decay rates inverses band matrices iterative solution linear systems implementation details coupled qmr algorithm numerical methods solving linear least squares problems parallel preconditioning approximate inverses connection machine parallel preconditioning approximate inverses connection machine factorized sparse approximate inverse preconditionings factorized sparse approximate inverse fsai preconditionings solving 3d fe systems massively parallel computers ii sparse approximate inverse preconditionings solving 3d cfd problems massively parallel computers flexible innerouter preconditioned gmres algorithm comparative study preconditioned lanczos methods nonsymmetric linear systems efficient high accuracy solutions gm resm gmresr family nested gmres methods unstructured grid solvers ipsc860 tr ctr e flrez garca l gonzlez g montero effect orderings sparse approximate inverse preconditioners nonsymmetric problems advances engineering software v33 n710 p611619 29 november 2002 sangback comparisons ilu0 pointssor spai preconditioners crayt3e nonsymmetric sparse linear systems arising pdes structured grids international journal high performance computing applications v14 n1 p3948 february 2000 thomas huckle factorized sparse approximate inverses preconditioning journal supercomputing v25 n2 p109117 june ravindra boojhawon muddun bhuruth restarted simpler gmres augmented harmonic ritz vectors future generation computer systems v20 n3 p389397 april 2004 j martnez g larrazbal waveletbased spai preconditioner using local dropping mathematics computers simulation v73 n1 p200214 6 november 2006 kai wang sangbae kim jun zhang kengo nakajima hiroshi okuda global localized parallel preconditioning techniques large scale solid earth simulations future generation computer systems v19 n4 p443456 may tanaka nodera effectiveness approximate inverse preconditioning using mr algorithm origin 2400 proceedings third international conference engineering computational technology p115116 september 0406 2002 stirling scotland davod khojasteh salkuyeh faezeh toutounian bilus block version ilus factorization korean journal computational applied mathematics v15 n12 p299312 may 2004 oliver brker oscar chinellato roman geus using python large scale linear algebra applications future generation computer systems v21 n6 p969979 june 2005 liang j weston szularz generalized leastsquares polynomial preconditioners symmetric indefinite linear equations parallel computing v28 n2 p323341 february 2002 olivier besson band preconditioners application preconditioned conjugate gradient methods parallel computers parallel numerical linear algebra nova science publishers inc commack ny 2001 george gravvanis konstantinos giannoutakis parallel preconditioned conjugate gradient square method based normalized approximate inverses scientific programming v13 n2 p7991 april 2005 n guessous souhar multilevel block ilu preconditioner sparse nonsymmetric mmatrices journal computational applied mathematics v162 n1 p231246 1 january 2004 bhuruth k jain gopaul preconditioned iterative methods ninepoint approximation convectiondiffusion equation journal computational applied mathematics v138 n1 p7392 1 january 2002 kai wang jun zhang multigrid treatment robustness enhancement factored sparse approximate inverse preconditioning applied numerical mathematics v43 n4 p483500 december 2002 stephen barnard luis bernardo horst simon mpi implementation spai preconditioner t3e international journal high performance computing applications v13 n2 p107123 may 1999 claus koschinski new methods adapting approximating inverses preconditioners applied numerical mathematics v41 n1 p179218 april 2002 dennis c smolarski diagonallystriped matrices approximate inverse preconditioners journal computational applied mathematics v186 n2 p416431 15 february 2006 matthias bollhfer volker mehrmann convergence estimates algebraic multilevel preconditioners contemporary mathematics theory applications american mathematical society boston 2001 oliver brker marcus j grote sparse approximate inverse smoothers geometric algebraic multigrid applied numerical mathematics v41 n1 p6180 april 2002 prasanth b nair arindam choudhury andy j keane greedy learning algorithms sparse regression classification mercer kernels journal machine learning research 3 312003 george gravvanis solution boundary value problems using fast generalized approximate inverse banded matrix techniques journal supercomputing v25 n2 p119129 june michele benzi miroslav tma parallel solver largescale markov chains applied numerical mathematics v41 n1 p135153 april 2002 keita teranishi padma raghavan esmond ng new datamapping scheme latencytolerant distributed sparse triangular solution proceedings 2002 acmieee conference supercomputing p111 november 16 2002 baltimore maryland g gravvanis k giannoutakis p bekakos b efremides parallel systolic solution normalized explicit approximate inverse preconditioning journal supercomputing v30 n2 p7796 november 2004 edmond chow parallel implementation practical use sparse approximate inverse preconditioners priori sparsity patterns international journal high performance computing applications v15 n1 p5674 february 2001 f f hernndez j e castillo g larrazbal large sparse linear systems arising mimetic discretization computers mathematics applications v53 n1 p111 january 2007 anwar hussein ke chen fast computational methods locating fold points power flow equations journal computational applied mathematics v164165 n1 p419430 1 march 2004 j dennis h tufo scaling climate simulation applications ibm blue genel system ibm journal research development v52 n1 p117126 january 2008 michele benzi preconditioning techniques large linear systems survey journal computational physics v182 n2 p418477 november 2002