rationalising renormalisation method kanatani renormalisation technique kanatani intended iteratively minimise cost function certain form avoiding systematic bias inherent common method minimisation due sampson within computer vision community technique generally proven difficult absorb work presents alternative derivation technique places context approaches first show minimiser cost function must satisfy special variational equation newtonlike fundamental numerical scheme presented property theoretical limit coincides minimiser standard statistical techniques employed derive afresh several renormalisation schemes fundamental scheme proves pivotal rationalising renormalisation schemes enables us show renormalisation schemes theoretical limit desired minimiser various minimisation schemes finally subjected comparative performance analysis controlled conditions b introduction many problems computer vision readily formulated need minimise cost function respect unknown parameters cost function often involve known covariance matrices characterising uncertainty data take form sum quotients quadratic forms parameters finding values parameters minimise cost function often difficult one approach minimising cost function represented sum fractional expressions attributed sampson initial estimate substituted denominators cost function minimiser sought scalarweighted numerators procedure repeated using newly obtained estimate convergence obtained emerges approach biased noting kenichi kanatani developed renormalisation method whereby attempt made iteration undo biasing effects many examples may found literature problems benefiting approach work carefully analyse renormalisation concept place context approaches first specify general problem form associated cost function renormalisation applicable show cost function minimiser must satisfy particular variational equation interestingly observe renormalisation estimate theoretical minimiser cost function neither estimates obtained via commonly used methods contrast fundamental numerical scheme present new derivations given kanatanis firstorder secondorder renormalisation schemes several variations theme proposed serves rationalising renormalisation making recourse various statistical concepts experiments carried benchmark problem estimating ellipses synthetic data points covariances renormalisation schemes shown perform better traditional methods face data exhibiting noise anisotropic inhomogeneous none methods outperforms relatively straightforward fundamental numerical scheme problem formulation wide class computer vision problems may couched terms equation form vector representing unknown parameters vector representing element data example locations pair corresponding points vector data transformed way component quadratic form compound vector one component ux equal 1 ancillary constraint may also apply involve data expressed scalarvalued function estimation problem stated fol lows given collection image data determine 6 0 satisfying 2 1 holds x replaced x 1 n n l noise present corresponding system equations overdetermined may fail nonzero solution situation concerned finding best fits data sense form vision problem involving known covariance information first studied detail kanatani 12 later various others see eg 4 131420 21 conic fitting one problem kind 2 23 two conformant problems estimating coefficients epipolar equation 6 estimating coefficients differential epipolar equation 3 22 problems involves ancillary cubic constraint precise way example problems accord problem form described companion work 4 3 cost functions estimators vast class techniques solving problem rest upon use cost functions measuring extent data candidate estimates fail satisfy 1 simplicityone sets aside ancillary constraint given cost function corresponding estimate b defined 60 since 1 change multiplied nonzero scalar natural demand b satisfy 3 together b nonzero scalar guaranteed j homogeneous henceforth homogeneous cost functions considered assignment b uniquely defined scalar factor x termed jbased estimator estimate generated minimising specific cost function ancillary constraint applies accommodated via adjustment pro cedure one possibility use general scheme delivering optimal correction described 12 subsec 952 follows shall confine attention estimation phase precedes adjustment 31 algebraic least squares estimator straightforward estimator derived cost function jals l summand square algebraic distance j ux j accordingly jals based estimate termed algebraic least squares als estimate denoted b als uniquely determined scalar factor eigenvector associated smallest eigenvalue 4 32 approximated maximum likelihood estimator als estimator treats data equally valuable information measurement errors available desirable incorporated estimation process present estimator capable informed weighting based principle maximum likelihood draws upon kanatanis work geometric fitting 12 chap 7 measurement errors generally unknowable regard collective data sample value taken aggregate vectorvalued random variables assume distribution exactly specified element collection fp j 2 hg candidate distributions h set n candidate distributions distribution p effect n noisedriven fluctuating quantity around x assume data come equipped collection positive definite k k covariance matrices matrices constitute repositories prior information uncertainty data put x use assuming 2 h p unique distribution satisfying following conditions random vectors x x j equivalently noises behind x x j stochastically independent normal distribution mean value vector x covariance matrix x distribution p readily described terms probability density function pdf x resorting principle maximum likelihood give greatest confidence choice likelihood function 7 fx attains maximum using explicit form pdfs involved one show maximum likelihood estimate parameter b ml cost function attains minimum 4 12 term summation represents squared mahalanobis distance x x note value b ml remains unchanged covariance matrices multiplied common scalar parameter naturally splits two parts 1 parts encompass principal parameters nuisance parameters respectively mostly interested 1 part b ml call maximum likelihood estimate denote b ml turns b ml identified minimiser certain cost function directly derivable jml cost function lend explicit calculation however tractable approximation 4 derived form function x vector k k matrix let next jaml simply written jaml based estimate called approximated maximum likelihood aml estimate denoted b aml observed jaml derived without recourse principles maximum likelihood example using gradient weighted approach also incorporates covariances various terms may therefore used describe methods aim minimise cost function jaml although terms may fully discriminating candidate labels include heteroscedastic regression 13 weighted orthogonal regression 1 9 gradient weighted least squares 24 33 variational equation since b aml minimiser jaml following equation holds jaml denotes row vector partial derivatives jaml respect term variational equation direct computation shows x symmetric matrix thus 7 written nonlinear equation unlikely admit solutions closed form obviously every solution variational equation point global minimum jaml attained however solution set equation provides severely restricted family candidates global minimiser within set minimiser much easier identify 4 numerical schemes closedform solutions variational equation may infeasible practice b aml found numerically throughout shall assume b aml lies close b als assumption increase chances candidate minimiser obtained via numerical method seeded b als coincides b aml 41 fundamental numerical scheme vector satisfies 9 falls null space matrix x thus k 1 tentative guess improved guess obtained picking vector k eigenspace x k 1 closely approximates null space x eigenspace course one corresponding eigenvalue closest zero proved soon sequence updates converges limit solution 4 fundamental numerical scheme implementing idea presented figure 1 algorithm regarded variant newtonraphson method 1 set 2 assuming k 1 known compute matrix x k 1 3 compute normalised eigenvector x k 1 corresponding eigenvalue closest zero take eigenvector k 4 k sufficiently close k 1 terminate procedure otherwise increment k return step 2 figure 1 fundamental numerical scheme 42 sampsons scheme let let modification jaml x variable denominators contributing fractions frozen value simplicity abbreviate j 0 first propose scheme aiming minimise function involving fractional expressions jaml although cost functions incorporate covariance matrices sampsons scheme smp applied jaml takes b als initial guess 0 given k 1 generates update k minimising cost function 7 j 0 assuming sequence f k g converges sampson estimate defined b note function aml k 1 quadratic finding minimiser function straight forward minimiser k eigenvector k 1 corresponding smallest eigenvalue moreover eigenvalue equal j 0 scheme summarised figure 2 1 set 2 assuming k 1 known compute matrix k 1 3 compute normalised eigenvector k 1 corresponding smallest nonnegative eigenvalue take eigenvector k 4 k sufficiently close k 1 terminate procedure otherwise increment k return step 2 figure 2 sampsons scheme quick glance shows scheme differs fundamental numerical scheme uses matrices form instead matrices form two types matrix related formula x letting k 1 11 taking account equality j 0 see b smp satisfies jaml l l l l identity matrix call sampson equation note different variational equation 9 result b smp genuine minimiser jaml renormalisation matrices e jaml l underlying variational equation sampson equation 12 viewed modified normalised forms first realised kanatani 12 different type modification proposed based statistical considerations requirement modified renormalised unbiased sense using renormalised one formulate equation analogous variational sampson equations equation turn used define estimate rationalise unbiasing procedure condition noise appropriate statistical model small next section various schemes presented numerically computing parameter estimate defined procedure later section devoted derivation unbiasing procedure appropriate noise necessarily small development schemes numerically computing parameter estimate defined general procedure 51 unbiasing regard given data value taken random vectors introduced earlier suppose form following random version true value view 4 ax 0 hand since rankone matrix ax nonnegative definite since also bx nonnegative definite 1 nonnegative definite ax independent generically positive definite e 0 thus average attain true value zero biased bias removed forming matrix terms e calculated explicitly matrixvalued function specified later thus bx positive definite merely nonnegative definite unbiased written random matrix raw model obtaining fully deterministic modification guided 14 take modified somewhat surprisingly choice turns satisfactory problem change x multiplied common scalar change properly designed algorithm employing modified give outcomes values remain intact x multiplied common scalar especially important aim estimate parameter also evaluate goodness fit therefore change numerators fractions forming necessary dependence dx fairly complex gain idea needs changed instructive consider simplified form dx firstorder sense approximation dx shown shortly defined 6 dependence bx simple multiplied scalar bx multiplied scalar suggests introduce compensating factor j com x com short property x multiplied scalar j com multiplied inverse scalar help j com form renormalised numerator j com b next set given 10 n defined numerators 16 clearly scale invariant note j com plays role similar played factors formula x given 8 indeed x multiplied b consequently multiplied 1 main difference j com latter fractions change index j com common numerators involved find proper expression j com take look x inspiration note account 8 x 0 analogy demand 0 equation together 16 implies n n obvious j com thus defined property required compensating fac tor moreover form j com accordance unbiasedness paradigm indeed form random version j com insofar e abbreviating j com x j com 0 see given unbiased justifies design since view 19 j com equal 1 mean difference blurred average thus refined renormalisation based 16 close spirit original normalisation based 15 52 renormalisation equation renormalisation equation analogue variational sampson equations alike naturally derived specific cost function result clear whether solution general belief close vicinity b als solution one solution termed renormalisation estimate denoted ren since renormalisation equation different variational distinct b aml b smp stressed difference b ren b aml may unimportant estimates regarded firstorder approximations b ml hence likely statistically equivalent practice b ren represented limit sequence successive approximations b ren favourable conditions sequence convergent limit genuine solution 20 various sequences taken calculate b ren way simplest choice results mimicking fundamental numerical scheme follows take b als initial guess 0 suppose update k 1 already generated form k 1 compute eigenvector corresponding eigenvalue closest zero take eigenvector k sequence f k g converges take limit b ren shall see shortly b ren thus defined automatically satisfies 20 recipe calculating b ren constitutes term renormalisation algorithm 6 firstorder renormalisation schemes firstorder renormalisation based formula already pointed subsection 51 justify formula retain sequence independent random vectors model data assume distribution p make fundamental assumption effect noise driving x small simplicity denote x x contract x since noise driving x small replace ux firstorder sum taylor expansion x next taking account 0 write hence view 5 6 account 13 establishes 21 formula 21 validated safely use form given 16 j com given 18 respective renormalisation estimate called firstorder renormalisation estimate denoted b ren1 61 fori scheme introducing appropriate stopping rule renormalisation algorithm readily adapted suit practical calculation case firstorder renormalisation resulting method termed firstorder renormalisation scheme version simply fori scheme given figure 3 1 set als 2 assuming k 1 known compute matrix k 1 using 16 3 compute normalised eigenvector k 1 corresponding eigenvalue closest zero take eigenvector k 4 k sufficiently close k 1 terminate procedure otherwise increment k return step 2 figure 3 firstorder renormalisation scheme version 62 forii scheme fori scheme slightly modified resulting firstorder renormalisation scheme version ii forii scheme effectively one two schemes proposed kanatani 12 chap 12 concerns secondorder renormalisation introduce function com abbreviating j 0 com com let com b com n immediately verified 6 0 also com previously take b als initial guess 0 suppose update k 1 already generated normalised eigenvector k 1 cor responding smallest eigenvalue k view 25 update k straightforwardly generated updates k c k turns certain approximation c k updated directly c k 1 rather appealing formula substituting k k 1 22 obtain last two equations imply assume j 0 since terms close j com k realistic assumption assumption c equation 26 becomes formula successive updating c k defining consecutive k help k c k 25 proceeding fori scheme obtain sequence f k g converges take corresponding limit b ren1 shown b ren1 thus defined satisfies renormalisation equation firstorder renormalisation scheme version ii forii scheme based algorithm summarised figure 4 63 foriii scheme help function j 0 com yet another defining sequence constructed take b als initial guess 0 suppose update k 1 already generated define k minimiser function 7 j 0 com k 1 60 com k 1 since com als c 2 assuming k 1 c k 1 known compute matrix 3 compute normalised eigenvector k 1 cor responding smallest eigenvalue k take eigenvector k define c k 27 4 k sufficiently close k 1 terminate procedure otherwise increment k return step 2 figure 4 firstorder renormalisation scheme version ii n com n n follows k satisfies assuming sequence f k g converges let b clearly b ren1 satisfies equation equivalent 20 note method b ren1 defined limit sequence minimisers cost functions algorithm similar sampsons algorithm latter course uses different cost functions minimisers k directly calculated see rewrite 28 com see k eigenvector j 0 com k 1 corresponding eigenvalue linear pencil p defined real eigenvector p k 1 eigenvector necessarily j 0 com com conclude j 0 eigenvector p k 1 corresponding 2 assuming k 1 known compute matrices k 1 n k 1 3 compute normalised eigenvector eigenvalue problem corresponding smallest eigenvalue take eigenvector k 4 k sufficiently close k 1 terminate procedure otherwise increment k return step 2 figure 5 firstorder renormalisation scheme version iii smallest eigenvalue observation leads firstorder renormalisation version iii foriii scheme given figure 5 matrices n k 1 singular eigenvalue problem p k 1 degenerate way reducing problem nondegenerate one based special form matrices n presented 4 7 secondorder renormalisation secondorder renormalisation rests knowledge exact form dx first determine form next use evolve secondorder renormalisation estimate various schemes calculating 71 calculating dx determining form dx tedious straightforward commence introducing notation vector variables append vector unital component yielding vector carriers given special form ux described section 2 u expressed k follows adopt einsteins convention according summation sign repeated indices omitted convention equation 29 becomes let x random gaussian k vector mean x covariance matrix clearly mean defined covariance matrix since standard result moments multivariate normal distribution view 31 hence l l matrices defined 2 matrices 32 written hence desired formula 72 redefining j com retain framework subsection 51 use full expression dx instead firstorder approximation aim modify term similar recall remembering need suitable compensation scale change main problem dx change equivariantly scale change 7 two components dx defined 34 undergo two different transformations solution follows introduce compensating factor j com x j com short property x multiplied j com multiplied 1 place factor front square front form modified numerator follows numerator obviously invariant respect scale change analogy 17 introduce analogy 16 let demanding obtain following quadratic equation j com equation two solutions com positive definite n 1 n 2 nonnegative definite com 0 since compensating factor used firstorder renormalisation nonnegative take analogy j com compensating factor denote j com thus j com multiplying numerator denominator j com n see j com n 2 small compared n may readily infer j com n expression similar formula 18 j com indicates solution adopted consistent firstorder renormalisation inserting j com given 38 37 obtain welldefined expression use define renormalisation estimate using renormalisation equation 20 call estimate secondorder renormalisation estimate denote b ren2 73 sori scheme mimicking fori scheme readily advance scheme numerically finding b ren2 call secondorder renormalisation scheme version sori scheme steps given figure 6 1 set 2 assuming k 1 known compute matrix k 1 using 37 38 3 compute normalised eigenvector k 1 corresponding eigenvalue closest zero take eigenvector k 4 k sufficiently close k 1 terminate procedure otherwise increment k return step 2 figure renormalisation scheme version 74 sorii scheme sori scheme modified similar way employed fori scheme resulting secondorder renormalisation scheme version ii sorii scheme effectively second two schemes originally proposed kanatani introduce com abbreviating j 0 com com let com immediately verified equations 22 24 hold j com com counterpart 23 take b als initial guess 0 suppose update k 1 already generated note let k normalised eigenvector k 1 corresponding smallest eigenvalue k intend find update c k appealing directly c k 1 end observe substituting k k 1 22 taking account 41 assuming explained analogously deriving forii scheme taking account 43 j 0 combining equation 44 yields taking account c 2 rewrite 45 quadratic constraint c k 1 given let equation 46 two solutions real k 1 0 suppose k 1 0 c k directly defined 43 would non negative therefore reasonable insist c k obtained updating c k 1 also nonnegative requirement met setting c next ensuring k reason select 1 c k 1 obtaining treat case k 1 0 first multiply numerator denominator fractional expression 48 obtaining next note k k small compared k formula similar 27 use equality sign instead approximation sign generate c k case k 1 0 way arrive following update formula sorii scheme formulated figure 7 1 set 2 assuming k 1 c k 1 known compute matrix k 1 k 1 using 42 3 compute normalised eigenvector k 1 k 1 corresponding smallest eigenvalue k take eigenvector k define c k using 49 4 k sufficiently close k 1 terminate procedure otherwise increment k return step 2 figure 7 secondorder renormalisation scheme version ii 75 soriii scheme estimate b ren2 represented limit sequence minimisers cost functions follows take b als initial guess 0 suppose update already generated define k minimiser function com k 1 60 com k 1 assuming sequence f k g converges take lim k1 k b ren2 readily shown b ren2 thus defined satisfies also shown com com k eigenvector j 0 corresponding eigenvalue defined real fact k eigenvector p k 1 corresponding smallest eigenvalue observation leads secondorder renormalisation scheme version iii soriii scheme given figure 8 1 set 2 assuming k 1 known compute matrices k 1 n 2k 1 3 compute normalised eigenvector eigenvalue problem corresponding smallest eigenvalue take eigenvector k 4 k sufficiently close k 1 terminate procedure otherwise increment k return step 2 figure 8 secondorder renormalisation scheme version iii eigenvalue problem quadratic pencil readily reduced eigenvalue problem linear pencil indeed satisfy exists 0 l l 0 case necessarily matrices n 1 n 2 appropriate soriii scheme nonnegative definite necessarily positive definite way reducing problem 51 similar problem involving positive definite matrices 2 given 4 method takes advantage special form matrices n 1 n 2 8 experimental results previously derived algorithms tested problem conic fitting constitutes classical benchmark problem literature 2 5 7 8 1011 1519 23 specifically fitting algorithms applied contaminated data arising portion ellipse synthetic testing employed enables precise control nature data associated uncertainties results obtained real world testing applications domains described earlier presented subsequent work tests proceeded follows randomly oriented ellipse generated ratio major minor axes range 2 3 major axis approximately 200 pixels length one third ellipses boundary chosen base curve included point maximum curvature ellipse set true points randomly selected distribution uniform along length base curve true points covariance matrix randomly generated using method described accordance chosen average level noise true points perturbed randomly accordance associated covariance matrices yielding data points general noise conformed inhomogeneous anisotropic distribution figure 9 shows large ellipse selected true points small ellipse points data points smaller ellipses represents level set probability density function used generate datum captures graphically nature uncertainty described covariance matrix figure 9 true ellipse data associated covariance ellipses following procedure adopted generating covariance matrices associated image points prescribing anisotropic inhomogeneous noise given average level scale particular covariance matrix first selected uniform distribution range 0 2 similar results obtained using distributions next skew parameter generated uniform distribution 0 05 intermediate covariance matrix formed setting matrix rotated angle selected uniform distribution 2 generate final covariance cos sin sin cos let tr denote trace matrix since clear procedure ensures e tr data points associated covariances prepared method test challenged determine coefficients best fitting conic note therefore assumed conic ellipse methods supplied data points specific method able utilise uncertainty information also supplied data points covariance matrices estimates generated measure error computed using recipe given testing repeated many times using newly generated data points covariance matrices true data points remaining intact average errors displayed method error measure employed follows assume particular method estimated ellipse error estimate declared sum shortest euclidean distances true point estimated ellipse note measure takes advantage fact underlying true points known unknown alternative measure might sum mahalanobis distances data points estimated ellipses methods tested follows least squares scheme renormalisation scheme 1 renormalisation scheme 2 renormalisation scheme 3 renormalisation scheme 1 renormalisation scheme 2 renormalisation scheme 3 table 1 shows average error obtained method applied 500 sets data points varying 1 10 pixels steps 1 set data points obtained perturbing shows tabular data graphical form algebraic least squares method performs worst renormalisation schemes fundamental numerical scheme perform best method systematically deficient generating average errors 22 greater best methods sori sorii schemes similarly deficient however best seen incremental developments leading soriii finally fori forii foriii soriii schemes seen trail fns slightly noise level als smp ii iii sor sor ii sor iii fns 10 2710 1093 1072 1072 1071 1093 1093 1071 1075 20 5579 2078 1990 1987 1987 2078 2078 1987 1976 30 8340 3169 3077 3067 3067 3169 3169 3067 3049 50 15091 5662 5129 5092 5092 5655 5661 5092 5054 80 26036 9294 8115 8037 8037 9254 9288 8037 7966 90 31906 10791 9036 8948 8948 10748 10776 8950 8827 table 1 error results obtained methods acerage noise level pixels515 average error pixels als figure results average noise level depicted graphically als refers algebraic least squares method errors range group 1 comprises tabulated results details 9 conclusion statistical approach parameter estimation problems kenichi kanatani occupies important place within computer vision literature however critical component work socalled renormalisation method concerned minimising particular cost functions proven difficult vision community absorb major aim paper clarify number issues relating renormalisation method relatively general problem form encompassing many vision problems first derived practical cost function claims optimality may advanced showed sampsonlike method minimisation generates estimates statistically biased renormalisation rationalised approach undoing bias generated several novel variations theme pivotal establishing framework comparing selected iterative minimisation schemes devising called fundamental numerical scheme emerges scheme considerably simpler derive implement renormalisationbased counterparts also exhibits marginally superior performance acknowledgements authors grateful insightful comments marino ivancic kenichi kanatani garry newsam naoya ohta robyn owens addition authors would like thank two anonymous referees providing suggestions led improvements presentation paper work part funded australian research council cooperative research centre sensor signal information processing r statistical analysis measurement error models applications arcata fitting conic sections scattered data determining egomotion uncalibrated camera instantaneous optical flow image vision computing 10 direct least square fitting ellipses buyers guide conic fitting measurement statistical bias conic fitting renormalisation heteroscedastic regression computer vision problems bilinear constraint role total least squares motion analysis fitting ellipses predicting confidence envelopes using bias corrected kalman filter note least square fitting ellipses nonparametric segmentation curves various representations fitting conics sections scattered data iterative refinement bookstein algorithm estimation planar curves new approach geometric fitting tutorial application conic fitting tr measurement error models fitting ellipses predicting confidence envelopes using bias corrected kalman filter estimation planar curves surfaces nonplanar space curves defined implicit equations applications edge range image segmentation ellipse detection matching uncertainty threedimensional computer vision note least squares fitting ellipses buyers guide conic fitting development comparison robust methods estimating fundamental matrix optimization criteria used twoview motion analysis direct least square fitting ellipses heteroscedastic regression computer vision statistical optimization geometric computation statistical bias conic fitting renormalization nonparametric segmentation curves various representations role total least squares motion analysis optimal estimation matching constraints motion analysis camera unknown possibly varying intrinsic parameters ctr wojciech chojnacki michael j brooks consistency normalized eightpoint algorithm journal mathematical imaging vision v28 n1 p1927 may 2007 n chernov convergence fitting algorithms computer vision journal mathematical imaging vision v27 n3 p231239 april 2007 n chernov c lesort n simnyi complexity curve fitting algorithms journal complexity v20 n4 p484492 august 2004 wojciech chojnacki michael j brooks anton van den hengel darren gawley fitting surfaces data covariances ieee transactions pattern analysis machine intelligence v22 n11 p12941303 november 2000 n chernov c lesort least squares fitting circles journal mathematical imaging vision v23 n3 p239252 november 2005 wojciech chojnacki michael j brooks anton van den hengel darren gawley revisiting hartleys normalized eightpoint algorithm ieee transactions pattern analysis machine intelligence v25 n9 p11721177 september wojciech chojnacki michael j brooks anton van den hengel darren gawley fns heiv link two vision parameter estimation methods ieee transactions pattern analysis machine intelligence v26 n2 p264268 january 2004