informationtheoretic bounds target recognition performance based degraded image data abstractthis paper derives bounds performance statistical object recognition systems wherein image target observed remote sensor detection recognition problems modeled composite hypothesis testing problems involving nuisance parameters develop informationtheoretic performance bounds target recognition based statistical models sensors data examine conditions bounds tight particular examine validity asymptotic approximations probability error imaging problems problems involving gaussian poisson multiplicative noise random pixel deletions considered well leastfavorable gaussian clutter sixth application involving compressed sensor image data considered detail study provides systematic computationally attractive framework analytically characterizing target recognition performance complicated nongaussian models optimizing system parameters b introduction typical target recognition problems involve detection classification targets well estimation target parameters location orientation set possible targets comprises objects tanks trucks planes scene targets present acquired sensors coherent laser radar imagers synthetic aperture radar systems forwardlooking infrared radar flir systems hyperspectral sensors 1 2 imaging sensors used object recognition numerous military civilian applications exploit capabilities sensors targetrecognition context imageunderstanding algorithms required interpret remote observations complex scenes context patterntheoretic framework 3 4 using deformable template representation targets considered targets modeled deformations rigidbody templates variability pose introduced via rigidbody motions various unknown parameters target class thermodynamic profile characterize withinclass variability modeled statistically statistical models scene clutter sensors also determined statistical approach provides systematic framework integrating prior knowledge scene targets observational models fusing information multiple sensors accurate statistical models identified principle possible compute optimal solutions problems detection classification parameter estimation application basic principles statistical inference 5 6 even optimal algorithms computationally intractable statistical theory provides fundamental bounds performance algorithm practical benefits approach documented prior work target recognition 3 4 related problems 7 target classification often described composite hypothesistesting problem various hypotheses dierent target types interest null hypothesis target present scene probabilistic models formulated hypothesis models may complicated due dependencies various unknown parameters target orientation motion reflectance properties parameters may viewed nuisance parameters leading composite hypothesistesting formulation additional diculties introduced sensors located remote location eg mobile platform case sensor data transmitted bandwidthlimited communication channel prior processing computer hosts target recognition algorithm lossy compression algorithms used eciently transmit sensor data host computer operation degrades recognition performance important select compression algorithm carefully heuristic evaluation methods used current practice 8 would preferable select parameters compression algorithm well system parameters optimize fundamental measures recognition performance measures include bayesian cost bayesian probability error typically intractable complex problems attractive alternative work criteria tractable meaningful approximations ideal performance measures natural candidates include cherno kullbackleibler distances 6 9 cherno distances provide upper bounds asymptotic expressions probability error p e detection problems kullbackleibler distances provide upper bounds probability miss p miss fixed probability false alarm p f cherno kullbackleibler distances belong broad category measures studied ali silvey quantify distance dissimilarity two distributions 10 distances satisfy certain axioms statistical inference makes particularly attractive problems involving quantization multisensor data fusion driving application used throughout illustrate theory numerous possible applications detection known target unknown orientation sensor data corrupted additive white gaussian noise subjected lossy compression using transform image coder show despite nonlinearity introduced quantizers tractable informationtheoretic bounds detection performance still derived composite hypothesis testing involving unknown nuisance parameters even informationtheoretic distances dicult evaluate case develop tractable upper lower bounds distances well precise expressions asymptotic probabilities error tightness bounds evaluated theoretical analysis well montecarlo simulations finally extend theory developed binary hypothesis testing mary case covers recognition problems involving multiple target classes paper organized follows sec 2 describes system model likelihood ratio test lrt used optimal detector sec 3 introduces alisilvey distances properties make attractive target recognition problems sec 4 briefly reviews basic asymptotic properties cherno bounds sec 5 performance bounds simple detection problem involving compressed noisy data derived compared actual probability error using theoretical analysis numerical simulations analysis extended complex detection problems involving nuisance parameters sec 6 bounds classification multiple targets derived sec 7 conclusions presented sec 8 system model demonstrate key ideas concepts behind methodology first consider composite binary detection problem task detection algorithm determine whether known target present may unknown nuisance parameters associated target methods developed binary case extended classification multiple targets sec 7 21 image model consider target depending unknown parameter taking values set instance target may known template unknown orientation groundbased targets would special orthogonal group so2 rotation matrices 3 general patterntheoretic framework miller et al 3 templates cad computer aided design representations twodimensional surface manifolds rigid objects element lie group whose elements characterize deformations template infrared imaging problems may vector smoothly varying function describing unknown thermodynamic state target case target denoted completely known known target parameterizations reduce variability targets lowdimensional unknown parameter 22 sensor model target seen projection map projected image captured noisy sensor let sensor data npixel image data related true image probabilistic map pi instance shall apply analysis additive white gaussian noise model case sensor noise mean zero variance 2 pi nvariate gaussian distribution mean covariance matrix equal 2 times n n identity matrix figs 1 2 show two examples used illustrate theory 64 64pixel images groundbased t62 tank orientation truck orientation along noisy sensor data signaltonoise defined denotes euclidean norm x snr example figs 1 2 15 db 23 data model order account need transmit remote sensor data central computer 8 include model lossy compression formulation special case setup conventional detection problem sensor data directly available target detection algorithm restrict attention class transformbased coders ubiquitous practice transform coding attractive due good compression performance low computational complexity 11 model simplifies theoretical analysis well use following simplified mathematical model let u unitary orthonormal transform used coder c transformed image data practice u could wavelet transform discrete cosine transform jpeg image compression standard denote transform projected image ut signal energy preserved orthonormal transforms c transform coecients c quantized quantized coecients denoted assume scalar quantizers q n applied individual coecient c standard choice used experiments uniform scalar quantizers deadzone near zero see fig 3 words nonlinear operation q separable resulting decompressed image diers due quantization errors throughout paper use tilde symbol denote quantities pertaining quantized data fig 4 summarizes model clean images noisy sensor data observed compressed data 24 conditionally independent data sets variety target recognition problems data may partitioned components statistically independent conditioned target property simplifies analysis suggests use asymptotic techniques encountered applications following 1 sensor noise various imaging modalities modeled independent identically distributed iid gaussian noise see sec 22 multiplicative noise coherent imaging systems poisson noise noncoherent optical imaging systems case individual pixels image data independent conditioned illuminating image field 2 multisensor data degradations introduced individual sensors often independent even though individual sensors may involve complicated statistical models 3 main application interest sensor noise iid gaussian transform coecients individually quantized sensor noise w viewed orthonormal transform domain still iid gaussian noise degradations introduced cascade sensor noise coecient quantization independent observed transform coecients c independent given discrete distribution coecients obtained integration distribution c n quantization cells 25 detection problem image sensor data models target detection formulated binary statistical hypothesis test h 0 h 1 refer hypotheses target absent present respectively 2 bayesian detection uncertainty modeled using prior distribution distributed according mixture distribution 3 26 optimal likelihood ratio detector hypotheses h 1 h 0 distributed according pdfs likelihood p 0 sucient statistic detection ie need know likelihood ratio deciding hypotheses h 1 h 0 5 likelihood ratio invariant invertible operations transform u transform coder variety optimality criteria detection algorithm takes form lrt 1 4 appropriate threshold value depends optimality criterion 5 neymanpearson test threshold chosen given probability false probability miss p miss minimized minimumprobability oferror rule optimal decision prior probability hypothesis h lrt 4 optimal equal prior probabilities probability error case 2 5 expectation hypothesis h 0 interest also conditional probability error p e characterizes detection performance specific target configuration consider two detection problems first simple hypothesistesting problem set reduces singleton second problem composite hypothesistesting 4 implicitly assume randomized decision made case use notation integrals 5 whether integration variable continuous discretevalued measure used either lebesgue counting measure problem detection problem 1 nuisance parameters problem known target deterministic detection problem becomes simple binary hypothesis testing problem assume sec 24 data partitioned independent components make discussion concrete focus third case sec 24 iid gaussian sensor noise transform coding distributions transform coefficients h 0 h 1 given respectively likelihood ratio product likelihood ratios lc individual coecients 6 hence log likelihood ratio sum log likelihood ratios coecient detection problem 2 presence nuisance parameters case nuisance parameter modeled random prior h 1 distribution compressed data c mixture product marginals lrt 4 computed using mixture 7 shall see sec 6 presence mixture model introduces significant computational analytical complications 3 informationtheoretic bounds detection previous section formulated target detection based compressed data statistical hypothesis testing problem threshold lrt 4 chosen minimize probability error p e probability miss p miss given value p f unfor tunately p e p miss intractable functions nvariate distributions general evaluated experimentally hence feasible optimize parameters highdimensional nonlinear systems lossy image coders respect p e p miss motivated us investigate general category performance measures provide tractable bounds p e p miss since ability distinguish two statistical hypotheses depends respective conditional distributions data measures distance dissimilarity two distributions natural performance metrics ali silvey studied generic category distances measure dissimilarity two distributions 10 alisilvey class distances based axiomatic definition takes general form f increasing function c convex function 0 likelihood ratio data e 0 expectation hypothesis h 0 convenient also allow pairs f c f decreasing c concave kassam 12 poor thomas 13 shown performance metrics used optimal quantizer design detection problems addition convexity properties ali silvey distances possess two attractive properties invariant application invertible maps data decreased application manytoone maps quantization 12 13 specifically observe f ln convex decreasing concave alisilvey class 8 even though p e practical choice design exist two distances alisilvey class closely related p f p miss first cherno distances 5 69 f convex decreasing c concave bhattacharyya distance 6 9 cherno distances give upper bound p f threshold lrt 4 p probability hypothesis h minimumprobabilityoferror rule together 13 give upper bound p e bound tightened scale factor 5 give also use kullbackleibler distances 9 f linear increasing c convex motivation considering 15 steins lemma 9 conditions lemma relates asymptotic probability miss kullbackleibler distance probability distributions without target fixed small probability false alarm asymptotic equality symbol fn gn means lim n kullbackleibler cherno distances related formulas ds ds direct relationship p f p miss p e makes kullbackleibler cherno distances appropriate choice obtaining performance bounds illustrate concepts consider simple hypothesistesting problem based gaussian model sec 22 absence compression case distances 11 15 take simple form snr hence gaussian data kullbackleibler cherno distances proportional snr nongaussian data compressed data direct relationship snr detection performance shall shortly see distances 11 still conveniently evaluated problems data compression takes place first examine conditions 11 15 give tight bounds target detection 4 asymptotic expressions cherno bounds 12 13 14 p f p miss p e hold distribution data sample size n many problems n large data contains many independent components see sec 24 central limit theorem applies distribution log likelihood ratio results sec 3 strengthened refer reader van trees 17 ch 27 lucid exposition main ideas results first fundamental result quantities fact asymptotic max s01 p 0 gives precise exponential rate convergence probabilities zero results strengthened using asymptotic integral expansion techniques yields exact asymptotic expressions specifically define notational convenience let first second derivatives large sample size exists 0 1 exponential factors 19 20 equal upper bounds 12 13 centrallimittheorem analysis provides multiplicative factor significant prior probabilities h 0 h 1 equal one combine 19 20 obtain following asymptotic approximation e 21 gaussian model maximization 18 respect gives optimal cherno exponent e snr8 22 holds large snr applicability asymptotic conditions target recognition examined next 5 bounds target detection without nuisance parameter first consider detection problem 1 sec 26 derive performance bounds optimal lrt detector 4 logarithm likelihood ratio 6 sum marginal log likelihood ratios l n transform coecient hence cherno kullbackleibler distances 11 15 additive n transform coecients additivity property simplifies analysis design systems using 23 24 optimality criterion instance paper 14 shows optimally design transform coders subject bit rate constraints using 23 performance measure thesis 15 compares performance wavelet dct coders using performance measures additivity property cherno kullbackleibler distances applies problem data partitioned independent components see sec 24 51 example investigate applicability theory target detection conducted experiments using database t62 tank images generated using prism 3 simulation package images corrupted iid gaussian sensor noise described sec 22 fig 1 shows one image orientation along noisy sensor data db noisy image data compressed using wavelet coder daubechies length4 d4 wavelet filter four decomposition levels deadzone scalar quantizers 16 dead zone quantizers twice step size model received transform coecients c independent distributions p 1n hypotheses h 0 computed numerical integration gaussian distribution unquantized coecients c n quantization bins bit rates estimated using firstorder entropy data presence target entropy compressed image data absence target slightly lower hypotheses h 0 h 1 assumed 3 courtesy dr alvin curran thermo analytics calumet mi equally likely used optimal lrt detector compressed data analyzed eects compression measured bit rate compressed data detection performance probability error p e guaranteed decrease bit rate ln p e alisilvey class fig 5 compares three estimates probability error p e first estimate computed using montecarlo simulations dierent noise realizations accuracy estimate high due large number independent experiments performed second estimate p es computed using cherno upper bound 14 evaluated bound motivation choosing choice quasioptimal see fig 6 large bit rates quantization eects negligible p e tends probability error unquantized data since data gaussiandistributed exact expression p e available equal priors z e x 2 2 dx marcums qfunction large snr p e 2 snr expsnr8 5 18 cherno distance maximum cherno bound 14 is2 expd expsnr8 bound approximately four times larger actual p e high bit rates lower rates upper bound slightly less conservative third estimate p e fig 5 discussed next 52 accuracy asymptotic cherno approximations order improve upper bound 14 tempting use asymptotic expression 21 cherno bound since bound established using central limit theorem arguments expect applicable log likelihood ratio sum n independent components n large however problem sec 51 components c n identically distributed validity 21 hinges whether central limit theorem independent identically distributed components applies roughly speaking requires individual component ln l n sum log likelihood ratios small relative sum precisely sucient lindeberg condition holds 18 ch xv6 lindeberg condition approximately satisfied application sec 51 asymptotic expression 21 quite accurate shown fig 5 general lindeberg conditions expected approximately hold highresolution imaging sensors multiple copies scene dierent noise realizations available conditions likely satisfied applications involving targets relatively pixels target even relatively large target like one fig 1 lindeberg conditions hold well low bit rates transformed coecients quantized zero log likelihood ratio dominated significant components 6 bounds target detection nuisance parameter consider complicated scenario involving nuisance parameters modeled random prior h 1 distribution data mixture distribution 7 sec 5 performance optimal detector evaluated using cherno bounds tight conditions however image coecients longer independent log likelihood longer additive coecients hence distances given ndimensional integrals general cannot evaluated analytically fortunately possible derive bounds informationtheoretic distances useful tractable performance measures 61 upper bounds alisilvey distances circumvent diculty evaluating exact distances compute average dis tance averaged turns upper bound exact alisilvey dis tance 3 likelihood ratio weighted average conditional likelihood ratios lc p1 c lc 27 jensens inequality 9 lc clc convex function c pdf hence alisilvey measure form lc inequality holds f increasing last equality follows definition alisilvey distances 8 result 28 also applies f decreasing c concave first apply 28 ln p e discussed sec 3 alisilvey distance case p p e refers probability error given evaluated sec 5 according 29 probability error least equal average conditional probability error cherno kullbackleibler distances 11 15 28 yields two important points made first even performance index independent 28 would general satisfied equality much stronger condition would need satisfied namely lc would independent c implying plays role inference hence equality achieved 28 trivial cases second nonlinear functions f expression 28 average distance dp 0 increasing resp decreasing c convex resp concave jensens inequality implies average distance upper bound 28 particular inequality 31 applies cherno distances case 28 upperbounded average distance p 0 refer 28 average f 1 distance upper bound next let us see average bound cherno distance relates p e 14 inequalities go direction righthand side 32 serve approximation p e 62 lower bounds alisilvey distances also explore possibility simple lower bounds kullbackleibler cher distances provide upper bounds p f p e lower bounds cherno distances yield upper bounds p e hence unbeatable bounds performance target detection algorithm minimization distance dp 0 possible mixture distributions form 7 illustrated fig 10a one might conjecture distance lowerbounded distance corresponding least favorable worst worst however inequality hold general 4 likewise inequality p e p eworst hold general still concept leastfavorable plays central role asymptotic analysis sec 63 well minimax detection 21 ch 9 results sec 5 immediately obtain upper bound probability error eworst 1s worst 63 asymptotic expressions significant simplifications arise asymptotic scenario asymptotic expressions probability error dominated leastfavorable classical paper 22 presents similar results closely related context understand basic idea consider simple case prior distribution concentrated two values 1 2 p 0 large n distributions increasingly well separated support sets become essentially disjoint functional prior greatest lower bound obtained minimizing convex set priors conjecture true minimum would need arise extremal point set property would hold concave generally hold convex 20 however similar arguments show favorable prior mass distribution resulting upper bound dp 0 useful though already established tighter upper bound 31 formal proof result beyond scope paper see 22 example analysis similar result holds prior distribution concentrated arbitrary finite number points even continuous priors smoothness assumptions words inequality 33 holds asymptotic equality cherno distances worst worst tractable asymptotic approximation p e obtained via 21 64 example section compare bounds secs 61 62 asymptotic approximation 35 sec 63 actual p e experiments performed database sec 51 images corrupted iid gaussian sensor noise compressed using wavelet coder 4 27 optimal lrt takes form prior uniform implementation approximated integral summation 36 orientations 0 used lrt montecarlo simulations accurately estimate p e also evaluated average approximation 32 p e upper bound 34 probability error minimax detector asymptotic expression 21 35 p e figs 7 8 show quantities function bit rate tank truck imagery average snr 14 db bit rate computed using firstorder entropy 25 approximated average 36 orientations described found average approximation 32 p e relatively accurate tank truck imagery hand upper bound 34 p e loose tank data compared truck image data asymptotic approximation 21 35 remarkably accurate truck data factor approximately two tank data explained examining fig 9 shows snr proportional kullbackleibler cherno distances case uncompressed gaussian data function orientation parameter clean tank images certain angles low energy content seen negative spikes tank image snr curve cherno distance worstcase angles give overly conservative upper bound p e moreover spikes narrow convergence asymptotic approximation 35 slow hand clean image energy content vary much orientation truck imagery lower upperbound curves relatively close asymptotic approximation 35 accurate variability snr target orientation shown fig 9 tank truck imagery though average approximation p e 32 close p e tank imagery may true general discussed sec 61 tightness bounds depend variation snr orientation see fig 9 7 mary hypothesis testing multipletarget case considered binary detection problem receiver decides whether known target present analysis extended case alphabet possible targets consists 2 possible targets includes binary detection problem special case 2 second hypothesis null hypothesis using notation similar sec 2 assume nuisance parameters associated target type let nuisance parameter hence detection problem transform domain formulated mary hypothesis test parameters modeled random priors data follow mixture distribution informationtheoretic distances pairs distributions derived using methods similar binary case mary hypothesis test 37 optimal decision minimumprobability oferror rule prior probability hypothesis h probability error p e case upper bounded using unionofevents bound 23 probability deciding h j correct hypothesis h h j h j represents probability error binary hypothesis test h h j inequality 38 follows h h 41 39 follows cherno bound 14 cherno distance distributions takes form 0 1 p ratio pdfs hypotheses h h righthand side 39 gives upper bound p e terms cherno distances simple hypotheses sets singletons analysis similar sec 5 applies asymptotic tightness cherno bound p e j case holds qualified sense discussed sec 4 likewise inequalities 38 41 tight hypotheses h far apart specifically conditions iid data one pair hypotheses dominates bounds p e 39 asymptotically n pair one least cherno distance p proposition 71 shows dp satisfies inequality analogous 28 proof proposition given appendix proposition 71 let dp alisilvey distance distributions p likelihood ratio hypotheses j depend random parameters j j respective priors 42 assumptions proposition 71 hold cherno kullbackleibler distances 11 15 using average upper bound 42 cherno distance 39 obtain average approximation p e similar sec 6 also note dicult derive nontrivial lower bounds dp reasons described sec 62 finally asymptotic conditions p e given 21 exponent takes asymptotic form similar 35 distance dp corresponding mixture distributions fig 10b probability error asymptotically determined worstcase pair angles corresponding mass distributions j 8 conclusion developed systematic framework analytically characterize target recognition performance facilitate optimization system parameters probability error usually intractable function system parameters informationtheoretic distances like kullbackleibler cherno distances advantageously used performance measures cases simple analytical expressions obtained distances provide asymptotically tight bounds p e studied qualified nature gap respect asymptotic performance practical target recognition problem reemphasize methodology directly applicable broad class object recognition problems presence nuisance parameters target pose thermodynamic state expressions informationtheoretic distances often unwieldy convexity arguments show average distance upper bound true distance asymptotic arguments provide simple asymptotic approximations due simplicity expressions provide insights problem dwelled upon optimal design target recognition parameters parameters lossy image compression algorithm provides theoretically motivated alternative heuristic design techniques used target recognition literature 8 issue needs explored detail challenging area future research acknowledgement authors thank dr aaron lanterman mark c johnson reading draft paper making valuable suggestions proof proposition 71 derive upper bound 42 two steps first define 44 44 l ij hence first step follows directly 28 45 44 l ji p j expectation righthand side 45 written cl function g shown cx convex hence argument e j 46 convex function l ji implies l ji hence 28 46 upper bounded averaging equality follows repeating steps 46 reverse order l ji replaced l ji equations 45 47 lead 42 r aided automatic target recognition based upon sensory inputs image forming systems ieee transactions image processing automatic target recognition via jumpdiusion algorithms hilbertschmidt lower bounds estimators matrix lie groups atr introduction signal detection estimation boston academic press bounds shape recognition performance data compression issues automatic target recognition measuring distortion elements information theory general class coecients divergence one distribution another optimal quantization signal detection applications alisilvey distance measures design generalized quantizers binary decision systems optimal design transform coders image classification image recognition compressed data performance analysis wavelet tour signal processing modulation theory introduction probability theory applications large deviation techniques decision convex analysis informationtheoretic asymptotics bayes meth ods new york mcgrawhill tr ctr lavanya vasudevan antonio ortega urbashi mitra applicationspecific compression time delay estimation sensor networks proceedings 1st international conference embedded networked sensor systems november 0507 2003 los angeles california usa e shaikin statistical estimation classification commutative covariance structures automation remote control v64 n8 p12641274 august