integrated range comparison dataparallel compilation systems abstracta major difficulty restructuring compilation parallel programming general compare parallel performance range system problem sizes execution time varies system problem size initially fast implementation may become slow system problem size scale paper introduces concept range comparison unlike conventional execution time comparison performance compared particular system problem size range comparison compares performance programs range ensemble problem sizes via scalability performance crossing point analysis novel algorithm developed predict crossing point automatically correctness algorithm proven methodology developed integrate range comparison restructuring compilations dataparallel programming preliminary prototype methodology implemented tested vienna fortran compilation system experimental results demonstrate range comparison feasible effective important asset program evaluation restructuring compilation parallel programming b introduction significant question parallel machines today many decades software applications take advantage hardware parallelism 1 traditionally distributed memory architectures programmed using message passing user responsible explicitly inserting communication statements sequential program development parallel languages vienna fortran 2 fortran 3 high performance fortran hpf 4 improved situation providing highlevel features specification data distributions among others vienna fortran compilation system vfcs 5 fortran compilation system 3 developed support languages automatically generate message passing program however current technology code restructuring systems inherently lacks power fully exploit performance offered distributed memory archi tectures primary motivation parallel processing high performance effectiveness efficiency restructuring compilation current barriers success simple highlevel programming model approach restructuring program seen iterative process parallel program transformed iteration performance current parallel program analyzed predicted iteration based performance result next restructuring transformation selected improving performance current parallel program iterative process terminates certain predefined performance criteria met result explicit user intervention integrating performance analysis restructuring system critical support automatic performance tuning iterative restructuring process development fully compiler integrated performance system scalable parallel machines especially challeng ing scalable environment performance program vary data distribution system size number processors problem size superior program implementation superior range system problem size predicting performance parallel programs integrating performance indices automatically restructuring compiler two major challenges facing researchers field 6 moreover current performance analysis visualization tools targeted messagepassing programming models parallelism interprocessor communication explicit fall short supporting highlevel languages readily integrated restructuring compilers two major functionalities dataparallel restructuring compilers distribution data arrays processors choice appropriate restructuring transformations key question realizing two functionalities predict scaled performances small number data distributions transformations automatically appropriate optimization decisions made order compare relative performance range problem system sizes scalability prediction proposed solution study scalability ability maintain parallel processing gain system problem size increase characterizes scaling property code given machine slow code good scalability may become superior system problem size scale system sizes performance ranking different code changes called crossing points paper introduce concept range comparison concerned determination crossing points based analytical results given section 32 automatic crossing point prediction automatic range comparison studied research iterative algorithm first derived predict scalability crossing point given parallel platform connection iterative algorithm existing static performance estimator p 3 7 discussed preliminary prototype automatic range comparison implemented vienna fortran compilation system vfcs finally two applications tested two different data distributions verify correctness feasibility range comparison approach current experimental results preliminary clearly demonstrate feasibility effectiveness range comparison approach program restructuring paper organized follows vfcs performance estimation tool introduced section 2 concept scalability performance crossing point range comparison presented section 3 iterative algorithm automatic performance prediction described detail experimental results given section 4 illustrate newly proposed algorithm integrated within vfcs order predict crossing point automatically finally section 5 concludes summary compilation system vfcs parallelizing compiler vienna fortran high performance fortran vfcs integrated several tools program analysis transformation among others provides parallelization technique based upon domain decomposition conjunction singleprogrammultipledata spmd programming model model implies processor executing program based different data domain work distribution parallel program determined based underlying data distribution according ownercomputes rule means processor owns datum perform computations make assignment datum nonlocal data referenced processor imply communication optimized several strategies 5 extracting single element messages loop combine vectors communication vectorization removing redundant communication communication fusion aggregating different communication statements communication aggregation analysis described paper targeted towards regular computations stencil computations relies heavily compiletime analysis optimization provided vfcs 21 performance estimator integrated tool vfcs assists users performance tuning regular programs compile time p 3 based single profile run obtain characteristic data branching probabilities statement loop execution counts well known 9 10 11 12 overhead access nonlocal data remote processors distributed memory architectures commonly orders magnitude higher cost accessing local data communication overhead therefore one important metrics choosing appropriate data distribution communication overhead two separate performance parameters number data transfers amount data transferred sake brevity issues static estimation communication overhead discussed section interested readers may refer 7 8 13 information regarding performance parameters p 3 note section 4 define communication time combines p 3 parameters mentioned various machine specific metrics number data transfers number data transfers critical parameter reflects high message startup costs distributed memory architectures commonly overhead communication decreasing hoisted outside nested loop moreover communication inside specific loop body many cases implies loop sequentialized due synchronization processors involved communication p 3 carefully models loop nesting level communication placed array access patterns data dependences distribution control flow compiler communication optimizations eg communication vectorization fusion order determine number data transfers high accuracy communication hoisted outside loop nest assume loosely synchronous communication model 14 implies involved processors communicate simultaneously communication statement number data transfers determined maximum number data transfers across involved processors communication cannot hoisted outside loop nest due data dependence assume sequentializes loop communication placed well data transfers implied communication number data transfers communication given sum data transfers across processors involved communication amount data transferred current generation distributed memory architectures reduces impact message length communication overhead applications transmit small data volumes startup cost predominate communication cost factor however increasing data volumes transmitted message transfer time per byte turn amount data transferred becomes first order performance effect order provide highly accurate estimate amount data transferred given bytes induced parallel program p 3 estimates number nonlocal data elements accessed incorporates machine specific data type sizes purpose p 3 examines loop nesting level communication placed array access patterns data dependences distributions control flow compiler communication optimizations compiler specifies communication pattern source code level target architecture part except data type sizes ignored consequently parameter ports easily large class distributed memory architectures performance range comparison execution time important performance metric optimizing parallel programs comparison bonds specific pair system problem size execution time alone sufficient performance comparison range system problem sizes scalability recognized important property parallel algorithms machines recent years 15 several scalability metrics proposed 16 17 18 however scalability traditionally studied separately independent property recently relation scalability execution time studied concept range comparison introduced 19 20 unlike conventional execution time comparison performance compared particular system problem size range comparison compares performance programs range system problem size via scalability performance crossing point analysis fully understand concept range comparison background scalability crossing point analysis needs introduced 31 isospeed scalability major driving force behind parallel computing solve large problems fast traditionally execution time measure choice fixedsize problems execution time however adequate scalable computing problem size scales system size speed defined work divided time proposed alternative primary metric scalable computing 1 average speed achieved speed divided number processors used average speed quantity ideally would unchanged scaled system size following definition first given 16 isospeed scalability algorithmmachine combination algorithmmachine combination scalable achieved average speed algorithm given machine remain constant increasing number processors provided problem size increased system size large class algorithmmachine combinations amcs average speed maintained increasing problem size necessary problem size increase varies algorithm machine combinations variation provides quantitative measurement scalability let w amount work algorithm p processors employed machine let w 0 amount work algorithm processors employed maintain average speed scalability system size p system size p 0 algorithmmachine combination work w 0 determined isospeed constraint finally let p w time computing w work p processors system equation 2 shows scaled execution time computed scalability three approaches proposed determine scalabilities 16 computing relation problem size speed directly measuring scalability predicting scalability certain predetermined parameters three approaches practically important scalability prediction seems less expensive benefits compiler support parallel execution time p w divided two parts ideal parallel processing time parallel processing overhead 1 define work general debatable scientific applications commonly agreed floating point flop operation count good estimate work sequential execution time delta computing capacity defined time per unit work single processor parallel processing overhead contains load imbalance overhead communication overhead possible parallelism degradations definition scalability see 1 scalability predicted scaled work size w 0 predicted prediction formula given 21 compute achieved average speed 0 parallel processing overhead p 0 proces sors parallel degradation exist ie 0 traceable 0 necessary sufficient condition equation 4 0 achieved processing overhead 0 general function problem size unknowns sides equation using formula 4 scalability prediction straightforward task 32 performance crossing point range comparison theorem 1 gives relation scalability execution time two different algorithm machine combinations analytically proven experimentally confirmed 19 theorem 1 algorithmmachine combinations 1 2 execution time ff delta respec tively initial state initial system problem size combination 1 higher scalability combination 2 scaled system size execution time combination 1 smaller ff multiple execution time combination 2 solving w 0 scaled system size w 0 scaled problem size combination 1 theorem 1 shows amc faster initial state better scalability others remain faster scalable range range comparison becomes difficult initially faster amc smaller scalability system size scales originally faster code lower scalability become slower another code better scalability finding fastslow crossing point critical optimizing performance choosing efficient data distributions program transformations dataparallel environment finding superiorityinferiority crossing point however difficult definition crossing point problem size system size dependent definition 2 gives formal definition crossing point based isospeed scalability 20 scaled crossing point ff 1 algorithmmachine combinations 1 2 execution time ff respectively initial state say scaled system size 0 crossing point combinations 1 2 ratio isospeed scalability combination 1 combination 2 greater ff p 0 let amc 1 execution time scalability phip p 0 scaled problem size w 0 let amc 2 execution time scalability psip p 0 scaled problem size w definition 2 p 0 crossing point amc 1 2 fact equation 2 phip notice since combination 2 smaller execution time initial state p w superiorityinferiority changing execution time gives meaning performance crossing point correctness theorems 2 3 proved 20 theorem 2 algorithmmachine combination 1 larger execution time algorithm machine combination 2 initial state scaled system size p 0 p 0 scaled crossing point combination 1 smaller scaled execution time combination 2 since two different algorithmmachine combinations may different scalabilities performances may cross crossing point p 0 different scaled problem sizes w 0 6 w scaled crossing point different equalsize crossing point performance crosses problem size theorem 3 gives relation scaled crossing point equalsize crossing point theorem 3 algorithmmachine combination 1 larger execution time algorithm machine combination 2 initial state p 0 scaled crossing point combination 1 larger execution time combination 2 solving w 0 system size scaled problem size combination 1 theorem 3 gives necessary condition equalsize performance crossing initial system size p p 0 equalsize crossing point p must scaled crossing point p hand p 0 scaled crossing point p equalsize crossing point p performance crossing occur scaled crossing point even terms equalsize performance theorem 3 provides mean range comparison based theoretical findings figure 1 gives procedure range comparison terms scalability 33 automatic crossingpoint prediction procedure range comparison listed figure 1 terms scalability scalabilities different code implementations different algorithmmachine combinations general still need determined range comparison scalabilities different algorithmic implementations prestored performance comparison many situations however premeasured results scaled systems available predictions necessary propose iterative method listed figure 2 compute w 0 predict scalability automatically assume underlying application scalable work w monotonically increasing function scaling parameter input data size also assume parallel overhead either independent parameter n ideally scalable monotonically increasing n parallel degradation exists iterative algorithm consists three parts main program two subroutines computing function oew inverse oew function oew implied equation 4 mathematically iterative algorithm find fixed point oew proof correctness algorithm provided appendix correctness proof give convergence rate iteration algorithm like iterative methods convergence rate algorithm application dependent depends properties function fn scientific computations fn low degree polynomial function algorithm converges fast experimental results show algorithm requires three five iterations converge solution error bound assumption algorithm assume algorithmmachine combinations 1 2 execution time fft respectively initial state ff 1 objective algorithm predict combination 2 superior range system sizes p p 0 range comparison begin determine scalability combination 1 determine scalability combination 2 combination 2 superior range else 0 scaled crossing point endfifg endfrange comparison g figure 1 range comparison via performance crossing point 4 automatic performance comparison vfcs implemented prototype version iterative algorithm within vfcs predicting scalability execution time parallelized code functionalities p 3 vfcs fully implemented described section 2 figure 3 shows structure scalability prediction within vfcs input program parallelized instrumented vfcs message passing code generated code compiled executed target parallel machine performance analysis tool analyzes tracefile obtained computes initial performance indices used scalability prediction finally scalability prediction implements iterative algorithm described section 3 iteration algorithm problem size specified source code automatically parallelized performance indices number transfers z amount data transferred estimated p 3 scalability prediction performed process iterates algorithm converges experimental results show approach provides effective solution capturing scaling properties parallel code supports optimizing dataparallel programs two cases presented detail section illustrate iterative algorithm used within vfcs environment prediction carried automatically experiments carried ipsc860 hypercube processors parallel processing overhead used scalability iteration algorithm described section 3 contains communication overhead load imbalance choose two codes jacobi redblack contain several 2 dimensional arrays imply good load balance contains communication time obtained formula assumption algorithm assume work w overhead increasing functions scaling parameter n constant assume parallel code study executed target machine w work p processors objective algorithm compute scalability system size p p 0 error ffl 0 iterative method begin initial value w compute begin iteration k0 k else begin iteration k0 k endfifg endfiterative methodg begin solve compute compute endfsubroutine oew g begin compute solve figure 2 iterative method predicting scalability ttracefile compute compute computew k yes compute vfcs parallelize z mp code instrum execution wt zdt k1 compute performance indices scalability prediction source code figure 3 scalability prediction within vfcs z predicted compile time problem size w using p 3 machine specific parameters ae fi startup time transfer time per message byte respectively represents additional overhead network hop h number hops jacobi redblack parallelized vfcs performance measured 4 processors ipsc860 hypercube performance indices obtained needed computing initial state scalability prediction given work w total execution time p processors p computation time c communication overhead execution models jacobi redblack based equation 3 follows assume computations jacobi redblack uniformly distributed across processors computing rate w average speed determined measured computation time total execution time initial value prediction algorithm p computed based work w starting iteration new input data size n obtained k 0 communication overhead 0 scaled predicted using 6 4 respectively scalability processors p processors determined terminating condition satisfied fixed ffl 0 used experiments otherwise method iterates new parameter tables show measured predicted scalability jacobi algorithm two different data distribution strategies twodimensional block distribution columnwise distribution program arrays twodimensional onedimensional processors array respectively difference percentage predicted measured values given third column tables meas pred meas diff pred meas diff p8 1000 1000 0 0842 0819 27 table 1 jacobi twodimensional distribution predicted measured scalability meas pred meas diff pred meas diff p8 1000 1000 0 0796 0808 15 table 2 jacobi column distribution predicted measured scalability experimental results confirm predicted scalabilities accurate variations scaled performance various data distributions also captured table 3 shows predicted measured scalability values redblack algorithm twodimensional distribution tables 4 5 6 present predicted execution times versus meas pred meas diff pred meas diff table 3 redblack twodimensional distribution predicted measured scalability measured ones jacobi twodimensional block distribution one dimensional distribution redblack twodimensional block distribution respectively initial problem size used tables 1 6 determined asymptotic speed 22 best performance chosen measured average execution time required single iteration covering parallelization p 3 scalability prediction fig 3 redblack parallelization time accounts 07 secs p 3 03 secs scalability prediction 01 secs overall every iteration took approximately 11 secs remains constant changing problem meas diff table 4 jacobi 2d predicted measured execution times meas diff table 5 jacobi c predicted measured execution times size execution time redblack written fft 4 2975 according tables 1 2 3 scalability jacobi higher redblack therefore theorem 1 smaller initial execution time larger scalability shows jacobi scales better redblack confirmed measured results given tables 4 5 6 interesting result given two different jacobi versions tables 1 2 see 2d distribution implementation larger initial execution time better scalability columnwise distribution according theorem 2 crossing point scaled system size p 0 however case crossing point greater 16 cannot confirmed prototype implementation figure 4 shows crossing point range 4 16 processors pointed 21 scaled performance sensitive small applications increasing system size cause noticeable change communicationcomputation ratio jacobi communicationcomputation ratio increases decrease problem size initial state 20 execution time jacobi columnwise distribution strategy given 4 jacobi 2d distribution fft 4 redblack 2d meas diff table 6 redblack 2d predicted measured execution times execution time 2d distribution column distribution figure 4 equalsize crossing point jacobi starting point 1267 considering scalability results tables 7 8 see 2d distribution scales better columnwise distribution ratio two predicted scalabilities 0652 0373 1747 greater ff therefore definition 2 crossing point execution time 2d distribution becomes less columnwise distribution crossing point due communication behavior involved ipsc860 confirmed measured execution times shown figure 5a p4 1000 0652 0548 table 7 predicted scalability jacobi 2d distribution p4 1000 0373 0333 p8 1000 0893 table 8 predicted scalability jacobi columnwise distribution order verify whether crossing point equalsize crossing point measured codes accordance theorem 3 corresponds equalsize crossing point results shown figure 5b execution time 2d distribution column distribution processors60001000014000 execution time 2d distribution column distribution b figure 5 scaled crossing point equalsize crossing point b jacobi n20 5 conclusion many ways parallelize program relative performance gain different parallelizations strategies varies problem size system size comparing performance different implementations algorithm range system problem sizes crucial developing effective parallelizing compilers ultimately reducing burden parallel programming study practical methodology developed automatic range comparison tested dataparallel compilation system proposed methodology built rigorous analytical models correct efficient experimental results confirm effectiveness part parallelizing compiler paper offers several contribution first identify importance feasibility range comparison dataparallel compilation systems next iterative algorithm developed experimentally predict scalability algorithmmachine combinations enable automatic range comparison existing static estimator modified integrate automatic range comparison dataparallel compilation systems finally range comparison approach tested part vienna fortran compilation system experimental results demonstrate feasibility high potential range comparison parallelizing compiler concept analytical results given section 31 32 general applicable algorithmmachine combinations scalability prediction algorithm given section 33 assumes workload deterministic function scaling factor n assumption quite reasonable algorithm requires estimation parallel processing overhead algorithm tested p 3 static performance estimator vienna fortran compilation system due availability vfcs p 3 experimental results presented paper limited 16 node ipsc860 available university vienna integrated range comparison methodology introduced research however general adopted large parallel systems well advance compilation systems 23 appendix proof correctness appendix gives formal proof correctness iterative algorithm listed figure 2 proof independent n scalability 1 instruction main program done otherwise assumption computation work w parallel processing overhead monotonically increasing functions scaling parameter n therefore oe according equation 4 problem size w 0 scaled problem size maintaining isospeed since f g increasing functions n f gamma1 g gamma1 also since 4 p constants scalability prediction process relation 7 implies w increases increases therefore conclude oe oe gamma1 also increasing functions w definition scalability 1 initial value w satisfies equation 7 ideal scalability scalability equals one reached since parallel processing overhead decrease n inequality w 0 w 0 w done instruction main program otherwise iterative method needs used find w 0 w case 1 main program use iterative method increasing function inductively w also since w 0 less w 0 oew increasing inductively therefore shown inequalities preserved nonnegative integers k holds initial value w 0 passing limits sides 8 yields implies iteration 8 converges true solution w 0 case else instruction main program shown figure 2 time iteration formula used following similar argument given case 1 conclude iteration converge solution stabaility iterative algorithm analyzed b denote purturbed values b delta b p since g f functions polynomial purturbed function b oe close oe long small enough iteration w purturbation magnitude derivative oe small derivative f away 0 ie k dfx therefore conclude iteration w small enough definition f k dfx easily satisfied similar arguments iteration w small enough requirement k dgx c always satisfied satisfied definition function g parallel overhead increase much original overhead close w case inverse iteration w used algorithm acknowledgment authors grateful mr yu zhuang help strengthening proof correctness iterative algorithm anonymous referees constructive comments revision paper r next 10 000 2 years programming vienna fortran fortran language specification high performance fortran language specification version 10 vienna fortran compilation system version 20 users guide integrated compilation performance analysis environment data parallel programs automatic performance prediction parallel programs estimating optimizing performance parallel programs buffersafe communication optimization based data flow analysis performance prediction unified framework optimizing communication dataparallel programs communication placement framework unified dependence dataflow analysis tau static parameter based performance prediction tool parallel programs solving problems concurrent processors development parallel methods 1024 processor hypercube scalability parallel algorithmmachine combinations introduction parallel computing performance metrics keeping focus runtime relation scalability execution time performance range comparison via crossing point analysis performance prediction case study using scalable sharedvirtual memory machine performance considerations shared virtual memory machines communication overhead prediction influence scalability tr ctr xianhe sun scalability versus execution time scalable systems journal parallel distributed computing v62 n2 p173192 february 2002 thomas fahringer bernhard scholz xianhe sun executiondriven performance analysis distributed parallel systems proceedings 2nd international workshop software performance p204215 september 2000 ottawa ontario canada