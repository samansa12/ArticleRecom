trust region algorithms timestep selection unconstrained optimization problems closely related systems ordinary differential equations odes gradient structure work prove results apply areas analyze convergence properties trust region levenbergmarquardt algorithm optimization algorithm may also regarded linearized implicit euler method adaptive timestep gradient odes optimization viewpoint algorithm driven directly levenbergmarquardt parameter rather trust region radius approach discussed example r fletcher practical methods optimization 2nd ed john wiley new york 1987 convergence theory developed give rigorous error analysis algorithm establishing global convergence unusual extremely rapid type superlinear convergence precise form superlinear convergence exhibitedthe ratio successive displacements limit point bounded geometrically decreasing sequences also show inexpensive change algorithm leads quadratic convergence ode viewpoint work contributes theory gradient stability presenting algorithm reproduces correct global dynamics gives rapid local convergence stable steady state b introduction work involves ideas two areas numerical anal ysis optimization numerical solution ordinary differential equations odes begin pointing connection underlying mathematical problems given smooth function f r algorithm unconstrained optimization seeks find local minimizer point x fx x neighborhood x following standard result gives necessary conditions sufficient conditions x local minimizer proofs may found example 5 6 7 theorem 11 conditions rfx positive semidefinite necessary x local minimizer whilst conditions rfx positive definite sufficient hand given smooth function f may consider ode system suppose f 11 form fx j gammarf x chain rule solves 11 dt f dt 12 see along solution ode quantity fxt decreases euclidean norm increases moreover strictly decreases unless hence solving ode large value may regarded attempt compute local minimum f conditions given theorem 11 may interpreted necessary conditions sufficient conditions x linearly stable fixed point ode possible write fx form gammarf x ode 11 said gradient structure see example 19 several authors noted department mathematics university strathclyde glasgow g1 1xh uk supported engineering physical sciences research council uk grant grk80228 manuscript appears university strathclyde mathematics research report 3 1998 connection optimization gradient odes schropp 18 examined fixed timestep rungekutta rk methods dynamical systems viewpoint found conditions numerical solution gradient ode converges stationary point f schropp also gave numerical evidence suggest certain problem classes ode formulation preferable optimization analogue book 11 shows many problems expressible optimization terms also written odes often gradient structure chu exploited idea order obtain theoretical results numerical methods particular problems see 3 review optimization literature gradient ode connection also mentioned see example discussion unconstrained optimization 17 related work 1 2 looked use ode methods solve systems nonlinear algebraic equations study numerical methods applied odes gradient form lead concept gradient stability 14 20 21 gradient structure arises many application areas provides useful framework analysis ode algo rithms contrast classical linear strictly contractive test problems gradient systems allow multiple equilibria 14 20 positive results proved ability rk methods preserve gradient structure hence capture correct long term dynamics small fixed timesteps results require extra assumption f imposes either onesided lipschitz condition form dissipativity adaptive rk methods methods vary timestep dynamically analyzed 21 authors considered special class rk formula pairs showed traditional error control approach forces good behavior sufficiently small values error tolerance independently initial data would regarded global convergence proof optimization litera ture results require onesided lipschitz condition f similar result proved 12 general ode methods successfully control local error perunitstep case error tolerance must chosen way depends initial data work presented two main contributions ffl first note close similarity trust region levenberg marquadt algorithm optimization adaptive linearized implicit euler method gradient odes analyze optimization algorithm establish new result convergence properties also adds theory gradient stability odes mild assumption f show method globally convergent enjoys rapid form superlinear convergence notion rate convergence equilibrium widely studied optimization appears considered gradient ode context easily seen fixed timestep rk formula approaches equilibrium generically linear rate terms timestep number ffl second use ideas gradient analysis construct timestepping method general odes gives rapid superlinear local convergence stable fixed point presentation organized follows next section introduce new tons method simple numerical ode methods section 3 concerned specific trust region algorithm unconstrained optimization algorithm essentially one found 6 defined x31 nonrigorous discussion convergence properties given x32 main convergence theorems proved x33 algorithm may also regarded timestepping process gradient ode algorithm analogous results stated x4 x5 develop timestepping scheme general odes gives superlinear local convergence stable fixed points 2 numerical methods numerical methods finding local minimizer f begin initial guess x 0 generate sequence fx k g similarly onestep methods ode 11 produce sequence fx k g x k xt k timelevels ft k g determined dynamically means timestep deltat k steepest descent method optimization form ff k scalar may arise example line search equivalent explicit euler method applied corresponding gradient ode timestep deltat k j ff k note passing poor performance steepest descent presence steepsided narrow valleys analogous poor performance eulers method stiff problems indeed figure 4j 7 figure 12 10 illustrate essentially behavior viewed two different perspectives newtons method optimization based local quadratic model note q k ffi quadratic approximation fx k ffi arises taylor series expansion x k r 2 fx k positive definite q k ffi unique minimizer thus arrive newtons method following result concerning local quadratic convergence newtons method may found example 5 6 7 theorem 21 suppose f 2 c 2 r 2 f satisfies lipschitz condition neighborhood local minimizer x x 0 sufficiently close x positive definite newtons method well defined k converges second order implicit euler method applied 11 fx j gammarf x using timestep deltat k produces equation generally nonlinear equation must solved x k1 applying one interation newtons method newtons method solving nonlinear equations initial guess x method sometimes referred linearized implicit euler method see example 22 note large values deltat k ode method looks like newtons method 23 hand small deltat k corresponds small step direction steepest descent 21 hence extremes large small deltat k ode method behaves like wellknown optimization methods however show much value deltat k method 25 identified trust region process optimization connection pointed goldfarb discussion unconstrained optimization 17 relevant optimization theory developed next section 3 trust region algorithm 31 algorithm seen newtons method based idea minimizing local quadratic model q k ffi 22 step since model valid locally makes sense restrict increment seek increment ffi minimizes q k ffi subject constraint kffik h k h k parameter reflects much trust prepared place model throughout work use k delta k denote euclidean vector norm corresponding induced matrix norm case solution locally constrained quadratic model problem characterized following lemma one half 6 theorem 521 weaker version proved 8 completeness give proof lemma 31 given g 2 r mthetam g 2 r 0 gammag g positive semidefinite b ffi solution min subject kffik k b ffik furthermore g positive definite b ffi unique solution 32 proof case g positive semidefinite straightforward show b ffi minimizes hence ffi b solves problem 32 g positive definite inequality strict ffi 6 b ffi hence solution unique note lemma 31 show compute increment b ffi given trust region constraint kffik h k increment may computed approximated using iterative technique see example 6 pages 103107 5 pages 131143 however mentioned 6 reasonable regard 31 parameter drives algorithmhaving chosen value checked gi positive definite may solve linear system 31 posteriori obtain trust region radius h k k b ffik easily shown g positive definite increasing 31 decreases k b ffik remarks motivate algorithm 32 use min denote smallest eigenvalue symmetric matrix let ffl 0 small constant given x 0 0 0 general step trust region algorithm proceeds follows algorithm 32 compute solve compute compute compute using 33 else set r r k 0 set x else set x algorithm involves function note r k records ratio reduction f x k x reduction predicted local quadratic model r k significantly less 1 model overoptimistic information used 33 update trust region parameter case local quadratic model performed poorly double parameter corresponds reducing trust region radius next step performance reasonable retain value case good performance halve value thereby indirectly increasing trust region radius emphasize algorithm 32 trust region algorithm sense step ffi k solves local restricted problem min subject kffik kffi k k also remark algorithm essentially described 6 pages 102103 underlying idea adding multiple identity matrix ensure positive definiteness first applied case f sumofsquares form leading levenbergmarquadt algorithm goldfeld et al 8 extended approach general objective function gave theoretical justification theorems 511 512 6 provide general convergence theory wide class trust region methods however results apply immediately algorithm 32 since algorithm directly control radius h k kffi k k rather controls indirectly via adaption k fact see behavior established theorem 512 6 local quadratic convergence hold algorithm 32 aware existing convergence analysis applies directly algorithm 32 except general results form encapsulated dennismore characterization theorem superlinear convergence 4 5 6 strongly consistent approximation hessian theory given 16 references discussed remarks follow theorem 34 32 motivation convergence analysis proofs x33 appendix rather technical hence help orient reader give heuristic discussion key points theorem 33 establishes global convergence proof uses arguments standard optimization literature essentially global convergence follows fact local quadratic model inaccurate algorithm chooses direction close steepest descent perhaps interest rate local convergence suppose x k positive definite suppose k b k r k 34 hence follows constant c 1 note also g k g gamma1 bounded large k given large k let ffi newt k denote correction would arise newtons method applied x k newt expanding 36 using 37 newt letting k x hence 38 newt using 35 find newt constant c 2 since x newt k newton step x k theorem 21 newt constant c 3 triangle inequality gives newt newt inserting 39 310 arrive key inequality constant c 4 first term righthand side 311 distinguishes algorithm newtons method dominates rate convergence proceed convenient consider shifted sequence let b e k e kn fixed n determined 311 k choosing n 2 n c 4 neglecting obe 2 leads addition ignoring obe 2 313 also assume equality holds get equality 314 0 0 see 315 error sequence quadratically convergent however 316 corresponds rapid form superlinear convergence although analysis used several simplifying assumptions main conclusions made rigorous show next subsection type superlinear convergence establish likely good quadratic convergence practice matter discussed proof theorem 34 33 convergence analysis trust region algorithm following theorem shows algorithm 32 satisfies global convergence result structure proof similar 6 theorem 511 theorem 33 suppose algorithm 32 produces infinite sequence x k 2 b ae r g k 6 0 k b bounded f 2 c 2 b accumulation point x 1 satisfies necessary conditions local minimizer theorem 11 proof sequence b must convergent subsequence hence collects indices convergent subsequence convenient distinguish two cases sup case form v r 33 must infinite subsequence whose indices form set b 4 also using boundedness g k g k hence suppose gradient limit exists descent direction normalized ksk 1 since ffi k solves local restricted subproblem 34 q k kffi k ks ks also taylor expansion fx conclude 318 320 321 r contradicts r k 1 hence suppose g 1 gx 1 positive semidefinite direction v pick k b k since solves local restricted subproblem 34 hence follows 318 321 323 r contradicts 4 hence g 1 positive semidefinite case ii form v r 33 must infinite subsequence whose indices form set hence gmax sup x2b gives hence removing earlier indices necessary h k kffi k k min deltaf r k 1 4 follows deltaq k 0 let kffik h set x hence feasible subproblem solved ffi k letting follows 325 q k ffi f also minimizes q 1 ffi kffik h since constraint inactive necessary conditions theorem 11 must satisfied hence g 1 6 0 contradicted case ii suppose g 1 positive semidefinite arguments giving 322323 may applied conclude r follows 33 k 0 since min must g 1 positive semidefinite gives required contradiction note mentioned 6 since algorithm computes nonincreasing sequence f k bounded region b required theorem exist level set bounded theorem 33 assume g k 6 0 k g b algorithm essentially terminates giving x however case cannot conclude r 2 fx k positive semidefinite next theorem quantifies local convergence rate algorithm 32 first part proof based 6 theorem 512 theorem 34 accumulation point x 1 theorem 33 also satisfies sufficient conditions local minimizer theorem 11 main sequence 1 displacement error e k constant c e k 0 k e constants e c ratio e k1 e 2 k unbounded proof first show case 317 proof theorem 33 ruled suppose case arises r k 1 b positive definite matrix g k also positive definite large k b case newton correction ffi newt newt gammag k well defined gives global minimum local quadratic model q k define ff ffkffi newt note since ffi k solves local restricted subproblem 34 ff 1 newt newt newt newt newt hence using f newt newt min 0 lower bound smallest eigenvalue g k large k b follows may conclude 321 r k 1 hence case cannot arise case ii k 1 k 2 since lower bound smallest eigenvalue g k large k b follows 321 k 1 must established k 0 know correction used algorithm looks like newton correction ffi newt k satisfies g k ffi newt gammag k let x newt newt k also let k x k 1 triangle inequality quadratic convergence property newtons method given theorem 21 implies x k sufficiently close x 1 constant 1 expanding term 331 find newt k find using 332 333 331 gives large k 2 e k1 2 constant repeating arguments generated inequalities 329 330 show neighborhood n around x 1 x r k 34 2 hence 334 k 2 x k 2 n main sequnce lies n k k main sequence x large k hence 334 may extended bound 3 4 constants lemma a1 gives 326 obtain lower bound e k1 use triangle inequality form 332 333 5 constants 5 0 6 lemma a1 gives required result list number remarks theorem 34 1 theorem shows algorithm 32 achieve quadratic local convergence rate caused fact k approach zero quickly enough reflected first term righthand side 335 straightforward adaptation proof shows increasing rate k 0 possible make second term righthand side 335 significant quadratic convergence recovered example occurs alter strategy changing k otherwise however explained item 4 would expect change improve performance practice quadratic convergence also discussed item 5 2 power k 2 3 appearing 326 327 chosen partly basis simplicityit clear proofs lemma a1 theorem 34 replaced ak 2 12 course cause constant c change 3 also clear proof result independent precise numerical values appearing algorithm values 14 34 33 replaced ff fi respectively factor 2 33 replaced factor greater unity factor 12 33 replaced 1k k 1 statement theorem remains true powers 2 replaced powers k changes mentioned course alter constants c e c b c 4 theorem 34 shows e k1 e k 0 hence convergence rate superlinear however geometrically decreasing upper lower bounds e k1 e k 328 give us much information asymptotically whilst newtons method gives twice many bits accuracy per step bound 328 corresponds k bits accuracy kth step cases asymptotic regime e k small enough make convergence rate observable small rounding errors significant likely consist small number steps 5 several authors found conditions sufficient necessary sufficient superlinear convergence algorithms optimization rootfinding comprehensive result form dennismore characterization theorem 4 5 theorem 824 6 theorem 623 also section 112 16 analyzes class rootfinding algorithms employ consistent approximations hessian approach may used establish superlinear convergence algorithm 32 however references cover general classes algorithms derive sharp upper lower bounds rate superlinear convergence type given theorem 34 terminology 16 x112 algorithm 32 uses strongly consistent approximation hessian superlinear convergence implied k 0 also follows 16 result 1127 quadratic convergence arises ensure k ckg k k convergence rorder least 1 52 occurs k ckx constant c 4 timestepping gradient systems identify trust region parameter k inverse timestep deltat k linearized implicit euler method 25 identical updating formula algorithm 32 hence algorithm 32 regarded adaptive linearized implicit euler method gradient odes convergence analysis x3 applies completeness rewrite algorithm 32 timestepping algorithm given deltat 0 0 x 0 x init general step algorithm gradient system 11 fx j gammarf x proceeds follows algorithm 41 compute solve compute compute compute using 41 else set r r k 0 set x else set x appropriate analogue 33 function deltat 1 r 3 following result restatement theorems 33 34 context theorem 42 suppose algorithm 41 11 fx j gammarf x produces infinite sequence x bounded f 2 c 2 b accumulation point x 1 satisfies necessary conditions local minimizer theorem 11 accumulation point x 1 also satisfies sufficient conditions local minimizer theorem 11 main sequence 1 displacement error e k constant c e k 0 k e constants e c ratio e k1 e 2 k unbounded addition remarks end x3 following points noted 1 algorithm 41 requires check positive definiteness symmetric unusual requirement timestepping algorithm however point inexpensive numerically stable test performed course cholesky factorization 13 page 225 test min omitted algorithm 41 local convergence rate unaffected global convergence proof breaks 2 rule changing timestep different spirit usual local error control philosophy odes 9 10 expected since aim reaching equilibrium quickly possible odds aim following particular trajectory accurately time timestep control policy algorithm 41 based measurement closeness linearity ode idea generalized next section also note local error control algorithms typically involve usersupplied tolerance parameter understanding smaller choice tolerance produces accurate solution algorithm 41 hand involves fixed parameters 3 12 shown certain assumptions use local error control gradient odes forces numerical solution close equilibrium typically solution remains within equilibrium point tolerance parameter suggests local error control may form alternative positive definiteness test means ensuring global convergence driven solution close equilibrium local error control closeness linearity test could used give superlinear convergence 5 timestepping general stable steady state motivated x4 develop algorithm gives rapid local convergence stable equilibrium general ode let f 0 denote jacobian f define f k fx k k symmetric general ratio l k indicates close f behaving linearly region containing x k x k1 given deltat 0 0 x 0 x init general step algorithm 11 proceeds follows algorithm 51 compute solve compute using 51 deltat else arbitrary since concerned local convergence properties action taken affect analysis theorem bfl z denotes open ball radius fl 0 z flg theorem 52 suppose fx neighborhood x f 0 strictly negative real parts given algorithm 51 fl 0 x 0 2 displacement error e k constant c e k 0 k e constants e c ratio e k1 e 2 k unbounded proof exists b fl 0 f 0 x nonsingular x 2 letting 1 upper bound kf 51 l k follows 55 reducing b fl necessary jl f k small e k exists deltat 0 deltat continuity reducing b fl necessary deltat hence large deltat contraction 56 show x 0 sufficiently close x deltat k increases beyond deltat x k remains bbfl x let deltatdeltat deltat 56 x k 2 bbfl x let b k 2 b k deltat 0 deltat 58 may choose sufficiently small 56 57 since deltat k deltat k b k hence e k 0 k 1 deltat deltat 0 k f deltat k f since f 0 k f bounded 56 gives k e k1 5 constants completes result straightforward show fixed timestep rk linear multistep method produce linear rate convergence equilibrium general theorem 52 see algorithm 51 provides systematic means increasing timestep order achieve rapid form superlinear convergence many applications particularly area computational fluid dynam ics common solve discretized steady partial differential equation introducing artificial time derivative driving solution equilibrium see example 22 clear proof theorem 52 sufficiently large deltat 0 algorithm permits local convergence unstable fixed point regarded consequence fact implicit euler method overstable sense absolute stability region contains infinite strip fz righthalf complex plane see example 15 page 229 another explanation newtons method optimizing f identical newtons method algebraic equations applied see example 5 page 100 hence unless measures taken reason stable fixed points preferred algorithm 41 gradient odes check min helps force numerical solution stable fixed point likely traditional ode error control would also direct solution away unstable fixed points hence idea combining optimization ode ideas forms attractive area future work acknowledgements work benefited conversations number optimizers timesteppers notably roger fletcher david griffiths appendix convergence rate lemma lemma a1 let k k constant c e k 0 k addition r k k e ratio e k1 e 2 k unbounded proof choose first prove result restricted circumstances generalize full result assume 3 induction hypothesis note a7 holds 3 a8 true using a1 using a6 a7 therefore induction a8 true k a7 holds consider shifted sequence b e k e kn fixed n possible choose n 3 a9 a10 result a8 holds shifted sequence k translating result original sequence find relabelling c c2 n 2 n letting b n hence clearly increasing c necessary result also hold finite sequence n hence a2 proved inequality a3 follows dividing e k a1 using a2 a2 sufficiently large k 2 k e k r2 r c clearly reducing c necessary result must hold k reduce necessary 1 a12 letting e e e inequalities a12 a13 give a5 required finally using a2 a5 find e r fast local convergence single multistep methods nonlinear equations solution nonlinear systems equations astable integration tech niques list matrix flows applications practical methods optimization practical optimization maximisation quadratic hillclimbing solving ordinary differential equations solving ordinary differential equations ii optimization dynamical systems analysis dynamics local error control via piecewise continuous residual accuracy stability numerical algorithms numerical methods ordinary differential systems iterative solution nonlinear equations several variables nonlinear optimization using dynamical systems methods solve minimization problems nonlinear dynamics chaos model problems numerical stability theory initial value problems essential stability local error control dynamical systems global asymptotic behaviour iterative implicit schemes tr