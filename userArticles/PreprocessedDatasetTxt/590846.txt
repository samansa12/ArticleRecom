incremental feature selection feature selection problem finding relevant features number features dataset large number patterns huge effective method feature selection help dimensionality reduction incremental probabilistic algorithm designed implemented alternative exhaustive heuristic approaches theoretical analysis given support idea probabilistic algorithm finding optimal nearoptimal subset features experimental results suggest 1 probabilistic algorithm effective obtaining optimalsuboptimal feature subsets 2 incremental version expedites feature selection number patterns large scale without sacrificing quality selected features b introduction feature selection finding useful relevant features describe application domain problem feature selection formally defined selecting minimum set relevant features n original features n probability distribution different classes given values features close possible original distribution given values n features mathematically fn original feature set fm chosen feature subset conditional probability pc j close possible pc j possible classes c f f n value vectors respective feature vectors fm fn 11 dimensionality domain expands number features increases general role feature selection threefold 1 simplifying data descrip tion 2 reducing task data collection 3 improving quality problem solving problem representation three features generally simpler one six features benefits simple representation abundant easier understanding problems better faster problem solving context data collection features means data collected many applications could time consuming costly quality improvement problem solving resulting feature selection illustrated via classical supervised learning task pattern classification problem given training set labeled patterns induce classification model predict class label set previously unseen patterns socalled testing set although features enhances discriminating power representation excessive features would introduce many difficulties induction algorithms 11 first time required induction algorithm often grows dramatically number features rendering algorithm impractical problems large number features second many learning algorithms viewed performing estimation probability class label given set fea tures many features distribution high dimension becomes complex unless exponential amounts data available difficult obtain good estimation training dataset third irrelevant redundant features may confuse learning algorithm obscuring distribution small set truly relevant features addition irrelevant redundant features require exponential increase data storage requirements 1 features much data required effective induction instance binary domain extra irrelevantrelevant features would times patterns describe whole data induction algorithm reduced features also result simpler induction model shorter fewer classification rules n features relevant exhaustive approach finding optimal features would require examining subsets number possible subsets grows exponentially researchers designed different strategies search optimal subsets features branch bound 20 variations 26 many heuristic stochastic methods 5 7 view feature selection algorithms perspective using induction algorithm pointed 8 work feature selection divided filter wrapper models filter model feature selector independent induction algorithm serves filter sieve irrelevant andor redundant features wrapper model feature selector wraps around inductive algorithm relying relevant features determined one problem wrapper model restricted time complexity learning algorithm 12 time complexity dependent number features often wrapper methods prohibitively expensive run intractable large number features recall many cases feature selection performed excessive number features favorite induction algorithm difficulties handling many features different models however suit different applications classifier chosen run application hand may wise choose wrapper model since feature selection classifier induction use bias work considers cases learning classifier becomes cumbersome ineffective due large size dataset largeness defined number features n number patterns p latter makes induction algorithms falter hence large datasets terms p main concern naturally filter model adopted aim provide simple practical method select features large datasets following first review related work feature selection related work problem feature selection long active research topic within statistics pattern recognition 30 6 7 work area dealt linear regression 12 assumptions apply machine learning algorithms 8 researchers 12 8 pointed common assumption monotonicity increasing number features improve performance learning algorithm 1 recently feature selection received considerable attention researchers machine learning knowledge discovery interested improving performance algorithms cleaning data handling large databases feature selection even important since many learning algorithms may falter take long time run data reduced feature selection methods 9 12 8 grouped two categories exhaustive heuristic search optimal set features example almuallim dietterichs focus algorithm 2 starts empty feature set carries exhaustive search finds minimal set features sufficient construct hypothesis consistent given set examples works binary noisefree data time complexity ominn 1 monotonicity assumption valid many induction algorithms used machine learning example dataset 1 corral section 5 reproduced 8 proposed three heuristic algorithms speed searching 2 selecting minimal subset known intractable problem practice often trade optimality solution less time spent searching many heuristic feature selection algorithms relief algorithm 9 assigns relevanceweight feature meant denote relevance feature target concept relief samples patterns randomly training set updates relevance values based difference selected pattern two nearest patterns opposite classes according 9 relief assumes twoclass classification problems help redundant features given features relevant concept including redundant features would select even though fraction necessary concept description preset algorithm 19 another heuristic feature selector uses theory rough sets rank features heuristically assuming noisefree binary domain order consider higher order relations among features liu wen 16 suggest use high order information gains select features since last two algorithms try explore combinations features certain fail problems whose features highly interdependent parity problem combining small number features help finding relevant ones another common understanding learning algorithms builtin feature selection example id3 23 fringe 21 c45 24 results 2 suggest one rely id3 fringe filter irrelevant features detailed survey found 5 latest development feature selection pattern recognition found 7 sum exhaustive search approach infeasible practice heuristic search approach reduce search time significantly fail hard problems eg parity problem cannot remove redundant features probabilistic approach proposed alternative 15 selecting op timalsuboptimal subsets features context large sized databases however would still take considerably long time check subset valid 2 firsthand experience problem probabilistic system dispatched local institute onsite usage evidence showed reducing data size significantly speed selection features see case study section 33 hence incremental probabilistic method designed implemented following describe probabilistic method first incremental one followed empirical study effectiveness algorithms verified end paper provide relevant discussion 2 checking done op p number patterns using hashing method 3 probabilistic feature selection proposed probabilistic approach las vegas algorithm 4 las vegas algorithms make probabilistic choices help guide quickly correct solution one kind las vegas algorithms uses randomness guide search way correct solution guaranteed even unfortunate choices made mentioned earlier heuristic search methods vulnerable datasets high interdependency among features las vegas algorithms free us worrying situations evening time required different situations another similar type algorithms monte carlo algorithms often possible reduce error probability arbitrarily cost slight increase computing time refer page 341 4 work lvf las vegas filter 3 suitable since probabilities generating distinct subsets time performance las vegas algorithm may better heuristic algorithms lvf algorithm input maxtries fl allowed inconsistency rate dataset n features output sets features satisfying inconsistency criterion best i1 maxtries best inconchecks fl else best printcurrentbests end lvf algorithm generates random subset n features every round number features c less current best ie best data features prescribed checked inconsistency criterion inconsistency rate defined later prespecified one fl c best best replaced c respectively new current best printed best inconsistency criterion satisfied equally good current best found printed max tries algorithm used control number loops value max tries defined according applications based experience exper imentation small big max tries affect performance lvf compromise made good fast solutions longer lvf runs better results refer analysis section 32 max tries set 77thetan experimental study 4 following ruleofthumb counterpart lvw wrapper feature selector applying las vegas algorithm 4 tried first constant c alone instead c theta n linked n 77 chosen c features dataset words larger n harder problem feature selection parity5 difficult parity2 eg hence tries needed lvf loops max tries times stops alternative stopping criterion let lvf run forever take full advantage anytime algorithm nature section 6 function randomsetseed returns set features randomly seed changed dynamically different set generated function numoffeaturess returns cardinality set inconchecks returns inconsistency rate data selected features specified printcurrentbests prints subset sophisticated version lvf like since know cardinality better subset smaller c best cardinality current best subset need randomly generate subsets whose cardinalities smaller c best new round selection sample features without replacement 31 measure feature goodness inconsistency criterion inconchecks fl key lvf criterion specifies extent dimensionally reduced data acceptable inconsistency rate data described selected features checked prespecified rate fl smaller fl means dimensionally reduced data acceptable default value fl 0 unless specified inconsistency rate dataset calculated follows 1 two patterns considered inconsistent match class labels 2 inconsistency count number matching patterns minus largest number patterns different class labels example n matching patterns among c 1 patterns belong label 1 c 2 label 2 c 3 label 3 3 largest among three inconsistency count 3 inconsistency rate sum inconsistency counts divided total number patterns p easily shown inconsistency rate 0 datasets n features fn original feature set fm chosen feature subset conditional probability pc exactly equals pc j different possible classes c f f n represent vectors values respective feature vectors fm fn inconsistency criterion conservative way achieving class separability commonly used pattern recognition basic selection criterion 6 limited version first proposed 2 minfeatures bias binary domain instead aiming maximize class separability measure tries maintain original class separability data inconsistency criterion also line informationtheoretic experiments paper tried use large c small large datasets could use one fixed max tries reader may done another version lvf link max tries percentage total search space according desired quality selected features considerations 28 suggest using feature good discrimination provides compact descriptions two classes descriptions maximally distinct geometrically constraint interpreted 17 mean feature takes nearly identical values examples class ii takes different values examples class inconsistency criterion aims retain discriminating power data multiple classes feature selection 32 theoretical analysis analysis shows lvf give good solution optimal solution max tries sufficiently large good pseudo random number generator 22 selecting optimal subset features considered sampling without replacement probability finding optimal subset k1th experiment 1 probability conduct k1 experiments finding optimal subset theta theta 1 n number original features n large max tries 2 n assume one optimum exist l optima many applications k 1th tossing probability finding one optimum l roughly number optima doubled number run times halved referring lvf algorithm notice inconsistency criterion checked c c best thus c best reduced due random search number inconsistency checking also reduced shown section 51 realworld datasets c best one fifth original number features mushroom addition time complexity checking op hence lvf expected run fast equivalently good subsets required last two lines inside forloop lvf algorithm removed lvf made even faster 33 applying lvf huge datasets practical case feature selection particularly useful datasets huge since many learning algorithms may encounter difficulties mentioned earlier feature selection help reduce dimensionality datasets learning algorithms used induce rules hence huge datasets also ultimate test feature selection algorithm lvf opportunity undergo real test huge datasets section 5 empirical study results lvf benchmark datasets reported datasets involved related service industry lvf given local institution 5 need method reduce number features applying machine learning algorithms datasets due huge size datasets datasets confidential access users institution ran lvf independently 5 japansingapore ai centre singapore without modification provided following account one dataset let us call hd1 65000 patterns 59 features hd2 5909 patterns 81 features datasets discrete feature values range 2 13 lvf found 10 35 features relevant describing hd1 hd2 respectively without sacrificing discriminating power hours running lvf sun sparc workstation due long waiting time another experiment 10000 patterns hd1 used took lvf 5 minutes complete run obtained results results summarized table stark difference hours minutes inspired us extend work lvf short findings manifest two points 1 lvf significantly reduced number features 2 reducing number patterns significantly reduced run time second finding leads us incremental feature selection data features patterns selected time hours hours largeness dataset differentiated two types 1 horizontal largeness number features 2 vertical largeness number patterns implementation lvf considered overcoming horizontal largeness applying las vegas algorithm order avoid exhaustive search attack vertical largeness using hash mechanism order speed however practical case shows done overcoming vertical largeness hence following mention largeness mean vertical one p 4 incremental probabilistic feature selection although lvf generate optimalsuboptimal solutions see experimental results datasets huge shown section 32 checking whether dataset consistent still takes time due op complexity natural think incremental version lvf significantly reduce number inconsistency checkings studying lvf algorithm notice reduce data decrease number checkings however features selected reduced data may suitable whole data following algorithm designed achieve features selected reduced data generate inconsistencies whole data furthermore done without sacrificing quality feature subsets measured number features relevance lvi algorithm percentage data used feature selection dataset n features fl allowed inconsistency rate output sets features satisfying inconsistency criterion randomly chosen rest data loop checkinconsubset 1 incondata return subset else removeincondata loop lvi checkincon similar inconcheck lvf addition saves inconsistent patterns 1 incondata experiments designed demonstrate claims made lvi incremental algorithm lvi starts portion data p acceptable inconsistency rate fl usually set 0 prior knowledge minimum value fl obtained applying inconcheckf lvf f set n features lvi splits data 0 1 0 p 1 remaining lvi uses subset features subset 0 found lvf check subset 1 actual inconsistency found 0 fi fl inconsistency rate 1 exceed stops otherwise appends patterns incondata 1 cause additional incon sistency 0 deletes incondata 1 selection process repeats solution found subset found whole set returned solution 5 empirical study error probability plays important role feature selection algorithms ultimately always used metaselection criterion 25 regardless different feature selection algorithms subset lowest estimated error always selected classification tasks error caused wrongly classified pattern number errors divided number total patterns set gives us error rate dataset split two sets training vs testing error rates obtained sets error rate testing set estimates performance classification algorithm order check error rates features selection artificial realworld datasets used study effectiveness lvf lvi datasets either commonly used comparison known relevant features two corral parity55 datasets obtained uci repository 18 artificial ffl corral data designed 8 six binary features irrelevant feature c correlated class label 75 time boolean target concept chose feature c root example datasets feature like c removed accurate tree result ffl monk1 monk2 monk3 datasets taken 27 six features training datasets provided used feature selection monk1 monk3 need three features describe target concepts monk2 requires six training data monk3 contains noise datasets used show relevant features always selected ffl led17 data generated artificially program uci data mining repository generates 24 features among first 7 used display value 0 9 seven segment display system remaining 17 features generated randomly values binary except class takes value 0 9 representable seven segments number patterns generated determined user 20000 patterns generated experiments ffl parity55 target concept parity five bits dataset contains uniformly random irrelevant training set contains 100 patterns randomly selected 1024 pat terns another independent 100 patterns drawn form testing set heuristic feature selectors fail sort problems since individual feature mean anything realworld ffl lungcan lung cancer data describes 3 types pathological lung cancers found uci repository data contains patterns 56 features taking values 03 ffl soybeanl uci machine learning repository found training testing datasets two separate files containing 307 376 patterns respectively contains 35 features describing symptoms 19 different diseases soybean plant ffl vote dataset includes votes us house representatives congresspersons 16 key votes identified congressional quarterly almanac volume xl dataset consists 16 features 300 training patterns 135 test patterns table 1 notations c number distinct classes n number features size dataset size training data size testing data training testing datasets split randomly specified dataset c n lungcan 3 56 mushroom 2 22 8125 7125 1000 ffl mushroom dataset total 8124 patterns 1000 patterns randomly selected testing rest used training data 22 discrete features feature 2 10 values ffl krvskp data chess endgame kingrook versus kingpawn a7 pawn a7 means one square away queening kingrooks side white move data contains 3196 patterns 36 features class value 1 indicates white win means white check black pawn advance vice versa pattern board description chess endgame first 36 features describe board last one classification major measurements datasets summarized table 1 since datasets first group large number patterns choose vote mushroom plus paritymix led17 krvskp form second group datasets show effectiveness lvi relation size datasets small medium large paritymix composed two parity55s side side 20 features total 51 effectiveness lvf artificial datasets evaluation lvf simple since relevant features known however realworld datasets clear relevant features therefore whether selected features relevant determined indirectly one way see effect fea table 2 results 100 runs lvf datasets one example minimum set features dataset n number original features number selected features f frequency vote mushroom 22 4 57 5 43 a4 a5 a12 a22 6 allowing 5 inconsistency four features selected chosen 3 plus a1 ture selection learning algorithm among many choices chose c45 24 nbc 29 experiments 1 c45 decision tree induction algorithm works well datasets reported many searchers 2 employs heuristic find simplest tree structures naive bayesian classifier employs bayes rule assuming features independent approximation bayesian classifiers optimal classifier nbc chosen works different way c45 lvf run 100 times training dataset numbers selected features frequencies reported table 2 condition inconsistency criterion satisfied also reported sample selected features dataset directly used readers experiments artificial datasets relevant features always selected albeit irrelevant ones also chosen sometimes problem like parity55 lvf correctly identifies correct features time plus one irrelevant feature sometimes realworld datasets number features reduced least half less one fifth original table 2 shows features last column necessary order satisfy inconsistency criterion inconsistency rate 0 except monk3 features used c45 nbc test performance improves compared using features tenfold cross validation usually recommended ttest used instead ztest calculation pvalues since need take account small sample effect 10 data sample 10fold cross validation default settings c45 used experiments experiments feature selection features shown last column table 2 used given tables 3 4 average accuracy rates c45 applying feature selection datasets applies nbc instead reporting tree size report table size table 3 10fold cross validation results tree size error rates nbc applying lvf datasets pval stands pvalue ttest means pooled variances zero table size dataset pval pval monk2 360 360 374 374 1 monk3 360 220 363 363 1 lungcan 7228 954 00 5666 6333 6685 vote 980 620 99 99 1 mushroom 2360 740 033 116 0004 cases indicated comparison obvious tables 3 shows results consistent known fact bad features standpoint bayesian decision rules 26 datasets tested using nbc table sizes reduced except monk2 due feature selection error rates significantly changed seven nine datasets two datasets soybeanl mushroom latters error rate increases little absolute percentage soybeanls error rate much worse feature selection training dataset fewer patterns test dataset recall division done data contributor features selected based training data datasets put together run 10fold cross validation verify conjecture another experiment features selected using data sets training testing fifteen instead fourteen features chosen results 10fold cross validation nbc c45 144 70 97 73 respectively thus error rates lower data used feature selection improvement nbcs performance parity55 statistically significant results table 4 suggests performance c45 improves general tree size getting smaller error rate lower artificial datasets experiment shows relevant features c45 better full set features realworld datasets c45 also better selected features indicates lvf selected relevant features datasets particular c45 poorly parity55 table 4 10fold cross validation results tree size error rates c45 applying lvf datasets pval stands pvalue ttest means pooled variances zero tree size dataset pval pval monk1 419 410 0782 13 00 1937 monk2 143 143 1 354 354 1 monk3 190 190 11 11 1 lungcan 183 166 03 567 575 2627 73 152 0001 vote 145 61 0001 53 55 8357 feature selection nevertheless c45 well mushroom 22 features 4 features demonstrates c45 select relevant features datasets though serious deterioration c45s performance seen results soybeanl reason given explaining nbcs poor performance dataset gain feature selection differs nbc c45 difference due way features used induce classifier c45 selective induction algorithm selects best feature test tree branching nbc uses features conditional probabilities determining patterns class since nbc assumes features conditionally independent given class conditional probabilities irrelevant feature given class approximately good discriminant 52 effectiveness lvi set experiments want verify four claims 1 lvi may suitable small datasets 2 lvi run faster lvf large datasets 3 lvi sacrifice quality selected features 4 solution found lvf earlier runs neither later runs earlier runs start less data five datasets second group chosen experiments 1 vote 2 mushroom 3 paritymix 4 krvskp 5 led17 experiments conducted follows dataset starting 10 data 0 feature selection run lvi 10 times recording number features features selection time run subsequently experiments 20 30 90 100 data average time number features computed experiment using 100 data reference calculate pvalues sized 0 low pvalue eg 5 suggests null hypothesis two averages rejected refer figures 1 2 varied pvalues shaded differently summarize findings experiments follows ffl effectiveness lvi becomes obvious data size larger lvi performs well three datasets data size small around hundred vote even time saving best 0 much however saving significant case paritymix clear trend observed due overheads required incremental feature selection since inconsistency checking fast op p sufficiently large time saving apparent may even negative p small lvi suitable large sized datasets ffl another issue number patterns lvi start dataset either many patterns affects lvis per formance patterns used 0 small lvf could select features cannot pass inconsistency check remaining data 1 incondata large worst case first loop 0 becomes whole dataset one case small 0 observed figure 1 vote 10 data used took longer time using 20 50 many patterns used 0 large overheads ie time spent steps inside loop lvi algorithm lvf call plus time lvf may exceed time simply running lvf cases vote mushroom difference times statistically significant 70 used ffl incremental algorithm sacrifice quality feature selection time saving mainly due 1 small 0 usually portion say 10 2 remembering inconsistent patterns lvi avoid checking wrong guesses twice quality measured two dimensions one number features relevance features shown figure 2 statistically significant difference number features selected various sized 0 numbers features lower using 100 data otherwise statistically significant difference according ttest paritymix relevant 5 features always selected plus 1 2 irrelevantredundant ones vote mushroom relevance test done learning algorithm c45 nbc performances deteriorate even improve conclude features relevant experimental results shown tables 3 4 verified sets features two datasets via 10fold cross validations quality selected features warranted reduction simplify data analysis rule induction well data collection future ffl lvi scale time complexity feature selection algorithm described along two dimensions number features n number patterns p approximating max tries lvf 77thetan reduced 2 n time complexity lvf mainly determined p since n relatively small incremental version lvi makes possible start fixed small number patterns eg thousand 0 matter large original dataset experimental results show time saving statistically significant p large ffl data krvskp feature removed 10 data 100 data used indicates side incremental feature selection lvf lvf cannot reduce features based smaller portion data data cannot help reduce features either things equal help extend run time example linking max tries percentage total search space 6 discussion conclusion time performance lvf reported first set data 1 lvf completes run fast seconds elapsed time 2 much compare among small datasets 3 time measurements lvi large datasets indicate time performance lvf lvi algorithms simple implement fast obtain results predefining fl according prior knowledge lvi lvf handle noisy data shown case monk3 deal multiple class values another feature lvf related socalled anytime algorithms 3 algorithms whose quality results improves gradually computational time increases lvf prints possible solution whenever found afterwards lvf reports either better subset equally good ones really nice feature works hard find optimal solution provides near optimal solutions need user wait results end search optimalsuboptimal solutions types search longer lvf runs better solutions produces one salient feature lvi scaling capability without losing quality selected features suggested modification sampling subsets features without replacement selected features constraining subset generation newly found minimum number features allow lvf work faster order verify filter feature selector easily turned wrapper one lvw built prove case 14 favorite induction algorithm available lvf easily transformed lvw experimental results show lvw much slower lvf finding consistent results reported 10 may problem using inconsistency feature selection criterion one feature alone social security number guarantee inconsistency data obviously feature irrelevant rule induction problem solved leaving feature feature selection process prior knowledge take one run lvf locate kind features 7 another run lvf features identify right set features lvf works discrete features since relies inconsistency cal culation one way apply discretization algorithm eg chi2 13 discretize continuous features first one runs lvf possibilities 1 simply treat continuous feature discrete one cases 2 apply lvf discrete features number features large work needed search new criteria addition inconsistency continues lvi lvf find uses well mentioned earlier las vegas algorithms may fast domain specific heuristic methods lvi still play role reference design domain specific heuristic method easy task verify heuristic method especially datasets involved huge lvi helpful case validate feature subsets found heuristic method another feature lvf may produce number equally good solutions one dataset based inconsistency criterion one solution chosen according predictive accuracy learning algorithm choose solution generates best accuracy suggests straightforward extension work ie combined filter wrapper model incremental probabilistic algorithm significant advantage lvi move one step handling large sized datasets necessary addition present repertoire 8 far algorithms automated feature selection mentioned another important practical issue using domain knowledge feature selection domain knowledge experts understanding data help tremendously feature selection instance domain knowledge used verify finding automated feature selection domain knowledge used remove obviously irrelevant redundant features domain knowledge also help designing heuristics expertise available one always start feature selection known first apply automated selection algorithms next acknowledgments authors would like thank hy lee suggestions earlier version paper hl ong pang providing results applying lvf huge datasets japan singapore ai center thanks also go manoranjan dash farhad hussain conducting experiments using 7 recall one run lvf max tries loops 8 lvi lvf available research purposes upon request lvf lvi jian shu implementing nbc used experiments suggestions anonymous referees also significantly helped improve paper r tolerating noisy learning boolean concepts presence many irrelevant features deliberation scheduling problem solving timeconstrained environments fundamentals algorithms feature selection methods classifications pattern recognition statistical approach feature selection evaluation irrelevant feature subset selection problem feature selection problem traditional methods new algorithm wrappers performance enhancement oblivious decision graphs toward optimal feature selection selection relevant features machine learning chi2 feature selection discretization numeric attributes feature selection classification probabilistic wrapper approach probabilistic approach feature selection filter solution concept learning feature selection principled constructive induction uci repository machine learning databases feature selection using rough sets theory branch bound algorithm feature subset selection boolean feature discovery empirical learning numerical recipes c induction decision trees inductive pattern classification methods features sen sors automatic feature selection monks problems performance comarison different learning algorithms pattern recognition human mechanical computer systems learn critical evaluation intrinsic dimensionality algorithms tr ctr stergios papadimitriou seferina mavroudi liviu vladutu g pavlides anastasios bezerianos supervised network selforganizing map classification large data sets applied intelligence v16 n3 p185203 mayjune 2002 myungkuk park ki k lee keymok shon wan c yoon automating diagnosis rectification deflection yoke production using hybrid knowledge acquisition casebased reasoning applied intelligence v15 n1 p2540 julyaugust 2001 weichou chen mingchun yang shianshyong tseng bitmapbased feature selection method proceedings acm symposium applied computing march 0912 2003 melbourne florida huilin ye bruce w n lo feature competitive algorithm dimension reduction selforganizing map input space applied intelligence v13 n3 p215230 novemberdecember 2000 weichou chen mingchun yang shianshyong tseng novel feature selection method largescale data sets intelligent data analysis v9 n3 p237251 may 2005 samuel h huang dimensionality reduction automatic knowledge acquisition simple greedy search approach ieee transactions knowledge data engineering v15 n6 p13641373 november ki k lee wan c yoon adaptive classification ellipsoidal regions multidimensional pattern classification problems pattern recognition letters v26 n9 p12321243 1 july 2005 xindong wu shichao zhang synthesizing highfrequency rules different data sources ieee transactions knowledge data engineering v15 n2 p353367 february bill b wang r bob mckay hussein abbass michael barlow comparative study domain ontology guided feature extraction proceedings twentysixth australasian conference computer science research practice information technology p6978 february 01 2003 adelaide australia