efficient instruction sequencing inline target insertion inline target insertion specific compiler pipeline implementation method delayed branches squashing defined method shown offer two important features discovered previous studies first branches inserted branch slots correctly executed second execution returns correctly interrupts exceptions one program counter two features result better performance less softwarehardware complexity conventional delayed branching mechanisms b introduction instruction sequencing mechanism processor determines instructions fetched memory system execution absence branch instructions instruction sequencing mechanism keeps requesting next sequential instructions linear memory space research supported national science foundation nsf grant mip8809478 dr lee hoevel ncr joint services engineering programs jsep contract n0001490j1270 national aeronautics space administration nasa contract nasa nag 1613 cooperation illinois computer laboratory aerospace systems software iclass office naval research contract n0001488k0656 w w hwu department electrical computer engineering university illinois urbana champaign illinois 61801 3 p p chang intel corporation 5200 ne elam young parkway hillsboro 97124 work presented paper conducted department electrical computer engineering university illinois urbanachampaign illinois 61801 sequential mode easy maintain steady supply instructions execution branch instructions however disrupt sequential mode instruction sequencing without special hardware andor software support branches significantly reduce performance pipelined processors breaking steady supply instructions pipeline 26 many hardware methods handling branches pipelined processors studied 39289291710 important class hardware methods called branch target buffers branch target caches use buffering extra logic detect branches early stage pipeline predict branch direction fetch instructions according prediction nullify instructions fetched due incorrect prediction28 branch target buffers adopted many commercial processors 2816 performance hardware methods determined ability detect branches early predict branch directions accu rately high branch prediction accuracy 8590 hit ratio reported hardware methods392829 another advantage using branch target buffers require recompilation binary translation existing code however hardware methods suffer disadvantage requiring large amount fast hardware effective2820 effectiveness also sensitive frequency context switching 28 compilerassisted methods also proposed handle branches pipelined processors table lists three methods delayed branching popular method absorb branch delay microsequencers microprogrammed microengines technique also adopted many recent processor architectures including ibm 80137 stanford mips14 berkeley risc 33 hp spectrum 3 sun sparc 43 mips r2000 25 motorola 8800030 amd 290001 approach instruction slots immediately branch reserved delay slots branch number delay slots large enough cover delay evaluating branch direction compiletime delay slots following branch filled instructions independent branch direction data control dependencies allow code movement13 regardless branch direction instructions delay slots always executed mcfarling hennessy reported first delay slot successfully filled compiler approximately 70 branches second delay slot filled 25 time29 clear delayed branching effective processors requiring one slot another compilerassisted method called delayed branches squashing adopted recent processors complement delayed branching291583023 method used compiler cannot completely fill delay slots delayed branching scheme number slots branch still large enough cover branch delay however instead moving independent instructions branch delay slots compiler fill slots predicted successors branch actual branch direction differs prediction instructions branch slots scratched squashed nullified pipeline least expensive side hardware predicts conditional branches either always taken stanford mipsx 8 always nottaken motorola 88000 30 predicting instructions taken achieves 65 accuracy whereas predicting nottaken 11 predicting branches either taken taken limits performance delayed branches squashing furthermore filling branch slots predictedtaken branches requires code copying general predicting branches taken result large amount code expansion mcfarling hennessy proposed profiled delayed branches squashing scheme execution profiler used collect dynamic execution behavior programs preferred direction branch29 profile information used compiletime code restructurer predict branch direction fill branch slots according prediction order allow branch predicted differently additional bit indicate predicted direction required branch opcode general23 bit compiler convey prediction decision hardware mcfarling hennessy also suggested methods avoiding adding prediction bit branch opcode using pipelines one two branch slots mcfarling hennessy showed method offer comparable performance hardware methods much lower hardware cost suggested stability using execution profile information compiletime code restructuring evaluated paper examines extension mcfarling hennessys idea processors employing deep pipelining multiple instruction issue techniques increase number slots branch result four issues arise first 3 5 instructions branches static program see section 42 order fill large number slots order ten one must able insert branches branch slots questions arise regarding correct execution branches branch slots second state information branch instructions instruction pipeline becomes large brute force implementations return interrupts exceptions involve savingrestoring large amount state information instruction sequencing mechanism third code expansion due code restructuring large important control code expansion without sacrificing performance fourth time penalty refilling instruction fetch pipeline due incorrectly predicted branch large important show extensive empirical results performance stability using profile information compiletime code restructuring first three issues addressed mcfarling hennessy 29 second issue addressed previous studies hardware support precise interrupt 18 40 order address issues specified compiler pipeline implementation method delayed branches squashing refer method inline target insertion reflect fact compiler restructures code inserting predicted successors branches sequential locations based specification show method exhibits desirable properties simple compiler hardware implementation clean inter ruptexception return moderate code expansion high instruction sequencing efficiency also provide proof inline target insertion correct correctness proof filling branch slots branch instructions also applicable previously proposed hardware scheme 34 paper organized five sections section 2 presents background motivation inline target insertion section 3 defines compiler pipeline implementation proves correctness proposed implementation suggests clean method return interrupt exception section 4 provides empirical results code expansion control instruction sequencing efficiency section 5 offers concluding remarks regarding costeffectiveness applicability inline target insertion background motivation 21 branch instructions branch instructions reflect decisions made program algorithm figure 1a shows c program segment finds largest element array two major decisions algorithm one decides elements visited decides current element larger ones visited far register allocationassignment assumption figure 1b machine language program generated given figure 2 three branches machine language program instruction ensures looping condition checked first iteration instruction checks loop iterate instruction f determines current array element larger others visited far simplified view machine language program figure 2 highlights effect branches arc corresponds branch head arc target instruction percentage arc indicates probability corresponding branch occur execution percentages derived program analysis andor execution profiling percentage arc greater 50 corresponds likely branch otherwise corresponds unlikely branch instructions shown figure 2a static instructions instructions generated compilers machine language programmers program execution static instruction executed multiple times due loops time static instruction executed generates dynamic instruction dynamic branch instruction redirects instruction fetch called taken branch 22 instruction sequencing pipelined processors problems instruction sequencing pipelined processors due latency decoding andor executing branches simple hardware example suffices illustrate problem instruction sequencing pipelined processors processor shown figure 3 divided four stages instruction fetch instruction decode id instruction execution ex result writeback wb instruction sequencing logic implemented ex stage sequencing pipeline consists id ex stages processor pipeline compareandbranch 4 instruction processed ex stage 5 instruction sequencing logic determines next instruction fetch memory system based comparison result dynamic pipeline behavior illustrated timing diagram figure 4 vertical dimension gives clock cycles horizontal dimension pipeline stages cycle timing diagram indicates pipeline stage instruction found pipeline fetches instructions sequentially memory branch encountered figure 4 instructions executed direction branch known cycle 7 time instructions j k already entered pipeline therefore cycle 8 instruction e enters pipeline j k scratched nonproductive cycles introduced incorrectly fetching j k reduce throughput pipeline 23 deep pipelining multiple instruction issue rate instruction execution equal clock frequency times number instructions executed per clock cycle one way improve instruction execution rate increase clock frequency pipeline stages longest delay critical paths limit clock frequency therefore subdividing stages potentially increase clock frequency improve overall performance adds stages pipeline creates deeper pipeline example instruction cache access instruction execution limit clock frequency subdividing stages may improve clock frequency timing diagram resultant pipeline shown figure 5 four instructions scratched compareandbranch redirects instruction fetch example 2 may scratched 1 redirects instruction fetch another method improve instruction execution rate increase number instructions executed per cycle done fetching decoding executing multiple instructions per cycle often referred multiple instruction issue 44 12 27 31 32 19 35 36 timing diagram pipeline shown figure 6 example two 4 although compareandbranch instructions assumed example methods paper apply condition code branches well 5 although unconditional branch instructions redirect instruction fetch id stage ignore optimization example simplicity instructions fetched per cycle compareandbranch 1 reaches ex stage five instructions may scratched pipeline 6 far instruction sequencing concerned multiple instruction issue effect deep pipeling result increased number instructions may scratched branch redirects instruction fetch 7 combining deep pipelining multiple instruction issue increase number instructions scratched relatively large number example tandem cyclone processor requires 14 branch slots due deep pipeline multiple instruction issue16 8 discussions paper distinguish deep pipelining multiple instruction issue based number instructions scratched branches 3 inline target insertion inline target insertion consists compiletime code restructuring algorithm runtime pipelined instruction fetch algorithm compiletime code restructuring algorithm transforms sequential program p parallel program p p inline target insertion correct instruction sequence generated executing p p pipelined instruction fetch unit identical generated executing p sequential instruction fetch unit section first formally define sequential instruction fetch algorithm formally define code restructuring algorithm pipelined instruction fetch algorithm inline target insertion formal models implementation derive proof correctness 31 sequential instruction fetch sequential instruction fetch unit defined dynamic instruction cycle address referred target instruction branch instruction referred targeti next sequential instruction branch instruction 6 number instructions scratched pipeline depends instruction alignment i2 rather i1 branch four instructions would scratched 7 difference multiple instruction issue deep pipelining multiple likely control transfer instructions could issued one cycle handling multiple likely control transfer instructions per cycle multiple instruction issue processor difficult inline target insertion details within scope paper 8 processor currently employs extension instruction cache approximates effect branch target buffer cope branch problem referred fallthrui sequential instruction fetch algorithm sif shown algorithm sif begin taken branch else correct successors dynamic instruction defined dynamic instructions executed specified sif k th correct successors denoted csi k noted csi k sequential program p whose execution starts instruction 0 instruction sequence 0 n first terminating instruction 32 compiler implementation compiler implementation inline target insertion involves compiletime branch prediction code restructuring branch prediction marks static branch either likely unlikely prediction based estimated probability branch redirect instruction fetch run time probability derived program analysis andor execution profiling prediction encoded branch instructions predicted successors ps instruction instructions tend execute definition predicted successors complicated frequent occurrence branches refer k th predicted successor predicted successors instruction defined recursively 9 discussions address arithmetics terms instruction words example address address1 advances address next instruction 1 likely branch psi 1 targeti otherwise psi 1 fallthrui 2 1 example one identify first five predicted successors f figure 2 shown since f likely branch first predicted successor target instruction h second predicted successor f likely branch thus third predicted successor f target instruction e code restructing algorithm inline target insertion shown also illustrated figure 7 algorithm itin begin 1 open n insertion slots every likely branch 10 2 likely branch adjust target label address psi 1 address psi 3 likely branch copy first n predicted successors psi 1psi 2 slots 11 inserted instructions branches make sure branch target copying 12 possible extend proofs nonuniform number slots pipeline details scope paper step performed iteratively first iteration first predicted successors likely branches determined inserted subsequent iteration inserts one predicted successor likely branches takes n iterations insert target instructions assigned slots 12 trivial code restructuring works assembly code case branch targets specified labels assembler automatically generates correct branch offset inserted branches goal iti ensure original instructions find predicted successors next sequential locations achieved inserting predicted successors likely branches next sequential locations refer slots opened iti algorithm insertion slots instead traditional terms delay slots squashing delay slots insertion slots associated likely branches instructions insertion slots duplicate copies others original different terms delay slots squashing delay slots usually mean often refer sequential locations likely unlikely branches contain original well duplicate copies figure 8 illustrates application part machine program figure 2 step 1 opens two insertion slots likely branches f step 2 adjusts branch label f branches h branches 2 step 3 copies predicted successors f h e f insertion slots f h 0 0 ie 0 f 0 note offset adjusted 0 f 0 branches target instructions f reader encouraged apply code insights algorithm inline target insertion instruction may duplicated multiple locations fore instruction may fetched one several locations original address dynamic instruction address original copy fetch address f dynamic instruction address fetched figure 8 original address 0 address fetch addresses 0 individual addresses noted moves fallthrui likely branch original address 33 sequencing pipeline implementation sequencing pipeline divided n 1 stages sequencing pipeline processes instructions fetch order instruction delayed due condition sequencing pipeline eg instruction cache miss instructions sequencing pipeline delayed includes instructions ahead one delayed net effect entire sequencing pipeline freezes ensures relative pipeline timing among instructions accurately exposed 1to compiler guarantees branch redirects instruction fetch instructions insertion slots entered sequencing pipeline note restriction applies instructions sequencing pipeline instructions execution pipelines eg data memory access floating point evaluation still proceed instruction sequencing pipeline freezes definition time instruction sequencing separates freeze cycles execution cycles freeze cycles affect relative timing among instructions sequencing pipeline paper cycle refers th cycle program execution excluding freeze cycles ik defined dynamic instruction k th stage sequencing pipeline cycle implementation keeps array fetch addresses instructions sequencing pipeline fetch address instruction stage cycle referred f hardware function ref ill 13 provided reload instruction fetch pipeline original address ref ill called program startup incorrect branch prediction return interruptexception easy guarantee program startup address original address show next subsection appropriate original address program resume incorrect branch prediction interruptexception handling always available ref illpc begin f pipelined instruction fetch algorithm pif implemented hardware shown sequencing pipeline fetches instructions sequentially default branch 13 refill excluded accounting time proving correctness inline target insertion refill may physically implemented loading initial address af i1 subsequently computing af refill included accounting time evaluating performance inline target insertion section 4 redirect instruction fetch andor scratch subsequent instructions reaches end sequencing pipeline branch redirects instruction fetch next fetch address adjusted target address determined algorithm decision branch incorrectly predicted scratches subsequent instructions sequencing pipeline algorithm p n begin branch f else likely taken f else unlikely taken f else unlikely taken else likely taken figure 9a shows timing diagram executing instruction sequence e machine program figure 8a inline target insertion figure 8e instruction sequence becomes case branch decision f predicted correctly compile time f reaches ex stage cycle 4 instruction scratched pipeline since f redirects instruction fetch instruction fetched stage cycle 5 e 0 adjusted target f rather next sequential instruction g figure 9b shows similar timing diagram executing instruction sequence g inline target insertion instruction fetch sequence becomes case branch decision f predicted incorrectly compile time f reaches ex stage cycle 4 instructions h 0 0 scratched pipeline since f redirect instruction fetch instruction fetch pipeline refilled next sequential instruction g 34 correctness implementation branches central issue inline target insertion without branches sequencing pipeline would simply fetch instructions sequentially instructions emerging sequencing pipeline would correct sequence therefore correctness proofs compiler pipeline implementation focus correct execution branches pipelines many slots highly probable branches inserted insertion slots see section 42 case branches insertion slots correctness follows description iti algorithm branch instructions would original would first predicted successors next n sequential locations whereas branch instruction insertion slot cannot n predicted successors next n sequential locations example figure 8e questions arise regarding correct execution f 0 f 0 redirects instruction fetch know resulting instruction sequence always equivalent correct sequence f insertion correct instruction sequence generated n first stop instruction shall prove instruction sequence issued p p p identical sif p unfortunately difficult compare output pif sif step step basis first identify sufficient conditions pif p p generate instruction sequence sif p show conditions guaranteed inline target insertion help reader read following lemmas theorems list important terms table 2 define two equality relations state variables instruction fetch pipeline theorem 1 states two equality relations sufficient ensure correctness inline target insertion theorem 1 rt st true proof theorem proved induction induction basis definition ref ill induction step assuming p true show case 1 incorrectly predicted branch according p in1 implies 1 correctly predicted instruction equal csin 1 hence case 2 unlikely taken pif performs according definition ref ill csin1 1 hence in1 case 3 likely taken pif performs according definition ref ill 1 likely branch allocates n insertion slots taken csin noted likely branch original copy fallthruin always according iti therefore aoin argument refill theorem 1 shows rt st sufficient ensure correct execution therefore formulate next theorem ultimate correctness proof inline target insertion theorem 2 pif ensure rt st true theorem 2 standard induction proof start proving r0 s0 true show rt st true rt1 st1 also true pif complex algorithms need consider several cases step proof instead presenting proof whole first present several lemmas proof theorem 2 naturally follows performed time entry true proof ensures original instructions find n predicted successors next n sequential addresses rt naturally follows definition refill implied definition refill therefore shows refilling instruction fetch pipeline original address ensures rt1 st1 true instruction sequence pipeline initialized 0 entry point program follows lemma 1 r0 s0 true proceed prove rt st true st 1 also true first prove case fetched original address prove case fetched one duplicate addresses lemma 2 rt st true f also true proof fetched original address cannot likely branch need consider following two cases case 1 branch unlikely branch taken pif performs f adding 1 sides st results f allocates insertion slots likely branches likely branch original addresses must adjacent words true case 2 unlikely branch taken pif performs ref correctness st lemma 1 note original therefore legal address refillthe case fetched insertion slot fairly difficult prove first prove intermediate lemma lemma 3 f 1 must k satisfies following four conditions likely branch 3 likely branches inclusively 4 incorrectly predicted branch inclusively proof fetched original address must fetched insertion slot therefore must least one likely branch among n instructions fetched 1 one fetched closest 1 2 3 prove 4 contradiction assume incorrectly predicted branch inclusively ref ill performed original address likely branch inclusively must fetched original address contradiction precondition lemma f therefore assumption incorrectly predicted branch cannot truelemma 4 f st true also true proof use k found lemma 3 case 1 likely branch case p performs f implies 1 pif performs f case 2 1 likely branch pif performed f 2 likely branch 3 likely branch inclusively 1 2 3 f 5 likely branch inclusively 4 5 f could included case 2 proof separate two cases make proof clear lemma 2 lemma 4 collectively ensure si ri true also true proceed show rt 1 also true lemma 5 rt st st 1 true rt 1 also true proof case 1 incorrectly predicted branch case pif performs refill lemma 1 ensures ii refill remains shown argument ref ill original address 1 unlikely branch argument refill original address likely branch argument ref ill f 1 according likely branch iti ensures case 2 incorrectly predicted branch 1 lemma 2 lemma 4 f 2 according original instruction find predicted successors next sequential instructions therefore must psin placed 3 incorrectly predicted branch p performs f 1n f therefore rt implies ii 2 3 rt 1 trueproof theorem 2 induction follows lemma 1 r0 s0 true lemma 2 lemma 4 lemma 5 rt st true rt also true 35 interruptexception return problem interruptexception return arises interrupts exceptions occur instructions insertion slots example assume execution code figure 8e involves instruction sequence branch f correctly predicted taken question h 0 caused page fault much instruction sequencing information must saved process resume properly page fault handled one saved address h 0 information f taken lost since h 0 branch hardware would assume 0 executed h 0 since 0 likely branch taken hardware would incorrectly assume g h resided insertion slots 0 instruction execution sequence would become incorrect problem resuming execution h 0 violated restriction empty sequencing pipeline always starts fetching original instruction hardware information h 0 first branch slot f f taken page occurred interrupts exceptions occur instructions insertion slots branch many likely branches slots problem cannot solved simply remembering branch decision one previous branch popular solution problem save previous n fetch addresses plus fetch address reentry instruction exception return n used reload corresponding instructions restore instruction sequencing state exception disadvantage solution increases number states pipeline control logic therefore slow circuit problem becomes severe pipelines large number slots inline target insertion interruptexception return instruction correctly performed available form f theorem 2 one record original addresses delivering instruction execution units guarantees original address instructions active execution units available therefore interruptexception occurs instruction processor save original address instruction return address lemma 1 ensures rt true ref ill original address figure shows effect exception sequencing pipeline figure 10a shows timing correct instruction sequence figure 8e without exception figure 10b shows timing exception h 0 h 0 reaches end sequencing pipeline ex stage availble form f i1 2 address maintained hardware h 0 finishes execution 16 exception detected saved return address exception return sequencing pipeline resumes instruction fetch h original copy h 0 note instruction sequence produced h equivalent one without exception note original copies must preserved guarantee clean implementation inter ruptexception return figure 8e normal control transfers always enter section opportunity remove e f inline target insertion reduce code size ever would prevent clean interruptexception return one occurs e 0 f 0 section 42 presents alternative approach reducing code expansion 36 extension outoforder execution inline target insertion extended handle instruction sequencing outoforder execution machines 46 47 45 18 19 41 major instruction sequencing problem outoforder execution machines indeterminate timing deriving branching conditions target addresses feasible general design efficient sequencing pipeline branches always conditions target addresses end sequencing pipeline allow efficient outoforder execution sequencing pipeline must allow subsequent instructions proceed whenever possible make inline target insertion correctness proofs applicable outoforder execution machines following changes made pipeline implementation 1 sequencing pipeline designed long enough identify target addresses programcounterrelative branches whose target addresses derived without interlocking 2 branch reaches end sequencing pipeline following conditions may occur real original address calculated exception detected one simply save af i1 calculate aoin exception actually occurs avoids requiring extra subtractor sequencing pipeline branch likely one target address available yet case sequencing pipeline freezes interlock resolved b branch unlikely one target address yet available case sequencing pipeline proceeds subsequent instructions extra hardware must added secure target address becomes available recover incorrect branch prediction execution pipeline must also able cancel effects subsequent instructions emerging sequencing pipeline reason c branch condition yet available case sequencing pipeline proceeds subsequent instructions extra hardware must added secure repair address recover incorrect branch prediction execution pipeline must able cancel effects subsequent instructions emerging sequencing pipeline reason branch program counter relative predicted alternative addresses available end sequencing pipeline difference original sequencing pipline model condition might derived later since hardware secures alternative address sequencing state properly recovered incorrectly predicted branches branch target address derived runtime data target address likely branch may unavailable end sequencing pipeline freezing sequencing pipeline specification ensures theorems hold case unlikely branches target address alternative address sequencing pipeline proceed long alternative address secured becomes available therefore proofs hold outoforder execution machines experimentation code expansion cost instruction sequencing efficiency inline target insertion evaluated empirically section reports experimental results based set production quality software unix 17 cad domains purpose show inline target insertion 17 unix trademark att effective method achieving high instruction sequencing efficiency pipelined processors experiments based instruction set architecture closely resembles mips r2000300025 modifications accommodate inline target insertion impacti c compiler optimizing c compiler developed deep pipelining multiple instruction issue university illinois used generate code experiments 42167 41 benchmark table 3 presents benchmarks chosen experiment c lines column describes size benchmark programs number lines c code counting comments runs column shows number inputs used generate profile databases performance measurement input description column briefly describes nature inputs benchmarks inputs realistic representative typical uses benchmarks example grammars c compiler lisp interpreter two ten realistic inputs bison yacc twenty files several production quality c programs ranging 100 3000 lines inputs cccp program twenty original benchmark inputs form input espresso experimental results reported based mean sample deviation program input combinations shown table 3 use many different real inputs program intended verify stability inline target insertion using profile information impacti compiler automatically applies trace selection placement removed unnecessary unconditional branches via code restructuring 46 42 code expansion problem code expansion frequent occurrence branches programs inserting target instructions branch adds n instructions static program figure 8 target insertion f increases size loop 5 9 instructions general q probability static instructions likely branches among benchmarks inline target insertion potentially increase code size n q 180 one may argue originals inserted instructions may deleted save space flow control allows shown however preserving originals crucial clean return exceptions insertion slots see section 35 large code expansion significantly reduce efficiency hierarchical memory systems problem code expansion must addressed pipelines large number slots table 4 shows static control transfer characteristics benchmarks static cond static uncond column gives percentage conditional unconditional branches among static instructions programs numbers presented table 4 confirm branches appear frequently static programs shows need able insert branches insertion slots see section 34 high percentage branches suggests code expansion must carefully controlled benchmarks simple solution reduce number likely branches static programs using threshold method conditional branch executes fewer number times threshold value automatically converted unlikely branch unconditional branch instruction executes fewer number times threshold value also converted unlikely branch whose branch condition always satisfied method reduces number likely branches cost performance degradation similar idea implemented ibm second generation risc architecture2 example two likely branches b program executed 100 times redirects instruction fetch 95 times b executed 5 times redirects instruction fetch 4 times marking b likely branches achieves correct branch prediction 99 954 times total 105 1005 code size increases 2 n since b executed nearly frequently one mark b unlikely branch case accuracy branch prediction reduced 96 951 times 105 code size increases n therefore large saving code expansion could achieved cost small loss performance idea static likely branches cause amount code expansion execution frequency may vary widely therefore reversing prediction infrequently executed likely branches reduces code expansion cost slight loss prediction accuracy confirmed results shown table 5 threshold column specifies minimum dynamic execution count per run likely branches converted unlikely branches eq column lists mean percentage likely branches among instructions sdq column indicates sample deviations code expansion pipeline n slots n eq example threshold value 100 one expect 22 increase static code size without code expansion control threshold0 static code size increase would 362 sequencing pipeline another example 11stage sequencing pipeline threshold value 100 one expect 11 increase static code size code expansion control threshold0 static code size increase would 181 sequencing pipeline note results based control intensive programs code expansion cost much lower programs simple control structures scientific applications 43 instruction sequencing efficiency problem instruction sequencing efficiency concerned total number dynamic instructions scratched pipeline due dynamic branches since insertion slots inserted predicted successors cost instruction sequencing function n branch prediction accuracy key issue whether accuracy compiletime branch prediction high enough ensure instruction sequencing efficiency remains high large values n evaluating instruction sequencing efficiency inline target insertion straighforward one profile program find frequency dynamic instances branch go one possible directions branch predicted go one direction frequency branch go directions contributes frequency incorrect prediction note correct dynamic instructions reach end sequencing pipeline branches executed therefore frequency executing incorrectly predicted branches affected inline target insertion figure 11a execution frequencies f 100 e f redirect instruction fetch 80 99 times respectively marking f likely branches predict correctly 179 times 200 21 dynamic branches incorrectly predicted since incorrectly predicted dynamic branch creates n nonproductive cycles sequencing pipeline know instruction frequencing cost 21n note number changed inline target insertion figure 11b shows code generated iti2 although know exactly many times f f 0 executed respectively know total execution count 100 also know total number incorrect predictions f f 0 20 therefore instruction sequencing cost figure 11b derived count incorrect prediction figure 11a multiplied n let p denote probability dynamic instruction incorrectly predicted note probability calculated dynamic instructions including branches non branches average instruction sequencing cost estimated following equation relative sequencing cost per instruction peak sequencing rate 1k cycles per instruction actual rate would 1 cycles per instruction 19 table 4 highlights dynamic branch behavior benchmarks dynamic cond dy namic uncond column gives percentage conditional unconditional branches among dynamic instructions measurement dynamic percentages branches confirm branch handling critical performance processors large number branch slots example 20 dynamic instructions bison branches p value program branch prediction miss ratio times 20 assume sequencing pipeline peak sequencing rate one cycle per instruction three slots 3 required prediction accuracy achieve sequencing rate 11 cycles per instruction calculated follows prediction accuracy must least 833 table 6 provides mean sample deviation p spectrum threshholds averaged benchmarks increasing threshhold effectively converts branches unlikely branches 2 relative sequencing cost per instruction 1036 per instruction threshhold equals zero optimization sequencing pipeline whose peak sequencing rate one instruction per cycle means sustained rate 1036 cycles per instruction sequencing pipeline sequences k instructions per cycle translates 1036k 518 formula provides measure efficiency instruction sequencing take external events instruction misses account since external events freeze sequencing pipeline one simply add extra freeze cycles formula derive actual instruction fetch rate cycles per instruction threshhold set 100 relative sequencing cost per instruction 104 10 relative sequencing cost per instruction 118 threshhold equals zero optimization threshhold set 100 sequencing cost per instruction instruction becomes 120 comparing table 5 table 6 obvious converting infrequently executed branches unlikely branches reduces code expansion little cost instruction sequencing efficiency 5 conclusion defined inline target insertion costeffective instruction sequencing method extended work mcfarling hennessy29 compiler pipeline implementation offers two important features first branches freely inserted branch slots instruction sequencing efficiency limited solely accuracy compiletime branch prediction second execution return interruptionexception program one single program counter need reload sequencing pipeline state information two features make inline target insertion superior alternative better performance less softwarehardware complexity conventional delayed branching mechanisms inline target insertion implemented impacti c compiler verify compiler implementation complexity software implementation simple straightforward impacti c compiler used experiments reported paper code expansion control method also proposed included impacti c compiler implementation code expansion instruction sequencing efficiency inline target insertion measured unix cad programs experiments involve execution billion structions size programs variety programs variety inputs program significantly larger used previous experiments overall compiletime branch prediction accuracy 92 benchmarks study pipeline requires 10 branch slots fetches two instructions per cycle translates effective instruction fetch rate 06 cycles per instructionsee section 43 order achieve performance level reported paper instruction format must give compiler complete freedom predict direction static branch easily achieved new instruction set architecture could also incorporated existing architecture upward compatible feature straightforward compare performance inline target insertion branch target buffers pipeline performance determined branch prediction accuracy hwu conte chang20 performed direct comparison inline target insertion branch target buffers based similar set benchmarks conclusion without context switches branch target buffers achieved instruction sequencing efficiency slightly lower inline target insertion context switches could significantly enlarge difference28 branch target buffers advantages binary compatibility existing architectures code expansion inline target insertion advantage requiring extra hardware buffers better performance performance insensitive context switching results paper suggest inline target insertion always superior branch target buffering rather contribution show inline target insertion costeffective alternative branch target buffer performance major concern achieve good performance deep pipelining multiple instruction issue compiler complexity inline target insertion simple enough major concern either proven impacti c compiler implementation cost fast hardware buffers context switching major concerns binary code compatibility code size branch target buffer used otherwise inline target insertion employed better performance characteristics lower hardware cost acknowledgements authors would like thank michael loui guri sohi nancy warter sadun anik thomas conte members impact research group support comments sugges tions also like thank anonymous referees comments extremely helpful improving quality paper research supported national science foundation nsf grant mip8809478 dr lee hoevel ncr joint services engineering programs jsep contract n0001490j1270 national aeronautics space administration nasa contract nasa nag 1613 cooperation illinois computer laboratory aerospace systems software iclass office naval research contract n0001488k0656 r am29000 streamlined instruction processor advance informa tion ibm secondgeneration risc machine organization beyond risc high precision architecture trace selection compiling large c application programs microcode forward semantic compilerassisted instruction fetch method heavily pipelined processors control flow optimization supercomputer scalar pro cessing aggressive code improving techniques based control flow analysis architecture tradeoffs design mipsx evaluation branch architectures branch folding crisp microprocessor reducing branch delay zero characterization processor performance vax11780 percolation code enhance parallel dispatching execution optimizing delayed branches mips vlsi processor architecture design decisions spur multiple instruction issue nonstop cyclone processor highly concurrent scalar processing checkpoint repair high performance outoforder execution machines exploiting concurrency achieve high performance singlechip microar chitecture comparing software hardware schemes reducing cost branches inline function expansion compiling realistic c pro grams efficient instruction sequencing inline target insertion i860tm 64bit microprocessor available instructionlevel parallelism superscalar superpipelined machines mips r2000 risc architecture architecture pipelined computers number operations simultaneously executable fortranlikeprograms resulting speedup branch prediction strategies branch target buffer design reducing cost branches design 88000 risc family measuring parallelism available long instruction word architectures hps new microarchitecture rationale introduction vlsi risc wisq restartable architecture using queues multiple instruction issue singlechip processors performance potential multiple functional unit processors 801 minicomputer espressomv algorithms multiplevalued logic minimization study branch prediction strategies implementation precise interrupts pipelined processors limits multiple instruction issue tradeoffs instruction format design horizontal ar chitectures sparctm architecture manual detection parallel execution independent instruc tions instruction issuing approach enhancing performance multiple functional unit processors efficient algorithm exploiting multiple arithmetic units instruction issue logic pipelined supercomputers tr design decisions spur instruction issuing approach enhancing performance multiple functinal unit processors highly concurrent scalar processing reducing cost branches hps new microarchitecture rationale introduction branch folding crisp microprocessor reducing branch delay zero wisq restartable architecture using queues architectural tradeoffs design mipsx checkpoint repair highperformance outoforder execution machines performance potential multiple functional unit processors trace selection compiling large c application programs microcode multiple instruction issue singlechip processors tradeoffs instruction format design horizontal architectures available instructionlevel parallelism superscalar superpipelined machines limits multiple instruction issue inline function expansion compiling c programs comparing software hardware schemes reducing cost branches forward semantic compilerassisted instruction fetch method heavily pipelined processors control flow optimization supercomputer scalar processing multiple instruction issue nonstop cyclone processor implementation precise interrupts pipelined processors design 88000 risc family optimizing delayed branches 801 minicomputer study branch prediction strategies characterization processor performance vax11780 hpsm ctr apoorv srivastava alvin despain prophetic branches branch architecture code compaction efficient execution proceedings 26th annual international symposium microarchitecture p9499 december 0103 1993 austin texas united states oliver rthing jens knoop bernhard steffen sparse code motion proceedings 27th acm sigplansigact symposium principles programming languages p170183 january 1921 2000 boston usa sofine tahar ramayya kumar practical methodology formal verification risc processors formal methods system design v13 n2 p159225 sept 1998