tracking best disjunction littlestone developed simple deterministic online learning algorithm learning kliteral disjunctions algorithm called sc winnow keeps one weight variables multiplicative updates weights develop randomized version sc winnow prove bounds adaptation algorithm case disjunction may change time case possible target disjunction schedule tgr sequence disjunctions one per trial shift size total number literals addedremoved disjunctions one progresses sequencewe develop algorithm predicts nearly well best disjunction schedule arbitrary sequence examples algorithm allows us track predictions best disjunction hardly complex original version however amortized analysis needed obtaining worstcase mistake bounds requires new techniques cases lower bounds show upper bounds algorithm right constant front leading term mistake bound almost right constant front second leading term computer experiments support theoretical findings b introduction one significant successes computational learning theory community littlestones formalization online model learning development algorithm winnow learning disjunctions littlestone 1989 1988 key feature winnow learning disjunctions constant size number mistakes algorithm grows logarithmically input dimension many standard algorithms perceptron algorithm rosenblatt 1958 number mistakes grow linearly dimension kivinen warmuth auer 1997 meantime number algorithms similar winnow developed also show logarithmic growth loss bounds dimension littlestone warmuth 1994 vovk 1990 cesabianchi et al 1997 haussler kivinen warmuth 1994 extended abstract appeared auer warmuth 1995 k warmuth acknowledges support nsf grant ccr 9700201 paper give refined analysis winnow develop randomized version algorithm give lower bounds show deterministic randomized version close optimal adapt versions used track predictions best disjunction consider following standard online learning model littlestone 1989 1988 vovk 1990 cesabianchi et al 1997 learning proceeds trials trial algorithm presented instance x case ndimensional binary vector used produce binary prediction algorithm receives binary classification instance incurs mistake goal minimize number mistakes algorithm arbitrary sequence examples hx course hopeless scenario deterministic algorithm adversary always choose sequence algorithm makes mistake trial reasonable goal minimize number mistakes algorithm compared minimum number mistakes made concept comparison class 11 nonshifting basic setup paper use monotone 1 kliteral disjunctions comparison class dimension number boolean attributesliterals n disjunctions boolean formulas form x distinct indices j lie ng number classification errors disjunction respect sequence examples simply total number misclassifications disjunction produces sequence goal develop algorithms whose number mistakes much larger number classification errors best disjunction sequence examples paper consider case mistakes best target disjunction caused attribute errors number attribute errors example respect target disjunction u minimum number attributesbits x changed resulting x 0 number attribute errors sequence examples respect target concept simply total number errors examples sequence note target u kliteral monotone disjunction number attribute errors k times number classification errors respect u ie k times number examples x sequence ux 6 winnow tuned function k makes oak lnnk mistakes sequence examples best disjunction incurs attribute errors littlestone 1988 give randomized version winnow give improved tunings original algorithm new algorithm tuned based k expected mistake bound sequence examples monotone kliteral disjunction attribute errors also show original deterministic algorithm tuned number mistakes 2a set sequences lower bounds show bounds close optimal show algorithm expected number mistakes must least upper bound correct constant leading term almost optimal constant second term deterministic algorithms lower bounds show constant leading term optimal lower bounds deterministic randomized case cannot improved significantly essentially matching upper bounds achieved nonefficient algorithms correct factors first second term algorithms use experts cesabianchi et al 1997 expert simply computes value particular kliteral disjunction one weight kept per expert amounts expanding ndimensional boolean inputs boolean inputs using single literals experts littlestone warmuth 1994 vovk 1990 cesabianchi et al 1997 comparison class instead k literal disjunctions expected number mistakes randomized algorithm q bound number classification errors best kliteral disjunction mistake bound deterministic algorithm exactly twice high observe algorithms use n k weights need much time trial calculate prediction update weights thus run time exponential k contrast algorithm uses n weights hand noise upper bounds efficient algorithm measured attribute errors rather classification errors arises since using one weight per attribute recall classification error respect kliteral disjunction equate k attribute errors capture errors affect k attributes efficiently expansion experts seems unavoidable nevertheless surprising version winnow able get right factor number attribute errors randomized version almost right factor square root term sense winnow compresses delta weights n weights point dont combinatorial interpretation weights interpretation found single literal expert case cesabianchi freund helmbold warmuth 1996 littlestone littlestone 1991 use amortized analysis entropic potential function obtain worstcase loss bounds however besides careful tuning bounds take amortized analysis method significant step proving mistake bounds algorithm compared best shifting disjunction 12 shifting disjunctions assume disjunction u specified ndimensional binary vector components value 1 correspond monotone literals disjunction two disjunctions u u 0 hamming distance measures many literals shifted obtain u 0 u disjunction schedule sequence examples length simply sequence disjunctions u shift size schedule zero vector original nonshifting case u equal kliteral disjunction u accordingly definition shift size k trial schedule predicts disjunction u define number attribute errors example sequence hx respect schedule total number attributes changed sequence examples make consistent schedule ie changed instances x 0 note loss bounds nonshifting case written cao delta number bits takes describe disjunction k literals randomized deterministic algorithm surprisingly able prove bounds form shifting disjunction case b number bits takes describe best schedule number attribute errors schedule z shift size schedule takes log 2 z bits describe schedule respect given sequence examples 2 worstcase mistake bounds similar bounds obtained competitive algorithms compare number mistakes algorithm number attribute errors best offline algorithm given whole sequence ahead time offline algorithm still incurs attribute errors bound additional loss online algorithm number attribute errors best schedule opposed coarser method bounding ratio online offline winnow multiplicative updates weights whenever algorithm makes mistake weights literals corresponding bit current input instance one multiplied factor case winnow2 version winnow paper based littlestone 1988 factor either ff 1ff ff 1 parameter algorithm multiplicative weight updates might cause weights algorithm decay rather rapidly since literal might become part disjunction schedule even misleading early part sequence examples algorithm predict well compared best disjunction schedule must able recover weights quickly extension winnow2 simply adds step original algorithm resets weight fin whenever drops boundary similar methods lower bounding weights used algorithm wml littlestone warmuth 1994 designed predicting well best shifting single literal called expert cesabianchi et al 1997 addition generalizing work littlestone warmuth 1994 arbitrary size disjunctions able optimize constant leading term mistake bound winnow develop randomized version algorithm herbster warmuth 1998 work littlestone warmuth 1994 generalized different direction focus predict well best shifting expert well measured terms loss functions discrete loss counting mistakes loss function used paper basic building block simple online algorithm uses multiplicative weight updates vovk 1990 haussler et al 1994 predictions feedback trial realvalued lie interval 0 1 class loss functions includes natural loss functions log loss square loss hellinger loss loss occur large discrete units instead loss trial arbitrarily small thus sophisticated methods needed recovering small weights quickly herbster warmuth 1998 simply lower bounding weights disjunctions important whenever richer class built small unions large number simple basic concepts methods applied simply expand original input many inputs basic concepts since mistake bounds depend logarithmically number basic con cepts even allow exponentially many basic concepts still polynomial mistake bounds method previously used developing noise robust algorithms predicting nearly well best discretized ddimensional axisparallel box maass warmuth 1998 auer 1993 well best pruning decision tree helmbold schapire 1997 cases multiplicative algorithm maintains one weight exponentially many basic concepts however examples multiplicative algorithms exponentially many weights still simulated efficiently example methods paper immediately lead efficient algorithm predicting well best shifting ddimensional box thus combining methods existing al gorithms design efficient learning algorithms provably good worstcase loss bounds general shifting concepts disjunctions besides experiments practical data exemplify merits worstcase mistake bounds research also leaves number theoretical open prob lems winnow algorithm learning arbitrary linear threshold functions methods tracking best disjunction still need generalized learning general class concepts believe techniques developed learning predict well best shifting disjunction useful settings developing algorithms predict nearly well best shifting linear combination discrete loss replaced continuous loss function square loss makes problem challenging 13 related work natural competitor winnow well known perceptron algorithm rosenblatt 1958 learning linear threshold functions algorithm additive instead multiplicative updates classical perceptron convergence theorem gives mistake bound algorithm duda hart 1973 haykin 1994 bound linear number attributes kivinen et al 1997 whereas bounds winnowlike algorithms logarithmic number attributes proof perceptron convergence theorem also seen amortized analysis however potential function needed perceptron algorithm quite different potential function used analysis winnow w weight vector algorithm trial u target weight vector perceptron algorithm 2 potential function jjjj 2 euclidean length vector contrast potential function used analysis winnow littlestone 1988 1989 also used paper following generalization 3 relative entropy cover case linear regression framework developed kivinen warmuth 1997 deriving updates potential function used amortized anal ysis framework adapted derive perceptron algorithm winnow different potential functions algorithms lead additive multiplicative algorithms respectively perceptron algorithm seeking weight vector consistent examples otherwise minimizes euclidean length winnow instead minimizes relative entropy thus rooted minimum relative entropy principle kullback kapur kesavan 1992 jumarie 1990 14 organization paper next section formally define notation use throughout paper already discussed introduction section 3 presents algorithm section 4 gives theoretical results algorithm section 5 consider practical aspects namely parameters algorithm tuned achieve good performance section 6 reports experimental results analysis algorithm proofs section 4 given section 7 lower bounds number mistakes made algorithm shown section 8 conclude section 9 2 notation target schedule sequence disjunctions represented nary bit vectors u size shift disjunction u disjunction u z j total shift size schedule z assume u get precise bounds case shifts target schedule distinguish shifts literal added disjunction shifts literal removed disjunction thus define z number times literal switched number times literal switched sequence examples consists attribute vectors classifications 2 f0 1g prediction disjunction u attribute vector x u number attribute errors trial respect target schedule minimal number attributes changed resulting x 0 u g total number attribute errors sequence respect schedule denote sz n class example sequences n attributes consistent target schedule shift size z attribute errors wish distinguish positive negative shifts denote corresponding class sz numbers literals added removed respectively target schedule 0 k n denote class example sequences n attributes consistent nonshifting target schedule ie attribute errors case upper bounds z z known denote corresponding classes z zz k loss learning algorithm l example sequence number misclassifications binary prediction learning algorithm l trial 3 algorithm present algorithm swin shifting winnow see table 1 extension littlestones winnow2 algorithm littlestone 1991 extension incorporates randomization algorithm guarantees lower bound weights used algorithm algorithm maintains vector n weights n attributes w denote weights end trial table 1 algorithm swin parameters algorithm uses parameters ff initialization set weights initial values trial 1 set r predict ae receive binary classification update 6 pr 1 w 0 2 w w 0 denotes initial value weight vector trial algorithm predicts using weight vector w prediction algorithm depends r 1 algorithm predicts 1 probability pr predicts 0 probability obtain deterministic algorithm one choose function p predicting algorithm receives classification ie weight vector modified since 2 f0 1g pr 2 0 1 occur prediction deterministic ie pr correct update occurs cases prediction wrong pr updates weights performed two steps first step original winnow update second step guarantees weight smaller fi parameter fi similar approach taken littlestone warmuth 1994 observe weights changed probability making mistake nonzero deterministic algorithm means weights changed algorithm made mistake furthermore ith weight modified x 1 weight increased multiplied ff decreased divided ff parameters ff fi w 0 function pdelta set appropriately good choice function pdelta following randomized prediction let rand deterministic version algorithm let det randomized version one choose fi ln ff observe det obtained rand choosing threshold rand corresponds straightforward conversion randomized prediction algorithm deterministic prediction algorithm theoretically good choices parameters ff fi w 0 given next section practical issues tuning parameters discussed section 5 4 results section give rigorous bounds expected number mistakes swin first general specific choices ff fi w 0 pdelta chosen rand det bounds shown close optimal adversarial example sequences details see section 8 theorem 1 randomized version let ff 1 n pdelta rand fi n e bound holds theorem 2 deterministic version let ff 1 n pdelta det fi n e bound holds theorem 3 nonshifting case let ff 1 swin uses function pdelta given rand swin uses function pdelta given det e bounds hold 2 remark usual conversion bound randomized algorithm bound deterministic algorithm would give 2m deterministic bound 4 observe deterministic bound 1 times randomized bound since time disjunction cannot contain n literals z gives following corollary w pdelta rand 2 z n j j pdelta det 2 z n ffn j j first give results number mistakes swin information besides n total number attributes given theorem 4 let n pdelta rand 2 z n n pdelta det 2 n pdelta rand 2 bound holds 2 n n 2 pdelta det 2 0 k n bound holds 2 n n 2 section 8 show bounds optimal constants z known advance parameters algorithm tuned obtain even better results example nonshifting case number k attributes target concept known get theorem 5 let n pdelta rand e bound holds 2 n k n e set e get emswins 144 e 2 pdelta det 2 0 k n e bound holds 2 n k n e set e get mswins 275 e 2 particular interest case dominant term ie ae k ln n theorem 6 let k ln n n pdelta rand 2 0 k n r e bound holds 2 n k n e n e e emswins e 2k pdelta det 2 0 k n r e bound holds 2 n k n e e e mswins 2a e shifting case get dominant ae z ln n theorem 7 let zminfnzg z zsuch ffl 1 n pdelta rand 2 z n r z zminfnzg z n pdelta det 2 z n r z section 8 show theorems 6 7 constants optimal furthermore show randomized algorithm also magnitude second order term theorem 6 optimal 5 practical tuning algorithm section give thoughts parameters ff fi w 0 swin chosen particular target schedules sequences examples recommendations based mistake bounds hold target schedule sequence examples appropriate bounds number shifts attribute errors thus mentioned since many target schedules many example sequences worst case bounds usually overestimate number mistakes made swin therefore parameter settings different recommendations might result smaller number errors specific target schedule example sequence hand swin quite insensitive small changes parameters see section 6 effect changes benign little known target schedule example sequence parameter settings theorems 4 5 advisable since balance well effect target shifts attribute errors good estimates number target shifts number attribute errors known good parameters calculated numerically minimizing bounds theorems 1 2 3 corollary 1 respectively average rate target shifts attribute errors known z r z r r z 0 r 0 large error rate r mswinst corollary 1 approximately upper bounded r randomized predictions r ffn deterministic predictions optimal choices ff fi obtained numerical minimization 6 experimental results experiment reported section meant give rigorous empirical evaluation algorithm swin instead intended illustration typical behavior swin compared theoretical bound also version winnow modified adapt shifts target schedule experiment used attributes target schedule length starts 4 active literals 1000 trials one literals switched another 1000 trials another literal switched switching switching literals continues depicted figure 1 thus initially 4 active literals example sequence hx chosen half examples half values attributes appearing target schedule chosen random x probability 12 examples exactly one active attributes chosen random set number trials number active literals figure 1 shifts target schedule used experiment 1 examples attribute errors relevant attributes either set 1 case set 0 case attribute errors occurred trials 1 trials figure 2 shows performance swin compared theoretical bound parameters set numerically minimizing bound corollary 1 described previous section yielded theoretical bound trial calculated actual number shifts attribute errors trial thus increase bound due shift target schedule attribute error trial figure 2 reasons increases indicated z literal switched z gamma literal switched attribute errors figure 2 shows theoretical bound accurately depicts behavior swin although overestimates actual number mistakes amount seen switching literal causes far less mistakes switching literal predicted bound also relation attribute errors mistakes seen performance swin whole sequence examples shown figure 3 compared performance version winnow modified target shifts seen swin adapts quickly shifts target schedule hand unmodified version winnow makes mistakes shift unmodified version winnow used swin thus weights lower bounded become arbitrarily small causes large number mistakes corresponding literal becomes active used z a4 z number trials number mistakes theoretical bound performance swin figure 2 comparison swin theoretical bound particular target schedule sequence examples shifts attribute errors indicated z number trials number mistakes theoretical bound performance swin performance winnow figure 3 version winnow lower bound weights makes many mistakes swin ff unmodified version set w optimal initial part target schedule therefore unmodified winnow adapts quickly initial part makes increasing number mistakes shift target schedule shift number mistakes made approximately doubles last plot figure 4 compare performance swin tuned parameters performance swin generic parameter setting given theorem 4 although tuned parameters perform better difference number trials number mistakes theoretical bound performance swin figure 4 tuned parameters swin versus generic parameters relatively small overall conclusion experiment first theoretical bounds capture actual performance algorithm quite well second mechanism lower bounding weights winnow necessary make algorithm adaptive target shifts third moderate changes parameters change qualitative behavior algorithm 7 amortized analysis section first prove general bounds given theorems 1 2 randomized deterministic version swin bounds calculate bounds given theorems 47 specific choices parameters analysis algorithm proceeds showing distance weight vector algorithm w vector u representing disjunction trial decreases algorithm makes mistake potentialdistance function used previous analysis winnow littlestone 1988 1989 1991 following generalization relative entropy arbitrary nonnegative weight vectors distance function also used analysis egu regression algorithm kivinen warmuth 1997 shows winnow related algorithm taking derivatives easy see distance minimal equal 0 w convention 0 assumption u 2 f0 1g n distance function simplifies start analysis randomized algorithm shifting target dis junctions cases derived easily analysis first calculate much distance du w changes trials observe term 1 might nonzero trial terms 2 3 nonzero weights updated trial fl ffi lower bound term 1 weights updated trial term 2 bounded third equality holds since x ti 2 f0 1g remember x 0 obtained x removing attribute errors x last inequality follows fact last observe w ti 6 w 0 case w 0 ffn get term 3 summing trials consider trials weights updated distinguish trials denote trials bounds terms 1 2 3 theta r want lower bound sum 0 1 expected total number mistakes algorithm choosing appropriate function pdelta w denote p probability algorithm makes mistake trial expected number mistakes observe since case thus sufficient find function pdelta constant c function pdelta satisfying 4 5 get assuming 2 sz upper bound expected number mistakes hence want choose pdelta c c big possible fix pdelta let r value pr becomes 1 5 since left hand sides equations 4 5 continuous get r combining two ff achieved choosing pdelta rand satisfies 4 5 course choose fi ln ff putting everything together following lemma assume weights used algorithm swin swin uses function pdelta given rand deterministic variant algorithm function pdelta take values f0 1g thus get 4 5 rgammafi1gamma1ff c r1gammaffln ff c yields get choosing pdelta det satisfies 4 5 assume weights used algorithm swin swin uses function pdelta given det going calculate bounds fl ffi 1 get bounds lower upper bounding w ti obviously w ti fi upper bound w ti derived observation w ti w tgamma1i pdelta rand det r w tgamma1i x ti find w ti ff thus ln efi lemma 3 fi weights w ti algorithm swin function pdelta rand det satisfy proof theorems 1 2 lemmas 1 2 3 2 proof theorem 3 nonshifting case u number attributes target disjunction u thus nonshifting case term upper bounds lemmas 1 2 replaced k gives theorem 2 71 proofs specific choices parameters proofs theorems 4 5 theorem 3 corollary 1 simple calcu lations 2 proof theorem 6 get theorem 3 2 second inequality used ffl 1 substituting values c ffl gives statements theorem 2 proof theorem 7 get corollary 1 j j j j 2 ffl 110 j j c c r z gives bounds theorem 2 8 lower bounds start proving lower bound shifting case show learning algorithm l example sequences learning algorithm makes many mistakes although expressed explicitly following theorems show sequences generated target schedules disjunction u consists exactly one literal ie jth unit vector first lower bound shows deterministic algorithm adversarial example sequence sz n makes least 2a many mistakes related upper bounds given theorems 4 7 theorem 8 deterministic learning algorithm l n 2 z 1 0 example sequence 2 sz n proof notational convenience assume r 1 construct example sequence depending predictions learning algorithm learning algorithm makes mistake trial partition trials r rounds first r gamma 1 rounds length last round length errors occur within last trials choose target schedule round target disjunction change equal e j beginning round disjunctions consistent examples round l trials round still 2 gammal consistent disjunctions construct attribute vector setting half attributes correspond consistent disjunctions 1 attributes 0 furthermore set prediction algorithm attribute vector obviously half disjunctions consistent example thus number consistent disjunctions divided 2 trial thus first r gamma 1 rounds disjunction consistent examples round last round two disjunctions consistent examples round remaining 2a1 trials fix attribute vector two disjunctions predict differently set thus one disjunctions disagrees times classifications disagreement seen caused attribute errors disjunction consistent examples last round attribute errors 2 remark observe lower bound deterministic algorithms like implies following lower bound randomized algorithms follows fact randomized learning algorithm turned deterministic learning algorithm makes twice many mistakes randomized algorithm makes average means theorem 8 implies randomized algorithm l sequences 2 sz n remark open problem remains show lower bounds form upper bounds theorem 7 square root term turn nonshifting case already lower bounds known lemma 4 littlestone warmuth 1994 deterministic learning algorithm l n 2 0 example sequence 2 0 1 n slight modification results cesabianchi et al 1997 gives lemma 5 cesabianchi et al 1997 functions nj j j 0 randomized learning algorithm l n nj j example sequence 2 0 1 n ln n extend results obtain following theorems corresponding upper bounds given theorems 5 6 theorem 9 deterministic learning algorithm l k 1 n 2k 0 example sequence 2 0 k n theorem 10 functions nj j j 0 randomized learning algorithm l k 1 n knj kan j example sequence 2 0 k n r proof theorems 9 10 proof reduction case 1 n attributes divided k groups g group consists attributes furthermore choose numbers xi group g choose sequence accordingly lemmas 4 5 respectively learning algorithm sequences extended sequences 0 n attributes setting attributes group 0 concatenating expanded sequences 0 get sequence easy see 2 sk n hand learning algorithm sequences n attributes transformed learning algorithm sequences smaller number attributes setting missing attributes 0 thus subsequence 0 learning algorithm l makes least many mistakes given 6 7 respectively hence r r function j chosen appropriately 2 last theorem shows randomized algorithms constant 1 theorem 6 optimal best constant square root term 1 2 9 conclusion developed algorithm swin variant winnow online learning disjunctions subject target shift proved worst case mistake bounds swin hold sequence examples kind target drift amount error examples amount shifts bounded deterministic randomized version swin analysis randomized version involved interesting right lower bounds show worst case mistake bounds close optimal cases computer experiments highlight explicit mechanism necessary make algorithm adaptive target shifts acknowledgments would like thank mark herbster nick littlestone valuable discussions also thank anonymous referees helpful comments notes 1 expanding dimension 2n learning nonmonotone disjunctions reduces monotone case 2 essentially one describe shift occurs literal shifted obviously necessity shift current disjunction correct current example thus trials current disjunction would make mistake disjunction shifted since target schedule might make mistakes due attribute errors z shifts get z trials candidates shifts choosing z choosing one literal shift gives z possibilities 3 potential function weights must positive negative weights handled via reduction littlestone 1988 1989 haussler et al 1994 4 worst case randomized algorithm makes mistake probability 12 trial deterministic algorithm always breaks tie wrong way makes mistake trial thus number mistakes deterministic algorithm twice expected number mistakes randomized algorithm 5 formally let r sequence hr j exists pdelta equal 1 everywhere value r 1 functions pdelta satisfying conditions algorithm swin forced make unbounded number mistakes even absence attribute errors r tracking best disjunction use expert advice pattern classification scene analysis tight worstcase loss bounds predicting expert advice tech predicting nearly well best pruning decision tree tracking best expert entropy optimization principles applications additive versus exponentiated gradient updates linear prediction perceptron algorithm vs linear vs logarithmic mistake bounds input variables relevant mistake bounds logarithmic linearthreshold learning algorithms redundant noisy attributes learning theory pp weighted majority algorithm information computation efficient learning virtual threshold gates learning theory pp tr ctr mark herbster manfred k warmuth tracking best expert machine learning v32 n2 p151178 aug 1998 p helmbold panizza k warmuth direct indirect algorithms online learning disjunctions theoretical computer science v284 n1 p109142 6 july 2002 chris mesterharm tracking linearthreshold concepts winnow journal machine learning research 4 1212003 mark herbster manfred k warmuth tracking best regressor proceedings eleventh annual conference computational learning theory p2431 july 2426 1998 madison wisconsin united states manfred k warmuth winnowing subspaces proceedings 24th international conference machine learning p9991006 june 2024 2007 corvalis oregon gentile nick littlestone robustness olivier bousquet manfred k warmuth tracking small set experts mixing past posteriors journal machine learning research 3 312003 claudio gentile robustness pnorm algorithms machine learning v53 n3 p265299 december giovanni cavallanti nicol cesabianchi claudio gentile tracking best hyperplane simple budget perceptron machine learning v69 n23 p143167 december 2007 mark herbster manfred k warmuth tracking best linear predictor journal machine learning research 1 p281309 912001 b kotsiantis zaharakis p e pintelas machine learning review classification combining techniques artificial intelligence review v26 n3 p159190 november 2006 peter auer using confidence bounds exploitationexploration tradeoffs journal machine learning research 3 312003