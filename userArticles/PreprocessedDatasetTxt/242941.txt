tridiagonalizing diagonalizing symmetric matrices repeated eigenvalues describe divideandconquer tridiagonalization approach matrices repeated eigenvalues algorithm hinges fact easily constructively verifiable conditions symmetric matrix band width b k distinct eigenvalues must block diagonal diagonal blocks size b k slight modification usual orthogonal bandreduction algorithm allows us reveal structure leads potential parallelism form independent diagonal blocks compared usual householder reduction algorithm new approach exhibits improved data locality significantly scope parallelism potential reduce arithmetic complexity close 50 matrices two numerically distinct eigenvalues actual improvement depends large extent number distinct eigenvalues good estimate thereof however worst algorithms behave like successive bandreduction approach tridiagonalization moreover provide numerically reliable effective algorithm computing eigenvalue decomposition symmetric matrix two numerically distinct eigenvalues matrices arise example invariant subspace decomposition approaches symmetric eigenvalue problem b introduction let n theta n symmetric matrix goal compute orthogonaltridiagonal decomposition aqqt q orthogonal tridiagonal reduction tridiagonal form standard preprocessing step dense eigensolvers based qr iteration bisection cuppens method 16 conventional tridiagonalization procedure 16 p 419 reduces one column time householder transformation cost o4n 3 3 flops reduction work supported advanced research projects agency contract dm28e04120 p95006 mathematical information computational sciences division subprogram office computational technology research us department energy contract w31109eng38 work author partially performed postdoctoral associate argonne national laboratory z prism working notes retrieved via anonymous ftp pubprism directory ftpsuperorg additional o4n 3 3 flops orthogonal matrix accumulated time algorithm employs mainly matrixvector multiplications symmetric rankone updates require memory references matrixmatrix operations 9814 block tridiagonalization algorithm 5 15 combines sets p successive symmetric rank updates one symmetric rankp update cost o2pn 2 extra flops result algorithm exhibits improved data locality hence likely preferable cachebased architectures block algorithm incorporated lapack library portable linear algebra codes highperformance architectures 1 2 parallel versions distributedmemory machines standard algorithm block algorithm described 12 13 respectively different approach tridiagonalization socalled successive band reduction sbr method completes tridiagonal reduction sequence band reductions 107 approach leads algorithms exhibit even greater degree memory locality among desirable features paper show number k say distinct eigenvalues symmetric matrix small considerable scope exists savings tridiagonalization algorithms demonstrated cheaply reduced block diagonal banded form slightly modified sbr approach final tridiagonal form achieved applying algorithm recursively subblocks diagonal compared conventional approach approach following advantages improved data locality tridiagonalization process employ mainly matrixmatrix oper ations reduction update transformation matrix q see also 107 enhanced scope parallelism traditional algorithm scope exploitation parallelism reduction limited application rank1 update unblocked algorithm rankp update blocked algorithm scope parallelism decreases subproblems become smaller contrast algorithm generates independent subproblems reduction worked independently whose number increases iteration proceeds thus shift occurs data parallelism updates large matrices functional parallelism several independent subproblems stage sufficient parallelism exploit reduced complexity depending number distinct eigenvalues may almost halve number floatingpoint operations addition need data movement reduced one particular situation repeated eigenvalues arise context invariantsubspace methods eigenvalue problems 31964 matrix two distinct predetermined eigenvalues generated either repeated application incomplete beta functions 19 matrix sign function 4 exact arithmetic tridiagonalization procedure would result block diagonal matrix diagonal blocks order larger 2 hence eigenvalue decomposition could computed easily independently diagonalizing 2 theta 2 blocks diagonal presence roundoff errors computed tridiagonal matrix may desirable structure however prove tridiagonal matrix diagonalized reliably method two cleanup sweeps sweep solves n2 independent 2 theta 2 eigenvalue problems paper organized follows show section 2 certain conditions verified easiliy banded symmetric matrix bandwidth b k distinct eigenvalues block diagonal diagonal blocks order bk section 3 present reduction algorithm achieve desired banded blockdiagonal structure slight modification conventional band reduction procedure approach employed develop divideand conquer tridiagonalization algorithm inexpensive algorithm decoupling invariant subspaces matrices eigenvalue clusters 0 1 given verified section 4 numerical experiments matlab implementation reported section 5 lastly summarize results 2 structure band matrices repeated eigenvalues tridiagonal matrix whose offdiagonal entries nonzero called unreduced well known 18 p 66 unreduced tridiagonal matrix multiple eigenvalues conse quently nthetan tridiagonal matrix kn distinct eigenvalues must block diagonal largest block cannot larger kthetak generalization fact banded matrices underpins algorithm propose yet straightforward might seem assuming n theta n symmetric matrix define ith row bandwidth denoted band rowi band rowi distance first nonzero element row ith diagonal element say nonincreasing row bandwidth b particular banded matrix zero bth subdiagonal nonzero bth subdiagonal nonincreasing row bandwidth b definitions prove following theorem theorem 1 let symmetric matrix k distinct eigenvalues block diagonal diagonal block nonincreasing bandwidth b size largest block cannot exceed kb proof assume diagonal block size p kb assumption nonincreasing bandwidth b pgammab rows first nonzero elements different columns left diagonal thus rankd gamma less hand since pkb k distinct eigenvalues eigenvalue multiplicity greater b hence rankd gamma less b contradiction verifies result theorem following example shows necessity nonincreasing bandwidth restriction theorem 1 let columns aqq symmetric 0 1 eigenvalues fact theta theta theta theta 0 0 theta theta theta theta 0 0 theta theta theta 0 0 theta 0 0 theta theta 0 theta 0 theta 0 0 theta theta theta theta see banded semibandwidth b 3 block diagonal blocks size since nonincreasing bandwidth condition violated a5 2a7 4 0 3 divideandconquer tridiagonalization approach example preceding section showed standard householder band reduction algorithm necessarily reveal blockdiagonal structure example applied standard algorithm reduction bandwidth 3 matrix example 3 matrix would remained unchanged fortunately minor modification standard algorithm enforces nonincreasing rowbandwidth hence prerequisites theorem 1 let us consider conventional reduction approach matrix reduced one column time semibandwidth b reduction pivot row always b rows diagonal whether reduction previous column skipped ie transformation identity example reduce matrix 3 semibandwidth 3 row number 4 pivot row reduction second column since a4 8 reduction skipped proceed column 3 using row 5 pivot row rowbandwidth increases instead employ householder transformation acting a4 8 3 eliminate a5 8 3 keeping row 4 pivot row obtain theta theta theta theta 0 0 theta theta theta theta 0 0 theta theta theta 0 theta 0 0 0 theta theta 0 theta theta 0 0 0 theta theta theta theta decoupled two diagonal blocks size 6 theta 6 example shows nonincreasing bandwidth obtained easily increase pivot row previous reduction skipped computational purposes define row bandwidth respect threshold band given tolerance threshold column ai n considered numerically zero 2 norm matlab function bred figure 3 shows conventional bandreduction algorithm augmented 1 threshold criterion generation householder vector 2 modified pivot row selection strategy change pivot row transformation skipped subroutines gen hh pre hh post hh sym hh generate householder vector apply left right symmetrically respectively note simplicity algorithm presented exploit symmetry however wish sym hh work triangular part omit post hh pre hh call working lower upper triangle also note algorithms presented paper available via anonymous ftp pubprism directory ftpsuperorg transformations skipped procedure identical conventional band reduction procedure otherwise may terminate earlier reduction reaches last column first diagonal block problem decoupled since drop pivot columns whose norm decomposition accurate residual order simplicity omitted optimization figure 3 reduction first column results bandwidth b say b b due small size entries b 1 directly pursue reduction trailing block nonincreasing bandwidth b fashion shown parameter b chosen kb n k number distinct eigenvalues theorem 1 predicts decoupling problem leading block size larger kb particular b chosen n2 expect bred generate two decoupled subproblems size recursively divide problem transformed matrix becomes tridiagonal ie 1 figure 3 serial implementation tridiagonalization based approach note various subproblems dealt independently simultaneously subroutine blk diag called tri sbr shown figure 3 reduces matrix block diagonal form given bandwidth example reduce 12 theta 12 matrix two eigenvalues bandwidth 3 diagonal block larger 6theta6 thus a4 1 a5 2 a6 nonzero reductions first three columns completed next three columns must already reduced partially reduced matrix theta theta theta theta 0 0 theta theta theta theta theta 0 0 theta theta theta theta theta theta 0 0 theta theta theta theta theta theta 0 0 theta theta theta theta theta 0 0 theta theta 0 0 theta theta theta theta theta theta theta theta theta theta theta theta theta theta theta theta theta theta theta theta theta theta theta theta result need perform reductions would otherwise occurred columns 4 6 complexity algorithm case reduction o125 n 3 update q compared o4n 3 3 operations usual approach savings q minor since updates later stages still involve vectors length n whereas diagonal subblocks affected addition work parallel independent problems estimate k number distinct eigenvalues inaccurate algorithm becomes either standard eigenvalue algorithm k n2 sbr tridiagonalization procedure suggested 10 either case return numerically accurate results 4 invariant subspace splitting computational cost degree parallelism algorithm depend k number distinct eigenvalues one particularly intriguing case matrices two eigenvalues arise eigensolvers based variant subspace decompositions 3 19 4 may assume without loss generality eigenvalues 1 0 two eigenvalues mapped 0 1 shifting scaling following corollary special case theorem 1 corollary 2 let matrix two distinct eigenvalues let aq tq tridiagonal ization block diagonal diagonal blocks size 2 theta 2 corollary 2 implies one determine range space ra null space n essence tridiagonalizing let aqqt orthogonaltridiagonal decomposition 1 theta 1 diagonal block j j since eigenvalues 2 theta 2 diagonal block jj 1 jj 1 must eigenvalues 0 1 since trace sum eigenvalues offdiagonal entry nonzero conclude gamma one see separation range null subspaces fact eigenvalue decomposition affected diagonalizing potentially parallel 2 theta 2 subproblems still occurring block tridiagonal decomposition presence rounding errors computed tridiagonal matrix may however exhibit block structure could expect corollary 2 perturbations eigenvalues g repeated eigenvalue numerically manifests eigenvalue cluster function block1 given symmetric matrix bandwidth b threshold tau bred computes orthogonalbanded matrix decomposition 5 otau denotes matrix twonorm order tau w orthogonal matrix output matrix aoutput 2x2 block diagonal matrix first diagonal block aoutput1block11block1 banded bandwidth nonincreasing b second block may empty current pivot row pivrow n pivrow j break end row column sets involved current transformation generate hh matrix annihilate apivrow1nj update jth row column rows reduction skipped perform symmetric update update q required shift pivot row arows acols rows rows increase pivot row apivrowj negligible j pivrow j1 else return end figure 1 nonincreasing rowbandwidthpreserving bandreduction algorithm function produces orthogonaltridiagonal decomposition symmetric matrix anew tridiagonal q orthogonal number k guess number numerically distinct eigenvalues matrices successively reduced smaller bandwidth attempt exploit divideandconquer nature becoming apparent successive bandreduction algorithm number chosen good guess actual number numerically distinct eigenvalues block1 n problem didnt decouple reduce tridiagonal form else first subproblem tridiagonal yet second subproblem nontrivial return figure 2 divideandconquer tridiagonalization function given symmetric matrix desired bandwidth b threshold tau produces orthogonalblockdiagonal decomposition otau denotes matrix whose norm order tay w orthogonal matrix block diagonal matrix block banded nonincreasing bandwidth b ith diagonal block starts blkveci blkveci q empty matrix input q postmultiplied w return end figure 3 reduction block diagonal form example 3 matrix e ffl eigenvalues hence seems numerically relevant computations would faced computing eigenvalue decomposition tridiagonal matrix case however exploiting special structure tridiagonal matrix diagonalize two sweeps compute eigendecomposition even odd 2theta2 blocks diagonal simultaneously respectively show fillins generated sweeps order perturbation eigenvalues hence considered negligible lemma 4 let symmetric tridiagonal matrix proof let q orthogonal respectively thus next lemma gives bounds elements givens rotation choose diagonalize 2 theta 2 block minimize size fillins c gammas c givens rotation diagonalizes 2 theta 2 symmetric matrix assume without loss generality fi 0 define oe 0 c chosen 1 proof let wish eliminate offdiagonal elements g g obtain choose oe defined 6 2oe claimed following theorem show employing givens rotations limit size fillin entries generated applying rotations tridiagonal matrix eigenvalue clusters around 0 1 theorem 6 let lemma 4 let c gammas c givens rotation diagonalizes one 2 theta 2 diagonal block namely fi fl assume fi 0 without loss generality fi 7 c chosen suggested lemma 5 proof comparing corresponding entries 2 invoking lemma 4 know exist ffl ffl ffl using identities hence express oe 2 defined 6 thus let 1 chosen fi equations 11 together fi 2oe imply 2oe 2oe using 12 easily show 2oe hence result theorem consequence theorem 6 able compute eigenvalue decomposition 2 theta 2 diagonal block tridiagonal matrix eigenvalue clusters 0 1 generated fillin negligible compared eigenvalue perturbation thus diagonalization done two sweeps potentially concurrent 2 theta 2 eigenvalue problems shown figure 4 first sweep diagonalize oddeven 2 theta 2 problem offdiagonal entry small set fillin entries zero otherwise set offdiagonal entry zero zero second sweep diagonalize evenodd blocks since rotations follow need set fillin entries zero theorem 6 shows frobenius norm fillin matrix introduced algorithm rr diagshown figure 4 bounded 3 n order perturbation eigenvalues subroutine diag2 shown computes diagonalizing rotations outlined lemma 5 hence algorithm rr diag numerically reliable approach diagonalizing albeit much cheaper exploits special structure 5 numerical experiments section report numerical experiments algorithms presented paper experiments performed matlab version 42a sun sparcstation ipx function q given tridiagonal matrix eigenvalues 1 0 contained 1tau1tau tautau rrdiag computes approximate eigendecomposition diagonal 2x2 matrices negligible fillins diagonal 2x2 matrices need zero fillins return end figure 4 diagonalization tridiagonal matrix eigenvalue clusters 0 1 readers wishing experiment matlab files employed generate results retrieved via anonymous ftp pubprism directory ftpsuperorg first apply bandreduction algorithm bred figure 3 recursively trailing subblock 200 theta 200 matrix two eigenvalue clusters size 50 1g radius cluster ffl 10e 3 ffl machine precision drop threshold tau bred set step bandwidth chosen decouple problem middle succession matrices generated shown figure 5 title picture shows current matrix size worked bandwidth reduced step compute residual current observe ffi 72e gamma13 given machine precision consistent theory experiment employing matrix 100 eigenvalues 0 1 eigenvalue perturbation drop threshold shown figure 6 note sufficient reduce matrix half bandwidth chosen figure 5 achieve decoupling observe ffi 27e gamma13 also note cases first third fourth splits occur row column 100 176 188 respectively second split occurs row 152 figure 5 row 150 figure 6 test behavior rankrevealing tridiagonalization rrdg compare standard eigenvalue decomposition eig qr factorization column pivoting qr test matrices 1 tridiagonal matrices eigenvalue clusters radius p ffl generated inserting random offdiagonal perturbations order p p ffl matrix shown example 3 2 matrices generated symmetrically multiplying matrices example 3 orthogonal matrices generated via qr factorization random matrix first case call rr diag shown figure 4 second case precede call rr diag call tri sbr shown figure 3 drop threshold divideandconquer tridiagonalization set 7p ffl threshold employed two final diagonalization sweeps 100 run 50 test cases matrix sizes 125 250 375 rrdg eig compute eigenvalue decomposition q diagonal compute roundd round diagonal entry nearest integer report relative eigenvalue residual kq gamma n2 well relative orthogonality residual n note n2 estimate kakf case qr factorization pivoting computes ap qr permutation matrix p upper triangular matrix r compute rank report first divide 50 100 150 2004080120160200 second divide 50 100 150 2004080120160200 third divide 50 100 150 2004080120160200 fourth divide 50 100 150 2004080120160200 figure 5 bandreduction applied trailing subblock 200 theta 200 matrix four first divide 50 100 150 2004080120160200 second divide 50 100 150 2004080120160200 third divide 50 100 150 2004080120160200 fourth divide 50 100 150 2004080120160200 figure bandreduction applied trailing subblock 200 theta 200 matrix two table 1 relative residual subspace splitting table 2 relative residual orthogonality small since q1 basis range space case report worst residual see divideandconquer tridiagonalization followed two cleanup sweeps resulting tridiagonal matrix performs well fullfledged eigenvalue decomposition cases residual subspace splitting op ffl expected residual qr factorization include perturbation eigenvalue 1 two approaches therefore smaller cases case computed orthogonal matrices orthogonal machine precision q computed eig function matlab slightly less orthogonal since eig involves transformations result accumulates rounding errors note three approaches worse full matrix case roundoff errors orthogonal reductions order machine precision p bigger roundoff errors dominated perturbation eigenvalues hence rrdg eig behave tridiagonal full matrices 6 conclusions paper introduced algorithm reducing symmetric matrix repeated eigenvalues tridiagonal form algorithm progresses series band reductions band reduction stage forcing decoupling band matrix independent subblocks compared usual householder tridiagonalization procedure approach save 50 floatingpoint operations also developed robust inexpensive numerical procedure diagonalizing resulting tridiagonal matrix case matrix two eigenvalue clusters around zero one case arises eigenvalue decomposition algorithms based invariant subspace approaches taken together two algorithms allow efficient diagonalization matrices algorithm generalized immediately reduction unsymmetric matrices hessenberg form irreducibility argument underlying theorem 1 goes hessenberg matrices also note exact arithmetic conjugate transposed eigenvalue pairs would end block however since one triangle hessenberg matrix still full potential computational savings greatly reduced apart divideandconquer nature resulting potential parallelism well reduced operation count divideandconquer algorithm another attractive feature since algorithm least early stages reduces matrices banded form relatively wide band easy block householder transformations using wy representation 11 compact wy representation 20 example described 17 fashion one easily capitalize favorable memory transfer characteristics block algorithms r divideandconquer algorithm eigenproblem via complementary invariant subspace decomposition design parallel nonsymmetric toolbox structured second higherorder derivatives univariate taylor series prism project infrastructure algorithms parallel eigensolvers parallel tridiagonalization twostep band reduction fundamental linear algebra computations highperformance com puters project developing linear algebra library highperformance computers framework band reduction tridiagonalization symmetric matrices wy representation products householder matrices parallel householder tridiagonalization stratagem using scattered square decomposition reduction condensed form eigenvalue problem distributedmemory architectures evolution numerical software dense linear algebra block reduction matrices condensed form eigenvalue computations matrix computations solution large theory matrices numerical analysis parallelizable eigensolver real diagonalizable matrices real eigenvalues storage efficient wy representation products householder transformations tr