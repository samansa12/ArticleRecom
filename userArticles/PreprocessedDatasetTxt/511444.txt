crossentropy rare events maximal cut partition problems show solve maximal cut partition problems using randomized algorithm based crossentropy method maximal cut problem proposed algorithm employs auxiliary bernoulli distribution transforms original deterministic network associated stochastic one called associated stochastic network asn iteration randomized algorithm asn involves following two phases1 generation random cuts using multidimensional berp distribution calculation associated cut lengths objective functions related quantities rareevent probabilities2 updating parameter vector p basis data collected first phasewe show berp distribution converges distribution degenerated one berpdast sense someelements pdast unities rest zeros unity elements pdast uniquely define cut taken estimate maximal cut similar approach used partition problem supporting numerical results given well numerical studies suggest maximal cut partition problems proposed algorithm typically polynomial complexity size network b introduction combinatorial optimization problems nphard example deterministic stochastic noisy scheduling traveling salesman problem tsp maximal cut network longest path network optimal buer allocation production line optimal routing deterministic stochastic networks ow control optimization topologies conguration computer communication trac systems well established stochastic methods combinatorial optimization problems simulated annealing 1 2 7 37 initiated metropolis 31 later generalized 19 22 26 tabu search 16 genetic algorithms 17 additional references deterministic stochastic combinatorial optimization see 34 2325 30 32 3336 recent works stochastic combinatorial optimization also subject paper include method andradottir 5 6 nested partitioning method np 42 43 stochastic comparison method 18 ant colony optimization aco meta heuristic dorigo colleagues 12 15 methods markov chain constructed almost sure convergence proved analyzing stationary distribution markov chain shall next review brie aco meta heuristic algorithms 815 20 41 4447 try mimic ant colonies behavior known ant colonies able solve shortestpath problems natural environment relying rather simple biological mechanism walking ants deposit ground chemical substance called pheromone ants tendency follow pheromone trails within xed period shorter paths nest food traversed often longer paths obtain higher amount pheromone turn tempts larger number ants choose thereby reinforce behavior real ants inspired many researcher use ant system models algorithms set articial ants cooperate via pheromone depositing either edges vertices graph consider example acs ant colony system approach dorigo maniezzo colorni 15 solving tsp problem described follows rst number articial ants also called agents positioned randomly node graph agent performs series random moves neighbor nodes controlled suitably dened transition probabilities agent visited nodes length tour evaluated pheromone values assigned arcs path increased amount depending length tour procedure repeated many times probability transition along specic arc computed based pheromone value assigned arc length arc higher pheromone value shorter length arc higher probability agent follow arc rst move note also updating transition probabilities iteration acs algorithm dorigo maniezzo colorni 15 also introduce socalled evaporation mechanism discounts pheromone values obtained previous iteration diverse modications acs algorithm presents natural generalization stochastic greedy heuristics applied eciently many dierent types discrete optimization problems produced results recently approach extended dorigo di caro 12 full discrete optimization meta heuristic called ant colony optimization aco meta heuristic covers well known combinatorial optimization problems gutjahr 20 21 rst prove convergence acs algorithm paper deals application crossentropy ce method maximal cut partition problems ce method rst introduced 28 estimating probabilities rare events complex stochastic networks applied 38 solving continuous multiextremal combinatorial optimization problems cop namely shortest path two given nodes deterministic network graph edge given length longest path tsp ce method combinatorial optimization employs auxiliary random mechanism equipped set parameters transforms deterministic network stochastic one called associated stochastic network asn iteration ce algorithm based asn involves following two phases 1 generation random trajectories walks using auxiliary random mechanism like auxiliary markov chain transition probability matrix calculation associated objective function 2 updating parameters like updating elements transition probability matrix basis data collected rst phase let original deterministic network denoted graph v set nodes e set edges depending particular problem introduce randomness asn associating form randomness either edges e b nodes v specically distinguish socalled stochastic edge networks sen b stochastic node networks snn stochastic edge networks sen trajectories typically generated using markov chain transition probability matrix transition j uniquely denes edge ij network sen one readily reduce tsp quadratic assignment problem deterministic stochastic ow shop models well others sen considered treated earlier 38 b stochastic node networks snn trajectories walks generated using ndimensional discrete distribution like ndimensional bernoulli ber p distribution component random vector rv uniquely denes node v k network snn one readily reduce maximal cut problem partition problem clique problem optimal buer allocation production line well others application ce snn particular maximal cut maximal partition problems subject paper notice terminology similar snn sen exists wagner lindenbaum bruckstein 47 graph covering problem called vertex ant walk vaw edge ant walk ean respectively crucial understand ce algorithm 32 38 sen algorithm 41 paper snn similar main dierence sample trajectories generation mentioned former 38 trajectories typically generated using markov chain latter generated say using ndimensional berp distribution shall show snn algorithm 41 maximal cut problem following properties similar properties apply maximal partitition problem 1 multidimensional bernoulli distribution converges degenerated one berp sense parameters p unities rest zeros 2 unity elements p uniquely dene cut taken estimate maximal cut perfectly matches sen algorithm 32 38 1 probability matrix converges degenerated one sense single element row equals unity remaining elements equal zero 2 unity elements degenerated matrix uniquely dene shortest tour say tsp problem shall nally show algorithm 41 fact presents simple modication algorithm 21 estimation probabilities rare events algorithm 21 algorithm 12 38 adapted estimation rare event probabilities sen problems like stochastic tsp algorithm 31 38 algorithm 12 38 also formed basis algorithm 32 paper elaborate let represent probability rareevent fmxxg mx sample performance stochastic system vector known distribution x xed number chosen probability x small order give example mx 11 consider graph whose edges random lengths given x mx may length shortest path two designated nodes graph called source sink formally mx dened jth complete path source sink p number complete paths similar algorithm 31 38 algorithm 21 paper adaptive algorithm estimation rare event probabilities using importance sampling crossentropy distinguishing feature algorithms x xed advance automatically generate sequence tuples f see 25 28 ensuring iteration number algorithm 21 algorithm stops turning cops note soon deterministic cop transformed stochastic one mx called sample performance asn eg cut value available cast asn rare event framework 11 recall original formula 11 x natural random vector asn articially constructed random vector say bernoulli random vector shall show analogy algorithm 21 algorithm 41 generates sequence tuples f see 46 47 converges distribution stationary point true maximal cut value p degenerated vector determining cut corresponding language rare events also means algorithm 41 able identify high probability small subset largest cut values follows shall show cops solved simultaneously estimation probabilities rareevents asn framework enables us establish tight connections rareevents combinatorial optimization also case 38 goal compare eciency proposed method wellestablished alternatives simulated annealing tabu search genetic algorithms done somewhere else goal merely establish theoretical foundation proposed ce method demonstrate theoretically numerically high speed convergence proposed algorithm promote approach applications section 2 review adaptive algorithm estimation rare event probabilities citing material 28 39 section 12 38 section 3 present maximal cut partition problems asn dene probability rareevent exactly 11 also present algorithms generation random cuts partitions section 4 presents main ce algorithm 41 maximal cut partition problems also prove theorem stating certain conditions sequence tuples f associated asn converges stationary point section 5 give modications main ce algorithm important modication present fully automated ce algorithm section 6 supportive numerical results presented section 7 concluding remarks directions research given 2 crossentropy method probability rare events estimation let fz v multivariate density parameter vector v consider estimating v importance sampling estimate given new likelihood ratio x fz v 0 v new called reference parameter nd optimal reference parameter v new one either minimize variance importance sampling estimate new see 29 39 maximize following crossentropy see 38 new g latter given sample new estimate optimal solution v new 22 optimal solution program vnew new readily seen programs 22 23 useful case small say rareevent context say 23 useless since owing rarity events fmx xg random variables fmx xg associated derivatives b new probability provided sample n small relative reciprocal rareevent probability x overcome diculty introduce auxiliary sequence f start choosing initial original pdf fz v probability g g small say specically set v sequentially iterate v outlined adaptive estimation xed v derive following simple onedimensional rootnding program fmx stochastic counterpart 24 follows xed derive following program readily seen tj jth order statistics sequence tj mx j b adaptive estimation v xed solution program fmx v v stochastic counterpart 27 follows xed derive following program resulting algorithm estimating x written 1 set v 0 v 0 v generate sample x xn pdf fx v 0 deliver solution 26 program 25 denote initial solution set t1 2 use sample x 25 solve stochastic program 28 denote solution 3 generate new sample x xn pdf fx deliver solution 26 program 25 denote solution 4 x set x solve stochastic program 28 x denote solution t1 stop otherwise set reiterate step 2 stopping estimate rareevent probability x using estimate 21 v 0 replaced t1 monotonicity sequence crucial convergence algorithm 21 proved 27 x one dimensional random variable distributed gammav monotonically increasing positive function one variable sequence generated algorithm 21 monotonically increases provided n 1 iteration theorem readily extended multidimensional x distributions exponential family normal beta poisson discrete details see 27 following theorem due 28 states sequence f monotonically increasing mild regularity conditions n 1 sequence f reaches x nite number iterations theorem 21 let x x 0 let h mapping corresponds iteration algorithm 21 ie v v assume rst following conditions hold 1 sequence f v g monotonically increasing 2 mapping v 7 v continuous 3 mapping v 7 v proper ie v belongs closed interval v belongs compact set 4 mapping v 7 v lower semicontinuous exist 1 lim pf proof given 27 readily follows x xed advance algorithm 21 automatically generate two sequences 3 maximal cut partition problems 31 cuts partition associated stochastic networks maximal cut problem graph formulated follows given graph e set nodes set edges e nodes partition nodes graph two arbitrary subsets v 1 v 2 sum weights edges going one subset maximized mathematically written f denotes symmetric matrix weights distances edges assumed known partition problem dened similarly dierence maximal cut partition problem former length say vector latter xed note solving 31 one needs decide whether since matrix l ij symmetric order avoid duplication shall assume without loss generality program 31 also written length value kth cut called objective function v kth set possible cuts graph jx j cardinality set fxg denote maximal cut maximal cut value optimal value objective functionby v respectively readily seen total number cuts similarly total number partitions xed equal n2 simplicity assume n even figure 31 6node network example consider figure 31 associated 6x6 distance matrix cardinalities maximal cut partition jx consider instance following two cuts function value partition cost rst second cases respectively mentioned order generate stationary tuple see algorithm 41 need transform original deterministic network associated stochastic one maximal cut problem associate n dimensional random vector rv n dimensional vector component x independent bernoulli distributed ie x k berp k interpretation x stated otherwise set p 1 1 maximal cut problem iteration main algorithm 41 comprises following two phases generation random cuts see algorithms 31 asn using calculating associated sample performance mx b updating sequence tuples f t1 g iteration algorithm 41 parameter vector ber p t1 independent components updating sequence tuples f t1 g iteration algorithm 21 note soon auxiliary discrete distributions dened sequence f t1 g viewed particular case sequence f t1 g t1 case partition problem generation random partitions performed dierent algorithm algorithm 32 algorithm 41 updating sequence tuples f t1 g problems consider separately phases b specically rest section deals phase section 4 deals phase b algorithm 41 presented 32 random cut generation algorithm generating random cuts asn based berp independent components written follows algorithm 31 random cut generation 1 generate ndimensional random vector independent components 2 x construct two vectors v 1 v 2 v 1 contains set dimensional vector containing set indices corresponding unities v 2 n dimensional vector containing set indices corresponding zeros note 0 n random variable 3 calculate sample function mx see also 31 associated random cut consider example fig 31 assume cut maximal one case starting arbitrary 6dimensional vector p goal algorithm 41 converge degenerated bernoulli distribution parameter vector nite number iterations algorithm 31 along algorithm 41 readily extended randomly partitioning nodes v graph e r 2 subsets sum total weights edges going one subset another maximized case one follow basic steps algorithm 31 using n rpoint distributions r instead n 2point berp j distributions 33 random partition generation unlike independent bernoulli case sample generated using sequence recall number nodes want v 1 dependent discrete distributions denoted goal fm p generate associated random walk length ie nodes network ie k takes values set nodes visited random walk repeated nodes constitute set v 1 let 1 clearly 1 discrete distribution probability selecting node 1 node thus selected denoted 1 sequence distributions derived recursively starting 1 used generating algorithm 32 1 generate 1 discrete pdf 1 set x 1 1 2 derive 2 1 follows first eliminate element 1 1 normalize remaining n 1dimensional vector call resulting vector 2 3 generate 2 2 set x 2 1 4 proceed steps 2 3 recursively 2 times 1 k generated k derived k 1 follows eliminate element k 1 corresponding node k 1 k 1 normalize remaining n k 1dimensional vector call resulting vector k 5 set remaining n elements x equal 0 6 calculate objective function mx 38 note algorithm assume x 1 1 modication x 1 1 straightforward one needs replace 311 1 rest similar 4 main algorithm assume given algorithms generating random cuts random partitions able calculate sample function mx mentioned shall cast asn rareevents context 41 rareevent framework consider 11 asn assume moment x close unknown true maximal cut represents unknown optimal solution programs 31 33 mind shall adapt basic singleiteration multiple iteration see 2223 2428 respectively used algorithm 21 asn particular maximal cut bearing mind x berp similar 22 23 singleiteration program new new new fix ik 1g respectively assumed expectation taken respect berp sample x optimal solutions 41 42 derived straightforward application lagrange multipliers technique fmxxg x r fmxxg fmx k xg fmx k xg respectively expectation numerator 43 expectation possible cuts node v r belongs v 1 44 sum generated cuts also 44 set newr arbitrary value say 12 fmx k xg realizing chance latter shrinks 0 n 1 let p newn assume optimal solution v 2 program 31 unique consider probability x 11 obvious x irrespective choice parameter vector p bernoulli distribution shall present rst important observation case let p denote degenerated probability vector ie contains combination unities zeros moreover let components dene unique maximal cut sense unity components p correspond components v 1 zero components p correspond components 2 shall call p optimal degenerated vector odv proposition 41 assume maximal cut v 2 unique let x random vector independent components distributed berp optimal vector p new 43 reduces odv p irrespective p provided proof proof follows immediately 43 clarify let x vector degenerated bernoulli distribution uniquely dening let v 2 corresponding cut random vector x corresponding cut must fmx therefore rst part proposition proved second part proposition note 0 second part follows fact due similar reasoning rst newr 44 either p dr 12 using convention fact dicult verify variance estimate n x new 21 note v replaced p similarly v new equals zero already mentioned shall approximate unknown true solution sequence tuples f generated algorithm 41 42 main algorithm proceed note singleiteration program 42 optimal solution newr 44 little practical use since arbitrary vector p berp indicators fmx j xg high probability provided x close optimal value sample size n small relative reciprocal rareevent probability pfmx xg overcome diculty use algorithm 21 use instead adaptive estimation xed p derive solution b adaptive estimation p xed derive tn solution fix ik 1g note programs 46 47 solved analytically solution 46 given 26 solution 47 see 44 written 0 set otherwise case never happened examples tried resulting algorithm estimating vector p follows algorithm 41 1 choose p asn generate n random vectors corresponding cuts using algorithm 31 deliver solution 26 program 46 denote initial solution 2 use n vectors deliver solution 48 program 47 denote solution 3 generate n new random vectors using algorithm 31 deliver solution 26 program 46 4 k k say stop deliver estimate otherwise set go step 2 alternative estimate stopping rule 49 one consider following 4 k k say 0st estimate otherwise set go step 2 remark 41 smoothed probability vectors instead p see 48 typically use following smoothed version 05 1 clearly reason using instead ti twofold smooth values ti b reduce probability values ti zeros unities specially beginning iterations readily seen starting say p 0 indices 2 also found empirically 07 09 algorithm 41 typically accurate particular noisy cop numerical studies used 09 note according antbased terminology 12 15 call p evaporation parameter pheromone vector respectively remark 42 relation root finding mentioned algorithm 41 might viewed simple modication algorithm 21 precisely similar algorithm 21 sense nding root x associated optimal solution rare event probability x turn implies algorithm 41 involves neither likelihood ratio calculations estimation probabilities rare events reason algorithm 21 algorithm 41 dierent stopping rules point may worth mentioning following theorem theorem 41 assume maximal cut v 2 unique conditions theorem 21 hold exists 1 sequence tuples f generated algorithm 41 converges distribution constant tuple 1 irrespective choice p provided proof according theorem 21 setting exists 1 lim using 48 reasoning similar proposition 41 get combining two previous facts exists 1 thus proving statement theorem remark 43 apply theorem maximal cut partition problems one needs prove conditions theorem 21 hold results settings maximal cut partition problems one referred 27 see eg proposition 31 27 remark 44 follows theorem 41 proposition 41 irrespective initial choice p multipleiteration procedure involving sequence f p converges degenerated parameter p singleiteration procedure 5 modications enhancements main algorithm 51 alternative sample functions natural modication twostage iterative procedure algorithm 41 would update p second stage see 47 using alternative sample functions rather indicators fm abbreviated notation mx used example 47 consider rst maximization problem alternatives fm one could use 47 1 0 2 boltzmann type function exp 0 3 linear loss function insensitive zone 4 huber loss function found modications typically lead increase convergence speed algorithm 41 two four times reason using indicators put equal weight associated top dne values 48 modied versions put weight proportional respective value consider minimization problem recall case use algorithm 41 bottom dne values instead top ones since use indicator function fm simple modication sample function fm g could use top dne values 1 mn say top dne values found however policy modication results typically worse performance ce algorithm 41 regardless intuitive explanation function 1 quite nonlinear non symmetric relative used maximization nd symmetric function rst nd symmetric function follows instead sample n use minimization problem say following sequence n 1 use top dne samples new sequence ie raise element sequence power etc one also use boltzmann type function written analogy 52 exp 0 similarly functions 53 54 52 singlestage ce algorithms versus twostage ce algorithm term singlestage means iteration ce algorithm updates p alone ie involve program 46 thus sequence particular say maximization problem would imply updating vector similar 48 taking entire rather truncated sample entire sample n example using entire sample obtain instead 47 following stochastic program fix ik 1g singlestage version would simplify substantially algorithm 41 disadvantage using singlestage sample functions typically takes long algorithm 41 converge since large number important untruncated trajectories slow dramatically convergence f p g p found numerically singlestage ce algorithm much worse twostage counterpart sense less accurate time consuming compared original twostage algorithm 41 practically found work maximal cut problems size n 30 hence important ce method use stages algorithm 41 also one major dierences ce antbased methods dorigo maniezzo colorni 15 others singlestage sample functions updating alone used 53 fully automated ce algorithm present modication algorithm 41 n 46 47 updated adaptively call modication ce fully automated ce algorithm addition face algorithm able identify dicult pathological problems fails problems face algorithm stops sample size n becomes prohibitively large say improvement objective function let order statistics corresponding used updating tuple f t1 g arranged decreasing order note departure convention used earlier done simplify notation used notice 46 main assumption face algorithm xed positive constant interval 001 c 1 thus cn corresponds number samples set tj lie upper 100 samples refer latter elite samples second parameter used 411 note c close 1 may chosen close unity say 099 contrast choose less say found numerically combinations like good choices stated otherwise shall use combination bear mind maximization problem particular maximal cut problem compared algorithm 41 introduce face algorithm following two enhancements 1 prior tth iteration keep portion 0 1 elite samples incorporate n replaced n respectively precisely implementing 46 47 addition current sample mx size generated ber p also include elite samples 1th iteration readily seen sequence f t1 g 46 47 based elite sampling size n obtained iterations use 02 numerical experiments reported paper even though values 2 let r constant r 1 iteration face algorithm design sampling plan ensures note implies improvement maximal order statistics t1 iteration stated otherwise shall take r 1 rst r iterations ie iteration number 0 iteration number r 1 face algorithm coincide algorithm 41 take p also typically take proceed follows algorithm 51 face algorithm 1 iteration sample size combine samples elite samples obtained iteration number 1 thus giving total sample size n samples tj tj 2 58 holds proceed 4647 using n samples mentioned step 1 3 58 violated check whether holds say stop deliver t1 estimate optimal solution call t1 reliable estimate optimal solution otherwise 4 increase sample size holds large say n former case set n proceed 46 47 latter case stop deliver 11 estimate optimal solution call 11 unreliable estimate optimal solution remark 51 step 3 use c numerical experiments reported paper even though values c 1 given range gave similar results remark 52 save sampling eort terminate step 4 slightly dierent manner stated algorithm 51 let n n 0 n n say n implicitly assuming 20n 10 6 usually case practice also sen networks take n iteration increasing n given step 4 obtain 58 still violated interrupt step 4 directly proceed updating 46 47 proceeding next iteration however slight change step 4 algorithm 51 keeps generating samples size n several iterations turn say complete step 4 given algorithm 51 remark 53 zigzag policy let k given constant k 1 decrease n 0 factor starting iteration 1 similarly 58 hold k consecutive iterations increase n 0 factor note generate least k latest increase decrease n 0 start check 511 start check 58s previous k iterations numerical experiments reported paper use increase decrease factor 2 note algorithm 51 uses maximal orderstatistics based stopping criterion given 59 seems natural compared quantile based stopping criterion used algorithm 41 note also 59 based fact approaching degenerating solution trajectories follow path associated thus number dierent trajectories less note nally 58 holds r automatically obtain n 8t case algorithm 51 reduces original algorithm 41 provided 6 numerical results present performance algorithm 41 algorithm 51 maximal cut partition problems performance algorithms mean convergence estimators see eg 49 410 true unknown optimal value mentioned choose respective asns since maximal cut partition np hard problems exact method known verifying accuracy method except naive total enumeration routine feasible small graphs say n nodes overcome diculty construct articial graph solution available advance verify accuracy method example consider following symmetric distance matrix z z components upper lefthand lower righthand quadrants sizes n respectively 0 n equal generated given distribution ua b uniformly distributed interval b etc remaining components equal choose c 2 partition v optimal one precisely assume random variable z pdf bounded support say z c 1 let example clearly 2 optimal solution suces c 2 c 1 similarly n2 examples set 001 46 took stopped algorithm 41 according stopping rule 49 parameter 5 found r 10 relative error dened equalled zero experiments corresponds stopping time algorithm 41 running time seconds algorithm 41 sun enterprise 4000 workstation 12 cpu 248 mhz reported well table 61 presents relative errors associated stopping times function sample size maximal cut problem following 6 cases z 61 z beta b beta distribution whose probability density function given b table 61 relative errors associated stopping times functions sample size maximal cut problems 6 cases z given 61 table 62 cpu times functions sample size data table 61 results tables 61 62 selfexplanatory similar results obtained algorithm 41 partition problems rest numerical results partition problems tables 63 64 presents performance along functions dened ts 05 ng ts 05 particular table 63 corresponds table 64 presents data similar table 63 cases obtained relative error 0 cpu times respectively results tables 63 64 selfexplanatory table 63 performance algorithm 41 z u45 5 6 1220356e06 0510000 0490000 9 1224679e06 0510000 0490000 13 1238418e06 0510000 0470000 14 1240769e06 0590000 0420000 19 1249318e06 0890000 0090000 table 64 performance algorithm 41 z u45 5 6 1308071e06 0516667 0483333 9 1410264e06 0833333 0466667 14 1456611e06 1000000 0233333 19 1458000e06 1000000 0000000 table 65 presents dynamics p t14 another smaller example table 65 dynamics 3 100 097 095 099 087 099 098 001 003 000 011 000 002 000 4 100 100 100 099 100 099 098 001 000 000 000 000 001 000 5 100 100 100 099 100 099 099 001 000 000 000 000 000 000 table 66 presents performance algorithm 51 input data table 64 n tn denotes best elite sample value mx obtained iteration found algorithm 51 least accurate algorithm 41 typically 23 times faster algorithm 41 table 66 performance algorithm 51 input data table 64 9 142e06 139e06 3000 001 087 043 point would like note performed extensive simulations case studies algorithm 41 dierent snn models found approximately 99 cases algorithm 41 performs well sense relative error dened 63 exceed 1 results reported somewhere else 61 empirical computational complexity let us nally discuss computational complexity algorithm 41 maximal cut partition problems dened n total number iterations needed algorithm 41 stops n n sample size total number maximal cuts partitions generated iteration g n cost generating n independent bernoulli distributions needed algorithm 31 sequence distributions fm p needed algorithm 32 cost updating tuple latter follows fact computing mx 38 2 operation model 61 found empirically 1000 maximal cut problem considering take n n n 10n g n obtain experiments complexity observed like partitition problem similar computational characteristics important note empirical complexity results solely model distance matrix 61 7 concluding remarks directions research paper presents application crossentropy ce method 38 maximal cut partition problems proposed algorithm employs auxiliary discrete dis tribution transforms original deterministic network associated stochastic one called associated stochastic network asn iteration ce method involves two major steps generation trajectories cuts partitions using auxiliary discrete distribution parameter vector p calculation associated objective function mx related quantities indicator functions associated rareevent probabilities b updating parameter vector p basis data collected rst step numerical studies maximal cut partition problem well cops reported paper suggest proposed algorithms typically perform well sense approximately 99 cases relative error dened 63 exceed 1 addition experiments suggest algorithm 41 algorithm 51 polynomial complexity size network topics investigation listed 1 establish convergence algorithm 41 nite sampling ie n 1 emphasis complexity speed convergence suggested stopping rules 2 establish condence intervals regions optimal solution 3 apply proposed methodology wide variety combinatorial optimization problems quadratic assignment problem minimal cut vehicle routing graph coloring communication networks optimization problems 4 apply proposed methodology noisy simulation based networks preliminary studies suggest algorithm 41 performs well case noisy networks 5 apply parallel optimization techniques proposed methodology acknowledgments would like thank alexander podgaetsky performing numerical experiments would also like thank three anonymous referees pietertjerk de boer victor nicola guest editor university twente netherlands perwez shahabuddin guest editor columbia university usa many valuable suggestions r john wiley local search combinatorial optimization network ows theory genetic algorithms search introduction global optimization global optimization action modern heuristic search methods discrete event systems sensitivity analysis stochastic optimization via score function method tr discrete optimization simulated annealing boltzmann machines stochastic approach combinatorial optimization neural computing approximating permanent network flows polynomialtime approximation algorithms ising model search method discrete stochastic optimization antbased load balancing telecommunications networks new parallel randomized algorithms traveling salesman problem ant colony optimization metaheuristic aco algorithms quadratic assignment problem ant algorithms discrete optimization graphbased ant system convergence genetic algorithms search optimization machine learning local search combinatorial optimization simulated annealing efficiently searching graph smelloriented vertex process nested partitions method global optimization ctr victor f nicola tatiana zaburnenko efficient importance sampling heuristics simulation population overflow jackson networks proceedings 37th conference winter simulation december 0407 2005 orlando florida victor f nicola tatiana zaburnenko efficient simulation population overflow parallel queues proceedings 37th conference winter simulation december 0306 2006 monterey california p de boer p kroese r rubinstein rare event simulation combinatorial optimization using cross entropy estimating buffer overflows three stages using crossentropy proceedings 34th conference winter simulation exploring new frontiers december 0811 2002 san diego california zdravko botev dirk p kroese global likelihood optimization via crossentropy method application mixture models proceedings 36th conference winter simulation december 0508 2004 washington dc victor f nicola tatiana zaburnenko efficient heuristics simulation population overflow series parallel queues proceedings 1st international conference performance evaluation methodolgies tools october 1113 2006 pisa italy victor f nicola tatiana zaburnenko efficient importance sampling heuristics simulation population overflow jackson networks acm transactions modeling computer simulation tomacs v17 n2 p10es april 2007