modified cholesky factorizations interiorpoint algorithms linear programming investigate modified cholesky algorithm typical used interiorpoint codes linear programming choleskybased interiorpoint codes popular three reasons implementation requires minimal changes standard sparse cholesky algorithms allowing us take full advantage software written specialists area tend efficient competing approaches use alternative factorizations perform robustly practical problems yielding good interiorpoint steps even coefficient matrix main linear system solved step components ill conditioned investigate surprisingly robust performance using analytical tools matrix perturbation theory error analysis illustrating results computational experiments finally point potential limitations approach b introduction interiorpoint codes linear programming share common feature major computational operationsolution large linear system equationsis performed direct sparse cholesky algorithm algorithm row column orderings determined priori wellknown heuristics minimum degree enhancements minimum local fill nested dissection based solely sparsity pattern numerical values nonzero elements ordering phase followed symbolic factorization phase nonzero structure cholesky factor determined storage allocated finally numerical factorization phase fills numerical values lower triangular interiorpoint codes first two phases usually performed either first interiorpoint iteration computation starting point interiorpoint context unadorned cholesky algorithm run difficulties extreme ill conditioning diagonal pivots encountered numerical factorization phase zero negative causing standard cholesky procedure break instead crashing codes apply patch algorithm handle pivots offending pivot element sometimes replaced huge number lipsol 17 pcx 1 codes ipmos 16 pivot replaced moderate number corresponding righthand side element set zero offdiagonal elements corresponding column cholesky factor first practical interiorpoint code ob1 6 explicitly zeroes components solution vector correspond small pivots strategies essentially equivalent algorithm describe paper date little investigation numerical analysis viewpoint patches described advantage implemented changing lines general sparse cholesky codes therefore possible take advantage longterm development effort gone designing codes underlying algorithms recent codes lipsol 17 pcx mathematics computer science division argonne national laboratory 9700 south cass avenue argonne il 60439 work supported mathematical information computational sciences division subprogram office computational technology research us department energy contract w31109eng38 1 make explicit use freely available sparse cholesky code ng peyton 8 codes either modify wellknown sparspak routines george liu 3 include customized linear algebra routines implement well established algorithmic ideas least one author experimented modifications standard heuristics meszaros 7 describes inexact version minimum local fill ordering one possible remedy small pivots diagonal pivoting iteration large diagonal element selected unreduced portion matrix moved pivot position symmetric row column pivoting algorithm terminated none remaining diagonal elements sufficiently large approximate solution computed partial factors see higham 4 chapter 10 details error analysis strategy particularly appealing context interiorpoint linear programming codes loss efficiency due shifting data numerical factorization moreover little incentive test strategy simple patches described perform well practice article use standard results numerical analysis explain good performance patching strategies vast majority problems also gain insight limitations fail error analysis modified cholesky algorithm rigorous explicitly stated assumptions precise bounds see sections 3 4 revert however informal style applying results interiorpoint context section 5 reason pure pragmatism fully rigorous analysis would impossibly notationally speaking unduly pessimistic informal analysis yields adequate insight typical performance algorithm computational results section 6 demonstrate number papers linear algebra operations barrier interiorpoint methods appeared recent years wright 12 considered newton logarithmic barrier method general constrained optimization linear system solved newton step positive semidefinite ill conditioned later iterations uses cholesky factorization diagonal pivoting identify subspace spanned active constraint jacobian infor mation accurate solution newton equations obtained components step range space active constraint jacobian null space transpose well resolved analysis similar flavor wrights application somewhat different unknowns linear system unconstrained dual variables rather primals since problem linear little interest resolving component step nearnull space coefficient matrix focus cholesky algorithms perform pivoting numerical factorization reflecting computational practice current generation interiorpoint linear programming codes earlier paper 14 considered stability algorithms symmetric indefinite form step equations iteration interiorpoint method linear programming showed despite illconditioning coefficient matrix steps obtained approach good search directions interiorpoint method forsgren gill shinnerl 2 perform similar analysis context logarithmic barrier methods remainder paper organized follows section 2 introduce primaldual interiorpoint methods derive linear equations solved iteration methods section 3 introduces algorithm modchol modified procedure examines accuracy solution obtained factorization certain assumptions eigenvalues factored matrix section 4 account effect finiteprecision floatingpoint arithmetic solution accuracy return interiorpoint application section 5 showing algorithm modchol yields good steps methods duality gap becomes small even linear program primal dual degenerate analytical results verified computational experiments interiorpoint code using algorithm modchol reported section 6 notation summarize notation used remainder paper ith singular value matrix denoted oe use oe alone denote ith singular value exact cholesky factor l section 3 matrix index steps j ij denotes submatrix formed elements ith column denoted deltai column submatrix consisting columns j 2 j denoted deltaj unit roundoff error denoted u higham 4 chapter 1 defines u implicitly statement ff two floatingpoint numbers op denotes gamma theta fldelta denotes floatingpoint representation real number positive integer mu 1 define 1 see higham 4 lemma 21 notation k delta k denotes euclidean vector norm k delta k 2 also induced matrix norm unless otherwise noted matrix matrix consisting absolute values element denoted jaj use 1 denote vector finally mention parameter ffl defines pivot threshold modified algorithm second quantity ffl related ffl appears frequently analysis incorporation scaling term 2m 2 saves notational clutter 2 primaldual algorithms linear programming consider linear programming problem standard form subject 2 dual 2 subject assume throughout paper full row rank n karushkuhntucker kkt conditions identify vector triple x primaldual solution 2 3 stated follows 4a 4b 4c 4d assume throughout paper primaldual solution exists make assumptions uniqueness nondegeneracy analysis section 5 continues hold problem 2 primal dual degenerate well known index set f1 ng partitioned two sets b n primaldual solutions x primaldual interiorpoint algorithms generate sequence iterates x satisfy strict inequality x 0 find search directions applying modification newtons method system nonlinear equations formed first three kkt conditions 4a4b4c namely general search direction deltax delta deltas obtained following linear delta deltas5 4 gammar c coefficient matrix jacobian 6 righthand side components r b r c defined pure newton affinescaling method remaining righthand side component r xs defined case denote solution 7 deltax aff delta aff deltas aff pathfollowing method duality gap defined centering parameter mehrotra predictorcorrector al gorithm used basis many practical codes search direction calculated setting deltax aff deltas aff diagonal matrices formed affinescaling step components deltax aff deltas aff hence mehrotras method requires solution two linear systems iterationthe affine scaling system 7 8 9 search direction system 7 8 12 heuristic based effectiveness affine scaling direction used determine value 12 search direction determined primaldual algorithm takes step form ff chosen maintain strict positivity x components codes ff chosen fraction steptoboundary ff max defined typical strategy set interiorpoint method approaches solution set applying block elimination 7 using notation obtain following equivalent system 16a 16c many codes solution obtained formulation sparse cholesky factorization modified handle small pivots applied symmetric positive definite coefficient matrix ad 2 16a solution delta obtained triangular substitution computed factor remaining direction components recovered 16b 16c technique yields steps deltax delta deltas useful search directions interiorpoint algorithm even matrix happens later iterations observation somewhat surprising since naive application error analysis results would suggest combination illconditioning roundoff would corrupt direction hopelessly results sections 3 4 5 provide explanation phenomenon following observation crucial analysis computing delta 16a interested much error delta effect error remaining step components deltas deltax recovered 16b 16c respectively relative errors components large positivity requirement may cause step length ff significantly shortened thereby curtailing algorithms progress return issue section 5 describing analyzing modified cholesky algorithm sections 3 4 3 modified cholesky algorithm section describe analyze algorithm modchol modified cholesky algorithm designed handle illconditioned matrices small negative pivots may arise factorization algorithm modchol accepts theta symmetric positive definite matrix input together small positive userdefined parameter ffl defines threshold acceptability pivot elements candidate pivot element smaller threshold algorithm simply skips step factorization algorithm modchol outputs approximate lower triangular factor l index set j ae containing indices skipped pivots following specification use denote unfactored part remains steps algorithm algorithm modchol given ffl igamma1 skip elimination step im else perform usual cholesky elimination step l ki ith column l zero 2 j denote denote complement j f1 j follows 17 row column index nonzero element e must lie j follows algorithm l exact cholesky factor perturbed matrix denote convenience partitioning equation j j components using 19 obtain 21a exact cholesky factor l whose existence guaranteed assumed positive definiteness satisfies given linear system matrix factored modchol exact solution obviously satisfies approximate solution z chosen partial vector z j solves reduced system z j complementary subvector z j set zero 21a see z j calculated performing pair triangular substitu tions z z note z z hand difference z z large relative sense z z reason expect z j small respect full vector z show however difference l z l z relatively small certain assumptions result culmination analysis section theorem 36 see section 5 difference determines usefulness computed solution 16 search direction interiorpoint algorithm simplify analysis assume implicitly throughout paper trivial scaling affects neither algorithm analysis always applied symmetric positive definite matrix yield 26 start sequence three results lead bound difference z results require assumptions matrix relatively simple prove lemma 31 submatrix formed last columns symmetric positive definite 1 moreover diagonal elements submatrices bounded 1 proof observation follows simple inductive argument assumption starting matrix positive definite suppose desired property holds igamma1 2 j lower right gamma theta gamma submatrix identical lower right gamma theta gamma submatrix igamma1 positive definite assumption otherwise obtained applying one step cholesky reduction igamma1 known remaining submatrix resulting operation positive definite hence lower right question positive definite desired property holds second claim follows immediately fact ii fact diagonal elements cannot increase algorithm modchol lemma 32 2 j therefore proof lemma 31 igamma1 ffl since diagonals submatrix igamma1 bounded 1 igamma1 il hence thereby proving first claim 18 thereby proving 27 case small pivots appear bottom right corner matrix index p estimate 27 improved stronger estimate applies instances interiorpoint application section 5 able derive estimate difference theorem 33 exact solution z approximate solution z defined 24 25 respectively proof 24 together 21 jj z j jj z j 25 z j z j j z combining two relations obtain since result follows immediately remaining analysis section requires additional assumptions distribution singular values parameter ffl accordingly introduce little notation eigenvalues denoted oe 2 define diagonal matrix sigma follows exists orthogonal matrix q largest diagonal 1 elementary analysis subsequent analysis assume integer p 1 ffl ffl small relative oe 2 significant gap spectrum oe 2 p p1 specific two assumptions presently partitioning spectrum gap obtain 33 q partitioned accordingly obtain singular values l fact must orthogonal matrix u sigma q defined use denote eigenvalues perturbed matrix follows immediately 20 singular values oe rank j j j lower triangular nonzero diagonals therefore 36 orthogonal theta matrices u q u immediate consequence eigenvalue perturbation result stewart sun 10 corollary iv413 lemma 32 main assumption section j correctly identifies numerical rank matrix one might expect assume equality allthat follow spectrum gap judicious choice ffl practical experience supports expectation algorithm little trouble determining numerical rank vast majority problems fact part resultthe bound j pfollows minimal assumption ffl lemma 34 proof 37 39 contradicting assumption ffl 12 oe 2 however conditions ffl oe p oe p1 needed prove half resultj rigorous useful consequence fact poorly conditioned triangular matrices need particularly small diagonal elements see lawson hanson 5 p 31 classic example phenomenon next result concerns perturbation subspace spanned q 1 invariant subspace large eigenvalues lemma 35 suppose j values oe p oe p1 31 ffl lemma 32 satisfy conditions 40a 40b p theta p symmetric positive definite matrix orthonormal theta p matrix constants used 40a similar expressions taken seriously assign specific values avoid excess notation proof result straightforward consequence theorem v28 stewart sun 10 p 238 since use 33 partition 35 obtain make following identifications quantities cited result sepdelta delta minimum distance spectra two arguments given result matrix p dimension gamma p theta p matrix defined invariant subspace moreover representation respect bound 42 follows 44 45 kq 2 follows immediately first equality 46 symmetric verifying inequality 43 inequality implies smallest singular value smaller oe 2 symmetric positive definite cited result states matrix orthogonal defines invariant subspace fact gamma p theta gamma p symmetric matrix since rank b must 0 hence 41 also satisfied proof complete combining 40b 39 obtain another quantity enters error bounds norm j denote j jth singular value j lower bound 1 simplifies analysis note 21a 1 34 49 assumption j nonzero part lthe submatrix j full rank p singular values oe oe p since j differs j presence additional rows j therefore additional rows j nontrivial magnitude relative j oe p may significantly larger gamma1 however oe p cannot large since 39 40b 34 purposes analysis make assumption p moderate size specifically assume 39 40b implies addition 3 prove main result section theorem 36 suppose j conditions 40 hold estimate 51 satisfied proof 36 since u orthogonal partition 35 using fact kq 2 unless course sigma 2 q 2 vacuous obtain first term expression easiest bound 35 ksigma gamma1 applying relations 41 20 38 47 29 27 48 respectively obtain therefore second third terms 53 require bound kz gamma zk 30 fact z therefore kz j k 47 k 27 substituting estimates 56 using 52 obtain finally using z together 1 34 57 obtain turning specifically second term 53 34 lemma 35 47 40 oe 22ffl 12 combining bound 58 ksigma gamma1 obtain third term 53 ksigma 2 result theorem obtained substituting 54 59 60 53 note conclusion theorem 36 holds trivially case well define oe 4 effect finite precision computations analysis preceding section assumed simplicity arithmetic exact section take account roundoff errors introduced approximate solution z calculated finiteprecision environment analysis focused approximate solution z obtained 25 subvector z j satisfies following system z subvector z j fixed zero section use z denote finite precision analog z examine errors z due ffl roundoff error algorithm modchol ffl error arising triangular substitutions 61 ffl evaluation error righthand side r see section 5 evaluation error righthand side significant feature application interiorpoint codes denote error e righthand side r j system 61 replaced r j fortunately results follow straightforward way existing results cholesky factorization since close inspection algorithm modchol shows simply performs standard cholesky factorization submatrix j stating main results introduce two assumptions first concerns relative sizes u specifically fl m1 defined section 1 since 1 1 follows immediately second assumption finite precision affect cutoff decisions algorithm modchol presence roundoff error submatrix igamma1 affect whether threshold criterion igamma1 ii fi ffl passes fails assumption concerns relative sizes u ffl requires ex planation cannot expect take care borderline cases candidate pivots fall one side threshold rather want cases clear distinction small large pivots exact arithmetic retain distinction finite precision arithmetic want threshold fi ffl fall comfortably inside gap settings finite precision size rounding error introduced igamma1 ii earlier steps algorithm mod chol comparable fiu time ii updated algorithm positive number larger subtracted since jm ii j fi floatingpoint error introduced bounded fiu want errors smaller threshold fi ffl pivots tiny exact arithmetic exceed threshold finite precision hence state assumption roughly follows following lemma accounts effects finite precision approximate solution z obtained algorithm modchol 25 lemma 41 suppose algorithm modchol triangular substitutions 61 performed finiteprecision arithmetic perturbed righthand side j yield approximate solution z suppose 62 holds roundoff error affect composition j z exact solution 23 proof algorithm modchol operates standard cholesky factorization j apply standard perturbation theorem bound error subvector z j higham 4 theorem 104 find z j satisfies comparing 66 61 find z manipulating usual way obtain z follows immediately 67 combining 50 62 63 obtain denominator 68 bounded 5 hence substitution 68 using 34 49 69 63 finally bound kz j k terms kzk 34 57 combining bound 70 obtain result major results sections 3 4 summarized following theorem theorem 42 suppose algorithm modchol triangular substitutions 61 performed finiteprecision arithmetic perturbed righthand side j yield approximate solution z suppose 62 holds roundoff errors affect composition j finally suppose either conditions 40 hold estimate 51 satisfied ae oe proof result immediate lemma 41 z remaining case obtain 71 combining results theorem 36 lemma 41 need note kz j k kzk 34 zk 5 application interiorpoint algorithm section return motivating application primaldual interiorpoint software linear programming particular linear system 16 solved iteration apply main resulttheorem 42and examine effect parameter ffl unit roundoff u quality computed search direction c deltax c delta c deltas focus later iterations interiorpoint algorithm small illconditioning ad 2 become acute results show errors arise c deltax c delta c deltas effect errors step length convergence algorithm accuracy attained algorithm also suggest appropriate size parameter ffl section revert informal style analysis using order notation hide constants moderate size thus j two positive numbers write ratio ji large similarly oj conventionally order notation used j quantities approach zero limit algorithm question however use connection unit roundoff u small fixed slight abuse notation results much clearer insight behavior algorithm modchol interiorpoint context next subsection look closely affinescaling step r xs defined 9 step important closely approximates steps taken rapidly converging algorithms final iterations subsection 52 shows steps calculated final stages mehrotras predictor corrector algorithm therefore interiorpoint codes essentially properties affinescaling steps 51 affinescaling steps start estimating sizes various constituents equations 16the residuals r b r c b n components x diagonal matrix standard infeasibleinteriorpoint algorithms see example wright 15 chapter 6 estimates also observed hold practice majority problems values greater u 12 immediate consequence estimates definition 15 assume coefficient matrix well conditioned oe 1 omegagammah1 assume submatrix deltab columns deltai well conditioned follows assumption together estimate 73 matrix deltab 2 deltab full rank minjbj fact since deltab well conditioned nonzero singular values deltab 2 deltab follows 15 73 deltan 2 conclude 74a since largest diagonal element ad 2 scaled coefficient matrix 16a consistency section 3 singular values matrix 75 denoted oe 2 definition together 74 75 deduce 76a recalling notation p section 3 case exact cholesky factor l see sections 3 suppose algorithm modchol used compute solution 16a righthandside component r xs set affinescaling value xs1 process result computed solution c delta aff 16a remaining step components c deltas aff c deltax aff obtained substitution 16b 16c respectively finiteprecision arithmetic main tool analyzing errors computed step theorem 42 consider exact affine scaling step deltax aff delta aff deltas aff standard results methods see example 15 theorem 75 together conditions 72 imply estimate holds falls datadependent threshold ffla b c defined wright 15 chapter 3 16b 72 follows assumptions well conditioning delta specific sizes critical components deltax aff deltas aff multiply third block row 7 xs gamma1 use definition 9 obtain deltax aff deltas aff therefore 72 78 2 n deltax aff therefore using 72 deltax aff similar way obtain deltas aff estimates 78 80 81 show nearunit step taken along direction deltax aff delta aff deltas aff without violating positivity x components substituting deltax delta verify estimate suppose 81 follows 72 corresponding component x 72 78 x deltax aff hence sufficiently small ff 2 0 1 logic applied remaining indices 2 n thereby completing verification 82 returning computed affinescaling step c deltax aff delta aff deltas aff apply theorem 42 checking assumptions satisfied small enough reasonable values u ffl doubleprecision computations hence since well conditioned expect condition 62 hold nonpathological circumstances 76 assumption 40a singular value distribution clearly holds sufficiently small condition 40b satisfied reasonable choice ffl assumption algorithm modchol correctly identifies numerical rank discussed section 3 difficult guarantee observed hold problems tested assumption rounding errors interfere makeup small pivot index set j likewise impossible verify rigorously discussed section 4 reasonably expected hold ffl u 64 good choice fflone satisfies assumptions mentioned keeping bound 71 small possibleis therefore generality continue use ffl ffl analysis follows substituting specific value 83 end verified reasonably expect theorem 42 hold system 16a estimate quantities righthand side 71 76a oe 1 oe o1 76b oe general estimate definition fl m1 gives estimate fl need account errors incurred evaluating righthand side 16a floatingpoint error forming r xs xs1 ou magnitude since single floatingpoint multiplication needed calculate component vector element see 72 residuals r b r c magnitude exact arithmetic see 72 calculated differences o1 quantities contain evaluation error absolute magnitude ou specifically componentwise errors computed version r c bounded u similarly r b estimate 73 errors r c magnified gamma1 u multiply ad 2 16a fact term dominant one total righthandside evaluation error errors occur perform floatingpoint addition terms r b ad 2 r c gamma1 r xs less significant lead additional terms sizes ou gamma1 u 2 summary total righthandside evaluation error gamma1 u hence scaling factor ae defined 75 e error vector section 4 substituting estimates 76 79 84 71 delta aff reasonable estimate cholesky factorization correctly identifies numerical rank deltab well conditioned error bound simplifies delta aff 77 ae orthogonal matrix q since orthogonal transformations affect euclidean norm vector substitute ae 12 da l 86 use 75 write delta aff delta aff note 58 65 79 84 delta aff delta aff delta aff delta aff delta aff approximate solution would obtained algorithm mod chol used solve 16a exact arithmetic next examine effect error c delta aff evaluation error righthand side 16b calculated step c deltas aff 79 88 delta aff delta aff hence taking account ou evaluation error term r c immediately 16b deltas deltas aff delta aff clearly large components snamely 2 n componentserrors magnitude affect step length ff max boundary defined 14 however critical components 2 b estimate 90 good enough guarantee ff max close 1 repeating argument follows 82 find fortunately refined estimate error b components available 90 deltas deltas aff delta aff 87 delta aff 73 b 91 obtain c deltas aff discussion following 82 find deltas aff possible estimate suggests nearunit steps taken least c deltas aff com ponents provided significantly larger u bets finally estimate errors computed version deltax aff obtained 16c estimate effect ff max consider components separately b ou evaluation error r xs magnified term gamma1 replacement deltas aff c deltas aff yields additional error size offl also magnified arithmetic errors less significant summary find c deltax aff usual reasoning find x deltax aff satisfying 94 ou evaluation error r xs magnified appreciably 90 u error deltas aff actually diminished multiplication find c deltax aff hence x deltax aff 94 97 conclude value ff max defined 14 calculated direction c deltax aff delta aff deltas aff replacing exact search direction satisfies estimate note 89 90 96 absolute sense errors c delta aff c deltas aff c deltax aff small contrast gamma1 u term 95 implies errors c deltax aff may become large 0 large errors may turn cause residuals r b grow 0 expectations confirmed computational experiments section 6 estimate 98 parameter choice suggest strongly algorithm terminated reaches threshold three terms estimate 98 balance threshold gamma1 u term c deltax aff may cause r b grow making reduction counterproductive convergence tolerances used interiorpoint codesarrived practical experience rather theoretical considerationsare similar 99 code pcx typical declares optimality following three conditions satisfied default value tol 10 gamma8 note 10 gamma8 u 12 double precision arithmetic machines 52 mehrotra predictorcorrector steps analyzed affinescaling search direction calculated approximation turn attention briefly search direction used mehrotras predictorcorrector algorithm mentioned section 2 steps obtained setting r xs 12 heuristic choice centering parameter write search direction deltax cc delta cc deltas cc correctorcentering step component satisfies following linear system4 0 delta cc deltas cc5 4 block elimination system yields following special case 16a since assume full rank since diagonal elements strictly positive coefficient matrix invertible result stewart 9 todd 11 states norm kad 2 bounded independently set positive definite diagonal matrices therefore independently x x 0 therefore 72 kx 78 follows kdeltax aff deltas aff 2 hence typical heuristic choosing centering parameter set aff value results full steptoboundary ff max along affinescaling direction search direction exact heuristic yields use calculated direction c deltax aff delta aff deltas aff together estimate 98 leads us expect case provided u 12 hence 101 kdelta cc 100 79 delta component mehrotra search direction also apply stewarttodd result formulae deltax cc deltas cc show kdeltax cc deltas cc therefore corresponding 78 estimates 102 103 analysis preceding subsection applied without modification calculated version search direction 100 particular redefine steptoboundary ff max terms calculated deltax c delta c deltas find estimate 98 still applies conclude nearunit steps still taken along direction provided u 12 6 implementation computational results interiorpoint codes use modified cholesky algorithms essentially properties algorithm modchol differ slightly however implementation ipmos code xu hung ye 16 replaces small pivot elements 1 fills corresponding column cholesky factor zeros also inserts zero righthand side criterion identifying small pivot explained reference 16 otherwise strategy equivalent algorithm modchol zhangs lipsol code 17 pcx code czyzyk mehrotra wright 1 replace small pivots huge number10 128 otherwise leave cholesky algorithm unchanged net effect however almost equivalent algorithm modchol triangular substitution procedure 25 advantage approach involves minimal changes standard sparse cholesky code need add loop calculate largest diagonal element fi small pivot check immediately point computation l ii performed test analysis paper reflected practical computations coded primaldual algorithm used algorithm modchol conjunction formulation 16 code used solve small random linear programs amount degeneracythe composition index sets b n carefully controlled iterate monitored various quantities compared estimates section 5 linear programming test problems posed standard form 2 12 matrix fully dense elements random variables drawn uniform distribution interval 0 1 course values 1 2 different element matrix reasonably expect matrix satisfy wellconditioning assumptions section 5 user specifies number indices appear b set primal solution x constructed x randomly drawn uniform distribution 0 1 choose dual solution vector 1 fix optimal dual slack vector random finally set code implementation infeasibleinteriorpoint algorithm described wright 13 details algorithm unimportant need note iterates satisfy estimates 72 exact arithmetic algorithm takes steps along affine scaling direction later iterations iteration algorithm calculated affine scaling direction whether actually used search direction printed norms k c deltax aff k1 k c delta aff k1 k c deltas aff k1 alongside duality measure residual norm kr b r c k 1 current point also kept track number small pivots encountered factorization number elements j parameter ffl set gamma12 100u sparcstation 5 used experiments results particularly sensitive parameter results shown tables 14 iteration algorithm tables list number small pivots jj j base10 logarithms kr affinescaling step norms mentioned steptoboundary ff max along calculated affinescaling direction also tabulated horizontal line table indicates iterate termination occurs according criterion 99 table 1 chose making linear program nondegenerate primaldual solution unique clear c delta aff c deltas aff satisfy estimates 88 90 respectively even algorithm continues past point normal termination component c deltax aff hand clearly shows influence gamma1 u error term 95 becomes comparable smaller u note error c deltax aff transmitted residual r b succeeding iterations effect become destructive much smaller normal termination threshold values ff max also consistent estimate 98 step length approaches 1 normal point termination reached errors c deltax aff r b make progress impossible table 2 shows interesting case choose 4 coefficient matrix 16a four singular values two second column shows algorithm modchol correctly identifies numerical rank last iterations interiorpoint algorithm continues generate useful steps make good progress even encounters small pivots apart feature behavior table 1 errors c deltax aff causing interiorpoint algorithm behave poorly permitted run past normal point termination noted iterations small pivots bottom right corner 28 rather general estimate 27 applies perturbation matrix e case replace ffl 12 ffl estimates section 5 93 95 98 table 3 illustrates another case 4 added complication rank deficient forced rank deficiency setting first second rows contain single nonzero last column 2 2 pivot skipped every invocation algorithm modchol becomes small final pivot skipped well numerical rank correctly determined since small pivots localized bottom right corner special bound 28 apply cannot strengthen bounds step components previous paragraph computational behavior qualitatively tables 1 2 table 4 illustrates problem 8 coefficient matrices retain full numerical rank iterates behavior similar reported table 1 one point difference errors c deltax aff start increase iteration 19 immediate effect residual r b reason simply particular interiorpoint algorithm chose take pathfollowing step iterations 21 22 rather affine scaling step deltax components calculated accurately path following step affinescaling step however taken iteration 28 effect error c deltax aff residual r b following iterate obvious r technical report otc 9601 stability symmetric illconditioned systems arising interior methods constrained optimization computer solution large sparse positive definite systems accuracy stability numerical algorithms solving least squares problems computational experience primaldual interior point method linear programming block sparse cholesky algorithms advanced uniprocessor com puters scaled projections pseudoinverses matrix perturbation theory dantzigwolfelike variant karmarkars interiorpoint linear programming algorithm properties hessian logarithmic barrier function simplified homogeneous selfdual linear programming algorithm implementation solving largescale linear programs interiorpoint methods matlab enviroment tr ctr francis r bach michael jordan kernel independent component analysis journal machine learning research 3 p148 312003