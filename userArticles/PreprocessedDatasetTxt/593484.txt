parallel formulations decisiontree classification algorithms classification decision tree algorithms used extensively data mining many domains retail target marketing fraud detection etc highly parallel algorithms constructing classification decision trees desirable dealing large data sets reasonable amount time algorithms building classification decision trees natural concurrency difficult parallelize due inherent dynamic nature computation paper present parallel formulations classification decision tree learning algorithm based induction describe two basic parallel formulations one based synchronous tree construction approach based partitioned tree construction approach discuss advantages disadvantages using methods propose hybrid method employs good features methods also provide analysis cost computation communication proposed hybrid method moreover experimental results ibm sp2 demonstrate excellent speedups scalability b introduction classification important data mining problem classification problem input dataset called training set consists number examples number attributes attributes either continuous attribute values ordered categorical attribute values un ordered one categorical attributes called class label classifying attribute objective use training dataset build model class label based attributes model used classify new data training dataset application domains include retail target marketing fraud detection design telecommunication service plans several classification models like neural networks 17 genetic algorithms 11 decision trees 20 proposed decision trees probably popular since obtain reasonable accuracy 9 relatively inexpensive compute current classification algorithms c45 20 sliq 18 based id3 classification decision tree algorithm 20 data mining domain data processed tends large hence highly desirable design computationally efficient well scalable algorithms one way reduce computational complexity building decision tree classifier using large training datasets use small sample training data methods yield classification accuracy decision tree classifier uses entire data set 24 5 6 7 order get reasonable accuracy reasonable amount time parallel algorithms may required classification decision tree construction algorithms natural concurrency node generated children classification tree generated concurrently furthermore computation generating successors classification tree node also decomposed performing data decomposition training data nevertheless parallelization algorithms construction classification tree challenging following reasons first shape tree highly irregular determined runtime furthermore amount work associated node also varies data dependent hence static allocation scheme likely suffer major load imbalance second even though successors node processed concurrently use training data associated parent node data dynamically partitioned allocated different processors perform computation different nodes high cost data movements data partitioned appropriately performance bad due loss locality paper present parallel formulations classification decision tree learning algorithm based induction describe two basic parallel formulations one based synchronous tree construction approach based partitioned tree construction approach discuss advantages disadvantages using methods propose hybrid method employs good features methods also provide analysis cost computation communication proposed hybrid method moreover experimental results ibm sp2 demonstrate excellent speedups scalability 2 related work 21 sequential decisiontree classification algorithms existing inductionbased algorithms like c45 20 cdp 1 sliq 18 sprint 21 use hunts method 20 basic algorithm recursive description hunts method constructing decision tree set training cases classes denoted fc 1 g case 1 contains cases belonging single class c j decision tree leaf identifying class c j case 2 contains cases belong mixture classes test chosen based single attribute one mutually exclusive outcomes g note many implementations n chosen 2 leads binary decision tree partitioned subsets contains cases outcome chosen test decision tree consists decision node identifying test one outlook tempf humidity windy class sunny sunny sunny 85 85 false dont play sunny 72 95 false dont play sunny 69 70 false play overcast 72 90 true play overcast 83 78 false play overcast overcast 81 75 false play rain 71 80 true dont play rain rain rain 68 80 false play rain 70 96 false play table 1 small training data set qui93 play dont play play play play dont play dont play play sunny overcast rain outlook sunny overcast rain play outlook humidity nonleaf node expandable leaf node nonexpandable leaf node initial classification tree b intermediate classification tree c final classification tree figure 1 demonstration hunts method branch possible outcome tree building machinery applied recursively subset training cases case 3 contains cases decision tree leaf class associated leaf must determined information example c45 chooses frequent class parent node attribute value class play dont play overcast 4 0 table 2 class distribution information attribute outlook attribute value binary test class play dont play table 3 class distribution information attribute humidity table 1 shows training data set four data attributes two classes figure 1 shows hunts method works training data set case 2 hunts method test based single attribute chosen expanding current node choice attribute normally based entropy gains attributes entropy attribute calculated class distribution information discrete attribute class distribution information value attribute required table 2 shows class distribution information data attribute outlook root decision tree shown figure 1 continuous attribute binary tests involving distinct values attribute considered table 3 shows class distribution information data attribute humidity class distribution information attributes gath ered attribute evaluated terms either entropy 20 gini index 4 best attribute selected test node expansion c45 algorithm generates classificationdecision tree given training data set recursively partitioning data decision tree grown using depthfirst strategy algorithm considers possible tests split data set selects test gives best information gain discrete attribute one test outcomes many number distinct values attribute considered continuous attribute binary tests involving every distinct value attribute considered order gather entropy gain binary tests efficiently training data set belonging node consideration sorted values continuous attribute entropy gains binary cut based distinct values calculated one scan sorted data process repeated continuous attribute recently proposed classification algorithms sliq 18 sprint 21 avoid costly sorting node presorting continuous attributes begin ning sprint continuous attribute maintained sorted attribute list list entry contains value attribute corresponding record id best attribute split node classification tree determined attribute list split according split decision hash table order number training cases mapping record ids record belongs according split decision entry attribute list moved classification tree node according information retrieved probing hash table sorted order maintained entries moved presorted order decision trees usually built two steps first initial tree built till leaf nodes belong single class second pruning done remove overfitting training data typically time spent pruning large dataset small fraction less 1 initial tree generation therefore paper focus initial tree generation pruning part computation 22 parallel decisiontree classification algorithms several parallel formulations classification rule learning proposed cently pearson presented approach combines nodebased decomposition attributebased decomposition 19 shown nodebased decomposition task parallelism alone several probelms one problem processors utilized beginning due small number expanded tree nodes another problem many processors become idle later stage due load imbalance attributebased decomposition used remedy first problem number expanded nodes smaller available number processors multiple processors assigned node attributes distributed among processors approach related nature partitioned tree construction approach discussed paper partitioned tree construction approach actual data samples partitioned horizontal parti tioning whereas approach attributes partitioned vertical partitioning 8 general approaches parallelizing c45 discussed dynamic task distribution dtd scheme master processor allocates subtree decision tree idle slave processor scheme require communication among processors suffers load imbalance dtd becomes similar partitioned tree construction approach discussed paper number available nodes decision tree exceeds number processors dprec scheme distributes data set evenly builds decision tree one node time scheme identical synchronous tree construction approach discussed paper suffers high communication overhead dpatt scheme distributes attributes scheme advantages loadbalanced requiring minimal communications however scheme scale well increasing number processors results 8 show effectiveness different parallelization schemes varies significantly data sets used kufrin proposed approach called parallel decision trees pdt 15 approach similar dprec scheme 8 synchronous tree construction approach discussed paper data sets partitioned among proces sors pdt approach designate one processor host processor remaining processors worker processors host processor data sets receives frequency statistics gain calculations worker processors host processor determines split based collected statistics notify split decision worker processors worker processors collect statistics local data following instruction host processor pdt approach suffers high communication overhead like dprec scheme synchronous tree construction approach pdt approach additional communication bottleneck every worker processor sends collected statistics host processor roughly time host processor sends split decision working processors time parallel implementation sprint 21 scalparc 13 use methods partitioning work identical one used synchronous tree construction approach discussed paper serial sprint 21 sorts continuous attributes beginning keeps separate attribute list record identifiers splitting phase decision tree node maintains sorted order without requiring sort records order split attribute lists according splitting decision sprint creates hash table records mapping record identifier node goes based splitting decision parallel implementation sprint attribute lists split evenly among processors split point node decision tree found parallel however order split attribute lists full size hash table required processors order construct hash table alltoall broadcast 16 performed makes algorithm unscalable respect runtime memory requirements reason processor requires memory store hash table communication overhead alltoall broadcast n number records data set recently proposed scalparc 13 improves upon sprint employing distributed 7hash table efficiently implement splitting phase sprint scalparc hash table split among processors efficient personalized communication used update hash table making scalable respect memory runtime requirements goil aluru ranka proposed concatenated parallelism strategy efficient parallel solution divide conquer problems 10 strategy mix data parallelism task parallelism used solution parallel divide conquer algorithm data parallelism used enough subtasks genearted task parallelism used ie processor works independent subtasks strategy similar principle partitioned tree construction approach discussed paper concatenated parallelism strategy useful problems workload determined based size subtasks task parallelism employed however problem classificatoin decision tree workload cannot determined based size data particular node tree hence one time load balancing used strategy well suited particular divide conquer problem 3 parallel formulations section give two basic parallel formulations classification decision tree construction hybrid scheme combines good features approaches focus presentation discrete attributes handling continuous attributes discussed section 34 parallel formulations assume n training cases randomly distributed p processors initially processor np cases 31 synchronous tree construction approach approach processors construct decision tree synchronously sending receiving class distribution information local data major steps approach shown 1 select node expand according decision tree expansion strategy eg depthfirst breadthfirst call node current node beginning root node selected current node 2 data attribute collect class distribution information local data current node 3 exchange local class distribution information using global reduction 16 among processors 4 simultaneously compute entropy gains attribute processor select best attribute child node expansion 5 depending branching factor tree desired create child nodes number partitions attribute values split training cases accordingly class distribution information class distribution information figure 2 synchronous tree construction approach depthfirst expansion strategy 6 repeat steps 15 nodes available expansion figure 2 shows overall picture root node already expanded current node leftmost child root shown top part figure four processors cooperate expand node two child nodes next leftmost node child nodes selected current node bottom figure four processors cooperate expand node advantage approach require movement training data items however algorithm suffers high communication cost load imbalance node decision tree collecting class distribution information processors need synchronize exchange distribution information nodes shallow depth communication overhead relatively small number training data items processed relatively large decision tree grows deepens number training set items nodes decreases consequence computation class distribution information nodes decreases average branching factor decision tree k number data items child node average 1 th number data items parent however size communication decrease much number attributes considered goes one hence tree deepens communication overhead dominates overall processing time problem due load imbalance even though processor started number training data items number items belonging node decision tree vary substantially among processors example processor 1 might data items leaf node none leaf node b processor 2 might data items node b none node node selected current node processor 2 work similarly node b selected current node processor 1 work load imbalance reduced nodes frontier expanded simultaneously ie one pass data processor used compute class distribution information nodes frontier note improvement also reduces number times communications done reduces message startup overhead reduce overall volume communications rest paper assume synchronous tree construction algorithm classification tree expanded breadthfirst manner nodes level processed time 32 partitioned tree construction approach approach whenever feasible different processors work different parts classification tree particular one processors cooperate expand node processors partitioned expand successors node consider case group processors pn cooperate expand node n algorithm consists following steps processors pn cooperate expand node n using method described section 31 node n expanded successor nodes processor group pn also partitioned successor nodes assigned processors follows case 1 number successor nodes greater jp n j 1 partition successor nodes jp n j groups total number training cases corresponding node group roughly equal assign processor one node group 2 shuffle training data processor data items belong nodes responsible 3 expansion subtrees rooted node group proceeds completely independently processor serial algorithm case 2 otherwise number successor nodes less jp n j 1 assign subset processors node number processors assigned node proportional number training cases corresponding node data item data item figure 3 partitioned tree construction approach 2 shuffle training cases subset processors training cases belong nodes responsible 3 processor subsets assigned different nodes develop subtrees inde pendently processor subsets contain one processor use sequential algorithm expand part classification tree rooted node assigned processor subsets contain one processor proceed following steps recursively beginning processors work together expand root node classification tree end whole classification tree constructed combining subtrees processor figure 3 shows example first top figure four processors cooperate expand root node like synchronous tree construction 1approach next middle figure set four processors partitioned three parts leftmost child assigned processors 0 1 nodes assigned processors 2 3 respectively sets processors proceed independently expand assigned nodes par ticular processors 2 processor 3 proceed expand part tree using serial algorithm group containing processors 0 1 splits leftmost child node three nodes three new nodes partitioned two parts shown bottom figure leftmost node assigned processor 0 two assigned processor 1 processors 0 1 also independently work respective subtrees advantage approach processor becomes solely responsible node develop subtree classification tree independently without communication overhead however number disadvantages approach first disadvantage requires data movement node expansion one processor becomes responsible entire subtree communication cost particularly expensive expansion upper part classification tree note number nodes frontier exceeds number processors communication cost becomes zero second disadvantage poor load balancing inherent algorithm assignment nodes processors done based number training cases successor nodes however number training cases associated node necessarily correspond amount work needed process subtree rooted node example training cases associated node happen class label expansion needed 33 hybrid parallel formulation hybrid parallel formulation elements schemes synchronous tree construction approach section 31 incurs high communication overhead frontier gets larger partitioned tree construction approach section 32 incurs cost load balancing step hybrid scheme keeps continuing first approach long communication cost incurred first formulation high cost becomes high processors well current frontier classification tree partitioned two parts description assumes number processors power 2 processors connected hypercube configuration algorithm appropriately modified p power 2 also algorithm mapped parallel architecture simply embedding virtual hypercube architecture precisely hybrid formulation works follows ffl database training cases split equally among p processors thus n total number training cases processor np training cases locally beginning processors assigned one partition root node classification tree allocated partition ffl nodes frontier tree belong one partition processed together using synchronous tree construction approach section 31 ffl depth tree within partition increases volume statistics gathered level also increases discussed section 31 point level reached communication cost become prohibitive point processors partition divided two partitions current set frontier nodes split allocated partitions way number training cases partition roughly equal load balancing done described follows hypercube two partitions naturally correspond sub cube first corresponding processors within two subcubes exchange relevant training cases transferred subcube exchange processors within subcube collectively training cases partition number training cases processor vary 0 2lambdan load balancing step done within subcube processor equal number data items ffl processing within partition proceeds asynchronously steps repeated one partitions particular subtrees process repeated complete classification tree grown ffl group processors partition become idle partition joins partition work number processors done simply giving half training cases located processor donor partition processor receiving partition computation frontier depth 3 figure 4 computation frontier computation phase key element algorithm criterion triggers partitioning current set processors corresponding frontier classification tree partitioning done frequently hybrid scheme approximate partitioned tree construction approach thus incur much data movement cost partitioning done late suffer high cost communicating statistics generated node frontier like synchronized tree construction approach one possibility splitting accumulated cost communication becomes equal cost moving records around splitting phase precisely splitting done partition 1 partition 2 figure 5 binary partitioning tree reduce communication costs communication movingcost loadbalancing example hybrid algorithm figure 4 shows classification tree frontier depth 3 far partitioning done processors working cooperatively node frontier next frontier depth 4 partitioning triggered nodes processors partitioned two partitions shown figure 5 detailed analysis hybrid algorithm presented section 4 34 handling continuous attributes note handling continuous attributes requires sorting processor contains np training cases one approach handling continuous attributes perform parallel sorting step attribute node decision tree constructed parallel sorting completed processor compute best local value split simple global communication among processors determine globally best splitting value however step parallel sorting would require substantial data exchange among processors exchange information similar nature exchange class distribution information except much higher volume hence even case useful use scheme similar hybrid approach discussed section 33 efficient way handling continuous attributes without incurring high cost repeated sorting use presorting technique used algorithms sliq 18 sprint 21 scalparc 13 algorithms require one presorting step need construct hash table level classification tree parallel formulations algorithms content hash table needs available globally requiring communication among processors definition total number training samples total number processors number processors cooperatively working tree expansion number categorical attributes c number classes average number distinct values discrete attributes present level decision tree start time communication latency kggk94 w perword transfer time communication latency kggk94 table 4 symbols used analysis existing parallel formulations schemes 21 13 perform communication similar nature synchronous tree construction approach discussed section 31 communication formulations 21 13 reduced using hybrid scheme section 33 another completely different way handling continuous attributes discretize preprocessing step 12 case parallel formulations presented previous subsections directly applicable without modification another approach towards discretization discretize every node tree two examples approach first example found 3 quantiles 2 used discretize continuous attributes second example approach discretize node spec 23 clustering technique used spec shown efficient terms runtime also shown perform essentially identical several widely used tree classifiers terms classification accuracy 23 parallelization discretization every node tree similar nature parallelization computation entropy gain discrete attributes methods discretization require global communication among processors responsible node particular parallel formulations clustering step spec essentially identical parallel formulations discrete case discussed previous subsections 23 4 analysis hybrid algorithm section provide analysis hybrid algorithm proposed section 33 give detailed analysis case discrete attributes present analysis case continuous attributes found 23 detailed study communication patterns used analysis found 16 table 4 describes symbols used section 41 assumptions ffl processors connected hypercube topology complexity measures topologies easily derived using communication complexity expressions topologies given 16 ffl expression communication computation written full binary tree 2 l leaves depth l expressions suitably modified tree full binary tree without affecting scalability algorithm ffl size classification tree asymptotically independent n particular data set assume tree represents knowledge extracted particular training data set increase training set size beyond point lead larger decision tree 42 computation communication cost leaf level class histogram tables need com municated size tables product number classes mean number attribute values thus size class histogram table processor leaf class histogram size number leaves level l 2 l thus total size tables combined class histogram tables processor c 2 l level l local computation cost involves io scan training set initialization update class histogram tables attribute local computation cost n c unit computation cost end local computation processor synchronization involves global reduction class histogram values communication cost 1 per level communication cost 2 processor partition split two leaf assigned one partitions way number training data items two partitions approximately order two partitions work independently training set moved around training cases leaf assigned processor partition load balanced system processor partition must n training data items movement done two steps first processor first partition sends relevant training data items corresponding processor second partition referred moving phase processor send receive maximum n data corresponding processor partition cost moving phase 2 internal load balancing phase inside partition takes place every processor equal number training data items moving phase load balancing phase starts processor training data item count varying 0 2lambdan processor send receive maximum n training data items assuming congestion interconnection network cost load balancing cost load balancing phase 2 detailed derivation equation 4 given 23 also cost load balancing assumes network congestion reasonable assumption networks bandwidthrich case commercial systems without assuming anything network congestion load balancing phase done using transportation primitive 22 time 2 n splitting done accumulated cost communication becomes equal cost moving records around splitting phase 14 splitting done communication cost moving cost balancing criterion splitting ensures communication cost scheme within twice communication cost optimal scheme 14 splitting recursive applied many times required splitting done computations applied partition partition processors starts idle sends request busy partition idle state request sent partition processors roughly size idle partition next round splitting idle partition included part busy partition computation proceeds described 43 scalability analysis isoefficiency metric found useful metric scalability large number problems large class commercial parallel computers 16 defined follows let p number processors w problem size total time taken best sequential algorithm w needs grow maintain efficiency e fe p defined isoefficiency function efficiency e plot fe p respect p defined isoefficiency curve efficiency e assume data classified tree depth l 1 depth remains constant irrespective size data since data fits particular classification tree total cost creating new processor subpartitions product total number partition splits cost partition split n using equations 3 4 number partition splits processor participates less equal l 1 depth tree cost creating new processors partitions l 1 communication cost level given equation 2 log p combined communication cost product number levels communication cost level combined communication cost processing attributes l 1 log p total communication cost sum cost creating new processor partitions communication cost processing class histogram tables sum equations 5 6 total communication cost computation cost given equation 1 total computation total parallel run time sum equations 7 8 communication time computation time parallel run serial case whole dataset scanned level serial time get isoefficiency function equate p times total parallel run time using equation 9 serial computation time therefore isoefficiency function isoefficiency p log p assuming network congestion load balancing phase transportation primitive used load balancing isoefficiency op 3 5 experimental results implemented three parallel formulations using mpi programming library use binary splitting decision tree node grow tree breadth first manner generating large datasets used widely used synthetic dataset proposed sliq paper 18 experiments ten classification functions also proposed 18 datasets used function 2 dataset algorithms dataset two class labels record consists 9 attributes 3 categoric 6 continuous attributes dataset also used sprint algorithm 21 evaluating performance experiments done ibm sp2 results comparing speedup three parallel formulations reported parallel runs 1 2 4 8 16 processors experiments hybrid approach reported 128 processors processor clock speed 667 mhz 256 mb real memory operating system aix version 4 processors communicate high performance switch hps implementation keep attribute lists disk use memory storing program specific data structures class histograms clustering structures first present results schemes context discrete attributes compare performance three parallel formulations 16 processor ibm sp2 results discretized 6 continuous attributes uniformly specifically discretized continuous attribute salary 13 commission 14 age 6 hvalue 11 hyears 10 loan equal intervals measuring speedups worked different sized datasets 08 million training cases 16 million training cases increased processors 1 16 results figure 6 show speedup comparison three parallel algorithms proposed paper graph left shows speedup 08 million examples training set graph shows speedup 16 million examples results show synchronous tree construction approach good speedup 2 processors poor speedup 4 processors two reasons first synchronous tree construction approach incurs high communication cost processing lower levels tree second synchronization done among different processors soon communication buffer fills communication buffer histograms discrete variables node thus contribution node independent tuples count tuple count node proportional computation process node processing lower levels tree synchronization done many times level every 100 nodes experiments distribution tuples decision tree node becomes quite different lower tree therefore processors wait synchronization thus contribute poor speedups partitioned tree construction approach better speedup synchronous tree construction approach however efficiency decreases number processors increases 8 16 partitioned tree construction approach processors speedup 08 million examples partitioned hybrid synchronous x processors speedup 16 million examples partitioned hybrid synchronous x figure 6 speedup comparison three parallel algorithms suffers load imbalance even though nodes partitioned processor gets equal number tuples simple way predicting size subtree particular node load imbalance leads runtime determined heavily loaded processor partitioned tree construction approach also suffers high data movement partitioning phase partitioning phase taking place higher levels tree processors involved takes longer reach point processors work local data observed experiments load imbalance higher communication order major cause poor performance partitioned tree construction approach number processors increase hybrid approach superior speedup compared partitioned tree approach speedup keeps increasing increasing number processors discussed section 33 analyzed section 4 hybrid controls communication cost data movement cost adopting advantages two basic parallel formulations hybrid strategy also waits long enough splitting large number decision tree nodes splitting among processors due allocation decision tree nodes processor randomized large extent good load balancing possible results confirmed proposed hybrid approach based two basic parallel formulations effective also performed experiments verify splitting criterion hybrid algorithm correct figure 7 shows runtime hybrid algorithm different ratio communication cost sum moving cost load balancing cost ie communication cost moving cost log2splitting criteria ratio x0 ratio1 runtimes runtimes splitting different values ratio 8 processors 08 million examples 10100300500log2splitting criteria ratio x0 ratio1 runtimes runtimes splitting different values ratio 16 processors 16 million examples figure 7 splitting criterion verification hybrid algorithm graph left shows result 08 million examples 8 processors graph shows result 16 million examples processors proposed splitting ratio 10 would optimal time results verified hypothesis runtime lowest ratio around 10 graph right 16 million examples shows clearly splitting choice critical obtaining good performance splitting decision made farther away optimal point proposed runtime increases significantly experiments 16 processors clearly demonstrated hybrid approach gives much better performance splitting criterion used hybrid approach close optimal performed experiments running hybrid approach number processors different sized datasets study speedup scalability experiments used original data set continuous attributes used clustering technique discretize continuous attributes decision tree node 23 note parallel formulation gives almost identical performance serial algorithm terms accuracy classification tree size 23 results figure 8 show speedup hybrid approach results confirm hybrid approach indeed effective study scaleup behavior kept dataset size processor constant 50000 examples increased number processors figure 9 shows runtime increasing number processors curve close ideal case horizontal line deviation ideal case due fact isoefficiency function op log p op current experimental data consistent derived isoefficiency function intend conduct additional validation experiments number processors speedup curves different sized datasets million examples 16 million examples 32 million examples 64 million examples 128 million examples 256 million examples figure 8 speedup hybrid approach different size datasets number processors total run time runtimes algorithm 50k examples processor figure 9 scaleup algorithm 6 concluding remarks paper proposed three parallel formulations inductiveclassification learning algorithm synchronous tree construction approach performs well classification tree remains skinny nodes level throughout trees relatively large number training cases nodes level thus communication overhead relatively small load imbalance avoided processing nodes level synchronization among processors however tree becomes bushy large number nodes level number training data items node decrease frequent synchronization done due limited communication buffer size forces communication processing fixed number nodes nodes lower depths tree tuples assigned may highly variable distribution tuples processors leading load imbalance hence approach suffers high communication overhead load imbalance bushy trees partitioned tree construction approach works better synchronous tree construction approach tree bushy approach pays big communication overhead higher levels tree shuffle lots training data items different processors every node solely assigned single processor processor construct partial classification tree independently without communication processors however load imbalance problem still present shuffling training data items since partitioning data done statically hybrid approach combines good features two approaches reduce communication overhead load imbalance approach uses synchronous tree construction approach upper parts classification tree since nodes relatively large number training cases associated nodes upper part tree communication overhead small soon accumulated communication overhead greater cost partitioning data load balancing approach shifts partitioned tree construction approach incrementally partitioning takes place reasonable number nodes present level partitioning gradual performs randomized allocation classification tree nodes resulting better load balance load imbalance lower levels tree processor group finished processing assigned subtree handled allowing idle processor group join busy processor groups size shape classification tree varies lot depending application domain training data set classification trees might shallow others might deep classification trees could skinny others could bushy classification trees might uniform depth trees might skewed one part tree hybrid approach adapts well types classification trees decision tree skinny hybrid approach stay synchronous tree construction approach hand shift partitioned tree construction approach soon tree becomes bushy tree big variance depth hybrid approach perform dynamic load balancing processor groups reduce processor idling acknowledgments significant part work done anurag srivastava vineet singh ibm tj watson research center work supported nsf grant asc9634719 army research office contract dadaah049510538 cray research inc fellowship ibm partnership award content necessarily reflect policy government official endorsement inferred access computing facilities provided ahpcrc minnesota supercomputer institute cray research inc nsf grant cda9414015 notes 1 message size large routing message parts communication step done constant k0 refer 16 section 37 details r database mining performance perspective clouds classification large outofcore datasets megainduction machine learning large databases experiments multistrategy learning met alearning metalearning multistrategy learning parallel learning concatenated parallelism technique efficient parallel divide conquer genetic algorithms search use contextual information feature ranking discretization new scalable efficient parallel classification algorithm mining large datasets unstructured tree search simd parallel computers decision trees parallel processors introduction parallel computing algorithm design analysis introduction computing neural nets sliq fast scalable classifier data mining coarse grained parallel induction heuristic sprint scalable parallel classifier data mining experiments costs benefits windowing id3 tr ctr amir baror daniel keren assaf schuster ran wolff hierarchical decision tree induction distributed genomic databases ieee transactions knowledge data engineering v17 n8 p11381151 august 2005 robert grossman yike guo data mining tasks methods parallel methods scaling data mining algorithms large data sets handbook data mining knowledge discovery oxford university press inc new york ny 2002 doina caragea adrian silvescu vasant g honavar analysis synthesis agents learn distributed dynamic data sources emergent neural computational architectures based neuroscience towards neuroscienceinspired computing springerverlag new york inc new york ny 2001 olcay taner yldz onur dikmen parallel univariate decision trees pattern recognition letters v28 n7 p825832 may 2007 vipin kumar mohammed zaki high performance data mining tutorial pm3 tutorial notes sixth acm sigkdd international conference knowledge discovery data mining p309425 august 2023 2000 boston massachusetts united states raymond ng alan wagner yu yin icebergcube computation pc clusters acm sigmod record v30 n2 p2536 june 2001 ruggieri efficient c45 ieee transactions knowledge data engineering v14 n2 p438444 march 2002 massimo coppola marco vanneschi parallel distributed data mining parallel skeletons distributed objects data mining opportunities challenges idea group publishing hershey pa aleksandar lazarevic zoran obradovic boosting algorithms parallel distributed learning distributed parallel databases v11 n2 p203229 march 2002 vipin kumar mohammed zaki high performance data mining tutorial pm3 tutorial notes sixth acm sigkdd international conference knowledge discovery data mining p309425 august 2023 2000 boston massachusetts united states massimo coppola marco vanneschi highperformance data mining skeletonbased structured parallel programming parallel computing v28 n5 p793813 may 2002 jack dongarra ian foster geoffrey fox william gropp ken kennedy linda torczon andy white references sourcebook parallel computing morgan kaufmann publishers inc san francisco ca