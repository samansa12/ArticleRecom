tuning compiler optimizations simultaneous multithreading compiler optimizations often driven specific assumptions underlying architecture implementation target machine example targeting sharedmemory multiprocessors parallel programs compiled minimize sharing order decrease highcost interprocessor communication paper reexamines several compiler optimizations context simultaneous multithreading smt processor architecture issues instructions multiple threads functional units cycle unlike sharedmemory multiprocessors smt provides benefits finegrained sharing processor memory system resources unlike current multiprocessors smt exposes benefits interthread instructionlevel parallelism hiding latencies therefore optimizations appropriate conventional machines may inappropriate smt revisit three optimizations light loopiteration scheduling software speculative execution loop tiling results show three optimizations applied differently context smt architectures threads parallelized cyclic rather blocked algorithm nonloop programs software speculated compilers longer need concerned precisely sizing tiles match cache sizes following new guidelines compilers generate code improves performance programs executing smt machines b introduction compiler optimizations typically driven specific assumptions underlying architecture implementation target machine example compilers schedule longlatency operations early minimize critical paths order instructions based processors issue slot restrictions maximize functional unit utilization allocate frequently used variables registers benefit fast access times new processing paradigms change architectural assumptions however must reevaluate machinedependent compiler optimizations order maximize performance new machines simultaneous multithreading smt 323121 13 multithreaded processor design alters several architectural assumptions compilers traditionally relied smt processor instructions multiple threads issue functional units cycle take advantage simultaneous threadissue capability processor resources memory subsystem resources dynamically shared among threads single feature responsible performance gains almost 2x wideissue superscalars roughly 60 singlechip shared memory multiprocessors multiprogrammed spec92 specint95 parallel splash2 specfp95 workloads smt achieves improvement limiting slowdown single executing thread 2 13 simultaneous multithreading presents compiler different model hiding operation latencies sharing code data operation latencies hidden instructions executing threads thread longlatency operation addition multithread instruction issue increases instructionlevel parallelism ilp levels much higher sustained single thread factors suggest reconsidering uniprocessor optimizations copyright 1997 ieee published proceedings micro30 december 13 1997 research triangle park north carolina personal use material permitted however permission reprintrepublish material advertising promotional purposes creating new collective works resale redistribution servers lists reuse copyrighted component work works must obtained ieee contact manager copyrights permissions ieee service center 445 hoes lane po box 1331 piscataway nj hide latencies expose ilp expense increased dynamic instruction counts smt latencyhiding benefits may needed extra instructions may consume resources could better utilized instructions concurrent threads multiple threads reside within single smt processor cheaply share common data incur penalty false sharing fact benefit crossthread spatial locality calls question compilerdriven parallelization techniques originally developed distributedmemory multiprocessors partition data physically distributed threads avoid communication coherence costs smt may beneficial parallelize programs process contiguous data paper investigates extent simultaneous multithreading affects use several compiler optimizations particular examine one parallel technique loopiteration scheduling compilerparallelized applications two optimizations hide memory latencies expose instructionlevel parallelism software speculative execution loop tiling results prescribe different usage three optimizations compiling smt processor found blocked loop scheduling may useful distributing data distributedmemory multiprocessors cyclic iteration scheduling appropriate smt architecture reduces tlb footprint parallel applications since smt threads run single processor share memory hierarchy data shared among threads improve locality memory pages software speculative execution may incur additional instruction overhead conventional wideissue superscalar instruction throughput usually low enough additional instructions simply consume resources would otherwise go unused however smt processor simultaneous multithread instruction issue increases throughput roughly 62 8wide processor software speculative execution degrade performance particularly nonloopbased applications simultaneous multithreading also impacts loop tiling techniques tile size selection smt processors far less sensitive variations tile size conventional processors must find appropriate balance large tiles low instruction overhead small tiles better cache reuse higher hit rates processors eliminate performance sweet spot hiding extra misses larger tiles additional threadlevel parallelism provided multithreading tiled loops smt decomposed threads compute tile rather creating separate tile thread done multiprocessors tiling way raises performance smt processors moderatelysized memory subsystems aggressive designs remainder paper organized follows section 2 provides brief description smt processor section 3 discusses detail two architectural assumptions affected simultaneous multithreading ramifications compilerdirected loop distribution software speculative execution loop tiling section 4 presents experimental methodology sections 5 7 examine compiler optimizations providing experimental results analysis section 8 briefly discusses compiler issues raised smt related work appears section 9 conclude section 10 2 microarchitecture simultaneous multithreading processor smt design eightwide outoforder processor hardware contexts eight threads every cycle instruction fetch unit fetches four instructions two threads fetch unit favors high throughput threads fetching two threads fewest instructions waiting executed fetching instructions decoded registers renamed inserted either integer floating point instruction queues operands become available instructions thread issue functional units execution finally instructions retire perthread program order little microarchitecture needs redesigned enable optimize simultaneous multithreading components integral part conventional dynamicallyscheduled superscalar major exceptions larger register file 32 architectural registers per thread plus 100 renaming registers two additional pipeline stages accessing registers one reading writing instruction fetch scheme mentioned several perthread mechanisms program counters return stacks retirement trap logic identifiers tlb branch target buffer notably missing list special perthread hardware scheduling instructions onto functional units instruction scheduling done conventional outoforder superscalar instructions issued operands calculated loaded memory without regard thread renaming hardware eliminates interthread register name conflicts mapping threadspecific architectural registers onto processors physical registers see 31 details large hardware data structures caches tlbs branch prediction tables shared among threads additional crossthread conflicts caches branch prediction hardware absorbed smts enhanced latencyhiding capabilities 21 tlb interference addressed technique described section 5 rethinking compiler optimizations explained simultaneous multithreading relies novel feature attaining greater processor performance coupling multithreading wide instruction issue scheduling instructions different threads cycle new design prompts us revisit compiler optimizations automatically parallelize loops enhanced memory performance andor increase ilp section discuss two factors affected smts unique design data sharing among threads availability instruction issue slots light three compiler optimizations affect interthread data sharing conventional parallelization techniques target multiprocessors threads physically distributed different processors minimize cache coherence interprocessor communication overhead data loop distribution techniques partition distribute data match physical topology multiprocessor parallelizing compilers attempt decompose applications minimize synchronization communication loops typically achieved allocating disjoint set data processor work independently 34107 contrast smt multiple threads execute processor affecting performance two ways first real false interthread data sharing entail local memory accesses incur coherence overhead smts shared l1 cache consequently sharing even false sharing beneficial second sharing data among threads memory footprint parallel application reduced resulting better cache tlb behavior factors suggest loop distribution policy clusters rather separates data multiple threads latencyhiding capabilities availability instruction issue slots workloads wideissue processors typically cannot sustain high instruction throughput low instructionlevel parallelism single executing thread compiler optimizations software speculative execution loop tiling blocking try increase ilp hiding reducing instruction latencies respectively often side effect increasing dynamic instruction count despite additional instructions optimizations often profitable instruction overhead accommodated otherwise idle functional units issue instructions multiple threads smt processor fewer empty issue slots fact sustained instruction throughput rather high roughly 2 times greater conventional superscalar 13 furthermore smt better job hiding latencies singlethreaded processors uses instructions one thread mask delays another environment aforementioned optimizations may less useful even detrimental overhead instructions compete useful instructions hardware resources smt simultaneous multithreading capabilities naturally tolerates high latencies without additional instruction overhead examining compiler optimizations describe methodology used experiments chose applications spec 92 12 spec 95 30 splash2 35 benchmark suites table 2 programs compiled multiflow trace scheduling compiler 22 generate dec alpha object files multiflow chosen generates highquality code using aggressive static scheduling wide issue loop unrolling ilpexposing optimizations implicitlyparallel applications spec suites first parallelized suif compiler 15 suifs c output fed multiflow blocked loop distribution policy commonly used multiprocessor execution implemented suif used applications compiled latest version suif 5 access source implemented alternative algorithm described section 5 hand suif also finds tileable loops determines appropriate multiprocessororiented tile sizes particular data sets caches generates tiled code experimented tile sizes manual coding speculative execution enableddisabled modifying multiflow compilers machine description file specifies instructions moved speculatively trace scheduler experimented statically generated profiledriven traces latter profiling information generated instrumenting applications executing training input data set differs set used simulation object files generated multiflow linked versions anl 4 suif runtime libraries create executables smt simulator processes unmodified alpha executables uses emulationbased instructionlevel simulation model detail processor pipelines hardware support oforder execution entire memory hierarchy including tlb usage memory hierarchy processor consists two levels cache sizes latencies bandwidth characteristics shown application data set instruc tions simulated f applu 33x33x33 array 2 iterations 272 x x mgrid su2cor 16x16x16x16 vector len 4k 2 iterations 54 b x x tomcatv 513x513 array 5 iterations fft 64k data points lu 512x512 matrix 431 x water nsquared 512 molecules 3 timesteps 870 x water spatial 512 molecules 3 timesteps 784 x compress train input set 64 x go train input set 2stone9 700 x li train input set 258 x test input set dhrystone 164 x perl train input set scrabble 56 x mxm matrix multiply 256x128 128x64 arrays gmt 500x500 gaussian elimination 354 x adi integration stencil computation solving partial differential equations table 1 benchmarks last three columns identify studies applications used speculative execution l1 icache l1 dcache l2 cache cache size bytes 128k 32k 128k line size bytes 64 64 64 banks 8 8 1 transfer timebank 1 cycle 1 cycle 4 cycles cache fill time cycles latency next level 10 table 2 memory hierarchy parameters choice values first aggressive represents forecast smt implementation roughly three years future used experiments second set typical todays memory subsystems used emulate larger data set sizes 29 used tiling studies table 2 model cache behavior well bank bus contention two tlb sizes used loop distribution experiments 48 128 entries illustrate performance loop distribution policies sensitive tlb size larger tlb represents probable configuration future generalpurpose smt smaller appropriate less aggressive design smt multimedia co processor page sizes typically range 28mb tlb sizes misses require two full memory accesses incurring 160 cycle penalty branch prediction use mcfarlingstyle hybrid predictor 256entry 4way setassociative branch target buffer 8k entry selector chooses global history predictor 13 history bits local predictor 2kentry local history table indexes 4kentry 2bit local prediction table 24 length simulations limited detailed simulation results parallel computation portion applications norm simulating parallel applications initialization phases applications used fast simulation mode simulates caches warm main computation phases reached turned detailed simulation model 5 loop distribution reduce communication coherence overhead distributedmemory multiprocessors parallelizing compilers employ blocked loop parallelization policy distribute iterations across processors blocked distribution assigns thread processor continuous array data iterations manipulate figure 1 figure presents smt speedups applications parallelized using blocked distribution two tlb sizes good speedups obtained many applications number threads increased smaller tlb performance several programs hydro2d swim tomcatv degrades 6 8 threads 8 thread case particularly important applications parallelized exploit 8 hardware contexts smt analysis simulation bottleneck metrics indicated slowdown result thrashing data tlb indicated tlb miss rates table 3 tlb thrashing direct result blocked partitioning increases total working set application threads work disjoint data sets severe cases 8 threads requires many tlb entries loops stride several large arrays since primary data sets usually larger typical 8kb page size least one tlb entry required array swim benchmark specfp95 illustrates extreme example one loop 9 large arrays accessed iteration loop loop parallelized using blocked distribution data tlb footprint 9 arrays 8 excluding entries required data size less 72 significant thrashing occur parallelization profitable lesson tlb shared resource needs managed efficiently smt least three approaches considered 1 using fewer 8 threads parallelizing 2 increasing data tlb size 3 parallelizing loops differently first alternative unnecessarily limits use thread hardware contexts neither exploits smt parallel applications fullest potential second choice incurs cost access time hardware although increasing chip densities future processors may able accommodate 1 even larger tlbs 1 found 64 entries solve problem however 128 entry data tlb avoids tlb thrashing figure 2b indicates achieves speedups least specfp95 data sets application number threads applu 07 09 10 09 10 hydro2d 01 01 01 07 63 mgrid 00 00 00 00 01 su2cor 01 52 77 62 55 tomcatv 01 01 01 20 107 table 3 tlb miss rates miss rates shown blocked distribution 48entry data tlb bold entries correspond decreased performance see figure 2 number threads increased however desirable reduce tlb footprint smt true smt workload would multiprogrammed example multiple parallel applications could execute together comprising threads hardware contexts thread scheduler could schedule 8 threads first parallel application context switch run second later switch back first type environment would performancewise minimize data tlb footprint required application example tlb footprint multiprogrammed workload consisting swim hydro2d would greater 128 entries third desirable solution relies compiler reduce data tlb footprint rather distributing loop iterations blocked organization could use cyclic distribution cluster accesses multiple threads onto fewer pages cyclic partitioning swim would consume 9 rather 72 tlb entries cyclic partitioning also requires less instruction overhead calculating array partition bounds non negligible although much less important factor compare blocked cyclic loop distribution code data figure 1 figure 3 illustrates speedups attained cyclic distribution blocked table 4 contains corresponding changes data tlb miss rates 48entry tlb applications better cyclic distribution cases significant decrease data tlb misses coupled long 160 cycle tlb miss penalty major factor cyclic increased tlb conflicts tomcatv 2 4 threads number misses low overall program performance suffer 6 8 threads tomcatvs original loop blocked parallelization c cyclic parallelization figure 1 blocked cyclic loop distribution example code example loop nest shown using blocked distribution code structured b cyclic version shown c right e illustrate portions array accessed thread two policies clarity assume 4 threads assume row array 2kb 512 double precision elements blocked distribution thread accesses different 8kb page memory cyclic e however loop decomposed manner allows four threads access single 8kb page time thus reducing tlb footprint dimension dimension thread 0 thread 1 thread 2 thread 3 blocked 48entry data tlb26 figure 2 speedups one thread blocked parallelization number threads applu hydro2d mgrid su2cor swim tomcatv average1030 b 128entry data tlb applu hydro2d mgrid su2cor swim tomcatv average1030speedup blocked data tlb miss rate jumped 2 11 causing corresponding hike speedup cyclic absolute miss rates larger data tlb low enough usually 02 except applu su2cor reached 09 changes produced little benefit cyclic contrast su2cor saw degradation cyclic scheduling increased loop unrolling instruction overhead performance degradation seen smaller tlb size cyclics improved tlb hit rate offset overhead mgrid saw large performance improvement tlb sizes reduction dynamic instruction count figures 1b 1c illustrate cyclic parallelization requires fewer computations longlatency divide summary results suggest using cyclic loop distribution smt rather traditional blocked distribution parallel applications large data footprints cyclic distribution increased program speedups saw speedups high 41 even smallish specfp95 reference data sets applications smaller data footprints cyclic broke even one application odd interaction loop unrolling factor cyclic worsen performance multiprocessor smt processors cyclic distribution would still appropriate within node application 48entry tlb 128entry tlb number threads number threads applu 0 50 58 53 15 0 91 98 85 69 hydro2d 0 0 14 91 99 0 0 0 0 14 mgrid 0 0 0 0 50 0 0 0 0 0 su2cor 14 99 99 99 97 0 0 98 91 94 tomcatv 0 60 60 96 99 0 60 60 60 60 table 4 improvement decrease tlb miss rates cyclic distribution blocked 35 41 applu hydro2d mgrid su2cor swim tomcatv mean10speedup versus blocked b 128entry data tlb applu hydro2d mgrid su2cor swim tomcatv mean10speedup versus blocked 48entry data tlb thread 4 thread 6 thread 8 thread figure 3 speedup attained cyclic blocked parallelization application execution time blocked normalized 10 numbers threads thus bar compares speedup cyclic blocked number threads hybrid parallelization policy might desirable though blocked distribution across processors minimize interprocessor communication 6 software speculative execution todays optimizing compilers rely aggressive code scheduling hide instruction latencies global scheduling techniques trace scheduling 22 hyperblock scheduling 23 instructions predicted branch path may moved conditional branch execution becomes speculative runtime branch path taken speculative instructions useless potentially waste processor resources inorder superscalars vliw machines software speculation necessary hardware provides scheduling assistance smt processor whose execution core outoforder superscalar instructions dynamically scheduled speculatively executed hardware multithreading also used hide latencies number smt threads increased instruction throughput also increases therefore latencyhiding benefits software speculative execution may needed less even unnecessary additional instruction overhead introduced incorrect speculations may degrade performance experiments designed evaluate appropriateness software speculative execution smt processor results highlight two factors determine effectiveness smt static branch prediction accuracy instruction throughput correctlyspeculated instructions instruction overhead incorrectlyspeculated instructions however add dynamic instruction count therefore speculative execution beneficial applications high speculation accuracy eg loopbased programs either profiledriven stateoftheart static branch prediction table 5 compares dynamic instruction counts profiledriven 2 speculative nonspeculative versions applications small increases dynamic instruction count indicate compiler assistance profiling information able accurately predict paths executed 3 consequently speculation may incur penalties higher increases dynamic instruction count hand mean wrongpath speculations probable loss smt performance instruction overhead influences effectiveness speculation factor level instruction throughput programs without speculation also important determines easily speculative overhead absorbed sufficient instruction issue bandwidth low ipc incorrect speculations may cause harm higher 2 used profiledriven speculation provide bestcase comparison smt without profiles mispredictions would occurred overhead instructions would generated consequently software speculation would worse performance report making absence appear even beneficial smt 3 specfp95 programs radix splash2 compress specint95 loopbased small increases dynamic instruction count speculation increase specint95 increase splash2 increase applu 21 compress 29 fft 137 hydro2d 19 go 126 lu 125 mgrid 05 li 73 radix 00 su2cor 01 m88ksim 40 waternsquared 30 tomcatv 12 table 5 percentage increase dynamic instruction count due profiledriven software speculative execution data shown 8 threads one thread numbers identical close applications bold high speculative instruction overhead high ipc without speculation italics former spec specint95 spec spec splash2 spec spec applu 49 47 compress 41 40 fft 60 64 hydro2d 56 54 go 24 23 lu 67 68 mgrid 72 71 li 45 46 radix 54 54 su2cor 61 60 m88ksim 42 41 water nsquared 64 61 tomcatv 62 59 spatial table throughput instructions per cycle without profiledriven software speculation 8 threads programs bold high ipc without speculation plus high speculation overhead italics former perthread ilp threads software speculation less profitable incorrectlyspeculated instructions likely compete useful instructions processor resources particular fetch bandwidth functional unit issue table 6 contains instruction throughput applications programs ipc higher software speculation indicating degree absorption speculation overhead others lower additional hardware resource conflicts notably l1 cache misses speculative instruction overhead related static branch prediction accuracy instruction throughput together explain speedups lack thereof illustrated figure 4 factors high nonloop based fft li lu speedups without software speculation greatest ranging 22 4 one factor low moderate speedups minimal nonexistent specfp95 applications radix waternsquared high ipc go m88ksim perl speculation overhead 5 without either factor software speculation helped performance reasons benefits architectures hid latencies executed speculative instructions 4 applications others well threads used advantage turning speculation generally becomes even larger additional threads provide parallelism therefore speculative instructions likely compete useful instructions processor resources applu hydro2d tomcatv mgrid su2cor swim05speedup speculation specfp95 thread 4 thread 6 thread 8 thread lu go fft li perl speculation b splash2 spec95 int nsquared spatial figure 4 speedups applications executing without software speculation speculation speculative execution cycles speculation execution cycles bars greater 10 indicate speculation better otherwise idle functional units bottom line loopbased applications compiled software speculative execution nonloop applications compiled without either improves smt program performance maintains current level performance never hurt 6 7 loop tiling order improve cache behavior loops tiled take advantage data reuse section examine two tiling issues tile size selection distribution tiles threads tile size chosen appropriately reduction average memory access time compensates tiling overhead instructions 20116 code figures 6b 6c illustrates source overhead smt however tiling may less beneficial first smts enhanced latencyhiding capabilities may render tiling unnecessary second additional tiling instructions may increase execution time given smts higher multithreaded throughput factors influence whether software speculate address issues examined tileable loop nests different memory access characteristics executing smt processor benefits tiling vary size cache changed smaller caches require smaller tiles naturally introduce instruction overhead hand smaller tiles also produce lower average memory latencies ie fewer conflict misses latency reducing benefit tiling better therefore varied tile sizes measure performance impact range tiling overhead also simulated two memory hierarchies gauge interaction cache size memory latency tile size larger memory configuration represents probable smt memory subsystem machines production approximately 3 years future see section 4 configuration smaller modeling todays memory hierarchies designed provide appropriate ratio data set cache size modeling loops larger ie realistic data sets benchmarks experiments thread given separate tile tiling norm figure 5 presents performance total execution cycles average memory access time dynamic instruction count range tile sizes larger memory configuration 8thread smt execution application compares singlethread run 5 even though floating point computations waterspatial high ipc without speculation 65 therefore speculative instructions bottlenecked integer units execution without speculation profitable approximating execution superscalar 13 results indicate tiling profitable smt conventional processors mxm may seem exception since tiling brings improvement exception shows harm applying optimization programs executing smt appear insensitive tile size almost tile sizes examined smt able hide memory latencies indicated flat amat curves still absorbing tiling overhead therefore smt less dependent static algorithms determine optimal tile sizes particular caches working sets contrast conventional processors likely tile size sweet spot even outoforder execution modern processors well alternative singledie processor architectures lack sufficient latencyhiding abilities consequently require exact tile size calculations compiler tile size also performance determinant less aggressive memory subsystem results shown indicating tiling smt robust across 6 keep mind speculated without runtime support pro filing relative benefit speculation versus speculation would higher example 8 threads waternsquared breaks even profiledriven speculation however relying multiflows static branch prediction gives speculation slight edge speedup 11 nevertheless general conclusions still hold good branch prediction low multithread ipc needed software speculation benefit applications executing smt figure 5 tiling results larger memory subsystem separate tilesthread horizontal axes tile size tile size 0 means tiling sizes greater 0 one dimension tile measured array elements vertical axes metrics evaluating tiling dynamic instruction count total execution cycles amat mxm dynamic instruction count millions total execution cycles millions average memory access time cycles amat total execution cycles millions average memory access time cycles amat adi 8 threads gmt 8 threads memory hierarchies alternatively range data set sizes execution time course higher performance dependent amat parameters rather tiling overhead adi became slightly less tolerant tile size changes largest tile size measured 32x32 amat increased sharply interthread interference small cache loop nest either tiles sized fit cache alternative tiling technique described used second loop tiling issue distribution tiles threads parallelizing loops multiprocessors different tile allocated processor thread maximize reuse reduce interprocessor communication smt however tiling manner could detrimental private perthread tiles discourage interthread tile sharing increase total thread tile footprint singleprocessor smt factors make blocked loop iteration scheduling inappropriate smt rather giving thread tile called blocked tiling single tile shared threads loop iterations distributed cyclically across threads cyclic tiling see figure 6 code explanation blocked cyclic tiling figure 7 effect perthread data layout tile shared cyclic tiling optimized increasing tile size reduce overhead 7c larger tiles cyclic tiling drop execution times applications executing small memory smts closer smts aggressive memory hierarchies put another way performance programs large data sets original loop blocked tiling jtlb jt ub jtjtsize it1 ititsize jjt j minnjtjtsize1j kmax1 kt iit minmititsize1i c cyclic tiling kmax1 kt figure code blocked cyclic versions tiled loop nest approach smaller example figure 8c illustrates larger tile sizes greater 8 array elements per dimension cyclic tiling reduced mxms amat enough decrease average execution time smaller cache hierarchy 51 compare blocked figure 8b within 35 blocked tiling memory subsystem several times size figure 8a smallest tile size increase tiling overhead overwhelm smts ability hide memory latency cyclic tiling still appropriate multiprocessor smts hierarchical 8 hybrid tiling approach might effective cyclic tiling could used maximize locality processor blocked tiling could distribute tiles across processors minimize interprocessor thread 1 thread 2 thread 3 dimension c optimized cyclic0011 blocked223344 dimension b cyclic21i dimension dimension figure 7 comparison blocked cyclic tiling techniques multiple threads blocked tiling shown tile 4x4 array elements numbers represent order tiles accessed thread cyclic tiling tile still 4x4 array tile shared threads example thread gets one row tile shown b cyclic tiling thread works smaller chunk data time tiling overhead greater c tile size increased 8x8 reduce overhead within tile thread responsible 16 elements original blocked example total execution time millions cycles average memory access time cycles amat dynamic instruction count millions b c figure 8 tiling performance 8thread mxm tile sizes along xaxis results shown blocked tiling larger memory subsystem b blocked tiling smaller memory subsystem c cyclic tiling also smaller memory subsystem 8 compiler optimizations addition optimizations studied paper compilerdirected prefetching predicated execution software pipelining also reevaluated context smt processor conventional processor compilerdirected prefetching 26 useful tolerating memory latencies long prefetch overhead due prefetch instructions additional memory bandwidth andor cache interference minimal smt overhead detrimental interferes thread prefetching also competes threads predicated execution 231628 architectural model instruction execution guarded boolean predicates determine whether instruction executed nullified compilers use ifconversion 2 transform control dependences data dependences thereby exposing ilp like software speculative execution aggressive predication incur additional instruction overhead executing instructions either nullified produce results never used software pipelining 927181 improves instruction scheduling overlapping execution multiple loop iterations rather pipelining loops smt execute parallel separate hardware contexts alleviates increased register pressure normally associated software pipelining multithreading could also combined software pipelining necessary optimizations discussed paper originally designed increase singlethread ilp intrathread parallelism still important smt processor simultaneous multithreading relies multiple threads provide useful parallelism throughput often becomes important performance metric smt raises issue compiling throughput singlethread example perspective single running thread optimizations traditionally applied may desirable reduce threads running time global perspective greater throughput therefore useful work achieved limiting amount speculative work 9 related work three compiler optimizations discussed paper widely investigated nonsmt architectures loop iteration scheduling sharedmemory multiprocessors evaluated wolf lam 34 carr mckinley tseng 7 anderson amarasinghe lam 3 cierniak li 10 among others studies focus scheduling minimize communication synchronization overhead restructured loops data layout improve access locality processor particular anderson et al discuss blocked cyclic mapping schemes present heuristic choosing global scheduling optimizations like trace scheduling 22 superblocks 25 hyperblocks 23 allow code motion including speculative motion across basic blocks thereby exposing ilp staticallyscheduled vliws wideissue superscalars study ilp limits lam wilson 19 found speculation provides greater speedups loopbased numeric applications nonnumeric codes study include effects wrongpath instructions previous work code transformation improved locality proposed various frameworks algorithms selecting applying range loop transformations 1433617347 studies illustrate effectiveness tiling also propose loop transformations enabling better tiling lam rothberg wolf 20 coleman mckinley 11 carr et al 6 show application performance sensitive tile size present techniques selecting tile sizes based problemsize cache parameters rather targeting fixedsize fixedcache occupancy conclusions paper examined compiler optimizations context simultaneous multithreading architecture smt architecture differs previous parallel architectures several significant ways first smt threads share processor memory system resources single processor finegrained basis even within single cycle optimizations smt therefore seek benefit finegrained sharing rather avoiding done conventional sharedmemory multiprocessors second smt hides intrathread latencies using instructions active threads optimizations expose ilp may needed third instruction throughput smt high therefore optimizations increase instruction count may degrade performance effective compilation strategy simultaneous multithreading processors must recognize unique characteristics results show specific cases smt processor benefit changing compiler optimization strategy particular showed 1 cyclic iteration scheduling opposed blocked scheduling appropriate smt ability reduce tlb footprint 2 software speculative execution bad smt decreases useful instruction throughput 3 loop tiling algorithms less concerned determining exact tile size smt performance less sensitive tile size 4 loop tiling increase rather reduce interthread tile sharing appropriate smt increases benefit sharing memory system resources acknowledgments would like thank john odonnell equator technologies inc tryggve fossum digital equipment corp source alpha axp version multiflow compiler jennifer anderson dec western research laboratory providing us suifparallelized copies benchmarks also would like thank jeffrey dean dec wrl referees whose comments helped improve paper research supported washington technology center nsf grants mip9632977 ccr9200832 ccr9632769 darpa grant f306029720226 onr grants n00014 92j1395 n000149411136 dec wrl fellowship intel corporation r optimal loop parallelization conversion control dependence data dependence data computation transformations multiprocessors portable programs parallel processors compiler blockability numerical algo rithms compiler optimizations improving data locality hierarchical tiling improved superscalar performance approach scientific array processing architectural design ap120bfps164 family unifying data control transformations distributed sharedmemory machines tile size selection using cache organization data layout new cpu benchmark suites spec simultaneous multithreading platform nextgeneration processors strategies cache local memory management global program transformation maximizing multiprocessor performance suif compiler highly concurrent scalar processing maximizing loop parallelism improving data locality via loop fusion distribution software pipelining effective scheduling technique vliw machines limits control flow parallelism cache performance optimizations blocked algorithms converting threadlevel parallelism instructionlevel parallelism via simultaneous multithreading multiflow trace scheduling compiler effective compiler support predicated execution using hyperblock combining branch predictors superblock effective technique vliw superscalar compilation design evaluation compiler algorithm prefetching scheduling techniques easily schedulable horizontal architecture high performance scientific computing cydra 5 departmental supercomputer scaling parallel programs multiprocessors methodology examples exploiting choice instruction fetch issue implementable simultaneous multithreading processor simultaneous multi threading maximizing onchip parallelism data locality optimizing algorithm loop transformation theory algorithm maximize parallelism splash2 programs characterization methodological considerations tr highly concurrent scalar processing strategies cache local memory management global program transformation optimal loop parallelization software pipelining effective scheduling technique vliw machines cydra 5 departmental supercomputer cache performance optimizations blocked algorithms data locality optimizing algorithm new cpu benchmark suites spec limits control flow parallelism design evaluation compiler algorithm prefetching effective compiler support predicated execution using hyperblock compiler blockability numerical algorithms multiflow trace scheduling compiler superblock compiler optimizations improving data locality unifying data control transformations distributed sharedmemory machines tile size selection using cache organization data layout splash2 programs simultaneous multithreading exploiting choice compilerdirected page coloring multiprocessors converting threadlevel parallelism instructionlevel parallelism via simultaneous multithreading conversion control dependence data dependence portable programs parallel processors scaling parallel programs multiprocessors maximizing multiprocessor performance suif compiler simultaneous multithreading loop transformation theory algorithm maximize parallelism hierarchical tiling improved superscalar performance maximizing loop parallelism improving data locality via loop fusion distribution scheduling techniques easily schedulable horizontal architecture high performance scientific computing ctr mark n yankelevsky constantine polychronopoulos coral multigrain multithreaded processor architecture proceedings 15th international conference supercomputing p358367 june 2001 sorrento italy nicholas mitchell larry carter jeanne ferrante dean tullsen ilp versus tlp smt proceedings 1999 acmieee conference supercomputing cdrom p37es november 1419 1999 portland oregon united states jack l lo luiz andr barroso susan j eggers kourosh gharachorloo henry levy sujay parekh analysis database workload performance simultaneous multithreaded processors acm sigarch computer architecture news v26 n3 p3950 june 1998 alex settle joshua kihm andrew janiszewski dan connors architectural support enhanced smt job scheduling proceedings 13th international conference parallel architectures compilation techniques p6373 september 29october 03 2004 evangelia athanasaki nikos anastopoulos kornilios kourtis nectarios koziris exploring performance limits simultaneous multithreading memory intensive applications journal supercomputing v44 n1 p6497 april 2008 gary zoppetti gagan agrawal lori pollock jose nelson amaral xinan tang guang gao automatic compiler techniques thread coarsening multithreaded architectures proceedings 14th international conference supercomputing p306315 may 0811 2000 santa fe new mexico united states steven swanson luke k mcdowell michael swift susan j eggers henry levy evaluation speculative instruction execution simultaneous multithreaded processors acm transactions computer systems tocs v21 n3 p314340 august calin cacaval david padua estimating cache misses locality using stack distances proceedings 17th annual international conference supercomputing june 2326 2003 san francisco ca usa james burns jeanluc gaudiot smt layout overhead scalability ieee transactions parallel distributed systems v13 n2 p142155 february 2002 joshua redstone susan j eggers henry levy analysis operating system behavior simultaneous multithreaded architecture acm sigplan notices v35 n11 p245256 nov 2000 joshua redstone susan j eggers henry levy analysis operating system behavior simultaneous multithreaded architecture acm sigarch computer architecture news v28 n5 p245256 dec 2000 luke k mcdowell susan j eggers steven gribble improving server software support simultaneous multithreaded processors acm sigplan notices v38 n10 october