nested fgmres method parallel calculation nuclear reactor transients semiiterative method based nested application flexible generalized minimum residualfgmres developed solve linear systems resulting application discretized twophase hydrodynamics equations nuclear reactor transient problems complex threedimensional reactor problem decomposed simpler manageable problems recombined sequentially gmres algorithms mathematically method consists using inner level gmres solve preconditioner equation outer level gmres applications performed practical threedimensional models operating pressurized water reactors pwr serial parallel applications performed reactor model two different details core representation appropriately tight convergence enforced gmres level results semiiterative solver agreement existing direct solution methods larger model tested serial performance gmres factor 3 better direct solver parallel speedups 4 using 13 processors intel paragon thus larger problem order magnitude reduction execution time achieved indicating use semiiterative solvers parallel computing considerably reduce computational load practical pwr transient calculations b introduction analysis nuclear reactor transient behavior always one difficult computational problems nuclear engineering computational load calculate detailed threedimensional solutions field equations prohibitive variations power flow temperature distributions treated approximately reactor calculation resulting considerable conservatism reactor operation researchers estimated using existing methods computational load calculate threedimensional distributions would exceed teraflop per time step 16 6 last several years computer speed memory increased dramatically motivated rethinking limitations existing reactor transient analysis codes researchers begun adapt threedimensional hydrodynamics neutron kinetics codes advanced computer architectures begun investigate advanced numerical methods take full advantage potential high performance computing overall goal work reported reduce computational burden threedimensional reactor core models thereby enabling high fidelity reactor system modeling specific objective investigate krylov subspace methods parallel solution linear systems resulting reactor hydrodynamics equations following section provides brief description hydrodynamic model reactor problem used work nested gmres method preconditioner developed work described section 3 serial parallel applications presented sections 4 5 respectively work supported electric power research institute school nuclear eng purdue university w lafayette 47907 2 hydrodynamic model reactor problem nuclear reactor analysis problem involves solution coupled neutron kinetics heat conduction hydrodynamics equations twophase flow hydrodynamics generally computationally demanding focus work hydrodynamic method used consistent reactor systems code retran03 7 widely used nuclear industry analysis reactor transient behavior method based semiimplicit solution mass momentum equations phase energy equation fluid mixture solution scheme uses finite difference representations fluidflow balance equations convective quantities source terms linearized ie expanded using first order taylor series result coupling finite difference equations implicitly included system coupled equations method considered semiimplicit since linearized equations used rather original nonlinear partial differential equations standard newtonraphson technique used solve nonlinear equations 21 hydrodynamic model application spatial finite differencing set governing partial differential equations performed using concept volumes connecting junctions results system ordinary differential difference equations may expressed dy dt 1 column vector nodal variables f column vector functions number dependentsolution variables solution vector consists n j junction mass flow rates w slip velocities v sl volume total mass total energy u vapor mass g inventories 2 vector fy linearized first order time difference approximation used resulting following linear system identity matrix j matrix jacobian deltay deltat values time levels n1 n respectively semiimplicit nature formulation linear system arises linearizing discretizing tightly coupled also high degree stability formulation imposes less stringent limit size time step indicated equation 3 larger time steps reduce diagonal dominance results illconditioned linear system several authors noted 5 3 illconditioned linear systems welldefined structure difficult solve efficiently many cases special handling required obtain acceptable solution present linear solver retran03 direct method based gaussian elimination thetan 3 execution time complexity n number unknowns models dominated onedimensional flow direct solutions complemented type matrix reduction technique efficient however case high fidelity model reactor three dimensional core rep resentation direct methods become inefficient demonstrated using model standard 4loop presurized water reactor aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa system model model see model side view top view channel upper plenum 26 33 28 14 31 17 43 23 48 36 343111 355769 7177737579656 4 pass fig 1 nodalization pwr model 22 pressurized water reactor problem reactor system model used work consists 4loop pressurized water reactor model detailed three dimensional representation reactor core schematic shown figure 1 loop system contains steam generator heat transferred pressurized primary loop secondary loop containing steam turbines core model consists several volumes stacked one upon along channel assuming layout c c channels axial volumes number volumes ac 2 number cross flow junctions number vertical junctions c 2 a1 first problem used work core model 9 channels 12 axial levels per channel shown figure constructed two versions core model one cross flow horizontal flow volumes without cross flow cross flow important reactor transients break steam line one loops results horizontal mixing hot cold water core model cross flow consists 173 volumes 339 junctions results linear system size 1173 core model cross flow contains 108 volumes 261 junctions forms nearly 70 system model without cross flow contains number volumes 183 junctions results linear system size 885 coefficient matrix problem cross flow shown schematically figure 2 structure shown figure corresponds sequential ordering junction volume variables convenient reductionelimination method currently used code reordering suitable preconditioning discussed following section columns rows fig 2 pwr model 3d core sparsity pattern coefficient matrix initial test problem simple core reactivity event modeled simulating insertion withdrawal control rod 20 seconds rod inserted core rate 008 dollars reactivity per second 5 seconds immediately withdrawn rate computational performance problem single processor intel paragon summarized table 1 models without cross flow direct linear system solution performed solve module indicated table table computatinal summary pwr example problem direct linear solver module wo cross flow w cross flow cpu time percent cpu time percent 15589 467 19617 49 total 33388 1000 493171 1000 immediately apparent table 1 use cross flow core increases computational time order magnitude primarily problem without cross flow results linear system containing predominantly tridiagonal submatrices lends efficiently reductionelimination methods large increase execution time discourages modeling cross flow general high fidelity reactor simulation although reduction operation count required direct solution primary motivation work reasons consider iterative linear solution methods first actual problem solved retran03 nonlinear direct solution resultant linearized equations waste floating point operations since usually accuracy considerably less machine precision adequate secondly unlike direct methods semiiterative solution methods accelerated information previous time steps initial guess preconditioner finally direct methods lend easily parallel computing distributed memory mimd multicomputers whereas many new semiiterative linear solvers efficiently parallelized 3 semiiterative linear solvers 3d hydrodynamics last several years considerable research performed nonstationary class techniques collectively known krylov subspace methods include classical conjugate gradient cg method shown efficient symmetric positive definite systems equations methods called krylov methods based building solution vector krylov sub space span fr residual initial solution coefficient matrix coefficients solution vector case cg method based minimization energy norm error gen eral linear systems encountered hydrodynamics problems symmetric positive definite therefore solved using cg method numerous krylov methods solving nonsymmetric problem proposed years several considered work include generalized minimal residualgmres10 method biconjugate gradient bicg method 1 conjugate gradient squared cgs method 12 biconjugate gradient stabilized bicgs method 15 particular interest work gmres method solves minimization problem iteration therefore guarantees monotonically decreasing residual well known convergence rate krylov methods depends spectral properties coefficient matrix proper preconditioning considerably improve rate convergence preconditioning involves additional cost initially per iteration tradeoff cost implementing preconditioner gain convergence speed since many traditional preconditioners large sequential component tradeoff serial performance preconditioner parallel efficiency several alternative preconditioners examined work 31 application krylov methods reactor problem previous work application semiiterative solvers reactor core hydrodynamic calculations 13 8 focused primarily solution linearized pressure equation resulting single phase onedimensional flow problems krylov methods found perform well resulting tridiagonal system equations particular turner achieved excellent convergence conjugate gradient squared method ilu preconditioner work provided useful insight linear systems resulting twophase threedimensional flow problems significantly different performance various krylov methods different figure 3 shows behavior absolute residual l2 norm application pwr problem various krylov methods preconditioner performance bicgstab bicg cgs irregular cgs shown residual increased several orders magnitude first iterations continues increase gmres method demonstrated acceptable behavior monotonically decreasingly residual expected basis minimization principle linear systems time steps examined behavior similar shown figure 3 observed although floating point operation count per iteration higher gmres particularly larger numbers iterations attractive applications inherent robustness preconditioning techniques examined could improve convergence behavior gmres 32 domain decomposition preconditioning knowledge physical characteristics system invaluable choosing good preconditioner evident figure 1 core excore systems interact upper lower plenum suggests natural way decompose problem domain system reordered solution variables belonging core placed contiguously structure resultant matrix given ac u blocks l u represent interactions core excore vari ables core excore interact upper lower plenum matrices non zero elements several options exist using equation 4 preconditioner one possible preconditioner neglect l u blocks entirely resulting block jacobi preconditioner given ac gmres iterations residual fig 3 figure performance krylov solvers pwr 3d core problem block ae represents interactions variables excore region block ac represents interactions variables core region preconditioner jacobi two blocks may solved individually attractive parallel computing noted earlier size core block considerably larger excore block later section methods discussed solving core problem block jacobi domain decomposition preconditioner applied 3d pwr example problem first using direct solution preconditioner equation table 2 contains results linear systems first two time steps transient described section 22 first case corresponds problem results shown figure 3 addition number iterations two measures effectiveness preconditioner shown table first condition original preconditioned matrix shown second third columns fourth column shown another measure effectiveness preconditioning suggested dutto4 uses ratio frobenius norm remainder matrix defined frobenius norm original matrix frobenius norm defined diagonal shown table measures indicate jacobi preconditioner effective tolerance 10 gamma6 relative residual convergence achieved less table results application domain decomposition pwr problem preconditioner kakf iterations 219 gauss seidel 830 219 modified gauss seidel 219 block jacobi preconditioner impact submatrices l u completely neglected solving preconditioner equation another possibility employ gaussseidel like technique preconditioner equations solved sequentially z c z c z c z e respectively two approaches examined solving equations preconditioner one approach first equation solved neglecting coupling ie z c symmetry sequence solution matter preconditioner form ac results applying technique shown gaussseidel preconditioner table 2 indicated minor change measures effectiveness preconditioning however convergence achieved fewer iterations second approach estimate excore solution z e formed using decoupled excore equation ae used solve sequentially core excore equations given equation 7 preconditioner approach expressed approach shown table 2 modified gausssiedel preconditioner indicated little differences observed effectiveness precondi tioning block jacobi method naturally parallel used work even though gausssiedel methods showed slightly better numerical performance results shown preconditioner equation solved directly using gaussian elimination excore problem predominantly onedimensional flow direct solution using reductionelimination proves efficient con versely threedimensional core model lend direct solution following section examines use second level gmres solve core problem 33 preconditioning core problem original matrix ordering shown figure 2 conducive preconditioning core problem ac z several alternate orderings examined junctions physically volumes reordering solution vector bears resemblance physical layout would help decreasing profile matrix goal increase density matrix regions around diagonal ie reduce bandwidth use portion matrix preconditioning purposes eg block jacobi etc structure existing core exploited defining supernode consisted volumes junctions physical domain discretized several supernodes introduced homogeneity structure matrix supernode could considered fractal representing smallest unit structure present system shown figure 4 supernode core problem consists volume 3 junctions one junctions vertical junction upstream volume two crossflow junctions leading volume different direction noted use supernodes leads problem extra junctions exterior channels junctions dummy junctions represented matrix appear solution hence size system increases condition number remains volume vertical junction crossflow junction crossflow junction fig 4 structure supernode several orderings supernodes considered eg channelwise plane wise cuthillmckee etc ordering planewise found best purpose preconditioning plane represents two dimensional grid supernodes planes form one dimensional structure linked two neighbors resulting block tridiagonal matrix shown figure 5 equation 10 case outer level preconditioner several options exist using equation 10 preconditioner inner level gmres 12002006001000columns rows fig 5 structure coefficient matrix plane wise ordering block diagonal preconditioner neglects planar coupling considered first table 3 shows results solving second level gmres first outer iteration four different block sizes examined convergence criterion 10 gamma6 two cases shown 1 c 2 c taken different time steps table results application jacobi preconditioning inner level preconditioner block size condition krkf kakf iterations condition krkf kakf iterations results outer iterations slightly different since source hence initial residual second level gmres different however results cases consistent general trend shown table 3 expected larger block sizes reduce number iterations however cost solving block directly would increase n 3 offsets reduction iterations also smaller blocks advantage scalability parallel computing domain decomposition scheme incorporating interactions diagonal blocks also examined approximate solution z c computed solving block jacobi system neglects coupling adjacent planes z 0 prime notation used distinguish z c r c vectors occur outer iteration equation 7 inner gmres preconditioner equation solved preconditioner md indicated table 4 improved preconditioning reduce number iterations solving preconditioner becomes expensive number iterations block size 81 reduced half twice number floating point operations required form md table results application domain decomposition preconditioning inner level kakf iterations block size preconditioners block jacobi investigated core problem primarily illconditioned nature coefficient matrix popular schemes ssor ilu ineffective example incomplete lu factorization scheme tested linear system arising 3d core prob lem banded constructed frobenius norm ilu preconditioner compared norm exact inverse shown frobenius norm approximate preconditioned system gamma1 three orders magnitude larger frobenius norm gamma1 suggests ilu preconditioner would effective 34 nested gmres methods described previous sections implemented nested gmres algorithm consists using inner level gmres solve preconditioner equation outer level gmres strategy suggested van der vorst demonstrated successfully several model problems 14 preconditioner inner level gmres could solved using third level gmres applications direct solver proved efficient schematic nested gmres algorithm shown figure 6 physical interpretation algorithm view overall problem decomposed three simpler manageable problems recombined sequentially gmres algorithms highest level take advantage naturally loose coupling core excore components solve separately linear systems core excore regions solutions recombined using highest outer level gmres second inner level gmres used solve 3d core flow problem focus coupling vertical flow channels core finally third lowest level gmres direct solver used restore coupling nodes plane core problem system problem gmres gmres ii gmres iii direct e z c z fig 6 nested gmres algorithm retran03 linear system solution 35 flexible general minimum residual fgmres solution preconditioning equation gmres method another gmres algorithm poses potential problem due finite precision inner level solution preconditioned gmres algorithm solution first built preconditioned subspace transformed solution space matrix set orthonormal vectors case iterative solution preconditioner transformation approximate extent determined convergence criterion case illconditioned matrices approximations could especially troublesome tight convergence required minimize error propagation problem inexact transformation alleviated work using slight variant gmres algorithm extra set vectors stored used update solution modification gmres algorithm suggested saad called flexible general minimum residualfgmres 9 algorithm allows complete variation preconditioner one iteration next storing result preconditioning basis vectors used krylov subspace instead using equation 15 final transformation solution subspace performed using matrix fgmres algorithm given appendix implemented nested gmres method 4 serial applications 41 static problem nested gmres algorithm first applied linear systems arising first time steps rod withdrawalinsertion transient parametrics performed convergence criterion number iterations levels maximum iteration limit set inner gmres since cases rate convergence slow sometimes termed critical slowing variation outer highest level residual iterations shown figure 7 different maximum number inner iterations miter results indicate rate decrease residual substantially greater miter 20 however shown figure 8 results subsequent timesteps indicate difference rate decrease residual miter gradually diminishes residual achieved inner second level gmres timestep corresponding figure 8 shown figure 9 maximum iteration limits residual increases first iterations one possible explanation desirable search directions outer level gmres become harder resolve leading degradation performance inner level however diminished quality preconditioning inner level gmres appear deleterious effect convergence outer iteration based convergence results analysis time steps strategy formulated implementation nested gmres transient analysis code retran03 following section discusses results applying gmres transient problem outer iteration number outer residual fig 7 reduction outer residual first time step outer iteration number outer residual fig 8 reduction outer residual subsequent time step outer iteration number inner residual fig 9 performance inner gmres subsequent time step 42 transient problem timedependent iterative solution hydrodynamics equations error incomplete convergence one time step propagated coefficient matrix subsequent timesteps preliminary assessment effect convergence criteria quality solution performed using null transient steadystate condition continued several seconds disturbance system several outer level convergence criteria investigated acceptable performance achieved tolerance 10e 09 relative residual higher tolerances minor deviation observed eg less 1 relative error performance parameters core power level provided initial guidance setting tolerance iteration limits transient problems pwr rod withdrawalinsertion transient described section 22 analyzed using retran03 nested fgmres model cross flow core studied using iterative solver transient time 20 seconds required steps performance iterative algorithm analyzed first varying number outer iterations using direct solver inner eg problem varying number outer inner gmres iterations results shown table 5 cpu seconds per time step maximum relative residual error outer iterations shown fourth column table purposes comparison retran03 solution table 1 uses direct solution linear system repeated case a1 table 5 first two cases a2 a3 inner two gmres levels see figure 6 replaced direct solver gmres used outer highest level iteration objective isolate impact number outer iterations table serial performance retran gmres 9 channel 12 axial case iterations number cpu secstime step case outer inner max krk a1 direct direct gammagamma 318 1489 1565 a3 12 direct 4 a4 a6 quality solution seen table 5 maximum residual error decreases number outer iterations increases however cases a2 a3 significant error observed important physical parameters number outer iterations reduced four minor deviations began occur solution 15 seconds transient noted execution times cases a2 a3 generally comparable direct solver gmres algorithm employed outer inner iterations keeping direct solution innermost third level order gain insight relation convergence inner outer gmres algorithms number outer inner iterations varied shown table 5 cases a4 a5 a6 a7 accuracy solutions examined important physical parameters solution normalized core power pressurizer pressure volume 58 figure 1 plotted versus time figure 10 direct solution a1 gmres solutions a4 a7 minor deviation observed solution iterations execution times greater direct a1 gmres outer direct inner a3 solutions however discussed next section algorithm inner level gmres attractive larger problems parallel computing 5 parallel applications one attractive features preconditioned krylov methods potential parallel computing emphasis work use distributed memory parallel architecture applications performed intel paragon section describes mapping nested gmres onto paragon execution time reductions achievable pwr sample problem natural mapping processors pwr model one processor excore one 12 planes core model matrices striped row wise implying l ae equation 4 stored processor assigned excore ideally ac u partitioned among 12 processors repartitioning data found expensive hence copy ac u maintained pe operations distributed among processors one primary concerns distributing data computation parallel processing communication overhead incurred transferring data pro cessors time necessary perform transfer consists two parts time necessary initiate transfer referred latency time necessary actually transfer data depends amount data core power retran solution 20 inner 20 outer psia retran solution 20 inner 20 outer fig 10 results retran03 gmres pwr transients gmres inner 40 sec machine bandwidth following sections discuss parallelization inner outer levels nested algorithm special emphasis given communication issues 51 parallelization outer iteration one dominant operations outer iteration matrix vector product iteration product formed matrix eq1 residual vector operation broken four parts first part involves product ae v e involves communication since data resides processor next two parts involve u v c l v e involve transfer parts vector processor 0 assigned excore processors 1 12 assigned bottommost topmost planes core respectively fourth part matrix vector product involves ac v c requires communication processors 1 12 two processors general matrix vector product requires communication selected processors specific pattern vector inner products hand require global communication means every processor requires information processors necessary sum product element first vector corresponding element second vector elements vectors first multiplied processor forms partial sum elements reside domain sum products required individual products number transfers required considerably reduced employing well known method treesumming communication costs vary dlog 2 n e opposed n case communication gramschimdt orthogonalization process used gmres involves several inner products iteration however result inner products dependent others therefore partial sums inner products performed one time reducing number transfers required 2 least square solution gmres involves reduced linear system involve sufficient operations merit parallelization 11 communication related least squares problem avoided entirely performing least square solution simultaneously processors also estimate residue gmres consequence least squares solution termination criterion could evaluated without need additional communication 52 parallelization inner iteration several operations required parallelize inner iteration similar outer iteration matrix ac striped row wise row corresponding plane stored corresponding processor planes linked immediate neighbors matrix vector product requires two sets data transfer vector inner products least square solution treated manner outer level since preconditioner second level block jacobi preconditioner equation could implemented without data transfers irrespective whether solved using direct solver iterative solver 53 results pwr model 9 channel 12 axial core parallelized version retran03 executed paragon 13 processors section presents result rod withdrawal case pwr model 9 channel12 axial volume core transient analyzed 20 seconds nested fgmres solution performed using tolerance 10 gamma11 iteration limit 20 inner outer iterations table serial performance second level 9 channel12 axial time step outer inner serial execution time precond total table parallel performance second level 9 channel12 axial time step outer inner parallel execution time precond comm total speedup tables 6 7 show results serial parallel implementation respectively inner second level gmres fourth column tables headed pre cond indicates time required preconditioning second level gmres iii direct solve figure 6 performed application using direct solver means lu factorization first timestep involves additional initialization costs consistent comparisons parallel serial versions retran possible differences code structure initialization high speedup 622 first iteration second timestep due lu factorization performed concurrently parallel implementation noted bulk time spent third level since part code naturally parallelizable one would expect high efficiencies however since problem size relatively small communicationoverhead implementing remainder gmres significantly reduces efficiency would case larger problems seen next section tables 8 9 show results serial parallel implementation outer level gmres bulk time spent preconditioning similar execution times obtained timesteps speedup slightly greater two obtained outer iterations execution time first four time steps summarized table 10 speedup order two achieved serial performance outer level 9 channel12 axial timestep outer serial execution time core excore total table parallel performance outer level 9 channel12 axial timestep outer parallel execution time core excore comm total speedup 54 results pwr model 25 channel 12 axial core order examine scalability results pwr model used instead 9 channels core linear system nearly three times size arising form 9 channel 12 axial case anticipated shown table 11 time required solve linear system using direct solver order magnitude larger 9 channel 12 axial case compare case a1 table 5 problem first executed using retran nested fgmres solver single processor paragon problem large executed ordinary node 32mb ram special fat node paragon used additional memory 128 mb support larger applications shown table 11 results single processor encouraging since factor 2 improvement achieved gmres compared direct solver problem executed using parallel version nested gmres solver domain decomposition method exactly 9 channel 12 axial case 12 processors assigned core 1 excore expected parallel efficiency much better speedups factor 4 respect serial version gmres table 11 shows comparison results first two time steps indicated table 11 parallel execution nested gmres provides order magnitude reduction execution time compared serial execution direct solver furthermore since memory requirement per node less 32mb parallel version could executed using standard sized nodes paragon thus parallelization key execution time reductions also alleviating memory constraints larger problems 6 summary conclusions nested fgmres method developed solve linear systems resulting threedimensional hydrodynamics equations applications performed practical pressurized water reactor problems threedimensional core models serial parallel applications performed overall performance nested fgmres 9 channel12 axial timestep number execution time speedup outers serial parallel table execution time 25 channel 12 axial case case timestep execution time per iteration outer total per inner timestep core excore precond direct solver 1 343310 9 channel 25 channel version reactor core appropriately tight convergence enforced gmres level semiiterative solver performed satisfactorily duration typical transient problem serial execution time 9 channel model comparable direct solver parallel speedup intel paragon factor 23 using 13 processors 25 channel model serial performance nested gmres factor 3 better direct solver parallel speedups vicinity 4 using 13 processors thus 25 channel problem order magnitude reduction execution time achieved results indicate use semiiterative solvers parallel computing considerably reduce computational load practical pwr transient calculations furthermore results indicate distributed memory parallel computing help alleviate constraints size problem exe cuted finally methods developed scalable suggest within reach model pwr core 193 flow channels explicitly represented 7 acknowledgements authors appreciate work mr jenying wu generating transient results reported paper r marching algorithms elliptic boundary value problems reducing effect global communication gmresm cg parallel distributed memory computers direct methods sparse matrices effect ordering preconditioned gmres algorithm numerical methods engineers scientists supercomputing applied nuclear reactors assessment advanced numerical methods twophase fluid flow flexible innerouter preconditioned gmres algorithm gmres generalized minimal residual algorithm solving nonsymmetric linear systems comparison preconditioned nonsymmetric krylov methods largescale mimd machine performance conjugate gradientlike algorithms transient twophase subchannel analysis gmresr family nested gmres methods computational challenges developing efficient parallel algorithms datadependent computation thermalhydraulics supercomputer applications tr