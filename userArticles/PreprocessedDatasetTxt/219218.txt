comparison id3 backpropagation english texttospeech mapping performance error backpropagation bp id3 learning algorithms compared task mapping english text phonemes stresses distributed output code developed sejnowski rosenberg shown bp consistently outperforms id3 task several percentage points three hypotheses explaining difference explored id3 overfitting training data b bp able share hidden units across several output units hence learn output units better c bp captures statistical information id3 conclude hypothesis c correct augmenting id3 simple statistical learning procedure performance bp closely matched complex statistical procedures improve performance bp id3 substantially domain b introduction universal learning algorithm take sample training examples arbitrary unknown function f produce good approximation f see dietterich 1989 instead every learning algorithm embodies assumptions bias nature learning problems applied algorithms example assume small number features describing data relevant algorithms assume every feature makes small independent contribution determining classifi cation many algorithms order hypotheses according syntactic simplicity representation attempt find simplest hypothesis consistent training examples unfortunately many popular learning algorithms assumptions embody entirely knownor known stated terms difficult check given application domain example quinlans decisiontree algorithm id3 assumes unknown function f represented small decision tree however given new learning problem difficult know whether assumption holds without first running id3 algorithm result good understanding range problems id3 appropriate similarly backpropagation algorithm rumelhart hinton williams 1986 assumes minimum unknown function f represented multilayer feedforward network sigmoid units although many successful applications backpropagation touretzky 1989 1990 still lack understanding situations appropriate furthermore clear statements assumptions made id3 backpropagation unavailable understand relationship two algorithms investigators even suggested algorithms making similar assumptions lorien pratt personal communication hence confront two related questions first assumptions embodied id3 backpropagation equivalently situations algorithms applied second id3 backpropagation related one conceive two different approaches answering questions theoretical approach could analyze algorithms attempt articulate assumptions experimental approach could test two algorithms nontrivial problems compare behavior paper take experimental approach apply id3 backpropagation task mapping english words pronunciations task pioneered sejnowski rosenberg 1987 famous nettalk sys tem employed backpropagation rosenbergs doctoral dissertation 1988 included analysis experiments domain replication work discover backpropagation outperforms id3 task demonstrates id3 backpropagation make identical assumptions go investigate difference id3 backpropagation formulate three hypotheses explain difference conduct experiments test hypotheses experiments show id3 combined simple statistical learning procedures nearly match performance bp also present data showing performance id3 backpropagation highly correlated collection binary concept learning problems data also show id3 bp tend agree concepts easy difficult given bp substantially awkward timeconsuming apply results suggest following methodology applying algorithms problems similar nettalk task first id3 combined statistical learning procedures applied performance adequate need apply backpropagation however id3s performance inadequate still used estimate performance backpropagation much expensive backpropagation procedure employed see yields better classifier id3 backpropagation 3 2 task conduct comparisons id3 backpropagation chosen task mapping english text speech complete texttospeech system involves many stages processing ideally sentences parsed identify word senses parts speech individual words senses mapped strings phonemes stresses finally phonemes stresses combined various techniques generate sound waves excellent review see klatt 1987 phoneme equivalence class basic sounds example phoneme p individual occurrences p slightly different considered p sounds example two ps lollypop pronounced differently members equivalence class phoneme p use 54 phonemes see appendix a1 stress perceived weight given syllable word example first syllable lollypop receives primary stress third syllable receives secondary stress middle syllable unstressed stress information coded assigning one six possible stress symbols letter consonants generally receive one symbols indicate principal vowel syllable left right respectively consonant vowels generally marked code 0 none 1 primary 2 secondary indicate degree stress lastly silent stress assigned blanks let l set 29 symbols comprising letters az comma space period data sets comma period appear let p set 54 english phonemes set 6 stresses employed sejnowki rosenberg task learn mapping f specifically f maps word length k string phonemes length k string stresses length k example notice letters phonemes stresses aligned silent letters mapped silent phoneme defined f complex discrete mapping large range assume word contains 28 letters length antidisestablishmen tarianism range would contain 10 70 elements existing learning algorithms focus primarily learning boolean conceptsthat functions whose range set f0 1g algorithms cannot applied directly learn f fortunately sejnowski rosenberg 1987 developed technique converting complex learning problem task learning collection boolean concepts begin reformulating f mapping g sevenletter window single phoneme single stress example word lollypop would converted 8 separate sevenletter windows function g applied 8 windows results concatenated obtain phoneme stress strings mapping function g range 324 possible phonemestress pairs substantial improvement finally sejnowski rosenberg code possible phonemestress pair 26bit string 21 bits phoneme 5 bits stress bit code corresponds property phoneme stress converts g 26 separate boolean functions h 26 function h maps seven letter window set f0 1g assign phoneme stress window 26 functions evaluated produce 26bit string string mapped nearest 324 bit strings representing legal phonemestress pairs used hamming distance two strings measure distance sejnowski rosenberg used angle two strings measure distance report euclidean distance metric gave similar results tests euclidean metric obtained results identical reported paper reformulation possible apply boolean concept learning methods learn h however individual h must learned extremely well order obtain good performance level entire words errors aggregate example h learned well 99 correct errors among h independent 26bit string correct 77 time average word 7 letters whole words correct 16 time far discussed representation outputs mapping learned inputs represented straightforward fashion using approach recommended sejnowski rosenberg 1987 sevenletter window represented concatenation seven 29bit strings 29bit string represents letter one bit letter period comma blank hence one bit set 1 29bit string produces string 203 bits window 203 bits provide input features learning algorithms id3 backpropagation 5 3 algorithms 31 id3 id3 simple decisiontree learning algorithm developed ross quinlan 1983 1986b constructs decision tree recursively starting root node selects feature tested node feature whose mutual information output classification greatest sometimes called information gain criterion training examples partitioned examples 1 algorithm invoked recursively two subsets training examples algorithm halts examples node fall class point leaf node created labelled class question basic operation id3 quite similar cart algorithm developed breiman friedman olshen stone 1984 treegrowing method developed lucassen mercer 1984 algorithm extended handle features 2 values features continuous values well implementation id3 employ windowing quinlan 1983 chisquare forward pruning quinlan 1986a kind reverse pruning quin lan 1987 apply one simple kind forward pruning handle inconsistencies training data remaining features zero information gain growth tree terminated leaf class training examples chosen label leaf case tie leaf assigned class 0 apply id3 task algorithm must executed 26 timesonce mapping h executions produces separate decision tree 32 backpropagation error backpropagation method rumelhart hinton williams 1986 widely applied train artificial neural networks however standard form algorithm requires substantial assistance user specifically user must specify transfer function artificial neuron unit network architecture number layers interconnections number hidden units layer learning rate momentum term initial weight values target thresholds 1 furthermore user must decide terminate training make comparison id3 backpropagation fair necessarily transform bp userassisted method algorithm involves user assistance developed transformation call resulting algorithm bpcv backpropagation crossvalidation define bpcv fix userspecified properties set remaining parameters via crossvalidation using methods introduced lang waibel hinton 1990 explained bpcv one hidden layer fully connected input layer output layer every unit hidden output layers implemented taking dot product vector weights w vector incoming activations x adding bias applying logistic function continuous differentiable approximation linear threshold function used perceptrons several parameters given fixed values learning rate always 025 momentum term 09 target thresholds used criterion minimized sum squared error sse basically parameters except target thresholds used sejnowski rosenberg conducted crossvalidation found performance insensitive parameter choices remaining parametersnumber hidden units random starting weights stopping total sum squared error tsseare set following crossvalidation procedure given set examples subdivide three sets training set tr crossvalidation set cv test set test execute backpropagation several times training set tr varying number hidden units random starting weights pass training data test performance network cv goal search parameter space find parameters give peak performance crossvalidation set parameters used train backpropagation union tr cv good estimate generalization performance obtained testing test advantage crossvalidation training information test set employed training hence observed error rate test set fair estimate true error rate learned network contrasts common unsound practice adjusting parameters optimize performance test set one advantage bpcv nettalk task unlike id3 necessary apply bpcv 26 output bits learned simultaneously indeed 26 outputs share set hidden units may allow outputs learned accurately however id3 batch algorithm processes entire training set bp incremental algorithm makes repeated passes data complete pass called epoch epoch training examples inspected oneatatime weights network adjusted reduce squared error outputs used implementation provided mcclelland rumelhart 1988 outputs bp floating point numbers 0 1 adapt hamming distance measure mapping nearest legal phonemestress pair used following distance measure dx reduces hamming distance x boolean vectors id3 backpropagation 7 table 1 optimal network size via crossvalidation number letters number hidden units correct tsse epochs 28 100 693 1041 19 693 477 28 33 data set sejnowski rosenberg provided us dictionary 20003 words corresponding phoneme stress strings dictionary drew random without replacement training set 800 words crossvalidation set 200 words test set 1000 words 4 results 41 crossvalidation training presenting results study first discuss results crossvalidation procedure bpcv performed series runs systematically varied number hidden units 40 60 80 100 120 140 160 180 random starting weights four sets random weights generated net work performance crossvalidation set evaluated complete pass training data epoch networks trained except cases training continued 60 epochs ensure peak performance found table 1 shows peak performance percent letters correctly pronounced network size total sum squared error tr gave peak performance tsse numbers appropriately adjusted number training examples used decide terminate training entire training set tr cv based runs best network size 160 hidden units completed crossvalidation training proceeded merge training set crossvalidation set form 1000word training set crossvalidation training stored snapshot weight values first complete epoch random network generated hence perform training entire training set used best stored 160hidden unit snapshot starting point 2 original training set tr contained 5807 sevenletter windows percent letters correct epochs figure 1 training curve best 160hidden unit network vertical bar indicates point maximum performance full training set tr cv contains 7229 sevenletter windows hence target tsse full training set 554 surprised figures shown table 1 since expected reasonably small network eg 80 hidden units would give good fit data however table clearly shows generalization steadily improves quality fit training data improves furthermore figure 1 shows training network continues past point peak performance performance decline appreciably previous work sejnowski rosenberg 1986 rosenberg 1988 used networks 40 80 120 hidden units however knowledge one previously conducted systematic study relationship network size performance nettalk task similar results showing larger networks give improved performance published martin pittman 1990 42 performance comparison table shows percent correct 1000word test set words letters phonemes stresses letter considered correct phoneme stress correctly predicted mapping nearest legal phoneme id3 backpropagation 9 table 2 percent correct 1000word test set level aggregation correct method word letter phoneme stress bit mean id3 96 656 787 772 961 bpcv 136 706 808 813 967 difference cell significant disagree 1385 192 agree 5857 809 backpropagation incorrect correct incorrect correct stress word correct letters correct virtually every difference table word letter phoneme stress levels statistically significant using onetailed test difference two proportions based normal approximation binomial distribution hence conclude substantial difference performance id3 bpcv task noted although test set contains 1000 disjoint words sevenletter windows test set also appear training set specifi cally 946 131 windows test set appear 1000word training set represent 578 distinct windows hence performance letter phoneme stress levels artificially high one concerned ability learning methods handle unseen cases correctly however one interested probability letter phoneme stress unseen word correctly classified numbers provide right measure take closer look performance difference study exactly 7242 sevenletter windows test set handled algorithms table 2 categorizes windows according whether correctly classified algorithms one algorithms neither one table shows windows correctly learned bpcv form superset learned id3 instead two algorithms share 4239 correct windows algorithm correctly classifies several windows algorithm gets wrong overall result bpcv classifies 361 windows table 3 average percent correct 1000word test set five trials level aggregation correct method word letter phoneme stress bit mean id3 102 652 791 765 961 bp 151 713 813 817 967 difference cell significant correctly id3 shows two algorithms overlap substantially learned fairly different texttospeech mappings information table summarized correlation coefficient specifically let x id3 xbpcv random variable 1 id3 bpcv respectively makes correct prediction letter level case correlation x id3 xbpcv 5648 four cells table 2 equal correlation coefficient would zero reference independent runs bpcv training set different random starting states correlation coefficient 6955 weakness table 2 shows performance values one particular choice training test sets replicated study four times total 5 independent trials trial randomly drew without replacement two sets 1000 words dictionary 20003 words note means overlap among five training sets among five test sets table 3 shows average performance 5 runs differences significant 0001 level using ttest paired differences another weakness table 2 shows performance values 1000 word training set might relative performance difference id3 bpcv might change size training set changes table 4 shows case rows table give results running id3 bpcv several different sizes training sets case bpcv trained using crossvalidation training methodology outlined four runs networks 5 10 20 40 80 120 160 hidden units difference methodology outlined training tr determining peak generalization performance testing 200word cv retrain union tr cv since would create training sets large instead simply tested best network 1000word test conclude consistent difference id3 bpcv performance algorithms increase size training set difference still observed remainder paper attempt understand nature differences bpcv id3 main approach experiment modifications two algorithms enhance eliminate differences id3 backpropagation 11 table 4 percent correct 1000word test set sample level aggregation correct size method word letter phoneme stress bit mean bpcv 16 492 597 731 939 100 id3 20 473 641 658 940 200 id3 44 566 705 722 951 bpcv 71 611 722 782 954 400 id3 62 587 737 721 955 bpcv 113 664 770 797 960 800 id3 96 638 778 756 962 bpcv 153 709 810 812 966 1000 id3 96 656 787 772 964 bpcv 147 709 811 814 966 difference cell significant table 5 results applying three overfittingprevention techniques level aggregation correct method data set word letter phoneme stress bit mean id3 test 96 656 787 772 961 b id3 2 cutoff test 91 648 784 771 961 c id3 pruning test 93 624 769 751 958 id3 rules test 82 651 785 772 961 unless stated otherwise experiments performed using 1000word training set 1000word test set table 2 5 three hypotheses causes differences id3 bpcv three hypotheses hypothesis 1 overfitting id3 overfit training data seeks complete consistency causes make errors test set hypothesis 2 sharing ability bpcv share hidden units among h allows reduce aggregation problem bit level hence perform better hypothesis 3 statistics numerical parameters network allow capture statistical information captured id3 hypotheses neither mutually exclusive exhaustive following three subsections present experiments performed test hypotheses 51 tests hypothesis 1 overfitting tendency id3 overfit training data well established cases data contain noise three basic strategies developed addressing problem criteria early termination treegrowing process b techniques pruning trees remove overfitting branches c techniques converting decision tree collection rules implemented tested one method strategies table 5 summarizes results first row repeats basic id3 results given comparison purposes second row shows effect applying 2 test 90 confidence level decide whether growth decision tree statistically justified quin lan 1986a authors reported mooney et al 1989 hurts performance nettalk domain third row shows effect applying quinlans technique reduceerror pruning quinlan 1987 mingers 1989 id3 backpropagation 13 provides evidence one best pruning techniques row decision tree built using 800word tr set pruned using cv crossvalidation set finally fourth row shows effect applying method converting decision tree collection rules quinlan 1987 describes threestep method converting decision trees rules first path root leaf converted conjunctive rule second rule evaluated remove unnecessary conditions third rules combined unnecessary rules eliminated experiment performed first two steps third step expensive execute rule set contains rules none techniques improved performance id3 task suggests hypothesis 1 incorrect id3 overfitting data domain makes sense since source noise domain limited size sevenletter window existence small number words like read one correct pronunciation sevenletter windows sufficient correctly classify 985 words 20003word dictionary may also explain observe overfitting excessive training crossvalidation runs backpropagation either 52 test hypothesis 2 sharing second hypothesis claims key bpcvs superior performance fact output units share single set hidden units one obvious way test sharing hypothesis would develop version id3 permitted sharing among 26 separate decision trees learned could see sharedid3 improved performance alternative remove sharing backpropagation training 26 independent networks one output unit learn 26 h mappings hypothesis 2 correct sharing among separate networks see drop performance compared single network shared hidden units furthermore decrease performance decrease differences bpcv id3 measured correlation errors call single network hidden units shared output units bp1 call 26 separate networks bp26 delicate issue arises training bp26 ideally want train collection 26 networks differences bp1 result lack shared hidden units means total summed squared error training set bp26 bp1 goal training procedure find among bp26 networks collection whose performance crossvalidation set maximized hence used following procedure first measured sum squared error training set 26 bits learned bp1 second train bp26 networks followed crossvalidation procedure trying alternative random seeds numbers hidden units time always 14 dietterich hild bakiri terminated training individual network attained squared error observed large network crossvalidation tried networks 1 2 3 4 5 10 20 hidden units four random seeds network size finally selected network whose summed squared error minimum 200word crossvalidation test set cv surprisingly unable train successfully separate networks target error level 1000word training set explored smaller subsets 1000word training set 800 400 200 100 50words found training succeed training set contained 50 words 100word training set example individual networks often converged local minima even though bp1 network avoided minima specifically bits 4 6 13 15 18 21 25 could trained criterion even 2000 epochs bit 100word training set conducted detailed study attempt understand training problem performed hundreds runs varying number hidden units learning rate momentum term initial random weights attempt find configuration could learn single bit level bp1 none runs succeeded run bp26 training converged error handful training examples 00 error remaining training examples 10 contrast errors bp1 extreme table 6 shows collection sevenletter windows test set squared error windows nine different training runs first training run bp1 120 units trained epochs next four columns show runs bp26 5hiddenunit network four different random starting seeds trained learning rate 4 momentum 8 initial random values range 0505 last four columns show runs bp26 10 hiddenunit network four different random starting seeds trained learning rate 4 momentum 7 initial random values range 0404 demonstrates even shared hidden units aid classification per formance certainly aid learning process consequence training problem able report results 50word training set crossvalidation training bp1 see determined best network training set contained 120 hidden units trained sumsquared error 13228 table 7 summarizes training process 26 output bits bp26 row gives number hidden units best bp26 network squared error obtained bp1 network squared error obtained bp26 network number epochs required training bp26 notice individual bit slightly overtrained compared bp1 program accumulates squared errors epoch stops falls target error level performance improves epoch final squared error somewhat lower table 8 shows performance 26 networks training test sets performance training set virtually identical 120hiddenunit id3 backpropagation 15 table 6 comparison individual errors bit window bp1 bp26 austr 10 sotted 10 breadwi bucksaw 10 10 mois 1000 10 cinnamo figurat 0026 10 10 10 10 10 10 lawyer muumuu pettifo 0028 10 10 10 10 10 10 10 10 ilton 1000 10 10 10 10 10 10 10 values shown 000 table 7 training statistics 26 independent networks bit number squared error squared error number hidden units bp1 network bp26 network epochs 9 4 19208 1894 22 5 00894 0074 61 26 20 00011 0 table 8 performance 26 separate networks compared single network 120 shared hidden units trained 50word training set tested 1000word test set level aggregation correct method data set word letter phoneme stress bit mean id3 test 08 415 605 601 926 b bp 26 separate nets train 920 990 990 1000 999 test 17 463 579 722 932 c bp 120 hidden units train 920 987 990 997 999 test 16 492 597 731 934 difference bc train test 01 gamma29 gamma18 gamma09 gamma02 difference ac test gamma08 gamma77 08 gamma130 gamma13 difference significant id3 backpropagation 17 table 9 correlation coefficients replication x id3 xbp1 x id3 xbp26 5167 4942 c 5347 5062 4934 4653 average decrease 0208 network shows training regime successful performance test set however shows loss performance sharing hidden units among output units hence suggests hypothesis 2 least partially correct however examination correlation id3 bpcv indicates wrong correlation x id3 xbp1 ie bp single network 5428 whereas correlation x id3 xbp26 5045 replicated comparison 5 times 5 different training testing sets using less rigorous efficient uncrossvalidated training procedure table 9 shows resulting correlation coefficients paired differences ttest shows differences correlation coefficients significant 0001 level hence removal shared hidden units actually made id3 bp less similar rather similar hypothesis 2 claims conclusion sharing backpropagation important improving training performance explain id3 bpcv performing differently 53 tests hypothesis 3 statistics performed three experiments test third hypothesis continuous parameters bpcv networks able capture statistical information id3 fails capture first experiment took outputs backpropagation network thresholded values 5 mapped 1 values 5 mapped mapping nearest legal phonemestress pair thresholding values change distances measured outputs legal phoneme stress patterns table 10 presents results 1000word training set results show thresholding significantly drops performance back propagation indeed phoneme level decrease enough push bpcv id3 levels aggregation bpcv still outperforms id3 table 10 performance backpropagation thresholded output values trained 1000word training set tested 1000word test set level aggregation correct method data set word letter phoneme stress bit mean id3 legal test 96 656 787 772 961 b bpcv legal test 136 706 808 813 967 c bpcv thresholded test 112 677 784 800 963 difference cb test gamma24 gamma29 gamma24 gamma13 gamma04 difference significant id3 backpropagation 19 results support hypothesis continuous outputs neural network aid performance bpcv however thresholding outputs bpcv cause behave substantially like id3 correlation x id3 xbpcv thresh 5685 compared 5648 xbpcv small increase close examination data shows sevenletter windows lost ie incorrectly classified bpcv thresholded include 120 windows correctly classified id3 112 windows incorrectly classified id3 hence mistakes introduced thresholding nearly independent mistakes made id3 experiment demonstrates importance continuous outputs tell us kind information captured continuous outputs reveal anything role continuous weights inside network must turn two experiments second experiment modified method used map output 26 bit string one 324 legal phonemestress pairs instead considering possible legal phonemestress pairs restricted attention phonemestress pairs observed training data specifically constructed list every phonemestress pair appears training set along frequency occurrence appendix a3 shows frequency information 1000word training set testing 26element vector produced either id3 bpcv mapped closest phonemestress pair appearing list ties broken favor frequent phonemestress pair call observed decoding method sensitive phonemestress pairs frequencies observed training set table 11 presents results 1000word training set compares previous technique legal decoded nearest legal phonemestress pair key point notice decoding method leaves performance bpcv virtually unchanged substantially improves performance id3 indeed eliminates substantial part difference id3 bpcvthe two methods statistically indistinguishable word phoneme levels mooney et al 1989 comparative study id3 bpcv task employed version decoding technique random tie breaking obtained similar results training set 808 words dictionary occur frequently english text examination correlation coefficients shows observed decoding increases slightly similarity id3 bpcv correlation id3observed xbpobserved 5865 compared 5648 legal decod ing furthermore observed decoding almost always monotonically better ie windows incorrectly classified legal decoding become correctly classified observed decoding vice versa table 12 shows results four replications paireddifferences ttest concludes correlation coefficient increases observed decoding significance level better 0001 results conclude bpcv already capturing information frequency occurrence phonemestress pairs table 11 effect observed decoding learning performance level aggregation correct method data set word letter phoneme stress bit mean id3 legal test 96 656 787 772 961 b bpcv legal test 136 706 808 813 967 c id3 observed test 130 701 815 792 964 bpcv observed test 143 715 820 814 967 id3 improvement ca test 34 45 28 20 03 difference cell significant table 12 correlation id3 bpcv observed decoding data set legal observed 5648 5865 c 5593 5796 5722 5706 e 5563 5738 average increase 0136 id3 backpropagation 21 id3 capturing nearly much hence experiment strongly supports hypothesis 3 drawback observed strategy never decode window phonemestress pair seen hence certainly make mistakes test set however phonemestress pairs observed training set make small fraction windows test set example 7 phonemestress pairs appear 1000 word test set appear 1000word training set test set account 11 7242 windows 015 train 19003 words dictionary appear 1000word test set would one phonemestress pair present test set would appear training set would appear one window final experiment concerning hypothesis 3 focused extracting additional statistical information training set motivated klatts 1987 view ultimately lettertophoneme rules need identify exploit morphemes ie commonlyoccurring letter sequences appearing within words fore analyzed training data find letter sequences length 1 2 k retained b mostfrequentlyoccurring sequences length parameters b determined crossvalidation described retained letter sequence formed list phonemestress strings sequence mapped training set frequencies example five pronunciations letter sequence ation training set hphoneme stringi hstress stringi hfrequencyi esxn 10 22 sxn 10 1 decoding word scanned left right see contains one top b letter sequences length l varying l k 1 word contains sequence letters corresponding sequence processed follows first l windows centered letters sequence evaluated ie 26 decision trees feedforward network obtain 26bit string strings concatenated produce bit string length l delta 26 observed pronunciations sequence converted l delta 26bit string according code given appendix a1 finally unknown string mapped nearest observed bit strings decoding block control skips end matched lletter sequence resumes scanning another top b letter sequence length l scan complete parts word yet matched rescanned look blocks length l gamma 1 every letter word eventually processed every individual letter block length 1 call technique block decoding 22 dietterich hild bakiri table 13 effect block decoding learning performance level aggregation correct method data set word letter phoneme stress bit mean id3 legal test 96 656 787 772 961 b bpcv legal test 136 706 808 813 967 c id3 block test 172 733 839 804 967 bpcv block test 185 737 838 813 967 id3 improvement ca test 76 77 52 32 06 difference cell significant agree 6147 849 disagree 1095 151 1905621374 id3 backpropagation incorrect correct incorrect correct employed crossvalidation determine maximum block length k number blocks b store evaluating different values training 800 words testing 200word crossvalidation testing set tried values 1 2 3 4 5 6 k values 100 200 300 400 b id3 peak performance attained 100 bpcv peak performance attained cases performance much sensitive k b table 13 shows performance results 1000word test set block decoding significantly improves id3 bpcv id3 improved much especially word level indeed two methods cannot distinguished statistically level aggregation furthermore correlation coefficient x id3block xbpblock 6122 substantial increase compared 5648 legal decoding hence block decoding also makes performance id3 bpcv much similar table 13 shows 7242 sevenletter windows test set handled id3 bpcv table 14 shows correlation coefficients along four replications paireddifferences ttest concludes correlation coefficient increases block decoding significance level better 0001 id3 backpropagation 23 table 14 correlation id3 bpcv block decoding table 15 classification test set windows id3 bpcv block decoding data set legal block 5648 6122 c 5593 6138 5722 5832 e 5563 6028 average increase 0385 note method supplies additional information id3 bpcv could expected improve correlation algorithms furthermore source new information would probably benefit poorer performing algorithm id3 better performing algorithm nonetheless fact block decoding eliminates differences id3 bpcv provides strong evidence identified important cause difference two methods hypothesis 3 correct experiment also suggests block decoding technique useful adjunct learning algorithm applied domain 6 discussion 61 improving algorithms many directions explored improving algorithms pursued several directions order develop highperformance texttospeech system efforts reported detail elsewhere bakiri 1991 one approach design better output codes phonemestress pairs experiments shown bch error correcting codes provide better output codes output code used paper randomlygenerated bitstrings produce similar performance improvements see dietterich bakiri 1991 another approach widen sevenletter window introduce context lucassen mercer 1984 employ 9letter window also include inputs phonemes stresses four letters left letter center window phonemes stresses obtained execution letters already pronounced scan lefttoright experiments 15letter window indicate produces substantial performance gains well however find works better word scanned righttoleft instead third technique improving performance supply additional input features program one feature letters helps bit indicating whether letter vowel consonant feature phonemes helps whether phoneme tense lax fourth technique pursued refine block decoding method blocks chosen carefully consideration statistical confidence decoding consider overlapping blocks fifth direction pursued implement buntines 1990 method obtaining class probability estimates decision trees algorithm produces fairly accurate probability estimates leaves decision tree use estimates map nearest phonemestress pair curious know whether approach would capture statistical information provided observed block decoding experiments showed however observed block decoding superior simply using legal decoding even observed decoding class probability trees id3 backpropagation 25 table correcting code sevenletter phoneme stress context domainspecific input features observed decoding simplified stresses level aggregation correct training set word letter phoneme stress bit mean 1000 words 406 841 870 914 921 19003 words 648 914 937 951 957 combining errorcorrecting output codes wider window right toleft scan include phoneme stress context domainspecific features obtained excellent performance 1000word training test sets table shows bestperforming configuration trained 1000 words trained 19003 words details configuration described bakiri 1991 unable test similar configuration bpcv huge computational resources would required bakiri 1991 describes study human judges compared output system output dectalk klatt 1987 lettertosound rule base results show system two machine learning approaches significantly outperform dectalk 62 applying id3 aid bpcv interesting observation studies performance id3 bpcv highly correlated suggests methodology using id3 aid bpcv even domains bpcv outperforms id3 many realworld applications inductive learning substantial vocabulary engineering required order obtain high performance vocabulary engineering process typically involves iterative selection testing promising features test features necessary train bpcv network using themwhich timeconsuming performance id3 correlated bpcv used instead test feature sets good set features identified bpcv network trained examine idea detail consider table 17 shows performance id3 bpcv 26 individual bits ie without decoding algorithm trained 1000word training set tested 1000word test set 160hidden unit network employed bpcv correlation coefficient 9817 significant well 001 level hence conclude generalization performance id3 good predictor generalization performance bpcv 26 dietterich hild bakiri table 17 performance complexity difficulty learning 1000word training set 1000word test set id3 bp 3 7104 981 7110 982 6 7065 976 7057 974 7 7207 995 7191 993 9 7206 995 7203 995 14 7148 987 7120 983 19 7242 1000 7242 1000 22 6658 919 6738 930 26 7242 100 id3 backpropagation 27 7 conclusions relative performance id3 backpropagation texttospeech task depends decoding technique employed convert 26 bits se jnowskirosenberg code phonemestress pairs decoding nearest legal phonemestress pair obvious approach reveals substantial difference performance two algorithms experiments investigated three hypotheses concerning cause performance difference first hypothesisthat id3 overfitting training datawas shown incorrect three techniques avoid overfitting applied none improved id3s performance second hypothesisthat ability backpropagation share hidden units factorwas shown partially correct sharing hidden units improve classification performance backpropagation andperhaps importantlythe convergence gradient descent search however analysis kinds errors made id3 backpropagation without shared hidden units demonstrated different kinds errors hence eliminating shared hidden units produce algorithm behaves like id3 suggests development shared id3 algorithm could learn multiple concepts simultaneously unlikely produce performance similar bpcv third hypothesisthat backpropagation capturing statistical information mechanism perhaps continuous output activationswas demonstrated primary difference id3 bpcv adding ob served decoding technique algorithms level performance two algorithms classifying test cases becomes statistically indistinguishable word phoneme levels adding block decoding technique differences algorithms statistically insignificant given block decoding two algorithms perform equivalently given bpcv much awkward apply timeconsuming train results suggest tasks similar texttospeech task id3 block decoding clearly algorithm choice applications bpcv id3 play extremely valuable role exploratory studies determine good sets features predict difficulty learning tasks paper also introduced new method experimental analysis computes error correlations measure effect algorithm modifications shown method applied discover ways algorithms related broader application methodology improve understanding assumptions biases underlying many inductive learning algorithms 28 dietterich hild bakiri 8 acknowledgements authors thank terry sejnowski providing nettalk phonemic dictionary without work would impossible correspondence jude shavlik ray mooney geoffrey towell helped clarify possible kinds decoding strategies discussions lorien pratt aided design crossvalidation studies research supported nsf grant numbers ccr87 16748 iri8657316 presidential young investigator award matching support sun microsystems 9 r converting english text speech machine learning approach classification regression trees theory learning classification rules school computing science limits inductive learning comparative study id3 backpropagation english texttospeech mapping review texttospeech conversion english information theoretic approach automatic determination phonemic base forms explorations parallel distributed processing empirical comparison pruning methods decision tree induction experimental comparison symbolic connectionist learning algorithms learning efficient classification procedures application chess endgames simplifying decision trees learning internal representations error propagation learning connection spelling sound network model oral reading parallel networks learn pronounce english text advances neural information processing systems 1 advances neural information processing systems 2 tr ctr thomas g dietterich approximate statistical tests comparing supervised classification learning algorithms neural computation v10 n7 p18951923 oct 1998 kang keysun choi two approaches resolution word mismatch problem caused english words foreign words korean information retrieval proceedings fifth international workshop information retrieval asian languages p133140 september 30october 01 2000 hong kong china john case sanjay jain matthias ott arun sharma frank stephan robust learning aided context proceedings eleventh annual conference computational learning theory p4455 july 2426 1998 madison wisconsin united states miroslav kubat robert c holte stan matwin machine learning detection oil spills satellite radar images machine learning v30 n23 p195215 feb march 1998 melody kiang comparative assessment classification methods decision support systems v35 n4 p441454 july rich caruana virginia r de sa benefitting variables variable selection discards journal machine learning research 3 312003 rich caruana multitask learning machine learning v28 n1 p4175 july 1997 walter daelemans antal van den bosch jakub zavrel forgetting exceptions harmful language learning machine learning v34 n13 p1141 feb 1999 sreerama k murthy automatic construction decision trees data multidisciplinary survey data mining knowledge discovery v2 n4 p345389 december 1998