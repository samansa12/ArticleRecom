extracting query modifications nonlinear svms searching www users often desire results restricted particular document category ideally user would able filter results text classifier minimize false positive results however current search engines allow simple query modifications automate process generating effective query modifications introduce sensitivity analysisbased method extracting rules nonlinear support vector machines proposed method allows user specify desired precision attempting maximize recall method performs several levels dimensionality reduction vastly faster searching combination feature space moreover effective realworld data b introduction searching www users often desire results specific category personal home pages conference nouncements unfortunately many search engines return results multiple categories thus forcing user manually filter results one solution use automated classifier identifies whether given result close users desired category however since search engines often low precision given category may necessary retrieve large number documents engines may expensive impossible search engines typically allow retrieving certain maximum number hits query single query modification home page improve precision often expense recall allow high precision high recall introduce method generate collection query modifications satisfies users desired precision attempts maximize recall copyright held authorowners www2002 may 711 2002 honolulu hawaii usa acm 1581134495020005 use nonlinear support vector machine svm initially classify documents problem building classifiers generalizewell especially important domain text classification typical problem instances highdimensional input spaces mapping word vector relatively small number positive examples scarcity positive examples related cost humans handclassify examples svms specifically designed generalizewell highdimensional spaces examples moreover shown extremely accurate robust text classification problems 8 9 use svms favor classification methods also know work extracts symbolic information svms hence also motivated desire use high accuracy svms extract valuable symbolic features similar earlier works multilayer perceptrons 14 feature extraction process uses sensitivity analysis underlying svm identify linear model single query modification explains subset desired documents extract additional query modifications iterating procedure new dataset composed false negatives linear model negative examples procedure repeats generating additional query modifications progress made way feature extraction method similar ripper 1 iteratively extract rules however method differs use svm guide choice rule selection end identify set query modifications whose combination covers large portion positive documents greatly reduces number false positives method yields precise search results built top existing search engines form metasearch engine moreover using svms guide rule search extracted rules predisposed many generalization qualities originating possesses approach differs work learning query modifications 6 producing set query modifications work together improve recall opposed set individually effective modifications may may work well together paper divided five sections section 2 discuss www search engines metasearch engines emphasis results used improve metasearch engines section 3 describe method producing effective query modifications uses text preprocessing svm classification sensitivity analysis dataset deflation procedure section 4 gives experimental results identifying personal home pages conference pages shows method gives high precision improved recall finally section 5 summarizes work discussion web page patterns exploited methods dimensionality reduction properties 2 www search metasearch primary tool accessing data web search engine altavista northern light google excite others search engines accept keyword queries return list relevance ranked results relevance usually topical measure every search engine user experienced typical search queries often return thousands urls placing burden user manually filter results one common trick combating problem spike search query modification designed narrow results specific context category example searching personal home pages machine learning researchers adding term home page increase density valuable results encouraging search engine rank personal home pages higher nonhome pages however using single query modification increases search precision expense call ie many home pages eg contain home page may missed coverage search query improved using metasearch engine dogpile savvysearch 7 metacrawler 13 profusion 3 submits users query multiple search engines fuses results allowing potentially higher recall simple metasearch engines fuse results considering titles urls short summaries returned underlying search engines result difficult assess topical relevance determine particular result type desired user addition since metasearch engines combine many search engine results risk one search engine causing total precision drop unlike typical metasearch engines contentbased metasearch en gines inquirus 10 download possible results consider full html page ranking although strategy may improve accuracy relevance ranking allow users control desired category results best organizes existing results metasearch engine inquirus low precision underlying search engines also low precision result improved coverage guarantee improved recall search metasearch engines northern light savvysearch allow users specify desired category limited set categories northern light constrains user searches results known fall chosen cluster savvysearch submits query search engines known desired category mp3 news sites approach offers user improved control still limited problem arises users desired document category exactly match provided choices user wishes submit query general purpose search engine improve call way provide custom categories search tools neither category personal home pages conference pages nevertheless users often desire documents welldefined category easily localized like covered specialized metasearch engines work motivated desire build metasearch engine adaptively specialize many different document cat egories prototype system inquirus 2 5 4 currently allows users focus search categories personal home pages research papers product reviews previous versions inquirus 2 used handcoded query modifications improve search performed search engines work gives method effective query modifications high precision high recall generated automatically 3 finding query modifications method automatically identifying effective query modifications fivestep process uses labeled examples first stage preprocess textual content html document ngram features appear discriminatory power document features labels train nonlinear svm classify documents sensitivity analysis svm used estimate importance document features svm clas sifier important features exhaustively analyzed find combination query modifications yields highest recall desired level precision documents labeled true positives best query modification removed dataset process repeats new svm classifier new query modifications found entire method described greater detail next four subsections 31 preprocessing preprocessing extracts full text title text document converts features consist three consecutive words nonletter characters converted whitespace capital letters converted lower case every page training set converted two feature histograms constructed positive negative exemplars since number features per document order thou sands reduce number features eliminating features rare proper names common features stop words removed feature scoring process well however since distinguish title text full document text particular stop word title apostrophe could strong feature even though common text dimensionality reduction considers relative ability given feature distinguish positive negative exemplars assigning score feature score generated via following four sets positive examples g negative examples g contains feature fg contains feature fg ignoring higher order correlations best features classifying occur one set worst features occur equal percentage set occur frequently sets simple scoring function scoref captures notion probability given equal sizes p n know set document containing feature f came worst one random 05 best absolute cer tainty 1 unfortunately equation would predict features occur perfect classifiers remedy problem add requirement feature occur least threshold percentage documents either p n feature occurring less threshold 75 experiments removed consideration feature scored top n taken experiments used n equal 100 personal home pages 300 conference pages top n features determined page converted binary vector feature assigned f1 1g 1 indicating feature absent 32 svm classification consider set data fx1 y1 xn yn g x input 2 f1 1g target output support vector machine model calculated weighted sum kernel function outputs kernel function svm written inner product gaussian polynomial function obeys mercers condition 15 simplest case kxa training data linearly separable computing svm data corresponds minimizing jjwjj thus svm yields lowest complexity linear classifier correctly classifies data solution w found solving quadratic programming problem defined objective function constraints linear case solution terms lagrange multipliers found primal dual lagrangians many cases lagrange multipliers zero nonzero multipliers correspond data points lie closest decision boundary formalism behind svms generalized accommodate nonlinear kernel functions slack variables miss classifications write output nonlinear svm thus k dot product nonlinear feature space x objective function minimized equation 1 subject box constraint 8 linear constrain 0 c userdefined constant represents balance model complexity approximation er ror lagrangian c multiplied sum magnitude slack variables used absorbing missclassifications equation 2 always single minimum respect lagrange multipliers minimum equation 2 found family algorithms based constrained quadratic programming used faster variation 2 sequential minimal optimization algorithm 11 12 experiments equation 2 minimal equation 1 classification margin maximized training set case linear kernel function kx svm finds decision boundary balanced class boundaries two classes nonlinear case margin classifier maximized nonlinear feature space results nonlinear classification boundary table 1 procedure find effective query modification given svm working dataset 1 calculate sensitivity dfdxjxx 2 find largest magnitude components c sensitivity 3 2 1 combinations c test query modification note statistics b precision rate desired value recall greater best found far save query modification return best found query modification experiments used gaussian kernel function choice usually made reflect smoothness feature space density training data general method selecting choice admittedly ad hoc heuristically set 15 however one may success using cross validation procedure choosing 33 sensitivity analysis training nonlinear svm classify labeled training data use form sensitivity analysis identify components document feature space important classifier suppose classifier linear input space ie linear case input features important largest coefficient magnitudes jw j nonlinear case linearize model point input space taylor expansion obtain f x xv fx linear approximation fx locally accurate vicinity v largest components fxjxv features important linear approximation thus want know inputs critical computing fx choices x near v restrict attention inputs relatively large values jfxj evaluated happen inputs changed produce largest change fx value v take interesting aspect svm learning nonzero lagrange multipliers correspond data points crucial determining maximum margin classifier data point zero lagrange multiplier redundant sense removal would alter solution data points lagrange multipliers solution taking nonzero values socalled support vectors data points strictly needed order build svm model many realworld problems text classification among number support vectors may dramatically fewer number data points thus search useful values v simplified searching points input space also support vectors ie nonzero lagrange multipliers moreover since interested discovering rules identify positive members document category restrict attention positive support vectors equal 1 way search restricted data points actually important svm model input sensitivity svm equation 1 gaussian kernel equation 3 easily derived f x captured region captured region captured region positive examples positive examples linear decision boundary linear decision boundary linear decision boundary positive examples figure 1 rules finding multiple regions positive documents extracted dataset deflation subsequent iterations find regions input space explained simple linear rules identified ruleregion mostly orthogonal therefore complementary previous rulesregions highdimensional input spaces typical text classification problems many orthogonal regions unlike 2dimensional example find effective query modification use procedure described table 1 procedure iterates positive training examples nonzero lagrange multiplier family query modifications found identifying largest components sensitivity vector since mainstream search engine allows user specify weights individual terms ie allow term term cannot use information coefficients sensitivity vector instead treat positive coefficients weight 1 negative 1 others 0 way query modification thought simple threshold classifier weights f1 0 1g line 3 table 1 examine possible variations query modification optionally zero one terms restricted search necessary implicit thresholding performing weights ever search expensive bounds second loop suggest sensitivity analysis may produce duplicate suggestions case hash test query modifications evaluate evaluated thus far end find query modification almost always satisfies prespecified desired precision tends maximize call query modification found procedure search effective query modifications finished 34 rinse repeat finding first effective query modification find additional query modifications altering training dataset svm true positive training exemplars removed dataset leaving negative exemplars false negatives deflating dataset manner find multiple local linear rules effective especially fused together figure 1 shows stylized example method work example three regions mostly disjoint one another region sensitivity analysis discovers linear rule following properties first location decision boundary always edge region closest regions guaranteed searching support vectors second decision boundary parallel direction greatest sensitivity region also implies decision boundary perpendicular regions direction greatest variance result two properties decision boundary viewed tightest slice removes region rest training examples composing multiple slices way allows us potentially separate large regions positive examples small number rules moreover procedure structured rule conjunction terms composition rules formed disjunction ie disjunctive normal form result rules submitted search engine multiple queries 4 experimental results describe method used generate effective query modifications identifying personal home pages conference pages results use notation ti tleterm indicate term occur title doc ument many search engines altavista support queries form query modifications specify terms appear title full text document 41 personal home pages training set experiment consisted 250 personal home pages 999 random urls given negative labels documents preprocessed vectors 100 fea tures negative exemplars contained approximately 1 false positives based human inspection random samples set desired precision least 50 svm optimized set 7 c set 5 extraction procedure set 5 extracted query modifications listed table 2 table one line represents single query modification notice query modifications contain less four additional search terms even though procedure searched many five terms single modification measure effectiveness query modifications tested three queries without modifications first four generated query modifications results presented table 4 whenever search engine returned 50 results manually classified first 50 pages performing statistics manner consistent goal since using query modifications bias search engines ranking function table 2 extracted query modifications personal home page data desired precision set 50 qm extracted query modification favorite sign 3 page interests 4 titlehome home 5 interests page 6 sign titlehome 7 page interests welcome table 3 extracted query modifications conference page data desired precision set 50 qm extracted query modification conference hotel workshops 4 workshop 5 fax email conference sponsored seen table 4 fourth query modification home pages gives better precision cases merged results four query modifications gives highest recall indicated low overlap returned results moreover case combined precision least high desired precision 50 also note test produced many pages listings links personal home pages type pages cv resume pages thus pages labeled false positives statistics far desirable results 42 conference pages conference web page data consisted 529 negative examples 150 positive examples documents preprocessed vectors 300 features set desired precision 50 svm optimized set 15 c set 10 extraction procedure set 5 earlier test first five extracted query modifications shown table 3 earlier test case used top four query modifications augment three test search queries neural net works learning linux test results summarized table 5 experiment manually classified first 20 web pages queries classifying conference pages hand difficult home pages seen merged results maintained precision level close desired precision increasing recall compared individual query modifications moreover combined query modifications offer insurance case single query modification fails 5 discussion conclusions stated section 2 motivation work automate process generating effective query modifications inquirus 2 metasearch engine previously query modifications humangenerated selected based unscientific notion seemed make sense humangenerated queries supported experiments automated method discovered many novel query modifications turn suggest interesting facts documents web method propose paper 51 category patterns part work performed many experiments personal home pages revealed interesting trends types home pages exist different usages language pages used distinguish among example one experiment found geocities effective modifier large number free personal web pages hosted geocities also found childrens home pages often made reference favorite things academics often wrote research interests thus method finding multiple query modifications appears identify language trends occur subgroups category unlikely derive set query modifications 100 recall 100 precision believe work supports use multiple query modifications increasing recall maintaining desired precision 52 eigenqueries data deflation one key facet proposed method dataset deflation step eliminates true positives training data retrains svm procedure table 1 performs constrained limited search effective query modification make analogy query modification found step akin aneigenquery spans large portion training data removing true positives dataset next iteration force procedure find query modifications complement previously found query modifications continuing analogy data deflation similar factoring eigenvector matrix next eigenvector found way proposed method attempts span large portion document space identifying many eigenqueries possible 53 dimensionality reduction another benefit approach performs dimensionality reduction least four different ways preprocessing eliminate ngrams either common uncom mon possible preprocessing eliminate features key identifying subset positive examples believe reducing feature space via preprocessing warranted considering without feature space would consist hundreds thousands words phrases search sensitive positive exemplars also assisted use svms nonlinear model discovered support vectors exemplars positive lagrange multipliers exactly training points whose removal would alter solution thus searching sensitivities positive support vectors reduce number documents must considered sensitivity analysis also provides ranking features input space considering largest components efficiently search optimal ddimensional query modification uses input features finally previously mentioned dataset deflation step eliminates significant portion training data step effect speeding training time svm table 4 test results using extracted home page query modifications total pages refers number examined always top results maximum 50 pages examined home total query pages pages precision information retrieval 0 50 0 information retrieval titles titlepage 3 3 100 information retrieval favorite sign 0 1 0 information retrieval page interests 0 12 0 information retrieval titlehome home 46 50 96 duplicates beagles titles titlepage 5 7 71 beagles page interests 0 9 0 beagles titlehome home 39 50 78 4 positive duplicates starcraft titles titlepage 24 34 71 starcraft favorite sign 27 50 54 starcraft page interests 15 starcraft titlehome home 44 50 88 table 5 test results using extracted conference page query modifications total pages refers number examined always top results maximum 20 pages examined conference total query pages pages precision neural networks 1 20 5 neural networks call neural networks program committee 12 20 60 neural networks conference hotel workshops 17 20 85 neural networks workshop 4 positive duplicates 1 negative duplicate learning conference hotel workshops 15 20 75 duplicates linux conference hotel workshops 12 20 60 simplifying document space searching 54 summary conclusions two alternative extremes identifying query modifications would use linear classifiers exhaustive search feature space former method fails cannot work effectively linearly inseparable case latter approach problematic exponential number binary classifiers would need considered approach exploits underlying regularity documents make www discover higherorder correlations form query modifications contain multiple termseven linearly inseparable feature spacebut also guarantee procedure terminate reasonable amount time also interesting compare approach straw man approach represented aggressive procedure attempts construct large set ridiculously complicated query modifica tions identifies one two pages set essentially acts lookup table training data case one would little hope generalizing realworld data enforcing dimensionality reduction many steps find query modifications appear generalize realworld data despite relatively small size training sets last issue especially important restricted searches say one trying find personal home page particular person common name instead one particular topic restricting complexity query modifications striving maximum recall approach using multiple query modifications increase likelihood finding narrow search results might normally ranked beyond limit given search engine 6 acknowledgements thank frans coetzee insightful discussions sven heinicke andrea ples help performing experiments anonymous reviewers many helpful comments 7 r fast effective rule induction efficient svm regression training smo intelligent fusion multiple architecture metasearch engine supports user information needs recommending web documents based user preferences improving category specific web search learning query modifications text categorization support vector machines learning many relevant features transductive inference text classification using support vector machines context page analysis improved web search fast training support vector machines using sequential minimal optimization using sparseness analytic qp speed training support vector machines metacrawler architecture resource aggregation web extracting refined rules knowledgebased neural networks nature statistical learning theory tr extracting refined rules knowledgebased neural networks nature statistical learning theory fast training support vector machines using sequential minimal optimization architecture metasearch engine supports user information needs efficient svm regression training smo context page analysis improved web search text categorization suport vector machines transductive inference text classification using support vector machines improving category specific web search learning query modifications ctr hai zhuge fuzzy resource space model platform journal systems software v73 n3 p389396 novemberdecember 2004 masayuki okabe kyoji umemura seiji yamada query expansion minimum user feedback transductive learning proceedings conference human language technology empirical methods natural language processing p963970 october 0608 2005 vancouver british columbia canada gabriel l somlo adele e howe using web helper agent profiles query generation proceedings second international joint conference autonomous agents multiagent systems july 1418 2003 melbourne australia aris anagnostopoulos andrei z broder kunal punera effective efficient classification searchengine model proceedings 15th acm international conference information knowledge management november 0611 2006 arlington virginia usa luis gravano vasileios hatzivassiloglou richard lichtenstein categorizing web queries according geographical locality proceedings twelfth international conference information knowledge management november 0308 2003 new orleans la usa luis gravano panagiotis g ipeirotis mehran sahami qprober system automatic classification hiddenweb databases acm transactions information systems tois v21 n1 p141 january bernard j jansen tracy mullen amanda spink jan pedersen automated gathering web information indepth examination agents interacting search engines acm transactions internet technology toit v6 n4 p442464 november 2006