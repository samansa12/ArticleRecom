approach communicationefficient data redistribution address development efficient methods performing data redistribution arrays distributedmemory machines data redistribution important distributedmemory implementation data parallel languages high performance fortran algebraic representation regular data distributions used develop analytical model evaluating communication cost data redistribution using algebraic representation analytical model approach communicationefficient data redistribution developed implementation results intel ipsc860 reported b introduction distributedmemory machines demonstrated potential high performance however use restricted due difficulty programming comput ers programming model based single address space provisions explicit specification data distributions shared arrays recently gained popularity 1 3 13 high performance fortran hpf 6 fortran90 ex tension provides programmers directives specify alignment arrays one another distribute aligned arrays userdefined virtual processor mesh directives dynamically change distribution arrays program execution provided redistribution data arrays used following circumstances work supported part arpa order number 7898 monitored nist grant number 60nanb1d1151 darpa order number 7899 monitored nist grant number appear 8th acm international conference july 1994 manchester england ffl different phases program vary access patterns shared array different data distribution array often best suited phase instance alternate direction implicit method 12 consists two phases first phase accesses twodimensional array along rows second phase along columns distribution array rows local processor columns dis tributed eliminates communication first phase similarly distribution array columns local rows distributed eliminates communication second phase need explicitly redistribute array two phases arises ffl scientific libraries tuned provide peak performance fixed set distributions input arrays distributions may conform distributions actual parameters leading performance degradation developing library routines possible input distributions practical hence actual array parameters must explicitly redistributed ffl data redistribution implicitly required execution array statements array sections statement entire arrays different distributions thus efficient methods performing data redistribution great importance distributedmemory implementations hpf data redistribution strategies presented literature issue reshaping perfect poweroftwo sized data arrays hypercubes addressed 15 closed form expressions processor sets processor needs communicate data sets need communicated data redistribution constructed 10 algorithms runtime array redistribution provided 22 17 closedform expressions characterize data communication required doall loops involving arrays identical block cyclic distributions simple index expressions developed 13 interprocessor data communication loops involving arrays invertible index expressions expressed composition index distribution functions schemes efficiently executing hpf array statements blockcyclically distributed arrays presented 4 11 21 schemes communication generation compiletime estimation communication costs presented 18 schemes based identifying communication primitives 7 comparing index expressions array references detailed techniques along lines 18 estimation communication cost developed 8 however schemes accurately identify communication cost corresponding arrays identically distributed schemes generating alignments arrays using cost metrics various interconnection networks developed 5 issue efficient redistribution arbitrary blockcyclic data distributions addressed 10 scheme presented therein performs data redistribution communicating one step data required source target data distributions refer singlephase data redistribution strategy certain circumstances characterization interprocessor communication possible facilitates development efficient redistribution schemes redistribution performed sequence singlephase data redistributions refer multiphase data redistribution strategy intermediate data distributions chosen total cost sequence lower cost singlephase redistribution example consider array 192 elements distributed blockwise eight processors let target distribution blockcyclic block size three singlephase data redistribution processor communicates seven processors suppose redistribution performed two phases redistribution block blockcyclic block size 12 followed second redistribution target distribution first data redistribution requires processor send two messages second requires four messages message setup time dominant component communication cost use twophase strategy reduce total communication cost communication cost model data redistribution take account total startup cost message transmission cost overhead arising due network con tention paper first present analytical model based number messages volume data communicated estimating communication cost singlephase data redistribution model network contention expressing communication sequence permutations executed fixed number contentionfree steps using analytical model tensor product representation data distributions 9 develop cost estimates singlephase data redistribution given source target distribution show use multiphase strategy reduce communication cost present methods determining intermediate distributions total communication cost multiphase strategy minimized paper organized follows section 2 briefly describes tensor product representation regular data distributions supported hpf section 3 communication cost model described used develop cost estimates singlephase data redistribution section 4 presents efficient algorithms data redistribution preliminary performance results intel ipsc860 reported section 5 conclusions presented section 6 regular data distributions section briefly describe tensor product representation regular data distributions details tensor product theory presented 14 hpf supports twolevel mapping data objects abstract processor array language introduces cartesian grid referred template array aligned template templates distributed onto virtual processor array using regular data distributions consider array a0 aligned template ai aligned dai b template distributed onto p processors using cyclick distribution template element di mapped processor div array divided p arrays referred loc residing local memory processor paper focus identity alignments ie ai aligned di 21 tensor product notation let theta n matrix b p theta q matrix tensor product b block matrix obtained replacing element ij ij b aomega b mp theta nq matrix defined focus tensor product vector bases vector basis element e column vector length one position zeros elsewhere tensor product e two vector bases e e n j called tensor basis e size tensor j mn expressing vector basis e tensor product vector bases e called factorization vector basis example vector basis e 12 8 factorized tensor bases e 2 0 e 4 expressing tensor basis vector basis e called linearization tensor basis example tensor basis e 4 2 linearized give vector basis 5 define compatibility two tensor bases definition 21 compatible tensor bases two tensor bases b1 b2 compatible factorized factorized tensor bases sizes vector bases corresponding positions equal ex ample compatible tensor bases factorization get note compatible define semantics regular data distributions using tensor basis 22 algebraic semantics regular data distributions consider array a0 processors using regular data distribution assume n multiple p 1 location element ai specified tuple p l p processor l local index ai cyclicb distribution element ai processor index div b mod p local index b block cyclic distributions equivalent cyclic n respectively indices a0 represented vector basis e n suppose a0 15 distributed onto four processors using block distribution element a9 mapped processor local processor 2 general element located processor local index 4 consider factorization vector basis e tensor basis tensor basis e 4 index vector basis e 4 corresponds processor element ai located index e 4 corresponds local address ai processor 1 say vector basis e 4 cor responds processor element ai located e 4 corresponds local address ai pro cessor denote vector basis corresponding processor ae tensor basis ae 4 represents array a0 15 distributed block fashion four processors tensor basis vector basis corresponding processor index identified called distribution basis vector basis e referred data basis ae processor basis thus distribution basis linear array a0 factorization e n every vector basis resulting tensor basis identified either processor basis data basis block distribution p proces cyclic distribution p processors distribution p processors block size b regular distribution bases linear array defined definition 22 distribution function onedimensional array correspond index ai let distribution function dist dist basis dist distribution basis define following indexing functions distribution basis indexing functions select vector bases distribution identify processor index local index global index element distributed array divide n array assumed padded dnpep definition 23 indexing functions let array a0 1 distribution basis e c processor index function proc local index function local global index function global index functions indices globale c locale c proce c distribution basis multidimensional array defined tensor product distribution bases dimension distribution indexing functions multidimensional array defined terms corresponding functions onedimensional array definition 24 distribution function let a0 n distributed using distribution distribution basis dist e n iomega deltaomega theta dist e n 0 distribution basis 20 theta 10 array distributed block cyclic 2 theta 2 virtual processor mesh ae 2 l 1 omega l square parenthesis separate distribution bases corresponding dimension definition 25 indexing functions index functions proc local global multidimensional array defined follows remainder paper assume source target distribution bases compatible also assume dimensionality virtual processor array number virtual processors greater underlying physical network define semantics redistribute command redistribute regular data distribution corresponds remapping processor basis distribution basis fi initial distribution basis fi distribution basis redistribution procfid may different implies redistribution involves communication also localfis localfid may different example l represents block distribution processors remapping processor basis gives new distribution basis fi l corresponds cyclic distribution 4 processors clearly globalfis 4lambdalp l implies communication required also localfis l localfid implies local index element change redistribution consider distribution bases source target distributions array a0 191 described intro duction source distribution basis ae 8 psomega e 24 l target distribution basis e 8 l l d2 intermediate data distribution used twophase redistribution strategy given distribution basis e 2 l l t2 singlephase redistribution corresponds remapping ae 8 psomega e 24 l l l d2 twophase redistribution corresponds sequence remappings ae 8 psomega e 24 l l l l l d2 estimate benefits multiphase redistribution necessary develop cost metrics communication cost data redistribution 3 communication cost estimation data redistribu tion section model communication cost singlephase data redistribution circuitswitched wormhole routed mesh networks using fixed routing rules survey routing techniques presented 19 communication time expressed terms following ffl startup time ts composed time packing nonconsecutive data elements buffer copying data user space system space unpacking data ffl transfer time te time data transmission dependent communication bandwidth path length source destination node circuitswitched wormhole routed net works large messages absence network contention time nearly independent path length 2 19 since transfer time message size n nte absence contention communication cost message n elements ts ffl network contention source node communicating destination node several pairs processors communicating across network path conflicts occur causing communication overheads shown 2 impact node conflicts negligible link conflicts significant general data redistribution requires alltomany personalized communication decompose communication set permutations performed fixed number contentionfree steps fixed underlying network configuration total time performing sequence permutations provides estimate communication cost taking account network contention evaluate redistribution cost linear data array distributed linear processor array first consider case data array size processor array size perfect powers integer r 2 z generalize nonperfectpower data processor array l psomega e bs l s2 fi l l d2 compatible source target distribution bases let cost data redistribution define terms used determine maximum number messages sent processor volume data communicated definition 36 distribution bases difference let tomega deltaomega oe n 0 deltaomega oe aeg difference fi 2 fi 1 given qfi1 l l l example l l 12 l factorized l l 12 l l difference qfi1 fi 2 size vector basis e 2 l 12 thus 2 similarly qfi2 2 number processors source target distributions equal distribution bases difference symmetric definition 37 distribution bases union let fi 1 fi 2 compatible distribution bases st deltaomega oe n 0 deltaomega oe aeg union l l l union ufi1 fi 2 distribution bases shown size tensor basis ae 2 l 12 thus number processors source target distribution p ufis fi p theta estimate communication cost data redistribution using difference union source target distribution bases 31 perfect power data processor array sizes loss generality let bs bd results hold otherwise l l s2 fi l l d2 two bases compatible l l l s2 l l l d22 consider processor source distribution elements global dices g st g div r bs located processor q elements global indices form ir pbs bs represented e r c note indices processor bases set values q1 q2 consider particular element located processor q global index element corresponds basis element e r c l l 2 omega ae r pgammab bs l 3 global index data redistribution unchanged located processor q communicate processor l 2 r pgammab bs data redistribution since q1 fixed elements processor q largest number processors q needs communicate determined total number different values l 2 elements located q since l 2 index vector basis e r b gammab l s2 take r b gammab values needs communicate r b gammab processors data redistribution estimate directly obtained comparing source target distribution bases number processors q communicate depends size data basis e r b gammab l si2 corresponds difference fi fi since similar argument holds source processors using def 36 follows maximum number messages processor needs send data redistribution difference qfis fi maximum size data sent message determined follows consider processor previous discussion follows processor q communicate processors ir pgammab bs data redistribution consider message processor q sends processor elements located processor q sent processor div r b elements indices r c 0 bs elements represented e r c thus total number data items communicated r c theta r bs total size vector bases e r c union ufis fi corresponds size tensor basis l r c bs thus number data elements communicated given n using presented cost model largest communication cost message absence contention given ts te model required communication sequence permutations first consider example let source target distributions cyclic block dis tributions respectively 27 element array 9 proces sors redistribution shown fig 1 necessary communication represented terms communication matrix c ci sends data processor p j else ci q2 permutation matrices underlying topology 3 theta 3 mesh permuta tions q0 q1 q2 scheduled three contentionfree steps using scheduling techniques presented 23 24 thus maximum communication time data redistribution correspond time performing three permutations sequence permutations q0 q2 said exactly cover communication c required data redistribution redundant messages sent messages required redistribution communicated general communication data redistribution requires processor communicate qfis fi distinct processors show communication expressed terms exactly qfis fi permutations processor expressed radix r permutation permutation fas expressed terms fas example corresponds q2 source distribution cyclic target distribution block figure 1 data redistribution a026 cyclic block input compatible distribution bases fi l l s2 fi l l d2 output permutations exactly cover redistribution communication else figure 2 algorithm 1 construct permutations cover communication data redistribution fig 2 present algorithm constructs sequence permutations exactly covers communication required data redistribution algorithm xtuple whose elements value 0 represented 0x ir corresponds base r representation using digits also x delta corresponds concatenation x example note difference qfis fi perfect power r theorem 31 communication data redistribution l l s2 fi l l d2 exactly covered qfis fi permutations proof prove sequence permutations constructed algorithm 1 exactly covers required commu nication consider case bs bd bs p bd v consider processor according argument section 31 processor q communicate processors ir pgammab bs algorithm 1 r pgammab bs thus also algorithm 1 ir v thus therefore permutation covers communication q processor r b gammab permutation fa communication distinct processor theorem follows noting q needs communicate r b gammab processors permutation covers required communication distinct processor 2 thus performing data redistribution requires totally permutations permutation performed fixed number contentionfree steps determined underlying mesh architecture say k step requires time equal time sending message size total cost data redistribution ts te uniformly scales redistribution cost considered remaining cost estimates use distribution bases also facilitates indexing code generation data redistribution perform redistribution processor u 0 needs evaluate following ffl send processor set processor u psendu set processors u send data ffl send data index set processor u processor v dsendu v local indices array elements resident u needed v receive processor set processor u processors u receive data ffl receive data index set processor u processor local indices array elements needed u resident v arrays block cyclic distribution data index sets processor sets characterized using regular sections closed forms 10 11 17 general cyclicbs cyclicbd redistribution closed form characterization sets using simple regular sections possible however source destination distribution bases compatible closed forms eval uated discussion section 31 follows processor communicates processors ir pgammab bs similar evaluation cases use recursive regular section notation represent data send sets twolevel recursive regular section array given l0 directly following loop nest thus case considered section 31 dsendu v locale r c l 2omega l s2 l s11 r b v 1 r b l s2 r b performing similar evaluation cases letting vr b gammap receive processor data sets evaluated similarly given data receive sets follows drecvu vr bs gammap 32 general data processor array sizes l psomega e bs l s2 fi l l d2 let results hold cases since two distribution bases compatible bs jbd bd jpbs l bs l pbs bs l s2 l bs pbs bs l d21omega e bs l d22 using argument similar section 31 shown processor bs needs communicate processors pbs bs thus largest number processors source processor needs communicate b bs qfis fi similarly largest message size communicated np required communication expressed sequence permuta tions theorem 32 communication data redistribution l psomega e bs l s2 fi l l d2 exactly covered qfis fi permutations proof similar theorem 31 permutations constructed shown fig 3 since qfis fi permutations required permutation performed fixed number contentionfree steps total communication cost given ts te example consider data distributions shown fig 4 distribution bases given fi l fi e 6 l basis difference qfis fi 6 basis union ufis fi 36 largest number messages sent processor six largest size messages one 2 cost singlephase data redistribution closed forms processor index data index sets determined manner similar used perfect power data array sizes section 31 distribution basis multidimensional array tensor product distribution basis dimen sion since shapepreserving data redistribution consid ered distribution along array dimension changes without changing shape data processor ar ray analysis performed independently array dimension results combined multidimensional array 4 communicationefficient data redistribution section describe multiphase redistribution strategy first consider specific example de 2 message sent processor also accounted input compatible distribution bases fi l psomega e bs l s2 fi l l d2 output permutations exactly cover redistribution communication ii mod bs pbs else bd bs pbd bs bs figure 3 algorithm 2 construct permutations cover communication data redistribution source distribution block target distribution cyclic figure 4 data redistribution a035 block cyclic scribe general procedure let fi l l s2 l l d2 0 redistribution corresponds remapping appropriate data basis processor basis redistribution fi fi corresponds remapping data bases e r e r processor bases ae r 1 data bases remapping involves communication subset processors remapping performed directly mapping data bases processor bases one step sequence steps consider performing redistribution using intermediate distribution data basis e r first mapped processor basis first redistribution fi l l s2 l l s3 processor communicates r processors cost redistribution given cfis p te redistribution fi fi involves mapping data base e r processor ba sis cost redistribution total cost twophase redistribution cfis fi p te singlephase redistribution would cost r 2 ts general data bases total size qfis fi need remapped processor bases multiphase strategy corresponds performing mapping one factor time choice intermediate distribution depends factorization qfis fi example r factorization used general difference qfis fi factorized k factors distributions used redistribution performed kphases phase corresponds redistribution distribution basis fi distribution basis fi i1 qfi using cost model developed section 3 cost phase r ts p te total cost kphase redistribution given ts redistribution method multidimensional arrays developed method linear arrays number intermediate distributions determined number factors difference two distribution bases let l s1 l s2 l omega l l d21 different factor izations consider two factorizations qfis fi 3 first factorization corresponds following sequence distributions ae 6 l s1 omega ae 4 l s2 l 11 l l t22 l l l d2 second factorization corresponds sequence distributions ae 6 s1omega e 6 l s1 l s2 l l l t2 l l l d2 first method costs 8ts 2 36te second costs 7ts 2 36te second method lowest communication cost among possible factorizations general redistribution strategy smallest cost depend actual values message setup transfer times formulate problem determining intermediate distributions total communication cost minimized following optimization problem given machine parameters ts te p data array parameters problem find multiplicative partition qfis fi r0 theta delta delta delta theta r tgamma1 total communication cost cfis fi minimized ts special case qfis finding multiplicative partition p equivalent finding additive partition p op algorithm finding optimal partition presented 20 case frequently occurring case number physical processors usually perfect powers two difference distribution bases perfect power two heuristics general case qfis fi perfect power provided 16 performance results section compare performance multiphase singlephase strategies specific source target data distributions 32node intel ipsc860 hyper cube due lack access meshconnected computer strategies could compared mesh network communication two approaches performed according permutations generated algorithm 2 total time required redistribution using singlephase multiphase strategies blockcyclic source destination distributions measured increasing data sizes time includes indexing time packing unpacking data actual communication time timings taken using millisecond wallclock timer mclock table 1a shows total times milliseconds redistribution cyclic256 cyclic16 16 proces sors difference corresponding distribution bases 16 twophase strategy used cyclic64 intermediate distribution according communication cost model two phase strategy corresponding factorization 4 lowest communication cost among possible multiphase strategies seen table 1a two phase strategy performs better singlephase strategy data size 28k larger data sizes increase transmission cost overrides decrease message startup times table 1b shows total times milliseconds redistribution cyclic256 cyclic8 32 processors difference corresponding distribution bases 32 two twophase strategies corresponding factorizations respectively used two strategies use cyclic32 referred twophase1 cyclic64 referred twophase2 intermediate data distri butions respectively according cost model two strategies identical communication times difference times twophase strategies shown table 1b negligible also twophase strategy performs better singlephase strategy data size 64k larger data sizes increase transmission cost overrides decrease message startup times 6 conclusion paper presented approach communication efficient data redistribution using tensor product representation regular data distributions developed cost estimates data redistribution using estimates demonstrated use multiphase redistribution strategy reduce total cost presented methods determining appropriate intermediate distributions performance results intel ipsc860 show multiphase redistribution strategy improve performance singlephase redistribution acknowledgments would like thank advanced concepts computational laboratory nasa lewis research center access intel ipsc860 hypercube r communication overheads intel ipsc2 hypercube vienna fortran fortran language extension distributed memory multiprocessors generating local addresses communication sets data parallel programs optimal evaluation array expressions massively parallel machines high performance fortran forum solving problems concurrent processors compiler optimizations fortrand mimd distributedmemory machines compiler support machineindependent parallel programming fortrand topics matrix analy sis complexity reshaping arrays boolean cubes compiling communicationefficient programs massively parallel machines survey wormhole routing techniques direct networks optimal phase barrier synchronization kary ncube wormholerouted systems using mul tirendezvous primitives efficient compilation array statements private memory multicomputers runtime array redistribution hpf programs tr solving problems concurrent processors vol 1 general techniques regular problems compiler optimizations fortran mimd distributedmemory machines compiletime generation regular communications patterns vienna fortranmyampersandmdasha fortran language extension distributed memory multiprocessors computer support machineindependent parallel programming fortran generating local addresses communication sets dataparallel programs implementing parallel c runtime system scalable parallel systems survey wormhole routing techniques direct networks compiling communicationefficient programs massively parallel machines compiletime estimation communication costs multicomputers online communication circuitswitched fixed routing meshes efficient compilation array statements private memory multicomputers ctr stavros souravlas manos roumeliotis pipeline technique dynamic data transfer multiprocessor grid international journal parallel programming v32 n5 p361388 october 2004 thakur alok choudhary j ramanujam efficient algorithms array redistribution ieee transactions parallel distributed systems v7 n6 p587594 june 1996 neungsoo park viktor k prasanna cauligi raghavendra efficient algorithms blockcyclic array redistribution processor sets ieee transactions parallel distributed systems v10 n12 p12171240 december 1999 minyi guo yi pan improving communication scheduling array redistribution journal parallel distributed computing v65 n5 p553563 may 2005 narasimhan ramasubramanian ram subramanian santosh pande automatic compilation loops exploit operator parallelism configurable arithmetic logic units ieee transactions parallel distributed systems v13 n1 p4566 january 2002 ram subramanian santosh pande framework performancebased program partitioning progress computer research nova science publishers inc commack ny 2001 minyi guo ikuo nakata framework efficient data redistribution distributed memory multicomputers journal supercomputing v20 n3 p243265 november 2001 ram subramanian santosh pande framework performancebased program partitioning progress computer research nova science publishers inc commack ny 2001 chinghsien hsu shengwen bai yehching chung chusing yang generalized basiccycle calculation method efficient array redistribution ieee transactions parallel distributed systems v11 n12 p12011216 december 2000 yehching chung chinghsien hsu shengwen bai basiccycle calculation technique efficient dynamic data redistribution ieee transactions parallel distributed systems v9 n4 p359377 april 1998 chinghsien hsu yehching chung efficient methods kr r r kr array redistribution1 journal supercomputing v12 n3 p253276 may 1 1998 chinghsien hsu yehching chung donlin yang chyiren dow generalized processor mapping technique array redistribution ieee transactions parallel distributed systems v12 n7 p743757 july 2001 chinghsien hsu yehching chung chyiren dow efficient methods multidimensional array redistribution journal supercomputing v17 n1 p2346 aug 2000 peizong lee wenyao chen generating communication sets array assignment statements blockcyclic distribution distributed memory parallel computers parallel computing v28 n9 p13291368 september 2002 edgar kalns lionel ni processor mapping techniques toward efficient data redistribution ieee transactions parallel distributed systems v6 n12 p12341247 december 1995 neungsoo park viktor k prasanna cauligi raghavendra efficient algorithms blockcyclic array redistribution processor sets proceedings 1998 acmieee conference supercomputing cdrom p113 november 0713 1998 san jose ca jihwoei huang chihping chu efficient communication scheduling method processor mapping technique applied data redistribution journal supercomputing v37 n3 p297318 september 2006 manojkumar krishnan jarek nieplocha memory efficient parallel matrix multiplication operation irregular problems proceedings 3rd conference computing frontiers may 0305 2006 ischia italy