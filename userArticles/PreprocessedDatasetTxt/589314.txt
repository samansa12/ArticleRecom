minimizing quadratic sphere new method sequential subspace method ssm developed problem minimizing quadratic sphere scheme quadratic minimized subspace adjusted successive iterations ensure convergence optimum sequential quadratic programming iterate included subspace convergence locally quadratic numerical comparisons recent methods given b introduction paper consider problem minimizing quadratic sphere subject kxk symmetric n n matrix b 2 r n denotes transpose k k euclidean norm minimization problem often called trust region subproblem since must solved step trust region algorithm 1 2 3 15 19 problems form arise many applications including regularization methods illposed problems 14 26 graph partitioning problems 10 although solution 1 expressed terms diagonalization representation practical n small paper focus largescale case one approach largescale case developed golub von matt 5 also see 4 partially tridiagonalize using lanczos process solve tridiagonal problems obtain approximate solution 1 developments approach including preconditioning fortran 90 implementation hsl vf05 harwell subroutine library see gould et al 7 method developed paper use approach spirit golubvon mattgould et al scheme obtain starting guess parametric eigenvalue approaches sphere constrained problem 1 developed sorensen 24 rendl wolkowicz 20 relationship two approaches discussed detail 20 roughly sorensens approach involves constructing approximation solution 1 solution related eigenvalue problem since approximation may satisfy bound norm solution series eigenvalue problems solved limit bound norm solution fullled approach rendl wolkowicz eigenvalue problem solved iteration however bound norm solution satised maximizing related dual func tion eigenvalue problems arising either approach solved using arnoldi techniques developed 13 hard case see 16 b orthogonal eigenvectors associated smallest eigenvalue sorensens approach needs modied ecient algorithm hard case developed rojas thesis 21 also uses algorithm solve dicult illposed problems hansen 11 12 approach rendl wolkowicz need modication hard case however convergence algorithms eigenvalue problem may slower computed eigevalue simple approach paper call sequential subspace method ssm involves solving 1 additional constraint x contained subspace show convergence locally quadratic locally cubic subspace contains iterate generated one step sequential quadratic programming sqp algorithm applied 1 convergence quadratic even original problem degenerate multiple solutions singular jacobian rstorder optimality system descent cost nonoptimal point ensured including subspace either cost gradient eigenvector associated smallest eigenvalue observe numerical experiments appropriate small dimensional subspaces generated preconditioned krylov space minimum residual techniques comparisons algorithms sorensen 24 rendl wolkowicz 20 gould lucidi roma toint given section 5 solution problem subject eigenvector associated smallest eigenvalue comparing ssm approach algorithms solving eigenproblem follows discussion sleijpen van der vorst 22 sqp iterate 2 closely connected rayleigh quotient iteration 18 p 70 cubically convergent 18 p 73 22 approximate solutions sqp system used build subspaces containing approximation eigenvector paper solve sqp system relatively precisely form small dimensional subspace containing sqp iterate computing new approximation subspace previous information discarded hence computer memory requirements relatively small complete diagonalization exists solution 1 kyk r positive semidenite global minimizer quadratic x ax 2b x thus minimizer 1 lies interior constraining sphere constraint ignored optimization problem approached using techniques unconstrained optimization consequently restrict attention following equality constrained problem subject solutions 3 characterized following result see 23 lem 24 lem lemma 1 vector x solution 3 r exists positive semidenite solution 3 expressed terms eigenpairs let diagonalization diagonal matrix diagonal elements matrix whose columns 1 orthonormal eigenvectors dening lemma 2 vector solution 3 c chosen following way degenerate case arbitrary scalars satisfying condition b nondegenerate case hold c 1 chosen proof simply check sucient optimality conditions lemma 1 satised degenerate case jacobian rstorder optimality system may singular coincides hard case sorensen 16 b orthogonal eigenspace associated smallest eigenvalue multiplier equal 1 nondegenerate case multiplier chosen positive denite solution constraint x nondegenerate case equation 5 leads upper lower bounds multiplier since kbk r obtain lower bound observe yields relation utilizing upper lower bound u l strict convexity left side 5 interval l u easy devise ecient algorithms compute solution 5 3 incomplete diagonalization local convergence iteration k sequential subspace method ssm 3 impose additional constraint x lies subspace k r n hence new iterate x k1 solution problem subject show convergence locally quadratic even original problem 3 degenerate include sqp iterate associated x k k v n l matrix orthonormal columns span k 8 equivalent problem subject substituting x 9 reduces following problem r l subject l small 10 solved complete diagonalization section 2 b sparse factorization 10 solved quickly using newton approach developed 16 theory tridiagonal b generated using lanczos process 6 particular v 1 unit vector v ith column v lanczos process expressed follows algorithm 1 lanczos ksk diagonal u superdiagonal tridiagonal matrix b lanczos process terminated column space v av coincide wellknown columns v generated process may deviate signicantly orthogonality due propagation rounding errors happens 9 longer equivalent 10 nonetheless gould et al observe 7 solution 10 often provides good approximation solution despite loss orthogonality lanczos process repaired order restore orthogonality using householder process generate columns v process however requires products vector previously computed columns v thus overhead needed maintain orthogonality grows like nl 2 number ops like nl storage overhead signicant n l large hand compute high accuracy solution need maintain orthogonality order obtain equivalent problem 10 leads us focus approaches involve subspaces l much smaller n particular implementation algorithm 4 ssm proposed later l either 4 5 since sequential quadratic programming sqp techniques often converge rapidly good starting guess always include sqp approximation subspace x k current iterate assume satises constraint k current approximation multiplier associated constraint sqp iterate expressed following way z solutions following linear system coecient matrix 1112 singular let z minimum residualminimum norm solution z gotten theory multiplying right side pseudoinverse coecient matrix see 8 sqp method equivalent newtons method applied nonlinear system solution x k1 subspace problem 8 approximation solution 3 obtain estimate multiplier lemma 1 minimize euclidean norm residual b ax k1 x k1 scalar works give b ax x examine local convergence solution x 8 multiplier estimate 14 assumption k contains solution 11 let denote set minimizers 3 let multiplier given lemma 1 nondegenerate setting positive denite show iteration locally quadratically convergent unique solution 3 degenerate case 1 one element obtain local quadratic convergence distance measured usual way nondegenerate degeneratecase contains single element obtain local quadratic convergence safeguarded choice k convergence result special nondegenerate degeneratecase given later lemma 5 local convergence result either nondegenerate case degenerate case multiple solutions following theorem 1 let multiplier lemma 1 associated set solutions 3 suppose either positive denite 4 strict inequality exist positive constants c property x k k subspace k contains sqp iterate xsqp associated 11 12 solution x k1 8 associated multiplier k1 given 14 satisfy following estimate eigenvalue problem 2 corresponding b 0 always degenerate multiple solution error following special form multiplier estimated using 14 shown error multiplier bounded constant times error solution vector squared see remark end section 31 follows constant convergence result rayleigh quotient iteration 31 nondegenerate problems begin derivation theorem 1 nondegenerate case lemma 3 3 solution x associated multiplier 1 exist neighborhood n x constant c property subspace k contains sqp iterate xsqp associated 1112 solution x k1 8 associated multiplier k1 given 14 satisfy following estimate proof since 1 matrix positive denite jacobian nonlinear system 13 nonsingular standard convergence theorem newtons method applied smooth system equations exist neighborhood n constant c whenever let positive scalars chosen x 2 r n let f cost function 3 let l lagrangian dened taylor expansion around x yields following relation x 2 b r rg combining 16 gives x 2 b r p projection xsqp onto b r hence xsqp follows p 2 k fx k1 fp combining inequality 17 18 gives implies making substitution gives r combining 21 19 proof complete remark eigenvalue problem 2 x case 20 yields 21 becomes 32 degenerate problems consider local convergence degenerate case referring lemma 2 degenerate case happen solution 3 degenerate case expressed x 1 linear combination vectors satisfying relation initially suppose k 1 0 case projection eigenspace associated contains sphere radius convergence result following lemma 4 suppose multiplier lemma 1 associated set solutions 3 given component element eigenspace associated e 1 exist positive constants c property x k k subspace k contains sqp iterate xsqp associated 11 12 solution x k1 8 associated multiplier k1 given 14 satisfy following estimate proof initially let us assume k near 1 k 6 1 case linear system 1112 nonsingular exists unique solution z expand z x k terms eigenvectors writing x utilizing 11 obtain substituting 12 gives let us dene ix k x 2 since let k error step k dened 26 since near 2 0 x k near 23 let x closest element x k dene 30 component error k since component x k error k 31 implies combining 28 hence 2 e component xsqp error 2 seminorm associated projection eigenspace associated x ng proceeding earlier replacing norms seminorms p projection xsqp onto ball b r xsqp 30 32 z perpendicular x k 12 implies consequently kp x combines 34 give triangle inequality let k k 1 seminorm dened recall kx k pythagorean theorem fact x k1 length r implies distance x k1 given x element relations 3538 yield distx combining estimates k1 analysis given assumption k 6 1 special case k show analysis modied change variables z substitution x sqp system 1112 equivalent orthogonal transformation diagonal matrix diagonal elements rst diagonal elements rst components vanish hence rst equations 39 imply next n equations give last equation 39 gives minimum norm solution last equation 40 combining bounds relations analysis 33 onward applied leading us estimate k1 lemmas 3 4 yield theorem 1 33 nondegenerate degenerateproblems finally let us consider nondegenerate degeneratecase 1 component x eigenspace associated smallest eigenvalue vanishes convergence result following lemma 5 3 solution x given 22 exist neighborhood n x 1 constant c property subspace k contains sqp iterate xsqp associated 11 12 solution x k1 8 associated multiplier k1 given 14 satisfy following estimate case k k c chosen proof focusing numerator 24 substituting substitution numerator 24 obtain denominator terms 43 following lower bound another lower bound gotten neglecting terms corresponding indices seminorm k k 1 dened 36 combining 4345 yields returning previous analysis degenerate case follows 29 46 exploit fact order analyze 47 consider two separate cases kx k x k ii kx k x k xed constant satisfying case x k x k derive similar bound left side 49 case ii case follows 42 x k x 2 r n subscript vector used denote projection eigenspace associated e substituting using 20 obtain x 2 b r assuming x k unit vector note kx k x establish uniform bound expression 51 x k near x facilitate analysis rst consider whether equation solution form form schwarz inequality gives jy since unit vector orthogonal eigenspace associated 1 multiplying 52 using 53 54 gives x 2 b r implies since yields relation referring 55 contradiction kx x k summary equation 52 solution set consisting satisfy following conditions lies closure 57 since solution 52 satises 55 cannot solution 52 since 52 solution closure following constant strictly positive since lim min 51 bounded uniformly x k near x x k thus either case ii left side 49 bounded 47 relation 30 degenerate case establish analogue 32 indices 2 e need dierent bound next last term 43 identity hence since implies follows estimate along lower bound 44 denominator 43 yields relation reminder analysis identical given degenerate case lemma 4 starting 32 since follows analysis lemma 4 special case k hence 60 absorbed kx k completes proof implementation experimentation ssm put following four vectors k iteration xsqp x k estimate eigenvector associated smallest eigenvalue including x k k value cost function decrease consecutive iterations multiple b ax k cost function gradient ensures descent current iterate satisfy rstorder optimality conditions eigenvector associated smallest eigenvalue dislodge iterates nonoptimal stationary point also use vector safeguard strategy designed keep positive denite 41 sqp system consider sqp system 1112 according 12 z orthogonal prior iterate x k let p matrix projects vector space perpendicular multiplying 11 p yields according 12 haved found preconditioned krylov space methods gaussseidel scheme 9 converge quickly applied 61 small illustra tion let us consider second test problem 24 1000 1000 diagonal matrix diagonal elements selected randomly uniform distribution 5 5 2qq q gotten rst generating random numbers 5 5 scaling resulting vector unit length vector b generated way q solid curve figure 1 gives convergence lanczos type process algorithm 1 starting vector v used generate matrix v used 9 lanczos process modied ensure orthogonality columns v value l algorithm 1 solve l l tridiagonal problem 10 obtain approximate solution x associated multiplier original problem 3 solid curve figure 1 plot base 10 logarithm norm residual kb aixk according lemma 1 residual vanishes optimal solution dashed curve figure 1 based ssm approach gotten following way taking algorithm 1 generate v 40 orthonormal columns solving 10 obtain starting guess x 0 iteration k ssm phase start vector v use gaussseidelkrylov space approach 9 generate matrix v orthonormal columns approximately contains solution 61 range using v generated way solve 9 obtain next iterate x k1 associated multiplier estimated using 14 kink dashed curve figure 1 corresponds number iterations needed obtain approximate solution 61 example roughly 15 multiplications elements matrix used solve 61 quadratic convergence ssm ected rapid decay residual norm approach generating v using nonsymmetric gaussseidel matrix krylov spaces orthogonalization become expensive n really large matrixvector products figure 1 convergence tridiagonalization approach solid ssm dashed second test problem 24 since columns v stored memory hence remainder paper focus lowstorage symmetric techniques solving 61 compare approaches solve 61 using preconditioned version paige saunders minres algorithm 17 precisely use algorithms 3 3a 9 three dierent choices symmetrizing preconditioner w paper corresponding unconditioned iterations ii diagonal matrix whose diagonal matches l strictly lower triangular matrix whose lower triangle matches c implementations ssm associated latter two preconditioners denoted ssm l respectively typically l matrix associated ip dense even sparse since p often dense nonetheless linear systems form solved time proportional number nonzero elements lower triangle due special structure c terms vectors w q p dened iw diagonal c expressed odiagonal elements c exploiting structure shown solution l computed following way algorithm statement i1ni algorithm 2 requires nonzero elements column beneath diagonal hence number oating point operations algorithm 2 plus number nonzero elements lower triangle analogous procedure transposed system following algorithm 3 42 positive deniteness theory minres algorithm use solve 61 applied symmetric matrix practice convergence extremely slow c indenite reason try choose k k positive denite e eigenvector matrix b 10 associated smallest eigenvalue pair v approximates eigenpair corresponding smallest eigenvalue error estimated following way closer 1 eigenvalues substituting residual r av v since 1 thus j 1 j krk implies insight replace least squares estimate 14 following safeguarded estimate approximate eigenpair v accurate safeguarded step 62 safe poor approximation hence whenever apply one iteration ssm quadratic eigenvalue problem 2 order compute accurate eigenpair due third sixth order estimates 15 simply one iteration ssm eigenproblem often yields highly accurate eigenpair 43 algorithm collect observations present algorithm used generate numerical results next section simplify presentation introduce following subroutines routine applies algorithm 1 matrix starting vector v 1 generate matrix v columns l routine solves problem 8 generating solution denoted x associated multiplier matrix whose columns orthonormal basis k estimate v smallest eigenvalue associated eigenvector gotten computing smallest eigenvalue associated eigenvector e setting routine computes minimum residual minimum solution z following linear system implementation sequential subspace method combines three routines safeguarded step 62 algorithm 4 safeguarded ssm lanczos startup kb algorithm 4 computational results reported next section took 01ng rand function appearing start algorithm 4 generates vector components uniformly distributed 0 1 5 computational results section compare performance ssm performance algorithms 7 20 24 denoted glrt rw respectively using three test problems presented 24 results report extracted 24 results reported glrt rw obtained using codes provided authors thank authors providing access codes codes used dierent stopping criteria glrt stopped kb aixkkbk bounded given tolerance rw stopped gap value primal dual problem hence error primal cost function smaller given tolerance order ensure code computed solution accuracy adjusted error tolerance parameter code value kb computed solution smaller given tolerance specied rst test problem 24 standard 2d discrete laplacian unit square based 5point stencil equallyspaced mesh points taking series 20 problems generated b vector elements uniformly distributed 0 1 problems solved using three dierent tolerances table 1 give average number matrixvector products involving algorithm iteration preconditioned minres algorithm lower trian tolerance rw glrt ssm ssm ssm l table 1 problem 1 average number matrixvector products versus tolerance gular preconditioner involves roughly twice many ops iteration either identity diagonal preconditioned schemes hence bookkeeping charged two matrixvector products iteration triangular preconditioned scheme seen table 1 ssm l converges twice fast identity diagonal preconditioned schemes overall ssm l uses smallest number matrixvector products test problem since parametric eigenvalue algorithms rw compute extreme eigenvalue series matrices also list parentheses table 1 number eigenproblems solved hence rw economical terms number eigenproblems solved second suite test problems 24 utilizes matrix described earlier section 3 problems radius sphere varied number matrixvector products tabulated radii one smaller solutions computed extremely quickly focused error tolerance 10 7 table 2 see fewest matrix radius rw glrt ssm ssm ssm l table 2 problem 2 average number matrixvector products versus radius vector products glrt fewest nal problem 24 employed discrete laplacian matrix 100 vector b designed make problem degenerate rst random b generated 1 component removed table 3 gives results various algorithms ssm l table 3 problem 3 average number matrixvector products placed asterisk result table 3 glrt since routine reduced error 10 4 10 7 tolerance used routines among routines achieved error tolerance ssm l performed best relative number matrixvector products note number matrixvector products given table 3 taken 24 rojas recent thesis 21 developed ecient implementation sorensens approach degenerate problems summary lanczos type process seems eective problem nondegenerate 1 problem becomes degenerate preconditioned schemes ssm ssm l appear eective number times rw computes extreme eigenpair often around 5 numerical experiments reported paper matlabs eig routine used compute extreme eigenpair routine computing extreme eigenpair could sped possibly using jacobi type methods sleijpen van der vorst 22 truncated rq iteration sorensen yang 25 number matrixvector operations used parametric eigenvalue approach would reduced r trust region algorithm nonlinearly constrained optimization trust region strategy nonlinear equality constrained optimization least squares quadratic constraint quadratically constrained least squares quadratic problems matrix computations solving trustregion subproblem using lanczos method applied numerical linear algebra iterative methods nearly singular linear systems graph partitioning continuous quadratic programming matlab package analysis solution discrete illposed problem geophysical data analysis discrete inverse theory solution sparse inde symmetric eigenvalue problem trust region algorithm equality constrained optimization semide largescale trustregion approach regularization discrete illposed problems jacobidavidson iteration method linear eigenvalue problems newtons method model trust region modi minimization largescale quadratic function subject spherical constraint truncated rq iteration large scale eigenvalue calculations inverse problem theory tr ctr peter graf wesley b jones projection based multiscale optimization method eigenvalue problems journal global optimization v39 n2 p235245 october 2007 stanislav busygin new trust region technique maximum weight clique problem discrete applied mathematics v154 n15 p20802096 1 october 2006