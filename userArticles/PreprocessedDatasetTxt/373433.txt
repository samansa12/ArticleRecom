margins adaboost recently ensemble methods like adaboost applied successfully many problems seemingly defying problems overfittingadaboost rarely overfits low noise regime however show clearly higher noise levels central understanding fact margin distribution adaboost viewed constraint gradient descent error function respect margin find adaboost asymptotically achieves hard margin distribution ie algorithm concentrates resources hardtolearn patterns interestingly similar support vectors hard margin clearly suboptimal strategy noisy case regularization case mistrust data must introduced algorithm alleviate distortions single difficult patterns eg outliers cause margin distribution propose several regularization methods generalizations original adaboost algorithm achieve soft margin particular suggest 1 regularized adaboostreg gradient decent done directly respect soft margin 2 regularized linear quadratic programming lpqp adaboost soft margin attained introducing slack variablesextensive simulations demonstrate proposed regularized adaboosttype algorithms useful yield competitive results noisy data b introduction ensemble collection neural networks types classifiers hypotheses trained task boosting ensemble learning methods used recently great success several applications e g ocr 29 16 far reduction generalization error adaboost completely understood low noise cases several lines explanation proposed candidates explaining well functioning boosting methods 28 7 27 recent studies noisy patterns 25 14 26 shown clearly myth boosting methods overfit work try understand adaboost exhibits virtually overfitting low noise strong overfitting high noise data propose improvements adaboost achieve noise robustness avoid overfitting section 2 analyze adaboost asymptotically due sim ilarity refer following adaboost 10 unnormalized arcing 8 exponential function adaboosttype algorithms ata especially focus error function atas find function written terms margin every iteration adaboost tries minimize function stepwise maximizing margin 22 26 12 asymptotical analysis function introduce hard margin concept show connections vapniks maximum margin classifiers support vector sv learning 4 linear programming lp bounds size margin given noisy patterns shown adaboost overfit holds boosted decision trees 25 rbf nets 26 also kinds classifiers section 3 explain property adaboost enforce hard margin must necessarily lead overfitting presence noise case overlapping class distributions hard margin plays central role causing overfitting propose relax hard margin section 4 allow misclassifications using soft margin concept already successfully applied support vector machines cf 9 view margin concept key understanding svms atas far know margin distribution look like learner achieve optimal classification nonoise case large hard margin clearly best choice 30 however noisy data always tradeoff believing data mistrusting data point could mislabeled outlier leads introduction regularization reflects prior knowledge problem introduce regularization strategy analogous weight decay adaboost subsequently extend lpadaboost algorithm grove schuurmans 14 slack variables achieve soft margins furthermore propose qpadaboost show connections svms finally section 5 numerical experiments several artificial realworld data sets show validity competitiveness regularization approach paper concluded brief discussion 2 analysis adaboosts learning process 21 algorithm tg ensemble hypotheses defined input vector x 2 x consider binary classification case results transfered easily classification two classes 27 binary classification case output one two class labels ie h ensemble generates label fx j f x weighted majority votes order train ensemble ie find appropriate hypotheses weights c convex combination several algorithms proposed eg windowing 24 bagging 5 boostingarcing adaboost 10 arcx4 7 bagging weighting simply weighting scheme com plicated wellknown ensemble learning algorithms sequel focus boostingarcing ie adaboosttype algorithms omit detailed description ata give pseudocode figure 1 details see eg 10 7 8 binary classification case define margin inputoutput algorithm adaboostoe input l examples initialize 1 train neural network respect weighted sample set fz wg obtain hypothesis 2 calculate training error ffl l abort delta small constant 3 set 4 update weights w z normalization constant output final hypothesis jbj jbj figure 1 adaboosttype algorithm ata 22 retrieve original adaboost algorithm 10 ata specialization unnormalized arcing 6 exponential l l denotes number training patterns margin z positive right class pattern predicted positivity margin value increases decision correctness ie decision stability becomes larger moreover 1 margin ae decision line smallest margin pattern training set ie define dz c rate incorrect classification cf edge breiman 6 one pattern also use definition b instead c unnormalized version c ie usually jbj 6 1 cf 4 7 figure 1 22 error function adaboost important question analysis atas kind error function optimized algorithmic formulation cf figure 1 straight forward understand aim algorithm consider one use weights hypotheses c patterns w z manner equation 4 5 let us remember following facts 1 weights w z tth iteration chosen previous hypothesis exactly weighted training error ffl 12 28 2 weight c hypothesis chosen minimizes functional g introduced breiman 8 essentially functional depends rate incorrect classification patterns defined l exp oe constant functional minimized analytically 8 26 one gets explicit form equation 4 solution 3 train tth hypothesis step 1 figure 1 either use bootstrap replicates training set sampled according w minimize weighted error function base learning algorithm observed convergence ata faster weighted error function used taking closer look definition g one finds computation sample distribution w cf equation 5 derived directly g let us assume g error function minimized ata essentially g defines loss function margin distributions depends value jbj larger margins mgz ie smaller rate incorrect classification smaller value g gradient g gives answer question pattern increase margin decrease g maximimally gradient descent information used compute sample distribution w training next hypothesis h important increase margin pattern z weight w z high otherwise low distribution w sums one surprisingly exactly atas lemma 1 computation pattern distribution w t1 tth iteration equivalent normalizing gradient gb respect mgz l proof found appendix lemma 1 analogy gradient descent method almost complete gradient descent first compute gradient error function respect parameters optimized corresponds computing gradient g respect mar gins second step size direction determined usually linesearch comparable minimization g mentioned point 2 list therefore adaboost related gradient descent method aims minimize functional g constructing ensemble classifiers 22 26 12 also explains point 1 list gradient descent method new search direction perpendicular previous one analogy perfect gap pattern distribution classifier difficult find classifier minimizes g knowing pattern distribution 26 mentioned two ways incorporating sample distribution first way create bootstrap replicate sampled according pattern distribution usually lot random effects hide true information contained distribution therefore information lost gap larger direct way use weighted error function employ weighted minimization breiman 8 therefore need iterations bootstrap weighted minimization 1 fastest convergence obtained one uses g directly finding hypothesis cf 12 considerations explain point 3 list friedman et al 12 mentioned sometimes randomized version shows better performance version weighted minimization connection discussion section 3 becomes clearer randomized version show overfitting effect possibly much later overfitting maybe observed whereas observed using efficient weighted minimization 23 adaboost annealing process definition g equation 11 also written exp inspecting equation closely see adaboost uses softmax function 3 parameter jbj would like interpret annealing parameter 22 temperature 1jbj high system state high energy patterns relevant high weights temperature goes patterns smallest margin get higher higher weights limit arrive maximum function patterns highest rate incorrectness ie smallest margin considered get nonzero weight lemma 2 learning process weighted training errors ffl bounded ffl jbj increases least linearly number iterations proof 3 4 smallest value b achieved ffl b delta also hence also therefore smallest value b log q always bigger constant fl depends oe delta thus jb annealing speed low achieved solution larger margins reason usual annealing process 15 error surface better local minimum could obtained locally annealing slow enough equation 4 observe training error ffl takes small value b becomes large strong learners reduce training errors strongly make jbj large ata iterations asymptotic point reached faster reduce annealing speed oe complexity base hypotheses decreased constraint ffl oe gamma delta figure shows error functions classification among error function different values jbj oe shown figure 2 left adaboost 2 squared kullbackleibler error ln mgz ln 2 plotted squared kullbackleibler similar error function adaboost jbj increases experiments often 10 2 200 iterations ata error function approximates 01 loss patterns margin smaller 0 general loss others loss 0 possible reduce error patterns 0 adaboost case asymptotically jbj 1 equivalent 01loss around 0 adaboost loss function oe 6 1 shown figure 2 right demonstrate different offsets step exhibited 01 loss 1051525loss figure loss functions estimating function fx classification abscissa shows margin yfx pattern ycoordinate shows monotone loss pattern 01loss solid squared error dashed kullbackleibler error dashdotted 100g left panel right plot oe one f13 23g oe controls position step 01loss asymptotically approximated adaboost loss function 24 asymptotical analysis 241 large margin main point explanation atas good generalization performance size hard margin achieved 28 8 low noise case hypothesis largest margin good generalization performance 30 28 thus interesting see large margin depending generalizing theorem 5 freund et al 10 case oe 6 1 2 get theorem 4 assume ffl weighted classification errors generated running following inequality holds 2 gamma1 1l l f final hypothesis proof found appendix b corollary 5 distributions margin ae bounded ae proof 6 maximum ffl 1gamma respect ffl reached for2 increasing monotonically ffl therefore replace ffl ffl equation 13 ae ii basis right hand side smaller 1 asymptotically p xyz yfx asymptotically example smaller margin biggest possible margin max solve equation max get get assertion ae always bigger equal max equation 14 see interaction oe ffl difference ffl oe small right hand side 14 small smaller oe important difference theorem 72 8 also weaker bound ae 1 gamma 2oe oe small ae must large ie choosing small oe results larger margin training patterns increase complexity basis algorithm leads increased ae error ffl decrease 242 support patterns decrease functional gc jbj gb predominantly achieved improvements margin mgz c margin c negative error gc jbj takes clearly big value additionally amplified jbj adaboost tries decrease negative margin efficiently improve error gc jbj let us consider asymptotic case number iterations therefore also jbj take large values cf lemma 2 case values mgz almost small differences amplified strongly gc jbj example margin mgz another margin difference amplified difference exp ie factor e 5 150 obviously function gc jbj asymptotically sensitive small differences margins training patterns equation 12 annealing parameter jbj takes big value adaboost learning becomes hard competition case patterns smallest margin get high weights patterns effectively neglected learning process therefore margins mgz c expected asymptotically converge fixed value ae subset training patterns asymptotically smallest margin ae call patterns support patterns cf figure 3 order confirm theoretical analysis correct asymptotic numerical simulations toy data several gauss blobs two di mensions cf figure made training data generated cumulative probability cumulative probability figure 3 margin distributions adaboost different noise levels oe 9 dashed 16 solid rbf nets 13 centers base hypotheses left 7 centers base hypotheses data oe adaboost iterations graphs experimentally confirm expected trends equation 14 several nonlinearly transformed gaussian uniform blobs 2 additionally disturbed uniformly distributed noise u00 oe 2 simulations used 300 patterns oe 2 one 0 9 16 simulations radial basis function rbf networks adaptive centers used learners cf appendix c detailed description figure 3 shows margin distributions 10 4 adaboost iterations different noise levels oe 2 left different strengths base hypotheses right figures becomes apparent margin distribution asymptotically makes step fixed size margin training patterns figure 3 one see influence noise data strength base hypotheses margin ae noise level high complexity low one gets higher training errors ffl therefore smaller value ae numerical results support theoretical asymptotic analysis interestingly margin distributions atas resembles one support vector machines svms separable case 4 9 30 cf figure 6 example cf figure almost patterns support vectors svs also lie within step part margin distribution adaboost adaboost achieves hard margin asymptotically svms separable case earlier study 26 observed usually high overlap among support vectors support patterns intuitively clear difficult patterns margin area emphasized strongly become support patterns support vectors asymptotically degree overlap depends kernel svm base hypothesis ata used svm rbf kernel highest overlap achieved average widths rbf networks used kernel width support vector detailed description generation toy data used asymptotical simulations found internet httpwwwfirstgmdderaetschdatabananatxt figure training patterns decision lines adaboost left rbf nets 13 centers svm right low noise case similar generalization errors positive negative training patterns shown lambda respectively support patterns support vectors marked chine 26 observed similarity support patterns sp adaboost sv svm also several applications sequel often assume asymptotical case hard margin achieved hypotheses combine better approximation indeed example often 200 adaboost iterations benchmark data used section 5 already approximation hard margin good cf equation 12 illustrated figure 5 shows typical distributions recapitulate findings section 1 adaboosttype algorithms aim minimize functional depends margin distribution minimization done approximate gradient descent respect margin cf 26 12 2 annealing part algorithm depends annealing parameter jbj controls good 01loss around approximated size margin decided certain annealing process speed annealing depends parameter oe implicit function strength learner training process 3 training patterns area decision bound ary asymptotically margin call patterns support patterns large overlap svs found svm 4 asymptotically hard margin achieved comparable one original sv approach 4 5 larger hard margins achieved ffl andor oe small cf corollary 5 low noise case choice 6 1 lead better generalization performance shown ocr 22 hard margin overfitting 11 cumulative probability figure 5 typical margin distribution graphs original adaboost 20 dotted 70 dashdotted 200 dashed iterations toy example 300 patterns networks centers used already 200 iterations asymptotical convergence almost reached 10103050709cumulative probability figure 6 typical margin distribution graphs normalized svm hard margin solid soft margin toy example rbf kernel width03 used generalization error svm hard margin two times larger 3 hard margin overfitting section give reasons ata noise robust exhibits suboptimal generalization ability presence noise give several references examples hard margin approach fail general noise present according understanding noisy data least one following properties overlapping class probability distributions b outliers c mislabeled patterns three kinds noise appear often data analysis therefore development noise robust version adaboost important first theoretical analysis adaboost connection margin distributions done schapire et al 28 main result bound generalization error p zd mgz 0 depending vcdimension base hypotheses class margin distribution training set probability least l satisfied 0 l denotes number patterns stated reason success adaboost compared ensemble learning methods eg bagging maximization margin experimentally observed adaboost maximizes margin patterns difficult ie smallest margin however increasing minimum margin patterns adaboost also reduces margin rest patterns hard margin overfitting 12 number iterations generalization error figure 7 typical overfitting behavior generalization error smoothed function number iterations left typical decision line right generated adaboost using rbf networks 30 centers case noisy data 300 patterns 16 positive negative training patterns shown lambda respectively support patterns marked approximation bayes decision line plotted dashed breiman 8 connection smallest margin generalization error analyzed experimentally could confirmed noisy data grove et al 14 linear programming lp approach freund et al 11 breiman 8 extended used maximize smallest margin existing ensemble classifiers several experiments lp adaboost uci benchmarks often noisy data made unexpectedly observed lpadaboost performs almost cases worse original adaboost algorithm even smallest margins larger experiments shown margin increases generalization performance becomes better datasets almost noise eg ocr however noisy data also observed adaboost overfits moderate number combined hypotheses example overlapping classes figure 7 left shows typical overfitting behavior generalization error adaboost data section 2 already 80 adaboost iterations best generalization performance achieved equation 14 clear adaboost asymptotically achieve positive margin oe 1 training patterns classified according possibly wrong labels cf figure 7 right complexity combined hypotheses increases achieved decision line far away bayes optimal line cf dashed line figure 7 right discuss bad performance hard margin classifier presence outliers mislabeled patterns analyze toy example figure 8 let us first consider case without noise left estimate optimal separating hyperplane correctly figure 8 middle outlier corrupts estimation adaboost certainly concentrate weights outlier spoil good estimate would get without outlier next let us consider complex decision lines overfitting problem gets even distinct gen 3figure 8 problem finding maximum margin hyperplane reliable data left data outlier middle mislabeled pattern right solid line shows resulting decision line whereas dashed line marks margin area middle left original decision line plotted dots hard margin implies noise sensitivity one pattern spoil whole estimation decision line erate complexity combining lot hypotheses training patterns even mislabeled ones outliers classified correctly figure 7 right figure 8 right see decision surface much shaky gives bad generalization cartoons becomes apparent adaboost noise sensitive maximizing smallest margin case noisy data lead bad results therefore need allow possibility mistrusting data bound 15 indeed obvious minimize smallest margin first term right hand side equation 15 takes whole margin distribution account would allow nonzero training error settings figure 8 first term right hand side 15 becomes nonzero 0 larger second term much smaller mason et al 18 similar bound used optimize margin distribution piecewise linear directly approach successful noisy data maximization smallest margin following introduce possibility mistrust parts data leads soft margin concept 4 improvements using soft margin original sv algorithm 4 similar problems ata respect hard margins sv approach training errors data overlapping classes allowed generalization performance poor noisy data introduction soft margins gave new algorithm achieved much better results compared original algorithm 9 cf figure 6 sequel show use soft margin idea atas section 41 change error function 10 introducing new controls importance pattern compared achieved margin section 42 show soft margin idea built lpadaboost algorithm section 43 show extension quadratic programming qpadaboost connections support vector approach 41 margin vs influence pattern first propose improvement original adaboost using regularization term 10 analogy weight decay define influence pattern combined hypotheses h r weighted average weight pattern computed atas learning process cf pseudocode figure 1 pattern often misclassified ie difficult classify high average weight high influence definition influence clearly depends base hypotheses space h corollary 5 theorem 2 8 training patterns get margin mgz larger equal 1 gamma 2oe many iterations cf figure 2 discussion section 2 asymptotically get following inequalities even better bounded equation 14 see relation ae gb sufficient large value jbj equation gb minimized ae maximized many iterations inequalities satisfied long oe 1 2 hard margin ae 0 achieved 28 lead overfitting case noise following consider case generalizations straight forward define soft margin pattern f tradeoff margin influence pattern final hypothesis follows f c 0 fixed constant p fixed exponent c p one modify tradeoff reformulate adaboosts optimization process terms soft margins 16 17 get f equivalent use z simplicity functional forms also used depending prior inequalities z positive training pattern high weights z increasing way force outliers classified according possibly wrong labels would imply high influence allow errors get desired tradeoff margin influence choose 19 original adaboost algorithm retrieved c chosen high data taken seriously retrieve bagging algorithm 5 inequality 18 derive new error function cf equation 10 aims maximize soft margin g reg b l exp ae gamma2 f oe l exp ae gamma2 theta mgz oe weight w t1 z pattern computed derivative equation 20 subject f given get update rule weight training pattern tth iteration 26 difficult compute weight b tth hypothesis es pecially hard derive weight analytically however get b line search procedure 23 20 unique solution g reg b 0 satisfied b 0 line search procedure implemented efficiently interpret approach regularization analogous weight decay whereby want incorporate prior knowledge patterns probably reliable therefore noisy case prefer hypotheses rely patterns high weights 3 instead looking hypotheses smaller values iz regularization adaboost changed easy classifiable patterns difficult patterns variables iz equation 19 also interpreted slack variables cf sv approach next section nonlinearly involved error function bigger values iz patterns allow larger soft margin ae summarizing modification adaboost constructed produce soft margin therefore avoid overfitting comparison soft margin distributions single rbf classifier adaboost reg see figure 9 3 interestingly also soft svm generates much sv high noise case low noise case therefore svm shows trend need patterns find hypothesis patterns noisy 30 cumulative probability cumulative probability figure 9 margin distribution graphs rbf base hypothesis scaled trained mean squared error left adaboostreg right different values c toy data set 10 3 iterations note values c graphs adaboostreg quite similar graphs singe rbf net 42 linear programming slack variables grove et al 14 showed use linear programming maximize smallest margin given ensemble proposed lpadaboost cf algorithm 23 approach first compute gain margin given hypotheses set defined matrix g gives information hypothesis contributes positive negative part margin pattern used formulate following maximin problem find weight vector c 2 r hypotheses t1 maximizes smallest margin ae min i1 l mgz solved linear programming 17 maximize ae subject linear program achieves larger hard margin original adaboost algorithm reasoning section 3 lpadaboost generalize well noisy data since even stronger overemphasizes difficult patterns eg outliers define softmargin pattern f introduce regularization lpadaboost technically approach equivalent introduction slack variables lpadaboost arrive algorithm lp reg adaboost 26 solves following linear program subject modification allows patterns smaller margins ae especially lower 0 tradeoff make margins bigger ae b maximize ae gamma c tradeoff controlled constant c 43 quadratic programming connection support vector machines following section extend lp reg adaboost algorithm quadratic programming using similar techniques support vector machines 4 9 17 gives interesting insights connection svms adaboost start transforming lp reg adaboost algorithm maximizes ae jcj kept fixed linear program ae fixed eg 1 jbj minimized unfortunately equivalent linear program use taylor expansions 4 get following linear program compare linear programming approaches related learning eg 31 13 1 minimize subject essentially algorithm 24 slack variables acting differently taylor expansion 1jbj used therefore achieve different soft margins previous section cf figure 10 instead using l 1 norm optimization objective 25 also use l p norm clearly p imply soft margin characteristics lead algorithm similar svm 4 24 straight forward get following problem fixed 0 equivalent 24 subject problem set ae 1 try optimize retrieve linear program use taylor expansion around 1 1 optimization objective svm find function h w minimizes functional form 30 l subject constraints variables slackvariables make soft margin possible norm parameter vector w measure complexity size margin hypothesis h w 30 functional 26 get tradeoff controlled c complexity hypothesis grade much hypothesis may differ training patterns ensemble learning yet measure com plexity empirically observed following different weights hypotheses higher complexity ensem ble mind use l p norm p 1 hypotheses weight vector kbk p complexity measure example assume kbk p small value elements approximately equal analogous bagging kbk p high values strongly emphasized hypotheses far away bagging intuitively clear bagging generates usually less complex classifiers lower kbk p example lpadaboost generate sparse representations kbk p large note arguments holds hypotheses weak enough otherwise kbk p carry desired complexity information hence apply optimization principles svms adaboost get following quadratic optimization problem minimize kbk 2 constraints given equation 25 algorithm call qp reg adaboost motivated connection lp reg adaboost cf algorithm 25 analogy support vector algorithm expected qp reg adaboost achieves large improvements solution original adaboost algorithm especially case noise comparison lp reg adaboost expect similar per formance type soft margin implied norm weight vector merits may needed specific dataset summarizing introduced soft margin adaboost regularizing objective function 10 b lp reg adaboost uses slack variables c qp reg adaboost interesting relation svms overall comparison margin distributions original adaboost svm adaboost reg lpqpadaboost see figures 5 6 9 10 5 experiments order evaluate performance new algorithms make extensive comparison among single rbf classifier original adaboost algorithm adaboost reg lqp reg adaboost support vector machine rbf kernel cumulative probability cumulative probability figure margin distribution graphs lpregadaboost left qpregadaboost right different values c toy data set 10 3 iterations lpregadaboost sometimes generates margins training set either 1 1 step distri bution 51 experimental setup use 13 artificial real world datasets uci delve statlog benchmark repositories banana toy data set used previous sections breast cancer 5 diabetes german heart image segment ringnorm flare sonar splice newthyroid titanic twonorm waveform problems originally binary classification problems hence random partition two classes used 6 first generate 100 partitions training test set mostly 60 40 partition train classifier get test set error experiments combined 200 hypotheses clearly number hypotheses may optimal however adaboost optimal early stopping often worse soft margin algorithms base hypotheses used rbf nets adaptive centers described appendix c data set used cross validation find best single classifier model used ensemble learning algorithms parameter c regularized versions adaboost parameters c oe svm optimized first five training datasets training set 5foldcross validation used find best model dataset 7 finally model parameters computed median five estimations way estimating parameters surely possible practice make comparison robust results reliable 5 breast cancer domain obtained university medical center inst oncology ljubljana yugoslavia thanks go zwitter soklic providing data 6 random partition generates mapping n two classes random sigma1 vector length n generated positive classes negative respectively concatenated 7 parameters selected cross validation nearoptimal 1020 values parameter tested two stages first global search ie wide range parameter space done find good guess parameter becomes precise second stage table comparison among six methods single rbf classifier adaboostab adaboostreg abrp2 lqpregadaboost lqprab support vector machine estimation generalization error 13 datasets best method bold face second emphasized adaboostreg gives best overall performance rbf ab abr lprab qprab svm banana 108sigma06 123sigma07 109sigma04 107sigma04 109sigma05 115sigma06 bcancer 276sigma47 304sigma47 265sigma55 268sigma61 259sigma46 260sigma47 diabetes 241sigma19 265sigma23 239sigma16 241sigma19 254sigma22 235sigma17 german 247sigma24 275sigma25 243sigma21 248sigma22 252sigma21 236sigma21 heart 171sigma33 203sigma34 166sigma37 145sigma35 172sigma34 160sigma33 image 33sigma06 27sigma07 27sigma06 28sigma06 27sigma06 30sigma06 ringnorm 17sigma02 19sigma03 16sigma01 22sigma05 19sigma02 17sigma01 fsonar 344sigma20 357sigma18 342sigma22 348sigma21 362sigma18 324sigma18 splice 99sigma10 103sigma06 95sigma07 99sigma14 103sigma06 108sigma06 thyroid 45sigma21 44sigma22 44sigma21 46sigma22 44sigma22 48sigma22 titanic 233sigma13 226sigma12 226sigma12 240sigma44 227sigma11 224sigma10 twonorm 29sigma03 30sigma03 27sigma02 32sigma04 30sigma03 30sigma02 waveform 106sigma10 108sigma06 98sigma08 105sigma10 101sigma05 99sigma04 mean 66sigma58 119sigma79 17sigma19 89sigma108 58sigma55 46sigma54 note perform simulations setup train adaptive rbf nets solve 10 5 mathematical programming problems task would taken altogether 2 years computing time single ultrasparc machine hadnt distributed computers 52 experimental results table 1 average generalization performance standard devia tion 100 partitions data sets shown second last line table 1 shows line mean computed follows dataset average error rates classifier types divided minimum error rate 1 subtracted resulting numbers averaged 13 datasets variance given last line shows probabilities particular method wins ie gives smallest generalization error basis experiments mean variance 13 datasets experiments noisy data cf table 1 show pi results adaboost almost cases worse single classifier shows clearly overfitting adaboost able deal noise data pi averaged results adaboost reg slightly better mean win results svm known excellent classifier single rbf classifier wins less often svm comparison regression case see 20 adaboost improves results adaboost due established soft margin results good results adaboost reg svm one reason hypotheses generated adaboost aimed construct hard mar may provide appropriate basis generate good soft margin mathematical programming approaches conclusion 21 pi observe quadratic programming gives slightly better results linear programming may due fact hypotheses coefficients generated lp reg adaboost sparse smaller ensemble bigger ensembles may better generalization ability eg due reduction variance 7 qpadaboost prefer ensembles approximately equal weighted hypotheses stated section 43 implies lower complexity combined hypothesis lead better generalization performance pi results adaboost reg cases much better adaboost better single rbf classi fier adaboost reg wins often shows best average per formance demonstrates noise robustness proposed algorithm slightly inferior performance svm compared adaboost reg may explained fixed oe rbfkernel svm loosing multiscale information b coarse model selection c bad error function sv algorithm noise model summarizing original adaboost algorithm useful low noise cases classes easily separable shown ocr 29 16 lqp reg adaboost improve ensemble structure introducing soft margin hypotheses another weight ing result ensemble shows much better generalization performance hypotheses used lqp reg adaboost may sub optimal part optimization process aims soft margin adaboost reg problem hypotheses generated appropriate form desired softmargin adaboost reg extends applicability boostingarcing methods nonseparable cases applied data noisy 6 conclusion shown adaboost performs approximate gradient decent error function optimizes margin cf equation 10 see also 8 22 12 asymptotically emphasis concentrated difficult patterns small margins easy patterns effectively contribute error measure neglected training process much similar support vectors shown theoretically experimentally cumulative margin distribution training patterns margin area converges asymptotically step therefore adaboost asymptotically achieves hard margin classification asymptotic margin distribution adaboost similar margin distribution svm separable case accordingly patterns lying step part support patterns show large overlap support vectors found svm however representation found adaboost often less sparse svms discussed detail adaboosttype algorithms hard margin classifiers general noise sensitive able overfit introduced three regularizationbased adaboost algorithms alleviate overfitting problem adaboosttype algorithms high noise data 1 direct incorporation regularization term error function proof lemma 1 22 adaboost reg use 2 linear 3 quadratic programming slack variables essence proposal achieve soft margin regularization term slack variables contrast hard margin classification used softmargin approach allows control much trust data permitted ignore noisy patterns eg outliers would otherwise spoiled classification generalization much spirit support vector machines also tradeoff maximization margin minimization classification errors introducing slack variables experiments noisy data proposed regularized versions adaboost adaboost reg lqp reg adaboost show robust behavior original adaboost algorithm furthermore adaboost reg exhibits better overall generalization performance algorithms including support vector machines conjecture unexpected result mostly due fact svm use one oe therefore loose multiscaling information adaboost limitation since use rbf nets adaptive kernel widths base hypotheses future work concentrate continuing improvement ada boosttype algorithms noisy real world applications also analysis relation adaboost qp reg adaboost support vector machines margins point view seems promising particular focus question good margin distributions look like moreover interesting see techniques established work applied adaboost regression scenario cf 2 acknowledgements thank valuable discussions b scholkopf smola frie schuurmans b williamson partial funding ec storm project number 25387 gratefully acknowledged proof lemma 1 proof 7 define z definition g get exp e e definition z 1l thus get e e z e e z cf step 4 figure 1 proof theorem 4 23 b proof theorem 4 proof follows one theorem 5 28 theorem 4 generalization oe 6 1proof 8 yfx also exp thus l exp exp l l exp l exp l exp exp exp exp e bt 2 exp recursively rbf nets adaptive centers 24 plugging definition b get oe oe oe oe c rbf nets adaptive centers rbf nets used experiments extension method moody darken 19 since centers variances also adapted see also 3 21 output network computed linear superposition k basis functions denotes weights output layer gaussian basis functions g k defined k denote means variances respectively first step means k initialized kmeans clustering variances oe k determined distance k closest kg following steps perform gradient descent regularized error function weight decay l 2l taking derivative equation 29 respect rbf means k variances oe k obtain l rbf nets adaptive centers 25 algorithm rbfnet input sequence labeled training patterns number rbf centers k regularization constant number iterations initialize run kmeans clustering find initial values k determine oe distance k closest 6 k 1 compute optimal output weights l 2a compute gradients e 31 30 optimal w form gradient vector v 2b estimate conjugate direction v fletcherreevespolakribiere cgmethod 23 3a perform line search find minimizing step size ffi direction v evaluation e recompute optimal output weights w line3b update k oe k v ffi output optimized rbf net figure pseudocode description rbf net algorithm used base learning algorithm simulations adaboost l two derivatives employed minimization equation 29 conjugate gradient descent line search always compute optimal output weights every evaluation error function line search optimal output weights notation computed closed form l denotes output vector identity matrix corresponds calculation pseudoinverse g simultaneously adjust output weights rbf centers variances see figure 11 pseudocode algorithm way network finetunes data initial clustering step yet course overfitting avoided careful tuning regularization parameter number centers k number iterations cf 3 experiments always used ten cg iterations r combining support vector mathematical programming methods induction boosting algorithm regression neural networks pattern recognition training algorithm optimal margin classifiers bagging predictors arcing edge prediction games arcing algorithms support vector networks decisiontheoretic generalization online learning application boosting game theory additive logistic regres sion statistical view boosting boosting limit maximizing margin learned ensembles optimization simulated annealing quantitative studies learning algorithms classification comparism handwritten digit recognistion nonlinear programming improved generalization explicit optimization margins fast learning networks locallytuned processing units using support vector machines time series prediction using support vector machines time series prediction asymptotic analysis adaboost binary classification case numerical recipes c boosting firstorder learning ensemble learning methods classifi cation improved boosting algorithms using confidencerated predictions boosting margin new explanation effectiveness voting methods adaboosting neural networks nature statistical learning theory density estimation using sv machines tr training algorithm optimal margin classifiers numerical recipes c 2nd ed c45 programs machine learning nature statistical learning theory networks bagging predictors game theory online prediction boosting improved boosting algorithms using confidencerated predictions connection regularization operators support vector kernels boosting limit using support vector machines time series prediction combining support vector mathematical programming methods classification regularizing adaboost improved generalization explicit optimization margins neural networks pattern recognition boosting margin adaboosting neural networks boosting algorithm regression theoretical views boosting barrier boosting boosting firstorder learning ctr masayuki nakamura hiroki nomiya kuniaki uehara improvement boosting algorithm modifying weighting rule annals mathematics artificial intelligence v41 n1 p95109 may 2004 rong jin huan liu robust feature induction support vector machines proceedings twentyfirst international conference machine learning p57 july 0408 2004 banff alberta canada theodore b trafalis alexander malyscheff analytic center machine machine learning v46 n13 p203223 2002 yuan alan qi thomas p minka rosalind w picard zoubin ghahramani predictive automatic relevance determination expectation propagation proceedings twentyfirst international conference machine learning p85 july 0408 2004 banff alberta canada yijun sun sinisa todorovic jian li increasing robustness boosting algorithms within linearprogramming framework journal vlsi signal processing systems v48 n12 p520 august 2007 jacek ski hokashyap classifier generalization control pattern recognition letters v24 n14 p22812290 october e andeli schaffner katz e krger wendemuth kernel leastsquares models using updates pseudoinverse neural computation v18 n12 p29282935 december 2006 alain rakotomamonjy variable selection using svm based criteria journal machine learning research 3 312003 rong jin jian zhang smoothed boosting algorithm using probabilistic output codes proceedings 22nd international conference machine learning p361368 august 0711 2005 bonn germany ulrike von luxburg olivier bousquet bernhard schlkopf compression approach support vector model selection journal machine learning research 5 p293323 1212004 training algorithms fuzzy support vector machines noisy data pattern recognition letters v25 n14 p16471656 15 october 2004 saharon rosset ji zhu trevor hastie boosting regularized path maximum margin classifier journal machine learning research 5 p941973 1212004 yoshikazu washizawa yukihiko yamashita kernel projection classifiers suppressing features classes neural computation v18 n8 p19321950 august 2006 steve billings kian l lee nonlinear fisher discriminant analysis using minimum squared error cost function orthogonal least squares algorithm neural networks v15 n2 p263270 march 2002 cynthia rudin ingrid daubechies robert e schapire dynamics adaboost cyclic behavior convergence margins journal machine learning research 5 p15571595 1212004 takashi takenouchi shinto eguchi robustifying adaboost adding naive error rate neural computation v16 n4 p767787 april 2004 manfred k warmuth jun liao gunnar rtsch totally corrective boosting algorithms maximize margin proceedings 23rd international conference machine learning p10011008 june 2529 2006 pittsburgh pennsylvania olivier chapelle vladimir vapnik olivier bousquet sayan mukherjee choosing multiple parameters support vector machines machine learning v46 n13 p131159 2002 koji tsuda motoaki kawanabe gunnar rtsch sren sonnenburg klausrobert mller new discriminative kernel probabilistic models neural computation v14 n10 p23972414 october 2002 n louw j steel variable selection kernel fisher discriminant analysis means recursive feature elimination computational statistics data analysis v51 n3 p20432055 december 2006 wei chu sathiya keerthi chong jin ong bayesian trigonometric support vector classifier neural computation v15 n9 p22272254 september stefano merler bruno caprile cesare furlanello parallelizing adaboost weights dynamics computational statistics data analysis v51 n5 p24872498 february 2007 cesare furlanello maria serafini stefano merler giuseppe jurman semisupervised learning molecular profiling ieeeacm transactions computational biology bioinformatics tcbb v2 n2 p110118 april 2005 jimmy liu jiang kiafock loe hong jiang zhang robust face detection airports eurasip journal applied signal processing v2004 n1 p503509 1 january 2004 carl gold alex holub peter sollich bayesian approach feature selection parameter tuning support vector machine classifiers neural networks v18 n56 p693701 june 2005 daniel yeung defeng wang wing w ng eric c tsang xizhao wang structured large margin machines sensitive data distributions machine learning v68 n2 p171200 august 2007 peter bhlmann bin yu sparse boosting journal machine learning research 7 p10011024 1212006 senjian wanquan liu svetha venkatesh fast crossvalidation algorithms least squares support vector machine kernel ridge regression pattern recognition v40 n8 p21542162 august 2007 arthur tenenhaus alain giron emmanuel viennet michel bra gilbert saporta bernard fertil kernel logistic pls tool supervised nonlinear dimensionality reduction binary classification computational statistics data analysis v51 n9 p40834100 may 2007 g blanchard c schfer rozenholc kr mller optimal dyadic decision trees machine learning v66 n23 p209241 march 2007 gonzalo martnezmuoz alberto surez using boosting prune bagging ensembles pattern recognition letters v28 n1 p156165 january 2007 gilles blanchard different paradigms choosing sequential reweighting algorithms neural computation v16 n4 p811836 april 2004 yoram baram ran elyaniv kobi luz online choice active learning algorithms journal machine learning research 5 p255291 1212004 e k tang p n suganthan x yao analysis diversity measures machine learning v65 n1 p247271 october 2006 erin l allwein robert e schapire yoram singer reducing multiclass binary unifying approach margin classifiers journal machine learning research 1 p113141 912001 gunnar rtsch manfred k warmuth efficient margin maximizing boosting journal machine learning research 6 p21312152 1212005 michael e tipping sparse bayesian learning relevance vector machine journal machine learning research 1 p211244 912001 jiannming wu natural discriminant analysis using interactive potts models neural computation v14 n3 p689713 march 2002 nigel duffy david helmbold boosting methods regression machine learning v47 n23 p153200 mayjune 2002 x hong r j mitchell backward elimination model construction regression classification using leaveoneout criteria international journal systems science v38 n2 p101113 01 february 2007 mathias adankon mohamed cheriet optimizing resources model selection support vector machine pattern recognition v40 n3 p953963 march 2007 robust loss functions boosting neural computation v19 n8 p21832244 august 2007 michael collins robert e schapire yoram singer logistic regression adaboost bregman distances machine learning v48 n13 p253285 2002 rong jin jian zhang multiclass learning smoothed boosting machine learning v67 n3 p207227 june 2007 philip long vinsensius berlian vega boosting microarray data machine learning v52 n12 p3144 julyaugust w john wilbur lana yeganova kim synergy pav adaboost machine learning v61 n13 p71103 november 2005 kaiquan shen chongjin ong xiaoping li einar p wildersmith feature selection via sensitivity analysis svm probabilistic outputs machine learning v70 n1 p120 january 2008 philip long minimum majority classification boosting eighteenth national conference artificial intelligence p181186 july 28august 01 2002 edmonton alberta canada hochreiter klaus obermayer support vector machines dyadic data neural computation v18 n6 p14721510 june 2006 masashi sugiyama dimensionality reduction multimodal labeled data local fisher discriminant analysis journal machine learning research 8 p10271061 512007 roman krepki gabriel curio benjamin blankertz klausrobert mller berlin braincomputer interfacethe hci communication channel discovery international journal humancomputer studies v65 n5 p460477 may 2007 gavin c cawley nicola l c talbot preventing overfitting model selection via bayesian regularisation hyperparameters journal machine learning research 8 p841861 512007 gunnar rtsch sebastian mika bernhard schlkopf klausrobert mller constructing boosting algorithms svms application oneclass classification ieee transactions pattern analysis machine intelligence v24 n9 p11841199 september 2002 ralf herbrich thore graepel colin campbell bayes point machines journal machine learning research 1 p245279 912001 gunnar rtsch ayhan demiriz kristin p bennett sparse regression ensembles infinite finite hypothesis spaces machine learning v48 n13 p189218 2002 ron meir gunnar rtsch introduction boosting leveraging advanced lectures machine learning springerverlag new york inc new york ny