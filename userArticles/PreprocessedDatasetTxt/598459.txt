joy sampling standard method handling bayesian models use markov chain monte carlo methods draw samples posterior demonstrate method two core problems computer visionstructure motion colour constancy examples illustrate samplers producing useful representations large problems demonstrate sampled representations trustworthy using consistency checks experimental design sampling solution structure motion strictly better factorisation approach reports uncertainty structure position measurements direct waysemi identify tracking errorssemi estimates covariance marginal point position reliable colour constancy solution strictly better competing approaches reports uncertainty surface colour illuminant measurements direct waysemi incorporates available constraints surface reflectance illumination direct waysemi integrates spatial model reflectance illumination distribution rendering model natural way one advantage sampled representation resampled take account information demonstrate effect knowing colour constancy example surface viewed two different images fact object conclude general discussion strengths weaknesses sampling paradigm tool computer vision b quite often practice easy come function f proportional posterior case posterior f r f u du integralthe normalizing constantcan difficult compute best way use sampling method attractive feature metropolishastingsalgorithmisthatweneednotknow normalizing constant distribution constant cancelled ratio 15 techniques building practical mcmc samplers easy build sampler using metropolishastings algorithm seems hard build good samplerone burns quickly mixes well gives trustworthy picture posteriorusing algorithm describe variety techniques building samplers conclude discussion possible sanity checks 151 gibbs samplers quite common encounter situations target distribution non standard form standard groups variables fixed values occurs vision problems see sections 23 32 case natural adopt proposal mechanism fixes one set variables draws sample full conditional distribution set vice versa useful technique known gibbs sampling named geman geman 1984 apparently due statistical physics literature known heat bath algorithm gilks et al 1996b p 12 usu ally group variables sampled chosen random sufficient samples drawn group variables visited many times gibbs sampling easy implement one considerable danger often quite dif ficult avoid groups variables strongly 112 forsyth haddon ioffe figure 1 correlated variables cause gibbs samplers behave badly figure top left shows 100 samples drawn gibbs sampler two independent normal random variables one variance one variance ten stars indicate samples line segments indicate order samples drawn note sampler makes quite large vertical moves variance direction large figure top right shows 100 samples drawn distribution rotated 45 using gibbs sampler case sampler make relatively small vertical horizontal moves position samples changes relatively slowly 100 samples graph bottom left consist first graph rotated 45 give much better picture distribution bottom right xcoordinate samples drawn second sampler solid line xcoordinates third figure dashed line solid curve correctly suggests samples drawn second sampler quite strongly correlated correlated gibbs sampler mix badly indeed effect well known full discussion see example gilks roberts 1996 easily illustrated see fig 1 152 hybrid monte carlo method common difficulty sampling methods state sampler appears perform slightly biased random walk difficulty random walk takes long time move distance along domain meaning sampler started point long way mode distribution take long time reaches mode perspective extremely important representation distribution around mode hybrid monte carlo method making proposals causes state sampler move rather quickly mode explore method due duane et al 1987 described detail neal 1993 write state sampler q method requires target distribution written let us think u potential function state sampler state particle mass subject potential function state determined considering momentum particle p writing hamiltonian particle hq p uq c need integrate hamiltons equations drqu determine state particle temporary excursion mechanics actually justified exponentiate negative hamiltonian particle get new target distribution larger set random variables two proposal moves 1 advance time particle model randomly chosen amount either forwards back wards updates q p long use symplectic integrator extent advance uniform random choice forward backward random accept probability one 2 fix q draw sample p full con ditional easy full conditional distribution p normal independent q sampler attractive qualitative behaviour state relatively large value u first type move travel quickly gradient u smaller values building momentum second move discards momentum sampler move quickly modewhere u smalland move around exploring mode influence random choice momenta good values particles mass range time values must chosen experiment practice hybrid method seems useful continuous problems easy implement colour constancy example given joy sampling 113 successfully used variety continuous problems neal 1993 16 mcmc random search vision markov chain monte carlo appeared vision literature various forms one common use attempt obtain map estimate random search usually using metropolishastings algorithm eg geman geman 1984 geman graffigne 1986 markov random field model spatial model gives posterior image labellings given measurements function measurement values local patterns pixel labels socalled clique potentials topic reviewed li 1995 standard method estimating map labellings use annealed version metropolishastings algorithm posterior sampled function parameter changes sampling process parameter often thought tempera ture intent high values parameter posterior one mode temperature reduced state sampler get stuck mode thereby obtaining global extremum possible guarantee practice occurs algorithm rather mixed reputation collins et al 1988 golden skiscim 1986 notion using sampling method perform inference generative model image pattern appears due grenander 1983 successful examples appear literature jolly et al 1996 annealing method used estimate map solution configuration motion motor car template image zhu 1998 random search method used find medial axis transform zhu et al 2000 mcmc method used find simple shapes road signs green 1996 mcmc used perform inference various visionlike situations including reconstruction single photon emission computed tomography data finding polygonal template duck heavy spatial noise phillips smith 1996 inference performed hierarchical model find faces version mcmc usedtofindanunknownnumberofdiskstemplatesare used restoration amit et al 1991 gibbs samplers quite widely used reconstruction geman geman 1984 geman graffigne 1986 zhu et al 1998 random search standard method estimating fundamental matrix structure 114 forsyth haddon ioffe motion problems review appears torr murray 1997 ransacan algorithm robust fitting due fischler bolles 1981 appearing statistical literature rousseeuw 1987proposes small sets correspondences uniformly random fits fundamental matrix set accepts set whose fit gives largest number correspondences sufficiently small residual number sets chosen ensure high probability correct set found main advantage mcmc method ransac mcmc method produce series hypotheses meaningful semantics indicating example posterior probability particular point outlier posterior probability pair measurements come single point 161 particle filtering condensation survival fittest resampling substantial impact sampling algorithms vision use resampling algorithms tracking best known algorithm known condensation vision community blake isard 1998 survival fittest ai community kanazawa et al 1995 particle filtering statistical signal processing community originated carpenter et al 1999 kitagawa 1987 wide range variants applications particle filtering described forthcoming book doucet et al 2001 algorithm modification factored sampling one draws samples prior represents state world k1th measurement propagates samples dynamical model weights using posterior incorporating kth measurement set weighted samples provides representation prior next iteration algorithm fast efficient quite widely applied lowdimensional problems attraction resampling algorithms used incorporate new information tracking applications new information comes new frame new measurements arrived new information may come sources colour constancy example assume algorithm told two patches two different images colour might occur recognition algorithm good match geometry knows patches represent object information strongly constrains inferred colours patches view section 3 recognition applications one often encounters form hierarchical model suggests resampling ioffe forsyth 1999 sampler usedtolabelgroupsofimagesegmentsusingtheircon sistency observed human kinematics human model used nine segments foolish attempt label nine segment groups instead algorithm uses sampler label individual segments frequency proportional posterior probability label given image data set individual segment labels resampled propose pairs labels pairs segments case new information use enhanced prior prior pairs labels emphasizes pairs segments lie particular configurations property meaningless single segments 2 example large scale sampling bayesian structure motion structure motion problem inferring description geometry sequence images problem long history huge literature space allow comprehensive review see beardsley et al 1997 faugeras et al 1998 faugeras robert 1996 gool zisserman 1997 hartley zisserman 2000 accurate solutions structure motion attractive technique used generate models rendering virtual environments eg debevec et al 1996 faugeras et al 1998 gool zisserman 1997 tomasi kanade 1992 21 structure motion matrix factorisation assume distinct views n points given correspondences known influential tomasikanade formulation structure motion tomasi kanade 1992 data arranged 2m n matrix measurements must factor uv u represents camera positions v represents point positions affine transform determined ua minimises set constraints associated camera a1v represents euclidean structure practice factorisation achieved using singular value decomposition maximum likelihood method isotropic gaussian error model adopted anisotropic gaussian error model see morris kanade 1998 formalism applied various camera models poelman 1993 tomasi kanade 1992 triggs 1995 missing data points interpolated known points jacobs 1997 tomasi kanade 1992 methods motion segmentation exist costeira kanade 1998 methods lines similar primitives known morris kanade 1998 noise estimates recovered structure morris kanade 1998 assume errors estimates structure independent assumption authors acknowledge always sustainable factorisation method one important weak ness algorithm two separate stages allow payoff model error extent recovered model violates required set camera constraintsand measurement errorthe extent model predictions correspond data observations means model cannot used identify measurement problems example tracker errors fig 5 subject reconstruction errors caused incorporating erroneous measurements property al gorithm rather problem u v relatively degrees freedom compared possible identify ignore many unreliable measurements full force model employed recent work dellaert et al shown strongly model constrains data use sampling method average correspondences weightingthembyconsistencywithmeasureddataand obtaining satisfactory reconstruction method removes need compute correspondences structure motion problems dellaert et al 2000 22 posterior structure motion useful think bayesian models generative modelseggrenander1983inagenerativestructure motion model u v drawn appropriate priors obtained adding noise uv assume noise obtained mixture model large probability gaussian noise used small probability measurement value replaced uniform random variable priors u v obtained constraints camera structure fix origin coordinate system represent points homogenous coordinates u v dimensions 2m 4 4 n respectively assume scaled orthographic viewing model unknown scale varies frame frame joy sampling 115 yields vector constraint equations contains elements form expressing fact camera basis consists elements length expressing fact camera basis elements perpendicular homogenous coordinates natural prior use proportional exp 22 constraint prior penalises violations constraints quite strongly allows constraint violations paid one approach essence penalty method alternative insist prior uniform constraints satis fied zero otherwisein practice would involve constructing parametrisation domain prior nonzero working parametrisation approach numerically complex implement also disadvantage one imposing constraints may fact violated ie scaled orthography model may sufficient imaging element may misaligned respect lens camera basis consists elements slightly different length etc write posterior model recall noise process mixture two processes first adds gaussian noise second replaces measurement value uniform random variable introduce set discrete mask bits one per measure ment matrix mask bits determine noise model measurement affected mask bit 1 good measurement ie one affected isotropic gaussian noise 0 bad measurement ie one contains information model bits compared mask bits used fitting mixture models using em haddon ioffe see discussion mclachlan krishnan 1996 boundary processes used among oth ers blake zisserman 1987 mumford shah 1989 introduce prior mm zero matrices fewer k nonzero elements row column uniform otherwise prior ensures attempt inference situations insufficient measurements likelihood pdju v proportional exponential c ij 2m2eas 2b2ad posterior proportional 22 constraint notice maximum posterior could well occur maximum likelihood although factorisation might fit data well u factor may satisfy camera constraints poorly 23 sampling structure motion model formulation contains discrete continuous component natural consider using gibbs sampler sampling full conditional point positions given fixed camera positions full conditional camera positions given fixed point positions works poorly variables highly correlateda tiny shift point position given fixed camera positions tends result large error instead continuous variables sampled using hybrid method described section 12 discrete variables sampled full conditional using strategy proposes inverting 5 bits randomly chosen time hybrid mcmc moves proposed probability 07 discrete variable moves proposed probability 03 3 example sampling unknown number components bayesian colour constancy image appearance set surfaces affected reflectance surfaces spectral radiance illuminating light recovering representation surface reflectance image information called colour constancy computational models customarily model surface flectances illuminant spectra finite weighted sum basis functions use variety cues recover reflectance including limited specular reflections lee 1986 constant average flectance buchsbaum 1980 illuminant spatial frequency land mccann 1971 lowdimensional families surfaces maloney wandell 1986 physical constraints reflectance illumination coefficients forsyth 1990 finlayson 1996 cue wellknown strengths weaknesses complete recent study appears brainard freeman 1997 uses cues make bayesian decisions maximise expected utility compares quality decision inaccurate decisions confound recognition funt et al 1998 31 probabilistic model assume surfaces flat shading variation due surface orientation terreflection four components model viewing model assume perspective view flat frontal surface focal point positioned center surface spatial resolution major issue work 50 50 pixel grid speed spatial model surface reflectances spatial statistics primary focus paper use model reflectances constant grid boxes grid edges known advance natural improvement would random polygon tesselation green 1996 spatial model illumination work described paper assume single point source whose position uniformly distributed within volume around viewed surface rendering model determines receptor responses resulting particular choice illuminant surface reflectance follows standard considerations 311 rendering model model surface flectances sum basis functions j assume reflectances piecewise constant xns j x set coefficients vary space according spatial model similarly model illuminants sum basis functions assume spatial variation given presence single point source positioned p diffuse component due source xne coefficients basis function p gain term represents change brightness source area viewed specular component due source xne mx p gain term represents change specular component area viewed standard considerations yield model kth receptor response cmxyp hiki z z hik ki kis sensitivity kth receptor class illuminant terms dx point source model mx p obtained using phongs model specularities write prior probability distribution model process image generated sample number reflectance steps x kx respectively prior kxky kxky sample position steps ex ey spectively prior exey jkxky ex jkxey j ky tile sample reflectance mth tile tile prior joy sampling 117 sample illuminant coefficients prior sample illuminant position p prior p rendser image adding gaussian noise known standard deviation cc value pixel gives likelihood posterior proportional ip 312 priors practicalities spatial model specify spatial model giving number edges x direction separately position edges reflectances within block assume seven edges patches within direction purely efficiency prior used poisson distribution censored ensure values greater seven zero prior rescaled edge positions chosen using hardcore model first edge position chosen uniformly second chosen uniformly number pixels first never fewer five third chosen uniformly number pixels second first never fewer five hardcore model ensures edge close together pixel evidence edges moot priors reflectance illumination surface reflectance functions never less zero greater one means coefficients functions lie compact convex set easy obtain representative subset family planes bounds set sampling basis functions set wavelengths similarly illuminant functions never less zero meaning coef ficients functions lie convex cone cone easily approximated constraints reflectance illuminant coefficients encoded prior use prior constant within constraint set falls exponentially estimate distance constraint set constraint sets convex expressed set linear inequalities surface reflectance haddon ioffe illuminant ci 0ifthe coefficients inequalities normalised ie rows matrices unit vectors largest negative value inequalities estimate distance constraint set use six basis elements illumination flectancesothatwecanhaveforexamplesurfacesthat look different one light another light phenomenon known metamerism occurs real world exploration ambiguities represent possibility represent surface colour colour surface rendered known white light 32 sampling colour constancy model proposals made mixture five distinct moves chosen random probability proposing particular type move uniform exception edges deaths proposed number edges particular direction maximum births proposed important advantage approach within movewe assume values variables changing correct apply standard algorithms estimate values calculations straightfor ward along lines green 1995 moving light proposals new x position light obtained filtering image apply filter whose kernel shape typical specularity zero mean r g b components separately responses divided mean intensity sum squared responses rescaled form proposal distribution kernel obtained averaging large number specularities obtained using draws prior illuminant position using image data construct proposal distributions appears lead quite efficient samplers also quite generally applicable zhu et al 2000 call data driven mcmc point proposals move light z uniform within small range current position real dataset specularities moves demonstrated synthetic data birth edge direction apply derivative gaussian filter red green blue components image divide response weighted average local intensity result squared summed along direction interest normalised 08 02 uniform distribution added process produces proposal distribution strong peaks edge specularity completely exclude legal edge point fig 2 using image information construct appropriate proposal process given state proposal distribution zeroed points close existing edges consistency hard core model proposed new edge position chosen result position chosen must choose new reflectances new patches created birth edge gen erally give two new patches reflectances similar old patch expect small change posterior advantageous encourages exploration cur rently average receptor responses within new patch use known illuminant estimate reflectance comes close possible achieving average value lying within constraint set add gaussian random variable estimated reflectance value currently use vector independent gaussian components standard deviation 05 choice depend basis fitted death edge edge whose death proposed chosen uniformly random death edge causes pairs surface patches fused new reflectance fused region obtained using mechanism birth ie receptor responses averaged known illuminant used estimate good reflectances patch vector independent gaussian components standard deviation 05 added result moving edge edge move chosen uniformly random within region available points governedbythehardcoremodeltheedgecannotget close edges either side new position proposed uniformly random somewhat efficient compared use filter energies proposal distribution use mechanism avoid problem posed hardcore model difficult sampler move state two edges placed close together either side real edge neither edge moved real edgethe repels itand new edge cannot proposed right side furthermore may little advantage killing either two edges proposing uniform moves alleviates problem increasing possibility one two edges move away move onto right spot joy sampling 119 figure 2 proposal distribution edge birth x direction mondrian image shown proposal distribution obtained filtering image dividing response weighted average local intensity summing ydirection result normalised 08 02 uniform distribution added note filtering process leads strong peaks near edges means proposal process relatively efficient completely rule edges away strong responses evidence found presence likelihood component posterior 120 forsyth haddon ioffe change reflectance illumination tempting use gibbs sampler chain moves extremely slowly instead sample flectance illumination simultaneously using hybrid method section 12 poor behaviour gibbs sampler explained follows assume sampler burnt means current choice surface flectance illuminant coefficients yields quite good approximation original picture assume fixed surface reflectance coefficients wish change illuminant coefficients expect normal distribution illuminant coefficients mean somewhere close current value fairly narrow covariance substantial change illuminant coefficients lead image different original picture means change illuminant coefficients results small similarly fix illuminant coefficients sample surface reflectance coefficients expect changes result small 4 experimental procedures case sampler started state chosen random state chosen start procedure described detail section 54 main difference methods choosing start point tends lead sampler appears burn quickly 41 structure motion results obtained using hotel dataset courtesy modeling videotaping group robotics institute carnegie mellon university report two types experiment first sampler run dataset second small percentage points dataset replaced uniform random numbers range image coordinates represents large noise effects coordinates dataset appear lie range 1512 algorithm appears quite well behaved rang choices constant values forpthe constants pfigs 5 6 9 meas 1 2 constraint 1 5000 bad slightly larger meas allowing points range distance measurement measuprement disallowed used meas 5 constraint figures experience suggestsitispossibletouse constraint verymuchsmaller without apparently affecting freedom sampler mixes 42 colour constancy fig 3 indicates sampler runs synthetic im ages makes reasonable estimates position edges specularity illuminant surface colours case basis constraints known advance applying sampler real data interesting data set shown fig 8 consists images originally used forsyth 1990 figure 3 left typical synthetic mondrian rendered using linear intensity scale thresholds specularity center proposal distribution x position specularity obtained image filtering shown highest value white right rendering typical sample case using samples illuminant successful sampler produces samples look like image results real images shown colour fig 8 images set patches mondrian coloured paper patches photographed white blue yellow purple red cyan light specularities used diffuse model data set original data lost used versions scanned paper images displayed crt photographed display subjected fourcolour printing scanned remarkable constancy possible circumstances basis obtained using bilinear fitting procedure marimont wandell 1992 determining appropriate constraint regions dif ficult obtained natural coordinate system using principal components constructed bounding box coordinate system box grown 10 along axis understanding none colours mondrians forsyth 1990 deeply saturated red green blue receptor responses represented numbers range zero one use cc 164 implying top six bits receptor response reliable 5 assessing experimental results sections 2 3 phrased two standard vision problems inference problems quite nasty inference problems large numbers continuous discrete variables possible sections indi cated extract representation posterior problems believe representations helpful well compare representations methods might offer cautions must observed making comparisons firstly important apply reality check representations sampler pro duces determine reason believe sampler burntin secondly comparing representation posterior given data result method reports minimum error solution offers perfunctory error check nature information produced two algorithms different meaningful comparison possible reports properties posterior gold standard tests available methods known produce accurate representations posterior density test sampler however compare representation produced sampler methods significantly cheaper computationally joy sampling 121 51 reality checks sampler burnt mixing convergence diagnostics mcmc methods eg see besag et al 1995 roberts 1992 suggest convergence none exists easy produce chain pass tests without burnt instead rely general methods firstly check ensure sampler move near maximal value posterior start position within reasonable number moves secondly check state sampler moves freely domain represented third built various consistency checks experiments 511 structure motion figure 4 shows series samples drawn posterior structure motion problem indication order samples drawn indicating sampler mixing relatively well samplers mixing rate appear sufficient give reasonable estimate structure posterior around mode clear sampler move around whole domain freely posterior contains discrete symmetry fixed value mask bits one multiply u square root identity left v square root identity right obtain value posterior creates particular difficulty practice solutions widely isolated one another sampler move peak peak probability hybrid method would obtain sufficient momentum cross large regions low probability effectively zero fact desirable property symmetry means accurate estimates mean value u v would zero consistency checks general expect sampler behaving properly able identify correspondence errors produce stable representation fact number subtle tracker errors hotel sequence figure 5 shows sampler identify tracker errors figure 6 illustrates large tracker errors artificially inserted dataset purpose identified 512 colour constancy sampler described run many synthetic images ground truth known case reaches small neighbourhood ground truth randomly 122 forsyth haddon ioffe figure 4 plots illustrate path taken state space structure motion sampler plot connects position given point every tenth sample starting 100th paths coded grey level clarity early samples light path moves darker grey levels fact paths repeatedly cross return regions suggests sampler mixing rather freely selected start pointie burns inwithin 1000 samples experimental data shown suggests sampler mixes well wide spread marginal densities reflectances consistency checks sampler run six images scene fact images scene built model spread samples surface reflectance coefficients recovered particular surface particular image quite wide see fig 8 however compare spread samples surface different images clusters overlap means representation correctly encoding fact surfaces could similar fact shall see section 52 representation encodes fact surface patches could similar 52 attractive properties sampled representations three attractive properties sampled representations derived provide covariance estimate inferred state resampled incorporate new information appear stable perturbations input data set describe properties 521 covariance samplers described produce representation posterior probability distribution joy sampling 123 figure 5 two cropped frames hotel sequence showing single sample reconstruction squares correspond measurements mask bit one ie measurement point frame believed correct white cross dark background corresponds measurement mask bit zero ie measurement point frame believed incorrect grey diamonds correspond model predictions extent diamond centered within square gives extent model prediction supported data right frame several locations tracker skipped another feature unknown reasons case reconstruction identifies data point erroneous reprojects point significantly different position measurement reported tracker lying correct measurement would seen position relative surface texture object given data set particularly attractive feature special datasets require additional analysis ex ample every element image colour expect colour constancy sampler produce wide spread samples surface reflectance similarly structure motion data set obtained camera translating plane sampler return set samples substantial variance perpendicular plane without ado second attractive feature expectations marginal probability distributions easily available compute expectation function average functions value samples compute marginal drop irrelevant terms state sample figure 7 illustrates kind information sampler produce structure motion data particular sampler reflects scatter possible inferred values single point figure 8 show set typical results sampler produce real images colour constancy problem spatial model identifies edges correctly groups samples drawn surface flectance different lights intersect expect furthermore groups samples drawn different surface reflectances light tend tersect meaning surfaces generally seen different figure shows rendering samples white light give impression variation descriptions results 522 resampling incorporate new information assume engaged colour constancy construct representation surface colour new 124 forsyth haddon ioffe figure 6 perturb hotel sequence replacing 5 data points draws uniform distribution image plane bayesian method started section 541 easily discounts noise points figure shows frames sequence fig 5 uncropped show noise sample reconstruction indicated using notation figuresquares correspond measurements mask bit one ie measurement point frame believed correct white cross dark background corresponds measurements mask bit zero ie measurement point frame believed incorrect grey diamonds correspond model predictions extent diamond centered within square gives extent model prediction supported data figure 7 black points show overhead view single sample 3d reconstruction obtained using 40 frames 80 points hotel sequence rotated hand show rightangled structure model indicating structure qualitatively correct cloud grey points samples position single point scaled 1000 show small uncertainty available single point measurement information arriveswhat representa ample assume sampled representation tion probabilistic answer relatively straight posterior two distinct images told forward adjust representation convey patch one image patch another posterior incorporating new information ex impact interpretation images sampled representation well suited determining effect information particular samples pa state j image pb state b j image b suppressed details rest state notation interpret mean patch sample gaussian distribution unknown mean fi known standard deviation would like obtain samples state state b j image image b image abbreviated im etc pim im b j state state bfi proportional z pim state j apa j fi da dbfi term inside integral pstate image pstate bb image b b two sets samples ween sure samples independent identically distributed shuffling remove correlations introduced mcmc means conditional density ith sample p ia ji pstate aaimage construct new sampler whose state fi jfig ensure produces samples distribution use theis js indexes previous set samples marginalise respect b simply dropping values sample joy sampling 125 result set samples distributed according desired distribution z pim state j apa j fi da dbfi building sampler obtains samples fi jfig space according desired distribution involves technical difficulties beyond scope paper approach essentially chooses pairs consisting sample set image sample set image b pairs chosen frequency higher values inferred particular patch similar course trick extends images figure 8 shows results obtained assuming single surface patch six images typically small number sets samples much higher probability others sampled representation consists large number copies samples interspersed one two others results much reduced variance rendering patch known similar six images error balls surface patch intersect relatively small region however mean variance inferred reflectances patches must duced reduced fig 8 representations recovered separate input image correctly captures possibility surface patches another important reality check strongly suggests sampled representation trustworthy algorithm able use information one patch image obtain representation strongly suggests patches 523 stability recovered representations reconstructions cannot compared basis accuracy ground truth available however demonstrate sampled representations stable various perturbations input structure motion small errors tracker response points could lead significant perturbations reconstruction points reconstructed point positions independent coupled reconstructed camera configu rations small errors tracker response actually occur 40 frames hotel sequence used six point measurements nine frames affected forsyth haddon ioffe small tracker errors shown fig 5 small errors affect reconstruction obtained using factorisation method factorisation matrix function entries equiv alently reconstructed point positions coupled reconstructed camera configurations compare stability methods introduce larger tracker errors small percentage data points randomly selected replaced draws uniform distribution image plane points included factorisation results essentially meaningless provide fair comparison use factorisations obtained using method section 541 start points sampler reconstructions guaranteed ignore large error points ignore significant percentage data comparison sampler quickly accretes points consistent model gives sig nificantly stable measurements cf torr zisserman 1998 uses maximum likelihood identify correspondences reconstruction unknown scaled euclidean frame reconstructions best compared comparing angles subtended corresponding triples points comparing distances corresponding points scaled minimize errors sampled representation significantly stable tracker errors noise factorisation method figs 9 10 joy sampling 127 53 comparing different algorithms obtaining covariance estimates probability distributions devices computing ex pectations computing expectation integration problem high dimensional problems like described curse dimensionality applies quadrature methods appropriate eg review numerical integration methods evans swartz 2000 leaves us two possibilities random quasirandom method analytic approximation integral applying quasirandom methods problems described appears pose substantial technical difficulties refer interested reader evans swartz 2000 traub werschulz 1999 analytic approximation currently used computer vision based laplaces method de scribed evans swartz 2000 form use ripley 1996 p 63 shall call approximation laplaces method follows approach models unimodal posterior distribution normal distribution whose mean mode posterior whose covariance matrix inverse hessian posterior mode essence approximation notes main contribution expectation computed using peaky probability distribution mode contribution tails estimated hessian mode figure 8 images set patches mondrian coloured paper patches photographed white blue purple red aqua yellow light scanned forsyth 1990 used inputs sampler b renderings typical representations obtained sampler case shown coloured light inferred successful result inferred representation looks like image note accuracy spatial model robustness image noise c renderings typical representations white light successful result implies similar renderings first two components surface reflectance samples plotted axes four different surfaces sample colour keyed image obtained red samples red image etc black corresponding white image circles show samples reflectance coefficients blue surface top left corner mondrian stars yellow surface second row plusses show samples orange surface top row mondrian crosses red surface bottom row surface generates smear samples represent uncertainty inferred surface reflectance given particular image input important consistency check data notice smear samples corresponding particular surface one image intersects smear corresponding surface another means representation envisages possibility commit e first two components surface reflectance samples plotted axes four different surfaces come samples shown resampled assumption blue surface top left hand corner mondrian image use representation axes figure notice single piece information hugely reduces ambiguity representation f samples reflectances returned patch mondrian using images shown light rendered white light four hundred samples per patch per illuminant rendered small square thus patch little information shows saltandpepper style texture rows show samples patch different illuminants column corresponds illuminant order aqua blue purple red white yellow notice substantial variation appearance white pixels denote samples saturated notice also patch samples look similar g samples obtained samples resampled assuming right blue patch patch image h samples obtained samples resampled assuming sixth yellow patch patch image notice substantial reduction variance constraint force patches look fact surface 128 forsyth haddon ioffe figure 9 factorisation method relatively unstable noise compare reconstructions obtained uncorrupted data set reconstructions obtained 5 entries replaced draws uniform distribution image plane represent factorisation method fairly use start points obtained using algorithm section 541 masks suspect measurements left shows histogram relative variations distances corresponding pairs points right shows histogram differences angles subtended corresponding triples points note scalessome interpoint distances misestimated factor 3 angles 2 figure 10 bayesian method stable noise compare reconstructions obtained uncorrupted data set reconstructions obtained 5 entries replaced draws uniform distribution image plane left shows histogram relative variations distances corresponding pairs points right shows histogram differences angles subtended corresponding triples points note significant increase stability factorisation method relative errors distance order 10 angular errors order 40 laplaces method natural linearisation used estimates covariance structure motion literature morris kanade 1998 however fig 11 indicates estimates produces differ substantially estimates produced sampler seen section 51 sampler appears mix acceptably samples significantly understate covariance com pare fig 11 fig 4 shows order samples drawn samples fig 11 instead laplaces method approximates probability density function poorly log posterior consists largely terms degree four cases hessian significantly poor guide structure logposterior long way mode joy sampling 129 figure 11 compare sampled representation posterior structure motion problem representation obtained using analytic approximation six plots depict three different estimates marginal posterior probabilities point position plane parallel optical axis points points fig 4 samples shown scatter plot case one standard deviation ellipse covariance estimate obtained laplaces approximation largest three shown substantially overestimates covariance orientation often misleading plotted light grey case second largest ellipse one standard deviation ellipse obtained using laplaces approximation assuming point camera positions independent still overestimate better estimate laplaces approximation plotted dark grey finally smallest ellipse case obtained sample mean covariance plotted darkest grey laplaces approximation appears significantly overestimate covariance almost certainly hessian mode poor guide behaviour tails posterior problem particular overestimates weight tails therefore overestimates covariance purely local estimate structure posteriorwe cannot rely second derivative function point necessarily convey helpful information function long way away point comparison sample involves least comparison values posterior sample previous sample samples relying local estimate structure posterior really useful comparison available case colour constancy current colour constancy algorithms report either exact solutions minimum error solutions laplaces method produce absurd covariance estimates domain integration heavily truncated constraints section 3the tails make contribution unreasonable expect sensible approximation method 54 speed samplers relatively slow samples take longer draw structure motion problem 2000 samples 40 views 80 points day haddon ioffe 300 mhz macintosh g3 system compiled matlab colour constancy problem 1000 samples hour compiled matlab computer irritatingly slow disqualify technology particular important keep mind cheaper technologiesthe laplace approximation estimate covariance section 53 comes mindmay offer significantly inaccurate representa tions several possibilities speedups intelligent choice start point particular reason start samplers random start point wait gradient descent component hybrid mcmc find mode instead start sampler decent estimate mode describe relevant methods faster mixing rate generally better sampler mixes fewer samples one needs draw samples increasingly mimic iid samples isnt clear build truly fastmixing sampler best strategy appears use image data structure proposal distribution section 3 zhu et al 2000 proofs leads fastmixing sampler lower persample cost unlikely decent representation covariance available fewer 1000 samples means sample cheap obtain current possibilities include faster integrator hybrid mcmc method used symplectic rungekutta nystrom method sanzserna calvo 1994 effort choose fastest overall integra tor grouping variables allows effi cient gibbs sampler separating cameras points leads standard form sampler makes minuscule changes state sample reason illustrated fig 1 fitting gaussian sample using gaussian propose new state1 541 starting sfm sampler samplers state given u v show examples n 40 80 n 24 100 means domain sampler 23200 resp 22400 copies 640 resp 592 relations discrete continuous variables complex small errors sampler started random point burns relatively quickly large errors burn slow values u v depend strongly mif mhasa1inaposition corresponding signifi cant tracker error error strongly affect values u v effect slows convergence sampler incorrect values continuous parameters mean many data points lie long way values predicted model little distinction points correspond model points start sampler fair initial estimate mode obtain initial value mask sampling independent distribution bits tends deemphasize points distant corresponding points previous next frames particular jth bit 0 probability 1ij dij dic1j 2 c dicmj dicmc1j 2 c dij di1j 2 c dicmj dicm1j 2 since problem quantity data swamps number parameters model choice w fairly unimportant main issue choose value small enough large tracker errors masked almost certainly ua va maximise dij uiakvkaj miaj obtained sweep algorithm fixes u resp v solves linear system v resp u swaps variables sweeps continue convergence guaranteed compute affine transformation us uaa vs a1va draw sample full conditional bit mask given us vs obtain ms start state us vs ms 542 starting colour constancy sampler sampler converges started random sample prior slow unnecessarily efficient good guess edge positions follows choosing set edges maxima edge proposal distributions censored ensure hardcore model applies similarly start point light position follows choosing maximum likelihood position proposal distribution specular position known estimate illuminant colour follows finally patch obtain reflectance estimate average colour within patch illuminant colour yields start point sampler converges relatively quickly 6 discussionups downs sampling methods good samplers fast burn quickly mix well proven samplers good least theory obviously bad merely mysterious behaviour possible build samplers yield representations pass wide range sanity checks fairly fast probably best hoped near future 61 points favour using sampled representations several points favour using sampled representations strongest simple management uncertainty comes methods samples available managing information simple computing expectations marginalization useful activities particularly easy incorporating new information principle simple output properly built sampler excellent guide inferences drawn ambiguities dataset example fig 7 show uncertainty position single point space deter mined structure motion method result image noise independence assumptions required obtain information furthermore required use specialised methods camera motion degenerateif example camera translates within plane effect appear scatter plots vary widely along axis perpendicular plane main benefit results simple information integration building vision systems reasonable scale requires cue integration example happens colour reports region blue shape says fire engine contradiction resolved understanding reliability reports properlybuilt bayesian model incorporates available information particularly attractive natural likelihood prior models available eg examples sections 2 3 principle sampling work arbitrary posteriors joy sampling 131 another feature sampled methods handle complex spatial models main difficulty models domains complicated topolo gies example simple deal domain whichconsistsofmanycomponentsofdifferentdimen sion green 1995 means spatial model part posterior example section 3 model layout mondrian grid rectangles neither position number horizontal vertical edges grid known instead inferred data offers prospect unifying information coherence spatial layout model appearance performing segmentation explicit spatial models sampling methods standard approach performing inference using spatial models geyer 1999 moller 1999 62 problems samplers samplers principle generic practice building good sampler requires significant degree skill number samples required large vision problems typically consist large numbers discrete continuous variables posterior complicated function high dimensional space many important modes extremely large number samples may required support useful representation either samples mixture model simplified parametric model fit ted samples however well phrased vision problems expect see small number quite tight modes posterior suggesting relevant portion posterior could represented manageable numbers samples furthermore accurate representation tails less significant need reasonable description modes samplers currently relatively slow however possible build samplers fast enough useful solutions real vision problems obtained reasonable amounts time generally prospect understanding build better systems precedes understanding build faster systems sampled representations claim universal ity conceivable representation scheme appears rest presence samples example one might wish approximate posterior mixture model one either fit model set samples compute various integrals representing error good numerical integrators high dimensions based sampling methods one form 132 forsyth haddon ioffe another suggests unless problem persuaded take series manageable parametric forms deterministic algorithms computing fits available one stuck difficulties come along sampling methods vision problems often form well adapted sampling methods particular usually preponderance evidence meaning posterior large wellisolated peaks whose location estimated furthermore commonly case computer vision algorithms compute values variables given others known metropolishastings algorithm gives framework within algorithms integrated easily produce series hypotheses meaningful semantics samplers poorly adapted problems lead large domains essentially uniform prob ability might occur example mrf model may large number states essentially nearmaximal posterior probability small number labelflips away extremum difficulty sampler representation produces quite easy set examples require large numbers samples represent regions particularly dimension domain large fair case made problems properly reparametrised perhaps imposing parametric whatever strategy adopted addressing firstly large domains essentially uniform probability suggest problem parameters dont significant effect outcome secondly estimates mode extremely unstable thirdly estimator expectation problem must high variance samples trusted typically first samples must discarded allow sampler burn rest represent posterior k usual approach start different sequences different points confirm give comparable answers eg gelman rubin 1993 geweke 1992 roberts 1992 another approach prove proposal process rapid mixing properties extremely difficult eg jerrum sinclair 1996 rapid mixing desirable faster sampler mixes lower variance expectations estimated using samples geyer 1999 mechanism available many practical problems structure ones experimental work give checks behaviour sampler example work structure motion sampler able identify bad measurements gave stable reconstructions section 52 similarly work colour constancy resampling algorithm correctly reduced variance inferred colour patches informed patches colour section 52 63 reasons cheerful interesting vision problems wellbehaved enough make samplers quite practical tools firstly vision problems overwhelming quantity data compared number parameters stud ied result usual expect posterior might small number quite wellpeaked modes exploration domain sampler restricted small subsets secondly substantial body algorithms make good estimates position modes eg derivative filters estimating position edges factorisation estimating structure motion etc sampler started good state finally many vision problems display kind conditional independence property allows large problem decomposed samplingresampling problem eg section 3 ioffe forsyth 1999 acknowledgments research supported part grant adobe systems part nsf fellowship si part nsf digital library award nsf iis 9817353 hotel sequence appears courtesy modeling videotaping group robotics stitute carnegie mellon university thanks stuart russell pointing significance mcmc inference technique note 1 indebted andrew zisserman suggestion r structural image restoration deformable templates bayesian computation stochastic systems visual reconstruction bayesian colour constancy spatial processor model object colour perception bayes empirical bayes methods data analysis improved particle filter nonlinear problems theory practice bayesian image labeling simulated annealingan annotated bibliography modeling rendering architecture photographs hybrid geometry imagebased approach structure motion without correspondence sequential monte carlo methods practice hybrid monte carlo approximating integrals via monte carlo deterministic methods two images tell us third one 3d reconstruction urban scenes image sequences colour perspective random sample consensus paradigm model fitting application image analysis automated cartography novel algorithm colour constancy machine colour constancy good enough joy sampling 133 gamerman inference iterative simulation using multiple sequences bayesian data analysis stochastic relaxation markov random field image models application computer vision evaluating accuracy sampling based approaches calculation posterior moments likelihood inference spatial point processes strategies improving mcmc introducing markov chain monte carlo introduction markov chain monte carlo using simulated annealing solve routing location problems automatic 3d model building video sequences reversible jump markov chain monte carlo computation bayesian model determination mcmc image analysis tutorial pattern theory general pattern theory multiple view geometry automatic symbolic traffic scene analysis using belief networks finding people sampling markov chain monte carlo method approach approximate counting integration vehicle segmentation classification using deformable templates stochastic simulation algorithms dynamic probabilistic networks lightness retinex theory method computing sceneilluminant chromaticity specular highlights markov random field modeling computer vision computational model color constancy linear models surface illuminant spectra em algorithm extensions markov chain monte carlo spatial point pro cesses unified factorization algorithm points optimal approximations piecewise smooth functions associated variational problems probabilistic inference using markov chain monte carlo methods toward templatebased tolerancing bayesian viewpoint dynamic bayesian network approach figure tracking using learned dynamic models bayesian model comparison via jump diffusion paraperspective projective factorisation method recovering shape motion stochastic simulation pattern recognition neural networks convergence diagnostics gibbs sampler markov chain concepts related sampling al gorithms robust regression outlier detection numerical hamiltonian problems perceptual organization using bayesian networks automated design bayesian perceptual inference networks object localization bayesian correlation introduction general statespace markov chain theory shape motion image streams orthography factorization method development comparison robust methods estimating fundamental matrix robust computation parametrization multiple view relations complexity information factorization methods projective structure motion stochastic computation medial axis markov random fields integrating bottomuptop object recognition data driven markov chain monte carlo tr ctr chenyu wu ce liu heungyueng shum yingqing xu zhengyou zhang automatic eyeglasses removal face images ieee transactions pattern analysis machine intelligence v26 n3 p322336 march 2004 zhuowen tu songchun zhu parsing images regions curves curve groups international journal computer vision v69 n2 p223249 august 2006 jens keuchel christoph schnrr christian schellewald daniel cremers binary partitioning perceptual grouping restoration semidefinite programming ieee transactions pattern analysis machine intelligence v25 n11 p13641379 november david forsyth okan arikan leslie ikemoto james obrien deva ramanan computational studies human motion part 1 tracking motion synthesis foundations trends computer graphics vision v1 n2 p77254 july 2006