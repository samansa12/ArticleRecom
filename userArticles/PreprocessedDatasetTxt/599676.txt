learnability design output codes multiclass problems output coding general framework solving multiclass categorization problems previous research output codes focused building multiclass machines given predefined output codes paper discuss first time problem designing output codes multiclass problems design problem discrete codes used extensively previous works present mostly negative results introduce notion continuous codes cast design problem continuous codes constrained optimization problem describe three optimization problems corresponding three different norms code matrix interestingly l2 norm formalism results quadratic program whose dual depend length code special case formalism provides multiclass scheme building support vector machines solved efficiently give time space efficient algorithm solving quadratic program describe preliminary experiments synthetic data show algorithm often two orders magnitude faster standard quadratic programming packages conclude generalization properties algorithm b introduction many applied machine learning problems require assigning labels instances labels drawn finite set labels problem often referred multiclass categorization classification examples machine learning applications include multiclass categorization component include optical character recognition text classifica tion phoneme classification speech synthesis medical analysis well known binary classification learning algorithms extended handle multiclass problem see instance 5 19 20 general approach reduce multiclass problem multiple binary classification problem dietterich bakiri 9 described general approach based errorcorrecting codes termed errorcorrecting output coding ecoc short output cod ing output coding multiclass problems composed two stages training stage need construct multiple supposedly independent binary classifiers based different partition set labels two disjoint sets second stage classification part predictions binary classifiers combined extend prediction original label test instance experimental work shown output coding often greatly improve standard reductions binary problems 9 10 16 1 21 8 4 2 performance output coding also analyzed statistics learning theoretic contexts 12 15 22 2 previous work output coding concentrated problem solving multiclass problems using predefined output codes independently specific application class hypotheses used construct binary classifiers therefore predefining output code ignore complexity induced binary problems output codes used experiments typically confined specific family codes several family codes suggested tested far comparing class rest comparing pairs classes 12 2 random codes 9 21 2 exhaustive codes 9 2 linear error correcting codes 9 heuristics attempting modify code improve multiclass prediction accuracy suggested eg 1 however yield significant improvements furthermore lack formal justification paper concentrate problem designing good code given multiclass problem sec 3 study problem finding first column discrete code matrix given binary classifier show finding good first column done polynomial time con trast restrict hypotheses class choose binary classifiers problem finding good first column becomes difficult result underscores difficulty code design problem furthermore sec 4 discuss general design problem show given set binary classifiers problem finding good code matrix npcomplete motivated intractability results introduce sec 5 notion continuous codes cast design problem continuous codes constrained optimization problem discrete codes column code matrix divides set labels two subsets labeled positive negative sign entry code matrix determines subset association magnitude corresponds confidence association given formalism seek output code small empirical loss whose matrix norm small describe three optimization problems corresponding three different norms code matrix l 1 l 2 l 1 l 1 l 1 show code design problem solved linear programming lp interestingly l 2 norm formalism results quadratic program qp whose dual depend length code similar support vector machines dual program expressed terms innerproducts input instances hence employ kernelbased binary classifiers framework yields special case direct efficient method constructing multiclass support vector machine number variables dual quadratic problem product number samples number classes value becomes large even small datasets instance english letter recognition problem 1000 training examples would require 26000 variables case standard matrix representation dual quadratic problem would require 5 giga bytes mem ory therefore describe sec 61 memory efficient algorithm solving quadratic program code design algorithm reminiscent platts sequential minimal optimization smo 17 however unlike smo algorithm optimize round reduced subset variables corresponds single example informally algorithm reduces optimization problem sequence small problems size reduced problem equal number classes original multiclass problem reduced problem solved using standard qp technique however standard approaches would still require large amount memory number classes large straightforward solution also time consuming therefore develop algorithm provide analytic solution reduced problems efficient algorithm calculating solution run time algorithm polynomial memory requirements linear number classes conclude simulations results showing algorithm least two orders magnitude faster standard qp technique even small number classes set training examples instance x belongs domain x assume without loss generality label integer set kg multiclass classifier function maps instance x element work focus framework uses output codes build multiclass classifiers binary classifiers discrete output code matrix size k l f11g row correspond class 2 column defines partition two disjoint sets binary learning algorithms used construct classifiers one column set examples induced column set fed training data learning algorithm finds hypothesis h f11g reduction yields l different binary classifiers l denote vector predictions classifiers instance x denote rth row r given example x predict label row closest hx use general notion closeness define innerproduct function l r r higher value k hx confident r correct label x according classifiers h example closeness function ku v easy verify choice k equivalent picking row attains minimal hamming distance hx given classifier hx example x say hx misclassified example hx 6 let 1 predicate holds 0 otherwise goal therefore find classifier hx 1 small would like note passing paper mainly focus empirical loss minimization problem standard classification problems loss separate test set generalization error also theoretically bounded given appropriate assumptions using uniformconvergence theory 3 13 23 leave future research l small might one row attains maximal value according function k accommodate cases relax definition define classifier hx based code mapping fy j k hx r g case pick one labels hx uniformly random use expected error hx 1 context output codes multiclass mapping hx thus determined two parameters coding matrix set binary classifiers hx assume binary classifiers chosen hypothesis class h following natural learning problems arise given matrix find set h suffers small empirical loss b given set binary classifiers h find matrix small empirical loss c find matrix set h small empirical loss previous work focused mostly first problem paper mainly concentrate code design problem problem b finding good matrix summary notation given appendix 3 finding first column output code assume given single binary classifier h 1 x want find first single column matrix minimizes empirical loss h brevity let us denote first column describe efficient algorithm finds algorithms running time polynomial size label set sample size first note case second note sample divided 2k equivalence classes according labels classification h 1 x b fraction examples label r classification b according denote r1 b r let bgj number elements u equal b brevity often use denote value b let assume without loss generality elements u otherwise k equivalent random guessing hence size hx using eqs 2 4 rewrite eq 3 r r using eq 5 expand h ur r ur r ur r r r r ur r r particular choice w maximized minimized setting u attain highest values r r set rest w indices 1 done efficiently k log k time using sorting therefore best choice u found enumerating possible values choosing value w achieves maximal value eq 6 since takes operations calculate r r total number operations needed find optimal choice first column om log k proven following theorem theorem 1 let set training examples label integer set kg let h binary hypothesis class given hypothesis h 1 x 2 h first column output code minimizes empirical loss defined eq 1 found polynomial time conclude section use reduction sat demonstrate learning algorithm corresponding class hypotheses h 1 chosen restricted form resulting learning problem hard let formula variables x 2 f11g interpret instance space let 1g sample size labels taken 1g define learning algorithm l follows algorithms input binary labeled sample form fx algorithm returns hypothesis consistent sample sample otherwise algorithm returns constant hypothesis hx 1 hx 0 agrees majority sample choosing note learning algorithm nontrivial sense hypothesis returns empirical loss less 12 binary labeled sample show multiclass learning algorithm minimizes empirical loss first column u hypothesis h 1 x returned algorithm l used check whether formula sat isfiable need consider two cases true using definition eq 3 get conditions hold hx constant let number examples hypothesis classifies correctly using eq 3 obtain thus minimum achieved formula satisfiable fore learning algorithm h 1 x also used oracle satisfiability setting discussed section somewhat superficial results underscore difficulty prob lem next show problem finding good output code given relatively large set classifiers hx tractable would like note passing efficient algorithm finding single column might useful settings instance building trees directed acyclic graphs multiclass problems cf 18 leave future research 4 finding general discrete output code section prove given set l binary classifiers hx finding code matrix minimizes empirical loss npcomplete given sample set classifiers h let us denote evaluation h sample h predictions vector ith sample show even problem npcomplete clearly problem remains npc k 2 following notation previous sections output code matrix composed two rows 2 predicted class instance x g simplicity presentation proof assume code hypotheses values h set f0 1g instead f11g assumption change problem since linear transform two sets theorem 2 following decision problem npcomplete input natural number q labeled sample question exist matrix 2 f0 1g 2l classifier hx based output code makes q mistakes proof proof based reduction technique introduced hoffgen simon 14 since check polynomial time whether number classification errors given code matrix exceeds bound q problem clearly np show reduction vertex cover order prove problem nphard given undirected graph v e code structure graph follows sample composed two subsets size 2jej jv j respectively set edge encoded two examples h set first vector h elsewhere set second vector h elsewhere set label example 1 example encodes node v 2 set label example v 2 second class show exists vertex cover u v q nodes exists coding matrix induces q classification errors sample vertex cover ju j q show exists code q mistakes let u 2 f0 1g jv j characteristic function u u define output code matrix denotes componentwise logical operator since u cover h 2 therefore examples predicted label equals true label suffer 0 errors exam ples example h 2 corresponds node therefore examples misclassified recall label example v 2 analogously example corresponds v 62 u get examples correctly classified thus shown total number mistakes according code achieves q mistakes construct subset u v follows scan add u vertices v corresponding misclassified examples similarly misclassified example corresponding edge fv either v v j random add u since q misclassified examples size u q claim set u vertex cover graph g assume contradiction edge fv neither v v j belong set u therefore construction examples corresponding vertices v v j classified correctly get summing equations yields addition two examples corresponding edge classified correctly implying summing equations yields comparing eqs 7 8 get contradiction 5 continuous codes intractability results previous sections motivate relaxation output codes section describe natural relaxation classifiers output code matrix reals classifier hx constructed code matrix set binary classifiers hx matrix size k l r row corresponds class 2 analogously binary classifier h x 2 h mapping h defines partition two disjoint sets sign element tth column interpreted set 1 1 class r belongs magnitude jm rt j interpreted confidence associated partition sim ilarly interpret sign h x prediction set 1 1 label instance x belongs magnitude jh xj confidence prediction given instance x classifier hx predicts label maximizes confidence function k g since code reals assume without loss generality exactly one class attains maximum value according function k concentrate problem finding good continuous code given set binary classifiers h approach take cast code design problem constrained optimization problem borrowing idea soft margin 7 replace discrete 01 multiclass loss linear bound r formulation also motivated generalization analysis schapire et al 2 analysis give based margin examples margin closely related definition loss given eq 9 put another way correct label confidence value larger least one confidences rest labels otherwise suffer loss linearly proportional difference confidence correct label maximum among confidences labels bound empirical loss say sample classified correctly using set binary classifier h exists matrix loss equal zero denote thus matrix satisfies eq 10 would also satisfy following constraints view code collection vectors define norm norm concatenation vectors constituting motivated 24 2 seek matrix small norm satisfies eq 12 thus entire sample labeled correctly problem finding good matrix stated following optimization problem subject 8i r k hx p integer note constraints automatically satisfied changed following derivation nonseparable case general case matrix classifies examples correctly might exist therefore introduce slack variables 0 modify eq 10 r corresponding optimization problem subject constant 0 optimization problem soft constraints analogously define optimization problem hard constraints subject relation hard soft constraints formal properties beyond scope paper discussion relation problems see 24 51 design continuous codes using linear programming develop eq 14 cases deal first cases result linear programs simplicity presentation assume ku case objective function eq 14 become ir jm ir j introduce set auxiliary variables get standard linear programming setting subject obtain dual program see also app b define one variable constraint primal problem use ir first set constraints tr second set dual program ir subject 8i r tr case similar objective function eq introduce single new variable obtain primal problem subject following technique get dual program ir subject 8i r tr tr programs solved using standard linear program packages 52 design continuous codes using quadric programming discuss detail eq 14 case 2 convenience use square norm matrix instead norm therefore primal program becomes subject solve optimization problem finding saddle point lagrangian r ir ir subject 8i r ir 0 16 saddle point seeking minimum primal variables maximum dual ones find minimum primal variables require r r similarly r require ir0 z eq 19 implies optimum objective function achieved row matrix linear combination hx say example support pattern class r coefficient r ir hx eq 19 zero two settings example support pattern class r first case label example equal r ith example support pattern ir 1 second case label example different r ith pattern support pattern ir 0 loosely speaking since r ir 0 r variable ir viewed distribution labels example example affects solution eq 19 point distribution concentrating correct label thus questionable patterns contribute learning process develop lagrangian using dual variables substituting eqs 17 19 eq 16 using various algebraic manipulations obtain target function dual program r ir details omitted due lack space let 1 vector components zero except ith component equal one let 1 vector whose components one using notation rewrite dual program vector form subject 8r easy verify q strictly convex since constraints linear problem single optimal solution therefore qp methods used solve sec 6 describe memory efficient algorithm solving special qp problem simplify equations denote difference correct point distribution distribution obtained optimization problem eq 19 becomes since look value variables maximize objective function q optimum q omit constants write dual problem given eq 20 subject 8r 1 finally classifier hx written terms variable r hx hx r ir r ir hx support vector machines dual program classification algorithm depend inner products form hx hx therefore perform calculations high dimensional innerproduct space z using transformation l z thus replace innerproduct eq 22 eq 23 general innerproduct kernel k satisfies mercer conditions 24 general dual program therefore subject 8i classification rule hx becomes ir k general framework designing output codes using qp program described also provides special case new algorithm building multiclass support vectors machines assume instance space vector space r n define hx x thus l n primal program eq 15 becomes min subject note reduces primal program svm take would also like note special case reminiscent multiclass approach svms suggested weston watkins 25 approach compared confidence confidences labels kx mk 1 slack variables primal problem contrast framework confidence kx compared max r 6y kx r slack variables primal program table 1 summarize properties programs discussed shown table advantage using l 2 objective function number variables dual problem function k depend number columns l number columns affects evaluation innerproduct kernel k formalism given eq 14 also used construct code matrix incrementally column column outline incremental inductive approach ever would like note method applies kv u first step incremental al gorithm given single binary classifier h 1 x need construct first column rewrite eq 14 scalar form obtain subject 8i r h 1 0 given constant b fore rest columns assume inductively provided first l columns matrix found addition provided new binary classifier h l1 x next column need find new column indexed l 1 substitute new classifier matrix eq 13 get r constraints appearing eq 14 become r r redefine b ir hx straightforward verify definition b ir results equation form eq 27 thus apply algorithms designed batch case case l 1 l 1 construction decomposes single problem l subproblems fewer variables constraints however l 2 size program remains lose ability use kernels therefore concentrate batch case need find entire matrix 6 efficient algorithm qp problem quadratic program presented eq 24 solved using standard qp techniques shown table 1 dual program depends mk variables km together converting dual program eq 24 standard qp form requires storing manipulating matrix mk 2 elements clearly would prohibit applications nontrivial size introduce memory efficient algorithm solving quadratic optimization problem given eq 24 primal variables 2kl kl kl 0constraints constraints dual variables km 0constraints constraints table 1 summary sizes optimization problems different norms see appendix b definitions constraints linear programming first note constraints eq 24 divided algorithm describe works rounds round picks single set f 1 modifies optimize reduced optimization problem algorithm reminiscent platts smo algorithm 17 note however algorithm optimizes one example round two smo let us fix example index p write objective function terms variables p brevity let isolate p q brevity omit index p drop constants affect solution reduced optimization k variables k subject 1 although program solved using standard qp technique still requires large amount memory k large straightforward solution also time con suming furthermore problem constitutes core innerloop algorithm therefore develop algorithm describe efficient method solving eq 32 write q eq 32 using completion quadratic form since 0 program eq 32 becomes min subject sec 61 discuss analytic solution eq 33 sec 62 describe time efficient algorithm computing analytic solution 61 analytic solution algorithmic solution describe section simple implement efficient derivation quite complex describing analytic solution eq 33 would like give intuition method let us fix vector denote 1 first note feasible point since constraint 1 1 satisfied hence feasible point constraints tight second note differences bounds r variables r sum one let us induce uniform distribution components variance since expectation constrained given value optimal solution vector achieving smallest vari ance components attain similar values much possible inequality constraints fig 1 illustrate motivation picked show plots two different feasible values xaxis index r point yaxis designates values components norm plot right hand side plot smaller norm plot left hand side right hand side plot optimal solution sum lengths arrows plots since sets points feasible satisfy constraint 1 1 thus sum lengths arrows plots one exploit observation algorithm describe sequel therefore seek feasible vector whose components equal threshold given define vector whose rth component equal minimum r hence inequality constraints satisfied define r figure 1 illustration two feasible points reduced optimization problem xaxis index point yaxis denotes values bottom plot smaller variance hence achieves better value q denote using f equality constraint eq 33 becomes let us assume without loss generality components vector given descending order 1 done k log k time let 1 prove main theorem section need following lemma lemma 3 f piecewise linear slope r range r1 r proof let us develop f r f figure 2 illustration solution qp problem using inverse f 06 optimal value solution equation f 05 note r u u r also equality holds range thus r1 r function f form completes proof corollary 4 exists unique 0 1 proof eq 35 conclude f strictly increasing continuous range 1 therefore f inverse range using theorem every strictly increasing continuous function verse since f 1 hence range f interval 1d 1 interval 1 1 clearly contains thus 0 needed uniqueness 0 follows fact function f onetoone mapping onto 1 prove main theorem section theorem 5 let 0 unique solution f 0 optimum value optimization problem stated eq 33 theorem tells us optimum value eq 33 form defined eq 34 exactly one value equality constraint f holds plot f solution fig 1 shown fig 2 proof corollary 4 implies solution exists unique note also definition 0 vector 0 feasible point eq 33 prove 0 optimum eq 33 showing 6 assume contradiction vector kk 2 k 0 k 2 let 6 0 define g since 0 satisfy equality constraint eq 33 since feasible point also definition set combining two properties get r 0 r 2 37 start simpler case case differs 0 subset coordinates however coordinates components 0 equal 0 thus obtain zero variance constant vector whose components 0 therefore feasible vector achieve better variance mally since terms r 2 cancel r2i r2i definition 0 eq 34 get 0 r2i r2i r2i r2i use assumption equality obtain get contradiction since 6 turn prove complementary case r2i r 0 exists u 2 u 0 use eq 36 conclude exists also 2 v 0 let us assume without loss generality u analogously switching roles u v define 0 follows r otherwise vector 0 satisfies constraints eq 33 since 0 0 equal except u v components get initialize sort components 1 compute r eq 40 return figure 3 algorithm finding optimal solution reduced quadratic program eq 33 substituting values 0 u 0 v definition 0 obtain using definition 0 u first term bottom equation negative since u 0 v 0 also u 2 hence 0 u second term also negative thus get contradiction 62 efficient algorithm computing analytic solution optimization problem eq 33 solved using standard qp methods interior point methods particular 11 methods computation time section give algorithm solving optimization problem ok log time solving equation assume components vector given descending order 1 denote 1 algorithm searches interval r1 r contains 0 use simple algebraic manipulations derive search scheme 0 convenience define potential function obtain choose f g feasible point eq 24 iterate choose example p eqs 29 30 compute fig 3 ap eq 33 output final hypothesis eq 25 figure 4 skeleton algorithm finding classifier based output code solving quadratic program defined eq 24 also note r g r1 g recall function f linear interval r f r1 solve equation f 1 1 first find r r 0 r implies using eq 38 equation f 1 r using linearity f obtain r therefore r complete algorithm described fig 3 since takes ok log time sort vector another ok time loop search total run time ok log k finally ready give algorithm solving learning problem described eq 24 since output code constructed supporting patterns term algorithm spoc support pattern output coding spoc algorithm described fig 4 also developed methods choosing example p modify round stopping criterion entire optimization al gorithm due lack space omit details appear full paper performed preliminary experiments synthetic data order check actual performance algorithm tested special case corresponding multiclass svm setting x code matrices test0 50 100 150 200 250 300 training examples log10run time qp figure 5 run time comparison two algorithms code design using quadratic programming matlabs standard qp package proposed algorithm denoted spoc note used logarithmic scale runtime axis columns varied size training set size examples generated using uniform distribution 1 domain partitioned four quarters equal quarter associated different label sample size tested ran algorithm three times run used different randomly generated training set compared standard quadratic optimization routine available matlab algorithm also implemented mat lab average running time results shown fig 5 note used logscale runtime axis results show efficient algorithm two orders magnitude faster standard qp package 7 conclusions future research paper investigated problem designing output codes solving multiclass problems first discussed discrete codes showed problem intractable general find first column code matrix polynomial time question whether algorithm generalized l 2 columns running time o2 l less remains open another closely related question whether find efficiently next column given previous columns also left open future research usage algorithm finding first column subroutine constructing codes based trees directed acyclic graphs 18 tool incremental column col umn construction output codes motivated intractability results discrete codes introduced notion continuous output codes described three optimization problems finding good continuous codes given set binary classifiers discussed detail efficient algorithm one three problems based quadratic programming special case framework also provides new efficient algorithm multiclass support vector machines importance efficient algorithm might prove crucial large classification problems many classes kanji character recognition also devised efficient implementation algorithm implementation details algorithm convergence generalization properties experimental results omitted due lack space presented elsewhere finally important question tackled barely paper problem interleaving code design problem learning binary classifiers viable direction domain combining algorithm continuous codes support vector machine algorithm acknowledgement would like thank rob schapire numerous helpful discussions vladimir vapnik encouragement support line research nir friedman ran bachrach useful comments suggestions r cloud classification using errorcorrecting output codes reducing multiclass binary unifying approach margin classifiers sample complexity pattern classification neural networks size weights important size network linear programming solving multiclass learning problems via errorcorrecting output codes machine learning bias practical methods optimization classification pairwise coupling decision theoretic generalizations pac model neural net learning applications robust trainability single neurons error coding method pict fast training support vector machines using sequential minimal optimization large margin dags multiclass classification learning internal representations error propagation using output codes boost multiclass learning problems improved boosting algorithms using confidencerated predictions estimation dependences based empirical data statistical learning theory support vector machines multiclass pattern recognition tr ctr eibe frank stefan kramer ensembles nested dichotomies multiclass problems proceedings twentyfirst international conference machine learning p39 july 0408 2004 banff alberta canada olivier lzoray hubert cardot comparing combination rules pairwise neural networks classifiers neural processing letters v27 n1 p4356 february 2008 pawalai kraipeerapun chun che fung kok wai wong multiclass classification using neural networks interval neutrosophic sets proceedings 5th wseas international conference computational intelligence manmachine systems cybernetics p123128 november 2022 2006 venice italy rong jin jian zhang multiclass learning smoothed boosting machine learning v67 n3 p207227 june 2007 libin shen aravind k joshi ranking reranking perceptron machine learning v60 n13 p7396 september 2005 ryan rifkin aldebaro klautau defense onevsall classification journal machine learning research 5 p101141 1212004 ana carolina lorena andr c p l f de carvalho protein cellular localization prediction support vector machines decision trees computers biology medicine v37 n2 p115125 february 2007 b kotsiantis zaharakis p e pintelas machine learning review classification combining techniques artificial intelligence review v26 n3 p159190 november 2006