implementation interprocedural bounded regular section analysis regular section analysis summarizes interprocedural side effects subarrays form useful dependence analysis avoiding complexity prior solutions shown practical addition production compiler optimizing compilers produce efficient code even presence highlevel language constructs however current programming support systems significantly lacking ability analyze procedure calls deficiency complicates parallel programming loops withcalls significant source parallelism performance regular section analysis compared two benchmarks linpack library linear algebra subroutines rice compiler evaluation program suite riceps set complete application codes variety scientific disciplines experimental results demonstrate regular section analysis effective means discovering parallelism given programs written appropriately modular programming style b introduction major goal compiler optimization research generate code efficient enough encourage use highlevel language constructs words good programming practice research supported national science foundation grant ccr 8809615 ibm corporation intel scientific computers rewarded fast execution time use subprograms prime example good programming practice requires compiler support efficiency unfortunately calls subprograms inhibit optimization programming support systems especially designed support parallel programming fortran absence better information compilers must assume two calls read write memory locations making parallel execution nondeterministic limitation particularly discourages calls loops compilers look parallelism traditional interprocedural analysis help cases consider following loop 100 continue source modifies locations ith column parallel execution loop deterministic classical interprocedural analysis discovers variables used defined side effects procedure calls must determine subarrays accessed order safely exploit parallelism earlier paper callahan kennedy proposed method called regular section analysis tracking interprocedural sideeffects regular sections describe side effects common substructures arrays elements rows columns diagonals 1 2 paper describes implementation regular section analysis rice parallel fortran converter pfc 3 automatic parallelization system also computes dependences parascope programming environment4 overriding concern implementation efficient enough incorporated practical compilation system algorithm 1 summarizes steps analysis whiich integrated threephase interprocedural analysis optimization structure pfc 5 6 regular section analysis added less 8000 lines pfc roughly 150000line pli program runs ibm vmcms remainder paper organized follows section 2 compares various methods representing side effects arrays section 3 gives additional detail exact variety bounded regular sections implemented sections 4 5 describe construction local sections propagation respectively section 6 examines performance regular section analysis two benchmarks linpack library linear algebra subroutines rice compiler evaluation local analysis procedure array formal parameter global static save section describing shape reference build ranges subscripts merge resulting section summary mod use section save summary sections call site array actual parameter save section describing passed locations scalar actual parameter global save range passed value interprocedural propagation solve interprocedural problems call graph construction classical mod use summary constant propagation mark section subscripts scalars invalidated modifications iterating call sites translate summary sections call context merge translated sections callers summary dependence analysis procedure call site summary section simulate loop running elements section test dependences banerjees gcd algorithm 1 overview regular section analysis program suite riceps set complete application codes variety scientific disciplines sections 7 8 suggest areas future research give conclusions array side effects simple way make dependence testing precise around call site perform inline expansion replacing called procedure body 7 precisely represents effects procedure sequence ordinary statements readily understood existing dependence analyzers however even whole program becomes larger loop nest contained call may grow dramatically causing time space explosion due nonlinearity array dependence analysis 8 gain benefits inline expansion without drawbacks must find another representation effects called procedure dependence analysis interested memory locations modified used procedure given call procedure p statement array global variable parameter wish compute ffl set locations may modified via p called 1 ffl set u locations may used via p called 1 need comparable sets simple statements well test dependence intersecting sets example exists true dependence statement 1 following statement based array several representations proposed representing interprocedural array access sets contrived example figure 1 shows different patterns represent precisely evaluating methods involves examining complexity precision classical summary triolet regular sections bounds bounds strides dadsimple section figure 1 summarizing references a1 2 a4 8 a10 6 ffl representing sets u merging descriptors summarize multiple accesses call meet operation descriptors may viewed forming lattice ffl testing two descriptors intersection dependence testing translating descriptors call sites especially array reshapes handling recursion turns issue iterative techniques guarantee convergence fixed point solution using cousots technique widening operators 9 10 li yew proposed preparatory analysis recursive programs guarantees termination three iterations 11 12 either methods may adapted regular sections 21 true summaries true summary methods use descriptors whose size largely independent number references summarized may make descriptors operations complicated limits expense translating descriptors interprocedural propagation intersecting dependence analysis classical methods classical methods interprocedural summary dataflow analysis compute mod use sets indicating parameters global variables may modified used procedure 13 14 15 summary information costs two bits per variable meet intersection may implemented using singlebit bitvector logical operations also exist algorithms compute complete solutions number meets linear number procedures call sites program even recursion permitted 16 unfortunately experiences pfc ptool indicate summary information coarse dependence testing effective detection parallelism 1 problem access sets representable method whole array none array see figure 1 coarse information limits detection data decomposition important source parallelism different iterations loop work distinct subsections given array triolet regions triolet irigoin feautrier proposed calculate linear inequalities bounding set array locations affected procedure call 17 18 representation intersection operation precise convex regions patterns array accesses nonunit stride nonconvex results meet operations given convex approximations operations regions expensive meet operation requires finding convex hull combined set inequalities intersection uses potentially exponential linear inequality solver 19 succession meet operations also produce complicated regions potentially many inequalities number primitive accesses merged together translation calls sites precise formal parameter array called procedure maps subarray shape caller otherwise whole actual parameter array assumed accessed call region method ranks high precision expensive complex representation 22 reference lists proposed methods summarize represent reference separately descriptors lists references meet operation list concatenation possibly check duplicates translation intersection repeated application corresponding operations simple references however two significant disadvantages ffl translation descriptor requires time proportional number references ffl intersection descriptors requires time quadratic number references reference list methods simple precise asymptotically expensive inline expansion linearization burke cytron proposed representing multidimensional array reference linearizing subscript expressions onedimensional address expression method also retains bounds information loop induction variables occurring expressions 20 describe two ways implementing meet operation one involves merely keeping list individual address expressions constructs composite expression polynomial loop induction variables disadvantages first method described second method appears complicated yet rigorously described linearization pure form illsuited summarization might useful extension true summary technique ability handle arbitrary reshapes atom images li yew extended parafrase compute sets atom images describing side effects procedures 21 11 like original version regular sections described callahans thesis 2 record subscript expressions linear loop induction variables along bounds induction variables reference linear subscript expressions triangular iteration space precisely represented keep separate atom image reference expense translating intersecting lists atom images high price pay precision converting atom images summary method would produce something similar regular sections described 23 summary sections precise methods described expensive allow arbitrarily large representations procedures access sets extra information may useful practice simple array access patterns probably common others avoid expensive intersection translation operations descriptor size independent number references summa rized operations descriptors linear worst quadratic rank array researchers rice defined several variants regular sections represent common access patterns satisfying constraints 2 1 22 23 original regular sections callahans thesis proposed two regular section frameworks first resembling li yews atom images dismissed due difficulty devising efficient standardization meet operations 2 restricted regular sections second framework restricted regular sections 2 1 limited access patterns subscript ffl procedureinvariant expression constants procedure inputs ffl unknown assumed vary entire range dimension ffl unknown diagonal one subscripts restricted sections efficient descriptors size linear number subscripts meet operation quadratic diagonals intersection operation linear however lose much precision omitting bounds information originally thought limitations necessary efficient handling recursive programs li yew adapted iterative techniques work general descriptors 12 bounded regular sections anticipating restricted regular sections would precise enough effective parallelization callahan kennedy proposed implementation regular sections bounds project subject paper regular sections implemented include bounds stride information omit diagonal constraints resulting analysis therefore less precise representation convex regions triolet regions data access descriptors described however first interprocedural summary implementation stride information provides increased precision nonconvex regions size bounded regular section descriptors time required meet operation linear number subscripts intersection implemented using standard dependence tests also take time proportional number subscripts 1 data access descriptors concurrently implementation balasundaram kennedy developed data access descriptors dads general technique describing data access 22 23 24 dads represent information shapes array accesses traversal comparison interested shapes simple section part dad represents convex region similar triolet et al except boundaries constrained parallel one coordinate axis 45 ffi angle two axes stride information represented another part dad data access descriptors probably precise summary method implemented reasonable efficiency represent likely rectangular diagonal trian gular trapezoidal accesses size time required meet intersection analysis ignores greatest common divisor computation used merging intersecting sections strides take time proportional values strides complexity quadratic number subscripts reasonable given arrays subscripts bounded sections implemented less expensive less precise dads implementation extended compute dads additional precision proves useful bounded sections ranges bounded regular sections comprise set rectangular subarrays written using triplet notation proposed fortran 90 standard 25 represent sparse regions stripes grids dense regions columns rows blocks n12 expressions ranges size 2 ranges size 3 finite ranges unknown figure 2 lattice regular section subscripts 31 representation descriptors bounded regular sections vectors elements subscript lattice figure 2 lattice elements include ffl invariant expressions containing constants symbols representing values parameters global variables entry procedure ffl ranges giving invariant expressions lower bound upper bound stride variant subscript ffl indicating knowledge subscript value ranges may constructed sequence meet operations common case read directly bounds loop induction variable used subscript since constraints subscripts maintained merging two regular sections array rank requires independent invocations subscript meet operation test intersection two sections single invocation standard ddimensional dependence tests translation formal parameter section one actual parameter also od operation larger two ranks 32 operations ranges ranges typically built represent values loop induction variables following loop enddo represent value l l u often referred lower upper bound respectively roles reversed negative produce standard lower toupper bound form know l u 1 operation described detail algorithm 2 standardization may cause loss information therefore postpone standardization required operation merging two sections begin diff constant signdiff 6 signs return empty range direction absdiff mod abss else diff constant direction signdiff else constant direction signs else return select direction perfect returnu select end algorithm 2 standardizing range lowerboundfirst form expressions ranges converted ranges example 2i1 loop represented 2 l invariant expressions accurately added multiplied range algorithm 3 constructs approximations sums ranges ranges merged finding lowest lower bound highest upper bound correcting stride expression merged range another expression treating range lower bound equal upper bound algorithm 4 thus computes result interesting subscript expressions containing references scalar parameters global variables represent symbolic expressions global value numbers may tested equality standardization merge operations procedure construct symbolic subscript expressions accumulate initial regular sections knowledge interprocedural effects precision analysis depends function build rangee begin e leaf expression constant formal global value returne subexpression e replace build ranges select form e l 0 u 0 returnl 0 l l otherwise return select end algorithm 3 moving ranges top level expression recording questions side effects answering results interprocedural analyses available 41 symbolic analysis constructing regular sections requires calculation symbolic expressions variables used subscripts many published algorithms performing symbolic analysis global value numbering 26 27 28 preliminary transformations complexity make difficult integrate pfc implementation builds global value numbers help pfcs existing dataflow analysis machinery leaf value numbers constants global parameter values available procedure entry build value numbers expressions recursively obtaining value numbers subexpressions reaching definitions value numbers reaching reference along different defuse edges merged either merging occurrence unknown operator creates unknown value whole expression lowered induction variables recognized defining loop headers replaced inductive range auxiliary induction variables currently identified example consider following code fragment function mergea b begin range let l u else let l u b range let l b else let l b abs return returns l else else returnl end algorithm 4 merging expressions ranges dimension enddo return end dataflow analysis constructs defuse edges subroutine entry uses n loop use therefore simple compute subscript regular section 1 converted range names n actually replaced formal parameter indices note expressions nonlinear local analysis may become linear later phases especially constant propagation 42 avoiding compilation dependences construct accurate value numbers require knowledge effects call sites scalar variables however using interprocedural analysis determine effects costly programming support system using interprocedural analysis must examine procedure least twice 2 gathering information propagated procedures using results propagation dependence analysis andor transformations precomputing local information construct interprocedural propagation phase iterates call graph without additional direct examination procedure achieve minimal number passes interprocedural analyses must gather local information one pass without benefit others interprocedural solutions however build precise local regular sections need information side effects calls scalars used subscripts following code fragment must assume modified unknown value unless proven otherwise dimension return end achieve precision without adding separate local analysis phase regular sections build regular section subscripts side effects occur annotating subscript expression hazards side effects would invalidate thus record modified sole parameter clobber hazard interprocedural phase producing classical scalar side effect solution propagating regular sections check see clobber may change change s1s array side effect similar technique proven successful interprocedural constant propagation pfc 31 6 hazards must recorded scalar expression saved use regular section analysis scalar actual parameters globals call sites well array subscripts merge two expressions ranges take union hazard sets 43 building summary regular sections machinery place use mod regular sections local effects procedure constructed easily one pass procedure examine reference 2 strictly true system computing summary information use mod context information alias make one pass pfc ir n parascope systems perform summary context analysis well constant propagation therefore require least two passes 30 5 6 formal parameter global static array symbolic analyzer provides value numbers subscripts demand resulting vector regular section section individual reference constructed immediately merged appropriate cumulative sections discarded 5 interprocedural propagation regular sections formal parameters translated sections actual parameters traverse edges call graph translated sections merged summary regular sections caller requiring another translation propagation step changes summary extend implementation recursive programs terminate must bound number times change occurs 51 translation call context analyzing pascal arrays mapping referenced section formal parameter array one corresponding actual parameter would simple would need replace formal parameters subscript values formal section corresponding actual parameter values copy resulting subscript values new section however fortran provides guarantee formal parameter arrays shape actual parameters even arrays common blocks declared shape every procedure therefore describe effects called procedure caller must translate referenced sections according way arrays reshaped easiest translation method would linearize subscripts referenced section formal parameter adding offset passed location actual parameter 20 resulting section would give referenced locations actual onedimensional array however subscripts original section ranges nonlinear expressions linearization contaminates subscripts greatly reducing precision dependence anal ysis reason forego linearization translate significantly reshaped dimensions algorithm 5 shows one method translating summary section formal parameter f function translatebounds f ref f bounds pass begin ref consistent ref else rankf consistent ref else ref else replace scalar formal parameters bounds f ref f corresponding actual parameters bounds lobounds f pass lobounds f pass consistent ref else strideref lobounds delinearization possible fits bounds assume fit ref else ref end end algorithm 5 translating summary section section corresponding actual parameter translation proceeds left right dimensions precise dimension encountered formal actual parameter inconsistent different sizes nonzero offset first inconsistent dimension also translated precisely last dimension f referenced section subscript values fit bounds delinearization implemented may used recognize reference f column stride column size corresponds row reference 52 treatment recursion current implementation handles nonrecursive fortran therefore sufficient proceed reverse invocation order call graph translating sections leaf procedures callers final summary regular sections built order incomplete regular sections need never translated call site however proposed fortran 90 standard allows recursion 25 plan extension reimplementation handle unfortunately straightforward iterative approach propagation regular sections terminate since lattice unbounded depth li yew 11 cooper kennedy 16 describe approaches propagating subarrays efficient regardless depth lattice however may convenient implement simple iterative technique simulating boundeddepth lattice maintain counter summary regular section array procedure limit number times allow section become larger lower lattice going best way keeping one small counter eg two bits per subscript variant subscripts go quickly leaving precise subscripts unaffected limit subscript lowered subscript lattice k times array rank effective lattice depth kd 1 since summary regular section lowered okd times associated call site affected okdv times time involving od merge v number referenced global parameter variables worst case require okd 2 subscript merge translation operations e number edges call graph technique allows us use lattice bounds information keeping time complexity comparable obtained restricted regular section lattice 6 experimental results precision efficiency utility regular section analysis must demonstrated experiments real programs current candidates real programs linpack library linear algebra subroutines 32 rice compiler evaluation program suite perfect club benchmarks 33 ran programs regular section analysis dependence analysis pfc examined resulting dependence graphs hand parascope editor interactive dependence browser program transformer 4 linpack analysis linpack provides basis comparison methods analyzing interprocedural array side effects li yew 21 triolet 18 found several parallel calls linpack using implementations university illinois translator parafrase linpack proves useful numerical codes written modular programming style parallel calls detected riceps rice compiler evaluation program suite collection 10 complete applications codes broad range scientific disciplines colleagues rice already run several experiments riceps porterfield modeled cache performance using adapted version pfc 34 goff kennedy tseng studied performance dependence tests riceps benchmarks 35 riceps riceps candidate codes also examined study utility inline expansion procedure calls 8 six programs studied two riceps codes linpackd track four codes inlining study perfect club benchmarks suite originally collected benchmarking performance supercomputers complete applications hope test performance implementation programs delay receiving prevented us obtaining preliminary results paper 61 precision precision regular sections correspondence true access sets largely function programming style analyzed linpack written style uses many calls blas basic linear algebra subroutines whose true access sets precisely regular sections determine true access sets subroutines riceps six programs analyzed dogleg linpackd actually call linpack exhibited linpack coding style exist regular sections precisely describe effects blas local analysis unable construct complicated control flow changes blas eliminate unrolled loops conditional computation values used subscript expressions implementation able build minimal regular sections precisely represented true access sets modified dscal example looks follows double precision da dx n le enddo return end obtaining precise symbolic information problem methods describing array side effects triolet made similar changes blas li yew avoided first performing interprocedural constant propagation fundamental nature problem indicates desirability clearer fortran programming style sophisticated handling control flow described section 7 62 efficiency measured total time taken pfc analyze six riceps programs 3 parsing local analysis interprocedural propagation dependence analysis included execution times table 1 compares analysis time required using classical interprocedural summary analysis alone ip using summary analysis regular section analysis combined ip rs 4 program ip ip name lines procs rs change efie 1254 euler 1113 13 117 138 15 vortex 540 19 track 1711 34 191 225 15 dogleg 4199 48 272 377 28 linpackd 28 44 36 total 9172 142 882 1103 25 table 1 analysis times seconds pfc ibm 3081d 3 able run perfect programs pfc yet obtained reliable timings recentlyupgraded ibm system rice 4 present times dependence analysis interprocedural information performs less analysis loops call sites discounting advantage time taken classical summary analysis seems less 10 percent time timeconsuming part added code local symbolic analysis subscript values includes invocation dataflow analysis symbolic analysis would improve practicality entire method overall additional analysis time comparable required analyze programs heuristicallydetermined inline expansion cooper hall torczons study 8 seen published execution times array side effect analyses implemented parafrase triolet li yew except li yew state method runs 26 times faster triolets 21 experiments run linpack would particularly interesting know methods would perform complete applications 63 utility chose three measures utility ffl reduced numbers dependences dependent references ffl increased numbers calls parallel loops reduced parallel execution time reduced dependence table 2 compares dependence graphs produced using classical interprocedural summary analysis alone ip summary analysis plus regular section analysis array dep calls loops dependences loop carried loop independent source ip rs efie 12338 12338 177 177 81 81 euler vortex 1966 1966 220 220 73 73 track 4737 4725 025 68 67 15 27 26 37 dogleg linpackd 496 399 196 191 116 393 67 45 328 riceps 23213 22921 125 952 818 141 358 314 123 linpack 12336 11035 105 3071 2064 328 1348 1054 218 table 2 effects regular section analysis dependences rs 5 linpack analyzed without interprocedural constant propagation since library routines may called varying array sizes first set three columns gives sizes dependence graphs produced pfc counting true anti output dependence edges scalar array references loops including references call sites sets columns count dependences incident array references call sites loops separate counts loopcarried loopindependent dependences preliminary results eight 13 perfect benchmarks indicate reduction 06 percent total size dependence graphs 6 parallelized calls table 3 examines number calls linpack parallelized summary interprocedural analysis alone ip li yews analysis 21 regular section analysis rs triolets results parafrase resembled li yews 17 call sites parallelized parascope based pfcs dependence graph transformations necessary eight parallel call sites detected summary interprocedural analysis alone apparent parascope exploiting parallelism requires variant statement splitting yet supported starred entries indicate parallel calls precisely summarized regular section analysis detected parallel due deficiency pfcs symbolic dependence test triangular loops one call qrdc mistakenly parallelized parafrase 36 results indicate least linpack benefit generality triolets li yews methods regular section analysis obtains exactly precision different number loops parallelized differences dependence analysis transformations improved execution time two calls riceps programs parallelized one dogleg one linpackd major computational loops linpackds dgefa doglegs 5 dependence graphs resulting interprocedural analysis comparable since calls parallelized dependences collapsed conserve space 6 sections yet propagated arrays common blocks deficiency probably resulted dependences larger programs routine calls parallel calls name loops ip liyew rs deltagbco deltageco deltapbco deltapoco deltappco deltatrco deltagedi deltapodi deltaqrdc 9 5 4 deltasidi deltasifa deltasvdc deltatrdi table 3 parallelization linpack dqrdc 7 running linpackd 19 processors one call parallelized enough speed execution factor five sequential execution sequent symmetry rice experiments improvements parallel execution time await acquisition fortran codes written appropriate style 7 future work experiments required fully evaluate performance regular section analysis complete applications find new areas improvement based studies conducted far extensions provide better handling conditionals flowsensitive side effects seem promising 71 conditional symbolic analysis consider following example derived blas double precision da dx enddo return end two computations initial value ix correspond different ranges subscript turns represented 1 merge operation produce precise result requires understanding control conditions expressions computed 7 inlining study rice none commercial compilers able detect parallel call dogleg even inlining presumably due complicated control flow 8 72 killed regular sections already found programs scalgam euler ability recognize localize temporary arrays would cut number dependences dramatically allowing calls parallelized could recognize interprocedural temporary arrays determining entire array guaranteed modified used procedure flowsensitive problem therefore expensive solve generality even limited implementation able catch initialization many temporaries subscript lattice killed sections one used use mod sections however since kill analysis must produce underestimates affected region order conservative lattice needs inverted addition approach requires intraprocedural dependence analysis capable using array kill information described rosene 37 gross steenkiste 38 8 conclusion regular section analysis practical addition production compiler local analysis interprocedural propagation integrated interprocedural techniques required changes dependence analysis trivialthe ones needed support fortran 90 sections experiments demonstrate regular section analysis effective means discovering parallelism given programs written appropriately modular programming style style benefit advanced analysis ways example keeping procedures small simplifying internal control flow techniques well programs written style minimizes use procedure calls compensate lack interprocedural analysis compilers compilers must reward modular programming style fast execution time take hold among computationintensive users supercomputers long run make programs easier writers automatic analyzers understand acknowledgements would like thank colleagues pfc parascope projects made research possible thank david callahan contributions regular section analysis kathryn c kinley mary hall reviewers critiques paper r analysis interprocedural side effects parallel programming environment global approach detection parallelism pfc program convert fortran parallel form interactive parallel programming using parascope editor implementation interprocedural analysis vectorizing fortran compiler interprocedural constant propagation catalogue optimizing transformations experiment inline substitution abstract interpretation unified lattice model static analysis programs construction approximation fixpoints semantic foundations program analysis interprocedural analysis program restructuring parallel pro grams program parallelization interprocedural analysis method determining side effects procedure calls interprocedural data flow analysis algorithm efficient computation flow insensitive interprocedural summary information interprocedural sideeffect analysis linear time direct parallelization call statements interprocedural analysis program restructuring parafrase direct parallelization call statements review interprocedural dependence analysis parallelization efficient interprocedural analysis program parallelization structuring interactive parallelization numerical scientific programs technique summarizing data access use parallelism enhancing transformations mechanism keeping useful internal information parallel programming tools data access descriptor x3j3 subcommittee ansi affine relationships among variables program symbolic program analysis almostlinear time global value numbers redundant com putations compiler analysis value ranges variables impact interprocedural analysis optimization ir n programming environment compilation dependences ambitious optimizing compiler philadelphia siam publications supercomputer performance evaluation perfect benchmarks software methods improvement cache performance supercomputer applications practical dependence testing private communication incremental dependence analysis structured dataflow analysis arrays use optimizing compiler tr impact interprocedural analysis optimization rsupnsup programming environment interprocedural constant propagation interprocedural dependence analysis parallelization direct parallelization call statements global approach detection parallelism analysis interprocedural side effects parallel programming environment interprocedural sideeffect analysis linear time efficient interprocedural analysis program parallelization restructuring global value numbers redundant computations technique summarizing data access use parallelism enhancing transformations mechanism keeping useful internal information parallel programming tools data access descriptor structured dataflow analysis arrays use optimizing complier practical dependence testing supercomputer performance evaluation perfect benchmarks efficient computation flow insensitive interprocedural summary information interprocedural data flow analysis algorithm abstract interpretation interactive parallel programming using parascope editor method determining side effects procedure calls compilation dependences ambitious optimizing compiler interprocedural recompilation interactive parallelization numerical scientific programs software methods improvement cache performance supercomputer applications incremental dependence analysis ctr peiyi tang exact side effects interprocedural dependence analysis proceedings 7th international conference supercomputing p137146 july 1923 1993 tokyo japan donald g morris david k lowenthal accurate data redistribution cost estimation software distributed shared memory systems acm sigplan notices v36 n7 p6271 july 2001 jimnez j llabera fernndez e morancho general algorithm tiling register level proceedings 12th international conference supercomputing p133140 july 1998 melbourne australia carr k kennedy compiler blockability numerical algorithms proceedings 1992 acmieee conference supercomputing p114124 november 1620 1992 minneapolis minnesota united states brent weatherly david k lowenthal mario nakazawa franklin lowenthal dynmpi supporting mpi non dedicated clusters proceedings acmieee conference supercomputing p5 november 1521 linda burton william hatchett mari hobkirk charles powell using high performance gis software visualize data handson software demonstration proceedings 1998 acmieee conference supercomputing cdrom p114 november 0713 1998 san jose ca helen parke alisa chapman proposal preservice student technology competence proceedings 1998 acmieee conference supercomputing cdrom p19 november 0713 1998 san jose ca compilation techniques blockcyclic distributions proceedings 8th international conference supercomputing p392403 july 1115 1994 manchester england kathryn mckinley evaluating automatic parallelization efficient execution sharedmemory multiprocessors proceedings 8th international conference supercomputing p5463 july 1115 1994 manchester england umit rencuzogullari sandhya dwardadas dynamic adaptation available resources parallel computing autonomous network workstations acm sigplan notices v36 n7 p7281 july 2001 craig chase kay crowley joel saltz anthony reeves compiler runtime support irregularly coupled regular meshes proceedings 6th international conference supercomputing p438446 july 1924 1992 washington c united states r veldema r f h hofman r f bhoedjang c j h jacobs h e bal sourcelevel global optimizations finegrain distributed shared memory systems acm sigplan notices v36 n7 p8392 july 2001 g agrawal sussman j saltz compiler runtime support structured block structured applications proceedings 1993 acmieee conference supercomputing p578587 december 1993 portland oregon united states honghui lu alan l cox sandhya dwarkadas ramakrishnan rajamony willy zwaenepoel compiler software distributed shared memory support irregular applications acm sigplan notices v32 n7 p4856 july 1997 tor e jeremiassen susan j eggers static analysis barrier synchronization explicitly parallel programs proceedings ifip wg103 working conference parallel architectures compilation techniques p171180 august 2426 1994 mary w hall ken kennedy efficient call graph analysis acm letters programming languages systems loplas v1 n3 p227242 sept 1992 jae bum lee chu shik jhon reducing coherence overhead barrier synchronization software dsms proceedings 1998 acmieee conference supercomputing cdrom p118 november 0713 1998 san jose ca keith cooper ken kennedy interprocedural sideeffect analysis linear time acm sigplan notices v39 n4 april 2004 sotiris ioannidis umit rencuzogullari robert stets sandhya dwarkadas craulcolon compiler runtime integration adaptation load1this work supported part nsf grants cda9401142 ccr9702466 ccr9705594semi external research grant compaq scientific programming v7 n34 p261273 august 1999 jay p hoeflinger yunheung paek kwang yi unified interprocedural parallelism detection international journal parallel programming v29 n2 p185215 april 2001 radu rugina martin rinard automatic parallelization divide conquer algorithms acm sigplan notices v34 n8 p7283 aug 1999 jaydeep marathe frank mueller tushar mohan bronis r de supinski sally mckee andy yoo metric tracking inefficiencies memory hierarchy via binary rewriting proceedings international symposium code generation optimization feedbackdirected runtime optimization march 2326 2003 san francisco california sandhya dwarkadas alan l cox willy zwaenepoel integrated compiletimeruntime software distributed shared memory system acm sigplan notices v31 n9 p186197 sept 1996 junjie gu zhiyuan li gyungho lee experience efficient array data flow analysis array privatization acm sigplan notices v32 n7 p157167 july 1997 mary h hall saman p amarasinghe brian r murphy shihwei liao monica lam detecting coarsegrain parallelism using interprocedural parallelizing compiler proceedings 1995 acmieee conference supercomputing cdrom p49es december 0408 1995 san diego california united states satish chandra james r larus optimizing communication hpf programs finegrain distributed shared memory acm sigplan notices v32 n7 p100111 july 1997 compiler optimizations fortran mimd distributedmemory machines proceedings 1991 acmieee conference supercomputing p86100 november 1822 1991 albuquerque new mexico united states dhruva r chakrabarti prithviraj banerjee global optimization techniques automatic parallelization hybrid applications proceedings 15th international conference supercomputing p166180 june 2001 sorrento italy chen ding ken kennedy improving cache performance dynamic applications data computation reorganization run time acm sigplan notices v34 n5 p229241 may 1999 compiling fortran mimd distributedmemory machines communications acm v35 n8 p6680 aug 1992 arun chauhan ken kennedy reducing vectorizing procedures telescoping languages international journal parallel programming v30 n4 p291315 august 2002 brent weatherly david k lowenthal mario nakazawa franklin lowenthal dynmpi supporting mpi mediumscale nondedicated clusters journal parallel distributed computing v66 n6 p822838 june 2006 saman p amarasinghe monica lam communication optimization code generation distributed memory machines acm sigplan notices v28 n6 p126138 june 1993 exploiting cache affinity software cache coherence proceedings 8th international conference supercomputing p264273 july 1115 1994 manchester england jaydeep marathe frank mueller tushar mohan sally mckee bronis r de supinski andy yoo metric memory tracing via dynamic binary rewriting identify cache inefficiencies acm transactions programming languages systems toplas v29 n2 p12es april 2007 tor e jeremiassen susan j eggers reducing false sharing shared memory multiprocessors compile time data transformations acm sigplan notices v30 n8 p179188 aug 1995 yunheung paek jay hoeflinger david padua efficient precise array access analysis acm transactions programming languages systems toplas v24 n1 p65109 january 2002 junjie gu zhiyuan li gyungho lee symbolic array dataflow analysis array privatization program parallelization proceedings 1995 acmieee conference supercomputing cdrom p47es december 0408 1995 san diego california united states paek navarro e zapata j hoeflinger padua advanced compiler framework noncachecoherent multiprocessors ieee transactions parallel distributed systems v13 n3 p241259 march 2002 manish gupta edith schonberg harini srinivasan unified framework optimizing communication dataparallel programs ieee transactions parallel distributed systems v7 n7 p689704 july 1996 radu rugina martin rinard symbolic bounds analysis pointers array indices accessed memory regions acm sigplan notices v35 n5 p182195 may 2000 evaluation compiler optimizations fortran mimd distributed memory machines proceedings 6th international conference supercomputing p114 july 1924 1992 washington c united states arun chauhan ken kennedy optimizing strategies telescoping languages procedure strength reduction procedure vectorization proceedings 15th international conference supercomputing p92101 june 2001 sorrento italy ayon basumallik rudolf eigenmann optimizing irregular sharedmemory applications distributedmemory systems proceedings eleventh acm sigplan symposium principles practice parallel programming march 2931 2006 new york new york usa ayon basumallik rudolf eigenmann towards automatic translation openmp mpi proceedings 19th annual international conference supercomputing june 2022 2005 cambridge massachusetts w hall hiranandani k kennedy cw tseng interprocedural compilation fortran mimd distributedmemory machines proceedings 1992 acmieee conference supercomputing p522534 november 1620 1992 minneapolis minnesota united states steve carr r b lehoucq compiler blockability dense matrix factorizations acm transactions mathematical software toms v23 n3 p336361 sept 1997 victor delaluz mahmut kandemir n vijaykrishnan anand sivasubramaniam mary jane irwin hardware software techniques controlling dram power modes ieee transactions computers v50 n11 p11541173 november 2001 kathryn mckinley compiler optimization algorithm sharedmemory multiprocessors ieee transactions parallel distributed systems v9 n8 p769787 august 1998 dhruva r chakrabarti prithviraj banerjee static single assignment form messagepassing programs international journal parallel programming v29 n2 p139184 april 2001 junjie gu zhiyuan li efficient interprocedural array dataflow analysis automatic program parallelization ieee transactions software engineering v26 n3 p244261 march 2000 mary w hall timothy j harvey ken kennedy nathaniel mcintosh kathryn mckinley jeffrey oldham michael h paleczny gerald roth experiences using parascope editor interactive parallel programming tool acm sigplan notices v28 n7 p3343 july 1993 chen ding ken kennedy improving effective bandwidth compiler enhancement global cache reuse journal parallel distributed computing v64 n1 p108134 january 2004 manish gupta sayak mukhopadhyay navin sinha automatic parallelization recursive procedures international journal parallel programming v28 n6 p537562 december 2000 ken kennedy kathryn mckinley chauwen tseng analysis transformation parascope editor proceedings 5th international conference supercomputing p433447 june 1721 1991 cologne west germany radu rugina martin c rinard symbolic bounds analysis pointers array indices accessed memory regions acm transactions programming languages systems toplas v27 n2 p185235 march 2005 mary w hall saman p amarasinghe brian r murphy shihwei liao monica lam interprocedural parallelization analysis suif acm transactions programming languages systems toplas v27 n4 p662731 july 2005 mohammad r haghighat constantine polychronopoulos symbolic analysis parallelizing compilers acm transactions programming languages systems toplas v18 n4 p477518 july 1996