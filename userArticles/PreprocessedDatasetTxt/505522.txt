highspeed architectures reedsolomon decoders new highspeed vlsi architectures decoding reedsolomon codes berlekampmassey algorithm presented paper speed bottleneck berlekampmassey algorighm iterative computation discrepencies followed updating errorlocator polynomial bottleneck eliminated via series algorithmic transformations result fully systolic architecture single array processors computes errorlocator errorevaluator polynomials contrast conventional berlekampmassey architectures critical path passes two multipliers 1log2t 1 adders critical path proposed architecture passes one multiplier one adder comparable critical path architectures based extended euclidean algorithm interestingly proposed architecture requires approximately 25 fewer multipliers simpler control structure architectures based popular extended euclidean algorithm blockinterleaved reedsolomon codes embedding interleaver memory decoder results reduction critical path delay one xor gate one multiplexer leading speed ups much order magnitude conventional architectures b introduction reedsolomon codes 1 3 employed numerous communications systems deep space digital subscriber loops wireless systems well memory data storage systems continual demand ever higher data rates makes necessary devise highspeed implementations decoders reedsolomon codes recently reported decoder implementations 5 19 quoted data rates ranging 144 mbs 128 gbs high throughputs achieved architectural innovations pipelining parallel processing majority implementations 2 8 15 19 employ architecture based extended euclidean ee algorithm computing greatest common divisor two polynomials 3 key advantage architectures based upon ee algorithm regularity addition critical path delay architectures best tmult tmult add tmux delays niteeld multiplier adder 2 1 multiplexer respectively suciently small applications contrast relatively decoder implementations employed architectures based berlekampmassey bm algorithm draft june 19 2000 1 3 10 presumably architectures found irregular longer critical path delay also dependent errorcorrecting capability code 5 paper show fact possible reformulate bm algorithm achieve extremely regular decoder architectures surprisingly new architectures operate data rates comparable architectures based ee algorithm also lower gate complexity simpler control structures paper begins brief tutorial overview encoding decoding reedsolomon codes section ii conventional architectures decoders based bm algorithm described section iii section iv show possible algorithmically transform bm algorithm homogenous systolic array architecture decoder developed finally section v describe pipelined architecture blockinterleaved reedsolomon codes achieves order magnitude reduction critical path delay architectures presented sections iii iv ii reedsolomon codes provide brief overview encoding decoding reedsolomon codes encoding reedsolomon codes data symbols bytes transmitted communication channel stored memory bytes regarded elements nite eld also called galois eld gf2 1 encoded codeword c n k bytes codeword symbols transmitted communication channel stored memory reedsolomon codes gf2 odd code correct n k2 byte errors encoding process best described terms data polynomial transformed codeword polynomial polynomials cz polynomial addition subtraction gf2 bitbybit xor bytes 2 1 nonzero elements also regarded powers primitive element product eld elements june 19 2000 draft multiples gz generator polynomial code dened typically 0 1 however choices sometimes simplify decoding process slightly since 2t consecutive powers roots gz cz multiple gz follows codeword polynomials cz fact arbitrary polynomial degree less n codeword polynomial satises 2 systematic encoding produces codewords comprised data symbols followed paritycheck symbols obtained follows let qz p z denote quotient remainder respectively polynomial z n k dz degree n 1 divided gz degree multiple gz furthermore since lowest degree term z n k dz 0 z n k p z degree n k 1 follows codeword given consists data symbols followed paritycheck symbols b decoding reedsolomon codes let cz denote transmitted codeword polynomial let rz denote received word polynomial input decoder rz assumes e 0 errors occurred transmission error polynomial ez written conventional say error values occurred error locations note decoder know ez fact draft june 19 2000 even know value e decoders task determine ez input rz thus correct errors subtracting ez rz e calculation always possible fewer errors always corrected decoder begins task error correction computing syndrome values 2t syndrome values zero rz codeword assumed errors occurred otherwise decoder knows e 0 uses syndrome polynomial sz dened calculate error values error locations dene error locator polynomial z degree e error evaluator polynomial z degree e 1 e e e polynomials related sz key equation 1 3 solving key equation determine z z sz hardest part decoding process bm algorithm described section iii ee algorithm used solve 6 e algorithms nd z z e algorithms almost always fail nd z z fortunately failures usually easily detected z z found decoder nd error locations checking whether j 0 j n 1 usually decoder computes value jth received symbol r j leaves decoder circuit process called chien search 1 3 one error locations say x words r j error needs corrected leaves decoder decoder june 19 2000 draft 6 submitted ieee trans vlsi systems calculate error value subtracted r j via forneys error value formula 3 z 0 z z j denotes formal derivative z note formal derivative simplies 0 since considering codes gf2 thus z terms odd degree z hence value z 0 z found evaluation z z j require separate computation note also 7 simplied choosing c reedsolomon decoder structure summary reedsolomon decoder consists three blocks syndrome computation sc block keyequation solver kes block chien search error evaluator csee block blocks usually operate pipelined mode three blocks separately simultaneously working three successive received words sc block computes syndromes via 3 usually received word entering decoder syndromes passed kes block solves 6 determine error locator error evaluator polynomials polynomials passed csee block calculates error locations error values via 7 corrects errors received word read decoder throughput bottleneck reedsolomon decoders kes block solves 6 contrast sc csee blocks relatively straightforward implement hence paper focus developing highspeed architectures kes block mentioned earlier key equation 6 solved via ee algorithm see 19 17 imple mentations via bm algorithm see 5 implementations paper develop highspeed architectures reformulated version bm algorithm believe reformulated algorithm used achieve much higher speeds achieved implementations bm ee algorithms furthermore shall show section ivb4 new architectures also lower gate complexity simpler control structure architectures based ee algorithm draft june 19 2000 iii existing berlekampmassey bm architectures section give brief description dierent versions berlekampmassey bm algorithm discuss generic architecture similar paper reed et al 13 implementation algorithm berlekampmassey algorithm bm algorithm iterative procedure solving 6 form originally proposed berlekamp 1 algorithm begins polynomials 0 iteratively determines polynomials r z satisfying polynomial congruence thus obtains solution 2t z z key equation 6 two scratch polynomials br z hr z initial values b0 used algorithm successive value r algorithm determines r z br z r 1 z br 1 z similarly algorithm determines z hr z r 1 z hr 1 z since sz degree 2t 1 polynomials degrees large algorithm needs store roughly 6t eld elements iteration completed one clock cycle 2t clock cycles needed nd errorlocator errorevaluator polynomials recent years researchers used formulation bm algorithm given blahut 3 r z br z computed iteratively following completion 2t iterations errorevaluator polynomial z computed terms degree 1 less polynomial product 2t zsz implementation version thus needs store 4t eld elements computation z requires additional clock cycles although version bm algorithm trades space time also suers problem berlekamp version viz iterations necessary divide coecient r z quantity r divisions eciently handled rst computing 1 r inverse r multiplying coecient r z 1 r unfortunately regardless whether method used whether one constructs separate divider circuits coecient r z divisions occur inside iterative loop timeconsuming multiplications obviously divisions could replaced multiplications resulting circuit implementation would smaller critical june 19 2000 draft 8 submitted ieee trans vlsi systems path delay higher clock speeds would usable 2 less wellknown version bm algorithm 4 13 precisely property recently employed practice 13 5 focus version bm algorithm paper inversionless bm ibm algorithm described pseudocode shown ibm algorithm actually nds scalar multiples z z instead z dened 4 5 however obvious chien search nd error locations follows 7 error values obtained hence continue refer polynomials computed ibm algorithm z z minor implementation detail 4 thus requires latches storage ibm algorithm must store note also b 1 r occurs steps ibm2 ibm3 constant value 0 r ibm algorithm initialization begin step ibm1 step ibm2 step ibm3 r 6 0 kr 0 begin else 2 astute reader noticed forney error value formula 7 also involves division fortunately divisions pipelined feedforward computations similarly polynomial evaluations needed csee block well sc block feedforward computations pipelined unfortunately divisions kes block occur inside iterative loop hence pipelining computation becomes dicult thus noted section ii throughput bottleneck kes block draft june 19 2000 begin r step ibm1 includes terms 1 r1 r 2 r2 involving unknown quantities fortunately known 3 deg r z r therefore unknown aect value r notice also similarity steps ibm1 ibm4 facts used simplify architecture describe next b architectures based ibm algorithm due similarity steps ibm1 ibm4 architectures based ibm algorithm need two major computational structures shown fig 1 discrepancy computation dc block implementing step ibm1 error locator update elu block implements steps ibm2 ibm3 parallel dc block contains latches storing syndromes gf2 arithmetic units computing discrepancy r control unit entire architecture connected elu block contains latches storing r z br z well gf2 arithmetic units updating polynomials shown fig 1 clock cycle dc block computes discrepancy r passes value together r control signal mcr elu block updates polynomials clock cycle operations completed one clock cycle assume mbit parallel arithmetic units employed architectures galois eld arithmetic units found numerous references including 7 discussed june 19 2000 draft block cycles 2t1 3t block syndromes block l r l t1 r 0011 fig 1 ibm architecture b1 dc block architecture dc block architecture shown fig 2 2t latches constituting ds shift register initialized latches ds contain syndromes rst 2t clock cycles 1 multipliers compute products step ibm1 added binary adder tree depth dlog 1e produce discrepancy r thus delay computing r typical control unit one illustrated fig 2 counters variables r kr storage r following computation r control unit computes bits r determine whether r nonzero requires 1 twoinput gates arranged binary tree depth dlog 2 counter kr implemented twoscomplement representation kr 0 signicant bit counter 0 delay generating signal mcr thus finally mcr signal available counter kr updated notice twoscomplement arithmetic addition needed kr 1 hand negation twoscomplement representation complements bits adds 1 hence update kr complementation bits kr counter note possible use ring counters r kr case kr draft june 19 2000 sc block syndromes control msb cntr r fig 2 discrepancy computation dc block updated tmux seconds mcr signal computed following 2t clock cycles bm algorithm dc block computes errorlocator polynomial z next clock cycles achieve ds ds latches reset zero 2tth clock cycle beginning 2t cycle contents ds register see fig 2 also outputs elu block frozen change computation z step ibm4 follows discrepancies computed next clock june 19 2000 draft cycles coecients 0 z architecture fig 2 enhanced version one described 13 latter uses slightly dierent structure dierent initialization ds register dc block requires storage makes less adaptable subsequent computation errorlocator polynomial note total hardware requirements dc block 2t mbit latches tipliers adders miscellaneous circuitry counters arithmetic adder ring counter gates inverters latches control unit fig 2 critical path delay dc block b2 elu block architecture following computation discrepancy r mcr signal dc block polynomial coecient updates steps ibm2 ibm3 performed simultaneously elu block processor element pe0 hereinafter pe0 processor updates one coecient z bz illustrated fig 3a complete elu architecture shown fig 3b see signals r r mcr broadcast pe0 processors addition latches pe0 processors initialized zero except latches initialized element 1 latches multipliers adders multiplexers needed critical path delay elu block given b3 ibm architecture ignoring hardware used control section total hardware needed implement ibm algorithm 4t multiplexers total time required solve key equation one codeword 3t clock cycles alternatively z computed iteratively computations require 2t clock cycles however since computations required update r z nearduplicate elu block needed 3 increases hardware requirements 6t z array pe0 processors draft june 19 2000 r r l r r l r010101010101010101 pe0001100110011l l l l 0t1 b fig 3 elu block diagrama pe0 processor b elu architecture latches initialized 1 2 gf pe0s initialized 0 either case critical path delay ibm architecture obtained figs 1 2 3 delay direct path begins dc block starting ds latches multiplier adder tree height dlog generating signal r feeding elu block multiplier adder latched assumed indirect path taken r control unit generating signal mcr feeding elu block multiplexer faster direct path ie tmult dlog 2 reasonable assumption technologies note half ibm due delay dc block contribution increases logarithmically error correction capability thus reducing delay dc block key achieving higher speeds next section describe algorithmic reformulations ibm algorithm lead june 19 2000 draft 14 submitted ieee trans vlsi systems systolic architecture dc block reduce critical path delay telu iv proposed reedsolomon decoder architectures critical path ibm architectures type described section iii passes two multipliers well adder tree structure dc block multiplier units contribute signicantly critical path delay hence reduce throughput achievable ibm architecture section propose new decoder architectures smaller critical path delay architectures derived via algorithmic reformulation ibm algorithm reformulated ibm ribm algorithm computes next discrepancy r 1 time computing current polynomial coecient updates b r 1s possible reformulated discrepancy computation use explicitly furthermore discrepancy computed block structure elu block blocks critical path delay reformulation ibm algorithm a1 simultaneous computation discrepancies updates viewing steps ibm2 ibm3 terms polynomials see step ibm2 computes step ibm3 sets br1 z either r z zbr z next note discrepancy r computed step ibm1 actually r r coecient z r polynomial product much faster implementations possible decoder computes coecients r z r even though r r needed compute r decide whether br set r z z br z suppose beginning clock cycle decoder available coecients r z r z course r z br z well thus available beginning clock cycle decoder compute r draft june 19 2000 furthermore follows 10 11 set either r z r z br z sz short r computed exactly manner r furthermore four polynomial updates computed simultaneously polynomial coecients well r1 r 1 thus available beginning next clock cycle a2 new errorevaluator polynomial ribm algorithm simultaneously updates four polynomials r z br z r z sz 2t iterations thus produce errorlocator polynomial 2t z also polynomial 2t z note since c z 2t z sz mod z 2t follows 11 loworder coecients 2t z z 2t iterations compute errorlocator polynomial 2t z errorevaluator polynomial z additional iterations step ibm4 needed highorder coecients 2t z also used error evaluation let 2t z h z degree e 1 contains highorder terms since x 1 root 2t z follows 11 2t x 1 0 thus 7 rewritten z 0 z zx 1 next show variation error evaluation formula certain architectural advan tages note choice preferable 12 used a3 reformulation since updating four polynomials identical discrepancies calculated using elu block like one described section iii unfortunately discrepancy r r computed processor pe0 r thus multiplexers needed route june 19 2000 draft appropriate latch contents control unit elu block computes r additional reformulation ibm algorithm described next eliminates multiplexers use fact aect value later discrepancy rj r j consequently need store r r r thus polynomials initial values follows polynomial coecients updated set either i1r ir r note discrepancy r always xed zeroth position form update nal comment note form update ultimately produces thus 12 used error evaluation csee block ribm algorithm described following pseudocode note b 1 values r quantities need stored updated ribm algorithm initialization begin step ribm1 step ribm2 begin draft june 19 2000 else begin next consider architectures implement ribm algorithm b highspeed reedsolomon decoder architectures ibm architecture described section iii ribm architecture consists reformulated discrepancy computation rdc block connected elu block b1 rdc architecture rdc block uses processor pe1 shown fig 4a rdc architecture shown fig 4b notice processor pe1 similar processor pe0 fig 3a ever contents upper latch ow pe1 contents lower latch recirculate contrast lower latch contents ow processor pe0 contents upper latch recirculate obviously hardware complexity critical path delays processors pe0 pe1 identical thus assuming tmult dlog 2 get note delay independent errorcorrection capability code hardware requirements proposed architecture fig 4 2t pe1 processors 4t latches 4t multipliers 2t adders 2t multiplexers addition control unit fig 2 june 19 2000 draft r r r r r r r sw sw b fig 4 rdc block diagrama pe1 processor b rdc architecture b2 ribm architecture overall ribm architecture shown fig 5 uses rdc block fig 4 elu block fig 3 note outputs elu block feed back rdc block blocks critical path delay add since operate parallel proposed ribm architecture achieves critical path delay less half delay add enhanced ibm architecture noted previous subsection end 2tth iteration pe1 contain coecients 2t z used error evaluation thus 2t clock cycles used determine z h z needed 12 ignoring control unit hardware requirement architecture 3t draft june 19 2000 r l l l l 0t1 elu block control rdcblock fig 5 systolic ribm architecture multiplexers compares favorably multiplexers needed implement enhanced ibm architecture section iii errorlocator errorevaluator polynomial computed 2t clock cycles using 1 additional multipliers additional multiplexers reduced critical path delay 50 furthermore ribm architecture consists two systolic arrays thus regular b3 ribm architecture show possible eliminate elu block entirely implement bm algorithm enhanced rdc block array 2t pe1 processors lengthened array 3t processors shown fig 6 completely systolic architecture single array computes z z since processors eliminated elu block reappear additional pe1 processors ribm architecture hardware complexity critical path delay ribm architecture june 19 2000 draft however extremely regular structure esthetically pleasing also oers advantage vlsi circuit layoutsh t1t control l r fig 6 homogenous systolic ribm architecture array pe0 processors ribm architecture see fig 5 carries polynomial computation array pe1 processors ribm architecture see fig 6 latter array polynomial coecients shift left clock pulse thus ribm architecture suppose initial loading pe1 0 pe1 1 pe1 2t 1 fig 4 loaded zeroes latches pe1 3t loaded iterations proceed polynomials updated processors lefthand end array eectively r z r z get updated shifted leftwards 2t clock cycles coecients h z processors next note pe1 3t contains 0 z b0 z iterations proceed r z br z shift leftwards processors righthand end array r b r stored processor pe1 3t ri 2t clock cycles processor pe1 ti contains 2t b 2t thus array carrying two separate computations computations interfere one another polynomials draft june 19 2000 stored processors numbered 3t r higher hand since deg r known upper bound deg r z known 3 lr nondecreasing function r maximum value errors occurred hence 2t 1 r thus r z br z shift leftwards overwrite coecients denote contents array ribm architecture polynomials z initial values z 3t ribm architecture implements following pseudocode note 3t1 values r quantity need stored updated ribm algorithm initialization begin begin else begin june 19 2000 draft 22 submitted ieee trans vlsi systems b4 comparison architectures table summarizes complexity various architectures described far seen comparison conventional ibm architecture berlekamps version proposed ribm ribm systolic architectures require 1 multipliers multiplexers three architectures require numbers latches adders three architectures require 2t cycles solve key equation terrorcorrecting code ribm ribm architectures require considerably gates conventional ibm architecture blahuts version also require 2t clock cycles compared 3t clock cycles required latter furthermore since critical path delay ribm ribm architectures less half critical path delay either ibm architectures conclude new architectures signicantly reduce total time required solve key equation thus achieve higher throughput modest increase gate count important regularity scalability ribm ribm architectures creates potential automatically generating regular layouts via core generator predictable delays various values comparison ribm ribm architectures ee architectures complicated fact recent implementations use folded architectures processor element systolic array arithmetic units units carry needed computations via timedivisionmultiplexing example hypersystolic ee architecture 2 2t elements containing one multiplier adder since iteration euclidean algorithm requires 4 multiplications processors 2 need several multiplexers route various operands arithmetic units additional latches store one addend addend computed multiplier etc result architecture described 2 requires many latches multiplexers also many clock cycles ribm ribm architectures furthermore critical path delay slightly larger multiplexers various paths hand niteeld multipliers consist large numbers gates possibly many 2m 2 fewer logic minimization techniques used thus complete comparison gate counts two architectures requires specic details multipliers nonetheless rough draft june 19 2000 comparison hardware complexity path delays architecture adders multipliers latches muxes clock critical cycles path delay ibm ibm euclidean euclidean 2folded 2t comparison ribm ribm architectures require three times many gates hypersystolic ee architecture solve key equation onesixth time course possible implement ee algorithm complex processor elements described shao et al 14 4 multiplications processor computed using 4 separate multipliers architecture described 14 uses 2t1 processors compared 3t processors needed ribm ribm architectures processor 14 4 multipliers 4 multiplexers 2 adders result ribm ribm architectures compare favorably ee architecture 14 new architectures achieve actually slightly higher throughput much smaller complexity one nal point made respect comparison ribm ribm architectures ee architectures controllers systolic arrays former actually much simpler ee architecture 14 processor also control section uses arithmetic adder comparator two multiplexers 2dlog 2 te bits arithmetic data passed processor processor array used generate multiplexer control signals processor similarly ee architecture 2 separate control circuit processor delays control circuits accounted critical path delays ee architectures listed table contrast multiplexers ribm ribm architectures receive signal computations architectures purely systolic sense processors carry exactly computation cycle multiplexers set way june 19 2000 draft processors cellspecic control signals preliminary layout results preliminary layout results core generator shown fig 7 kes block 4errorcorrecting reedsolomon code gf2 8 processing element pe1 shown fig 7a upper 8 latches store element r lower 8 latches store element r complete ribm architecture shown fig 7b 13 pe1 processing elements arrayed diagonally error locator error evaluator polynomials output latches seen arrayed vertically critical path delay ribm architecture reported synthesis tool synopsys 213 ns tsmcs 025m 33v cmos technology b fig 7 ribm architecture synthesized 33v 025m cmos technologya pe1 processing element b ribm architecture next section develop pipelined architecture reduces critical path delay much order magnitude using blockinterleaved code draft june 19 2000 v pipelined reedsolomon decoders iterations original bm algorithm pipelined using lookahead transformation 12 liu et al 9 method applied ribm ribm algorithms however pipelining requires complex overhead control hardware hand pipeline interleaving also described 12 decoder blockinterleaved reedsolomon code simple ecient technique reduce critical path delay decoder order magnitude describe results ribm architecture section iv techniques also applied ribm architecture well decoder architectures described section iii blockinterleaved reedsolomon codes a1 block interleaving errorcorrecting codes use channels errors occur bursts often interleaved symbols codeword transmitted consecutively burst errors thus causes single errors multiple codewords rather multiple errors single codeword latter occurrence undesirable since easily overwhelm errorcorrecting capabilities code cause decoder failure decoder error two types interleavers block interleavers convolutional interleavers commonly used see eg 16 18 restrict attention blockinterleaved codes blockinterleaving n k code depth results nm km interleaved code whose codewords property c n 1mi c n codeword n equivalently codeword nm km code multichannel data stream channels carries codeword n code a2 interleaving via memory arrays usual description see eg 16 18 encoder blockinterleaved nm km code involves partitioning km data symbols blocks k consecutive symbols encoding block codeword n codewords stored rowwise n memory array memory read columnwise form blockinterleaved codeword notice blockinterleaved code june 19 2000 draft 26 submitted ieee trans vlsi systems word systematic sense paritycheck symbols follow data symbols reedsolomon encoding process described section iia results blockinterleaved codeword data symbols transmitted channel order entered encoder 4 receiver interleaving process reversed storing nm received symbols columnwise n memory array memory read rowwise received words length n decoded decoder n code information symbols appear correct order deinterleaved stream decoder output passed destination a3 embedded interleavers alternative form block interleaving embeds interleaver encoder thereby transforming encoder nm km code interleaved reedsolomon codes mathematical description encoding process generator polynomial interleaved code gz gz denotes generator polynomial n dened 1 codeword formed described section iia ie dz denoting data polynomial km 1 z degree km 1 polynomial z n km dz divided gz obtain remainder p z degree n km 1 transmitted codeword z n km dz p z essence data stream treated multichannel data stream stream channel encoded n code output encoder codeword blockinterleaved reedsolomon code separate interleaver needed property data symbols transmitted channel order entered encoder astute reader observed already encoder nm km code delayscaled encoder n code delayscaling transformation architecture replaces every delay latch architecture delays retimes architecture account additional delays encoder treats input multichannel data stream produces multichannel output data stream blockinterleaved reedsolomon fact data symbol ordering produced interleaving data stream blocks k symbols depth draft june 19 2000 codeword note also interleaver array eliminated delayscaled encoder uses times much memory conventional encoder blockinterleaved reedsolomon codewords produced delayscaled encoders contain data symbols correct order thus delayscaled decoder used decode received word nm symbols output decoder also data symbols correct order note separate deinterleaver array needed receiver however delayscaled decoder uses times much memory conventional decoder example delayscaling pe1 processors ribm architecture fig 6 results delayscaled processor dpe1 shown fig 8 note 0 2t 1 top bottom sets latches dpe1 initialized syndrome set 0m 1 ij th syndrome j th codeword 2t 3t 1 latches dpe1 initialized 0 latches dpe1 3t initialized 1 2 gf2 2tm clock cycles processors dpe1 0 dpe1 1 contain interleaved errorevaluator polynomials processors dpe1 dpe1 2t contain interleaved errorlocator polynomials md md fig 8 delayscaled dpe1 processor initial conditions latches indicated ovals delayscaled ribm architecture obtained replacing pe1 processors fig 6 dpe1 processor delayscaling control unit well remark delayscaled decoders also used decode blockinterleaved reedsolomon codewords produced memory array interleavers however data symbols june 19 2000 draft 28 submitted ieee trans vlsi systems output decoder still interleaved k memory array needed deinterleaving data symbols correct order array smaller n array needed deinterleave codewords prior decoding conventional decoder conventional decoder also uses less memory delayscaled decoder delayscaling encoder decoder eliminates separate interleavers deinterleaver thus natural choice generating decoding blockinterleaved reedsolomon codewords however delayscaled decoder critical path delay original decoder hence cannot achieve higher throughput original decoder hand extra delays used pipeline computations critical path leads signicant increases achievable throughput discuss concept next b pipelined delayscaled decoders critical path delay ribm architecture mostly due niteeld multipliers processors delayscaled processors dpe1 shown fig 8 multipliers pipelined critical path delay reduced signicantly assume describe pipelined niteeld multiplier stages b1 pipelined multiplier architecture pipelining multiplier especially feedforward structure trivial case rs decoders pipelining done manner initial conditions pipelining latches consistent syndrome values generated sc block design niteeld multipliers depends choice basis representation consider standard polynomial basis mbit byte represents galois eld element pipelined multiplier architecture based writing product two gf x let pp denote sum rst terms sum multiplier processing element shown fig 9a computes pp i1 adding either x draft june 19 2000 simultaneously mpe multiplies x since constant multiplication requires xor gates computed delay hand delay computing pp i1 thus critical path delay order magnitude smaller tremendous speed gains achieved pipelined multiplier architecture used decoding blockinterleaved reedsolomon code practical considerations delays due pipelining latches clock skew jitter prevent fullest realization speed gains due pipelining nevertheless pipelined multiplier structure combination systolic architecture provide signicant gains existing approaches dd pp pp pp pp m1d m2d b fig 9 pipelined multiplier block diagrama multiplier processing element mpe b multiplier architecture initial conditions latches input indicated ovals pipelined multiplier thus consists mpe processors connected shown fig 9b inputs pp initial conditions latches input zero therefore initial conditions lower latches mpes aect circuit operation product xy appears upper latch mpem 1 clock cycles succeeding clock cycle thereafter computes new product notice also rst june 19 2000 draft clock cycles initial contents upper latches mpes appear succession output mpem 1 property crucial proper operation proposed pipelined decoder b2 pipelined control unit pipelined multiplier architecture described shown fig used dpe1 processors fig 8 critical path delay dpe1 reduced tmult add thus control unit delay computing mcr inconsequential ribm architecture well ibm ribm architectures delayscaled versions determines largest delay pipelined ribm architecture fortunately computation mcr also pipelined say stages done noting delays dpe1 0 mdelay scaled ribm architecture see fig 8 retimed outputs control unit subsequently employed pipeline note however latches dpe1 0 retimed initialized 0d 1 begininng every decoding cycle hence retimed latches control unit need initialized values function syndromes 0d 1 problem syndromes produced sc block beginning decoding cycle b3 pipelined processors pipelined multiplier units described used delayscaled dpe1 processor control unit pipelined described get pipelined ppe1 processor shown fig 10 pipelined ribm pribm architecture also described fig 10 initial values stored latches described earlier dpe1 processors note latches store coecients part latches pipelined multiplier however initial values latches lower multiplier fig 10 0 thus rst clock cycles ow leftmost latches without change description obvious pribm architecture based ppe1 processor fig 10 critical path delay draft june 19 2000 md md mrjmdd mmd fig 10 pipelined ppe1 processor initial conditions latches indicated ovals pipelined ribm architecture obtained replacing pe1 processors fig 6 ppe1 processor employing pipelined delayscaled controller thus pribm architecture clocked speeds much order magnitude higher achievable unpipelined architectures presented sections iii iv c decoders noninterleaved codes pribm architecture decode blockinterleaved code signicantly faster rates ribm architecture decode noninterleaved code fact dierence large enough designer asked devise decoder noninterleaved codes give serious thought following design strategy read successive received words blockinterleaver memory array read blockinterleaved received word decoder pribm architecture decode blockinterleaved word read data symbols blockdeinterleaver memory array read deinterleaved data symbols deinterleaver array obviously similar decoder design strategies used situations well example decode convolutionally interleaved code one rst deinterleave received words reinterleave blockinterleaved format decoding similarly block interleaved code large interleaving depth pribm architecture may large implement single chip case one deinterleave rst reinterleave suitable depth fact deinterleave reinterleave strategy used construct june 19 2000 draft universal decoder around single decoder chip xed interleaving depth vi concluding remarks shown application algorithmic transformations berlekampmassey algorithm result ribm ribm architectures whose critical path delay less half conventional architectures ibm architecture ribm ribm architectures use systolic arrays identical processor elements blockinterleaved codes deinterleaver embedded decoder architecture via delayscaling furthermore pipelining multiplications delayscaled architecture result order magnitude reduction critical path delay fact high speeds pribm architecture operate makes feasible use decode noninterleaved codes simple stratagem internally interleaving received words decoding resulting interleaved word using pribm architecture deinterleaving output future work directed towards integrated circuit implementations proposed architectures incorporation broadband communications systems highspeed digital subscriber loops wireless systems vii acknowledgments authors would like thank reviewers constructive criticisms resulted signicant improvements manuscript r algebraic coding theory theory practice errorcontrol codes applied coding information theory engineers control systems digital communication storage tr systems digital communication storage applied coding information theory engineers ctr kazunori shimizu nozomu togawa takeshi ikenaga satoshi goto reconfigurable adaptive fec system interleaving proceedings 2005 conference asia south pacific design automation january 1821 2005 shanghai china tong zhang keshab k parhi highspeed vlsi implementation errorsanderasures correcting reedsolomon decoders proceedings 12th acm great lakes symposium vlsi april 1819 2002 new york new york usa w chang k truong j h jeng vlsi architecture modified euclidean algorithm reedsolomon code information sciences international journal v155 n12 p139150 1 october zhiyuan yan dilip v sarwate universal reedsolomon decoders based berlekampmassey algorithm proceedings 14th acm great lakes symposium vlsi april 2628 2004 boston usa jung h lee jaesung lee myung h sunwoo design applicationspecific instructions hardware accelerator reedsolomon codecs eurasip journal applied signal processing v2003 n1 p13461354 january zhiyuan yan dilip v sarwate new systolic architectures inversion division gf2m ieee transactions computers v52 n11 p15141519 november