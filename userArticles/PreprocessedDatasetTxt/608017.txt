data squashing empirical likelihood data squashing introduced w dumouchel c volinsky johnson c cortes pregibon proceedings 5th international conference kdd 1999 idea scale data sets smaller representative samples instead scaling algorithms large data sets report success learning model coefficients squashed data paper presents form data squashing based empirical likelihood method reweights random sample data match certain expected values population computation required relatively easy convex optimization also theoretical basis predict wont produce large gains credit scoring example empirical likelihood weighting also accelerates rate coefficients learned also investigate extent benefits translate improved accuracy consider reweighting conjunction boosted decision trees b introduction staple problem data mining construction classication rules data data warehouses large becomes impractical train classication rule using available data instead sample available data may selected training instance enterprise miner sas institute features semma process acronym leading stands sample dumouchel et al 1999 introduce data squashing improve upon sampling instead scaling algorithms large data sets one scales data suit existing algorithms instead relatively passive sampling large data set construct data set way make suitable training algorithms suppose original data consist n pairs x x vector predictor variables variable predicted x data squashing one constructs much smaller data set assigning weights w necessarily connection points like x 1 x 1 index indeed value like x 1 might correspond x idea training algorithm n weighted much faster training n original data points large speed gains may expected squashed data main memory outline paper section 2 describes data squashing presents version using empirical likelihood weights points connections data squashing numerical integration variance reduction techniques used monte carlo simulation survey sampling finding empirical likelihood weights reduces tractable convex optimization problem empirical likelihood squashing also theoretical underpinnings predict wont work outlined section 2 section 3 describes credit scoring problem data values simulated distorted obfuscating transformations variable names data source hidden condentiality assured remains good test case algorithms section 4 applies logistic regression small data samples without empirical likelihood reweighting reweighting accelerates rate coecients learned section 5 replaces logistic regression boosted decision trees section 6 presents conclusions less pleased results squashing dumouchel et al 1999 though describe sort problem expect squashing add value dierent conclusions could due dierences algorithms dierences way results assessed simply data sets dierent conclude section references madigan raghavan dumouchel nason posse ridgeway 2000 oer likelihood based form squashing geared exploit userspecied statistical model bradley fayyad reina 1998 goals similar dumouchel et al 1999 madigan et al 2000 instead representing data weighted set points employ mixture models elements mixtures include gaussian distributions multinomial distributions products thereof rowe 1983 describes earlier work direction recent cited work much ambitious bets greater computational power available today 2 data squashing begin outlining data squashing method dumouchel et al 1999 cast older methods new light special cases data squashing notation diers somewhat original dumouchel et al 1999 distinguish predictor response squashing deferring distinction training stage allows squashed data set used multiple prediction problems also choose weights w average weight 1 training algorithms aected scaling case simple alternate conventions dumouchel et al 1999 choose w outlined rst step group x vectors regions suggest several ways construct regions simplest method points x region share values every discrete variable also share values discretized versions every continuous variable points region low order moments noncategorical variables computed region set points corresponding weights w chosen weighted moments squashed data match nearly match unweighted moments original data function x pairs moment within region corresponds taking g product powers noncategorical variables multiplied function one inside region zero outside let z weights would provide perfect match withn given enough moments regions ideal weights possible dumouchel et al 1999 minimize zm instead 0 larger values lower order moments value 3 minimized w x n scalar valued variable like persons age number children household squashed data value need match sample values allowed go outside range data thus squashed data may records 22 children records 3 children 21 sampling squashing issues data mining echo sampling two good references sampling cochran 1977 lohr 1999 simple random sampling cast trivial version squashing let subset n distinct simple random sample without replacement take stratied sampling population partitioned strata sample n h values taken stratum h weight nn h nn h makes 1 hold functions g indicators strata regression estimator used sampling theory incorporate known value population mean suppose 1n zm known form weights z zs 1 z regression weights satisfy 1 regression estimator shown subsume stratication introducing indicator variables z regression stratication also combined several ways regression estimator also widely used monte carlo simulation known method control variates two general references bratley fox schrage 1987 ripley 1987 hesterberg 1995 good presentation reweighting approach control variates 22 empirical likelihood squashing problem regression weights take negative values may unusable training algorithms one insists w 0 either solutions 1 2 else n 1 dimensional family solutions solutions one might either increase n remove moments consideration suppose n 1 dimensional family solutions natural pick one somehow closest equal weights empirical likelihood weights maximize subject z owen 1990 describes compute weights reduces minimizing convex function convex domain taken dimensional euclidean space splus function available httpwwwstatstanfordeduowen computes empirical likelihood weights empirical likelihood provides one way picking weights w closest equality one also use distance measures kullbackliebler distance hellinger distance w 12 empirical likelihood weights advantage computation slightly simpler alternatives minimizing euclidean distance simpler still reduces regression weights 4 may negative owen 1991 23 benets weighting stratication generally regression weighting advantage reducing variance associated estimators let hx function data cases let simple random sample estimate h h variance approximately 2 h n main error approximation multiplicative factor 1 nn take virtually one data squashing eect regression weighting reduce variance 1 2 h n correlation hx 1 reduction factor 1 r 2 r 2 proportion variance explained linear regression z shows empirical likelihood reduces asymptotic variance estimated means factor regression estimators training method estimates means accurately often shown predict accurately simplest cases like linear regres sion prediction constructed smooth function sample moments complicated settings like maximum likelihood estimation parameter vector dened equationsn log estimate solves equationsn log fx qin lawless 1994 show empirical likelihood weights produce variance reduction compared unweighted estimate extent reduction depends well z correlated derivatives averaged 5 baggerly 1998 shows reduction holds distance measures kullbackliebler hellinger 24 diminishing returns better parameter estimates translate directly better prediction rules generally diminishing returns example consider logistic regression parameter vector simplicity logistic regression fact accurate knowing means knowing bayes rule ordinary sampling estimate approaches error order n 12 weighted misclassication losses loss using typically 1 bayes loss wol stork owen 1996 reason bayes rule derivative respect expected misclassication zero expect error approximate form b squashing used approximate form b empirical likelihood squashing used xed list functions generally expect 0 bayes error b 0 dominates estimation error 1 regression empirical likelihood squashing bring small benet logistic model fails hold instead taking b 0 bayes error take best error rate available within logistic family squashing method dumouchel et al 1999 adjusts weights also estimates new values x since sampled cannot quote results like empirical likelihood squashing suppose searching x w eect matching approximately matching many functions value n similar way gauss quadrature rule adjusts location weights numerical integration davis rabinowitz 1984 integrates higher order polynomials one adjust weights locations matching function values possible come closer bayes error rate course reduce bayes error rate thus could reasonably expect error form thus expect squashing various forms eective cases bayes error dominated sampling approximation errors particular settings zero bayes error may benet enormously squashing 25 expect benets reasonable expect better model coecients squashing albeit eventually diminishing gains prediction accuracy order realize gains coecients must related quantities correlated values g x precisely vector log fx well approximated linear combination g x expect improved estimate helps distinguish local global features data logistic regression uses global features data reasonable expect features could highly correlated judiciously chosen global features nearest neighbor method uses local features averages small regions determined x values reasonable expect one local averages correlated global features data therefore squashing global features g help nearest neighbors much improvement one must consider ways employ large number local functions g method like classication tree would seem priori intermediate rst split global feature data nal splits made least large tree local features thus squashing global g help rst splits later ones 3 example data data set inspired real commercial problem problem disguised order preserve condentiality training data 92000 rows 46 columns data arise credit scoring problem source known data set transformed obfuscated described row data describes one credit case rows presented random order column contains one variable response variable column 41 0 1 describing bad good credit outcomes respectively may possible attribute dollar value bad good outcome dollar values data received indeed may existed data roughly 85 good cases although necessarily percentage good population variables 2 40 42 predictor variables describing credit history case original data values transformed original values given predictor put vector v 92000 elements transformed values z v minvmaxv minv p power p chosen random independently predictor variable missing values remained missing transformation contribute minv maxv column 1 score variable used predict response constructed knowledge input variables mean unknown possibly proprietary algorithm used generate column custombuilt score serves benchmark compare performance training methods missing values original data stored 99990 missing values interpreted available believed 309262 missing values 85 predictor values column 19 almost 97 missing dropping column 5 columns 10 missing left data 78165 missing values values imputed missing entries described result 38 remaining predictors prior building prediction models data transformation applied column predictor values nonmissing values replaced x ij raised power p 0 chosen among values 10g value p 0 chosen maximize normalized separation means means variances pairs x nonmissing x ij n yj number ij missing missing value x p ij simply replaced imputed value 02 idea replace missing values ones neutral possible regarding classication hand 24430 observations one imputed values 4 logistic regression rst classication method applied simple logistic regression training data contain cases randomized order therefore simple random sample obtained taking x rst n cases 1000 2000 4000 8000 92000 weighted unweighted logistic regressions run weights 10 making weighted unweighted analyses identical weights chosen 2 f0 1g weighted mean x ij matched unweighted mean x ij reason choice follows simple global classiers based solely response group conditional means variances covariances predictors reasonable expect conditional means carry relevant information many predictor variables allow use conditional second moments conditional moments matched imposing equation 1 taking weights shown figure 1 smallest weight 035 largest 325 n increases weights become nearly equal one possible reweight data match conditional moments using positive weights smallest sample size use figure shows euclidean distance estimated coecient vectors full data coecient vector decreases n increases decrease faster empirical likelihood weighted estimates terms accuracy estimating coecients empirical likelihood weighting increases eective sample size roughly 4 figure 1 shown empirical likelihood weights credit scoring data increased accuracy coecient estimation leads increased accuracy classication diminishing returns figure 3 shows receiver operating characteristic roc curves several classiers described data roc curve plotted classier produces score function x predictors interpretation score figure 2 shown distances logistic regression coecients 92 000 sample points based subsamples lower line weighted logistic regressions using empirical likelihood weights function larger values x make likely point classied x 0 threshold 0 chosen trade error rates false positive false negative predictions roc curve plots proportion good versus proportion bad 0 decreases 1 1 roc curve arcs 0 0 1 1 top roc curve figure 3 corresponds customized score vector supplied data solid lines correspond empirical likelihood weighted logistic regressions n points 8000 92000 lines increase increasing n dashed lines correspond unweighted logistic regression 92000 weighted unweighted roc curves reference point 02 08 describes hypothetical classication rule accepts 80 good cases 20 bad ones custom rule nearly good roc curves tend make performance dierences among classiers look small part reason underlying probabilities plotted ranges 0 100 important distinctions among real clas siers much smaller example dierence 75 80 acceptance good cases small plot like likely practical importance despite clear diminishing returns n increases whether weighted unweighted logistic regression 8000 cases produces roc curve essentially overlaps logistic regression 92000 figure 3 shown roc curves logistic regressions proprietary score percent good cases classied good plotted percent bad cases classied good example point 02 08 describes unrealized setting 80 good cases would accepted along 20 bad cases solid curves top bottom proprietary score empirical likelihood weighted logistic regression samples sizes 92000 8000 4000 2000 1000 dashed curves top bottom unweighted logistic regression 8000 4000 2000 1000 cases curves overlap signicantly described text cases empirical likelihood weighting produces overlap smaller sample perhaps although coecients keep getting better performance tends converge limit reasonable expect better squashing techniques would get logistic regressions good full data logistic regression even smaller sample sizes empirical likelihood weighted logistic regression roc curves figure 3 computed n points including points used training little risk overtting sample sizes n either large compared 39 small compared evidence logistic regressions overt notice logistic regression 92000 cases produced roc curve much better one 4000 cases 5 boosted trees logistic regression fairly old classication technique modern classication methods also make use observation weights also considered boosted classication trees boosted classication trees make predictions combining large number typically small classication trees extreme individual trees one split taking weighted sum stumps produces additive model friedman 1999a friedman 1999b describe multiple additive regression tree mart modeling constructing boosted tree classiers builds earlier work friedman hastie tibshirani 1999 built turn freund schapire 1996 roc curves obtained mart using samples size 1000 2000 4000 8000 92000 using empirical likelihood weighted unweighted analyses plotted roc curves tend hard distinguish well logistic regression customized method figure 3 curves separate visually interval 01 02 horizontal axis range roughly parallel crossings among close curves custom 0217 0479 0651 0792 09448 099217 099761 099895 0999793 mart 0190 0485 0634 0774 09433 099172 099733 099891 0999871 logistic 0163 0431 0604 0754 09244 099026 099720 099881 0999858 mart 4 0188 0456 0626 0770 09361 099150 099689 099877 0999832 mart 8 0189 0477 0636 0774 09419 099147 099707 099889 0999871 mart 1w 0143 0430 0585 0745 09238 098980 099656 099844 0999651 mart 2w 0178 0432 0598 0750 09274 098830 099571 099829 0999625 mart 4w 0170 0431 0599 0753 09326 098950 099624 099846 0999754 mart 8w 0183 0477 0633 0775 09435 099163 099720 099885 0999819 table 1 roc values boosted trees shown heights 11 roc curves corresponding 11 methods described text roc curves evaluated horizontal values given top row table show numerical values roc curves values smaller 05 given 3 signicant places values close 1 given dierence 1 may computed 3 signicant places region 010 020 weighted unweighed mart models tend better larger sample sizes use weights sometimes helps sometimes hurts seem make much dierence mart models respond global local features data anticipated weighting might help global portion local one appear weights greatly accelerate mart also investigated boosted trees using evaluation copy mineset unable obtain results better logistic regression data appear benet using empirical likelihood weights even boosting stumps global nature 6 discussion results empirical likelihood based data squashing encouraging original paper dumouchel et al 1999 outline dierences describe positive results might expected first based comparisons primarily quality estimated logistic regression coecients like get good results coe cients nd diminishing returns classication performance also compare predicted probabilities squashed models predicted probabilities full data set probabilities deterministic functions coecients wont show diminishing returns way mis classication rates second dierence report results local methods addition global ones found little benet area ambitious squashing described dumouchel et al 1999 might able make big improvement thirdly reasonable expect optimistic results entirely appropriate one data set another data sets need investigated data set 7 predictors used 38 consequence able look interactions consider sample sizes large enough reasonable match interaction moments case data sets comparable total size 744963 records compared 92000 point original motivation squashing speed although much article stresses accuracy reason essentially speed gains achieved sampling squashing represent gain sampling accurate n diminishing returns suggest small n squashing could much better sampling larger n practical value disappear suggests squashing useful problems even one lls computer memory data one undersampling settings maximize promise squashing first problems near zero bayes error might benet squashing secondly classication one needs compute score right side threshold problems one must predict numerical value eg prot versus protable diminishing returns might set much later third records 7 38 predictors large n memory records many thousands millions predictors much smaller values n memory could gain form squashing finally squashing described dumouchel et al 1999 might serve good data obfuscation device organization could release squashed training data set squashed test set researchers evaluate learning methods without ever releasing single condential data record acknowledgements thank bruce hoadley valuable discussions data mining jerome friedman making available early version mart code work supported nsf grants dms9704495 dms0072445 r scaling clustering algorithms large databases guide simulation second edition methods numerical integration 2nd squashing additive logistic regression statistical view boosting stochastic simulation tr