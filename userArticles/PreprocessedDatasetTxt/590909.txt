knowledge extraction transducer neural networks previously neural networks shown interesting performance results tasks classification still suffer insufficient focus structure knowledge represented therein paper analyze various knowledge extraction techniques detail develop new transducer extraction techniques interpretation recurrent neural network learning first provide overview different possibilities express structured knowledge using neural networks analyze type recurrent network rigorously applying broad range different techniques argue analysis techniques weight analysis using hinton diagrams hierarchical cluster analysis principal component analysis may useful providing certain views underlying knowledge however demonstrate techniques static lowlevel interpreting recurrent network classifications contribution paper particularly broad analysis knowledge extraction techniques furthermore propose dynamic learning analysis transducer extraction two new dynamic interpretation techniques dynamic learning analysis provides better understanding network learns transducer extraction provides better understanding network represents b introduction lot interest lately knowledge structures representation articial neural networks holldobler 1990 kurfe 1991 sperduti et al 1995 wermter 1995 hallam 1995 medsker 1995 sun 1995 wermter et al 1996 elman et al 1996 craven 1996 wermter 1999 articial neural networks connectionist networks already demonstrated interesting learning results various classication tasks however continues dicult understand underlying representations within connectionist networks lead perfor mance better understanding connectionist representations learned important improving credibility computational technique also improving network performance integration possibilities symbolic representations several attempts made interpret connectionist networks focusing feedforward networks particular andrews diederich 1996 abe et al 1993 shavlik 1994 stance visualizations internal activations weight strengths used get impression internal knowledge hinton 1986 gorman sejnowski 1988 eort also made reduce network size order simplify knowledge expressed therein elimi 28 wermter nating small weights furthermore groups similar weights replaced average strength shavlik 1994 addition techniques hierarchical cluster analysis used interpret connectionist networks never theless often interpretation dynamics learning process underlying knowledge neglected especially case dynamic recurrent neural networks interpretation recurrent networks dicult nonrecurrent feedforward networks since previous context recurrent networks important dynamic uence within networks internal states recurrent networks depend input also internal state local memory based previous inputs elman 1995 giles omlin 1993 omlin giles 1996 reason date focus primarily smaller recurrent networks articially generated data instance interesting current approach interprets training srn network two input two output two internal elements learning sequence n b n wiles elman 1996 discovered network behaved like spiral moved x point whereas seems plausible interpretation behavior recurrent networks trained learning sequences n b n dierent interpretations required move dierent tasks data sets closer realworld scenarios past developed large real world system spoken language analysis makes extensive use srn networks wermter weber 1997 wermter meurer 1997 spoken input recognized speech recognizer analyzed syntactic semantic dialog levels based incremental analy sis parallel syntactic semantic interpretation robust processing errors date however yet possible focus interpretation learning process interpretation connectionist knowledge paper primarily concerned detailed interpretation learning behavior well symbolic interpretation learned knowledge training order carry detailed analysis concentrate syntactic transformation task representative task largescale speechlanguage system task recurrent network process sentences associate syntactic classes phrasal level eg noun phrase prepositional phrase etc using task analyze recurrent neural network using many dierent techniques structured paper follows first introduce representative syntactic transformation task dene illustrate dynamic learning analysis b weight analysis c hierarchical activation analysis component activation analysis e transducer extraction rigorously compare techniques network data set argue dierent techniques provide mutually complementary interpretations contribution paper particularly broad concrete analysis knowledge extraction process done furthermore propose dynamic learning analysis transducer extraction two new interpretation techniques dynamic learning analysis provides better understanding network learns transducer extraction provides better understanding network represents 2 extracting structured knowledge using syntactic analysis task order examine number dierent techniques extracting structured knowledge connectionist networks rigorous manner focus particular task spoken language environment wermter lochel 1996 wermter weber 1997 wermter meurer 1997 trained many variations srn networks elman 1991 many sentences using various corpora several thousand words based corpus sentences domain scheduling appointments 2355 words table 1 summarizes accuracy label assignment unknown test set related experiments results reported elsewhere detail wermter lochel 1996 wermter weber 1997 wermter meurer 1997 wermter 1998 want illustrate realworld network performance table 1 focus knowledge extraction transducer neural networks 29 ever analysis process extracting explicit knowledge implicitly learned knowl edge paper concentrate syntactic phrasal assignment marked table 1 table 1 performance networks test set appointment scheduling corpus task accuracy test set basic syntactic disambiguation 89 basic semantic disambiguation 86 syntactic phrasal assignment 84 semantic phrasal assignment 83 dialog act assignment 79 word repair detection 94 phrase repair detection 98 demonstrate process knowledge ex traction use 15 sentences containing 76 words domain appointment scheduling illustration purposes concentrate learning syntactic phrasal assignment task sequence basic categories words associated sequence abstract syntactic categories actually occurring syntactic basic categories noun n verb v adverb adjective j preposition r determiner pronoun u abstract phrasal categories noun group ng verb group vg prepositional group pg task recurrent network learn assign phrasal categories basis basic syntactic categories order support robust understanding spontaneously spoken language show example utterances corpus together syntactic categories basic phrasal level 1 u ng thought v vg r pg 2 u ng v vg ng thursday n ng r pg easter n based seven basic syntactic three phrasal syntactic categories use srn network seven input units three internal units three output units networks actual system contain categories trained several thousand words illustration purposes restrict smaller network learning rate 005 momentum 09 weight updates performed incrementally training pattern training pattern consisted basic syntactic category input layer abstract phrasal category output layer figure 1 shows simplied example recurrent network task syntactic phrase assignment output input context layer aaaa aaaa aaaa noun group prepositional group verb group pronoun noun adjective verb adverb preposition determiner fig 1 recurrent network knowledge extraction syntactic phrase assignment activation output element j time srn networks computed basis weighted activation h incoming connections limited logistic function f activation element internal layer h l computed similar manner activation input layer k time used activation internal layer previous time step 1 wermter 3 dynamic learning analysis knowledge structuring lazy learning past work knowledge structures connectionist networks focused static connectionist network representations however important insights gained examining certain knowledge structures emerge develop time certain task learned completely frequently interpretation learning behavior demonstrated means learning curve overall error reduction time however learning curve rst step detailed analysis provide preliminary hints performance network training time figure 2 shows learning curve overall sum squared error time patterns 50000 100000 150000 2000000206error fig 2 learning curve syntactic phrasal assignment learning curve shows speed learning diers substantially time see dierent stages learning process beginning learning proceeds fast later learning slower takes longer make signicant improvements instance 70000 140000 seems learning nish nal signicant improvement examine network reaches performance start analysis directly random initialization weights state learning starts want give overview overall performance input patterns dierent time steps ef fect show error 76 patterns demonstration set dierent time steps figure 3 shows individual error 76 patterns trainingindividual patterns fig 3 performance individual patterns learn ing based random initialization patterns show relatively high error point expected values output element dier desired value 0 1 05 therefore expected error individual pattern three output elements expected error value conrmed gure shown gure 2 error decreases quickly start training state 100 patterns training set shown gure 4 first observe 100 training pat terns error 76 patterns shown could reduced signicantly patterns still show high error obviously network started learn patterns selectively knowledge extraction transducer neural networks 31 individual patterns patterns patterns fig 4 performance individual patterns 100 training patterns detailed analysis revealed patterns lower error exactly patterns belong noun group ng 100 patterns network recognized global error minimized signicantly focusing ng patterns since patterns occur frequently instance prepositional groups verb groups therefore rst network learned constant mapping patterns noun group since reduces overall error stage explains certain patterns gure 4 still exhibit high error others low error patterns low error exactly patterns classied correctly noun groups figure 5 shows detailed performance patterns network learned constant mapping ng observe performance ng patterns improved even however also observe v g patterns learned detailed analysis output preferences reveals stage addition ng v g patterns also learned correctly also demonstrated gure 5 remaining error patterns stage patterns belong prepositional group pg still categorized noun groups ng ng patterns v g patterns classied correctly network learned frequent ng patterns second frequent v g patterns learned thus one could state network pursues conservative lazy learning strategy learns frequently occurring simple regularities rst individual patterns patterns patterns patterns fig 5 performance individual patterns 600 training patterns afterwards network attempts improve patterns especially remaining patterns prepositional groups pg occurring nouns pronouns determiners adjectives either part pg patterns ng patterns order resolve potential ambiguity previous context must used learn correct class assignment example conservative lazy learning strategy network individual patterns 3 exceptions pg patterns patterns ng patterns vg patterns fig 6 performance individual patterns 3000 training patterns wermter since rst network learned patterns need previous context knowledge category assignment simple noncontextdependent category assignments learned patterns learned require context previous pattern assignments state network 3000 patterns shown gure 6 patterns classied correctly exception three comparing g ures 5 6 remaining error individual patterns could reduced signicantly learning pg patterns necessary network integrate local preceding con text 150000 patterns regularities learned shown gure 7 comparison gure 6 point smaller scaling vertical axis stage patterns learned even though dierences error rates individual patterns order reach 100 correctness training set may necessary give reasonably good state certain stage order reach even better stage later also ected global learning curve gure 2 individual patterns ng patternsvg patterns pg patterns correct fig 7 performance individual patterns 150000 training patterns general network pursues conservative lazy learning strategy first simple frequently occurring generalizations one category learned network cannot minimize error signicantly frequently occurring categories integrated fur thermore patterns learned require previous local context patterns learned require context correct category assignment otherwise ambiguous input finally remaining exceptions learned conservative learning process may possible overall error increases brie order reach better overall state later 4 weight analysis knowledge extrac tion visualizations internal weight strengths used get impression internal knowl edge experiments training set learned correctly 150000 patterns start analysis start weight analysis since weights provide lowest level interpretation connectionist represen tation figure 8 shows weights network three dierent time steps illustrated weights change time learning gure identiers source connectionist elements shown horizontally identiers goal connectionist elements shown vertically start horizontal axis left right see weights threshold element input connectionist elements syntactic basic categories n j v r u three internal elements three context elements c3 vertical axis top bottom see weights three internal elements output elements representing abstract syntactic categories vg ng pg knowledge extraction transducer neural networks 33 100 patterns 600 patterns 150000 patterns fig 8 weight analysis beginning training 100 patterns training 600 patterns training 150000 patterns white boxes represent positive weights black boxes negative weights size boxes corresponds size weights copy connections internal layer context layer changed therefore shown since always equal 1 start analysis rst third gure 8 random initialization rst third shows weights network 100 patterns point ng patterns classied correctly patterns learned yet network learned constant output order reduce overall error much possible see gure 8 network produced constant ng class 34 wermter see weights input elements syntactic basic categories n j v u internal elements relatively small similar holds weights context elements c1 c2 c3 internal elements due random initialization beginning train ing weights internal elements output elements abstract syntactic categories negative v g pg internal elements ng close 0 reason network produces constantly ng category stage focus state network presenting 600 patterns also shown gure 8 point ng v g patterns assigned correctly also ected weights observe positive weights n internal elements positive weights internal elements ng however see negative weights v internal elements internal elements v g pg patterns categorized correctly point one reason pg patterns depend signicantly previous con text however point network learned obvious preferences starting change weights context layer network state 150000 patterns shown bottom gure internal layer distributed representation developed therefore direct interpretation easily pos sible however observed rst internal element primarily important pg detec tion second internal element plays important role v g assignment third internal element important ng nevertheless distributed rather local representation additional uence ele ments furthermore weights context layer c h changed necessary order learn pg group assignment generally speaking explain certain phenomena using type weight analysis lowest interpretation level network however dicult extract explicit knowledge deeper understanding behavior network directly weights reasons diculty include 1 static representation weights show dynamics recurrent network 2 distribution weights activation 3 number weights especially case larger networks fore eort could made reduce size network eliminating small weights furthermore groups similar weights could replaced average strength nonethe less weight analysis still detailed larger networks 5 component activation analysis knowledge extraction weight analysis focuses weights provides lowlevel analysis one way address problem move towards activation analysis activations internal elements analyzed since internal elements receive activation number weighted connections activation internal element integrates several weighted connections provides higher abstraction level analysis order demonstrate analysis performed use srn network introduced previous section store vector representations internal layer pattern vector representations constitute input cluster algorithm provides hierarchical representation form dendrogram vectors similar vector representations end cluster figure 9 shows initial part patterns clustered according internal activations clearly observed internal representations ect classication according three classes pg based weights internal layer learned representations particularly support classication single word appear dierent contexts lead different internal representations instance word shown two dierent represen tations one representation use part ng class part pg class therefore nd representations dierent positions within dendrogram knowledge extraction transducer neural networks 35 allung iung iung usung weung wednesdaynng morningnng inrpg weeknpg isvvg comevvg mustvvg isvvg fig 9 hierarchical cluster analysis internal classication representations visibility purposes portion shown 6 principal component analysis knowledge extraction another kind analysis used interpreting internal representations clas sications principal component analysis figure shows result analysis current task vectors internal layer corresponding identiers provide input principal component analysis vectors dier substantially depicted gure large distance also observed internal representations ect preference mappings learned three category classes ng v g pg patterns distributed across dierent areas thus classication internal representations clearly seen shows network actually learned classication task well learning completed internal representation characterizes preference map ping according cluster analysis principal component analysis similar internal vector representations responsible representation similar preference assignments equal cate gories however interpretation weights means hinton diagrams activations via cluster analysis principal component analysis provides limited form structuring extracted knowledge 36 wermter iung inrpg thursdaynng morningnng weung unsung onapg thedng isvvg wednesdaynng iung comevvg mustvvg iung thatung isvvg allung tuesday couldvvg usung letvvg usung dasung isvvg makevvg tillrpg weung inrpg marchnpg otherjng thatung isvvg fig 10 principal component analysis internal classication representations 7 transducer extraction words sequences words represented syntactic semantic pragmatic category preferences input instance srn networks input representing sequence category preferences associated sequence corresponding output preferences simple description sequence analysis similar function synchronous sequential machines booth 1967 kohavi 1970 shields 1987 although preferences learning yet considered machines therefore shall focus extensions synchronous sequential machines representing sequential knowl edge especially synchronous moore machines start basic denition synchronous sequential machine also called transducer denition synchronous sequential chine transducer synchronous sequential machine tuple 1 nite nonempty sets input output 2 nonempty set states 3 function f state transition function 4 function f output function output depends state input machine socalled mealy machine output function f output depends state machine later socalled moore machine output function f synchronous sequential machines sometimes called transducers sequential machine assigns output new state input old state done whole sequence inputs states discrete time set necessarily nite booth 1967 although assumed case nite machines whereas automata knowledge extraction transducer neural networks 37 acceptors languages decide whether certain input belongs corresponding grammar sequential machines transducers change internal states dynamically depending inputs previous states also providing output input mealy moore machines slightly dier ent moore machines determine state rst afterwards state used provide output contrast output mealy machine depends also directly current input however shown moore machine equivalent mealy machine vice versa booth 1967 hopcroft ullman 1979 case concentrate moore machines since output certain neural networks based internal state holds stance feedforward networks srn networks whereas sometimes sun 1995 sequential machine used model single element neural network want use sequential machine description whole network also motivated fact real neuron systems seen physical entities perform state transitions churchland sejnowski 1992 specify language knowledge describing moore machines state transition function f output function f also integrate f f function f os f corresponds instance transformation within srn network specication moore machine could performed using state tables potential entry task assigning syntactic phrasal categories syntactic basic categories could verb current state prep group new state verbal group output verbal group may possible assign direct interpretation state reason simple identiers may used verb current state 4 new state 5 output verbal group possible dene state transition tables assign combination input current state output new state way symbolic synchronous sequential machine specied clear regularities known beforehand number limited tables composed manually however number input state combinations quickly gets large automatic procedures become necessary abovementioned state transition tables discrete symbolic therefore support gradual representations instance input state could ambiguous dierent gradual preferences could exist dierent inter pretations instance meeting could stronger preference syntactic interpretation noun smaller preference verb form consequently want use preferences input output states machines preferences type able take values 0 1 multiple preferences represented integrated extend single category verb ndimensional preference input mdimensional preference output obtain new synchronous machine call preference moore machine want describe synchronous sequential preference moore machine transforms sequential input preferences sequential output preferences see simple recurrent networks feedforward networks interpreted neural preference moore machines furthermore show symbolic neural knowledge integrated quite naturally using preference moore machines 38 wermter denition preference moore machine preference moore machine pm synchronous sequential machine characterized 4tuple nonempty sets inputs outputs states sequential preference mapping contains state transition function f output function f n ldimensional preferences values 0 1 n 0 1 0 1 l respectively generalized version preference moore machine shown gure 11 left preference moore machine realizes sequential preference mapping uses current state preference input preference assign output preference new state preference preference mapping states aaaa aaaa output input fig 11 neural preference moore machine relationship srn network describe new technique extracting knowledge within recurrent network form transducer symbolic transducer extracted recurrent network assigns input vector basic syntactic categories new output vector phrasal categories depending previous context network internal state context represented threedimensional vector simplicity strict symbolic interpretation threedimensional vector take 2 3 8 states order acquire symbolic interpretation network presented patterns training set stored internal state vectors hidden layer network output vector state vector next corner preference determined using euclidean distance metric thus euclidean distance metric assigned one three symbolic abstract syntactic phrase categories output vector one eight state number identiers state vector knowledge extraction transducer neural networks 39000 100010 110011 nng rpg vvg dng dng nng rpg ung dng nng nng dpg jpg npg vvg dng ang apg jng nng vvg vvg rpg vvg ung rpg nng nng dng ung nng rpg apg vvg vvg ung fig 12 transducer extraction recurrent network example sentence ung vvg dng thursday nng rpg easter nng figure 12 shows knowledge learned network extracted symbolic transducer corner nodes represent eight strict states center node represents start state trans ducer edges nd symbols single transductions input output categories separated colon eg ng means starting source state edge determiner preference assigned noun group preference ng transduction made end state edge extracted transducer see clear regularities certain states instance transductions state 100 primarily responsible assignments prepositional group pg examples transductions state 010 state 000 primarily responsible verbal group vg assignment furthermore gure 12 shows example transductions sentence thursday easter beginning start state center see transduction ng word assigns noun group ng pronoun u assigns verb group vg verb transductions ng ng assign noun group ng thursday finally transductions assign prepositional group pg sequence easter dierent abstract syntactic categories ng pg assigned category n depending learned previous context nng rpg vvg dng dng nng rpg ung dng nng nng dpg jpg npg vvg dng ang apg jng nng vvg vvg rpg vvg ung rpg nng nng dng ung nng rpg apg vvg vvg ung fig 13 transducer extraction recurrent network example sentence ung thought vvg rpg dpg next jpg week npg detailed less detailed transducers obtained state output vectors mapped fewer nodes thus general abstraction level symbolic transducer quite variable symbolic transducer represents abstraction detailed network knowledge abstraction also hides numerical complexity allows direct symbolic interpretation provides summary network behavior give example gure 13 shows transductions example sentence thought next week beginning start state center see transduction ng word assigns noun group ng pronoun u assigns verb group vg verb thought finally transductions r pg assign prepositional group pg word sequence next week one advantage transducer extraction higher abstraction level used representations recurrent network leads better understanding function original network contains detailed knowledge numerical weights activations possible see declarative sequential symbolic knowledge network represents extraction symbolic transducer allows better understanding learned sequential knowledge represented explicit manner knowledge extraction transducer neural networks 41 8 discussion analysis 81 comparison knowledge extraction technique previous work using individual techniques isolation interpreting neural networks extracting structural knowledge paper analyzed dierent techniques using trained network order interpret network knowl edge extensive comparisons detailed network knowledge needed order gain better understanding knowledge extraction represented neural networks also introduced two new techniques dynamic learning analysis transducer extraction dynamic learning analysis examines formation development categories time learning thus provides much deeper understanding neural network arrives learned representation transducer extraction developed represent sequential processing recurrent network higher level abstraction general found dierent interpretation techniques provide dierent views knowledge contained neural network thus single best technique dier ent aspects knowledge extraction use particular technique depends rather requirements interpretation table 2 illustrate summarize general properties dierent techniques dynamic learning analysis dla based output representations provides high level understanding based known output representations technique easy interpret used network types hand particularly support recurrent networks symbolic integration exible knowledge structuring furthermore structural relationships cannot extracted transducer extraction te new technique uses output representations well internal activations main advantages technique high level understanding form extracted symbolic transducer specic support sequentiality recurrent networks possibility extracting structural relationships extracted transducer integrated symbolic knowledge eg coded symbolic transducers dierent transducers generated exibility based number states used internal activation layer leads relatively straightforward interpretation network involved compared techniques also requires additional eort extracting symbolic transducer internal activations output representations compare dla caa see dla techniques specically provide high level interpretations dynamic learning processing argue wa haa caa techniques tendency towards general detailed lowlevel interpretation dla te however techniques specialized highlevel dynamic interpretation focusing output interpretations dynamics recurrent networks provides new level understanding whereas lot previous work focused lowlevels terpretation believe future higher levels interpretation knowledge extraction required 82 related work transducer extraction related work finite state automata transducers widely used various forms within traditional eg hopcroft ullman 1979 basically automata transducers always certain context state analyze certain word symbol move new state potentially generate new word sym bol using changing states possible encode sequential context although nite automata regular languages sucient describe possible constructions natural language completely see eg winograd 1983 automata still constitute central minimal requirement representation natural language thus occupy lowest level chomsky hierarchy languages hopcroft ullman 1979 furthermore possible design ecient realizations nite automata dierent domains kaplan 1995 eg 42 wermter table 2 comparison dierent knowledge extraction techniques dynamic learning analysis dla weight analysis wa hierarchical activation analysis haa component activation analysis caa transducer extraction te abbreviations activationsweightsoutputs lowmediumhigh network representations used w ao general level understanding h l h specic support recurrent networks l l l l h degree structural relationships l l h integration symbolic knowledge l l h flexibility level knowledge structuring l l h computational eort l l easiness interpretation h l h generality portability networks h h h h morphology lexicon access information extraction sentences syntactic tagging etc recurrent networks potential learn sequential preference mapping f automatically based input output examples see gure 11 whereas traditional moore machines fuzzysequentialfunctions santos 1973 involve manual encoding recently illustrated srn networks emulate symbolic moore machine nite automaton kremer 1995 kremer 1996 also shown however goudreau giles 1995 goudreau et al 1994 recurrent network single input layer one context layer one output layer socalled single layerrstordernetwork sucient realization arbitrary nite automata natural language processing representations least powerful nite au tomata consequently singlelayerrstorder networks appropriate used srn networks recurrent networks contain nite transducers special case also support much powerful properties based gradual mdimensional preference representations instance could shown srn networks emulate certain restricted properties pushdown automaton particular recursive representation structures limited depth elman 1991 wiles elman 1996 apart traditional symbolic regular rep resentations gradual learned representations also represented furthermore number input state output preferences necessarily nite therefore neural preference moore machines powerful nite transduc ers recurrent neural networks seen learning augmenting simple nite symbolic transducer respect learning within gradual preference space perspective symbolic knowledge special abstract region neural preference space important line research automata recurrent networks reported giles et al 1992 goudreau giles 1995 tino et al 1995 giles colleagues studied nite state automata neural networks substantial dierences research started often known nite state automaton used generate sequences sequences used training secondorder neural network using partition algorithm nite state automaton extracted network activations minimized compared original known nite state automaton way giles colleagues could study computational properties extraction particularly well nite state automata also frequently relied relatively simple 10 sequences knowledge extraction transducer neural networks 43 motivation methodology dierent several respects assume initial nite state automaton transducer known especially realworld problems interesting case one automaton known advance whereas interesting comparison sequence gen eration generating sequences nite state automaton already introduces certain regularities training set thus sequence generation important uence learning behav ior something want rule fact interested situations know machine ex tracted especially noisy realworld learning data underlying regularities may quite disparate regularly generated sequences furthermore task networks quite dierent secondorder networks employed giles colleagues trained recognition output layer represents state representations fed back input layer next step recurrent networks perform assignment task sequence inputs associated sequence outputs determining whether certain sequence belongs certain automaton simple structure sequence interested transducer extraction rather recognizer ex traction general designated nal states networks since network extracted symbolic transducer produce output long input provided transducer behavior therefore quite dierent recognition performance reported giles omlin 1993 based acceptors articial languages 9 conclusion main contribution paper particularly broad analysis knowledge extraction recurrent networks addition propose dynamic learning analysis transducer extraction two new dynamic interpretation tech niques dynamic learning analysis provides better understanding network learns transducer extraction provides better understanding network represents learning conservative lazy learning strategy leads connectionist representations described symbolic transducers transducers allow much better interpretation sequential network knowledge compared standard analysis using hierarchical clustering hinton diagrams weight analysis cluster analysis principal component analysis detailed static contrast new method extracting symbolic transducers describe learned classication performance much bet ter since transducer extraction considers sequential character learned representations recurrent network allows better symbolic inspection possibilities direct integration symbolic classiers explored future work conclude dynamic learning analysis transducer extraction lot potential improved knowledge structuring based recurrent networks r extracting algorithms pattern classi rules networks sequential machines automata theory computational brain extracting comprehensible models trained neural networks distributed repre sentations language dynamical system learning extracted recurrent neural networks representing hybrid prob lems learning distributed representations concepts introduction automata theory finite state technology switching finite automata theory computational power elmanstyle recurrent networks theory grammatical induction connectionist paradigm hybrid intelligent systems extraction rules discretetime recurrent neural networks fuzzy sequential func tions framework combining symbolic neural learning introduction automata theory learning distributed representations classi finite state machines recurrent neural networks hybrid connectionist natural language processing hybrid approach arti preference moore machines neural fuzzy integration building lexical representations dynamically using arti screen learning syntactic semantic spoken language analysis using arti learning count without counter case study dynamics activation landscapes recurrent net works language cognitive process tr