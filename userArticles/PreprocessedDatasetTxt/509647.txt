performance characteristics gang scheduling multiprogrammed environments gang scheduling provides spaceslicing timeslicing computer resources parallel programs thread execution parallel job concurrently scheduled independent processor order achieve optimal level program performance timeslicing parallel jobs provides better overall system responsiveness utilization otherwise possible lawrence livermore national laboratory deployed three generations gang scheduler variety computing platforms results indicate potential benefits technology parallel processing less significant timesharing 1960s b introduction interest parallel computers propelled economics commodity priced microprocessors growth rate computational requirements exceeding processor speed increases symmetric multiprocessor smp massively parallel processor mpp architectures proven quite popular yet suffer significant shortcomings applied large scale problems multiprogrammed environments problems must first recast form supporting high levels parallelism achieve inherent parallelism performance necessary concurrently schedule cpu resources threads processes associated program system throughput frequently used metric success however ease use good interactivity fair distribution resources substantial importance customers multiprogrammed environment efficiently harnessing power multitude processors satisfying customer requirements difficult proposition schedulers mpp computers provide concurrent scheduling spaceslicing schemes program allocated collection processors retains processors completion program scheduling critical yet decision unknown impact upon future job scheduled risk blocking larger jobs later processors left idle anticipation future arrivals lack timeslicing mechanism precludes good interactivity high levels utilization gang scheduling solves dilemma combining concurrent resource scheduling spaceslicing timeslicing impact scheduling decision limited timeslice rather jobs entire lifetime empirical evidence gang scheduling cray t3d installed lawrence livermore national laboratory llnl demonstrates additional flexibility improve overall system utilization responsiveness smp computers provide spacesharing timeslicing schedule process independently good parallelism may achieved way lightly loaded system luxury rarely available purpose gang scheduling environment improve throughput parallel jobs concurrent scheduling without degrading either overall system throughput responsiveness moreover scheme extended scheduling parallel jobs across cluster computers order address larger problems gang scheduling dec alpha computers llnl explored standalone cluster environments shown fulfill expectations overview gang scheduling term gang scheduling refers programs threads execution grouped gang concurrently scheduled distinct processors furthermore timeslicing supported concurrent preemption later rescheduling gang 4 threads execution necessarily posix threads components program execute simultaneously threads may span multiple computers andor unix processes communications threads may performed shared memory message passing andor means concurrent scheduling jobs threads shown improve efficiency individual parallel jobs system 3 13 jobs perspective similar dedicated machine timeslices execution reduction io bandwidth may experienced due interference jobs cpu memory resources dedicated job efficiency improvements results reduction communications latency promoting finegrained parallelism system efficiency improved reductions context switching virtual memory paging cache refreshing advantages gang scheduling similar timesharing uniprocessor systems ability preempt jobs permits scheduler efficiently utilize system several ways long running jobs high resource requirements executed without monopolizing resources interactive high priority jobs provided near realtime response even jobs high resource requirements periods high system utilization jobs high processor requirements initiated timely fashion without waiting processors made available piecemeal fashion jobs terminate low priority jobs executed provided otherwise unused resources preempted higher priority jobs become available high system utilization sustained wide range workloads job preemption incur additional overhead cpu resources sacrificed context switching slight explored later paper increase storage requirements possibly substantial running preempted jobs must storage requirements satisfied simultaneously preempted jobs must also vacate memory jobs memory contents written disk acquisition additional disks order increase utilization cpu memory resources likely costeffective need must considered approaches parallel scheduling mpp computers use variable partitioning paradigm variable partitioning job specifies processor count requirement submission time processors allocated job retained termination inability preempt job prevent timely allocation resources interactive high priority jobs shortage jobs small processor counts result incomplete allocation processors conversely execution job high processor requirements delayed fragmentation necessitates accumulation processors piecemeal fashion multiple jobs smaller processor counts terminate variable partitioning result poor resource utilization due resource fragmentation 10 18 processors left idle anticipation high priority job arrival 12 processors left idle order accommodate jobs substantial resource requirements another option dynamic partitioning operating system determines number processors allocated job dynamic partitioning possess allure high utilization interactivity 11 15 16 make program development significantly difficult program required operate efficiently without knowledge processor count execution time variable processor count also causes execution time variability may unacceptable workloads containing long running jobs even moderate size jobs high priority mpp scheduling variable partition paradigm prevalent mpp architectures lack job preemption makes responsiveness particularly difficult provide order ensure responsive service mpp computers divide resources pools reserved interactive jobs batch jobs available job type pools also used reserve portions computer specific customers systems since jobs normally span multiple pools partitioning computer fashion reduces maximum problem size addressed fragmentation reduces scheduling flexibility system utilization optimal configuration places resources single pool available job type assuming support resource allocation interactivity provided means scheduling spacesharing timesharing norm smp computers providing good interactivity utilization computers schedule process independent works well workload consisting many independent processes however solution large problems dependent upon use parallel jobs suffer significant inefficiencies without benefit concurrent scheduling 3 13 parallel job development efforts national energy research supercomputer center nersc illustrates difficulties parallel job scheduling 2 order encourage parallel job development nersc provided dedicated time parallel jobs cray c90 computer several parallel jobs running dedicated mode able achieve parallel performance cpu timewall time 155 16 cpu machine approach 10 gflops per second 16 gflops per second peak speed batch jobs suspended dedicated period interactive jobs initially permitted execute concurrently parallel job several instances observed single computebound interactive program reducing parallel jobs throughput system utilization almost 50 percent suspension interactive jobs found necessary order achieve reasonable parallel job performance benefits smp computers scaled larger problems clustering efficient execution parallel jobs environment requires coordination scheduling across entire cluster communications threads consisted exclusively message passing possible base scheduling upon message traffic achieve high level parallelism 13 14 llnl workload makes extensive use shared memory communications preventing us pursuing strategy llnl workload characterization llnl customers long relied upon interactive supercomputing program development rapid throughput jobs short moderate execution times work performed smaller computers customers use target platform reasons problem size hardware compatibility software compatibility llnl began transition parallel computing acquisition bbn tc2000 1989 additional parallel computers llnl included cray c90 cray t3d meiko cs2 ibm sp2 many problems substantial size well parallelized llnl cray t3d workload typical parallel computers model llnl 256 processors 64 megabytes dram processors configured single pool available job llnl cray t3d configured permit interactive execution jobs 64 processors 2 hours execution time interactive jobs account 67 percent jobs executed consume 13 percent cpu resources delivered interactive workload typically ranges 64 256 processors 25 100 percent computers processors working hours drops negligible level late night hours peak interactive workloads reach 320 processors 25 percent oversubscription rate timely execution interactive jobs dependent upon preemption batch jobs extreme cases timesharing processors among interactive jobs large jobs account high percentage resources utilized cray t3d shown table 1 memory requirements quite variable jobs use 42 56 megabytes per processor due cray t3d hardware constraints contiguous block processors specific shape must made available execute job 1 making fragmentation particularly onerous problem gang scheduling permits us preempt jobs needed execute large jobs timely fashion minimal overhead job size cpu cpu utilization table 1 cpu utilization job size cray t3d llnl programs use either pvm parallel virtual machine mpi message passing interface libraries communications threads although cray t3d distributed memory architecture support shared memory programming model permitting job read write memory local another processor assigned job shared memory programming model considerably lower overhead message passing widely used small number programs utilize combination paradigms shared memory within cray t3d message passing communicate threads serial program components executing cray ymp frontend multiple paradigm model expected common dec alpha cluster ibm sp2 containing smp nodes llnl gang scheduler design strategy llnl customers expect rapid response even heavily utilized multiprogrammed computer however need rapid response uniform across entire workload order provide interactivity required minimize overhead context switching divide workload six different classes job class significantly different scheduling characteristics described express jobs deemed management mission critical given rapid response optimal throughput interactive jobs require rapid response time good throughput extended working hours jobs response time throughput reduced hours sake improved system utilization throughput production jobs debug jobs require rapid response time extended working hours jobs response time reduced hours sake improved system utilization debug jobs preempted cray t3d production jobs require rapid response time receive good throughput night weekends benchmark jobs require rapid response time preempted standby jobs low priority suitable absorbing otherwise idle compute resources default classes production batch jobs debug totalview debugger initiated jobs interactive jobs directly initiated user terminal jobs placed express class system administrator benchmark standby classes specified user job initiation time job class number scheduling parameters including relative priority processor limit also several systemwide scheduling parameters aggregate processor limit gang scheduled jobs scheduling parameters altered realtime permits periodic reconfiguration llnl emphasizes interactity extended work hours 730 1000 pm throughput times gang scheduler daemon also updated time upon receipt appropriate signal completion critical tasks daemon writes state file initiates new daemon program sockets utilized user communications occur job initiation sockets also used job state change requests scheduling parameter changes rare job processor state information written periodically data file globally readable file read gangster application users window gang scheduling status delegated issue fair resource distribution systems including centralized user bank cub 8 distributed production control system dpcs 17 systems provide resource allocation accounting capabilities manage entire computing environment single tool systems exercise coarse grained control scheduling batch jobs fine grained control adjusting nice values interactive batch processes gang scheduler designed recognize changes nice value schedule accordingly jobs classes preempted automatically changed standby class high nice value returned requested class upon reduction nice value mechanism proven function well managing resource distribution minimizing interdependence systems significant differences exist three llnl gang scheduler implementations differences largely result architectural differences based upon experiences previous implementations implementation described results bbn tc2000 order pioneer parallel computing llnl bbn tc2000 126 processors acquired 1989 tc2000 shared memory architecture originally supported spacesharing gang scheduler tc2000 reserves resources system startup controls resource scheduling time 6 7 user programs require code changes communicate gang scheduler however program must load modified version mandatory parallel program initiation library rather securing resources directly operating system library secures resources gang scheduler daemon program may also increase decrease processor count execution explicitly notifying gang scheduler daemon otherwise job must continue execution specific processors originally assigned timesharing performed use sleep wake signals issued concurrently threads job mechanism provide rapid context switches order milliseconds implementation fair share mechanism objective providing user access comparable resources gang scheduler would determine resource allocation made ten second timeslice objective insuring interactivity equitable distribution resources high utilization gang scheduler implemented tc2000 able provide good responsiveness throughput shortcomings noted despite shared memory architecture tc2000 possible relocate threads order better balance continuously changing workload scheduler could react workload changes timeslice boundaries limited responsiveness user based fair share mechanism abandoned later implementations due availability independent comprehensive resource allocation system cray t3d cray t3d massively parallel computer incorporating dec alpha 21064 microprocessors capable 150 mflops peak performance processor local memory system configured nodes consisting two processors local memory network interconnect nodes connected bidirectional threedimensional torus communications network also four synchronization circuits barrier wires connected processors tree shaped interconnect 1 getting great detail t3d severely constrains processor barrier wire assignments jobs must allocated processor count power two minimum two processors one node jobs built run valid processor count processor count change execution begins processors allocated job must specific shape specific dimensions given problem size example allocation processors must made contiguous block eight processors x direction two processors direction two processors z direction furthermore possible locations processor assignments restricted specific shapes locations processor assignment result barrier wire structure jobs must allocated one four barrier wires initiated barrier wire assignment job change job relocated circumstances two jobs sharing single barrier wire may located adjacent prior installation gang scheduler cray t3d forced make several significant sacrifices order satisfy customers need interactivity 5 9 execution time batch jobs restricted insure reasonable responsiveness jobs large processor requirements interactive jobs although one may argue delays order hours may reasonable batch jobs limited four hours one batch queue permitted execution times 19 hours queue enabled brief periods weekends holidays since jobs could preempted batch workload dramatically reduced 400 batch jobs completed released resources might remain unused interactive job many hours times heavy interactive use initiation interactive job wait jobs terminate release resources processor allocation restrictions also made severe fragmentation problems interactivity generally good processor utilization rate 33 percent considered unacceptable cray t3d gang scheduler implementation several significant differences bbn tc2000 since initiation parallel jobs t3d conducted single application program placed wrapper around communicate gang scheduler daemon changes user application scripts required fixed timeslice period replaced event driven scheduler ability react instantly changes workload allocation specific processors barrier wires dramatically effect performance t3d substantial effort placed developing software optimize selections processor selection criterion includes bestfit packing placement jobs similar scheduling characteristics close proximity minimizing contention barrier wire circuits context switching logic also required substantial modification bbn tc2000 supported memory paging cray t3d lacks support paging cray t3d job preemption results entire memory image preempted job written disk processors reassigned requires one second per preempted processor case 64 processor jobs context switched one minute required store preempted jobs context another minute load state another job similar size t3d gang scheduler calculates value job including processor count job type location disk memory make job preemption initiation decisions additional information associated job class used calculation include maximum wait time processor limit donotdisturb time multiplier minimum timeslice job product jobs processor count class donotdisturb time multiplier minimum timeslice mechanism reduce responsiveness prevents costly thrashing jobs memory disk critical maintaining high utilization level cray t3d gang scheduler operation since march 1996 able dramatically modify batch environment fully subscribe machine day oversubscribe night much 100 percent normal mode operation daytime interactive class jobs preempt production class jobs shortly initiation interactive jobs usually continue execution completion without preemption night processors oversubscribed temporarily express purpose executing larger jobs 128 256 processor strategy results eight percent jobs ever preempted batch queue long running jobs substantially reconfigured time limit increased 19 40 hours maximum processor allocation running jobs queue increased 64 128 processors enabled times system utilization increased substantially weekly cpu utilization rates 96 percent sustained figure 1 shows monthly cpu utilization period 15 months three different schedulers utilized period unicos max native cray t3d operating system cray research djm distributed job manager parallel job scheduler originally developed minnesota supercomputer center substantially modified cray research llnl developed gang scheduler also shown cpu utilization reported cpu actually assigned program memory resident cpu utilization reduced three things 1 context switch time cpu unavailable programs image transferred disk memory 2 sets processors job fit packing problem 3 insufficient work particularly weekends figure 1 cray t3d cpu utilization benchmarks run unicos max early prototype llnl gang scheduler showed 21 percent improvement interactive job throughput reduction aggregate throughput cost moving jobs state memory disk provide responsiveness fully compensated efficient packing processor torus current implementation additional performance enhancements show aggregate throughput improvement percent figure 2 shows gangster display typical cray t3d daytime utilization note 12 256 processors six 128 nodes assigned job total ten interactive debug class jobs running using 148 processors half batch workload currently paged interactive jobs left side display shows contents node two processors letter indicates job period indicates unused node right side display describes job w field shows barrier wire used mmss field shows total execution time accumulated st field shows jobs state iswapping nnew job yet assigned nodes barrier wire oswapping oswapped rrunning wawaiting commencement execution assigned nodes barrier wire gang scheduler state information written disk 15 second intervals gangster display updated rate gangster 1325 42377 pettest 4 700 0 r 524 h f dbug susan 81280 aaaa8123 g prod mahdi 83818 ge 64 420 0 r146243 h prod u eduardo 79879 new 64 1 w prod delarub 79890 new 64 1 prod n tomas 80331 mdterrep figure 2 typical daytime gangster display cray t3d utilization rate quite satisfactory responsiveness also great importance responsiveness quantified slowdown ratio total time spent system run time three week period july 23 august 12 2659 interactive jobs executed using total 445 million cpuseconds 1328 batch jobs executed using total 2895 million cpuseconds slowdown aggregate interactive workload 18 viewed quite favorably investigation shows great deal variation slowdown longer running interactive jobs enjoy slowdowns percent interactive jobs executed daytime typically begin execution within seconds preempted interactive jobs executed late night early morning hours experienced slowdowns high 1371 one second job delayed 23 minutes however computer configured high utilization batch job execution hours high slowdowns unexpected dec alpha digital unix 40d operating system includes class scheduler essentially fine grained fair share scheduler class scheduler permits processes process groups sessions grouped class class allocated share cpu resources example eight threads job could placed single class allocated 80 percent cpu resources ten cpu computer class scheduler explicitly reserve eight cpus eight threads parallel job net effect close operating system also recognizes advantage keeping process particular cpu avoid refreshing cache found class scheduler actually delivers percent cpu resources desired gang scheduling minimal thread migration processors job fails sustain target number runable threads class scheduler allocate cpu resources jobs order sustain high overall system utilization since parallel jobs normally registered digital unix operating system gang scheduler relies upon explicit registration use library highly desirable user code modification gang scheduler avoided impossible achieve time imbedding gang scheduler remote procedure calls directly mpi pvm libraries would free many users modify programs presently gang scheduled application must modified simple library calls including register job gang scheduler global gang scheduler job id returned register resource requirements cpu memory disk space requirements specified job computer gang scheduled register processes associate specific process process group session gang scheduler job id since necessary coordinate activities across multiple computers dec alpha gang scheduler returned fixed timeslice model used bbn tc2000 order manage timeslices across multiple computers concept tickets introduced tickets represent specific resource allocations specific times issued jobs spanning multiple computers jobs execute single computer preissued tickets managed computers gang scheduler daemon makes scheduling decisions start timeslice design permits computer operate independently possible permitting gang scheduling jobs across cluster needed tickets managed gang scheduler daemon computer cluster associated job lifetime job may given additional tickets revoked depending upon changes overall system load job also permitted alter resource requirements execution change number cpu required job may result revoked additional tickets gangster display program rewritten tcltk order provide richer environment detailed information system job status includes history cpu real memory virtual memory use updated timeslice boundaries information helpful system administrators programmers tuning systems example programs cpu allocation cpu use substantially different desired level parallelism achieved matter investigated dramatic changes real memory use program execution may indicate substantial paging also warrants investigation figures 3 4 show displays system program status c machine status figure 4 gangster display dec job status order insure good responsiveness preserved jobs gang scheduler permits processors reserved nongang scheduled jobs prevents gang scheduled jobs reserving processors computer entire timeslice could prevent logins interactions significant period gang scheduler daemon also reacts changes overall workload insuring resources fairly distributed active jobs gangster also permits several systemwide operations gang scheduled job including suspend resume kill change job class example kill job operation send kill signals processes registered gang scheduled job computers running job conclusions gang scheduling shown provide attractive multiprogrammed environment multiprocessor systems several ways parallel jobs provided access required resources simultaneously providing illusion dedicated environment interactive high priority jobs provided rapid response excellent throughput jobs large resource requirements initiated rapidly without wait multiple jobs terminate release resources high level utilization maintained wide range workloads experience cray t3d overwhelmingly positive sustain processor utilization rates 95 percent course week providing aggregate interactive workload slowdown less 20 percent performance give distributed memory mpp range performance spanning largescale parallel applications general purpose interactive computing experience dec alpha environment also favorable testing period rich interactive environment available fast smps true supercomputing class problems addressed harnessing power cluster parallel jobs plans call bringing two 80 cpu dec alpha 8400 clusters control gang scheduling fall 1997 r cray research inc dedicated computing ympc916 effective distributed scheduling parallel workloads survey scheduling multiprogrammed parallel systems improved utilization responsiveness gang scheduling timesharing massively parallel machines centralized user banking user administration unicos gang scheduler timesharing cray t3d dynamic processor allocation policy multiprogrammed sharedmemory multiprocessors analysis nonworkconserving processor partitioning policies dynamic coscheduling workstation clusters comparing gang scheduling dynamic space sharing symmetric multiprocessors using automatic selfallocating threads asat process control scheduling issues multiprogrammed sharedmemory multiprocessors distributed production control system new graph approach minimizing processor fragmentation hypercube multiprocessors tr process control scheduling issues multiprogrammed sharedmemory multiprocessors twodimensional buddy systems dynamic resource allocation partitionable mesh connected system dynamic processor allocation policy multiprogrammed sharedmemory multiprocessors effective distributed scheduling parallel workloads new graph approach minimizing processor fragmentation hypercube multiprocessors comparing gang scheduling dynamic space sharing symmetric multiprocessors using automatic selfallocating threads asat analysis nonworkconserving processor partitioning policies demandbased coscheduling parallel jobs multiprogrammed multiprocessors improved utilization responsiveness gang scheduling ctr adrian wong leonid oliker william c kramer teresa l kaltz david h bailey esp system utilization benchmark proceedings 2000 acmieee conference supercomputing cdrom p15es november 0410 2000 dallas texas united states junglok yu jinsoo kim seungryoul maeng runtime resolution scheme priority boost conflict implicit coscheduling journal supercomputing v40 n1 p128 april 2007 atsushi hori hiroshi tezuka yutaka ishikawa highly efficient gang scheduling implementation proceedings 1998 acmieee conference supercomputing cdrom p114 november 0713 1998 san jose ca shahaan ayyub david abramson gridrod dynamic runtime scheduler grid workflows proceedings 21st annual international conference supercomputing june 1721 2007 seattle washington gyu sang choi jinha kim deniz ersoz andy b yoo chita r das comprehensive performance energy consumption analysis scheduling alternatives clusters journal supercomputing v40 n2 p159184 may 2007 gyu sang choi jinha kim deniz ersoz andy b yoo chita r das coscheduling clusters viable alternative proceedings 2004 acmieee conference supercomputing p16 november 0612 2004 bin lin peter dinda vsched mixing batch interactive virtual machines using periodic realtime scheduling proceedings 2005 acmieee conference supercomputing p8 november 1218 2005