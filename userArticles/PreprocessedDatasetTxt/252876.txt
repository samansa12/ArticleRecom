estimation generalized multisensor hidden markov chains unsupervised image segmentation abstractthis paper attacks problem generalized multisensor mixture estimation distribution mixture said generalized exact nature components known belongs finite known set families distributions estimating mixture entails supplementary difficulty one must label class sensor exact nature corresponding distribution generalized mixtures studied assuming components lie pearson system adaptations classical algorithms expectationmaximization stochastic expectationmaximization iterative conditional estimation used estimate mixtures context independent identically distributed data hidden markov random fields propose general procedure applications estimating generalized multisensor hidden markov chains proposed method applied problem unsupervised image segmentation method proposed allows one identify conditional distribution class sensor ii estimate unknown parameters distribution iii estimate priors iv estimate true class image b introduction hidden markov chains useful tool tackling numerous concrete problems instance speech processing 27 communications 15 image processing 1 8 25 fall framework estimating discrete phenomenon observed noisy data noise often modelled gaussian many applications radar sonar ultrasound infrared magnetic resonance images noise necessarily gaussian 15 18 furthermore given sensor given class nature noise distribution vary time example form grey level sea surface radar images vary weather 6 thus may desirable determine automatically correct noise distribution class sensor given time early algorithms treating problem proposed 6 26 applied unsupervised image segmentation combine mixture estimation algorithms expectationmaximization em 7 28 stochastic expectationmaximization sem 20 22 iterative conditional estimation ice 23 24 recognition form distribution pearson system assuming given sample generated unique distribution present paper lies within scope general prob lem first propose multisensor generalized mixture estimation method based ice valid general hidden data context tailor method generalized multisensor hidden markov chain estimation effectiveness proposed method validated simulations algorithms applied problem unsupervised image segmentation image segmentation statistical methods based random field models set pixels consider two sets random variables called random fields takes values finite set takes values ir segmentation problem estimate unobserved realisation field x observed realisationy field observed image wo families methods global methods use hidden markov field models 2 4 5 9 11 16 19 31 32 local methods pixel classified observations local neighbourhood 13 20 21 22 30 efficiency global methods striking many cas es although local methods still remain interest 3 proposed 1 third method uses hidden markov chains instead hidden markov fields transformation twodimensional set pixels onedimensional set using hilbertpeano scans 29 results comparable obtained hidden markov field based methods added benefit sufficiently versatile treat spatiotemporal unsupervised segmentation problems use markov chains instead markov fields also affords computational advantages indeed several relevant distributions calculated analytically case markov chains whereas iterative estimation procedures like gibbs sampler would required using markov fields parameter estimation restoration algorithms 1 10 thus faster markov chain case furthermore use em algorithm complicated hidden markov field context modifications may required 4 32 paper organized follows next section address generalized mixture estimation problem broad setting order present icebased method section iii describes hidden markov chain model section iv present particular methods simulation 2results case monosensor hidden markov chains multisensor case treated section v applications problem unsupervised statistical image segmentation described section vi section vii contains conclusions ii generalized mixture estimation let us consider finite set random variables consider first monosensor case thus x takes values inomega takes values ir distribution x depends parameter ff denoted ff random variables s2s independent conditionally x distribution conditional x equal distribution conditional x thus distributions conditional x determined k distributions conditional respectively specified densities f respect lebesgue measure problem mixture estimation find ff f classical mixture case general forms f known depend parameter fi estimated instance f gaussian fi contains k means k variances generalized mixture case contrast general form f k known exactly however form f j assumed belong given finite set forms precise set families distributions instance f 1 may gaussian distributions f 2 gamma distributions f j belongs one families f know problem determining f twofold f j find family f f j belongs find parameter fixes f j f propose general algorithm called icegemi gemi generalized mixture solve problems based following assumptions ffx ff x available may simulate realizations x according distribution conditional family f j characterized parameter fi j ie f practice b j subset ir n j n j depending gaussian fi available sample z generated distribution rule available sample rule associates z best suited density among f according criterion call method icegemi seen generalization ice turn general method estimating hidden models particular estimating classical mixtures 23 24 icegemi algorithm iterative method step q let ff q f q k current prior parameters current densities f updating follows simulate x q realization x according dis tribution conditional based ff q k b calculate ff denotes conditional expectation given ff q f k calculation impossible calculate ff c g let q estimate parameters fi 1 g putting f q1 dy q multisensor case takes values ir shall assume independent conditionally x means given class observations different sensors independent thus f densities densities ir given densities ir note many practical situations probably strong assertion taking account stochastic dependence among sensors however non trivial context model study would necessary solve problem icegemi algorithm multisensor case differs little icegemi algorithm monosensor case roughly speaking formulas model stay valid except f replaced f 1 thus steps b icegemi described remain steps c e differ slightly follows become c g let q qr r sensor class parameters fi 1r apply sensor giving e class update f f q1 sets q known workload sensors times monosensor case resulting monosensor updates followed sensor update using product monosensor functions obtained remark applies noise parameter updates complete msensor estimation algorithm cannot reduced use monosensor algorithm times latter procedure would give particular different priors iii hidden markov chains present brief review hidden markov chain model sake simplicity describe monosensor case generalization multisensor case immediate shall assume xn n2in markov chain g stationary transition probabilities assume depend n thus initial distribution given transition matrix entries define virtue kolmogorovs theorem distribution x assume conditional structure described beginning section ii denote x realizations let socalled forward backward probabilities calculated following forward backward recursions initialization also let thus indicating psi computable using forward backward procedures multisensor hidden markov chain model formulas stay valid except replaced f 1 iv estimation generalized hidden markov chains icegemi algorithm consider first monosensor case verify assumptions parameter ff given 2 use empirical frequencies estimators shown conditional x nonstationary markov chain transition matrix time given defined 9 10 thus x simulated numerous standard families verifying conditions sample gn empirical cumulative distribution function gn n gm cumulative distribution functions associated distributions one base choice f j measure distance gn g j instance let k j kolmogorov distance gn g j define verified may apply corresponding icegemi algorithm following ae section ii note step b calculation fact ij c q1 results 9 11 multisensor case analogous replacing densities ir densities ir sequel denote icekolm icegemi based minimization kolmogorov distance b icepear algorithm algorithms proposed earlier work 6 26 use pearson family contains eight subfamilies first four moments pearson distribution determine subfamily belongs values parameters 17 methods 26 based sem algorithm stochastic variant em algorithm used estimate classical mixtures 20 icebased procedures call icepear later proposed 6 applications independently identically distributed samples hidden markov random fields model considered paper icepear slightly different resembles icegemi save steps c become following c g calculate four first moments kq decide family f ji 2 ff distribution lies calculate fi ji four first moments set fi q1 c icekolm icepear icegauss numerical results present numerical results unsupervised maximum posterior mode mpm restoration using meth ods compare efficiency cases involving nongaussian noise classical method assumes noise gaussian classical method used ice based mpm noise gaussian approaches theoretically also compare icekolm icepear procedure mpm restoration follows 2 ng ii 2 ng let class maximizing respect although chosen mpm algorithm socalled maximuma posteriori map algorithm could also ap plied map based principle analytic solution given viterbi algorithm 10 hidden markov chain model enjoys computational advantage hidden markov field model mpm used restoration step even pronounced using map hidden markov field model one must resort simulated annealing algorithm 11 time consuming note iterated conditional mode icm 2 used fast approximation map convergence ensured segmentation result depends strongly initialisation initialize three methods take c 0 c 0 kg calculate empirical mean empirical variance oe 2 sample noise distributions assumed gaussian variances equal oe 2 means distributed around distanced oe 2 take distributions first kind 14 f 2 denotes gamma distributions 14 f 3 denotes gaussian distributions along markov chain defined c 049 001 001 049 common marginal distribution transition matrix observe initialization use gives 22 far true values however results show poor initialization little effect efficiency different methods chain corrupted two noise sequences noise 2 noise 3 whose distributions presented figure 1 noises noise 1 noise 4 used sections vib vid influence measured theoretical blind error rates blind theoretical bayes error rates using true parameters estimating x ie without reference context order generate different noises used apply methods described 17 table results presented 12 lead following remarks 1 noises used rather strong comparing blind different mpm obtained real parameters one note advantage using markovian model considered 2 icekolm quite efficient recognizing nature mixture components icepear encounter difficulties 3 correct components determined noise parameters appear correctly estimated 4 restoration results obtained generalized mixture estimation always better obtained gaussian mixture estimation 5 error rates mpm based icekolm estimates often close error rates mpm based true parameters attests stability whole procedure v multisensor generalized hidden markov chains restoration multisensor hidden markov chains interest many situations example image pro cessing sensors different natures radar infrared optical ultrasound thus distributions corresponding given class different natures also nature noise vary class sensor given class given class sensor time briefly present section numerical results comparing icekolm icegaus multisensor case table ii additional results presented 12 lead following remarks twosensor experiments 1 results obtained generalized mixture estimation better obtained gaussian mixture estimation 2 error rates mpm based icekolm estimates still close error rates mpm based true parameters however less striking monosensor case stability whole procedure retained two sensors problem choosing maximum number useful sensors could arise vi unsupervised generalized image segmentation present section applications generalized mixture estimation icekolm ice pear unsupervised image segmentation use markov hidden chain model results previous sections apply almost immediately transform set pixels support set hidden markov chain proposed 1 use hilbertpeano scan showed results obtained case gaussian noise better obtained using classical raster scan model intuitively less satisfying hidden markov field model several simulations performed gaussian case reveal competitive 1 monosensor image segmentation let set pixels according hilbertpeano curve first three stages whose construction presented figure 2 starting four pixel image step multiplying image four continuation sequence creates hilbertpeano scan image size 2 n theta 2 n n hilbertpeano scans defined images size 29 random field classes observed field ie digital image segmented considering x making usual assumptions problem image segmentation one generalized hidden markov chain restoration b synthetic images consider family section iv two synthetic images letter b ring corrupted noise 3 noise 4 section ivc mentioned section ivc table noise perturbations quite sizeable priors equal 05 blind classification error rates 313 275 spectively images noisy versions noise 4 segmentation results presented figure 3 tables iii iv display families estimated icekolm icepear error rates given mpm based true distributions icegausmpm icepear mpm icekolmmpm also present results obtained unsupervised hidden markovfield based algorithm called icefield parameterestimation step done ice segmentation step mpm note use ising model simplest one complex models could produce better results note icekolm always efficient icepear advantage generally slight also pronounced letter bnoise 3 case hand icekolm icepear perform better icegaus cases letter bnoise 4 ringnoise 3 icegaus gives poor results advantage generalized mixture based methods quite apparent otherwise two simple examples show careful comparing method classical methods based hidden markov fields gaussian noises former clearly gives better results case letter b second takes upper hand case ring noise cases reveals structure images determines prior distribution plays important role c real images present results different segmentations three real images purpose twofold one hand show icekolm yields better results icegaus hand compare efficiency icekolm icefield address important problem estimating number classes paper although refer 20 procedure proposed using estimating classical mixtures sem quite useful fixed images according different visual impressions say clear general tendency appears hierarchy efficiencies methods considered subject particular image concerning image clouds figure 4 efficiency three methods seems comparable nearly distributions detected icekolm mal equivalence icekolm icegaus surprising fact results obtained icekolm icegaus comparable results obtained icefield indicates competitive used images kind speed san francisco image figure 5 icegaus gives clearly better results icefield reflects hidden markov chains suitable even outside generalized mixture considerations could due fact noted 3 hidden markov field based methods encounter difficulties detecting fine details hidden markov chain based methods seem better suited situation furthermore icekolmmpm clearly effective icegausmpm undoubtedly due fact icekolm detects beta gamma distributions leads us two conclusions first noise different forms associated different classes exist image second detection icekolm improve final unsupervised mpm segmentation results comparison computer times reliable computer programs optimized time used every method subject different numbers iterations subjectively chosen working ultra spark enterprise 2 case san francis co 256 theta 256 size respective times ice gausmpm icefieldmpm icekolmmpm 1 min 15 min 50 min shows icegausmpm interesting respect ice fieldmpm icekolmmpm rather time expensive however contrast icegausmpm icefieldmpm time icekolmmpm could undoubtedly reduced particular use whole subsets q maybe little superfluous use iteration subsets q could speed whole procedure significantly case amazo nia image figure 6 result obtained icefield mpm seems visually better obtained ice gausmpm suggests markov field model appropriate markov chain model wise icekolm detects three different forms noise dis tributions actually makes icekolmmpm effective icegausmpm shows advantage generalized mixture estimation comparison icefieldmpm icekolmmpm diffi cult latter seems restore fine details better difficult see real image whether details exist synthetic multisensor images segmentation multisensor icegaus icekolm based segmentations noisy letter b ring presented table v results allow conclusions analogous section v icekolm recognizes correct families efficiency corresponding unsupervised segmentation close method based true parameters visual results segmentations presented figure 7 real multisensor image segmentation consider section unsupervised segmentation real multisensor radar image area surrounding town sunbury pennsylvania colours image assigned different frequencies polarizations sirc radar consider two sensors seem complementary different visual aspect presented figure 8 present results twosensor icegaus icekolm based mpm segmentations 2 3 4 classes figure 9 rational comparison results difficult absence clear knowledge ground truth however note nature components varies class icekolm based mpm segmentation seems richer vii discussion addressed work problem estimating generalized multisensor mixtures applications generalized multisensor hidden markov chains specifi cally unsupervised statistical image segmentation prob lems contribution paper rests respect earlier works 6 26 three points 1 general family based iterative conditional estimation ice generalized mixture estimation methods proposed particular family valid iid case hidden markov fields hidden markov chains family provided f parametrized estimated separately 2 effectiveness particular method based ice kolmogorov distance frame generalized multisensor hidden markov chains shown using simulations 3 applications unsupervised image segmentation generalize results obtained gaussian monosen case 1 concerning results obtained using generalized hidden markov chains may state general conclusion component recognition parameter estimation quite efficient however multisensor case investigations would desirable order choose judiciously sensors among available used concerning unsupervised image segmentation results presented allow us put forth two conclusions first form noise indeed change class real images renders proposed techniques effective classical methods assume form noise classes second little said relative value proposed method classical hidden markov field based approaches general case inhomogeneous zones like urban areas present image markov chain based methods appropriate markov field ones close directions future work concerns proposed general method would desirable introduce nonparametric components study detected estimated icegemi another direction unsupervised generalized segmentation sequences multisensor images even 3d multisensor images flexibility proposed model renders well suited tasks suffices define hilbertpeano scan threedimensional set pixels preliminary results spatiotemporal segmentation monosensor gaussian im ages presented 1 encouraging although important problem estimating number classes remains devising reliable method estimating number classes remains definite interest authors thank professor titterington associated edi tor anonymous reviewers help improving readability paper work supported convention cnetint 93 pe 74 05 authors also thank bruno choquet danielle pele ccett rennes collaboration image amazonia provided cirad sa project gdr isis r estimation des parametres dans les cha statistical analysis dirty pictures global local methods unsupervised bayesian segmentation images iterative gibbsian technique reconstruction mary images markov random fields estimation generalized mixture unsupervised statistical radar image segmentation maximum likelihood incomplete data via em algorithm hidden markov mesh random field models image analysis random field models image anal ysis viterbi algorithm stochastic relaxation segmentation non supervisee dimages multispec trales par chaines de markov cachees phd thesis universite de technologie de compiegne context classifier distributions statistics continuous univariate distributions joint parameter estimation symbol detection linear nonlinear uknown channels simultaneous parameter estimation segmentation gibbs random fields simulation modelling anal ysis modified beta density function model synthetic aperture radar clutter statistics probabilistic solution illposed problems computational vision sem algorithm unsupervised segmentation satellite images simulation study contextual clasification methods remotely sensed data adaptive mixture estimation unsupervised local bayesian image segmentation mixture distributions statistical image segmentation use gibbs markov chain models analysis images based secondorder pairwise interactive distributions unsupervised bayesian segmentation sar images using pearson sys tem tutorial hidden markov models selected applications speech recognition mixture densities generalized hilbert scan image printing estimation context statistical classification multispectral image data parametric inference imperfectly observed gibbsian fields mean field theory em procedures blind markov random field image restoration tr ctr yihua yu qiansheng cheng mrf parameter estimation accelerated method pattern recognition letters v24 n910 p12511259 01 june jn provost c collet p rostaing p prez p bouthemy hierarchical markovian segmentation multispectral images reconstruction water depth maps computer vision image understanding v93 n2 p155174 february 2004 greg breinholt christoph schierz algorithm 781 generating hilberts spacefilling curve recursion acm transactions mathematical software toms v24 n2 p184189 june 1998 nicolas brunel wojciech pieczynski unsupervised signal restoration using hidden markov chains copulas signal processing v85 n12 p23042315 december 2005 wojciech pieczynski pairwise markov chains ieee transactions pattern analysis machine intelligence v25 n5 p634639 may franois destrempes max mignotte statistical model contours images ieee transactions pattern analysis machine intelligence v26 n5 p626638 may 2004 stphane derrode grgoire mercier unsupervised multiscale oil slick segmentation sar images using vector hmc model pattern recognition v40 n3 p11351147 march 2007