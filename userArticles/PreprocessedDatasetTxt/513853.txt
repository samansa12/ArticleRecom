efficient register memory assignment nonorthogonal architectures via graph coloring mst algorithms finding optimal assignment program variables registers memory prohibitively difficult code generation application specific instructionset processors asips mainly order meet stringent speed power requirements embedded applications asips commonly employ nonorthogonal architectures typically characterized irregular data paths heterogeneous registers multiple memory banks result existing techniques mainly developed relatively regular orthogonal generalpurpose processors gpps obsolete recently emerging asip architectures paper attempt tackle issue exploiting conventional graph coloring maximum spanning tree mst algorithms special constraints added handle nonorthogonality asip architectures results study indicate algorithm finds fairly good assignment variables heterogeneous registers multimemories runs extremely faster previous work employed exceedingly expensive algorithms address issue b introduction embedded system designers strive meet cost performance goals demanded applications complexity processors ever increasingly optimized certain application domains embedded systems optimizations need process design space exploration 8 find hardware configurations meet design goals final configuration processor resulting design space exploration usually instruction set data path highly tuned specific embedded applications sense collectively called application specific instructionset processors asips asip typically nonorthogonal architectre characterized irregular data paths containing heterogeneous registers multiple memory banks example architecture figure 1 shows motorola dsp56000 commercial offtheshelf asip specifically designed digital signal processing dsp applications note data path architecture lacks large number centralized generalpurpose homogeneous registers stead multiple small register files different files distributed dedicated different sets functional units also note employs multimemory bank architecture consists program data memory banks architecture two data memory banks connected two independent data buses conventional von neumann architecture single memory bank type memory architecture supported many embedded processors analog device adsp2100 dsp group pinedspcore motorola dsp56000 nec upd77016 one obvious advantage architecture access two data words one instruction cycle multimemory bank architectures shown effective many operations commonly found embedded applications n real multiplies example see application operate ideal rate processor two data memory banks multiplier alu 56 shifterlimiter shifter address alu address alu memory global data bus memory agu figure 1 motorola dsp56000 data path dual data memory banks x two variables xi yi fetched simultane ously also see ideal speed operation possible one condition variables assigned different data memory banks instance following dsp56000 assembly code implementing n real multiplies arrays x assigned respectively two memory banks x move xr0x0 yr4y0 mpyr x0y0a xr0x0 yr4y0 n1end mpyr x0y0a axr1 yr4y0 move xr0x0 move axr1 unfortunately several existing vendorprovided compilers tested able exploit hardware feature dual data memory banks efficiently thereby failing generate highly optimized code target asips inevitably implies users asips handoptimize code assembly fully exploit dual memory banks makes programming processors quite complex time consuming paper describe implementation two core techniques code generation nonorthogonal asips register allocation memory bank assignment register allocation decoupled two phases handle heterogeneous register architecture asip follows 1 physical registers classified set register classes collection registers dedicated machine instructions register classification algorithm allocates temporary variable one register classes 2 conventional graph coloring algorithm slightly modified assign temporary physical register within register class previously allocated memory bank assignment whose goal efficiently assign variables multimemory banks asips also decoupled two phases follows 1 maximum spanning tree mst algorithm used find memory bank assignment variables 2 initial bank assignment mstbased algorithm improved graph coloring algorithm also used register assignment algorithms differ previous work assigns variables heterogeneous registers multimemory banks separate decoupled code generation phases shown previous work single tightlycoupled phase 13 14 reported later paper performance results quite encouraging first found code generation time dramatically reduced factor four magnitudes order result somewhat already expected decoupled code generation phases greatly simplified register bank assignment problem overall meanwhile benchmarking results also showed generated code nearly identical quality code generated coupled approach almost every case section 2 discusses asip architecture targeting work section 3 presents algorithms section presents experiments additional results recently obtained since earlier preliminary study 5 section 5 concludes discussion 2 target machine model section characterize nonorthogonal architecture asips two properties formally define register architecture start section first presenting following definitions definition 1 given target machine let ng set instructions defined r rmg set registers instruction define set operands opi g assume c jl set registers appear position operand jl l k say c jl forms register class instruction definition 2 definition 1 define j collection distinct register classes instruction j follows fc jl g 1 turn define follows say whole collection register classes machine see difference homogeneous heterogenous register architectures first consider sparc example processor homogenous registers typical instruction sparc three operands op code reg register file appear first operand reg case set registers forms single register class op code since operands reg j regk registers appear form class instruction thus one register class defined instruction op code hand dsp56000 instruction form multiplies first two operands places product third operand dsp56000 restricts reg reg j input registers x0 x1 y0 y1 regk accumulator b case two register classes defined mpya fx0 x1 y0 y1g reg reg j fabg regk examples j op code mpya spectively y1gfabgg say typical processor n general purpose registers like sparc homogeneous register architecture mainly usually set single element consisting n registers processor definitions 1 2 equivalently means n registers homogeneous machine instructions case dsp56000 however registers dedicated differently machine instructions make partially homogeneous subsets machine instruc tions example see even one instruction like mpya dsp56000 two different sets homogenous reg isters xyn ab list whole collection register classes defined dsp56000 table 1 general say machine complex register classes heterogeneous register architecture id register class physical registers 9 y0 y1 table 1 register classes motorola dsp56000 22 multiple data memory banks example multimemory bank asips use dsp56000 whose data path shown figure 1 alu operations divided data operations address operations data alu operations performed data alu data registers consist four 24bit input registers x0 x1 y0 y1 two 56bit accumulators b address alu operations performed address generation unit agu calculates memory addresses necessary indirectly address data operands memory since agu operates independently data alu address calculations occur simultaneously data alu operations shown figure 1 agu divided two identical halves address alu two sets 16bit register files one set register files four address registers r0 r3 also four address registers r4 r7 address output multiplexers select source xab yab source effective address may output address alu indexed addressing address register registerindirect addressing every cycle addresses generated alus used access two words parallel x memory banks consists 512word 24bit memory possible memory reference modes dsp56000 four types x l xy x memory reference modes operand single word either x memory bank l memory reference mode operand long word two words x memories referenced one operand address xy memory reference mode two independent used move two word operands memory simultaneously one word operand x memory word operand memory independent moves data cycle called parallel move figure 1 see two data buses xdb ydb connect data path dsp56000 two data memory banks x respectively buses parallel move made memories data registers architectural features dsp56000 like asips multimemory banks allow single instruction perform one data alu operation two move operations parallel per cycle certain conditions due hardware constraints case dsp56000 following parallel move conditions met maximize utilization dual memory bank architecture 1 two words addressed different memory banks memory indirect addressing modes using address registers used address words address register involved parallel move must different set among two register files agu implementation attempt make parallel move conditions meet code many parallel moves possible generated 3 registerallocationandmem section detail code generation phases register allocation memory bank assignment briefly described section 1 explain step step code generator produces final code use example dsp56000 assembly code shown figure 2 code obtained immediately instruction selection phase note still sequential unoptimized form initial code given subsequent phases optimized dual memory architecture dsp56000 described section move r0 mac r0 r1 r2 move lowr2 lowv move highr2 highv move r3 move e r4 move f r5 mac r3 r4 r5 move lowr5 loww move highr5 highw figure 2 example uncompacted dsp56000 assembly code produced instruction selection 31 register class allocation compiler instruction selection decoupled register allocation subsequent phases fact many conventional compilers gcc lcc zephyr targeting gpps also separate two phases separating register allocation instruction selection relatively straightforward compiler targeting gpps gpps homogeneous registers within single class possibly classes registers instruction selection phase instructions need registers assigned symbolic temporaries later register allocation phase mapped available registers register class asips however register classes individual instruction may differ register may belong many different register classes see table 1 implies relationship registers instructions tightly coupled select instruction somehow also determine register classes registers assigned instruction therefore phasecoupling 9 technique cleverly combine closely related phases norm compilers generating code asips however phase coupling may create many constraints code generation thus increasing compilation time tremendously case previous work compared approach section 41 relieve problem decoupled approach still handle heterogeneous register structure implemented simple scheme enforces relationship binds two separate phases inserting another phase called register class allocation scheme represent register two notions register class register number class register class allocation phase temporaries allocated physical registers set possible registers register class placed operands instruction physical registers selected among register class instruction later phase call register assignment since focus paper register class allocation cannot discuss whole algorithm refer 6 details register classes allocated code figure shown associated temporary ri referenced code register class allocation register assignment code compaction phase results reduced code size also exploitation machine instructions perform parallel operations one add plus parallel move figure 3 shows resulting instructions code figure 2 compacted see compacted code one mac multiplyandadd two moves combined single instruction word two moves combined one parallel move instruction use traditional list scheduling algorithm code compaction move ar0 br1 move cr2 dr3 mac r0 r1 r2 er4 fr5 mac r3 r4 r5 lowr2lowv move highr2highv lowr5loww move highr5highw figure 3 code sequence compacting code figure 2 32 memory bank assignment register class allocation code compaction variable resulting code assigned one set memory banks example banks x dsp56000 section present memory bank assignment technique using two wellknown algorithms 321 using mst algorithm memory bank assignment phase use mst al gorithm first step basic phase construct weighted undirected graph called simultaneous reference graph srg graph contains variables referenced code nodes edge srg means variables v j vk referenced within instruction word compacted code figure 4a shows srg code figure 3 weight edge two variables represents number times variables referenced within word c e f c e f srg move xar0 ybr1 move xcr2 ydr3 mac r0 r1 r2 xer4 yfr5 mac r3 r4 r5 lowr2xlowv move highr2xhighv lowr5yloww move highr5yhighw c memory bank assignment b assigned memory bank11 figure 4 code result memory bank assignment determined srg built code figure 3 according parallel move conditions two variables referenced instruction word must assigned different memory banks order fetch single instruction cycle otherwise extra cycle would needed access thus strategy take maximize memory throughput assign pair variables referenced word different memory banks whenever pos sible conflict occurs two pairs variables variables one pair appear frequently words shall higher priority pair notice frequency denoted weight srg figure 4b shows variables c e v assigned x memory remaining ones b f w memory optimal pairs variables connected via edges assigned different memories x thus avoiding extra cycles fetch variables seen resulting code figure 4c case variables v w still need two cycles move long type variables doubleword length however also benefit optimal memory assignment half variables moved together cycle memory bank assignment problem face reality always simple one figure 4 illustrate realistic complex case problem consider figure 5 srg five variables b maximum spanning tree srg figure 5 complex example simultaneous reference graph maximum spanning tree constructed view process assigning n memory banks dividing srg n disjoint subgraphs nodes subgraph assigned memory bank corresponds subgraph compiler therefore try obtain optimal memory bank assignment given srg finding partition graph minimum cost according definition 3 definition 3 let e connected weighted graph v set nodes e set edges let weight edge e 2 e suppose partition graph g divides g n disjoint subgraphs g cost partition p defined finding optimal partition minimum cost another npcomplete problem developed greedy approximation algorithm ojejjv jlgjv ity shown figure 6 since practice jej jv j problem algorithm usually runs fast ojv jlgjv algorithm assume virtually existing asips two data memory banks algorithm easily extended handle cases 2 memory bank assignment algorithm first identify maximum spanning tree mst srg given connected graph g spanning tree g connected acyclic subgraph covers nodes g mst spanning tree whose total weight edges less spanning trees g one interesting property spanning tree bipartite graph tree actually bipartite given spanning tree graph g obtain partition p g1 g2 starting arbitrary node say u assigning g1 nodes even distance u g2 odd distance u based observation algorithm designed first identify spanning tree srg compute partition use heuristic chooses ordinary spanning tree maximum spanning tree rationale heuristic build partition mst eliminate heavyweighted edges mst thereby increasing chance reduce overall cost resultant partition unfortunately constructing partition mst guarantee optimum solution according earlier preliminary work 5 notion mst provides us crucial idea find partition low cost turn necessary find nearoptimal memory bank assignment instance algorithm find optimal partitioning srg figure 5 find mst algorithm uses prims mst algorithm 11 algorithm global applied across basic blocks node following sequence repeatedly iterated srg nodes marked algorithm edges priority queue q sorted order weights edge highest weight removed first one edge highest weight one inserted first removed note simultaneous reference graph gsr necessarily con nected opposed assumption made therefore create set msts one connected subgraph gsr also note algorithm least one nodes w z always marked edges marked node u always inserted q earlier algorithm figure 5b shows spanned tree obtained algorithm applied srg given figure 5a see x memory assigned even depth memory odd depth tree 322 using graph coloring algorithm graph coloring approach 4 traditionally used register allocation many compilers central idea graph coloring partition variable separate live ranges live range candidate allocated register rather entire variables found idea also used improve basic memory bank assignment described section 321 relaxing namerelated constraints variables assigned input simultaneous reference graph output set vsr whose nodes colored either x algorithm set msts q priority queue nodes v vsr unmark v select unmarked node inv sr return every node vsr marked create new mst find msts connected subgraphs gsr mark u eu set edges incident u sort elements eu incresing order weights add q remove edge highest priority q z unmarked w unmarked od u marked nodes connected subgraph gsr visited select unmarked node inv sr select node another subgraph gsr add st create new mst od nodes v vsr uncolor v every mst 2 st assign variables memory banks x next visitors q nodes vsr xcolor nodes vsr color select arbitrary node v nodes xcolored color v color else nodes ycolored color v xcolor repeat every node u adjacent v u colored color u color different color v append u next visitors q extract one node next visitors q nodes colored od nodes vsr xcolor nodes vsr color 0 xcolored nodes colored ones 9 uncolored node v 2 vsr color v color else break 0 colored nodes colored ones 9 uncolored node v 2 vsr color v xcolor else break uncolored node v vsr color v alternately x colors return figure memory bank assignment algorithm dual memories memory banks approach build undirected graph called memory bank interference graph determine live ranges conflict could assigned memory bank disjoint live ranges variable assigned different memory banks giving new name live range additional flexibility graph coloring approach sometimes result efficient allocation variables memory banks show section two techniques called name splitting merging newly implemented help memory bank assignment benefit graph coloring approach example figure 4 simple illustrate hence let us consider another example figure 7 serve clarify various features techniques c e f f c move cr2 dr3 mac r0r1 r2 er4 fr5 mac r3r4 r5 r2a cr6 add r2r6 r7 r5d er8 move ar9 dr10 b c e f cost result code compaction b live range variables c srg partitioned memory move xar0 ybr1 move xdr3 ycr2 mac r0r1r2 xfr5 yer4 mac r3r4r5 r2xa ycr6 add r2r6r7 r5xd yer8 move r10xd assignment figure 7 code example data structures illustrate name splitting merging figure 7a shows example code generated code compaction figure 7b depicts live ranges variables note variables multiple live ranges figures 7c 7d show srg assignment variables memory banks see single parallel move cannot exploited example assigned data memory finally figure 7e shows resulting code memory banks assigned using mst algorithm memory partitioning information figure 7d figure 8 shows name splitting improve example figure 7 name splitting technique tries reduce code size compacting memory references parallel move instructions technique based wellknown graph coloring approach therefore instead presenting whole algorithm describe technique example given figure 8 see figure 8a live range variable candidate assigned memory bank example two variables disjoint live ranges split live range variables given different names figures 8b 8c show modified srg improved assignment variables memory banks figure 8d demonstrates considering live ranges opposed entire variables bank assignment place two live c e f e c d2 d2 live range local variable renaming b srg c partitioned memory move xa1r0 ybr1 move xd1r3 ycr2 mac r0r1r2 xfr5 yer4 mac r3r4r5 r2xa2 yc r6 add r2r6r7 r5xd2 yer8 move xa2r9 yd2r10 move r10xd2 result name splitting figure 8 name splitting local variables ranges different memory banks allows us exploit parallel move eliminating one move instruction code figure 7e although name splitting helps us reduce code size may increase data space monitored figure 8 mitigate problem merge names name splitting figure 9 shows data space example improved using name merging earlier example split two names a1 a2 according live ranges new names assigned memory bank note live ranges conflict means turn assigned location memory e c f b c d1 e f live range merging b partitioned memory figure 9 name merging local variables compiler merge nonconflicting live ranges variable case variable also merge nonconflicting live ranges different variables see figure 9b two names b d2 merged save one word memory key idea name splitting merging consider live ranges instead entire variables candidates assigned memory banks seen examples compiler potentially reduce number executed instructions exploiting parallel moves number memory words required shown work applying graph coloring techniques assigning variables memory banks greater potential improvement applying techniques register assignment reason number memory banks typically much smaller number registers thereby algorithm name splitting merging practically polynomial time complexity even though name splitting merging basically use theoretically npcomplete graph coloring algorithm asymptotically time required name splitting merging scales worst case data memory banks yet much faster conventional graph coloring register allo cation whose time complexity onm n typically 32 gpps already empirically proven practice register allocation high complexity runs polynomial time thanks numerous heuristics pruning name splitting merging demonstrate section 4 33 register assignment memory banks determined variable code physical registers assigned code use graph coloring algorithm special constraints added handle nonorthogonal architectures explain constraints recall allocated register classes temporaries earlier register class allocation phase register assignment temporary assigned one physical register among register class allocated temporary example temporary r0 figure 4 shall replaced one register among four candidates register class 1 currently allocated r0 shown section 31 addition register class constraints register assignment also needs consider additional constraints certain types instructions instance register assignment instructions containing parallel move figure 4 must meet following architectural constraints dual memory banks data memory bank moved predefined set registers constraint also due partially heterogeneous register architecture asips back example figure 4 variable parallel move r0 allocated memory x therefore registers eligible r0 confined x0 x1 physical registers already assigned instructions register spill occur satisfying constraints register classes memory banks graph coloring algorithm assigned temporaries physical registers code figure 10 shows resulting code register assignment applied code shown figure 4c see figure 10 memory references code represented symbolically terms variable names like b converted real ones using addressing modes provided machine conversion done memory offset assignment phase comes register assignment final phase applied algorithm similar maximum weighted path mwp algorithm originally proposed leupers marwede 7 move xr1x0 yr5y0 move xr1a yr5y1 mac x0 y0 xr1x1 yr5b figure 10 resulting code register assignment memory offset assignment 4 comparative empirical studie evaluate performance memory bank assignment algorithm implemented algorithm conducted experiments benchmark suites dsp56000 10 performance measured two metrics size time section report performance obtained exper iments compare results work 41 comparison previous work recently code generation asips received much attention main stream conventional compiler research one prominent example compiler study targeting asips may araujo malik 2 proposed lineartime optimal algorithm instruction selection register allocation instruction scheduling expression trees like previous studies asips algorithm designed specifically multimemory bank ar chitectures best knowledge earliest study addressed problem register memory bank assignment saghir et al 12 however work differs target asips heterogeneous registers assume processors large number centralized generalpurpose registers token approach also differs raw project mit 3 since memory bank assignment techniques neither assume heterogeneous registers even asips recently problem extensively addressed project called spam conducted researchers princeton mit 1 14 fact spam closely related work currently available us therefore work compared algorithm experimenting set benchmarks targeting processor 42 comparison code size figure 11 list benchmarks compiled spam compiler benchmarks adpcm dspstone 15 suites reason could port spam successfully machine plat form numbers spam figure borrowed literature 14 comparison experimental result020611418 convolution complexmultiply iirbiquadnsections leastmeansquare matrixmultiply1 adaptquant adaptpredict1 iadptquant scalefactor1 speedcontrol2 tonedetector1 benchmarks size ratio figure 11 ratios code sizes spam code sizes figure displays size ratios code spam code spam code size 1 code size normalized spam code size figure see sizes output code comparable code overall fact seven benchmarks twelve output code smaller spam code results indicate memory bank assignment algorithm effective simultaneous reference allocation algorithm cases 43 comparison compilation time compilers demonstrate comparable performance code size difference compilation times significant depicted figure 12 according literature 14 experiments spam conducted sun microsystems ultra enterprise featuring eight processors 1gb ram unfortunately could find exactly machine used instead experimented sun microsystems ultra enterprise two processors 2gb ram101000100000 convolution complexmultiply iirbiquadnsections leastmeansquare matrixmultiply1 adaptquant adaptpredict1 iadptquant scalefactor1 speedcontrol2 tonedetector1 benchmarks compilation time ratio figure 12 ratios log scale compilation times compiler spam compiler see figure compilation times roughly three four orders magnitude faster despite differences machine platforms therefore believe large difference compilation times clearly demonstrates advantage approach terms compilation speed comparative experiments show evidence compilation time spam may increase substantially large ap plications opposed found long compilation time spam compiler results fact use coupled approach attempts deal register memory bank assignment single combined step several code generation phases coupled simultaneously considered address issue approach variables allocated physical registers time assigned memory banks support coupled approach build constraint graph represents multiple constraints optimal solution problem sought unfortunately multiple constraints graph turn problem typical multivariate optimum problem tractable npcomplete algorithm coupled approach multivariate constraints unavoidable various constraints many heterogeneous registers multimemory banks involved find optimal reference allocation simulta neously consequence avoid using expensive algorithm inevitably resorted heuristic algorithm called simulated annealing based monte carlo approach however even heuristic observed literature 13 14 compiler still take 1000 seconds even moderately sized program mainly number constraint constraint graph rapidly becomes large complicated code size increases see slowdown compilation obviously caused intrinsic complexity coupled approach con trast compilation times stayed short even larger bench marks credit mainly decoupled approach facilitated application various fast heuristic algorithms individually conquer subproblem encountered code generation process dual memory bank system specifically approach register allocation decoupled code compaction memory bank assignment thereby binding physical registers temporaries comes code compacted variables assigned memory banks could initially expect degradation output code quality due limitations newly introduced considering physical register binding separately memory bank signment however conclude results careful decoupling may alleviate drawbacks practice maximizing advantages terms compilation speed often critical factor industry compilers 44 comparison execution speed estimate impact code size reduction running time generated three versions code follows uncompacted first version uncompacted code shown figure 2 generated immediately instruction selection phase compileroptimized uncompacted code optimized dsp56000 using techniques section 3 produce code like one figure 10 handoptimized uncompacted code optimized hand handoptimized code compiler used input handoptimized one may provide us upper limit performance benchmarks dsp56000 execution times compared figure 13 ratios speedup improvement produced compiler optimization handoptimization compared speedup produced uncompacted code instance compiler optimized code complex multiply achieves speedup 23 uncompacted code handoptimized code achieves additional speedup 9 tantamount 32 total uncompacted code figure 13 see average speedup compileroptimized code uncompacted code 7 handoptimized code compileroptimized code 8 results indicate compiler achieved roughly half speedup could get hand optimiza tion although numbers may satisfactory results also indicate six benchmarks twelve compiler achieved greater part performance gains achieved hand optimization course also several benchmarks fir2dim convolution least mean square compiler much room improvement according anal ysis main cause creates difference execution0206114fir2dim convolution complexmultiply iirbiquadnsections leastmeansquare matrixmultiply1 adaptquant adaptpredict1 iadptquant scalefactor1 speedcontrol2 tonedetector1 benchmarks ompiler figure 13 speedups execution times compiler optimized handoptimized code execution time unoptimized code time compilergenerated code hand optimized code incapability compiler efficiently handle loops illustrate consider example figure 14 shows typical example software pipelining required optimize loop 16 l10 mov xax0 yby0 mpy x0y0a xcx1 ydy1 add x1y1a mov axe compiled compacted code approach 16 l10 mov xax0 yby0 mpy x0y0a xcx1 ydy1 add x1y1a mov axe compiled compacted code approach mov xax0 yby0 15 l10 mpy x0y0a xcx1 ydy1 add x1y1a xax0 yby0 mov axe b handoptimized compacted code figure 14 compaction difference compiled code handoptimized code notice example parallel move variables b cannot compacted instruction word containing add dependence mpy however placing one copy parallel move preamble loop merge move add although optimization may reduce total code size eliminates one instruction within loop undoubtedly would reduce total execution time noticeably example informs us since execution time spent loops compiler cannot match hand optimization run time speed without advanced loop opti mizations software pipelining based rigorous dependence analysis currently issue remains future research 5 conclusion paper proposed decoupled approach supporting dual memory architecture six code generation phases performed separately also presented name splitting merging additional techniques comparing work spam analyzed pros cons decoupled approach opposed coupled approach comparative analysis experiments revealed compiler achieved comparable results code size yet decoupled structure code generation simplified data allocation algorithm dual memory banks allows algorithm run reasonably fast analysis also revealed exploiting dual memory banks carefully assigning scalar variables banks brought speedup run time however analysis exposed several limitations current techniques well instance approach limited scalar variables expect memory bank assignment arrays achieve large performance enhancement computations performed arrays number crunching programs actually illustrated figures 11 13 even highly handoptimized code could make significant performance improvement terms speed although made visible difference terms size mainly impact scalar variables performance relatively low compared space occupy code another limitation would perform memory bank assignment arguments passed via memory functions would require interprocedural analysis since caller must know memory access patterns callee passing arguments also certain loop optimization techniques like listed section 44 need implemented improve execution time output code 6 r challenges code generation embedded processors code generation fixedpoint dsps compiler support scalable efficient memory systems register allocation spilling via graph coloring efficient fast allocation onchip dual memory banks portable optimizer digital signal processors algorithms address assignment dsp code generation retargetable compilers embedded core processors code generation embedded processors motorola inc shortest connection networks generalizations exploiting dual datamemory banks digital signal processors code optimization libraries retargetable compilation embedded digital signal processors simultaneous reference allocation code generation dual data memory bank asips tr exploiting dual datamemory banks digital signal processors algorithms address assignment dsp code generation code generation fixedpoint dsps simultaneous reference allocation code generation dual data memory bank asips portable optimizer digital signal processors compiler support scalable efficient memory systems retargetable compilers embedded core processors code generation embedded processors register allocation myampersandamp spilling via graph coloring study data allocation onchip dual memory banks code optimization libraries retargetable compilation embedded digital signal processors ctr yihsuan lee cheng chen effective efficient code generation algorithm uniform loops nonorthogonal dsp architecture journal systems software v80 n3 p410428 march 2007 g grwal coros banerji morton comparing genetic algorithm penalty function repair heuristic dsp application domain proceedings 24th iasted international conference artificial intelligence applications p3139 february 1316 2006 innsbruck austria chungi lyuh taewhan kim memory access scheduling binding considering energy minimization multibank memory systems proceedings 41st annual conference design automation june 0711 2004 san diego ca usa jeonghun cho yunheung paek david whalley fast memory bank assignment fixedpoint digital signal processors acm transactions design automation electronic systems todaes v9 n1 p5274 january 2004 rajiv ravindran robert senger eric marsman ganesh dasika matthew r guthaus scott mahlke richard b brown increasing number effective registers lowpower processor using windowed register file proceedings international conference compilers architecture synthesis embedded systems october 30november 01 2003 san jose california usa rajiv ravindran robert senger eric marsman ganesh dasika matthew r guthaus scott mahlke richard b brown partitioning variables across register windows reduce spill code lowpower processor ieee transactions computers v54 n8 p9981012 august 2005 zhong wang xiaobo sharon hu energyaware variable partitioning instruction scheduling multibank memory architectures acm transactions design automation electronic systems todaes v10 n2 p369388 april 2005 xiaotong zhuang santosh pande parallelizing loadstores dualbank memory embedded processors acm transactions embedded computing systems tecs v5 n3 p613657 august 2006 yihsuan lee cheng chen efficient code generation algorithm nonorthogonal dsp architecture journal vlsi signal processing systems v47 n3 p281296 june 2007 zhong wang xiaobo sharon hu power aware variable partitioning instruction scheduling multiple memory banks proceedings conference design automation test europe p10312 february 1620 2004