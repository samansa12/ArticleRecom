inverse free preconditioned krylov subspace method symmetric generalized eigenvalue problems paper present inverse free krylov subspace method finding extreme eigenvalues symmetric definite generalized eigenvalue problem x basic method takes form innerouter iterations involves inversion b shiftandinvert matrix alambda0 b convergence analysis presented leads preconditioning scheme accelerating convergence equivalent transformations eigenvalue problem numerical examples given illustrate convergence properties demonstrate competitiveness method b introduction iterative methods lanczos algorithm arnoldi algorithm widely used solving large matrix eigenvalue problems see 21 22 eective applications algorithms typically use shiftandinvert transformation sometimes called preconditioning 22 requires solving linear system equations original size iteration process truly large problems solving shiftandinvert equations direct method lu factorization often infeasible inecient cases one employ iterative method solve approximately resulting two levels iterations called innerouter iterations however methods like lanczos algorithm arnoldi algorithm sensitive perturbations iterations therefore require highly accurate solutions linear systems see 8 therefore innerouter iterations may oer ecient approach methods recently great interest iterative methods also based shift andinvert equations tolerate low accuracy solutions one simple example methods inexact inverse iteration linear convergence property preserved even inversion solved low accuracy see 7 13 14 27 several sophisticated competitive methods developed also possess property include jacobidavidson scientic computing computational mathematics program department computer science stanford university stanford ca 94305 email golubsccmstanfordedu research supported part national science foundation grant dms9403899 department mathematics university kentucky lexington ky 405060027 email qyemsukyedu part research supported nserc canada author university manitoba method 5 24 25 truncated rq iterations 26 32 others 14 29 30 31 one diculty methods easy determine accuracy shiftandinvert equations solved hand several works aim generalizing concept preconditioning linear systems eigenvalue problem 1 2 4 11 12 17 18 15 20 28 mostly done however directly adapting preconditioner used inverting certain matrix eigenvalue iteration situations role preconditioners usually clear although regarded using inexact shiftandinvert 19 overall new methods demonstrated work successfully problems general lack understanding work furthermore optimal eigenvalue projection methods lanczos algorithm mostly incorporated developments paper shall present variation krylov subspace projection methods computing extreme eigenvalues generalized eigenvalue problem called pencil problem b symmetric matrices b 0 method iteratively improves approximate eigenpair step uses either lanczos arnoldi iteration produce new approximation rayleighritz projection krylov subspace resulting form innerouter iterations shall present theoretical numerical ndings concerning convergence properties method derive bounds asymptotic linear convergence rates furthermore shall develop convergence analysis equivalent transformations eigenvalue problem accelerate convergence called preconditioning particular transformations based incomplete factorization thus generalize preconditioning linear systems best knowledge rst preconditioning scheme eigenvalue problem based justied convergence theory paper organized follows present basic algorithm section 2 analyze convergence properties section 3 give numerical examples section 4 illustrate convergence properties present preconditioned version algorithm section 5 followed numerical examples preconditioning section 6 nish concluding remarks section 7 basic inverse free krylov subspace method section present basic algorithm nding smallest eigenvalue corresponding eigenvector x pencil b b symmetric b 0 note method developed modied trivial way nding largest eigenvalue simply considering b given initial approximation aim improving rayleighritz orthogonal projection certain subspace ie minimizing rayleigh quotient subspace noting gradient rayleigh quotient x 0 r wellknown steepest descent method chooses new approximate eigenvector x minimizing clearly considered rayleighritz projection method subspace k 1 spanfx g hand inverse iteration constructs new approximation x inversion solved inexactly iterative solver ie inexact inverse iteration 7 x 1 indeed chosen krylov subspace generated 0 b since x 1 extracted krylov subspace solve linear system may good choice approximating eigenvector consider natural extension two approaches nds new approximate eigenvector x 1 krylov subspace km spanfx xed using rayleighritz projection method projection carried constructing basis km forming solving projection problem pencil b repeating process arrive following iteration call inverse free krylov method b algorithm 1 inverse free krylov subspace method input 1 initial approximation x 0 construct basis find smallest eigenpair end algorithm apply projection shifted pencil update approximation accordingly theoretically equivalent using projection b directly formulation however may improve stability saving matrixvector multiplications utilizing need computed construction basis constructing basis z many possible choices theoretically equivalent new approximate eigenpair obtained dened 1 however numerically consider basis orthonormal certain inner product basis krylov subspace typically constructed iterative method called inner iteration original iteration algorithm 1 called outer iteration shall discuss subsections later three methods constructing basis presentation algorithm 1 assumed convenience construct full basis z generally rayleighritz projection simply carried replacing zm 1 still valid numerically however early termination step p inner iteration likely full basis usually constructed even p theory causes problem larger space spanned vectors would yield better approximation given basic ingredients algorithm 1 projection krylov subspaces surprising similar methods considered 10 11 knyazev discussed analyzed general theoretical methods suggested several special cases among use km 1 morgan scotts preconditioned lanczos algorithm 18 takes similar iteration uses smallest ritz value matrix k b rather pencil update eigenvalue varied iteration quadratic convergence property point however quadratic convergence desirable property achieved cost increasingly larger prevents improvement convergence preconditioning hardly need accelerate quadratic convergent algorithm study somewhat dierent nature consider accelerating convergence changing certain conditions problem equivalent transformations see section 5 opposed increasing also related methods based inverting shifted matrix k b projection include inverse iteration jacobidavidson method 24 inversion solved approximately iterative method solution extracted krylov subspace generated k b projection cases chosen satisfy related linear system note jacobidavidson method also uses rayleighritz projection outer iteration cost increases iteration xing size subspaces projection cost algorithm 1 xed per outer iteration easy see algorithm 1 standard restarted lanczos algorithm regard investigation version xed aects convergence furthermore development lead preconditioning strategy transforms pencil problem l 1 al suitably chosen l algorithm 1 applied transformation complicated problem may seem counter intuitive important feature algorithm 1 case oers advantage general b discuss details construction basis km algorithm 1 21 orthonormal basis lanczos algorithm one obvious choice basis krylov subspace km orthonormal one constructed applying lanczos algorithm c simultaneously lanczos process produce tridiagonal matrix lanczos process requires m1 matrixvector multiplications c basis constructed multiplications b note state lanczos algorithm algorithm 2 orthonormal basis lanczos b approximate end orthonormal basis bm general full matrix need solve generalized eigenvalue problem bm exact arithmetic may valid nite precision arithmetic larger could severe loss orthogonality among z corrected either computing explicitly using reorthogonalization 6 lanczos process note c k zm computed lanczos algorithm stored forming 22 borthonormal basis arnoldi algorithm also construct borthonormal basis km modied gramschmidt process binner product essentially arnoldi algorithm advantage approach simpler projection problem cost longer recurrence also need compute state algorithm algorithm 3 borthonormal basis arnoldi input end step arnoldi algorithm requires 2 matrixvector multiplications one c k one b addition need store bz iteration order save matrixvector multiplications resulting storage cost vectors note larger b orthogonality among columns zm may gradually lost leads deterioration equation case need either reorthogonalization arnoldi algorithm explicit computations comparing two constructions computational costs associated comparable require 2m multiplications arnoldi recurrence expensive ops storage lanczos recurrence produces compact projection matrix lanczos algorithm clearly dierences minor large case interest practical implementations terms numerical stability two theoretically equivalent processes testing suggests little dierence however preconditioned version algorithm 1 discuss section 5 approach arnoldi algorithm seems advantage see section 5 23 c k orthogonal basis variation lanczos algorithm also possible construct zm c k orthogonal variation lanczos algorithm three term recurrence projection compact form leading computationally eective approach previous two however less stable owing indeniteness c k theoretical interest outline variation lanczos algorithm form full matrix tridiagonalization standard tridiagonalization lanczos algorithm c tridiagonal q orthogonal x k kx k k rst column sake simplicity presentation assume k rst second eigenvalue implies c exactly one negative eigenvalue noting 1 1 entry x block ldl decomposition write easy check lanczos three term recurrence easily derived 2 construct columns z still form basis krylov subspace essentially corthogonal however tests show numerically less stable therefore shall consider omit detailed algorithm 3 convergence analysis section study convergence properties algorithm 1 include global convergence result local one rate linear convergence particular identify factors aect speed convergence develop preconditioning strategy improve convergence rst prove algorithm 1 always converges eigenpair need following proposition proof straightforward proposition 1 let 1 smallest eigenvalue b k x k eigenpair approximation obtained algorithm 1 step k theorem 1 let k x k denote eigenpair approximation obtained algorithm 1 step k k converges eigenvalue b converges direction corresponding eigenvector proof proposition 1 obtain k convergent since x k bounded convergent subsequence x n k let x bx follows 3 suppose r 6 0 consider projection b onto rg dening r noting fx rg orthogonal br indenite thus smallest eigenvalue b denoted less ie furthermore step k dene r let k1 smallest eigenvalue b hence continuity property eigenvalue hand k1 smallest eigenvalue projection b implies finally combining together obtained contradiction 4 therefore eigenvalue show suppose subsequence k 0 subsequence k subsequence n k x n k convergent hence virtue proof contradiction completes proof next study speed convergence local analysis particular show k converges least linearly smallest eigenvalue b x corresponding unit eigenvector eigenpair approximation obtained algorithm 1 step k let 1 smallest eigenvalue k b u 1 corresponding unit eigenvector asymptotically k 1 proof first denition furthermore 1 k b 0 0i smallest eigenpair smallest eigenpair b clearly k b indenite hence 1 0 using theorem 3 appendix leads bound 5 prove asymptotic expansion let 1 smallest eigenvalue tb using analytic perturbation theory obtain 0 hence choosing expansion follows present main convergence result assume k already rst second smallest eigenvalues theorem 1 converges smallest eigenvalue theorem 2 let 1 2 n eigenvalues b k1 x k1 approximate eigenpair obtained algorithm 1 k x k let 1 2 n eigenvalues k b u 1 unit eigenvector corresponding 1 assume 1 k 2 kbk pm denoting set polynomials degree greater proof first write g step k algorithm let k eigenvalue decomposition k b orthogonal g let q minimizing polynomial follows x using proposition 1 hence hand also note q 1 thus kbk used 8 9 finally combining 7 10 lemma 1 kbk leads theorem well known theorem bounded terms see 16 theorem 164 example speed convergence depends distribution eigenvalues k b b dierence fundamental importance allows acceleration convergence equivalent transformations change eigenvalues k b leave b unchanged see discussion preconditioning section 5 hand bound shows accelerated convergence increased regard numerical tests suggests convergence rate decreases rapidly increases see section 4 special case steepest descent method easy check case using theorem 2 recover classical convergence bound steepest descent method 9 p617 note stronger global convergence result case ie k guaranteed converge smallest eigenvalue initial vector nontrivial component smallest eigenvector see 9 p613 result known case b 6 asymptotically also express bound terms eigenvalues 1 b instead dependent k state following corollary point bound theorem 2 informative n eigenvalues 1 b asymptot ically p 2m proof follows combining 11 4 numerical examples section present numerical examples illustrate convergence behavior algorithm 1 demonstrate linear convergence property eect convergence rate example 1 consider laplace eigenvalue problem dirichlet boundary condition lshape region nite element discretization triangular mesh 7585 interior nodes using pde toolbox matlab leads pencil eigenvalue problem apply algorithm 1 nd smallest eigenvalue random initial vector stopping criterion set kr k kkr 0 give convergence history residual kr k k top resp figure 1 present figure 2 number outer iterations required achieve convergence range correspondingly figure 2 b total number inner iterations observe residual converges linearly rate decreased increases fur thermore figure 2 number outer iterations decreases rapidly quadratically even exponentially increases almost reaches stationery limit around 70 peculiar property see figure 2 b total number inner iterations near minimal large range 40 80 case example 2 consider standard eigenvalue problem point nite dierence discretization laplace operator mesh unit square apply algorithm 1 nd smallest eigenvalue random initial vector figure example 1 convergence r k outer iterations 2norm residuals case simply restarted lanczos algorithm shall consider comparison lanczos algorithm without restart figure 3 present convergence history k 1 k approximate eigenvalue obtained inner iteration plotted dot lines top respectively corresponding plot lanczos algorithm without restart given solid line also considered number outer iterations total number inner iterations function observed behavior example 1 omit similar gure particular nearly exponential decrease outer iteration count implies convergence history moderate case even 16 close one large ie lanczos without restart solid line examples conrm linear convergence property algorithm 1 furthermore numerical testing consistently shown number outer iterations decreases nearly exponentially increases implies near optimal performance algorithm achieved moderate attractive implementations unfortunately able explain interesting behavior convergence results even restarted lanczos algorithm seems phenomenon observed explained convergence theory lanczos algorithm either 5 preconditioning section discuss accelerate convergence algorithm 1 equivalent transformations call preconditioning shall present preconditioned version algorithm 1 figure 2 example 1 outer total inner iterations vs outer iterations parameter inner iterations total inner iterations b convergence result theorem 2 rate convergence depends spectral distribution c ie separation 1 rest eigenvalues 2 n c k approximate eigenpair k x k consider matrix l k transformed pencil eigenvalues b thus applying one step algorithm 1 bound 6 theorem 2 rate convergence 2 determined eigenvalues suitably choose l k obtain favorable distribution hence smaller shall call 13 preconditioning transformation one preconditioning transformation constructed using ldl factorization symmetric matrix 6 example ldl factorization k diagonal matrix 1 choosing l k results convergence stage 1 k 2 implies thus theorem 2 figure 3 example 2 eigenvalue convergence history inner iteration restarted dot lines top lanczos without restart solid line total inner iterations error ritz value conclude algorithm 1 applied k step k using exact ldl factor ization converges quadratically even true ie steepest descent method similarly light corollary 1 use constant l obtained ldl factorization assuming 1 known k diagonal matrix 0 1 algorithm 2 also converges quadratically described ideal situations fast quadratic convergence property achieved using exact ldl factorization practice use incomplete ldl factorization k b l k k l incomplete lu factorization see 23 chapter 10 nonzero small hence fast linear convergence indeed ecient consider constant l obtained incomplete ldl factorization 0 suciently good approximation 1 apply algorithm 1 13 preconditioned algorithm converges linearly rate determined eigenvalues better spectral distribution long 0 k l 1 bl small relative note 0 k need small l 1 bl small eg discretization dierential operators may work even 0 1 incomplete ldl factorization becomes incomplete cholesky factorization also possible construct l based factorization approximate eigenvalue decomposition preconditioned iterative methods linear systems preconditioned iteration algorithm 1 implemented implicitly ie without explicitly forming transformed problem c k derive preconditioned version algorithm rest section approximate eigenpair obtained step k pencil b corresponding approximate eigenpair transformed pencil 13 applying one step iteration transformed pencil new approximation obtained constructing basis z z krylov subspace form projection problem smallest eigenpair projection problem v new approximate eigenpair b z new approximate eigenpair written k1 projection problem therefore complete kth iteration need construct basis subspace l km actual construction z depends method use given details subsections later summarize preconditioned algorithm follows algorithm 4 preconditioned inverse free krylov subspace method input initial approximation x 0 convergence construct preconditioner l k construct preconditioned basis km find smallest eigenvalue 1 eigenvector v bm end remark linear system case algorithm takes form original one except using preconditioned search space l km following subsections discuss construction preconditioned basis arnoldi algorithm lanczos algorithm corresponding construction sections 21 22 numerical testing suggests arnoldi algorithm might stable lanczos algorithm cases 51 preconditioned basis arnoldi method arnoldi method construct borthonormal basis km correspond ingly z borthonormal basis l km starting b recurrence z h z z borthonormal h ji ensures condition arrive following algorithm algorithm 5 preconditioned borthonormal basis arnoldi input preconditioner l k end see algorithm l k needed construction use 0 1 incomplete cholesky factor ie 0 b use matrix approximating explicitly forming l k example dierential operators use multigrid domain decomposition preconditioners 0 b directly 52 preconditioned basis lanczos method lanczos method construct z z orthonormal basis km corresponding basis z k starting recurrence z z z z resulting tridiagonal matrix constructed satises zm thus using clearly formulas i1 ensures z j morthonormal thus alternative formulas derive recurrence construct basis note construction normalizes z mnorm practice could nearly singular therefore appropriate normalize 2norm following algorithm one several possible formulations algorithm preconditioned basis lanczos input preconditioner l k end 6 numerical examples ii section present numerical examples demonstrate eectiveness competitiveness preconditioned inverse free krylov subspace method algorithm 4 example 3 b example 1 apply algorithm 4 nd smallest eigenvalue closest use constant l k obtained threshold incomplete ldl factorization drop tolerance 10 2 compare algorithm jacobidavidson algorithm uses number inner iterations kind preconditioner give figure 4 convergence history residual kr k k algorithm 4 solid lines jacobidavidson algorithm dot lines top respectively figure 5 also present number outer iterations total number inner iterations required reduce kr k kkr 0 k 10 7 mark algorithm 4 mark jacobidavidson algorithm comparing example 1 results clearly demonstrate acceleration eect preconditioning signicantly reducing number outer iterations fig 4 fig 5a furthermore values total number inner iterations near minimum signicantly smaller preconditioning around fig 5 although jd algorithm smaller number total inner iteration small corresponding outer iteration count larger increases cost also considered example ideal preconditioning l k chosen exact ldl factorization c k case use initial vector kax 0 0 suciently close 1 present residual convergence history figure 6 steepest descent method 2 4 result conrms quadratic convergence property figure 4 example 3 residual convergence history lines algorithm 4 dot lines jacobidavidson outer iterations 2norm residuals also tested case uses l l exact factorization 1 b case converges one iteration conrming corollary 1 next example standard eigenvalue problem preconditioned algorithm implicitly transforms pencil problem example 4 matrix example 2 use constant l k obtained incomplete ldl decompositions llin compare jacobidavidson algorithm kind preconditioner also consider shiftand spectral transformed lanczos algorithm ie applying lanczos 1 give figure 7 convergence history residual kr k k algorithm 4 solid lines top jacobidavidson algorithm dot lines corresponding marks residual spectral transformed lanczos given dashdot line figure 8 number outer iterations total number inner iterations vs preconditioning signicantly accelerates convergence result compares favorably jacobidavidson method interesting point algorithm 4 based incomplete factorization outperforms shiftandinvert lanczos algorithm although suggest case general underline eectiveness preconditioned algorithm figure 5 example 3 outer total inner iterations vs algorithm 4 outer iterations total inner iterations b 7 concluding remarks presented inverse free krylov subspace method based classical krylov subspace projection methods incorporates preconditioning ecient implementations convergence theory developed method preliminary tests preconditioned version demonstrate competitiveness comparing existing methods relatively well understood theory simple numerical behavior point algorithm xed cost per outer iteration makes easy implement future work consider generalizations three directions namely ecient block version computing several eigenvalues simultaneously strategy compute interior eigenvalues algorithm general nonsymmetric problem appendix perturbation bounds generalized eigenvalue problems present perturbation theorem used proof lemma 1 might general interest well following b e symmetric theorem 3 let 1 smallest eigenvalue b x corresponding unit eigenvector let 1 smallest eigenvalue u corresponding unit eigenvector min e figure convergence ideal preconditioning min e max e denote smallest largest eigenvalues e respectively proof using minimax characterization z 60 z z bz similarly min e completes proof note 1x bx 1u bu wilkinson condition number 1 1 bounds therefore agree rst order analytic expansion sharper traditional bounds based kbk figure 7 example 4 residual convergence history lines algorithm 4 dot lines jacobidavidson dashdot shiftandinvert lanczos r subspace preconditioning algorithm eigenvec toreigenvalue computation davidson method applied numerical linear algebra minimization computational labor determining matrix computations inexact inverse iterations eigenvalue problems large sparse symmetric eigenvalue problems homogeneous linear constraints lanczos process innerouter iterations functional analysis normed spaces convergence rate estimates iterative methods mesh symmetric eigenvalue problem preconditioned eigensolvers oxymoron elec toward optimal preconditioned eigensolver locally optimal block preconditioned conjugate gradient method inexact inverse iteration large sparse eigenvalue problems inexact rational krylov sequence method restarted arnoldi method applied iterative linear solvers computation rightmost eigenvalues computer solution large linear systems generalizations davidsons method computing eigenvalues sparse symmetric matrices preconditioning lanczos algorithm sparse symmetric eigenvalue problems geometric theory preconditioned inverse iteration new method generalized eigenvalue problem convergence estimate preprint symmetric eigenvalue problem numerical methods large eigenvalue problems iterative methods sparse linear systems jacobidavidson iteration method linear eigenvalue problems truncated rq iteration large scale eigenvalue calculations robust preconditioning large sparse symmetric eigenvalue problems restarting techniques jacobidavidson symmetric eigenvalue method elec dynamic thick restarting davidson implicitly restarted arnoldi methods inexact newton preconditioning techniques large symmetric eigenvalue problems elec convergence analysis inexact truncated rq iterations elec tr ctr james h money qiang ye algorithm 845 eigifp matlab program solving large symmetric generalized eigenvalue problems acm transactions mathematical software toms v31 n2 p270279 june 2005 pa absil c g baker k gallivan truncatedcg style method symmetric generalized eigenvalue problems journal computational applied mathematics v189 n1 p274285 1 may 2006