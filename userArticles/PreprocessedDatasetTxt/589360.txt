numerical approximation sqptype method parameter paper deals numerical approximation levenbergmarquardt sqp lmsqp method parameter identification problems presented analyzed burger w mhlhuber inverse problems pp 943969 shown galerkintype discretization leads convergent approximation indefinite system arising karushkuhntucker kkt system wellposedin addition present multilevel version levenbergmarquardt method discuss simultaneous solution discretized kkt system preconditioned iteration methods indefinite problems discussion numerical effort conclude approaches may lead considerable speedup respect standard iterative regularization methods eliminate underlying state equation numerical efficiency lmsqp method confirmed numerical examples b introduction parameter identication denotes procedure determining unknown parameters appearing underlying state equation usually partial dierential equation indirect measurements related solution equation problems frequently appear many applications mathematical models physical chemical biological economical processes used cf eg 1 12 16 references since problems illposed general ie parameter reconstructed depend observation stable way regularization methods used order compute stable approximation parameter presence data noise due illposedness identication problem numerical approximation problems simple task standard approach found literature based apriori elimination state equation application discretized regularization method resulting operator equation involving parametertooutput map operator mapping parameter corresponding observation main part work supported austrian national science foundation fwf project grants f 1308 f 1309 introduction evaluation map solution underlying state equation given parameter numerically realized standard discretizations nite elements approach particular combined iterative regularization methods cf 17 overview applied success even rather complicated parameter identication problems cf eg 9 24 25 however since methods need high number direct solves ie solutions state equation ne discretizations parameter yield considerable computational eort results high cputimes even impossibility use ne discretizations another drawback approach discretizations state parameter rather independent makes numerical analysis extremely dicult therefore fundamentally dierent methods solution parameter identication problems investigated recently whose common idea avoid apriori elimination state equation cf 10 20 26 aim paper discuss numerical approximation iterative regularization method based idea sequential quadratic programming cf 10 investigate galerkintype discretizations product space parameter state variable corresponding lagrangian variables leads sequence wellposed indenite systems approach able show convergence numerical approximation quadratic programming problem arising iteration step overall minimization procedure general setup paper follows assume given noisy measurement z satisfying exact data satisfy z eu 12 aim identify parameter q 2 q ad q q ad closed subset q non empty interior underlying equation continuous nonlinear operator setup x x q z hilbert spaces x identied dual x finally assume e continuously frechetdierentiable x q partial derivative e u 2 lx x selfadjoint satises coercivity condition u u qv vi e kvk 2 e setup typical partial dierential equation elliptic type also main type application mind want mention innite dimensional analysis carried preceding paper 10 restricted elliptic problems assumed wellposedness state equation given parameter ever since numerical approximation techniques elliptic problems dier ones parabolic hyperbolic problems cf eg 32 overview one cannot expect successful unied approach corresponding parameter identication problems reason start investigation elliptic case paper want mention numerical identication parameters transient equations even mixed systems equations important challenging problem future research 10 mentioned parameter identication problem setup illposed inverse problem proposed following iterative regularization method based idea sequential quadratic programming programming method let given initial value let k k2n bounded sequence positive real numbers levenbergmarquardt sequential quadratic programming lmsqp method consists iteration procedure minimizer quadratic programming problem2 subject linear constraint iteration procedure stopped soon appropriately chosen 1 due results 10 lmsqpmethod convergent regularization method particular quadratic programming problems form 17 18 solved iteration step wellposed aim paper investigate numerical approximation lmsqpmethod galerkintype approach shall show leads indenite system iteration step whose solution approximation optimal order solution 17 18 moreover show reconstructions obtained discretized lmsqpmethod converge solution parameter identication problem noise level discretization size tend zero appropriate stopping rule used relates residual noise level measures discretization moreover shall discuss solution discretized karushkuhn tucker system indenite linear system solved discretized equivalents state parameter lagrangian variable standard approaches solution discretized problems arising partial dierential equations reduced sqpmethods state lagrangian variable eliminated apriorily recall basic properties reduced sqpapproach mainly focus iterative solution whole system appropriate preconditioning promising approach employed recently parameter identication cf 20 26 optimal control problems cf 2 3 4 5 good numerical results particular respect eciency paper organized follows section 2 investigate numerical approximation lmsqpmethod galerkintype approach discuss wellposedness stability approximation properties discretized karushkuhntucker kkt system convergence discretized solutions shown section 3 numerical methods implementation outer iteration ie sqpiteration assumption able solve quadratic optimization problems arising step lmsqp method examined section 4 discuss correct scaling variables globalization strategies well multilevel approach leads speedup method section 5 devoted inner iteration ie numerical solution discretized kktsystem basic properties symmetric indenite problem studied well iterative solution appropriate preconditioning rst application investigate identication potential elliptic boundaryvalue problem give quantitative error estimates terms discretization sizes numerical experiments related identication problems presented section 7 nally conclude give outlook interesting problems related topic section 8 discretization techniques following investigate discretization lmsqpmethod galerkin ap proach first assume discretized data z 2 z z form z orthogonal projector onto nitedimensional subspace z note give error estimate z using 11 kr let x h x q h q nitedimensional subspaces x q corresponding orthogonal projectors discretize lmsqpmethod follows let given initial value moreover let k k2n bounded sequence positive real numbers galerkin levenbergmarquardt sequential quadratic programming glmsqp method consists iteration procedure minimizer quadratic programming problem2 kr eu z k 2 subject linear constraint 21 discretized karushkuhntucker system 5 note constraint 25 rewritten operator form solved u notation p adjoint p h assumption 15 obtain v 2 x h ie discrete bilinear form associated operator p coercive x h implies laxmilgram theorem 26 uniquely solvable respect u given q 2 q h consequently analogous way proof proposition 21 10 may show following result wellposedness quadratic programming problem solved step method 2 proposition 21 let e continuously frechetdierentiable let 15 hold let k 0 quadratic programming problem 24 25 unique solution also local minimum 21 discretized karushkuhntucker system 10 karushkuhntucker system innitedimensional version lmsqp method derived analyzed framework linear saddle point problems discuss discretized analogue system namely rstorder optimality conditions quadratic programming problem 24 25 lagrangian 24 25 given equal identity x h q h respectively rewrite lagrangian operators k k l k dened 27 28 kktsystem deduced computing partial derivatives lagrangian respect u q ie solves linear saddlepoint problem p 10 dene symmetric bilinear form bilinear form b moreover use righthand sides kktsystem 212 interpreted galerkin approximation indenite variational problem ie u q solution analogous way proof theorem 23 10 show bilinear form satises kernelellipticity condition x h q h ie exists constant 0 b satises lbbcondition sup b 0 implies following wellposedness result cf 7 8 discretized problem 217 218 theorem 22 let e continuously frechetdierentiable let 15 hold let k 0 indenite system 217 218 unique solution u q depends continuously righthand sides f k g k since constants b corresponding innitedimensional conditions x q particular independent discrete subspaces x h allows deduce approximation result solutions 217 218 solution innitedimensional kktsystem given variational form k u q k given g k dened 21 discretized karushkuhntucker system 7 theorem 23 suppose assumptions theorem 22 satised let denote unique solution 217 218 exists constant c 0 independent x h q h u q denotes unique solution 219 220 proof first let u h solution 217 218 k replaced k k theorem 21 8 implies existence constant c 1 0 independent x h q h moreover stable dependence solutions 217 218 righthand side implies existence c 2 0 sup hg ja sup r r triangle inequality may conclude 223 theorem 23 provides error estimate solutions discretized saddlepoint problem 217 218 consisting two parts corresponding numerical approximation image space z preimage spaces x q obvious estimate rst term h inf possibly lead quantitative estimate since additional information smoothness noisy data alternative estimate inmum usually estimated easily since exact data z smoother due fact u solution state equation parameter q eg state equation elliptic type solution embedding operator r results standard nite element discretization grid neness least another important observation last term vanishes discrete spaces z equal achieved applications second term 223 shows galerkin approximation kktsystem optimal order x h q h x h estimated standard methods nite element discretizations quantitative estimates obtained using regularity iterates part depends course strongly specic application 3 convergence analysis section analyze galerkin lmsqp method respect convergence ie convergence reconstruction obtained appropriate stopping rule noise level measure discretization neness tend zero identify innitedimensional case ie assume discrete subspaces satisfy denote e k f k error terms rewrite karushkuhntucker system 212 p r ka 32 r k denotes remainder 10 require condition nonlinearity summarized following assumption 1 let 15 satised dene remainder ru q assume exists constant kee exists solution u parameter identication problem dene discretization measures h h kee u holds remark 1 x h z q h standard niteelement spaces triangulations h h estimated approximation error elements particular discretization parameter ie maximal size triangle tends zero triangulation regular one guarantee h h tend zero cf 32 details choice stopping index use numerical version 19 involves discretization measures dened appropriate choice allows us prove following monotonicity property iterates lemma 31 let assumption 1 fullled let noise bounded 11 assume addition k chosen k k 1 k 2 n stopping index k chosen according generalized discrepancy principle 311 estimates hold k k proof assume q deduce identity noise bound 11 implies using cauchyschwarz inequality together 39 obtain estimate 316 follows dividing 315 k fact induction show q k 2 b q 0 k k satisfying 314 analogous way proof lemma 32 10 prove following statement niteness stopping index k 0 lemma 32 assumptions lemma 31 discrepancy principle 311 yields nite stopping index k chosen according 314 one observes estimates term h plays role noise level innitedimensional setup therefore also possible prove convergence way convergence innitedimensional case 0 cf 10 theorem 35 consequently give detailed convergence proof refer 10 details technique proof recall basic assumptions e give nal convergence result use notation u h k iteration according 212 initial value p h discretization parameter h assumption 2 addition assumption 1 assume e form continuously frechetdierentiable nonlinear operators moreover assume n satisfy nonlinearity conditions kee kee positive constants 2 3 theorem 33 convergence let assumption 2 312 fullled suciently small let noise bounded 11 moreover let k chosen 313 satised perturbed iteration stopped according generalized discrepancy principle 311 uniformly bounded h satisfying 314 q h u q solution 13 proof analogous proof theorem 35 10 4 numerical realization sqpiteration following want discuss numerical methods variants outer iteration ie galerkin lmsqp algorithm assumption able solve discretized kktsystem numerically inner iteration namely numerical solution indenite system 212 investigated section 5 41 scaling state variable parameter lagrangian variable performance iteration algorithm often depends crucially way problem formulated scaling wellknown technique reformulating optimization problem whose main objective twofold one hand variables similar magnitude hand also value derivatives similar size unconstrained optimization problem rescaled way changes iterate one direction result far larger changes value objective changes another direction constrained optimization statements also true constraint additionally set constraints well balanced respect constraint equal weight furthermore set constraints balanced respect objective scaling high practical importance optimization problem many aspects found monographs optimization cf eg 19 30 want consider last aspect context ie scaling state constraint respect objective also high importance achieving fast convergence outer iteration inner iteration aspect scaling included construction good preconditioner outer iteration sqp method tries attain two goals time feasibility iterate respect state constraint optimality iterate respect objective one aspect dominating results usually bad convergence properties feasibility aspect dominates small changes iterate possible order ensure almost feasibility optimality aspect dominates violation state constraint reduced slowly lmsqp method form 2425 turned many situations feasibility aspect strongly dominating using line search methods globalization see also subsection 42 results usually step lengths much smaller one replacing state constraint preconditioned state constraint leads better balanced formulation much faster convergence furthermore step length parameter equal one accepted almost steps another aspect kind rescaling treated subsection 62 42 globalization strategies lmsqp method variant newtons method therefore locally convergent see also analysis section 3 reason globalization strategies trust region methods line search strategies two popular classes globalization techniques optimization needed basic idea trust region methods add additional constraint maximal increment quadratic optimization problem correction step current iterate ie instead 24 25 one would solve 24 25 ku u k q q k k k k chosen appropriately trust region methods successfully applied pde constrained optimization problems see eg 14 38 often using reduced sqp approach want mention similar eect trustregion methods could reached principle controlling penalty parameter k also restricts step size produced good numerical results see example 71 comprehensive overview trust region methods refer conn et al 13 code used twodimensional problems cf 10 example 72 use line search algorithm globalization contrast trust region methods calculation increment split two phases rst search direction determined secondly estimation step length parameter indicating far search direction one go computation search direction solve optimization problem 25 order determine step length cannot use objective criterion unconstrained optimization use merit function balances minimization objective feasibility respect state constraint applied discretized optimization problem form subject equation constraint form possible choices l 1 merit function 43 nested multilevel optimization techniques 13 variants augmented lagrangian estimate lagrangian variable corresponding discretized equation constraint merit functions exact sense suciently large minimizers original constrained optimization problem also minimize merit function crucial property design merit function accept step length one close solution order preserve quadratic convergence sqp method augmented lagrangian works well long estimate lagrangian multiplier accurate enough whereas l 1 merit function sometimes suers socalled marathoseect ie accept unit step length therefore causes slowdown convergence strategy overcome diculty using second order correction found 30 nevertheless performed well numerical experiments see example 72 43 nested multilevel optimization techniques important tools ecient numerical approximation innitedimensional optimization problems multilevel optimization methods nested multilevel setup one starts optimization procedure coarse level x iteration procedure carried eciently appropriate stopping rule satised one interpolates state parameter obtained way ner level x h 2 serving starting value level procedure repeated nest level reached usually nested space used approach ie x h 1 leads simple interpolation operators since one cannot choose discretization data arbitrarily general consider case xed multilevel approach realized analogous way necessary nested multilevel methods outperform standard discretization techniques many cases cf eg 21 22 29 usually considerable number iterations needed coarse level numerical eort per iteration low nest levels stopping rule often satised already one iteration step overall eort less direct discretization nest level galerkin lmsqp method formulate multilevel algorithm follows algorithm 41 nested multilevel galerkin lmsqp given decreasing sequence 1l nested spaces x h x h 1 q h q h 1 eg h nonincreasing sequence satisfying 314 nested multilevel galerkin lmsqp method consists following iterative procedure 1 2 perform galerkin lmsqp method stopping criterion 311 satised stopping index k 3 stop iteration else prolongate iterate u k ner level results new starting value u 1 go step 2 analysis section 3 shows estimate holds error corresponding interpolation iterates level 1 level ie kr ee z kf monotonicity estimate corresponds well intuition iterations needed ne levels particular k decreasing leads kr ee z ne level small expect second term expected negligible ie stopping rule level probably satised k typical conditions x h q h correspond standard niteelement spaces dierent renement levels initial triangulation domain one show least consequently ch ch 1 constant c 2 r together estimate one show standard proof technique converges solution u q parameter identication problem 5 numerical solution kktsystem following discuss numerical solution discretized kktsystem 212 xed iteration number k seen galerkintype approximation 212 original kktsystem stable convergent discuss structural properties important application iterative solution methods construction preconditioners 51 system matrix 15 choosing bases nitedimensional subspaces x h q h may represent via coordinate vectors v order transform 212 linear system v dene matrices vectors allows us rewrite discretized kktsystem penalty parameter respectively matrix 56 structural properties submatrices examined following section 51 system matrix due wellposedness result discretized kktsystem 212 cf theorem 22 may conclude system matrix regular order obtain insight structure investigate properties submatrices g h k l proposition 51 matrices k 2 r mm h 2 r nn symmetric positive denite matrix g 2 r mm symmetric positive semidenite addition operator injective x h g regular proof let u q 52 exist constants c 1 h c 2 h jj denotes euclidean norm r n r respectively thus moreover identity implies g positive semidenite regular assumption e injective x h symmetry matrices g h k veried similar way using symmetry scalar products selfadjointness operator k k matrix l 2 r mn dicult analyze neither symmetric regular general particular n 6 however fundamental properties regularity rely rather g h k l moreover classical splitting symmetric saddlepoint problem g 0 k h h c schurcomplement possible know g h regular particular may conclude n positive negative eigenvalues 52 reduced sqp approaches basic idea reduced sqpmethods apriori elimination equality constraint written matrix form equivalent elimination v 56 due proposition 51 k regular symmetric matrix thus may compute yields calculations n nsystem reduced sqpapproach seems particular interest n frequently used discretization strategy parameter identication optimal control problems cf eg 35 36 37 original matrix indenite matrix size 2mn 2mn 53 simultaneous solution kktsystem 17 reduced system matrix r 512 size n n however r sparse matrix even submatrices sparse since involves inverse k moreover evaluation r expensive evaluation original system matrix since involves solution two systems form dierent righthand sides g evaluation direct evaluations k needed cheap typical nite element discretization state constraint practice one usually tries compensate disadvantage reduced sqpmethods using broydentype update reduced system matrix instead exact matrix r leads ecient optimization algorithms small n 53 simultaneous solution kktsystem recently simultaneous solution kktsystems iterative methods investi gated particular connection optimal control problems cf 2 4 5 20 compared reduced sqpapproach simultaneous solution strategy obvious advantage allocation evaluation system matrix much cheaper r payo indenite larger r might cause additional eort ever main eort reduced sqpapproach related evaluation assembly system matrix r respectively therefore simultaneous solution kktsystem result tremendous speedup sqpmethod particular ne discretizations rst glance seems rather straightforward solve 57 standard iterative method indenite systems inexact uzawa methods cf 6 15 krylovsubspace methods gmres cf 34 minres cf 31 qmr cf 18 however case largescale problems expect large condition number note usually small singular complicated eigenvalue pattern matrix might cause iterative methods diverge need high number iterations therefore appropriate preconditioning technique seems necessary methods go details refer forthcoming paper 11 discussion preconditioners following distinguish two types solvers seem appropriate solution indenite system 57 discuss basic properties respect special structure inexact uzawa iterations inexact uzawa methods similar iteration procedures developed solution classical stokes system similar problems cf 32 overview classical uzawa method gradient method dual corresponding lagrange functional inexact uzawa method interpreted preconditioned version cf 32 following exposition zulehner 39 write inexact uzawa method system form 56 followed preconditioner diagonal matrix c preconditioner schurcomplement c dened 58 terms 57 write inexact uzawa iteration preconditioner system matrix given convergence analysis method available case regular matrix cf 6 39 means assume g regular latter true eg data z represent distributed data state ie e embedding operator case structure rather simple dicult task construct preconditioner even exact preconditioning seems possible note g mass matrix typical nite element discretization since matrices g h change sqpiteration may even compute decompositions preprocessing step construction preconditioner schurcomplement c dicult must take account specic nature underlying state equation krylovsubspace methods krylovsubspace methods gmres qmr variants cgalgorithm applicable indenite problems basic idea methods defect minimization krylovsubspace generated x 1 kth iteration step since preconditioned cgmethods probably successful class iteration methods positive denite systems methods seem attractive also indenite case although additional diculties may arise cf eg 34 convergence analysis 34 18 shows error bounds obtained methods essentially mainly dependent eigenvalue distribution condition number system matrix therefore appropriate preconditioning high importance case also possibility g singular refer 11 detailed discussion problem rst application investigate identication potential q elliptic boundary value problem state observation l 2 wellstudied problem literature cf eg 33 10 shown setup denotes space dimension operators satisfy assumptions needed convergence analysis lmsqpmethod shall study concrete niteelement discretization kktsystem derivation estimates numerical errors h h 61 error estimates discretized kktsystem case write whole kktsystem classical form homogenous dirichlet boundary conditions upon v l dimensiondependent dierential operator order 2d corresponding norm h 7 eg supplemented homogenous boundary conditions order 1 f 2 l 3 standard elliptic regularity argument shows 0for k 2 n way show k 2 h k 2 h 7 additional regularity employed derive standard error estimates niteelement discretizations kktsystem 212 use piecewise linear nite elements regular triangulations h discretization spaces z x h h represent neness grids classical approximation result nite elements cf 32 p96 implies course one could also use piecewise constant elements would yield however practical applications higherorder approximation often desirable since signicantly larger reasonable choice h canonical approximation parameter q nite element space order greater equal regular triangulation h assumptions exact solution q one obtain quantitative estimates h terms h rst glance seems surprising one needs apriori assumptions parameter state order derive error estimates however due illposedness identication problem respect parameter q apriori knowledge seems necessary approximation state corresponds rather approximation underlying elliptic state equation wellposed respect u yields regularity nally want mention according theory developed one could choose h independent h would cause unnecessary complications implementation method note alternatively one use space 3 yields appropriate discretization strategy eg choose q h space piecewise constant elements underlying grid h advantage approach elements order greater one necessary 2 avoided 62 structure system matrix potential identication problem parts system matrix 56 well understood first g l 2 mass matrix positive denite triangulations h coincide assume following eigenvalues g order h matrix h stiness matrix dierential operator l minimal eigenvalue order h maximal eigenvalue order h matrix k sum stiness matrix laplacian weighted mass matrix weight q k l 2 scalar product one expect rst part sum dominating thus stiness matrix k laplacian good preconditioner k maximal minimal eigenvalues k k order h 2 h respectively remaining part system matrix namely matrix l dicult understand since elements weighted l 2 scalar products basis functions dierent nite element spaces however spectral norm l estimated order h construction preconditioners g h wellinvestigated even exact preconditioning seems applicable k seems reasonable use preconditioner k laplacian eg multigrid preconditioner preconditioning k system matrix transformed corresponding schurcomplement k appropriate preconditioner k estimate minimal eigenvalue min c min maximal eigenvalue hence condition number c independent h depends h h one observes condition number decreasing h tends h note usually h h uzawa iteration one choose preconditioner c case multiple k 1 even g 1 h h uzawa iteration seems optimal case one apply either reduced sqpapproach use krylovsubspace methods dierent preconditioning strategies details latter refer 11 7 numerical experiments order test theoretical results numerically solve model problems already investigated respect convergence behavior lmsqpmethod 10 example 71 rst example identication potential q 61 62 state observation u 2 l 2with exact potential given element 3 problem implemented softwaresystem matlab data generated solving state equation ne grid subsequent interpolation coarser grid noise additive highfrequency perturbation used uniform grids nodes discretization state u lagrangeparameter n nodes parameter q ie parameters k chosen according lead convergence method even starting value q 0 kktsystem 56 solved qmr method using uzawatype preconditioner described section 62 k preconditioner laplacian convergence results overall lmsqpmethod shown 10 compared levenbergmarquardt method following feasible path turned methods lead almost iteration sequence q k particular number iterations needed stopping rule satised methods compare numerical eciency lmsqpmethod feasible path approaches namely levenbergmarquardt method lm feasible path galerkin discretization lmsqp solution gaussnewton system preconditioned cgmethod broydentype variant levenbergmarquardt method cf 23 details 22 7 numerical experiments table 1 cputime seconds needed lmsqpmethod lmmethod broydentype variant lmmethod sake choose dierent discretization levels xed iteration measure cputime needed lmsqpmethod stopping rule satised xed noise level results shown table 1 one observes lmsqpmethod simultaneous solution kktsystem outperforms feasiblepath approaches dierent discretizations since lmsqp lmmethod need number outer iterations dierence numerical eort caused fact eort evaluation system matrix lmmethod signicantly higher evaluation preconditioning system matrix simultaneous lmsqpmethod obviously gain numerical eort evaluation system matrix increases number discretization points explains extremely large cputime lm method nest discretization level much faster lmmethod caused fact evaluation system matrix carried eciently however number iterations needed broydentype variant much larger two methods use full information derivatives finally investigate spectral condition system matrix well matrix dened 613 use preconditioner laplacian k left picture figure 1 shows condition number function discretization size h logarithmic scale xed one observes condition number grows quadratically h 1 condition number much smaller almost independent h second part figure 1 shows plot condition numbers vs parameter doubly logarithmic scale seems growth condition number 0 slower original matrix cases condition number seems convex function unique minimum however value rather large values signicantly larger interest purpose since would cause tremendous slowdown outer iteration therefore focus attention case condition number increases monotonically 1 example 72 second numerical example identication conductivity q 2 logcondm condition number vs discretization size original preconditioned state log b logcondm condition number vs b original preconditioned state figure 1 plot spectral condition matrix vs discretization size h logarithmic scale left vs parameter doubly logarithmic scale right solid line shows condition number original matrix dashed line matrix preconditioned state equation state observation u 2 l 2 domain ball r 2 missing rst quadrant ie radial coordinates exact parameter reconstructed q 1 righthand side 71 given r corresponding solution state equation 3r data generated using exact solution u perturbed uniformly distributed random noise discretization used triangular nite elements piecewise quadratic shape functions state u lagrange parameter piecewise constant shape functions parameter q results calculated using nite element code fepp 27 developed department analysis computational mathematics university linz want mention identication problem quite challenging due complicated geometry also due fact q identiable along level line interior u attains extremum destroy theoretical identiability results set lebesguemeasure zero expected create numerical diculties results exact data found table 2 good performance method respect cpu time number outer iterations observed clearly especially problems ne discretizations parameter q method still realized eciently classical approaches yield results reasonable time plot level dim q dim u avg qmr sqp time table 2 cputime number inner qmr outer sqp iterations exact data figure 2 parameter distribution exact data level 4 parameter q found figure 2 one observes parameter reconstructed well except neighborhood level curve 0g additional speedup gained using multilevel approach described subsection 43 used nested spaces q u subdividing triangular element four smaller elements rening mesh table 3 presents results approach seen ne discretization levels one sqp step sucient fullling stopping criterion corresponds well theoretical predictions made section 43 comparison results ones table 2 shows xed discretization level solution identication problem level 5 slightly faster identication q level 6 fourfold number parameters using multilevel approach plot level dim q dim u avg qmr sqp time acc time table 3 cputime per level accumulated time number inner qmr outer sqp iterations exact data using nested multilevel approach figure 3 parameter distribution exact data level 4 using nested multilevel approach parameter found figure 3 approximation parameter area identied far better classical approach using one discretization level compare figure 2 8 conclusions outlook developed framework galerkintype approximations lmsqpmethod parameter identication problems elliptic partial dierential equations discussed implementation galerkin lmsqpmethod iterative solution kktsystem numerical results show resulting iteration method clearly outperforms stateoftheart methods iterative regularization provides tool ecient solution identication problems ne discretizations moreover developed multilevel version galerkinlmsqp method yields speedup crucial point possibility obtain ecient implementation lmsqp method preconditioning kktsystem solved iteratively indenite problem product space state parameter lagrangian variable construction preconditioners simple task discussed detail present paper investigated 11 dierent preconditioning techniques compared numerical aspects investigated future research adaptive discretization strategies fast parallel solvers based domaindecomposition techniques adaptive discretization optimal control problems closely related subject discussed becker et al 3 possibly ideas work carried identi cation problems parallel solution optimal control problems investigated lions pironneau 28 case quadratic problems recently biros ghattas 4 5 performed numerical study parallel solver sqpmethod outer preconditioned krylovsubspace methods inner iteration many ideas seem applicable also parameter identication problems solved lmsqp method rises hope ecient parallel versions lmsqpmethod designed also largescale identication problems impedance tomography finally want recall framework problem apply transient problems parabolic hyperbolic type since numerical methods dierent types partial dierential equations many typespecic features general surprising also numerical treatment parameter identication problems depend type underlying state equation however seems possible construct ecient convergent discretized methods least case parabolic equations important task future research acknowledgments authors thank dr walter zulehner university linz dr joachim schoberl cur rently texas university useful stimulating discussions preconditioning indenite system 56 r estimation techniques distributed parameter systems preconditioners karushkuhntucker matrices arising optimal control distributed systems parallel lagrangenewtonkrylovschur methods pdeconstrai ned optimization problems parallel lagrangenewtonkrylovschur methods pdeconstrai ned optimization problems analysis inexact uzawa algorithm saddle point problems existence mixed hybrid finite element methods iterative regularization parameter identi inverse problems partial di inexact preconditioned uzawa algorithms saddle point problems inverse problems di convergence rate results iterative methods solving nonlinear illposed problems qmr quasiminimal residual method nonhermitian linear systems preconditioned allatonce methods large numerical solution control problem governed phase broydens method regularization nonlinear illposed prob lems projectionregularized newton method nonlinear illposed problems application parameter identi cation problems nite element discretization sur le controle parallele des systemes distribues solution sparse inde numerical approximation partial di determination source term linear di gmres generalized minimal residual algorithm solving nonsymmetric linear systems control applications reduced sqp methods partially reduced sqp methods largescale nonlinear optimization problems solving discretized optimization problems partially reduced sqp methods global convergence trustregion interiorpoint algorithms nitedimensional nonconvex minimization subject pointwise bounds analysis iterative methods saddle point problems uni tr