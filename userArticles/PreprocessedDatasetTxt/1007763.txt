optimal time bounds approximate clustering clustering fundamental problem unsupervised learning studied widely problem learning mixture models optimization problem paper study clustering respect kmedian objective function natural formulation clustering attempt minimize average distance cluster centers one main contributions paper simple powerful sampling technique call successive sampling could independent interest show sampling procedure rapidly identify small set points size ok log fracnk summarize input points purpose clustering using successive sampling develop algorithm kmedian problem runs onk time wide range values k guaranteed high probability return solution cost constant factor times optimal also establish lower bound nk randomized constantfactor approximation algorithm kmedian problem succeeds even negligible say frac1100 probability best previous upper bound problem nk notation hides polylogarithmic factors n k best previous lower bound nk applied deterministic kmedian algorithms focus presentation kmedian objective upper bounds valid kmeans objective well context algorithm compares favorably widely used kmeans heuristic requires onk time one iteration provides useful approximation guarantees b introduction given set points pairwise distances points goal clustering problems partition points number sets points set close respect objective function clustering algorithms widely used organize large data sets areas data mining information retrieval example may wish partition set web logs infer certain usage patterns divide corpus documents small number related groups given set points associated interpoint distances let median set point set minimizes sum distances points set remark median essentially discrete analog centroid also called medoid 9 clustering problem consider asks us partition n weighted points k sets sum sets weight point times distance median set minimized approaches type clustering problem kmeans heuristic wellstudied 4 9 refer problem clustering variant classic kmedian problem kmedian problem asks us mark k points sum points x weight x times distance x nearest marked point minimized straightforward see convert solution kmedian problem solution clustering variant onk time thus focus kmedian problem developing upper bounds also restrict attention metric version problems throughout paper given distance matrix defines metric space set input points distances nonnegative symmetric satisfy triangle inequality distance points x zero remark sake brevity write kmedian problem mean metric kmedian problem throughout remainder paper since problem instances application areas mentioned tend large motivated ask input characteristics point weights interpoint distances affect complexity kmedian problem clustering variant weighted points useful number applications example may wish prioritize objects input ask following natural question allowing inputs arbitrary point weights incur substantial time penalty note even moderate weights say 2 naive approach viewing weighted point collection unitweight points increases input size dramatically certain applications interpoint distances may lie relatively small range thus motivated ask constraining distances small range admit substantially faster algorithms resolve questions wide range input parameters establishing time bound nk kmedian problem clustering variant thus show many cases large point weights incur substantial time penalty cannot hope develop substantially faster algorithms even interpoint distances lie small range stating results introduce useful terminology use throughout pa per let u denote set points given instance kmedian problem assume u nonempty configuration nonempty subset u mconfiguration configuration size points x u let wx denote nonnegative weight x let dx denote distance x cost configuration x denoted cost x defined denote minimum cost mconfiguration opt brevity say mconfiguration cost opt k aconfiguration kmedian algorithm aapproximate produces aconfiguration kmedian algorithm aapproximate k aapproximate let r denote ratio diameter u ie maximum distance pair points u minimum distance pair distinct points u let rw denote ratio maximum weight point u minimum nonzero weight point u remark assume without loss generality least one point u nonzero weight since problem trivial otherwise let r main result randomized o1approximate kmedian algorithm runs onk time subject constraints k algorithm succeeds high probability positive constant adjust constant factors definition algorithm achieve failure probability less n establish matching lower bound running time randomized o1approximate kmedian algorithm nonnegligible success probability eg least 1 subject requirement r exceeds nk sufficiently large constant factor relative desired approximation ratio obtain tight bounds clustering variant also prove time lower bound o1approximate algorithm require r sufficiently large constant relative desired approximation ratio additionally lower bounds assume o1 main technical result successive sampling technique use algo rithms basic idea behind technique take random sample points set aside constant fraction n points close sample recurse remaining points show technique rapidly produces configuration whose cost within constant factor optimal specifically case uniform weights successive sampling algorithm yields k log nk o1configuration high probability onmaxfk log ng time addition sampling result algorithms rely extraction technique due guha et al 5 uses black box o1approximate kmedian algorithm compute k o1 configuration o1assignment black box algorithm use lineartime deterministic online median algorithm mettu plaxton 10 developing randomized algorithm kmedian problem first consider special case uniform weights 1 special case provide randomized algorithm running onmaxfk log ng time subject constraint r log n uniformweights algorithm based directly two building blocks discussed apply successive sampling algorithm obtain k log nk o1configuration use extraction technique obtain k o1configuration use algorithm develop kmedian algorithm case arbitrary weights algorithm begins partitioning points r w powerof2 weight classes applying uniformweights algorithm within weight class ie ignore differences weights belonging weight class less factor 2 apart union r w kconfigurations thus obtained r w k o1configuration make use extraction technique obtain k o1configuration r w k o1configuration 11 problem definitions without loss generality throughout paper consider fixed set n points u associated distance function u u ir associated nonnegative demand function assume metric nonnegative symmetric satisfies triangle inequality dx configuration x set points let cost set points x let wx denote x2x wx define assignment function u u assignment let u denote set fx j x 2 ug refer assignment j uj massignment given assignment define cost denoted c wx straighforward see assignment cost u c brevity say assignment j uj cost opt k aassignment assignment set points x let c input kmedian problem u w integer k 0 k n since goal obtain k o1configuration assume without loss generality input points nonzero weight note 0 n removing zero weight points mconfiguration doubles cost see consider mconfiguration x obtain mconfiguration x 0 replacing zero weight point closest nonzero weight point using triangle inequality straightforward see cost x 0 2cost x argument used show minimumcost set size contained set nonzero weight input points cost twice opt also assume input weights scaled smallest weight 1 thus input weights lie range 1 rw output kmedian problem requires us compute minimumcost kconfiguration uniform weights kmedian problem special case wx fixed real points x output also minimumcost kconfiguration 12 comparison previous work first o1approximate kmedian algorithm given charikar et al 3 subsequently several improvements approximation ratio see eg 2 results ci tations section focus results relevant present paper compare results recent sublineartime algorithms kmedian problem first results due indyk gives randomized ok o1approximate algorithm uniform weights kmedian problem 6 indyks algorithm combines random sampling input points blackbox k approximate kmedian algorithm achieve 163k 2approximate algorithm desired success probability given 2 time 1 blackbox kmedian algorithm indyks algorithm runs factor running time 26 2 k indyks algorithm takes nk log points runs blackbox kmedian algorithm points obtain configuration blackbox algorithm run set points distant points x produce another configuration final output union x shown ok o1configuration thorup 13 gives randomized o1approximate algorithms kmedian kcenter 1 onotation omits polylogarithmic factors n k facility location problems graph problems given metric distance function rather graph input points positively weighted edges distances must computed algorithms 13 run om time thorup 13 also gives randomized constantfactor approximation algorithm kmedian problem consider polylogarithmic factor running time 44 4 n part kmedian algorithm thorup gives successive sampling technique also consists series sampling steps produces ok log 2 n configuration positive real 0 04 probability 12 successive sampling technique similar spirit algorithms take total olog nk samples size ok construct ok log nk o1 assignment union samples overall sample size much smaller indyks algorithm ok log nk points versus nk log smaller sample size thorups algorithm logarithmic factor however algorithm produces ok log nk o1assignment whereas indyks algorithm produces ok o1configuration addition ally algorithms indyk thorup succeed constant probability sampling algorithm guaranteed succeed high probability guha et al 5 give kmedian algorithms data stream model computation data stream model computation input data processed sequentially performance algorithm measured many passes makes input space requirements guha et al 5 give singlepass o1approximate algorithm kmedian problem runs time requires space positive constant algorithm uses indyks kmedian algorithm black box hence polylogarithmic factor running time also mishra et al 11 show order find k o1configuration enough take sufficiently large sample input points use input blackbox o1approximate kmedian algorithm compute k o1configuration arbitrarily high constant prob ability required sample size k running time technique depends blackbox algorithm used general case size sample may large n depending diameter input metric space technique yield running times diameter 2 k noted earlier also make use technique due guha et al 5 takes o1configuration extracts k o1configuration use technique isolation divideandconquer fashion develop kmedian algorithms view extraction technique postprocessing step yields k o1approximate kmedian algorithm given o1approximate kmedian algorithm algorithms take advantage fact postprocessing step performed rapidly example blackbox algorithm requires 2 time time required postprocessing ok 2 guha et al 5 establish lower bound deterministic o1approximate kmedian algorithms note work slightly different definition kmedian problem distance two distinct points allowed 0 adopt view points distance zero represented single point commensurately higher weight view avoids infinite value r proof lower bound guha et al 5 construct problem instance optimal solution cost 0 reduce problem graph k partitioning problem 7 intuition algorithm producing kconfiguration nonzero cost o1approximate although problem instance contains distinct points distance 0 ie infinite r slight modification proof requires r exceed n sufficiently large constant factor relative desired approximation ratio intuitively large setting r deterministic kmedian algorithm taking onk time making one mistake fails achieve desired approximation ratio lower bounds stronger sense focus constructing problem instances small values r show randomized kmedian algorithms running onk time likely make many mistakes instances 13 outline rest paper organized follows section 2 present analyze successive sampling algorithm section 3 make use sampling algorithm conjunction extraction result develop o1approximate uniform weights kmedian algorithm section 4 use uniform weights algorithm subroutine develop o1approximate kmedian algorithm case arbitrary weights present lower bounds kmedian problem clustering variant appendix approximate clustering via successive sampling first result successive sampling algorithm constructs assignment cost probability make use algorithm develop uniform weights kmedian algorithm remark assume arbitrary weights proofs since arguments generalize easily weighted case furthermore weighted result may independent informally speaking algorithm works sampling steps step take small sample points set aside constant fraction weight whose constituent points close sample recurse remaining points since eliminate constant fraction weight sampling step number samples taken logarithmic total weight able show using samples taken possible construct assignment whose cost within constant factor optimal high probability uniform weights kmedian problem sampling algorithm runs onmaxfk log ng time give kmedian algorithm case arbitrary weights section 4 throughout section use symbols k 0 denote real numbers appearing definition analysis successive sampling algorithm value k 0 chosen ensure failure probability algorithm meets desired threshold see paragraph preceding lemma 23 discussion choice k 0 asymptotic bounds established paper valid choice 0 1 also make use following definitions ball pair x r center x belongs u radius r nonnegative real given ball x r let pointsa denote set fy 2 u j dx rg however sake brevity tend write instead pointsa example write x 2 b instead x 2 pointsa pointsa pointsb respectively set x nonnegative real r define ballsx r set x2x x r 21 algorithm following algorithm takes input instance kmedian problem produces assignment high probability c ocost x kconfiguration x construct set points sampling replacement bk 0 c times u sampling step probability selecting given point proportional weight point u compute distance nearest point using lineartime selection distances computed previous step compute smallest real wballss x c choose point dx let let u note loop terminates since wu wu i1 0 let total number iterations loop let c choice c iteration loop termination condition olog wuk 0 uniform demands kmedian problem simply olog nk 0 first step follows juj otk 0 first step algorithm performed onk 0 iterations iteration second third steps performed time oju using weighted linear time selection algorithm uniform demands kmedian problem computation requires iterations running times third fourth steps negligible thus uniform demands kmedian problem total running time algorithm onk 22 approximation bound goal section establish theorem 1 proof theorem makes use lemmas 23 25 211 established remark theorem 1 used sections 3 4 theorem 1 high probability c ocost x kconfiguration x proof claim lemma 23 holds high probability set k log ng appropriately large theorem follows lemmas 23 25 211 proof lemma 23 relies bounding failure probability certain family random experiments begin bounding failure probability simpler family random experiments related wellknown coupon collector problem positive integer nonnegative reals b let us define fm b probability bins remain empty dbe balls thrown random uniformly independently bins techniques analyzing coupon collector problem see eg 12 used obtain sharp estimates fm b however following simple upper bound sufficient purposes lemma 21 positive real exists positive real positive integers real b fm b e b proof note crude upper bound fm b given probability obtaining 1 successes dbe bernoulli trials success probability claim follows choosing sufficiently large applying standard chernoff bound mind following tail bound x random variable drawn bernoulli distribution n trials trial success probability p 0 1 pr fx 1 npg e 2 np2 see 1 appendix derivation develop weighted generalization preceding lemma positive integer nonnegative reals b mvector nonnegative reals r define define gm b v follows consider set bins numbered 0 1 bin associated weight r let r denote total weight bins assume dbe balls thrown independently random one bins bin chosen probability define gm b v probability total weight empty bins balls thrown ar lemma 22 positive real exists positive real positive integers real b gm b v e b mvectors v nonnegative reals proof fix b v use lemma 21 deduce existence suitable choice depends strategy reducing claim unweighted counterpart partition almost weight associated weighted bins subbins equal weight specifically let denote r partition weight r associated bin complete subbins weight one incomplete subbin weight less furthermore ball thrown particular bin imagine throw refined particular subbin bin probability particular subbin chosen proportional weight note total weight incomplete subbins less r2 furthermore assume without loss generality 1 since claim holds vacuously 1 follows less half total weight r lies incomplete subbins thus standard chernoff bound argument positive real 0 choose sufficiently large ensure following claim holds probability failure e b 2 ie half desired failure threshold appearing statement lemma least 0 b dbe balls thrown complete subbins number complete subbins since least half total weight r belongs complete subbins 0 2m accordingly suitable application lemma 21 establish existence positive real 0 depending least 0 b balls landed complete subbins probability number empty complete subbins exceeds 0 2 e b 2 claims two preceding paragraphs conclude exists de pending following statement holds probability failure e b number empty complete subbins 0 2 note total weight complete subbins 2t argued earlier total weight incomplete subbins also r2 thus exists positive real dbe ball tosses probability total weight empty bins r e b remainder section fix positive real 1 also let denote minimum real exists kconfiguration x property wu establishes main probabilistic claim used analysis algorithm section 21 note lemma holds high probability taking neg appropriately large lemma 23 positive real exists sufficiently large choice 2 0 probability failure e k 0 proof fix let x denote kconfiguration wballsx wu us define point u good belongs ballsx bad otherwise let g denote set good points associate good point closest point x breaking ties arbitrarily point x x let x denote set good points associated x note sets x form partition g recall denotes ith set sample points chosen algorithm x x say covers x iff x nonempty point say covers iff exists x x belongs x covers x let g 0 denote set points covered note g 0 g establish lemma proving following claim positive reals exists sufficiently large choice wg 0 1 wg probability failure e k 0 claim implies lemma factor appearing definition less factor appearing definition points covered remains prove preceding claim first note definition implies least fraction total weight associated good points thus standard chernoff bound argument implies positive reals exists sufficiently large choice least k 0 bk 0 c samples associated construction good probability failure e k 0 2 ensure wg 0 least 1 wg failure probability e k 0 2 apply lemma 22 viewing sample associated good point ball toss set x bin weight wa x claim follows lemma 24 0 c c proof observe second step follows definition c construction x 0it proof observe 0it 0it first step follows since sets c 0 form partition u second step follows lemma 24 throughout remainder section fix arbitrary kconfiguration x 0 let f denote set fx 2 u g integer 0 lemma 26 let j integers 0 0 j 0 proof without loss generality assume j definition f j intersect lemma 27 let integer 0 let subset f wf cost x wy proof first note definition wf least 1 definition f thus cost x lemma 28 j 0 0 j 0 cost proof lemma 26 j 0 t0 j 0 cost cost lemma 27 cost x f claim follows remainder section let 3e lemma 29 0 wf ir 1wf proof note wf ir last step follows lemma 27 claim follows definition r lemma 210 0 wf r wf r second step follows lemma 29 lemma 211 kconfiguration x cost x 0it g fix kconfiguration x cost x least cost r 0it 0it 0it 0it first step follows lemma 28 second step follows averaging choice j third step follows lemma 210 fourth step follows lemma 27 last step follows since c u 3 efficient algorithm case uniform weights theorem 24 guha et al 5 implies given o1configuration x compute k o1configuration simply running o1approximate kmedian algorithm modified problem instance obtained redistributing point weights follows weight given point x moved point x dx follows analysis algorithm smallspace guha et al 5 since corresponds case o1configuration output step 2 remarked although algorithm smallspace presented manner assumes output step 2 ok o1configurations analysis smallspace given 5 easily seen hold general case output step 2 collection o1 configurations theorem 1 output sampling algorithm o1assignment high probability ng log nk case uniform weights thus u o1configuration high probability directly apply guha et al 5 technique extract k o1configuration u trouble approach direct application technique expends computing closest point u point u fortunately straightforward verify following variation guha et al 5 technique also valid given o1assignment redistribute weight point x x run o1approximate kmedian algorithm modified problem instance remark 5 section 2 point 0 defined median closest point purposes variation point 0 instead defined analyze running time algorithm compute assignment use sampling algorithm parameter k 0 set omaxfk log ng time required compute onmaxfk log ng note required weight function computed execution sampling algorithm without increasing running time deterministic online median algorithm mettu plaxton 10 used complete extraction step ojuj 2 time total time taken algorithm therefore first step follows analysis sampling algorithm case uniform weights choice k 0 overall running time log nk maxfk log ng note k 148 n kr 2 simplifies onk 4 efficient algorithm case arbitrary weights algorithm developed sections 2 3 o1approximate kmedian problem arbitrary weights however time bound established case uniform weights apply case arbitrary weights running time successive sampling procedure slightly higher latter case precisely running time sampling algorithm section 2 onk 0 log wu case arbitrary weights section use uniformweight algorithm developed sections 2 3 develop kmedian algorithm case arbitrary weights time optimal certain range k first give informal description algorithm consists three main steps first partition input points according weight r w sets next run uniform weights kmedian algorithm resulting sets show union resulting outputs okrw o1configuration obtain k o1configuration creating problem instance okrw o1configuration computed previous step feeding problem instance input o1approximate kmedian algorithm give precise description kmedian algorithm let uniform weights kmedian algorithm sections 2 3 let b o1approximate kmedian algorithm compute x set input points distance function fixed weight parameter k denote output let denote assignment induced z point x x 2 z let let assignment corresponding union assignments defined previous step let w denote weight function corresponding union weight functions run b u set input points distance function weight function output resulting kconfiguration note second step k 0 defined terms n ie ju j jb j thus argument proof theorem 1 implies succeeds high probability terms n assuming r w polynomially bounded n high probability every invocation successful observe algorithm corresponds special case algorithm small space 5 parameter set r w uniform weights algorithm section 3 used step 2 smallspace online median algorithm 10 used step 4 small space thus 5 theorem 24 implies output b k o1configuration high probability discuss running time algorithm straightforward compute sets b time uniform weights kmedian algorithm requires ojb time compute z time required invocations r w nk 0 r w kr w kr w first step follows fact sum maximized jb note weight function computed ojb j w computed time employ online median algorithm 10 blackbox kmedian algorithm b since juj kr w time required invocation b okrw time required kmedian algorithm therefore kr w r note k 197 n kr 2 krw simplifies onk r probabilistic method improved combinatorial algorithms facility location kmedian problems pattern classification scene analysis clustering data streams time algorithms metric space problems randomized query processing robot path planning lower bounds randomized exclusive write prams foundations statistical natural language processing online median problem clustering randomized algorithms quick kmedians probabilistic computations toward unified measure complexity tr randomized algorithms constantfactor approximation algorithm italickitalicmedian problem extended abstract time algorithms metric space problems foundations statistical natural language processing sublinear time approximate clustering learning mixtures arbitrary gaussians new greedy approach facility location problems quick kmedian kcenter facility location sparse graphs online median problem improved combinatorial algorithms facility location kmedian problems learning mixtures gaussians clustering data streams pattern classification 2nd edition approximation algorithms np hard clustering problems ctr shai bendavid framework statistical clustering constant time approximation algorithms kmedian kmeans clustering machine learning v66 n23 p243257 march 2007 kobbi nissim sofya raskhodnikova adam smith smooth sensitivity sampling private data analysis proceedings thirtyninth annual acm symposium theory computing june 1113 2007 san diego california usa c greg plaxton approximation algorithms hierarchical location problems journal computer system sciences v72 n3 p425443 may 2006 dan feldman morteza monemizadeh christian sohler ptas kmeans clustering based weak coresets proceedings twentythird annual symposium computational geometry june 0608 2007 gyeongju south korea gereon frahling christian sohler fast kmeans implementation using coresets proceedings twentysecond annual symposium computational geometry june 0507 2006 sedona arizona usa