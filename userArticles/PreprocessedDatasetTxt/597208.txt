determination based video audio features determining automatically constitutes scene video challenging task particularly since precise definition term scene left individual set attributes shared consecutive shots group scenes certain basic attributes dialogs settings continuing sounds consistent indicators therefore developed scheme identifying scenes clusters shots according detected dialogs settings similar audio results experiments show automatic identification types scenes reliable b introduction depending length shots either letters words video productions hence contain little semantic information information image track often reduced simple keyframe panorama 25 audio track usually either incomplete like part dialog limited like cry order partition video semantically richer entities shots must grouped together based content procedure often denoted shot clustering 2 general shot clustering performed easily restricted certain type film genre exam ple clustering shots newscasts often boils finding anchor shots 171930 news stories scenes determined grouping shots two anchor shots including first anchor shot however restrict shot clustering work specific film genre preferring universally applicable approach possible work focuses shot clustering genreindependent fashion ultimate goal automatically determine shot clusters human would judge scenes scene usually composed small number interrelated shots unified location dramatic incident4 literature also called video paragraphs 9 story segments25 story units6 unfortunately clustering shots scenes depends subjective judgements semantic correlation analogous clustering text sentences paragraphs school simple rules forming paragraphs might longer 7 lines describe one idea despite general rules text given different people yield different paragraphs true video scenes experiments however showed basic units clustered viewers contiguous shots setting dialogs shot sequences combined audio continuing music background paper present algorithms automatic determination scenes begin discussion related work section 2 section 3 gives overview system automatic determination shots addressed section 4 section kept short concerns rather wellexamined subject clustering shots depends attribute stands foreground attribute calculated via one content features deemed important representing attribute audio content described section 51 face dialog determination section 52 like setting information based color orientation hints section 53 features yield normalized table distances shots video exploited clustering section 6 present experimental results algorithms section 7 prior concluding paper discussion research area 1this research mainly performed author university mannheim praktische informatik iv 68131 mannheim germany 2this term actually misnomer since clustering contiguous shots one aspect shot clustering ably thus frames part dissolve fade removed shots 5 feature calculation 51 audio sequence determination great semantic changes videos occurs usually occur conjunction profound changes audio track therefore define audio sequence sequence shots similar audio signal sequences recur examples include music continuing several shots ongoing dialog ongoing noise like cheering crowd define two types audio sounds handled differently background foreground background sounds video give general feeling atmosphere scene carry information necessary understanding film story otherwise would foreground audience could easily hear understand however background sounds dynamic foreground sounds treated manner foreground sounds cause many audio sequence endpoints intended interest therefore first step determining audio sequences determine time periods containing exclusively background sounds call periods background segments background segments disregarded processing foreground segments analyzed spectral content great changes registered audio cuts calculation characteristic vector calculated time period two audio cuts time periods called audio shots vectors used calculate table distances video shots sequential video shots whose audio shots differ greatly belong different audio sequences must therefore assigned large distance value profound changes audio track may also occur within shots example would explosions changes problem disturb measure two reasons first audio shots usually also part video shot enter distance measure giving small distance second several sequential video shots compared similarity odd one disturb big picture 511 features audio background whenever sounds video meant background loudness reduced substantially therefore background segments determined via loudness threshold implementation first calculates perceptionbased loudness measure amplitude values audio track video22 determine maximum loudness value used calibrate digitization level sound file percentage maximum loudness value gives loudness threshold value percentage critical analysis automatic determination difficult used values 005 05 percentage depends average loudness level movie great amounts quietly spoken speech choose low level order declare speech background audio file analyzed windows size advanced steps size windows contain loudness values threshold considered background windows size dependent desired minimaum resolution background segments selection small window might mean detecting small breaks words background segments selecting big window skiping small important background segments scenes found good choices 50 250 ms close value alternative method determining background segments would calculate signal power analysis windows measure works similar loudness measure except calculate signal power windows perform thresholding calculated power values experimented work well loudness thresholding signal power seem good feature describe background segments audio cuts audio cuts time instances delimit time periods similar sound hereto sliding hamming window size 100 ms used calculate fourier transform complex fourier coefficients converted real values calculation decibels resulting realvalued feature vector window called audio feature vector represents distribution intensities frequencies within window l term shot clustering denotes two fundamentally different kinds shot grouping first kind aims finding groups similar shots within entire video without thought temporal order see eg 21431 useful similarity measures considered detail 16 approach paper belongs second type shot clustering grouping contiguous shots according semantic correlation much work published type shot clustering via video analysis features yeo yeung proposed scheme recover story units video using timeconstrained clustering 262728 basic idea assign label shots similar analyze patterns resultant strings three different kinds temporal events extracted label patterns dialog action story unit event regarded scene video yeo yeung use color histograms measure similarity never compare computed scenes handmade scenes aoki shimotsuji hori used labelled keyframes represent content shots cluster 1 general keyframebased systems suffer ignorance dynamic aspects shots duration eventsactions time take place shot example appearance new actor end shot approaches determine scenes based audio analysis confusingly use digitized audio basis written transcript closed captions 1419 many cases transcript closed captions either available exist home videos order make approach general possible use information therefore work concentrates ambitious task analyzing digitized audio directly idea similar approach audio sequence determination presented saraceno leonardi 24 segment classify audio track videos silence speech music noise segments using segments support detection shot boundaries top information calculated hereby proposed merge shots scenes knowledge never implemented tested proposal informedia tm project combines audio image analysis order determine scenes 9 determine socalled audio paragraphs based analysis audioaligned transcript match paragraphs nearest shot boundaries determine video paragraphs video paragraphs defined collection shots adhering context approach also employs audio video information although different manner surpasses informedia tm approach also uses setting dialog information scene determination 3 system overview system proceeds several steps follows first step shots recovered video atomic units clustering system values semantic feature calculated currently determine audio features color features orientation features faces appearing shots important types scenes want determine system extended time additional features next determine distances shots respect feature integrate different features one distance measure three reasons firstly possible determine fair integrated distance measure different features features may important others stage possible say secondly integrated distance measure would destroy semantic message individual feature easy separately judge correctness clusters based single feature nearly impossible combined thirdly semantics different features may conflict instance continuous audio track often used move smoothly one setting another thus shot clustering results respective features contradictory finally based calculated shot distance tables able merge shots feature separately means single algorithm 4 determination shots shots defined one uninterrupted image single static mobile framing 7 many algorithms proposed shot boundary detection see 29 references therein however try determine type extent edits precisely algorithms able detect classify determine extent hard cuts fades dissolves proposed 14 use since necessary eliminate editing effects concatenate shots edits may bias similarity measure contiguous shots unfavor window advanced 34 total window size calculate next audio feature vector forecasting vector calculated exponential smoothing forecasting ie called smoothing constant set 02 audio feature vector first window forecast weighted sum last observation previous forecast forecasting vector therefore contains values similar previous vector values represents distribution intensities frequencies previous windows therefore constitutes template regarded audio shot speed forecasting vector adapts new feature values controlled smoothing constant decision audio cut based difference new feature vector forecasting vec tor difference calculated via euclidean distance two difference thresholds high threshold directly determines cut significant difference lower threshold determines similarity ie difference small many consecutive feature vectors classified similar also deduce cut cut calculation forecasting vector starts audio cuts determine audio shots whose spectral content represented last forecasting vector vector also calculated background segments normalized versions vectors used calculation distance table thus background audio cut algorithms result table audio shots described vector specifies frame sequence covered audio shot vector real values signifies audio content audio shot normalization performed calibration order get values independent loudness influences order produce distance values comparing two audio shots 512 calculation distance table table distances video shots calculated audio shots comparing spectral content vectors shot based normalized euclidean distance metric closest vectors define dis tance interesting note audio shot overlaps two consecutive shots distance 0 calculated spectral content vector 52 dialog determination special scene type dialog form dialogs presented differ dialogs show dialog partners turn one time frontal view pattern called shotreverse shot pattern dia logs dialog partners visible scene typically side dialog setups possible concentrate widely used shotreverse shot pattern detect shot grouping system must understand actors appear video therefore implemented face detection algorithm method recognizing face actor across shot boundaries 521 feature frontal face detector one reliable face detectors digital images developed rowley baluja kanade 23 system detects 90 upright frontal faces hardly ever identifying nonfaces faces recreated neuralnetworkbased frontal face classification system arbitrary images eg photos newspapers single video frames basis frontal face detector video sequences widen range detectable faces detector also searches slightly tiltedrotated faces degrees necessary faces actors motion pictures always moving contrast faces typical still images portraits photographs sports teams usually depicted upright however general face search increases factor three number patterns tested image speed processing candidate frontal face locations drastically reduced extremely fast prefiltering step locations whose pixel colors approximate human skin colors 11 show structure nose mouth eyes local neighborhood passed face detector prefilter reduces number candidate face locations 80 moreover every third frame video sequence investigated face detected described vector specifies frame face size pixels detected well x ycoordinates x pos pos center angle inclination far detected face isolated unrelated faces video next task classify frames k similar faces order find groups frames showing actors group related frames called facebased class first step faces within shots related according similarity position size neighboring frames assuming features change slightly frame frame especially true dialog scenes addition dispose accidental misclassifications face detector discarding facebased classes fewer three occurrences face allowing two dropouts facetracking process simple grouping algorithm works well within shots computationally cheap demand complex face recognition algorithms described 13 second step facebased classes similar faces within shot merged eigenface face recognition algorithm 21 order obtain largest possible facebased classes face recognition algorithm used identify merge facebased classes actor across shots throughout video resulting socalled facebased sets describe size actors appear video however eigenface face recognition algorithm cannot guarantee face groups actors merge together actors face varies much throughout video grouping algorithm typically splits main actors distinguished facebased sets 522 dialog detection easy detect typical shotreverseshot dialogs multiperson dialogs frontal face detector sequence contiguous shots minimum length three shots denoted dialog 1 least one facebased class present shot apart neighbor 1 second 2 eigenfacebased shotoverlapping relations facebased classes interlink shots crossings length dialog cut first last facebased set shotoverlapping relation example detected dialog shown figure 1 53 setting determination setting defined locale action takes place often detected repetitive appearance background constant illumination makes possible use color structure information determine setting 531 features ccv orientation ccv shots similar color content usually belong common setting share common background color content changes dramatically setting setting within single setting color content usually measured sort refined color histogram technique color coherence vector ccv20 ccv makes use spatial coherence discriminates much better basic color histogram instead counting number pixels certain color ccv additionally distinguishes coherent incoherent pixels within color class j depending size color region belong region ie connected 8neighbor component color larger threshold ccv pixel regarded coherent otherwise incoherent thus two values associated color number coherent pixels color j number incoherent pixels color j color coherence vector defined vector normalized number pixels two ccvs compared figure 1 dialog shotoverlapping relation j experimental results measure outperformd euclidean distance retrieving images similar given image large database 20 distance values range 0 orientation orientation structures images another feature suitable characterize scene extent instance pictures city scenes many buildings one expect many vertical edges frontal view many horizontal lines visible contrast type orientation much unlikely images humans natural scenes 8 thus may reasonable describe picture orientation con tains moreover orientation might especially suitable describe characteristics background settings prototype local orientation defined image structure gray color values change exactly one direction remain static orthogonal direction orientation however distinguish direction consequently varies 0 unlike direction ranges 0 12 various algorithms determine local orientation commonly operate grayscale images computation useful increase global contrast order prevent structures emerging inadequately dark bright images minimum maximum grayscale value occur least certain significance ie frequency exceeds threshold value determined grayscale histogram used scale pixels grayscale values full grayscale range determination orientation carried histogram normalized images derived orientation via inertia tensor 512 allows neighborhoods constant gray values distinguished neighborhoods isotropic structures local orientation following presentation based approach 5 detailed derivation found 12 designating grayscale value pixel position x ix gradients along x directions gaussian filter gxy second momentum window matrix position x computed let denote eigen values jxy orientation eigen vector associated determined measures angle orientation image structure around location x relation eigen values used certainty measure estimated orientation three cases distinguished orientation direction 2 grayscale values change similarly directions thus structure isotropic 3 local environment constant grayscale value pixels dominant local orientation considered addition question local orientation determined also important find suitable aggregated feature captures characteristics local orientation entire image standard approach would summarize local orientation orientation histogram although histograms robust slight changes camera view local object motion little discriminating power therefore suitable large video databases typical proposal overcome drawback divide image several rectangular regions foreground background region histogram calculated j 1 j j x g x x l 1 tan however approaches deal problems caused important orientations close boundaries regions result accidental assignments one another region took another approach captures local orientation image independently translations small middle scalings local orientation image captured orientation correlogram defined like color correlogram 10 table indexed orientation pair ij kth entry orientation pair ij specifies probability within distance k orientation orientation j found image thus orientation correlogram describes spatial correlation orientation pairs changes distance feature robust detailed raw orientation image avoiding poor discriminating power highly aggregated features histograms local orientation defining orientation correlograms orientation discretized n classes using different distances space requirements come experiments chose distance two orientation correlograms measured based probability components 532 calculation distance tables distance two shots respect color orientation content measured based disaggregated set representation shots using minimum distance common feature values see 16 details ie shots described set features values derived frames compared 6 scene determination question use distance matrix determine scenes two issues arise firstly although scenes defined common feature imply feature present shot scene many cases set feature values describes scene cannot presented shot one example setting threedimensional space usually introduced shots different perspectives thus one must look neighboring shots also several shots ahead secondly easy humans judge whether two shots similar respect feature however algorithm requires decision function considered two possible decision functions 1 absolute thresholding 2 adaptive thresholding two possibilities adaptive thresholding choose threshold based either distance matrix video specifying number scenes based distance values temporal neighborhood ever clear temporal neighborhood influence threshold higher distance values could either mean distances shots scene greater part movie adaptation threshold would correct could also mean story part movie developing rapidly shots nothing common example different short views different landscapes implying actors undertook long journey like setting determination would adapt threshold would reduce threshold thereafter group settings dissimilar tested clustering scheme absolute thresholding worked best two movies analyzed determined optimal absolut thresholds automatically refer experimental results details loo kahead chosen value 3 shots scene determination algorithm works follows shot cluster comprises shots two shots apart lookahead distance threshold overlapping shot clusters grouped scenes 1 2 orientation dist 7 experimental results 71 setup proposed individual shot clustering algorithms combination implemented c experiments performed video database consisting two fulllength feature films groundhog day forrest gump former digitized motion jpeg german tv resolution 360x270 compression rate 115 latter extracted mpeg1 video cdi audio tracks sampling rate 8000 hz mono coded 8 bit law sufficient audio content present frequency bands 4000 hz feature film calculated features performed shot clustering described 72 methodology task shot clustering scene determination formulated either task finding scenes eliminating shot boundaries formulations result equivalent experimental results section convenient use shot boundary elimination view order judge results clustering algorithms determined hand feature ground truth telling shots belong together stored 1 shot boundary reference track two associated shots belong scene 0 otherwise reference track constructed jointly authors intensive discussion critical points performance different features shot clustering measured three basic numbers definition use term scene boundary place holder dialog audio sequence setting boundaries hit rate hit rate h specifies percentage correctly detected scene boundaries relation actual number miss rate miss rate determines percentage missed scene boundaries relation actual number ie 10 h false hit rate fthe false rate f gives percentage falsely detected scene boundaries relation actual number hit false hit rates three shot clustering algorithms influenced respective parameters general change parameter increases hit rate false hit rate also increases thus difficult define optimal parameters always tradeoff hit rate false hit rate section 73 show performance changes parameters visualization performance different shot clustering algorithms gives intuitive overview quality quantitative data therefore constructed tool compares detected clusters manually produced reference clusters figure 2 shows results beginning groundhog day audio setting rectangle specifies shot specific length row shows shots clustered manually scene humans respect chosen semantic feature gap two shots signifies clustering algorithm two automatically clustered shots overflow manual cluster end arrow painted analogous evaluation performance image text databases tens thousands shots scenes needed evaluate performance video comparison algorithms reliably unfortunately building large video database determining ground truth presently exceeds possibilities multimedia lab thus restrict experiments much smaller database 73 results 731 quality audio sequence determination first performed tests distance table order determine optimum distance threshold clustering algorithm lead us threshold 0087 groundhog day see figure 3 value also set analysis forrest gump threshold compared resulting audio sequences manually determined ones resulting hit miss false hit rates shown table 1 first column table specifies number automatically detected shot boundaries second column gives number boundaries clustered manually reference database three following columns give performance described groundhog day hit rate 81 false hit rate 17 found 47 unconnected audio movie shots sequences hits hit rate missed hits miss rate false hits false hit rate groundhog day 713 586 474 81 112 19 99 17 forrest gump 918 809 530 66 279 34 table 1 performance audio setting determination figure 2 interface performance visualization tool1030507090 audio feature hit rate false rate miss rate figure 3 performance audio setting determination dependence absolute threshold sequences calculates 94 semantic units based audio sequences intermediate shots integrated one semantic unit implies reduction number shots 87 forrest gump hit rate 66 suboptimal parameter setting false hit rate 7 get 76 unconnected audio sequences implying reduction semantic units 83 qualitative assessment via performance visualization tool scene overview shows audio sequences based continuing music necessarily imply scene boundary often music movies used transitions one scene next order facilitate emotional change intensify existing mood 3 audio sequence operator deletes shot limit may fundamentally important semantic break however happen often music often accompanied soft edit dissolve fade causing audio sequence boundary fall within following shot difficult determine audio sequences based speech speech dynamic general structure often interrupted short breaks spectral composition changes quickly semantic examination audio stream determination music speech noise parts similar 18 overcome difficulty 732 quality video setting determination first performed tests groundhog day order obtain optimal choice distance threshold clustering algorithm distance table see figure 4 b process led us threshold 0025 orientation 010 color threshold hit rate settings 82 76 orientation color respectively false hit rates 15 10 found 28 unconnected like settings via orientation 52 via color forrest gump thresholds used leading hit rates 90 58 false hit rates 10 respectively 42 found 75 like settings via orientation 82 via color see table 2 table 3 qualitatively speaking seems settings either determined high precision algorithm general case completely screwed movie shots settings hits hit rate missed hits miss rate false hits false rate groundhog day 713 580 475 82 105 18 90 15 forrest gump 918 680 613 90 67 10 195 28 table 2 performance setting determination orientation movie shots settings hits hit rate missed hits miss rate false hits false rate groundhog day 713 580 438 76 142 24 56 10 forrest gump 918 680 391 58 289 42 69 10 table 3 performance setting determination color1030507090 hit rate false rate miss rate figure 4 performance setting determination ccv b orientation dependence absolute threshold1030507090 orientation hit rate false rate miss rate b 733 quality dialog determination unlike features dialog detection requires additional parameter since feature detector already made binary decision hit rate shotreverse shots dialogs 35 60 respectively false hit rates 5 118 734 quality scene determination also combined scenes dialog detection setting detections audio sequence construct even better setting scenes algorithm proceeds combining determined clusters clusters maximum size whenever two clusters overlap form one bigger cluster scenes split two shots combined fade since fades always separate scenes finally gaps clusters merged scenes scene merge procedure resulted 35 scenes groundhog day 83 scenes forrest gump percentages shot boundaries algorithms correctly found midscene ones quite impressive 96 92 see table 5 means deleted midscene boundaries human would also delete however also deleted 18 respectively 11 8 conclusion outlook presented four features allow shots clustered audio sequences settings dialogs scene type provides important information structure video measured performance shot clustering approach ground truth manually created authors hit rate ranged 35 90 false hit rates 5 118 two feature films groundhog day forrest gump knowledge first time performance shot clustering algorithm evaluated ground truth general performance depends mainly feature much less type clustering algorithm employed better feature feature set captures semantics certain kinds scenes correct constructed scenes thus future try improve features capture audio setting setting general dialogs moreover working using distance tables construct hierarchical video representation would lead intuitive video table contents vtoc finding acts scenes shots browsing abstracting video annotation applications could benefit automatically generated vtoc r shot classification method selecting effective keyframes video browsing directing television film dictionary film terms recognition images large databases using learning framework video query research directions film art introduction glance image indexing using color correlograms locating tracking human faces neural networks digital image processing face recognition convolutional neural network approach vision digital video library methods content analysis towards visual grep systematic analysis various methods compare video sequences automatic news video parsing enhanced video handling based audio analysis broadcast news navigation using story segmentation comparing images using color coherence vectors importance perceptive adaptation sound features audio content processing human face recognition visual scenes audio support scene change detection characterization video sequences panoramaexcerpts extracting packing panoramas video browsing video browsing using clustering scene transitions compressed sequences video content characterization compaction digital library applications featurebased algorithm detecting classifying scene breaks automatic parsing indexing news video clustering methods video browsing annotation tr ctr wen wen hsieh arbee lp chen constructing bowling information system video content analysis proceedings 1st acm international workshop multimedia databases november 0707 2003 new orleans la usa wen wen hsieh arbee l chen constructing bowling information system video content analysis multimedia tools applications v26 n2 p207220 june 2005 cees g snoek marcel worring multimodal video indexing review stateoftheart multimedia tools applications v25 n1 p535 january 2005