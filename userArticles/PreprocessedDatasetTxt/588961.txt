practical algorithm general large scale nonlinear optimization problems provide effective efficient implementation sequential quadratic programming sqp algorithm general large scale nonlinear programming problem algorithm quadratic programming subproblems solved interior point method prematurely halted trust region constraint numerous computational enhancements improve numerical performance presented include dynamic procedure adjusting merit function parameter procedures adjusting trust region radius numerical results comparisons presented b introduction series recent papers 3 6 8 authors developed new algorithmic approach solving large nonlinear constrained optimization problems proposed procedure essence sequential quadratic programming sqp method uses interior point algorithm solving quadratic subproblems achieves global convergence application special merit function trust region strategy past several years theory supporting approach analyzed strengthened theory presented companion paper 4 addition implementations algorithm extensively tested variety large problems including standard test problems problems engineering scientific origin ranging size several hundred several thousand variables several thousand constraints specific strategies developed handling parameters utilized algorithm dealing nontrivial pathologies e g linearly dependent active constraint gradients inconsistent linearized constraints quadratic subprob lem often occur large scale problems paper present results efforts based theoretical foundation numerical experience confident algorithm provides efficient means attacking large sparse nonlinear program equality andor inequality constraints rigorous comparisons algorithms large nonlinear problems notoriously difficult especially given extensive set options typically available codes problems nevertheless algorithm conservative default parameter settings successful problems caused difficulties algorithms consequently encouraged believe competitive current stage development methods solving large problems give outline basic procedure succeeding sections provide specific detail component parts implemented algorithm including strategies safeguards used also exhibit comment results numerical tests paper relies heavily results paper theory motivation basic ideas applied computational mathematics division national institute standards tech nology gaithersburg md 20899 department mathematical sciences carnegie mellon university pittsburgh pa 152133890 z mathematics department university north carolina chapel hill nc 27599 assume general nonlinear programming problem form subject gx 0 smooth functions nonlinear equality constraints included description order avoid distracting technicalities modifications necessary insertion inferred 6 nonlinear equality constraints included code problems tested sequential quadratic programming method backbone algorithm see 7 review techniques kth step iterate x k denoting current approximation solution nlp addition xiterate also maintain nonnegative iterate z k 2 r measures infeasibility x k stage nlp modeled quadratic program form min subject rgx k taken appropriate approximation hessian lagrangian nlp ie h xx represents hessian respect x function applied see section 45 discussion choice b k used numerical experiments form qp generates step provides search direction improving current iterate two significant points made concerning phase algorithm first apply interior point quadratic program solver qp specifically use method found 1 solutions calculated solving sequence low dimensional quadratic programs pertinent details solver properties relative use sqp method found section 2 second try solve qp complete accuracy iteration rather often terminate interior point method prematurely particular halt quadratic program solver steplength exceeds trust region radius modified iteration according well improvement merit function predicted thus algorithm said truncated newton method sense 18 see also 15 particular merit function useful working version discussed section 3 strategy updating trust region radius given section 42 output qp solver vector determines direction step xvariable turn yields step direction slack variable z explained section 3 combined step direction two variables descent direction working version merit function also constraint thus choose steplength direction decrease merit function andor infeasibility iterate choice steplength determines new iterate x k1 also new value z k1 strategy choosing steplength algorithmic details including modifications safeguards necessary make implementation robust given section 4 results numerical tests contained section 5 results demonstrate overall effectiveness procedure highlight beneficial effect trust region strategy procedures finally section 6 briefly consider weaknesses current version algorithm suggest possible avenues research improve efficiency discussion theoretical practical questions related large scale nonlinear programming see recent surveys 12 14 21 2 interior point qp solver interior point methods linear programming demonstrated successful especially large problems recent research lead extension quadratic programs particular method method optimizing lowdimensional subspaces performed well linear programs extended quadratic programming case see 1 2 references contained therein method good numerical results quadratic programs reported properties make particularly compatible sqp algorithm describing paper brief description essential features method importance purposes follow many details actual algorithm reported may found references quadratic program solve qp form subject nthetan 2 r nthetam b 2 r assumptions 21 necessary apply interior point algorithm problem bounded full column rank exist feasible points ie constraints consistent note q indefinite assumption fulldimensional interior required equality constraints present handled writing two inequalities important prerequisite solving 21 interior point method feasible initial point algorithm uses big method construct phase problem min subject e vector ones artificial variable clearly large enough point feasible 22 sufficiently large algorithm applied 22 reduce artificial variable nonpositive point current value feasible e terms dropped value artificial variable found 22 consistent algorithm stops discussed make use step obtained 22 even feasible qp note equality constraints present entire solution procedure takes place phase always present defining characteristic algorithm proceeds solving sequence lowdimensional subspace approximations 21 application follow reported results dimension subspace taken three following outline o3d optimizing 3dimensional subspaces version algorithm variable treated essentially components o3d algorithm see however step 6 dependence incorporated formulation given 21 o3d algorithm quadratic programming 1 given feasible point 0 2 generate 3 independent search directions matrix whose columns p 3 form solve restricted quadratic program subject call solution 4 set j1 appropriate value steplength ae 2 0 1 5 stopping criteria met exit 6 go 2 step component vector corresponding artificial variable become nonpositive eliminated problem search directions step 2 solutions theta ad 2 fi scalar depending current iterate particular values chosen one directions always descent direction respect objective function steplength ae set lesser 99 distance boundary distance minimum objective function form matrix 23 allows efficient exploitation sparsity note q positive semidefinite matrix 23 positive definite interior points otherwise may latter case modification similar 20 used application algorithm using procedure obviates need matrix b k positive definite turn allows us use hessian lagrangian finite difference approximation thereof standard stopping criterion algorithm least one following holds relative change two successive values objective function small b relative difference primal dual objective function values small c difference two successive iterates small use sqp algorithm added length solution vector exceeds specified value additional condition implemented allow trust region strategies particular criterion cause algorithm halt qp unbounded case terminal vector useful direction context purposes point discussed next section recent version o3d described 1 contains option perform special recentering step subspace optimizing step step 4 generally improved efficiency option used results reported see section 6 comment 3 updating iterates merit functions section review definitions properties merit functions provide formulas updating iterates reader referred companion paper proofs motivations concepts stated section 1 iteration algorithm yields pair x k approximation solution nlp z k corresponding approximate slack vector step directions updated values approximations based approximate solution quadratic program min ffi subject rgx k obtained described preceding section vector ffi k gives step direction x k determine step direction q k slack vector z k formula theta rgx k note ffi k feasible qp theta rgx k case z k slack vector qp corresponding ffi k thus slack variable linear approximation gx k1 given step direction update iterate means formulas z value steplength parameter ff observe z k 0 fact means z k1 nonnegative ff 2 0 1 algorithm nonnegativity slack vector iterates preserved fact sometimes turns useful maintain z k positive level see section 48 important emphasize ffi k determined qp quadratic approximation nlp dependent choice z k z k generated solely use merit function described solve slack variable problem comment notation also order point denote iterate step ffi k q k whereas conventional notation would use x k z k clear context meant optimization algorithms value steplength parameter generally chosen reduce value suitably chosen merit function typically merit function nlp scalarvalued function unconstrained minimum x solution nlp reduction function implies progress made towards solution used determine appropriate steplength given search direction 5 6 merit function equalityconstrained problems derived important properties visavis steps generated sqp algorithm using slackvariable formulation nlp merit function inequality constrained problem constructed form cx z z nonnegative scalar use merit function approximations defined choosing value steplength parameter ff noted approximate slack vectors generated algorithm z k always remain nonnegative thus nonnegativity constraint z imposes theoretical difficulty function cx z defined plays important role algorithm used measure feasibility pair x z define function kdeltak denotes standard euclidean norm set c 0 corresponds feasible set nlp hence close feasible c j small j sufficiently small merit function desirable property solution nlp corresponds constrained minimum addition small ffi k exact solution qp implies step descent direction sufficiently close feasibility despite useful properties two deficiencies limit use efficient algorithm first ffi k q k descent direction near feasibility second evaluation rf rg additional nontrivial computational algebra required assess prospective point order overcome difficulties approximate merit function developed working version values k fixed k easily evaluated line search algorithm choosing appropriate value ff approximate merit function k essentially properties respect step stronger property step descent direction k everywhere moreover sufficiently small outside ball around solution sufficient reduction k implies sufficient reduction mean sufficient reduction wolfe condition satisfied thus able use k surrogate testing progress iterates towards minimum important property step ffi k assumption exact solution qp descent direction function r defined 34 thus basic algorithm case qp solved exactly follows given initial value j use steps ffi k q k reduce r iterates c j iterates contained c j sufficient reduction k yield sufficient reduction reduce j course algorithm remains bounded away zero convergence follows fact wolfe condition satisfied j goes zero convergence follows observation radius ball wolfe condition satisfied also goes zero essentially algorithm global convergence proved paper theory paper primarily interested enhancements convert theoretical algorithm one practical efficient requires make provisions situations assumptions performed convergence analysis valid adopt numerical procedures reduce computational effort note modifications even theoretically justified believe firm foundation underlying algorithm evidence accumulated extensive numerical testing validate use implementation algorithm trust region constraint used possibly truncates quadratic programming algorithm exact solution achieved case theory described apply step obtained approximate solution although general convergence theory based step yet available shown theory paper approximate solution obtained o3d algorithm k large resulting step appropriate descent properties functions r k particular convergence achieved k goes zero suitable manner properties justify use truncation procedure speed algorithm important note approximation procedure also allows us handle difficulty arises sequential quadratic programming methods quadratic subproblem inconsistent 4 truncated sqp algorithm section give somewhat detailed description algorithm initially assume hessian approxima positive definite matrices k nonsingular linearized constraints qp consistent realworld applications assumptions always valid tried make algorithm flexible enough perform well situations assumptions fail hold describe adaptations end section implementation algorithm depends upon four important parameters need either computed modified throughout course algorithm globalization parameter j introduced 35 measure size domain feasible region direction ffi k q k descent direction true merit function current estimate j maintained algorithm trust region parameter upper bound weighted norm approximate solution qp positive definite diagonal matrix trust region radius updated every iteration parameter ff steplength parameter determines length step variables x z direction ffi k q k chosen guarantee progress towards solution decreasing either merit function infeasibility finally merit function parameter must small enough guarantee theoretical properties described preceding section valid although theory allows arbitrarily small values algorithm becomes slow small thus monitored throughout algorithm either increased decreased appropriate outline algorithm followed specific comments procedures justifications version contains practical modifications described simplify notation define recall r given 34 basic truncated sqp algorithm 1 initialization given x initialize slack variable z 0 0 b set k 0 2 calculation basic trust region step kffik iterate using o3d min subject rgx k obtain ffi k k b set theta rgx k theta rgx k otherwise c decrease necessary 3 computation steplength parameter choose ff 2 0 1 k sufficiently reduced b reduce ff necessary r sufficiently reduced c reduce ff necessary 4 update estimate globalization parameter set 5 update variables check termination set z b convergence criteria met quit c update b k b k1 6 adjustment merit function trust region parameters update necessary b adjust trust region radius 7 return b go step 2 41 globalization parameter globalization step based work 6 4 step 3 require approximate merit function reduced addition current iterate lies outside set c j require constraint infeasibilities also reduced possible result descent properties described section 3 good estimate j true merit function also reduced case estimate j large reduce value step 4 procedure eventually lead sufficiently small value j note arrangement allows steps may increase merit function controlled way also allows steps may increase constraint infeasibilities inside c j 42 updating procedure updating trust region radius step 6b similar standard strategy used trust region algorithms see 17 31 base decision change comparison predicted relative reduction pred k actual relative reduction ared k function used measure progress toward solution various formulas predicted relative reduction pred k suggested different merit functions especially equality constrained programming problems see example 19 distinctive procedure use different functions computing pred k ared k depending current status algorithm linearized constraints satisfied use approximate merit function compute predicted actual reductions trust region constraint causes o3d terminate phase ie linearized constraints satisfied predicted actual reductions infeasibility used case feasible solution qp obtained k used compute predicted actual reductions method defining pred k differs standard methods used unconstrained optimization step finding subproblem based solely merit function moreover trust region constraint appear explicitly subproblem nevertheless updating want assess well approximation k agrees k direction uses quadratic approximation lagrangian objective function linearized constraints form approximation based quadratic approximation function k 1 given linear approximation note k z based considerations results 16 define predicted relative reduction pred ae oe derivatives respect x z steplength parameter ff k size recently accepted step value actual relative reduction ared k taken difference values k points divided value k valid criticism formula pred k dependence higher order derivatives therefore use available approximation hessian lagrangian r 2 k 1 example cellcentered finite difference approximations hessian lagrangian function used numerical results presented unless analytic second derivative formulas readily available choice pred k used step returned o3d feasible situations resulting step dominated feasibility improving component makes little sense adjustment determined k rather comparison predicted actual improvement constraint infeasibility seems appropriate therefore case function rx z used comparison purposes values pred k ared k given follows case o3d algorithm terminates phase pred ared heuristics choosing pred k ared k appear work well specifically allow trust region radius increased even event step returned o3d satisfy linearized constraints results increase true merit function experience alternative formulas based solely constraint violations never employed close solution indeed iterates preceding convergence always observed well inside c j satisfying linearized constraints decreasing merit functions usually pose problem 43 steplength ff steplength ff determined step 3 al gorithm sufficient decrease referred 3a 3b requires wolfe condition satisfied given function oe potential step w point v condition requires ff satisfy fixed oe 2 0 1 numerical experiments reported section 5 employed simple backtracking procedure factor onehalf find ff satisfy condition k r also experimented sophisticated line search methods motivated unconstrained optimization techniques 18 observations date suggest complicated line searches result little improvement algorithm except iterates quite far solution 44 adjusting choosing effective value merit function parameter essential algorithm clear compact set sufficiently small value assure results given 4 valid three important practical reasons parameter must adjusted rather fixed first angle direction generated o3d gradient approximate merit function becomes nearly orthogonal steps might become small adjust avoid possibility second approximate merit changing iteration possible previous iterate might acceptable current k ie cycling might occur worry also alleviated adjusting third reason changing allow larger steps seen theory verified numerical experience small form merit function forces path iterates follow nearly active constraints closely causes algorithm take small steps particular slow moving away nonoptimal active set making possible increase significantly improve algorithms performance implementation algorithm two opportunities adjust step 2 solving quadratic subproblem step 6 step taken first adjustments decreased second parameter may increased decreased step 2 angle gradient approximate merit function step direction ffi k q k computed two vectors become nearly orthogonal conclude small enough ensure good decrease k decrease parameter specific compute wd gamma1 calculate value w gamma5 safeguard procedure allowing certain percentage decrease current version use 50 decreased step 2 consider modifying step taken step 6 primary concern avoid cycling compute interval penalty parameter follows fixed integer seek value parameter inequality 42 implies none past iterates acceptable approximate merit function new value thus cycling would possible accomplish use decomposition 1 k defined section 42 compute values k k consider inequalities define u l upper lower values ensure inequality 44 satisfied letting obtain interval l u assuming interval exists case value next step chosen interval next iterate return one previous iterates practice value 5 usually sufficient prevent cycling interval doesnt exist make change given choose avoid cycling second objective juncture increase allow bigger steps u larger current safely increase without worrying possible cycling however safeguard increase two ways first require predicted reduction based approximate merit function must greater predicted reduction infeasibility linearized constraints restriction prevents increased prematurely due primarily large decrease constraint infeasibilities specifically writing predicted reduction k see 41 insist new value second use maximum allowable change currently factor 2 limit growth computationally simple procedures updating appear effective especially presence highly nonlinear constraints poorly scaled problems 45 hessian approximation numerical experimentation reported used finite difference approximation hessian lagrangian although hessian lagrangian strong solution positive definite appropriate subspace may indefinite general even positive definite finite difference approximation may experimented two approaches handling possibility first simply modified approximate hessian matrix adding nonnegative elements diagonal ensuring cholesky factorization matrix positive elements along diagonal see 20 modification easy implement observed slow convergence problems modification guarantees positive definite matrix delivered qp solver takes place iterates get close solution generally precludes local qsuperlinear convergence alternative modifying approximate hessian lagrangian simply allow o3d iterate indefinite qp subproblem halting iterations solution exceeds trust region radius implemented approach seemed yield superior results obtained making approximate hessian positive definite especially iterates close solution even though theoretically prove obtain descent direction approximate hessian positive definite 46 convergence criteria convergence criteria used standard similar 3 first insist constraints satisfied close tolerance specifically require also require either criterion 49 stronger indication kkt point reached weaker criterion 410 suggests progress slowed drastically iterates may may drawn close solution reason criterion 49 usually preferable criterion 410 lagrange multipliers returned quadratic program used 49 unless trust region constraint determines approximate solution qp case use least squares approximation multipliers replacing negative multipliers machine zeros problems solved date trust region never comes play iterates get close solution therefore qp multipliers used convergence test solution 47 inconsistent quadratic subproblems one difficulty occur making linear approximations nonlinear constraints qp may inconsistent case o3d even runs completion exit phase return positive value artificial variable note always occurs equality constraints present small resulting direction descent direction k r result step taken direction generally decrease infeasibility making less likely inconsistent set linearized constraints encountered subsequent iterations recent versions algorithm include constraint relaxation procedure appears yield acceptable step even event inconsistent linearizations constraints encountered situation surface numerical experiments presented paper include description perturbation procedure note however encountered important application problems procedure crucial performance algorithm see example 24 48 updating slack variables one difficulty algorithm updating slacks event sqp step satisfy linearized constraints well enough ie k small enough occur qp inconsistent trust region bound encountered solution qp case slack variable updating scheme would ensure nonnegative slacks remain non negative direction may one descent resolve dilemma opting descent ie computing q k replacing negative slacks using following rule z k1 z k1 ae ffl mach g gammag ffl mach machine epsilon sometimes referred closing constraints see example 33 49 linearly dependent constraint gradients linearly dependent constraint gradients cause many theoretical computational difficulties constrained optimization theoretical algorithm obtain convergence even linearly dependent constraint gradients provided approximate multipliers become unbounded practice even though o3d difficulty dealing problem evaluating merit function computing least squares approximation lagrange multipliers become problematical computational experience shows solve many problems degeneracy constraints simply maintaining slacks positive described allows us factor crucial matrices continue algorithm however algorithm failed solve problems large amount degeneracy linearized constraint matrix course problem dependent observed current implementation usually solve problems 25 percent constraint gradients linearly dependent degeneracy causes performance merit functions deteriorate particular least squares approximation lagrange multipliers seems especially poor resulting small steps allowed even close solution 5 numerical results modified algorithm coded fortran installed sparcstation 10 using ieee floating point arithmetic 64 bit current implementation used solve wide variety medium large scale problems section report results set performance tests designed specifically answer questions trust region strategy procedure update penalty parameter conclude section results algorithm applied test problems publicly available emphasize problems solved default settings parameters see table 1 ie attempt made pick parameter settings optimize performance individual problems although many applications analytic derivatives available use analytic derivative information used numerical experiments possible first second derivatives computed using forward central finite differences respectively costly onetime calculation provided zerononzero stencil hessian lagrangian jacobian matrix constraint function stencils used duration solution process problems finite difference approximations convenient use case control problems governed partial differential equations see 29 30 partial differential equation solved using finite element method piecewise linear elements evaluating derivative objective parameter value z mach table numerical values default parameters function respect control variables quite cumbersome cases occurred control problems test suite one approximate first derivatives objective function solving adjoint problem computational cost comparable one function evaluation examples see 22 objective function portion hessian lagrangian approximated forward finite differences set eight problems chosen first test suite problems ranged size 5001000 variables 10002000 constraints first four relatively straightforward nonlinear programming test examples last four actual applications two discretized control problems density estimation problem statistics molecular distance problem complete description problems found appendix problems nonlinear inequality constraints exploitable sparsity problem 4 nlp4 designed controllable percentage linear dependency constraint gradients demonstrate weaknesses algorithm associated difficulty ran three versions algorithm problem using positive definite modification hessian matrix discussed section 4 without trust region strategy using unmodified hessian trust region using unmodified hessian results failure cases trust region strategy employed addition problem run two starting points one labeled c close solution sense variables order magnitude solution distant start labeled f results numerical tests problems summarized tables 13 first two columns table gives number sqp iterations nli total number o3d iterations qpi next two columns contain stopping criterion met value gradient lagrangian solution unless algorithm failed denoted failure tables feasibility condition 48 satisfied solutions stopping criterion denoted either 1 2 depending whether 49 410 satisfied conditions satisfied 3 appears column remaining columns give information values parameter run columns five eight giving initial maximum minimum final values parameter final column giving last iteration changed results tests illustrate using unmodified hessian trust region effective reducing number o3d iterations number sqp iterations trust region strategy prevented long unprofitable steps generated far solution use unmodified hessian allowed trust region become inactive near solution thus allowing rapid local convergence requiring hessian positive definite often precluded rapid local qsuperlinear convergence used conjunction trust region strategy resulted trust regions active close solution results also show value parameter varied several orders magnitude procedures discussed section 4 allowed value increase decrease greatly enhanced algorithm earlier tests using either fixed value allowing reduction yielded inferior results another modification algorithm reflected table included description preceding section made force o3d algorithm take minimum number steps found trust region radius became small algorithm would sometimes exit o3d one iteration resulting poor step direction poor step would result decrease eventually algorithm would fail required minimum number steps taken o3d choice 7 problem disappeared recently collection test problems become available testing comparing optimization algorithms see 13 constrained unconstrained testing enviroment cute quickly becoming standards researchers establish viability effectiveness numerical algorithms problems replacing smaller well scaled test problems hock schittkowski 25 schittkowski 32 intended used test large scale al gorithms results cute test problems summarized tables 6 7 8 problems solved stopping conditions problems likewise table format used present numerical results detailed description problems structure motivation sources see 9 appears cute test problem set rich large small scale unconstrained equality constrained test problems present many large scale problems include inequality constraints particularly nonlinear inequality constraints chose problems reflected class problems algorithm designed solve least one inequality constraint present problem number variables andor constraints large enough exploitation special sparsity structure important problems selected cute report corkscrew manne svanberg zigzag associated problem sizes recorded table 5 worth commenting much machinery developed paper deals effectively handling nonlinear inequality constraints performance algorithm cute test problem set therefore slightly deceiving since many constraints problems simple bounds primal variables purely linear instance approximately 83 constraints corkscrew 50 constraints manne 66 constraints zigzag linear many equality constraints although caused problem algorithm structure constraints completely exploited extra machinery code resulted overhead performance benefit clearly algorithm designed specifically deal linear equality constraints outperform algorithm problems problem algorithm appeared perform best svanberg problem inequality constraints substantial number nonlinear succeeded solving four problems reasonable number inner outer iterations however many algorithmic enhancements contributed little solution process measure distance feasibility jtube strategy nonmonotone updating penalty parameter trust region strategy essentially dormant solution process regardless iterates proximity solution feasibility fact evidence enhancements small number cute test problems solved occurred decreased slightly solving problem manne employing modified hessians trust region strategy see third fourth rows table 7 noteworthy iterates resulted solving problem penalty parameter artificially held fixed identical iterates resulted adjusted solution appears illustrate case adjustment purely superficial truss c 103 3561 1 44e8 987e1 101e00 957e2 957e1 100 molecc 37 1376 1 98e9 988e1 121e1 117e2 981e1 34 molecf table modified hessians trust region bcheat c 257 2898 1 81e8 918e1 415e00 138e1 410e00 249 bcheat f 289 3071 1 99e8 100e00 374e00 244e1 389e00 281 molecc molecf 44 621 1 66e8 100e00 148e00 739e2 952e2 38 table modified hessians trust region trussc 94 2242 1 39e8 987e1 103e00 126e1 911e1 87 molecc molecf table unmodified hessians trust region problem variables constraints corkscrew 96 159 zigzag 304 1206 table minimization parameters problem nli qpi conv krx lk1 d0 maxd mind final last dcha table modified hessians trust region problem nli qpi conv krx lk1 d0 maxd mind final last dcha table modified hessians trust region problem nli qpi conv krx lk1 d0 maxd mind final last dcha table unmodified hessians trust region 6 future directions paper discussed detail sqp algorithm solving large scale nonlinear problems numerical results default parameter settings indicate procedures implemented robust effective efficient convergence theory 4 provides sound theoretical basis procedure nevertheless several areas techniques used improved allow solution larger difficult problems algorithmically observe current implementation requires factorization rg rgz rgrg latter o3d sparse matrix package makes reasonable problems currently considered clearly expensive maintain results reported use analytic finite difference hessian approximations examination details o3d reveals limited memory bfgs limited memory sr1 could readily incorporated code done experimentation techniques results reported elsewhere 26 many problems seen degenerate significantly slows convergence method primary culprit extremely poor multiplier estimates provided least squares procedure improvements area certainly required problems reported nonlinear equality constraints occasionally observed significant difficulty trying satisfy linearized equality constraints ie completing phase cases success relaxing constraints 26 context o3d accomplished simply fixing artificial variable positive value continuing o3d iterations approach often find o3d converges recentering procedure mentioned section 2 led improvements theory 4 supports ideas details reported elsewhere r interiorpoint method general large scale quadratic programming problems interior point method linear quadratic programming problems merit function inequality constrained nonlinear programming problems family descent functions constrained optimization truncated sqp algorithm large scale nonlinear programming problems cute constrained unconstrained testing environment functional numerical solution control problem originating heat transfer exact approximate boundary controlla bilities heat equation large scale numerical optimization introduction overview lancelot fortran package largescale nonlinear optimization siam journal numerical analysis globally convergent inexact newton methods robust trust region algorithm nonmonotonic penalty parameter scheme constrained optimization new york constrained nonlinear pro gramming numerical methods nonlinear variational problems molecular conformations distance matrices optimal signal sets nongaussian detectors test examples nonlinear programming codes use optimization techniques solution partial differential equations science engineering solution metric stress sstress problems multidimensional scaling using newtons method numerical solution nonlinear parabolic control problem reduced sqp method optimal control systems governed partial differential equations computing trust region step test examples nonlinear programming codes role slack variables quasinewton methods constrained optimiza tion nonparametric probability density estimation nonparametric function estimation used illustrate separability nonlinear programming distance matrices tr ctr thomas f coleman jianguo liu wei yuan new trustregion algorithm equality constrained optimization computational optimization applications v21 n2 p177199 february 2002 e bradley ogallagher j rogers global solutions nonlinear systems using qualitative reasoning annals mathematics artificial intelligence v23 n34 p211228 1998 rubin gong gang xu quadratic surface reconstruction multiple views using sqp integrated image graphics technologies kluwer academic publishers norwell 2004