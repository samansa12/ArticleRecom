empirical study decentralized ilp execution models recent fascination dynamic scheduling means exploiting instructionlevel parallelism introduced significant interest scalability aspects dynamic scheduling hardware order overcome scalability problems centralized hardware schedulers many decentralized execution models proposed investigated recently crux models split instruction window across multiple processing elements pes independent scheduling instructions decentralized execution models proposed far grouped 3 categories based criterion used assigning instruction particular pe execution unit dependence based decentralization edd ii control dependence based decentralization cdd iii data dependence based decentralization ddd paper investigates performance aspects three decentralization approaches using suite important benchmarks realistic system parameters examine performance differences resulting type partitioning well specific implementation issues type pe interconnectwe found ringtype pe interconnect ddd approach performs best number pes moderate cdd approach performs best number pes large currently used approachedddoes perform well configuration realistic crossbar performance increase number pes partitioning approaches results give insight best way use transistor budget available implementing instruction window b introduction extract significant amounts parallelism sequential programs instructionlevel parallel ilp processors often perform dynamic scheduling hardware typically collects decoded instructions instruction window executes instructions source operands become available going todays modest issue rates 12 16way issue centralized dynamic schedulers face complexity phases outoforder execution 2 10 hardware needed forward new results subsequent instructions identify readytoexecute instructions instruction window limits size hardware window important therefore decentralize dynamic scheduling hardware importance decentralization underscored recently developed processorsexecution models mips r10000 20 r12000 dec alpha 21264 7 multiscalar model 4 14 superthreading model 17 trace processing model 13 15 19 misc multiple instruction stream computer 18 pews parallel execution model 6 11 multicluster model 3 execution models split dynamic instruction window across multiple processing elements pes dynamic scheduling parallel execution structions dynamic scheduling achieved letting execute instructions operands become available important issue pertaining decentralization criterion used partitioning instruction stream among pes three types decentralization approaches proposed based criterion use parti tioning execution unit dependence based decentralization edd ii control dependence based decentralization cdd iii data dependence based decentralization ddd first category groups instructions use execution unitsuch adder multiplierinto pe examples r10000 r12000 alpha 21264 second category groups controldependent instructions pe multiscalar superthread ing trace processing models come category last category groups datadependent instructions pe examples misc pews multicluster models three categories different hardware requirements tradeoffs paper reports results set experiments conducted provide specific quantitative evaluations different tradeoffs address following specific questions kind programs benefit kind partitioning ffl well performance scale decentralization ffl much benefit would crossbar used interconnect pes question select best decentralization approach use granularity parallelism important one discuss might accomplished immediate concern question whether even worth attempting use decentralization techniques pes yet know exact shape execution models take future show right choices made decentralization approaches provide reasonable improvements instruction completion rate without much impact cycle time rest paper organized follows section 2 provides background motivation behind decentralization dynamic scheduling hardware also describes three decentralization approaches investigation section 3 describes experimentation methodology section presents detailed simulation results different decentralization approaches particular examines impact increasing number pes effects two different pe interconnection topologies section 5 presents discussion results conclusions paper decentralized ilp execution models programs written current instruction set architectures generally controldriven form ie control assumed step instructions sequential der dynamically scheduled ilp processors convert total ordering implied program partial ordering determined dependences resources control data involves identifying instructions mutually resourceindependent controlindependent data independent order scale degree multiple sue resources high demand decentralized reordering parallel execution instruc tions constraints due resource dependences overcome replicating resources fetch unit decode unit physical registers execution units eus ie functional units ii providing multiple banks resources dcache shown figure 1a 21 decentralizing dynamic scheduler inspecting block diagram figure 1a see important structures remain decentralized dynamic scheduler ds register rename dynamic ds scheduler control misprediction information isavisible registers decode units register rename unit control flow icache physical registers memory address resolution dcache banks eu eu eu eu eu eu eu eu dynamic ds scheduler dynamic ds scheduler dynamic ds scheduler dynamic ds scheduler control misprediction information flow control instruction isavisible registers icn distribution unit address dcache banks memory resolution b figure 1 generic organization dynamically scheduled ilp processors centralized scheduler b decentralized scheduler unit memory address resolution unit 1 incidentally difficult parts decentralize deal interinstruction dependences preclude decentralization mere replication parts ds hardest decentralize often needs handle active instructions simultaneously present processor detailed studies 08m 035m 018m cmos technology 10 also confirm centralized ds scale well thus important decentralize ds many researchers proposed decentralizing ds use multiple pes set eus shown figure 1b decentralized processor dynamic instruction stream partitioned across pes operate parallel 1 complexity structures partly reduced offloading part work special hardware critical path program execution 8 9 19 instructions assigned pes control dependences data dependences natural question arises point basis instructions distributed among decentralized pes criterion used partitioning instruction stream important improper partitioning could fact increase interpe communication degrade performance true decentralization aim reduce demand pe also aim minimize demand pe interconnect localizing major share interinstruction communication occurring processor within decentralized pes three current approaches grouping instructions pes revolve around three important constraints execute instructions parallel execution unit dependences ii control dependences iii data dependences shall look three decentralization approaches ensuing discussion use example control flow graph cfg code shown figure 2 cfg consists three basic blocks b c block b controldependent conditional branch block c controldependent conditional branch b shall assume control flow predictor selected blocks b c trace i12 br r13 0 i9 br r4 0 i2 i4 br r4 0 figure 2 example control flow graph code 22 execution unit dependence based decentralization type decentralization instructions assigned pes based eu execute thus instructions resource dependent particular eu execute pe artifact arrangement instructions wait near eu dependence resolved interestingly one pioneer dynamic scheduling schemes implemented ibm 36091 16 incorporated type decentralization 1967 recently mips r10000 r12000 processors also use approach 20 potential advantage edd approach pe need one types execution units another advantage instruction partitioning straightforward static nature single pe eu particular type situation dynamic instances given static instruction always get assigned pe multiple pes eu particular type choice involved allocating instructions require eu type one option situation static allocation compiler offline hardware another option dynamic allocation alpha 21264 7 perhaps based queue lengths concerned pes either option ready instruction may sometimes wait allotted eu become free although another eu type another pe free furthermore processor performs speculative execution recovery actions arising incorrect speculations necessitate selective discarding instructions different pes main shortcoming edd approach however generally result pe may needed pe necessitating global interconnect pes scale well 10 23 control dependence based decentralization cdd second decentralization approach contiguous portion dynamic instruction stream assigned pe thus instructions controldependent conditional branch generally assigned pe branch assigned instructions wait near control dependences resolved examples approach multiscalar execution model 4 14 superthreading model 17 trace processing model 13 15 19 2 controldependencebased decentralization fits well controldriven program specification typically adopted current isas controldependent instructions tend grouped together program executable partitioning instructions among pes easily done statically partitioning cfg furthermore regrouping instructions needed instruction commit time cdd hardware implementations proposed far multiscalar processor superthreading processor trac processors organize pes circular queue shown figure 3 circular queue imposes sequential order among pes head pointer indicating oldest active pe programs execute processors follows cycle tail pe idle control flow predictor cfp predicts next task dynamic instruction stream invokes tail pe task path subgraph contained cfg executed program instance cdd processor uses tracebased tasks blocks b c example code forms trace assigned single pe invocation tail pointer advanced invocation process continues new tail next cycle successor task example code one starting predicted target conditional branch block c thus cfp steps cfg distributing tasks speculatively multiprocessors also partition instructions based control de pendences however partitioning granularity generally much coarser several hundreds instructions per task fur thermore multiple tasks multiprocessor share register space pes head pe completes task instructions committed head pointer advanced causing pe become idle task misprediction detected pes incorrect speculation point tail pe discarded known squash figure 3 block diagram 8pe cdd processor whereas trace processors consider trace single path consisting multiple basic blocks task multiscalar processors consider subgraph control flow graph task thereby embedding alternate flows control task processors also differ terms instructions task fetched whereas trace processors fetch instructions task single cycle supply pe multiscalar processors let active pes parallelly fetch instructions one one architectural support provided facilitate hardware determining data dependences studies 5 19 shown cdd processors register operands produced pe nearby pe unidirectional ringtype pe interconnect quite sufficient pe typically keeps working copy register file also helps maintain precise state task boundaries 24 data dependence based decentralization ddd third approach decentralization data dependences used basis partitioning instructions data dependent instruction typically dispatched pe producer instruction dispatched mutually dataindependent instructions likely dispatched different pes thus instructions wait near data dependences resolved misc multiple instruction stream computer 18 pews execution model 6 11 dependencybased model given 10 multicluster model 3 come category data dependences dictate communication occurring instructions ddd approach attempts minimize communication across multiple pes instructions pe mostly datadependent becomes less important runtime scheduling within pe 10 however partitioning instructions ddd processor generally harder cdd pro cessor programs generally written controldriven form causes individual strands datadependent instructions often spread large segment code thus hardware first construct data flow graph dfg instruction parti tioning shown figure 4 notice programs specified datadriven form datadependencebased partitioning would easier reduce hardware complexity dfg corresponding path trace generated offline hardware stored special icache later reuse i4 br r4 0 i9 br r4 0 i12 br r13 0 figure 4 register data flow graph rdfg trace abc figure 2 ddd hardware implementations proposed far pews 6 11 dependencebased model 10 multicluster 3 differ terms pes interconnected pews uses unidirectional ringtype con nection whereas misc dependencebased model 10 use crossbar crossbar employed pes proximity hence instruction partitioning algorithm becomes straightforward however discussed earlier crossbar scale well multicluster execution model isavisible registers partitioned across pes instruction assigned pe based source destination isavisible regis ters thus partitioning static nature pews execution model partitioning done dynamically order reduce burden partitioning hardware complexity instruction pipeline dfg corresponding path built offline hardware stored special icache 11 alternately architectural support provided permit compiler convey dfg relevant information hardware 25 comparison seen three approaches partitioning instructions amongst decentralized processing elements table 1 succinctly compares different attributes hardware features three decentralization approaches implementation point view cdd edd potentially edge static nature partition ing cdd implementations advantage due attribute edd cdd ddd basis partitioning resource usage control dependence data dependence execution unit types pe eu types eu types eu types logical ordering among pes partitioning granularity instruction task instruction time partitioning done staticdynamic staticdynamic staticdynamic complexity dynamic partitioning hardware moderate moderate high table 1 comparison different decentralization approaches partitioning higher level instead 16 way instruction fetch mechanism fetches decodes instructions every cycle icache trace cache instruction fetch mechanism including icache distributed across pes done multiscalar processor 4 14 3 experimental methodology previous section presented detailed description comparison three decentralization approaches next present detailed simulationbased performance evaluation three decentralization approaches 31 experimental setup setup consists 3 executiondriven simulatorsbased mipsii isathat simulate 3 decentralization approaches detail simulators cyclebycycle sim ulation including execution along mispredicted paths simulators equivalent every respect except instruction partitioning strategy particular following aspects common simulators instruction fetch mechanism execution models use common control flow predictor speculate outcome multiple branches every cycle highlevel predictor extension treelevel predictor given 1 considers treelike subgraph dynamic control flow graph basis prediction tree depth 4 8 paths used predictor predicts one 8 paths using 2level pag predictor treepath trace allowed maximum 16 instructions first level table subgraph history table predictor 1024 entries direct mapped uses pattern size 6 second level table pattern history table entries consist 3bit updown saturating counters 128 kbyte trace cache 12 used store recently seen traces trace cache 8way setassociative 1 cycle access time block size 16 instructions traces starting address map set trace cache every cycle fetch mechanism fetch dispatch 16 instructions data memory system execution models use memory system l1 data cache perfect l2 cache reduce memory requirements simu lators l1 data cache 64 kbytes 4way setassociative 32way interleaved nonblocking 16 byte blocks 1 cycle access latency memory address disambiguation performed decentralized manner using structure called arcade 11 provision execute memory references prior address disambiguation instruction retirement investigated execution models retire ie commit instructions program order one trace time support precise exceptions pe interconnection topology three types pe interconnects modeled simulatorsa unidirectional ring bidirectional ring crossbar rings take 1 cycle adjacent pepe transfer crossbar takes log 2 p cycles pepe transfers p number pes parameters study ffl maximum fetch size f instructions ffl pe issue width maximum number instructions executed pe per cycle fixed 3 cause higher values gave marginal improvements thus pe 3 eus ffl pe issue strategy default strategy use oforder execution within pe experiments involve varying 3 parameters partitioning strategy number pes p pe interconnect 32 benchmarks performance metrics table 2 gives list spec95 integer programs use along input files use compress95 average path benchmark input file trace length prediction gcc stmti 1306 8178 go 9stone21in 1429 7017 li testlsp 1228 9104 vortex vortexraw 1359 9498 table 2 benchmark statistics program based unix compression utility performs compressiondecompression sequence large buffer data gcc program version gnu c compiler many short loops poor instruction locality go program based internationally ranked go program many faces go li benchmark percentage instrs using eu type eueu communication program integer loadstore branch intint intloadstore intbranch loadstoreint gcc 432 361 207 262 326 139 112 go 524 322 154 349 289 90 145 li 266 487 247 143 378 63 76 vortex 287 524 189 135 476 75 87 table 3 distribution instructions based execution unit used program lisp interpreter written c m88ksim program simulator motorola 88100 processor vortex program singleuser objectoriented database program exercises system kernel coded integer c programs compiled mips r3000ultrix platform mips c version 30 compiler using optimization flags specified spec benchmark suite benchmarks simulated completion 500 million instructions depending whichever occurred first table also gives execution statistics number instructions simulated average treepath trace length path prediction accuracy statis tics see gcc go poor control flow predictability primarily arising poor instruction local ity causes many conflicts first level table predictor measuring performance execution time sole metric accurately measure performance integrated softwarehardware computer system accordingly simulation experiments measure execution time terms number cycles required execute fixed number instructions reporting results execution time expressed terms instructions per cycle ipc notice ipc figures include committed instructions include nops also measure register traffic get insight behavior different decentralization approaches 33 partitioning algorithms simulated edd edd system pe execution units eus particular type decide many pes eus particular type measured percentage instructions use eu type table 3 gives percentages based percentage instructions using particular eu used following eu assignments system single pe 3 eus pe execute type instruction system 2 pes first pe houses 3 integerfp eus second pe houses 3 loadstorebranch eus system 4 pes division pes table 4 pes eus number integer loadstore branch fp pes pes pes pes pes table 4 division pes edd scheme kind placed adjacent set pes loadstore eus placed immediately set pes integer eus significant amount traffic integer eus loadstore eus cf table 3 instruction partitioning strategy dynamic component instruction assigned multiple pes assigned candidate pe least number instructions cdd studying cdd partitioning approach connect pes circular queuelike manner two different task sizes namely 8 16 used first case called cdd8 trace 8 instructions fetched cycle assigned pe tail pe circular queue second case called cdd16 trace instructions fetched cycle assigned tail pe ddd studying ddd partitioning approach use two different partitioning algorithms first algorithm called dddmulticluster follows multicluster approach depicted 3 subset isavisible registers assigned pe isavisible register notion homepe studies n th pe considered homepe registers r r number generalpurpose registers p number pes assignment instructions pes done depicted table 5 number number pe source dest instruction registers registers assigned dest register source register dest register st source register 2 source registers destination register homepe register else homepe dest register table 5 instruction assignment dddm partitioning scheme second ddd algorithm called dddp dddpews makes better use data dependence information uses offline hardware construct register data flow graph rdfg trace treepath trace encountered first time rdfg trace data dependence chains strands identified rdfg dependence strands may communication strands identified relative pe assignment made strands view ipc number pes dddm edd ipc gcc number pes31 go ipc number pes31 li ipc number pes31 ipc number pes31 vortex ipc number pes figure 5 ipc without nops varying number pes unidirectional ring pe interconnect reduce communication latency strands flow data one strand another strands given relative pe assignment consumer strands pe one immediately following producer strands pe strands data dependences strands trace marked relocatable time instruction dispatch dispatch unit decides pe placement strand based dependences data coming outside trace relative pe placement decided statically offline hardware 2cycle penalty stall imposed trace seen first time order form rdfg relative pe assignments pe assigned instruction full instruction assigned closest succeeding pe empty slot performance results 41 ipc unidirectional ring first set studies focuses comparing performance different partitioning algorithms number pes varied unidirectional ring used connect pes figure 5 plots ipc values obtained default parameters pe scheduler benchmark values p consider f1 2 4 8 12 16g graph figure 5 corresponds particular benchmark program 3 plots one corresponding decentralization approach edd first edd approach perform well ringtype pe interconnect expected edd approach unable exploit localities communication important using ring topology interconnect pes performance increases slightly number pes increased 2 thereafter downhill ddd performance two ddd partitioning algorithms quite different performance dddm algorithm generally poor similar performance edd algorithm simulated get good performance dddm approach optimizing compiler needs rename register specifiers considering idiosyncrasies dddm execution model otherwise little data dependence localities likely captured dddp approach performance generally keeps increasing number pes p increased 1 8 values p dddp algorithm performs best among investigated partitioning algorithms dddp better able exploit localities communication instructions spread across moderate number pes striking observation performance dddp starts dropping number pes increased beyond 8 drop performance datadependent instructions getting allocated distant pes resulting large delays forwarding register values distant pes one reason spreading datadependent instructions rdfg formation instruction partitioning done individual trace basis knowledge subsequent traces available made use partitioning instructions better placement instructions made cdd performance cdd schemes keeps increasing steadily number pes increased 1 16 two reasons available parallelism increases instruction window size ii register instances short lifetime 5 19 resulting little communication register values nonadjacent pes number pes increased beyond 8 cdd approach starts performing better ddd approaches ddd edd begin perform worse arena notice however three benchmarks compress95 li m88ksim performance dddp 4 pes better performance cdd remaining three benchmarks performance 4 pe dddp processor much lower 16 pe cdd16 processor highlights importance developing ddd algorithms perform better distribution instructions large number pes 42 ipc bidirectional ring investigate unidirectional nature ring cause drop dddps performance higher values p also experimented bidirectional ring table 6 tabulates ipc values obtained 12 pe ddd p unidirectional pe interconnect bidirectional pe interconnect simulated bidirectional ring configuration 12 pes performance dddp starts dropping data table 6 indicate bidirectional ring little improve performance dddp except m88ksim registers modest improvement 313 346 benchmark ipc obtained program unidirectional ring bidirectional ring gcc 227 227 go 182 182 li 315 317 vortex 359 364 table unidirectional ring bidirectional ring pe interconnects 43 ipc crossbar results presented far obtained ringtype interconnections pes next investigate decentralization approaches scale pes interconnected realistic crossbar figure 6 plots ipc values obtained pes interconnected log 2 pcycle crossbar comparison data figures 5 6 show crossbar interconnect performance edd improved slightly benchmarks dddp performance decreased compared ring interconnect lower values p remains less higher values p cdd performance crossbar consistently lower performance ring fact contrary case ringtype interconnect performance cdd16 realistic crossbar decreases number pes increased overall results realistic crossbar show performance dddp slightly better cdd16 benchmarks 44 register traffic order get better understanding ipc results seen far next analyze register traffic occurring decentralized processors different partitioning algorithms used figure 7 plots distribution register results based number pes travel benchmark distributions given edd cdd16 dddp partitioning algorithms curves edd indicate significant amount register traffic distant pes cdd ddd amount register traffic pes steadily decreases pe distance increases dddp traffic dies almost zero register values travel 7 pes explains using bidirectional ring fetch much performance improvements however noticeable fraction register values travel 56 hops affects performance dddp scheme one reasons dddp scheme forms dfgs trace independently assigns instructions trace pes without considering dfgs subsequent traces need values produced trace cdd register traffic almost dies almost zero register values travel 3 pes register instances short lifetime 5 19 explains performance cdd ringtype interconnect continues increase number pes increased 5 discussion conclusions central idea behind decentralized execution models split dynamic execution window instructions amongst smaller parallel pes keeping pe relatively small circuitry needed search forwarding newly produced values greatly reduced thus reducing impact dynamic scheduling clock speed allocating dependent instructions pe much possible communication localities exploited thereby minimizing global communication within processor examined three categories decentralized execution models based type dependence use basis instruction partitioning categories execution unit dependence based decentralization edd ii control dependence based decentralization cdd iii data dependence based decentralization ddd detailed performance results obtained ensemble wellknown benchmarks lead us two important conclusions first currently used approach edddoes provide good performance even instruction window split across moderate number pes crossbar used connect pes second unidirectional ring used interconnect pes dddp approach provides best ipc values ipc number pes 12number pes gcc ipc edd dddp number pes number pes li31 number pes31 number pes vortex figure nops realistic log 2 p cycle crossbar pe interconnect moderate number pes used due ability exploit localities communication instructions large number pes used performance dddp starts dropping cdd approach begins perform better inability implemented dddp algorithm judiciously partition complex data dependence graphs across large number pes nevertheless performance implemented dddp algorithm 4 pes comparable better performance implemented cdd pes although results presented paper help understanding general trends performance different decentralization approaches study topic complete means variety execution modelspecific techniques isalevel microarchitectural level need explored decentralized execution models conclusive verdict reached addition important investigate extent factors value prediction instruction replication multiple flows control introduce additional wrinkles performance finally would worthwhile explore possibility good blending cdd ddd models using dddp processor ie cluster dddp pes basic pe cdd processor processor attempt exploit data independences lowest level granularity control independences higher level acknowledgements work supported us national science foundation nsf research initiation award ccr 9410706 career award mip 9702569 research grant ccr 9711566 indebted reviewers comments paper dave kaeli helps getting spec95 programs compiled mipsultrix platform r control flow prediction treelike subgraphs superscalar processors understanding simple processor performance limits multicluster architecture reducing cycle time partitioning multiscalar architecture register traffic analysis streamlining interoperation communication finegrain parallel processors pews decentralized dynamic scheduler ilp processing alpha 21264 500 mhz outoforder execution microprocessor exploiting fine grained parallelism combination hardware software techniques complexityeffective superscalar processors complexity effective pews microarchitecture trace cache low latency approach high bandwidth instruction fetching trace processors mul tiscalar processors multiscalar execution along single flow control efficient algorithm exploiting multiple arithmetic units superthreaded archi tecture thread pipelining runtime data dependence checking control speculation misc multiple instruction stream computer improving superscalar instruction dispatch issue exploiting dynamic code sequences mips r10000 superscalar micro processor tr exploiting finegrained parallelism combination hardware software techniques misc register traffic analysis streamlining interoperation communication finegrain parallel processors multiscalar architecture multiscalar processors control flow prediction treelike subgraphs superscalar processors trace cache improving superscalar instruction dispatch issue exploiting dynamic code sequences exploiting instruction level parallelism processors caching scheduled groups complexityeffective superscalar processors trace processors multicluster architecture understanding simple processorperformance limits mips r10000 superscalar microprocessor multiscalar execution along single flow control alpha 21264 superthreaded architecture ctr morano khalafi r kaeli k uht realizing high ipc scalable memorylatency tolerant multipath microarchitecture acm sigarch computer architecture news v31 n1 march aneesh aggarwal manoj franklin scalability aspects instruction distribution algorithms clustered processors ieee transactions parallel distributed systems v16 n10 p944955 october 2005 ramadass nagarajan karthikeyan sankaralingam doug burger stephen w keckler design space evaluation grid processor architectures proceedings 34th annual acmieee international symposium microarchitecture december 0105 2001 austin texas joanmanuel parcerisa julio sahuquillo antonio gonzalez jose duato onchip interconnects instruction steering schemes clustered microarchitectures ieee transactions parallel distributed systems v16 n2 p130144 february 2005 rajeev balasubramonian cluster prefetch tolerating onchip wire delays clustered microarchitectures proceedings 18th annual international conference supercomputing june 26july 01 2004 malo france balasubramonian sandhya dwarkadas david h albonesi dynamically managing communicationparallelism tradeoff future clustered processors acm sigarch computer architecture news v31 n2 may