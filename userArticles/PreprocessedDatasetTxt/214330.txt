buffer management sharedmemory time warp systems mechanisms managing message buffers time warp parallel simulations executing cachecoherent sharedmemory multiprocessors studied two simple buffer management strategies called sender pool receiver pool mechanisms examined respect efficiency particular interaction multiprocessor cachecoherence protocols measurements implementations kendall square research ksr2 machine using synthetic workloads benchmark applications demonstrate sender pools offer significant performance advantages receiver pools however also observed schemes especially sender pool mechanism prone severe performance degradations due poor locality reference large simulations using substantial amounts message buffer memory third strategy called partitioned buffer pool approach proposed exploits advantages sender pools exhibits much better locality measurements approach indicate partitioned pool mechanism yields substantially better performance sender receiver pool schemes largescale smallgranularity parallel simulation applicationsthe central conclusions study 1 buffer management strategies play important role determining overall efficiency multiprocessorbased parallel simulators 2 partitioned buffer pool organization offers significantly better performance sender receiver pool schemes studies demonstrate poor performance may result proper attention paid realizing efficient buffer management mechanism b introduction largescale sharedmemory multiprocessors kendall square research ksr2 recently convex spp important class parallel computers high performance computing applications recently sharedmemory machines become popular compute servers multiprocessor workstations sgi challenge sun sparcserver becoming common engineering scientific computing laboratories technological advances enable multiple cpus placed within single chip substrate multichip module simpler programming model offered sharedmemory machines enable remain important class parallel computers foreseeable future well known many largescale discrete event simulation computations excessively time consuming natural candidate parallel computation time warp well known synchronization protocol detects outoforder executions events occur recovers using rollback mechanism 8 time warp demonstrated success speeding simulations combat models 14 communication networks 12 queuing networks 4 digital logic circuits 1 among others assume reader familiar time warp mechanism described 8 concerned efficient implementation time warp sharedmemory multiprocessor computers prior work area focused data structures 4 synchronization 9 implementation shared state 6 11 concerned efficient buffer management strategies message passing sharedmemory machines assume hardware platform cachecoherent sharedmemory multiprocessor commercial machines mentioned earlier type assume multiprocessor contains set processors local cache automatically fetches instructions data needed assumed mechanism place ensure duplicate copies memory location different caches remain consistent typically accomplished either invalidating copies caches one processor modifies block updating duplicate copies 13 particularly concerned largescale small granularity discreteevent simulation applications specifi cally envision applications containing thousands tensor hundreds thousands simulator objects modest amount computation per simulator event event may require execution little hundreds machine structions small granularity arises many discreteevent simulation applications eg celllevel simulations asynchronous transfer mode atm networks simulations wireless personal communication services networks gatelevel simulations digital logic circuits applications even modest amount overhead central event processing mechanism lead substantial performance degradations thus important overheads incurred message passing event processing loop parallel simulation executive kept minimum messagepassing fundamental time warp mecha nism messagepassing utilized frequently crucial efficiently implemented particularly small granularity simulations become apparent later efficient buffer management essential achieving efficient message passing mechanism issue examined remainder paper organized follows section 2 describe underlying message passing mechanism assumed throughout study contrast message passing mechanisms messagebased parallel computers section 3 describes simple approach buffer allocation call receiver pools alternative approach called sender pools proposed performance relative receiver pools compared evaluated experimentally using implementation executing ksr2 multiprocessor section 4 observe sender pool approach generally superior receiver pool approach suffers severe performance degradations simulations utilizing large amounts memory section 5 describes third mechanism called partitioned pool approach addresses flaw sender pool scheme section 6 presents performance results using mechanism section 7 extend results considered synthetic workloads compare performance three mechanisms two benchmark applications studies demonstrate partitioned pool approach yields superior performance compared two mechanisms 2 message passing mechanism let us first consider implementation message passing sharedmemory multiprocessor assume time warp executive maintains certain number memory buffers contains information associated single event message use terms synonymously transmitting message one processor another involves following steps 1 sender allocates buffer hold message 2 sender writes data transmitted message 3 sender enqueues pointer message queue accessible receiver 4 receiver detects message queue removes reads possibly modifies contents message buffer many time warp systems message buffer include various fields pointers enqueuing buffer local data structures fields eg flags required time warp executive case receiver modify contents buffer implementations application program may allowed modify received message buffer although time warp executive would maintain separate copy case rollback occurs alternatively pointer flag fields may kept separate data structure external message buffer case contents message may modified receiver essential question concerned organization pool free unallocated message buffers central global pool clearly become bottleneck nat ural distributed approach associate buffer pool processor consider alternative organizations assuming processor maintains separate buffer pool instructive contrast message passing mechanism described messagebased parallel computer messagebased machine pool unallocated memory buffers maintained processor new buffer must allocated prior sending message message written buffer transmitted destination processor buffer reclaimed returned sending contrast sharedmemory multiprocessor message need explicitly transmitted copied receiver required messagepassing machines instead sufficient pass pointer message buffer destination processor memory caching mechanism transparently transfer contents message referenced receiver second difference messagepassing sharedmemory architectures latter provides global address space presents much richer set possible buffer organizations compared messagepassing machines 3 sender vs receiver buffer pools assume principal atomic unit memory buffer buffer contains fixed amount storage hold single event processor maintains pool free unallo cated buffers single buffer allocated prior message send buffers reclaimed message cancellation fossil collection either case processor last received buffer via message send responsible adding free pool key issues discussions follow interaction cache coherence protocol message sending mechanism number lock operations required operations require accesses nonlocal memory typically expensive existing machines instance ksr2 tens hundreds machine instructions may executed time single cache miss hundreds thousands instructions may executed time single lock operation 31 receiver pools one simple approach managing free buffers associate pool processor receiving message means buffer allocation routine obtains unused buffer buffer pool processor receiving message prior message send call receiver pool strategy althoughsimple implement receiver pools suffer two drawbacks first locks required synchronize accesses free pool even sender receiver lp mapped processor 1 processors list shared among processors send messages processor second drawback concerned caching effects discussed next multiprocessor systems using invalidatebased cachecoherence protocols receiver pools make effective use cache buffers free pool processor usually resident cache processor assuming buffer deleted caches replacement policy cases buffer last accessed event processing procedure executing processor assume sender receiver message reside different processors sending processor allocates buffer receiver writes message buffer series cache misses invalidations occur buffer moved senders cache later receiver dequeues message buffer places local queue second set misses occur buffer contents transferred back receivers cache invalidations also occur message buffer modified received processed thus two rounds cache misses one two rounds invalidations occur message transmission 32 sender pools alternative approach use sender pools scheme sending processor allocates buffer local pool ie buffer pool processor sending message writes message enqueues receiver scheme free pool local processor locks required control access also sender allocates buffer writes contents message memory references hit cache scenario described receiving processor accesses message buffer cache misses possibly invalidations occur case receiver pool mechanism thus round cache misses avoided using sender pools compared receiver pool approach sender pools create new problem however message send effect transfers buffer sending receiving processors buffer pool message buffers always reclaimed receiver fossil collection cellation buffer effect migrates processor processor less random fashion reused message sends different processors contrast receiver pool scheme buffers always return processors pool pool buffer initially allocated problem buffer migration memory buffers accumulate processors receive messages send leads unbalanced distribution buffers buffer pools processors becoming depleted processors bloated excess buffers locks could circumvented local message sends however separate buffer pool local messages address problem mechanism required redistribute buffers processors receive messages send processors opposite behavior buffer redistributioncan accomplished using additional global buffer pool serves conduit transmitting unused buffers example processor accumulating many buffers place extras globalpool diminished buffer pools extract additional buffers needed pool 7 thus principal advantages sender pool elimination lock free pool better cache behavior multiprocessors using cache invalidation protocols central disadvantage overhead buffer redistribution 33 cache update protocols previously described cache behavior invalidatebased coherence protocols let us consider updatebased protocols updatebased cache coherence protocols sender receiver pools exhibit similar cache behavior though receiver pools offer slight edge sender pools result ad ditional unnecessary update traffic among caches see consider following receiver pool scheme buffer reside cache receiving processor assuming sufficient space cache sender writes message buffer misses occur unless buffer recently referenced sending processor another message send update protocol automatically transfer message via cache updates receivers copy buffer resulting cache hits receiver reads message sender pool scheme cache hits occur sender writes message buffer receiver accesses result cache misses unless buffer already receivers cache thus schemes experience one round hits one round misses message transmission however update protocol processor accesses buffer maintain copy local cache even using buffer longer example receiver pool scheme five different processors reuse buffer send message processor p buffer appear cache five processors well p sixth processor uses buffer send message p writes message buffer update copies residing five processors well p updates sent five sender processors clearly unnecessary sender pool exhibits even worse behavior message buffer may migrate arbitrary number processors thus reside many caches receiver pool scheme receiver pool approach buffer migration limited processors sending messages p number processor caches must updated course reduced processors replace cache block make room others cache misses however systems large caches replacement may occur time busbased multiprocessor main disadvantage updating several processors contention cache directories updates inherently broadcast requests thus see neither sender receiver pool scheme provides totally satisfactory solution multiprocessors using updatebased cache coherence protocols however later propose third scheme called partitioned buffer pools provide satisfactory solution avoids unnecessary updates described 34 hardware platform compare performance sender receiver pools schemes implemented multiprocessorbased time warp system implementation uses direct cancellation minimize cost message cancellation 4 buffer contains various pointers flags described earlier modified receiver message hardware platform used experiments kendall square research ksr2 multiprocessor ksr processor contains 32 mbytes local cache memory faster 256 kbyte subcache ksr somewhat different cachecoherent multiprocessors main memory hold data stored processors cache effect secondary storage acts main memory ksr processors organized rings ring containing processors cache invalidation protocol used maintain coherence data neither subcache local cache fetched another processors cache reside another cache secondary storage via virtual memory system details machine architecture performance described 10 3 machine contains 40 mhz twoway superscalar pro cessors accesses subcache require 2 clock cycles accesses local cache require 20 cycles time access another processors cache depends ring traffic cache miss serviced processor ring takes approximately cycles 10 cache miss serviced processor another ring requires approximately 600 cycles discussion follows cache miss refers miss megabyte cache subcache estimate single ksr 2 processor approximately 20 faster sun sparc2 workstation based measurements sequential simulations lock unlock operation requires 3 sec absence contention 14 sec pair processors ring 32 sec pair processors different rings 3 p 10 experiments described use single ring except processor runs use processors two different rings experiments performed ksr2 running ksr os r122 manufacturer provided c compiler used experiments except explicitly stated otherwise special compiler optimizations used generate object files sense performance reported conservative performance measurements receiver pool implementation memory evenly distributed across processors sender pool scheme 30 total memory placed global pool rest evenly distributed among processor free pools fraction selected empirically maximize performance number buffers processors buffer pool exceeds initial allocation excess buffers moved free pool buffer pool contains five fewer buffers additional buffers reclaimed global pool available restore processor initial allocation initial experiments used synthetic workload model called phold 5 model uses fixedsized message popu lation event generates one new message timestamp increment selected exponential distribution destination logical process lp selected uniform distribution first measured send times messages transmitted different processor time includes allocating free buffer writing message buffer enqueuing buffer queue destination processor phold 64 lps message population 128 executed 8 processors 76 megabytes memory allocated state event buffers scheme time perform message send measured using ksr x user timer primitive sender receiver pool implementations use prefetch mechanism gain exclusive copy buffer prior receiving message prefetch also invalidates copy message residing sending processors cache figure 1 shows average time message send using receiver pool uppermost line sender pool third line top strategies different message sizes ex pected sender pool outperforms receiver pool scheme line two second line top separates performance improvement results elimination free pool lock distance upper two lines improvement results better cache behavior distance second third lines lock time measured approximately 20 sec consistent times reported 3 one considers contention lock increases access time degree additional caching overheads receiver pools implementation increases size message misses occur writing data message buffer measured approximately 34 sec per cache miss consistent vendor reported cache miss times lower line represents message copy time ie time write data message sender pool scheme ie cache misses using hardware monitors provided ksr machine measured number cache misses occurred sender receiver pool implementations figure 2 shows number cache misses different message sizes phold 64 lps 256 messages 8 processors run length approximately one million committed events approximately megabytes allocated simulation data confirms receiver pool approach encounters cache misses sender pool approach100300500700900 time msg size kb recv pool sender pool msg copy figure 1 message send times sender receiver pools four curves represent top bottom 1 message send time using receiver pools 2 message send time sender pools plus additional caching overhead encountered receiver pools 3 message send time sender pools 4 time copy message message buffer sender pools 1 larger slope 3 additional caching overheads receiver pools mentioned earlier central drawback send pool scheme need redistribute buffers buffer redistribution using global pool performed fossil collection turn occurs time processor determines fewer five free buffers remaining free pool performance metric used studies number events committed per second real time execution calculated total number events committed simulation divided total execution time refer metric committed event rate simply event rate note event rate declines number rollbacks increases raw performance eg speed message sends decreases2468100 1 cache page millions message size kb receiver pools sender pools figure 2 cache misses sender receiver pools figure 3 shows committed event rate parallel simulator using receiver pool sender pool mechanisms experiments use eight ksr2 processors benchmark program phold using 256 lps message population 1024 seen sender pool significantly outperforms receiver pool strategy indicating reduced time perform message sends far outweighs time required buffer redistribution third curve shown figure 3 shows performance alternate buffer management scheme discussed later finally note since hardware provides mechanism prefetch data may possible lessen caching effects receiver pool scheme prefetching buffer prior message send assumes course sender determine destination processor sufficiently far advance send perform prefetch currently examining performance improvement results technique however even prefetching message sends still faster sender pools lock free pool needed 4 performance large simulations although sender receiver pools good performance small amounts memory observed dramatic performance degradations large simulations utilizing substantial amount message buffer memory performance phold simulations assuming 8 proces sors 256 lps message population 1024 total amount memory allocated message buffers changed shown figure 4 run includes execution approximately one million committed events point represents median value least 5 runs typically 9 runs used seen performance declines dramatically total amount buffer memory across processors exceeded 20 25 megabytes observed behavior independent number processors used simulation decline performance surprising runs utilizing 8 processors total 256 megabytes cache memory available yet significant performance degradations occur amount buffer memory increased comparatively modest levels eg performance eventssec processors partitioned pools sender pools receiver pools figure 3 number events committed per second real time phold sender receiver partitioned pool mechanisms reason declining performance internal fragmentation virtual memory system resulting poor spatial locality leads excessively large number pages processors working set page thrashing behavior among caches page size ksr kbytes discussed page thrashing lead disk accesses ksr would resulted much severe performance degradations nevertheless cause significant reduction performance2500035000450005500065000 performance eventssec memory allocated mb partitioned pools sender pools receiver pools figure 4 performance amount memory changed consider execution sender pool simulation ini tially buffers allocated processor packed contiguous block memory however sender pool scheme buffers migrate one processor another message send global pool via buffer redistribution mechanism thus short period time set buffers contained sender pool processor include buffers originally allocated many different processors thus scattered across address space thus instant time processors buffer pool includes portions many different pages entirely possible many cases likely buffers particular proces sors buffer pool include portion every page buffer memory available system500001500002500003500000 20 40 page memory allocated mb receiver pools sender pools partitioned pools figure 5 page miss counts amount memory changed well known performance virtual memory systems declines number pages processors working set increases memory management overheads become large uniprocessors multiprocessors minimally misses translation lookaside buffers tlbs occur eventually page faults occur performance poor ksr space must allocated entire page portion resides cache cache miss occurs referenced page already mapped processors cache space must allocated page cache miss serviced referenced block loaded cache page miss subsequent blocks loaded cache demand referenced ie result cache misses page misses page misses usually result access secondary storage assuming total amount memory well amount physical cache memory page usually reside one caches nevertheless tables managed virtual memory system must updated miss page misses due poor locality reason performance poor total amount buffer memory exceeded 25 megabytes amount buffer memory less 25 megabytes pages entire buffer memory across processors maintained cache processor however amount exceeds size processors working set pages longer fit local 32 megabyte cache pages must mapped processor results thrashing behavior excessive overheads incurred mapping unmapping pages validate effect reason poor per formance number page misses measured experiments depicted figure 4 data shown figure 5 seen aforementioned decline performance beyond 25 megabytes memory accompanied dramatic increase number page misses page miss problem also severe receiver pool scheme causes significant performance degradations large simulations receiver pool approach avoid page miss overheads processor hold pages corresponding buffers pool plus receive pools processors sends messages thus processor sends messages small subset processors system likely pages fit cache page misses occur worst case however processor send messages processors system case page miss overheads may severe sender pool scheme phold application represents worst case processor sends messages every processor system sender pools outperforms receiver pools low amounts memory cache locality verified better page miss figures bigger amounts memory sender pool cannot hold pages tlb subsequently shows worse performance page misses receiver pools verify evaluate effect page misses local ity set experiments conducted using receiver pool strategy modified version phold workload original phold workload selects destination process processor message uniform distribution modified workload processor selects among k processors number processors destination using uniform distribution value k referred size processors neighborhood workload processor receives messages k processors number page misses measured amount memory varied k set 1 implying unidirectional ring topology 2 4 using receiver pools results experiments shown figure 6 seen amount memory necessary yield aforementioned increase page misses becomes progressively smaller k increased agreement analysis assume processor effectively utilize 25 megabytes memory knee observed earlier experiments message buffer pool k equal 1 page misses avoided long two buffer pools local pool one remote pool processor messages sent utilize less 25 megabytes data implies buffer pools large 125 megabytes per processor 100 megabytes entire system recall 8 processors toler ated similarly one would anticipate knee occur neighborhood sizes 67 megabytes k equal 2 40 megabytes k equal 4 data figure 6 consistent approximate analysis500001500002500003500000 20 40 page memory allocated mb locality 4 figure page misses receiver pools different neighborhood sizes vs amount memory allocated lines top bottom neighborhood sizes localities 4 2 1 partitioned buffer pools third strategy developed designed capitalize advantages sender pool scheme time avoid page miss problem minimize number page misses buffer management scheme minimize number pages containing message buffers utilized processor another way stating set buffers used processor packed contiguous memory locations much possible achieve necessary prevent arbitrary migration buffers one processor another partitioned buffer pool scheme uses sender buffer pools pool processor subdivided set sub pools one processor sends messages let refer buffer pool ie subpool processor used send messages processor j processor must allocate buffer b ij whenever wishes send message j processor j reclaims message buffer via fossil collection message cancellation must returned buffer b ji buffer subsequently returned processor either j sends message utilizes buffer buffer returned via buffer redistribution mechanism strict rules concerning buffer returned pool global pool needed buffer redistribution elaborated upon later partitioned pool scheme memory buffer initially allocated b ij may reside b ij b ji lifetime simulation size working set processor pages hold b ij k page miss problem avoided long pages reside processors cache second advantage partitioned pool scheme provides type flow control must provided using separate mechanisms original sender receiver pool schemes time warp prone buffer hogging phenomena certain processors may allocate disproportionate share buffers classic example behavior source process serves purpose provide stream messages simulation source processes used simulations open queuing networks instance model new jobs arrive system source processes never receive messages never roll back fact generate true events events eventually commit however processes throttled flow control mechanism easily execute far ahead simulated time processes fill available memory new events leaving buffers messages phenomenon severely degrade performance processors memory filled messages generated sources original sender receiver pool schemes nothing prevent source filling buffers certain processors messages problem compounded sender pool scheme source immediately scoop additional free buffers appear global pool via buffer redistribution mechanism thereby hogging even larger portion systems buffers partitioned buffer pool scheme buffer hogging limited buffer pools utilized processor executing source process communications processors affected separate pools reserve buffers use thus partitioned pool scheme provides protection buffer hogging problem third advantage partitioned buffer pool scheme updatebased cache protocols operate efficiently either original sender receiver pool schemes recall problem original schemes buffers may simultaneously reside several caches ie caches processors used buffer resulting unnecessary cache update traffic partitioned pool scheme buffer may utilized two processors buffer used processor reside processors caches processor allocates buffer writes contents message cache hits occur assuming buffer deleted replaced memory references update protocol immediately write message destination processors cache also holds copy message buffer receiver accesses buffer experience cache hits fewer unnecessary update requests generated buffer management scheme central disadvantage partitioned pool scheme buffer pool processor subdivided several smaller pools buffers resulting somewhat less efficient utilization memory thus scheme may require memory either original sender receiver pool schemes processor sending message processor j b ij empty message send cannot performed even though many buffers may reside pools local processor sending message one could course allocate buffer another pool satisfy request however would quickly degenerate original sender pool scheme result performance problems cited earlier general desirable use different sized buffer pools within processor size pool proportional amount traffic flowing processors general size pool change dynamically ever purposes study consider fixedsized buffer pools size pool manually set beginning simulation 6 implementation details performance partitioned buffer pool scheme implemented sharedmemory time warp system message send performed buffer available designated pool event aborted returned unprocessed event list typically results busy wait behavior event continually aborted retried buffer redistribution pair processors necessary performed fossil collection performed periodically user defined interval net increase number buffers residing particular pool exceeds certain threshold 25 original size pool experiments processor attempts return number buffers equal net gain restore initial distribution buffers among pools noted earlier partitioned buffer pool scheme requires buffer reclaimed fossil collection returned appropriate buffer pool specifically buffer corresponding message sent processor j must returned processor js buffer pool rather laboriously scan fossil collected buffers returning appropriate pool mechanism called onthefly fossil collection used soon message processed immediately returned even though buffer may still required handle future rollbacks buffer allocator allowed allocate buffer timestamp larger gvt ensures buffers reclaimed gvt guaranteed message contained buffer longer needed buffers reclaimed message cancellation assigned zero timestamp returned buffer pool ensure immediately reused onthefly fossil collection amortizes overhead fossil collection entire simulation committed event rate receiver sender partitioned pools strategies different amounts memory compared figure 4 workload phold 256 lps message population 1024 seen partitioned buffer pool approach consistently outperforms two schemes unlike original receiver pool sender pool schemes decline performance detected beyond 25 megabytes figure 5 verifies page miss problem eliminated partitioned buffer pool scheme number remains relatively low partitioned pool scheme memory sizes tested applications performance data presented thus far resulted measurements synthetic workloads appropriate ask behaviors also prevalent actual parallel simulation applications one would encounter practice impact buffer management cache effects overall per formance answer questions additional experiments performed using three buffer management policies certain small granularity simulations namely hypercube topology communications network personal communication services pcs network simulation benchmark amount memory used sender receiver pool schemes optimized experimentally maximize performance use less 20 megabytes partitioned pool implementation uses 8 megabytes per processor message buffers experiments 71 hypercube routing first application message routing simulation 7 dimensional binary hypercube 128 nodes messages routed randomly selected destination nodes using wellknown e cube routing algorithm message lengths selected uniform distribution addition transmission delays may delays due congestion nodes queued messages messages served nodes using firstcomefirstserve discipline message reached destination immediately reinserted network new destination selected uniform distribution experiments 2048 messages continuouslyrouted throughthe network fashion figure 7 shows committed event rates simulation three buffer management schemes different numbers processors partitioned pools strategy outperforms two schemes cases performance differential increases number processors increased2000060000100000140000 performance eventssec processors partitioned pools sender pools receiver pools figure 7 performance hypercube routing simulator 72 personal communications services network pcs 2 simulation wireless communication network set radio ports structured square grid one port per grid sector grid sector cell assigned fixed number channels portable mobile phone resides cell period time moves another cell phone call arrives portable moves new cell new radio channel must allocated connect maintain phone call corresponding portable channels busy call blocked dropped principal output measure interest blocking probability simulated pcs network contains 1024 cells grid 25000 portables cell contains 10 radio channels portable remains cell average 75 minutes time selected exponential distribution portable moves one four neighboring cells equal probability call length time period calls also exponentially distributed means 3 minutes 6 minutes respectively average computation time event excluding time schedule new events approximately microseconds lps pcs simulation selfinitiating ie send messages advance simulated time communications highly localized typically 90 messages transmitted lps mapped processor many messages sent lp figure 8 shows committed event rate simulation using three buffer management schemes seen partitioned buffer pool scheme outperforms strategies though differential large experiments due high amount locality communication pcs simulation50000150000250000 performance eventssec processors partitioned pools sender pools receiver pools figure 8 performance pcs three buffer management schemes 8 conclusion future work implementation efficient parallel simulation systems sharedmemory multiprocessors requires careful consideration interaction simulation executive hardware caching virtual memory systems work focused one aspect namely buffer management message passing mechanism experiences ksr2 multiprocessor demonstrate severe performance degradations may result interaction carefully considered simulation executives design studied three buffer management strategies termed sender pool receiver pool partitioned pool schemes sender pool scheme generally performs better receiver pool scheme flawed severe performance degradations occur simulations requiring large amounts memory much less physical memory provided machine partitioned pool scheme outperforms two approaches much factor two benchmark applications future avenue research refining partitioned buffer scheme automatically allocate appropriate amounts memory individual pools automatically adjust sizes pools maximize performance another open question quantitatively evaluate effects examined context machine architectures although experiments performed context time warp executing ksr2 believe results also applicable contexts first methods improve performance involve restructuring simulation executive maximize locality memory reference pattern locality fundamental efficient utilization cache virtual memory system sense believe measurements suggest approaches fruitfully applied sharedmemory multiprocessors though performance gains realized depend heavily specifics architecture fur ther message passing common widely used construct nearly parallel simulation mechanisms proposed date thus believe results ramifications synchronization protocols conservative opti mistic also application nonsimulation applications require high performance message passing acknowledgments work supported national science foundation grant number mip94085550 thankful samir das comments r distributed simulation largescale pcs networks ring performance kendall square multiprocessor time warp shared memory multiprocessor performance time warp synthetic workloads parallel discrete event simulation using spacetime memory virtual time synchronous parallel discrete event simulation sharedmemory multi processors shared variables distributed simulation benchmarking time warp operating system computer network simulation distributed combat simulation time warp model performance benchmarking smtw ss7 performance model simulation tr virtual time time warp shared memory multiprocessor highperformance computer architecture 2nd ed shared variables distributed simulation distributed simulation largescale pcs networks ctr girindra sharma radharamanan radhakrishnan umesh kumar v rajasekaran nael abughazaleh philip wilsey time warp simulation clumps proceedings thirteenth workshop parallel distributed simulation p174181 may 0104 1999 atlanta georgia united states chris j booth david bruce peter r hoare michael j kirton k roy milner ian j relf dynamic memory usage parallel simulation case study largescale military logistics application proceedings 28th conference winter simulation p975982 december 0811 1996 coronado california united states christopher carothers kalyan perumalla richard fujimoto effect statesaving optimistic simulation cachecoherent nonuniform memory access architecture proceedings 31st conference winter simulation simulationa bridge future p16241633 december 0508 1999 phoenix arizona united states z xiao b unger r simmonds j scheduling critical channels conservative parallel discrete event simulation proceedings thirteenth workshop parallel distributed simulation p2028 may 0104 1999 atlanta georgia united states kiran panesar richard fujimoto adaptive flow control time warp acm sigsim simulation digest v27 n1 p108115 july 1997 christopher carothers david bauer shawn pearce ross highperformance low memory modular time warp system proceedings fourteenth workshop parallel distributed simulation p5360 may 2831 2000 bologna italy marcelctlin rou karsten schwan richard fujimoto supporting parallel applications clusters workstations virtual communication machinebased architecture cluster computing v1 n1 p5167 1998 samir r das richard fujimoto empirical evaluation performancememory tradeoffs time warp ieee transactions parallel distributed systems v8 n2 p210224 february 1997 samir r das richard fujimoto adaptive memory management optimism control time warp acm transactions modeling computer simulation tomacs v7 n2 p239271 april 1997