joint approximate diagonalization positive definite hermitian matrices paper provides iterative algorithm jointly approximately diagonalize k hermitian positive definite matrices bfgamma1 dots bfgammak specifically calculates matrix b minimizes criterion sumk1k nk log detdiagbckb logdetbckb nk positive numbers measure deviation diagonality matrices bckb convergence algorithm discussed numerical experiments performed showing good performance algorithm b introduction problem diagonalizing jointly approximately several positive definite matrices arisen least two different contexts first one statistical problem common principal components k group introduced flury 1984 considers k populations multivariate observations size n 1 obeying gaussian distribution zero means covariance matrices assume gamma k written b k b orthogonal matrix b diagonal matrices k symbol denotes transpose problem estimate matrix b colum common principal components sample covariance matrices c k populations well known n k c k distributed independently according wishart distribution n k degrees freedom covariance matrices gamma k see example seber 1984 log likelihood function based c constant term tr denotes trace therefore log likelihood method estimating b k amounts minimizing fixed b hard see expression minimized respect k notation diagm denoting diagonal matrix diagonal thus one led minimization respect b since b unit determinant 11 precisely measure global deviation matrices diagonality since hadamard inequality noble daniel 1977 exercise 1151 det det diagm equality diagonal thus minimizing 11 viewed trying find matrix b diagonalizes jointly matrices c much recently several authors cardoso souloumiac 1993 belouchrami et al 1977 pham garat 1997 introduced joint approximate diagonalization method separation source problem problem k sensors record linear mixture k sources denoting xt st vectors measurements sources time one goal extract sources observations called blind separation one specific knowledge sources statistically independent thus sensible method try find matrix b components bxt represent reconstructed sources independent possible easier work non correlation rather independence simple method would try make crosscorrelation eventually lagged sources vanish would lead joint approximate diagonalization certain set covariance matrices proposed belouchrami et al 1977 note pham garat 1997 also consider joint diagonalization use two matrices diagonalization exact see ex golub van loan 1989 cardoso souloumiac 1993 hand consider lagged covariance use higher order cumulants sources instead construct certain set matrices cumulants appear diagonal elements separation source solved joint approximate diagonalization matrices pointed authors use different measure deviation diagonality flury measure simply sum squares offdiagonal elements considered matrices common feature works diagonalizing matrix b taken orthogonal work shall drop restriction orthogonality condition part assumption flury 1984 clear reason satisfied condition justified works cardoso souloumiac 1993 belouchrami et al 1997 since authors prenormalized observations uncorrelated unit variance want avoid pre normalizing stage separation source procedure adversely affect performance since statistical error committed stage cannot corrected following effective separation stage dropping orthogonality restriction obtain singlestage separation procedure simpler perform better note without orthogonality restriction exact joint diagonalization possible two matrices see ex golub van loan 1989 two one achieve approximate joint diagonalization relative measure deviation diagonality take measure 11 two following reasons firstly traced back likelihood criterion widely used statistics secondly criterion invariant respect scale change remains matrices diagonalized pre postmultiplied diagonal matrix measure consists taking sum squares offdiagonal elements matrices nice invariant property course one introduce property first normalizing matrices unit diagonal element resulting criterion would hard manipulated main result paper derivation algorithm perform joint approximate diagonalization sense criterion 11 without restriction diagonalizing matrix orthogonal algorithm similarity cardoso souloumiac 1993 even flury gautschi 1996 also operates successive transformations pairs rows columns matrices diagonalized however convergence proof completely different since longer rely orthogonality property incidentally method proof easily adapted prove convergence result flury gautschi 1996 much simpler way 2 algorithm one frequently encounters complex data signal processing applications shall consider complex hermitian instead real symmetric positive definite matrices note cardoso soulomiac 1993 bellouchrani et al 1997 also work complex setting goal find complex matrix b matrices close diagonal possible notation denoting transpose complex conjugated measure deviation diagonality taken 11 n k positive weighs need integers note since c k depend b minimization criterion 11 reduced algorithm consists performing successive transformations time pair rows b ith row b idelta jth row say according b c way criterion sufficiently decreased whether decrease sufficient question shall returned next section done procedure repeated another pair indices convergence achieved denote u ij general element matrix decrease criterion associated transformation 21 du ii u k ii u k natural idea chose b c maximize decrease however maximization cannot done analytically idea maximize lower bound instead since logarithm function convex two sequences positive 1 one applying inequality decrease bounded ii ii u k introducing matrices ii n lower bound rewritten bp log c dq maximization 23 done analytically shown since 23 vanishes clear maximum non negative zero maximum also attained thus decrease criterion associated transformation using values b c realizing maximum 23 positive unless attains maximum let us return maximization 23 given always parameterize b c b c provided last matrix non singular put one express 23 log ja 0 first term right hand side 23 evaluated point therefore necessary sufficient condition point realizes maximum 23 last term right side non negative ffl j term seen equivalent nfflp 0 hence necessary condition condition hold matrices p q jointly diagonalized right hand side 26 reduces 0 last term expression seen equivalent jfflj 2 p 0 j therefore also necessary quadratic form variables ffl j non negative requirement satisfied p 0 2 hand using inequality seen bounded log ja 0 p 0 2 quadratic form jfflj variables ffl j non negative entailing expression bounded n logja choices ffl j thus proved necessary sufficient condition realize maximum 23 jointly diagonalizes p q diagonal terms 2 diagonalized matrix satisfy p 0 note last condition satisfied one simply needs permute appendix problem jointly diagonalizing two hermitian matrices p q size two necessarily positive definite completely solved shown p q positive definite proportional solution exists unique permutation scaling solutions obtained representative one premultiplying diagonal matrix permutation matrix denoting diagonal upper diagonal elements p q putting representative solution given 2ff positive shall prove p 2 q 1 1 equality matrices p q proportional ii p 0 defined 25 p 0 1 sign obviously results show solution maximization 23 given b 2fl c 2ff meaning proportional course result doesnt apply case two matrices p q proportional case diagonalize one would diagonalize proved would enough ensure 23 maximized thus case exists infinite number solutions even eliminating ambiguity associated scaling note flury gautschi 1996 algorithm operates similar principle however authors iterate transformation 21 fixed pair convergence changes another pair feel less efficient using pair decrease criterion tends smaller time changing one get big decrease first iterations algorithm also simpler program proved results ii announced formula 22 one l ii ii l ii ii ii ii l hi u k ii ii ii ii follows p 2 q 1 1 equality u 1 ii u 1 ii u k jj last case p q proportional proves result hand one gets 25 hence product p 0 denoting real part product p 0 2 obtained formula interchanging therefore difference p 0 last three terms expression may regrouped putting delta one hence noting fi2 therefore combining result one gets fig fi purely imaginary last term expression equals one thus obtains finally p 0 sign note computation ff fi fl could subjected large relative error matrices p q nearly proportional doesnt matter long matrices p q diagonalized sufficient accuracy criterion adequately decreased end note solution problem remains one replaces q arbitrary ae one chose ae p q nearly proportional r almost zero numerical calculation r subjected large relative error main source error since near cancellation subsequent calculation precisely let r calculated r algorithm would diagonalize p r hence would diagonalize absolute error small relative error large p q still accurately diagonalized argument repeated role p q interchanged one takes jointly diagonalizes q r alternative would preferable numerical accuracy point view leads smaller absolute value ae simple rule chose ae require results zero diagonal element r small possible thus defined defined want ae small chose first possibility q 1 second otherwise shall assume case case handled similar way thus diagonal upper diagonal elements r r one jointly diagonalizes p r first case q r second case hence first case compute ff fi fl second case second right hand sides formula provide efficient computations without loss accuracy remain applicable even matrices p q proportional replacing arbitrary non zero number ff first case fl second case taken arbitrary indeed computation ensures first case sufficient entail matrix p diagonalized see proof proposition a1 hence q proportional p similarly second case computation ensures q diagonalized hence p 3 convergence algorithm shown previous section algorithm decreases criterion step unless 25 maximized results section implies matrix p q diagonal already proved 1 occurs pair indexes j one would skip pair continue algorithm another pair occurs pairs algorithm stops explicitly stops one may recognize condition condition b stationary point criterion 11 indeed consider small change b form ffib hence matrix ffi represents relative change corresponding change 11 logu ri r rs denoting general elements ffi expanding expression respect ffi first order one gets thus vector components 2n ij 6 j viewed relative gradient vector criterion 11 prove decrease criterion step algorithm sufficient ensure convergence zero gradient vector proved previous section parameterizing b c 24 point realizing maximum 23 one express 23 defined 25 take ffl j left hand side 24 identity matrix 23 vanishes thus 26 0 must also vanish hence upper bound derived section 2 must positive therefore noting p 0 one log ja 0 ffl j elements inverse matrix left hand side 34 value 23 transformation 21 step algorithm uses precisely thus decrease criterion associated transformation bounded 23 must least large left hand side 34 definition ffl j 25 one hence noting right hand side 34 seen equal last expression rewritten therefore noting middle matrix quadratic form eigenvalues 1 sigma ae 0 ae 1 bounded nq 0 also lower bound decrease criterion step since criterion always decreased algorithm must converge limit therefore decrease criterion step algorithm must converge zero implying q 0 tends zero note p q ij ji defined 31 2n ij components relative gradient vector step algorithm still result proved convergence zero vector difficulty due lack normalization indeed algorithm constructs transformation scaling rows hence row b arbitrary large arbitrary small effect gradient even relative gradient considered avoid shall renormalize transformation matrices b step algorithm reasonable normalization procedure simplicity definiteness consider normalization makes rows b unit norm u ii bounded smallest largest eigenvalue c k therefore letting minimum smallest eigenvalues maximum largest eigenvalues c respectively one u k note 0 since matrices positive definite therefore 22 25 mn 0 mn 0 inequalities hold q 0 1 similar inequalities 0 b 0 replaced 2 thus p 0 2 bounded mm mm follows relative gradient vector criterion evaluated step algorithm components 2n ij converges zero result shows algorithm converges limit must stationary point criterion since algorithm always decreases criterion point actually local minimum unless algorithm started stationary point case stops immediately note sequence transformation matrices constructed algorithm normalized hence lying compact set admit convergent subsequence fact also holds subsequence therefore criterion admit unique local minimum algorithm converge however flury gautschi 1996 shown extreme cases minimization criterion 11 orthogonality constraint admits one local minimum therefore seems likely problem maximization without constraint uniqueness local minimum also satisfied cases nevertheless several local minima one still expect algorithm converge one indeed since proved gradient vector converge 0 algorithm must jump continually one local minimum another quite implausible thing existence finite number local clearly hold matrices c proportional single matrix extreme case conclude section showing algorithm behaves near solution much like newtonrhapson iteration provided matrices nearly jointly diagonalized derive newtonrhapson iter ation one makes second order taylor expansion criterion around current point minimizes expansion instead true criterion obtain new point already computed change criterion corresponding change ffib b resulting formula 32 need expand second order ffi note first order expansion already given 33 need pursue expansion second order yielding r rs u k ii r ir u k ii ffi u k assume matrices c nearly jointly diagonalized near solution diagonal term u k rs r 6 u k ir r 6 matrices would small relative diagonal term u k ii hence may neglect expression term second order ffi containg factor u k rs u k ii r 6 u k ir u k ii r 6 approximations expansion 32 second order reduces approximate newtonrhapson algorithm consists minimizing 32 respect ffi change b b solution minimization note 32 0 written sum unordered pairs minimization expression respect ffi ij easily done yielding worthwhile note diagonal elements ffi appear 32 0 anything long small convenience take 0 justified fact dividing ith row b ffib 1 one led matrix 0 zero diagonal element diagonal element note also ffi ij small matrix b ffib obtained successive transformations form 21 associated distinct pairs indexes j 6 j reverting notation defined 22 p 2 q 1 p q thus hand small p q fi defined 27 28 approximated p 1 hence since one gets b c equal approximately 2fifi results show new matrix b resulting one step newtonrhapson algorithm resulting sweep algorithm section 2 constituted successive steps associated distinct pairs indexes threfore algorithm quadratic convergence speed newtonrhapson iteration near solution newtonrhapson method may converge badly even started point far solution algorithm would better convergence behavior case since always decreases criterion 4 numerical examples consider example flury gautschi 1996 following 6 theta 6 matrices diagonalized gamma125 275 gamma45 gamma5 204 gamma372 gamma5 gamma45 245 gamma95 gamma372 gamma204 gamma45 gamma5 gamma95 245 372 204 gamma204 204 gamma372 372 5476 gamma468 372 gamma372 gamma204 204 gamma468 51247 7 7 7 7 5 take algorithm b identity matrix following table reports values criterion sweep steps associated 15 possible pairs indexes criterion 0809676 0189367 000562301 last sweep produces zero value criterion machine precision slightly negative value got comes rounding errors note since 2 matrices exact joint diagonalization achieved actually 3 sweeps sweep 0 corresponds initial matrices diagonalization already quite good 500000 gamma00198 gamma00013 gamma00001 00000 00000 gamma00013 00001 298099 00000 gamma00000 gamma00000 gamma00001 00000 00000 390333 gamma00000 gamma00000 00000 gamma00000 gamma00000 gamma00000 201550 gamma00000 00000 gamma00000 gamma00000 gamma00000 gamma00000 1004497 7 7 7 7 5 200000 00177 00052 00000 gamma00000 gamma00000 00177 100000 gamma00003 gamma00000 00000 00000 00052 gamma00003 400120 gamma00000 00000 00000 00000 gamma00000 gamma00000 307912 00000 00000 gamma00000 00000 00000 00000 591714 00000 gamma00000 00000 00000 00000 00000 4847167 7 7 7 7 5 corresponds transformation matrix definiteness rows b normalized unit norm fourth sweep zeros diagonal elements c 1 c 2 least 4 digits decimal point without changing diagonal elements transformation matrix b also almost unchanged 05000 05000 gamma05000 gamma05000 00000 gamma00000 05000 05000 05000 05000 gamma00000 00000 05527 gamma05527 04206 gamma04206 01688 gamma00817 03977 gamma03977 gamma05752 05752 gamma00664 gamma01324 00073 gamma00073 gamma00272 00272 06082 079287 7 7 7 7 5 one see algorithm converges quite fast flury gautschi needs 4 5 sweeps converge moreover makes several iterations pair indexes make one however algorithm solves problem since require transformation matrix orthogonal simple way implement orthogonality constraint least approximately add another matrix c 3 identity matrix give large weigh n 3 1 values criterion sweep given criterion 0809676 0226183 00291083 00290463 00290454 00290454 criterion decrease 4 sweeps change transformation produced fifth sweep also slight affecting last digit never 2 units matrix sweep 5 corresponding matrices c 1 c 2 500000 00000 00000 00000 gamma00000 gamma00000 00000 299224 00000 gamma18497 22318 01111 00000 00000 600000 00000 gamma00000 gamma00000 00000 gamma18497 00000 397221 gamma07727 10432 gamma00000 22318 gamma00000 gamma07727 202390 gamma00385 gamma00000 01111 gamma00000 10432 gamma00385 1002407 7 7 7 7 5 200000 gamma00000 gamma00000 gamma00000 gamma00000 gamma00000 gamma00000 402097 gamma00000 gamma23088 44428 17605 gamma00000 gamma00000 100000 gamma00000 gamma00000 gamma00000 gamma00000 gamma23088 gamma00000 317746 gamma12795 72126 gamma00000 44428 gamma00000 gamma12795 593949 05032 gamma00000 17605 gamma00000 72126 05032 4834577 7 7 7 7 5 results similar flurry gautsch 1986 note matrix b transposed course orthogonality constraint exactly satisfied 10000 gamma00000 00000 gamma00000 00000 00000 gamma00000 10000 gamma00000 00119 gamma00185 gamma00047 00000 gamma00000 10000 gamma00000 00000 00000 gamma00000 00119 gamma00000 10000 00060 gamma00253 00000 gamma00185 00000 00060 10000 gamma00007 00000 gamma00047 00000 gamma00253 gamma00007 100007 7 7 7 7 5 difference matrix identity matrix slight mention algorithm designed enforce orthogonality numerical results given examples showing good convergence property appendix joint diagonalization two hermitian matrices size two following results provide explicit complete solutions problem joint diagonalization two non proportional hermitian matrices order two proportionality understood large sense null matrix proportional one note matrices proportional problem degenerates diagonalization single matrix proposition let p q two non proportional hermitian matrices order two diagonal elements diagonal elements respectively zero real matrix b c diagonalizes simultaneously p q b c vanishes ii proportional 2ff fi ffi c 6 0 0 proportional 2ff ffi fi ffi one two square roots delta remark proportional 2ff fi implies proportional conversely since b proportional 2ff fi reduces proportional 0 1 proportional fi gamma ffi 2fl ffi chosen fi gammafi similar conclusion holds also condition ii proposition one exclude case since c needs proportional anything similarly case c excluded hence c b thus p q diagonalized actually transformed singular matrix null matrix proof since p q non proportional vectors p 1 linearly independent entails ff fi fl zero prove real expand consider solutions joint diagonalization problem condition transformed matrices diagonal written bp bq c dp c dq exclude trivial solutions equations imply matrices left hand side zero determinants thus equation holds b replaced c expansion one gets equations ffb solution b equation ffb determined multiplicative factor thus ff 6 0 either 6 0 ba root quadratic polynomial ffz square root delta therefore b proportional similarly fl 6 0 b must proportional chosen minus sign ffi solution previous one ffi case ff 6 0 note solutions still apply case since reduce b proportional 0 fi fi 0 ab 0 similar calculations apply equation ffd solutions c must proportional 2ff square root necessarily ffi shown rows b c matrix jointly diagonalizing p q must form havent proved converse results yield two choices b modulo constant multiple depending choice square root ffi delta similarly two choices c hence one must determine choice later must associated one former purpose shall consider choice possible corresponding choices c form ff fi sigma ffi fi upsilon ffi 2fl see one bpc convenient write two square roots delta form sigma ffi suppose ff 6 0 need consider choice since proportional c ffi bpc last expression expanded note p 1 hence first term expression using equality fi2 2ffp reduces expression vanishes minus sign used sigma hence bqc choice c ffi similar calculation q 1 q 2 q place shows choice leads bqc one needs consider choices indeed exists another choice c proportional 2ff fi bpc since one already bp2ff vectors c 2ff fi linearly independent one must similar argument one also must two equalities easily seen imply p q proportional contradicts assumption calculations similar case fl 6 0 interchanging b c ff fl p 1 reversing sign ffi finally possible choice b either proportional 0 1 1 0 according ffi equal plus minus fi easily seen case happen either pq real p q non proportional checked possible corresponding choice c first case proportional 1 0 0 1 second case 0 1 1 0 completes proof proposition corollary notation assumption proposition assume least one matrices p q positive determinant matrix b c jointly diagonalizes p q equals denotes sign function premultiplied permutation diagonal matrix proof first show delta 0 det p 0 det q 0 shall prove result case det q 0 proof case similar last two terms right hand side written thus first condition implies second condition implies thus p proportional q contradicting assumption roots real denote usual delta positive one delta fi since one obtains proposition a1 b c equals premultiplied diagonal matrix hand delta similar calculation proposition shows b c equals diagonal matrix times yields result corollary r blind source separation technique using secondorder statistics matrix computations algorithm simultaneous orthogonal transformation several positive definite symmetric matrices nearly orthogonal form common principal components k groups applied linear algebra multivariate observations tr ctr ale holobar milan ojsterek damjan zazula distributed jacobi joint diagonalization clusters personal computers international journal parallel programming v34 n6 p509530 december 2006 moudden jf cardoso jl starck j delabrouille blind component separation wavelet space application cmb analysis eurasip journal applied signal processing v2005 n1 p24372454 1 january 2005 ch servire pham permutation correction frequency domain blind separation speech mixtures eurasip journal applied signal processing v2006 n1 p177177 01 january andreas ziehe motoaki kawanabe stefan harmeling klausrobert mller blind separation postnonlinear mixtures using linearizing transformations temporal decorrelation journal machine learning research v4 n78 p13191338 october 1 november 15 2004 andreas ziehe motoaki kawanabe stefan harmeling klausrobert mller blind separation postnonlinear mixtures using linearizing transformations temporal decorrelation journal machine learning research 4 1212003 andreas ziehe pavel laskov guido nolte klausrobert mller fast algorithm joint diagonalization nonorthogonal transformations application blind source separation journal machine learning research 5 p777800 1212004