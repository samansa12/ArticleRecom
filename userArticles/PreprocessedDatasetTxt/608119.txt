combining classifiers meta decision trees paper introduces meta decision trees mdts novel method combining multiple classifiers instead giving prediction mdt leaves specify classifier used obtain prediction present algorithm learning mdts based c45 algorithm learning ordinary decision trees odts extensive experimental evaluation new algorithm performed twentyone data sets combining classifiers generated five learning algorithms two algorithms learning decision trees rule learning algorithm nearest neighbor algorithm naive bayes algorithm terms performance stacking mdts combines classifiers better voting stacking odts addition mdts much concise odts thus step towards comprehensible combination multiple classifiers mdts also perform better several approaches stacking b introduction task constructing ensembles classiers 8 broken two subtasks rst generate diverse set baselevel classiers baselevel classiers generated issue combine predictions arises several approaches generating baselevel classiers possible one approach generate classiers applying dierent learning algorithms heterogeneous model representations single data set see eg merz 14 another possibility apply single learning algorithm dierent parameters settings single data set finally methods like bagging 5 boosting 9 generate multiple classies applying single learning algorithm dierent versions given data set two dierent methods manipulating data set used random sampling replacement also called bootstrap sampling bagging reweighting misclassied training examples boosting techniques combining predictions obtained multiple baselevel classiers clustered three combining frameworks voting used bagging boost ing stacked generalization stacking 22 cascading 10 voting baselevel classier gives vote prediction prediction receiving votes nal prediction stacking learning algorithm used learn combine predictions baselevel classiers induced metalevel classier used obtain nal prediction predictions baselevel classiers cascading iterative process combining classiers iteration training data set extended predictions obtained previous iteration work presented focuses combining predictions baselevel classiers induced applying dierent learning algorithms single data set adopts stacking framework learn combine baselevel classiers end introduces notion meta decision trees mdts proposes algorithm learning evaluates mdts comparison methods combining classiers meta decision trees mdts novel method combining multiple classiers dierence meta ordinary decision trees odts mdt leaves specify baselevel classier used instead predicting class value directly attributes used mdts derived class probability distributions predicted baselevel classiers given example developed mlc45 modication c45 17 inducing meta decision trees mdts mlc45 described section 3 performance mdts evaluated collection twentyone data sets mdts used combine classiers generated baselevel learning algorithms two tree learning algorithms c45 17 ltree 11 rulelearning algorithm cn2 7 knearest neighbor knn algorithm 20 modication naive bayes algorithm 12 experiments compare performance stacking mdts performance stacking odts also compare mdts two voting schemes two stacking approaches finally compare mdts boosting bagging decision trees state art methods constructing ensembles classiers section 4 reports experimental methodology results experimental results analyzed discussed section 5 presented work put context previous work combining multiple classiers section 6 section 7 presents conclusions based empirical evaluation along directions work combining multiple classiers paper focus combining multiple classiers generated using dierent learning algorithms single data set rst phase depicted left hand side figure 1 set n baselevel classiers generated applying learning algorithms 1 single training data set l training set cn x new example cn cml figure 1 constructing using ensemble classiers left hand side generation baselevel classiers applying n dierent learning algorithms single training data set l right hand side classication new example x using baselevel classiers c combining method cml assume baselevel classier c predicts probability distribution possible class values thus prediction baselevel classier c applied example x probability distribution vector set possible class values p c c jx denotes probability example x belongs class c estimated predicted classier c class c j highest class probability p c c j jx predicted classier c classication new example x metalevel depicted right hand side figure 1 first n predictions fp c1 x p c2 baselevel classiers c x generated obtained predictions combined using combining method cml dierent combining methods used dierent combining frameworks following subsections combining frameworks voting stacking presented 21 voting voting framework combining classiers predictions baselevel classiers combined according static voting scheme change training data set l voting scheme remains dierent training sets sets learning algorithms baselevel classiers simplest voting scheme plurality vote according voting scheme baselevel classier casts vote prediction example classied class collects votes renement plurality vote algorithm case class probability distributions predicted baselevel classiers 8 let p c x class probability distribution predicted baselevel classier c example x probability distribution vectors returned baselevel classiers summed obtain class probability distribution metalevel voting classier predicted class x class c j highest class probability p cml 22 stacking contrast voting cml static stacking induces combiner method cml metalevel training set based l addition baselevel classiers cml induced using learning algorithm metalevel metalevel examples constructed predictions baselevel classiers examples l correct classications latter combiner intended combine predictions baselevel classiers induced entire training set l unseen examples special care taken constructing metalevel data set end crossvalidation procedure presented table 1 applied table 1 algorithm building metalevel data set used induce combiner cml stacking framework function build combinerl fa stratified partitionl n let c ji classier obtained applying j l n l let cv ji class values predicted c ji examples l let cd ji class distributions predicted c ji examples l endfor endfor apply aml lml order induce combiner cml return cml endfunction first training set l partitioned disjoint sets equal size partitioning stratied sense set l roughly preserves class probability distribution l order obtain baselevel predictions unseen examples learning algorithm j used train baselevel classier c ji training set l n l trained classier c ji used obtain predictions examples l predictions baselevel classiers include predicted class values cv ji well class probability distributions cd ji used calculate metalevel attributes examples l metalevel attributes calculated n learning algorithms joined together set metalevel examples set one parts metalevel data set lml repeating procedure times set l obtain whole metalevel data set finally learning algorithm aml applied order induce combiner cml framework combining multiple classiers used paper based combiner methodology described 6 stacking framework 22 two ap proaches class values predicted baselevel classiers used metalevel attributes therefore meta level attributes procedure used frameworks trivial returns class values predicted baselevel classiers approach use class probability distributions predicted baselevel classiers addition predicted class values calculating set metalevel attributes metalevel attributes used study discussed detail section 3 3 meta decision trees section rst introduce meta decision trees mdts discuss possible sets metalevel attributes used induce mdts finally present algorithm inducing mdts named mlc45 31 meta decision trees structure meta decision tree identical structure ordinary decision tree decision inner node species test carried single attribute value outcome test branch leading appropriate subtree leaf node mdt predicts classier used classication example instead predicting class value example directly odt would dierence ordinary meta decision trees illustrated example presented tables 2 3 first predictions baselevel classiers table 2a obtained given data set include predicted class probability distributions well class values metalevel data set table 2b metalevel attributes c 1 c 2 class value predictions two baselevel classiers c 1 c 2 given example two additional metalevel attributes conf 1 conf 2 measure condence predictions c 1 c 2 given example highest class probability predicted baselevel classier used measure prediction condence table 2 building metalevel data set predictions baselevel classiers b metalevel data set predictions baselevel classiers baselevel attributes x p c1 0jx p c1 1jx pred p c2 0jx p c2 1jx pred 0875 0125 0 0875 0125 0 b metalevel data set meta decision tree induced using metalevel data set given table 3a mdt interpreted follows condence conf 1 baselevel classier c 1 high c 1 used classifying example otherwise baselevel classier used ordinary decision tree induced using metalevel data set given table 3b much less comprehensible despite fact ects rule choosing among baselevel predictions note mdt odt need predictions baselevel classiers order make predictions table 3 dierence ordinary meta decision trees meta decision tree induced metalevel data set mlc45 b odt induced metalevel data set c45 c mdt written logic program mdt induced conf1 0625 c2 conf1 0625 c1 b odt induced conf1 0625 conf1 0625 conf1 0625 1 conf1 0625 c mdt written logic program comprehensibility mdt table 3a entirely due extended expressiveness mdt leaves mdt odt table 3a b induced propositional data set odt induced purely propositional mdt rst order logic program equivalent mdt presented table 3c predicate combineconf 1 c 1 conf 2 c 2 c used combine predictions baselevel classiers c 1 c 2 class c according values attributes variables conf 1 conf 2 clause program corresponds one leaf node mdt includes nonpropositional class value assignment rst second clause propositional framework possible assignments one class value another way interpreting meta decision trees meta decision tree selects appropriate classier given example domain consider subset examples falling one leaf mdt identies subset data one baselevel classiers performs better others thus mdt identify subsets relative areas expertise baselevel classiers area expertise baselevel classier relative sense predictive performance area better compared performances baselevel classiers dierent area expertise individual baselevel classier 15 subset data predictions single baselevel classier correct note process inducing meta decision trees two types attributes used ordinary attributes used decision inner nodes mdt eg attributes conf 1 conf 2 example metalevel data set role attributes identical role attributes used inducing ordinary decision trees class attributes eg c 1 c 2 hand used leaf nodes baselevel classier class attribute values attribute predictions baselevel classier thus class attribute assigned leaf node mdt decides baselevel classier used prediction inducing odts combining classiers class attributes used way ordinary attributes partitioning data set relative areas expertise based values ordinary metalevel attributes used induce mdts existing studies areas expertise individual classiers 15 original baselevel attributes domain hand used use dierent set ordinary attributes inducing mdts properties class probability distributions predicted baselevel classiers ect certainty condence predictions however original baselevel attributes also used induce mdts details two sets metalevel attributes given following subsection 32 metalevel attributes metalevel attributes calculate properties class probability cdp distributions predicted baselevel classiers ect certainty condence predictions first maxprobx c highest class probability ie probability predicted class predicted baselevel classier c example x next entropyx c entropy class probability distribution predicted classier c example x finally weightx c fraction training examples used classier c estimate class distribution example x decision trees weight examples leaf node used classify example rules weight examples covered rules used classify example property used nearest neighbor naive bayes classiers apply straightforward fashion entropy maximum probability probability distribution ect certainty classier predicted class value probability distribution returned highly spread maximum probability low entropy high indicating classier certain prediction class value hand probability distribution returned highly focused maximum probability high entropy low thus indicating classier certain predicted class value weight quanties reliable predicted class probability distribution intuitively weight corresponds number training examples used estimate probability distribution higher weight reliable estimate example mdt induced image domain uci repository 3 given table 4 leaf denoted asterisk species c45 classier used classify example 1 maximum probability class probability distribution predicted knn smaller 077 2 fraction examples leaf tree used prediction larger 04 examples training set 3 entropy class distribution predicted c45 less 014 sum knn classier condent prediction 1 c45 classier table 4 meta decision tree induced image domain using class distribution properties ordinary attributes knn maxprob 077147 c45 weight 000385 knn c45 weight 000385 c45 entropy 014144 c45 c45 entropy 014144 ltree condent prediction 3 2 leaf recommends using c45 prediction consistent commonsense knowledge domain classier combination another set ordinary attributes used inducing meta decision trees set original domain baselevel attributes bla case relative areas expertise baselevel classiers described terms original domain attributes example mdt table 5 table 5 meta decision tree induced image domain using baselevel attributes ordinary attributes shortlinedensity5 0 shortlinedensity2 0 knn shortlinedensity2 0 ltree shortlinedensity5 0 shortlinedensity5 0111111 ltree shortlinedensity5 0111111 c45 leaf denoted asterisk table 5 species c45 used classify examples shortlinedensity5 values larger 011 mdts based baselevel ordinary attributes provide new insight applicability baselevel classiers domain use however human expert domain use interpret mdt induced using attributes cannot interpreted directly point view classier combination note another important property mdts induced using cdp set metalevel attributes domain independent sense language expressing meta decision trees used domains x set baselevel classiers used means mdt induced one domain used domain combining set baselevel classiers although may perform well part due fact cdp set metalevel attributes domain independent depends set baselevel classiers c however odt built set metalevel attributes still domain dependent two reasons first uses tests class values predicted baselevel classiers eg tests root node odt table 3b second odt predicts class value clearly domain dependent sum three reasons domain independence mdts 1 cdp set metalevel attributes 2 using class attributes decision inner nodes 3 predicting baselevel classier used instead predicting class value 33 mlc45 modication c45 learning mdts subsection present mlc45 1 algorithm learning mdts based quinlans c45 17 system inducing odts mlc45 takes input metalevel data set generated algorithm table 1 note data set consists ordinary class attributes four dierences mlc45 c45 1 ordinary attributes used internal nodes 2 assignments form class attribute made mlc45 leaf nodes opposed assignments form class value 3 goodnessofsplit internal nodes calculated dierently described 4 mlc45 postprune induced mdts rest mlc45 algorithm identical original c45 algorithm describe c45s mlc45s measures selecting attributes internal nodes patch used transform source code c45 mlc45 available httpaiijssibernardmdts c45 greedy divide conquer algorithm building classication trees 17 step best split according gain gain ratio criterion chosen set possible splits attributes according criterion split chosen maximizes decrease impurity subsets obtained split compared impurity current subset examples impurity criterion based entropy class probability distribution examples current subset training examples denotes relative frequency examples belong class c gain criterion selects split maximizes decrement info measure mlc45 interested accuracies baselevel classiers c c examples ie proportion examples class equal class attribute c newly introduced measure used mlc45 dened accuracyc denotes relative frequency examples correctly classied baselevel classier c vector accuracies probability distribution properties elements sum 1 entropy calculated reason replacing entropy based measure accuracy based one c45 splitting process stops least one following two criteria satised 1 accuracy one classiers current subset 100 leading info ml 2 user dened minimal number examples achieved current subset case leaf node constructed classier maximal accuracy predicted leaf node mdt order compare mdts odts principled fashion also developed intermediate version c45 called ac45 induces odts using accuracy based info measure 4 experimental methodology results main goal experiments performed evaluate performance meta decision trees especially comparison methods combining classiers voting stacking ordinary decision trees well methods constructing ensembles classiers boosting bagging also investigate use dierent metalevel attributes mdts performed experiments collection twentyone data sets uci repository machine learning databases domain theories 3 data sets widely used comparative studies remainder section rst describe classication error rates estimated compared list baselevel metalevel learning algorithms used study finally describe measure diversity baselevel classiers use comparing performance metalevel learning algorithms 41 estimating comparing classication error rates experiments presented classication errors estimated using 10fold stratied cross validation cross validation repeated ten times using dierent random reordering examples data set set reorderings used experiments pairwise comparison classication algorithms calculated relative improvement paired ttest described order evaluate accuracy improvement achieved given domain using classier c 1 compared using clas sier c 2 calculate relative improvement 1 errorc 1 errorc 2 analysis presented section 5 compare performance meta decision trees induced using cdp ordinary metalevel attributes approaches c 1 thus refer combiners induced mlc45 using cdp average relative improvement across domains calculated using geometric mean error reduction individual domains classication errors c 1 c 2 averaged ten runs 10fold cross validation compared data set errorc 1 errorc 2 refer averages statistical signicance dierence performance tested using paired ttest exactly folds used c 1 c 2 signicance level 95 right gure tables results means classier c 1 signicantly betterworse c 2 another aspect tree induction performance simplicity induced decision trees experiments presented used size decision trees measured number internal leaf nodes measure simplicity smaller tree simpler 42 baselevel algorithms five learning algorithms used baselevel experiments two treelearning algorithms c45 17 ltree 11 rulelearning algorithm cn2 7 knearest neighbor knn algorithm 20 modication naive bayes algorithm 12 algorithms used default parameters settings output baselevel classier example test set consist least two components predicted class class probability distribution baselevel algorithms used study calculate class probability distribution classied examples two knn naive bayes calculate weight examples used classication see section 3 code three c45 cn2 adapted output class probability distribution well weight examples used classication classication errors baselevel classiers twentyone data sets presented table 7 appendix smallest overall classication error achieved using linear discriminant trees induced ltree however dierent data sets dierent baselevel classiers achieve smallest classication error 43 metalevel algorithms metalevel evaluate performances eleven dierent algorithms constructing ensembles classiers listed nine make use exactly set baselevel classiers induced algorithms previous section brief two perform stacking odts using algorithms c45 ac45 see previous sec three perform stacking mdts using algorithm mlc45 three dierent sets metalevel attributes cdp bla cdpbla two voting schemes selectbest chooses best baselevel classier scann performs stacking nearest neighbor analyzing dependencies among baselevel classiers addition boosting bagging decision trees considered create larger ensembles 200 trees uses ordinary decision trees induced c45 combining baselevel classiers uses odts induced ac45 combining baselevel classiers mdtcdp uses meta decision trees induced mlc45 set class distribution properties cdp metalevel attributes mdtbla uses mdts induced mlc45 set baselevel attributes bla metalevel attributes mdtcdpbla uses mdts induced mlc45 union two alternative sets metalevel attributes cdp bla pvote simple plurality vote algorithm see section 21 cdvote renement plurality vote algorithm case class probability distributions predicted baselevel classiers see section 21 selectbest selects baselevel classier performs best training set estimated 10fold stratied crossvalidation equivalent building single leaf mdt scann 14 performs stacking correspodence analysis nearest neighbours correspondence analysis used deal highly correlated predictions baselevel classiers scann transforms original set potentially highly correlated metalevel attributes ie predictions baselevel classiers new smaller set uncorrelated metalevel attributes nearest neighbor classier used classication new set metalevel attributes boosting decision trees two hundred iterations used boosting decision trees induced using j48 2 c45 default parameters settings pre post pruning weka 21 data mining suite implements adaboost 9 boosting method reweighting training examples bagging decision trees two hundred iterations decision trees used bagging using j48 default settings detailed report performance methods found appendix classication errors found table 9 sizes ordinary meta decision trees induced dierent metalevel combining algorithms given table 8 finally comparison classication errors method mdtcdp terms average relative accuracy improvement number signicant wins losses given table 10 summary detailed report given table 6 44 diversity baselevel classiers empirical studies performed 1 2 show classication error metalevel learning methods well improvement accuracy achieved using highly correlated degree diversity predictions baselevel classiers measure diversity two classiers used studies error correlation smaller error correlation greater diversity baselevel classiers 2 experiments bagging boosting performed using weka data mining suite includes j48 java reimplementation c45 dierences j48 results c45 results negligible average 001 maximum relative dierence 4 correlation dened 1 2 probability classiers make error denition error correlation normalized maximum value lower two classication errors alternative denition error correlation proposed 11 used paper error correlation dened conditional probability classiers make error given one makes error predictions classiers c c j given example x cx true class x error correlation set multiple classiers c dened average pairwise error correlations relative improvement ltree baselevel degree error correlation baselevel classifiers australian balance breastw bridgestd car chess diabetes echocardiogram german glass heart hepatitis hypothyroid image ionosphere iris soya tictactoe vote waveform wine insignificant significant relative improvement knn baselevel degree error correlation baselevel classifiers australian balance breastw bridgestd car chess diabetes echocardiogram german glass heart hepatitis hypothyroid image ionosphere iris soya tictactoe vote waveform wine insignificant significant figure 2 relative accuracy improvement achieved mdts compared two baselevel classiers ltree knn dependence error correlation among baselevel classiers graphs figure 2 conrm results meta decision trees relative accuracy improvement achieved mdts ltree knn two baselevel classiers highest overall accuracies decreases error correlation baselevel clas siers increases linear regression line interpolated points conrms trend shows performance improvement achieved mdts correlated diversity errors baselevel classiers table summary performance metalevel learning algorithms compared mdts induced using class distribution properties mdtcdp metalevel attributes average relative accuracy improvement number signicant winslosses average tree size applicable details tables 10 8 appendix metalevel algorithm ave rel acc imp mdtbla 1447 80 6694 mdtcdpbla 1391 80 7373 selectbest 781 52 1 scann 1395 96 na boosting 936 136 na bagging 2226 104 na 5 analysis experimental results results experiments summarized table 6 brief following main conclusions drawn results 1 properties class probability distributions predicted baselevel clas siers cdp better metalevel attributes inducing mdts baselevel attributes bla using bla addition cdp worsens performance 2 meta decision trees mdts induced using cdp outperform ordinary decision trees voting combining classiers 3 mdts perform slightly better scann selectbest methods 4 performance improvement achieved mdts correlated diversity errors baselevel classiers higher diversity better relative performance compared methods 5 using mdts combine classiers induced dierent learning algorithms outperforms ensemble learning methods based bagging boosting decision trees look claims experimental results detail 51 mdts dierent sets metalevel attributes analyzed dependence mdts performance set ordinary metalevel attributes used induce used three sets attributes properties class distributions predicted baselevel classiers cdp original baselevel domain attributes bla union cdpbla average relative improvement achieved mdts induced using cdp mdts induced using bla cdpbla 14 cdp signicantly better 8 domains see table 6 table 10 appendix mdts induced using cdp times smaller average mdts induced using bla cdpbla see table 8 results show cdp set metalevel attributes better bla set furthermore using bla addition cdp decreases performance remainder section consider mdts induced using cdp analysis results ordinary decision trees induced using cdp bla cdpbla results cdp actually presented paper shows claim holds also odts result especially important highlights importance using class probability distributions predicted baselevel classiers identifying relative areas expertise far baselevel attributes original domain typically used identify areas expertise baselevel classiers 52 meta decision trees vs ordinary decision trees compare combining classiers mdts odts rst look relative improvement using mlc45 cc45 see table 6 column cc45 table 10 appendix left hand side figure 3 performs signicantly better 15 signicantly worse 2 data sets 4 overall decrease accuracy geometric mean entirely due result tictactoe domain combining methods perform well exclude tictactoe domain 7 overall relative increase obtained thus say mlc45 performs slightly better terms accuracy however mdts much smaller size reduction factor 16 see table 8 despite fact odts induced c45 postpruned mdts relative improvement tictactoe balance breastw hypothyroid soya heart image vote hepatitis glass echocardiogram waveform iris german australian chess diabetes car bridgestd wine ionosphere avg insignificant significant relative improvement iris bridgestd image heart soya waveform vote breastw hepatitis german balance diabetes echocardiogram glass ionosphere australian hypothyroid chess wine car tictactoe avg insignificant significant figure 3 relative improvement accuracy using mdts induced mlc45 compared accuracy odts induced ac45 c45 get clearer picture performance dierences due extended expressive power mdt leaves compared odt leaves compare mlc45 cac45 see table 6 column cac45 table 10 right hand side figure 3 mlc45 ac45 use learning algorithm dierence types trees induce mlc45 induces meta decision trees ac45 induces ordinary ones comparison clearly shows mdts outperform odts combining classiers overall relative accuracy improvement 8 mlc45 signicantly better cac45 12 21 data sets signicantly worse one ionosphere consider also graph right hand side figure 3 mdts perform better odts two domains performance gains much larger losses furthermore mdts average 34 times smaller odts induced ac45 see table 8 reduction tree size improves comprehensibility meta decision trees example able interpret comment mdt table 4 sum meta decision trees performs better ordinary decision trees combining classiers mdts accurate much concise comparison mlc45 ac45 shows performance improvement due extended expressive power mdt leaves 53 meta decision trees vs voting combining classiers mdts signicantly better plurality vote 10 domains signicantly worse 6 however signicant improvements much higher signicant drops accuracy giving overall accuracy improvement 22 since performs slightly better plurality vote smaller overall improvement 20 achieved mdts mlc45 signicantly better 10 data sets signicantly worse 5 results show mdts outperform voting schemes combining classiers see table 6 table 10 appendix 04 relative improvement degree error correlation baselevel classifiers australian balance breastw bridgestd car chess diabetes echocardiogram german glass heart hepatitis hypothyroid image ionosphere iris soya tictactoe vote waveform wine insignificant significant 04 relative improvement degree error correlation baselevel classifiers australian balance breastw bridgestd car chess diabetes echocardiogram german glass heart hepatitis hypothyroid image ionosphere iris soya tictactoe vote waveform wine insignificant significant figure 4 relative improvement accuracy mdts two voting schemes dependence degree error correlation baselevel classiers explored dependence accuracy improvement mdts voting diversity baselevel classiers graphs figure 4 show mdts make better use diversity errors baselevel classiers voting schemes namely domains low error correlation therefore higher diversity baselevel classiers relative improvement mdts voting schemes higher however slope linear regression line smaller one improvement baselevel classiers still trend clearly shows mdts make better use error diversity baselevel predictions voting 54 meta decision trees vs selectbest combining classiers mdts signicantly better selectbest 5 domains signicantly worse 2 giving overall accuracy improvement almost 8 see table 6 table appendix relative improvement selectbest degree error correlation baselevel classifiers australian balance breastw bridgestd car chess diabetes echocardiogram german glass heart hepatitis hypothyroid image ionosphere iris soya tictactoe vote waveform wine insignificant significant figure 5 relative improvement accuracy mdts selectbest method dependence degree error correlation baselevel classiers results dependence analysis accuracy improvement mdts select best diversity baselevel classiers given figure 5 mdts make slightly better use diversity errors baselevel classiers selectbest slope linear regression line smaller one improvement voting methods 55 meta decision trees vs scann combining classiers mdts signicantly better scann 9 domains signicantly worse 6 see table 6 table 10 appendix however signicant improvements much higher signicant drops accuracy giving overall accuracy improvement almost 14 results show mdts outperform scann method combining classiers 04 relative improvement degree error correlation baselevel classifiers australian balance breastw bridgestd car chess diabetes echocardiogram german glass heart hepatitis hypothyroid image ionosphere iris soya tictactoe vote waveform wine insignificant significant figure relative improvement accuracy mdts scann method dependence degree error correlation baselevel classiers explored dependence accuracy improvement mdts scann diversity baselevel classiers graph figure 6 shows mdts make slightly better use diversity errors baselevel classiers scann slope linear regression line smaller one improvement voting methods 56 meta decision trees vs boosting bagging finally compare performance mdts performance two state art ensemble learning methods bagging boosting decision trees performs signicantly better boosting 13 signicantly better bagging 10 21 data sets mlc45 performed signicantly worse boosting 6 domains signicantly worse bagging 4 domains overall relative improvement performance 9 boosting 22 bagging see table 6 table appendix clear mdts outperform bagging boosting decision trees comparison fair sense mdts use baselevel classiers induced decision trees four learning methods boosting bagging use decision trees baselevel learning algorithm however show approach constructing ensembles classiers competitive existing state art approaches 6 related work overview methods constructing ensembles classiers found 8 several metalevel learning studies closely related work let us rst mention study using scann 14 combining baselevel classiers mentioned scann performs stacking using correspondence analysis classications baselevel classiers author shows scann outperforms plurality vote scheme also case baselevel classiers highly correlated scann use class probability distribution properties predictions baselevel classiers although possibility extending method direction mentioned therefore comparison cd voting scheme included study study shows mdts using cdp attributes slightly better scann terms performance also concept induced scann metalevel directly interpretable used identifying relative areas expertise baselevel classiers cascading baselevel classiers induced using examples current node decision tree step divide conquer algorithm building decision trees new attributes based class probability distributions predicted baselevel classiers generated added set original attributes domain baselevel classiers used study naive bayes linear discriminant integration two baselevel classiers within decision trees much tighter combiningstacking framework similarity approach class probability distributions used version stacked generalization using class probability distributions predicted baselevel classiers implemented data mining suite weka 21 however class probability distributions used directly properties maximal probability entropy makes domain dependent sense discussed section 3 indirect use class probability distributions properties makes mdts domain independent ordinary decision trees already used combining multiple classiers 6 however emphasis study partitioning techniques massive data sets combining multiple classiers trained dierent subsets massive data sets study focuses combining multiple classiers generated data set therefore obtained results directly comparable combining classiers identifying areas expertise already explored 15 13 studies description area expertise form ordinary decision tree called arbiter induced individual baselevel classier single data set many arbiters needed baselevel classiers combining multiple classiers voting scheme used combine decisions arbiters however single mdt identifying relative areas expertise baselevel classiers much comprehensible another improvement presented study possibility use certainty condence baselevel predictions identifying classiers expertise areas original baselevel attributes data set present study also related previous work topic metalevel learning 18 introduced inductive logic programming 16 ilp framework learning relation data set characteristics performance dierent base level classiers expressive nonpropositional formulation used represent metalevel examples data set characteristics eg properties individual attributes induced metalevel concepts also nonpropositional mdt leaves expressive odt leaves language mdts still much less expressive language logic programs used ilp 7 conclusions work presented new technique combining classiers based meta decision trees mdts mdts make language decision trees suitable combining classiers select appropriate baselevel classier given example leaf mdt represents part data set relative area expertise baselevel classier leaf relative areas expertise identied basis values original baselevel attributes bla data set also basis properties class probability distributions cdp predicted baselevel classiers latter ect certainty condence class value predictions individual baselevel classiers extensive empirical evaluation shows mdts induced cdps perform much better much concise mdts induced blas due extended expressiveness mdt leaves also outperform ordinary decision trees odts terms accuracy conciseness mdts usually small easily interpreted regard step towards comprehensible model combining clas siers explicitly identifying relative areas expertise contrast existing work uses nonsymbolic learning methods eg neural networks combine classiers 14 mdts use diversity baselevel classiers better voting outperform voting schemes terms accuracy especially domains high diversity errors made baselevel classiers mdts also perform slightly better scann method combining classiers selectbest method simply takes best single classier finally mdts induced cdps perform better boosting bagging decision trees thus competitive state art methods learning ensembles mdts built using cdps domain independent principle transferable across domains x set baselevel learning algorithms sense mdt built one data set used data set since uses set attributes several potential benets domain independence mdts first machine learning experts use mdts domain independent analysis relative areas expertise dierent baselevel classiers without knowledge particular domain use furthermore mdt induced one data set used combining classiers induced set baselevel learning algorithms data sets finally mdts induced using data sets contain examples originating dierent domains exploring options already gives us topics work combining data dierent domains learning mdts especially interesting avenue work would bring together present study metalevel learning work selecting appropriate classiers given domain 4 case attributes describing individual data set properties added class distribution properties metalevel learning data set preliminary investigations along lines already made 19 several obvious directions work ordinary decision trees already known postpruning gives better results prepruning preliminary experiments show prepruning degrades classication accuracy mdts thus one priorities work development postpruning method meta decision trees implementation mlc45 interesting aspect work use classdistribution properties metalevel learning work combining classiers uses predicted classes corresponding probability distributions would interesting use learning algorithms neural networks bayesian classication scann combine classiers based probability distributions returned comparison combining clas siers using class predictions vs class predictions along class probability distributions would also worthwhile consistency meta decision trees common sense classiers combination knowledge brie discussed section 3 opens another question research process inducing metalevel classiers biased produce metalevel classiers consistent existing knowledge achieved using strong language bias within mlc45 probably easily within framework meta decision rules rule templates could used acknowledgments work reported supported part slovenian ministry education science sport eufunded project data mining decision support business competitiveness european virtual enterprise ist199911495 thank joao gama many insightful inspirational discussions combining multiple classiers many thanks marko bohanec thomas dietterich nada lavrac three anonymous reviewers comments earlier versions manuscript r explaining degree error reduction due combining multiple decision trees reduction learning multiple descriptions uci repository machine learning databases analysis results bagging predictors accuracy metalearning scalable data mining rule induction cn2 recent improvements experiments new boosting algorithm combining classi discriminant trees linearbayes classi er integrating multiple classi using correspondence analysis combine classi exploiting multiple existing models learning algorithms learning logical de study distancebased machine learning algorithms data mining practical machine learning tools techniques java implementations stacked generalization tr ctr michael gamon sentiment classification customer feedback data noisy data large feature vectors role linguistic analysis proceedings 20th international conference computational linguistics p841es august 2327 2004 geneva switzerland saso deroski bernard enko combining classifiers stacking better selecting best one machine learning v54 n3 p255273 march 2004 efstathios stamatatos gerhard widmer automatic identification music performers learning ensembles artificial intelligence v165 n1 p3756 june 2005 christophe giraudcarrier ricardo vilalta pavel brazdil introduction special issue metalearning machine learning v54 n3 p187193 march 2004 joo gama functional trees machine learning v55 n3 p219250 june 2004 pavel b brazdil carlos soares joaquim pinto da costa ranking learning algorithms using ibl metalearning accuracy time results machine learning v50 n3 p251277 march nicols garcapedrajas domingo ortizboyer cooperative constructive method neural networks pattern recognition pattern recognition v40 n1 p8098 january 2007 b kotsiantis zaharakis p e pintelas machine learning review classification combining techniques artificial intelligence review v26 n3 p159190 november 2006