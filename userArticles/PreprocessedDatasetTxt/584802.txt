topicoriented collaborative crawling major concern implementation distributed web crawler choice strategy partitioning web among nodes system goal selecting strategy minimize overlap activities individual nodes propose topicoriented approach web partitioned general subject areas crawler assigned examine design alternatives topicoriented distributed crawler including creation web page classifier use context approach compared experimentally hashbased partitioning crawler assignments determined hash functions computed urls page contents experimental evaluation demonstrates feasibility approach addressing issues communication overhead duplicate content detection page quality assessment b introduction crawler program gathers resources web web crawlers widely used gather pages indexing web search engines 12 23 may also used gather information web data mining 16 20 question answering 14 28 locating pages specific content 1 9 crawler operates maintaining pending queue urls crawler intends visit stage permission make digital hard copies part work personal classroom use granted without fee provided copies made distributed profit commercial advantage copies bear notice full citation first page copy otherwise republish post servers redistribute lists requires prior specific permission andor fee cikm02 november 49 2002 mclean virginia usa crawling process url removed pending queue corresponding page retrieved urls extracted page urls inserted back pending queue future processing performance crawlers often use asynchronous io allow multiple pages downloaded simultaneously 46 structured multithreaded programs thread executing basic steps crawling process concurrently others 23 executing crawler threads parallel multiprocessor distributed system improves performance 4 10 23 implementation distributed system potential allowing widescale geographical distribution crawling nodes minimizing competition local network bandwidth distributed crawler may structured group n local crawlers single local crawler running node nnode distributed sys tem local crawler may multithreaded pro gram maintaining pending queue data structures shared locally executing threads local crawler may even parallel program executing cluster workstations centralized control global pending queue paper use term distributed crawler reference systems data structures system control fully distributed local crawlers cannot operate entirely independently collaboration necessary avoid duplicated eort several respects local crawlers must collaborate reduce eliminate number resources visited one local crawler extreme case collaboration local crawlers might execute identical crawls visiting resources order overall distributed crawler must partition web local crawlers focuses subset web resources targeted crawl collaboration also needed identify deal duplicated content 5 13 23 often multiple urls used reference site moneycnncom cnnfncom cnnfncnncom cases large sets interrelated pages encountered repeatedly crawl crawler avoid visiting copy entirety example many copies sun java jdk documentation found web results crawl used web search system user might presented authoritative copy resource copy java documentation sun web site similarly webbased question answering 14 depends independence assumptions duplicated content validates recognize duplicated content local crawler might compute hash function contents web page 13 23 distributed crawler sucient information must exchanged local crawlers allow duplicates handled correctly collaboration may necessary eectively implement url ordering algorithms algorithms attempt order urls pending queue desired pages retrieved first resulting order might reflect expected quality pages need refresh local copy important pages whose contents known change rapidly cho et al 12 compare several approaches ordering urls within pending queue introduce ordering based pagerank measure 4 measure pages expected quality compare breadthfirst ordering random ordering ordering based simple backlink count reported experiments demonstrate pagerank ordering likely place important pages earlier pending queue given page p pagerank rp may determined pagerank pages t1 link damping factor whose value usually range 08 09 c outdegree total number pages links given set pages pagerank values may computed assigning page initial value 1 iteratively applying formula context crawler pagerank values may estimated urls pending queue backlink information provided pages already retrieved dependence backlink information local crawlers may need exchange information order accurately estimate pagerank values within local pending queues 10 ideally local crawlers would operate nearly indepen dently despite need collaboration amount data exchanged local crawlers mini mized preserving network bandwidth actual download targeted web resources scalability total amount data sent received local crawler small constant factor larger amount received data actually associated crawlers partition synchronization local crawlers operation one local crawler may delayed need communicate another local crawler avoided avoiding synchronization especially important local crawlers geographically distributed network node failures may disrupt communication finally desirable output local crawler usable independent subcollection example crawl generated use distributed appli cation distributed search engine may possible transfer data directly source nodes distributed crawler destination nodes distributed application without ever centralizing data output local crawler used independent subcollection exhibit internal cohesion link graph containing densely connected components allowing pagerank quality metrics accurately estimated 2 4 27 paper present x4 topicoriented distributed crawling system x4 web partitioned content local crawler assigned broad content category target crawl pages downloaded pretrained classifier applied page determines unique content category local crawler operates independently unless encounters boundary page page associated assigned category boundary pages queued transfer associated local crawlers remote nodes boundary pages arrive remote crawlers treated directly downloaded local crawler next section paper provides review related work section 3 discusses approach collaborative crawling view primary alternative topicoriented approach alternative web partitioned hashing urls page contents sections 4 5 examine issues design x4 crawler section 6 x4 evaluated experimentally including comparison hashbased partitioning 2 related research comprehensive study web crawlers design chos 2001 stanford phd thesis 10 par ticular chapter 3 thesis along subsequent paper 11 represents study aware attempts map explore full design space parallel distributed crawlers sharing many goals work addresses issues communication band width page quality division work local crawlers part work cho proposes evaluates hashbased approach similar discussed next section suggests occasional exchange link information nodes improve accuracy locally computed page quality estimates chos thesis directly address issue duplicate content detection distributed context apart chos work designs parallel crawlers implement form global control pending queue related data structures maintained centrally actual download pages taking place worker nodes 4 6 one example webfountain crawler 20 based clusterofworkstations architecture software consists three major components ants duplicate detectors single system controller controller manages system whole maintaining global data structures monitoring system performance ants responsible actual retrieval web resources controller assigns site ant crawling retrieves urls site duplicate detectors recognize identical nearidentical content major feature webfountain crawler maintenance todate copies page contents identifying pages change frequently reloading needed technical details crawlers used commercial web search services naturally regarded trade secrets published details structure implementation one significant exception merca tor crawler used altavista search service replacing older scooter crawler 1 heydon najork 23 24 describe detail problems associated creating commercialquality web crawler solutions used address problems mercator written java mercator achieves scalability extensibility largely careful software engineering research focused crawlers closely connected work described present paper essence focused crawlers attempt order pending queue pages concerning specific topic placed earlier queue goal creating specialized collection target topic chakrabarti et al 9 introduce notion focused crawler experimentally evaluate implementation concept using set fairly narrow topics cycling mutual funds hivaids order determine next url access implementation uses hypertext classifier 8 determines probability page relevant topic distiller identifies hub pages pointing many topicrelated pages 27 mukherjea 33 presents wtms system gathering analyzing collections related web pages describes focused crawler forms part system wtms crawler uses vector space similarity measure compare downloaded pages target vector representing desired topic urls pages higher similarity scores placer earlier pending queue mccallum et al 30 diligenti et al 18 recognize target pages focused crawl necessarily link directly one another describe focused crawlers learn identify apparently otopic pages reliably lead ontopic pages menczer et al 31 consider problem evaluating comparing eectiveness strategies used focused crawlers intelligent crawler proposed aggarwal et al 1 generalizes idea focused crawler encompassing much prior research area work assumes existence predicate determines membership target group starting arbitrary point crawler adaptively learns linkage structures web lead target pages considering combination features including page content patterns matched url ratios linking sibling pages also satisfy predicate 3 hashbased collaboration one approach implementing distributed crawler partitions web computing hash functions urls page contents local crawler extracts url retrieved page representation first normalized converting absolute url necessary translating escape sequences ascii values hash function computed normalized url assigns one n local crawlers assigned local crawler located remote node url transferred node url present correct node may added nodes pending queue similarly local crawler computes hash function contents page downloads pagecontent hash function assigns contents one local crawlers duplicate detection postprocessing takes place hashbased scheme collaboration three local crawlers may involved processing url encountered 1 local crawler encountered 2 node assigned url hash function node contents assigned page content hash function although many transfers local crawlers cannot avoided local crawlers maintain local tables urls page contents previously seen prevent unnecessary transfers url hash function need take entire url account example url hash function based hostname ensures urls given server assigned local crawler download 10 allowing load placed server better controlled similarly pagecontent hash function based normalized content allowing nearduplicates assigned node 5 13 use pagecontent hash function may force considerable data transfer local crawlers n 2 uniform hash function map retrieved pages remote nodes downloaded page mapped local node must exported assigned node nnode distributed crawler expected ratio exported data total data downloaded n 1n data exported remote nodes data imported nodes local crawler local crawlers download data rate expected ratio imported data total data downloaded also n 1n limit number nodes increased amount data transferred local crawlers twice amount data downloaded web uniform hash function contents page map local crawler independently locations pages reference number local crawlers increases probability referenced page assigned node referencing page decreases proportionally property may negative impact quality heuristics used order pending queue turn eectiveness crawl page quality heuristics use backlink information may information locally available accurately estimate ordering metrics solution proposed cho 10 local crawlers periodically exchange backlink information cho demonstrates relatively small number exchanges substantially improves local page quality estimates approach increases complexity communication local crawlers implement approach local crawler must either selectively query others backlink information transfer backlink information local crawlers important parameter collaborative crawler probability p l linked page linking page assigned node hashbased collaborative crawler using uniform hash function p l 1n one goal topicoriented collaboration reduce dependence p l number nodes n 4 topicoriented collaboration previous section assumed hash value contents specific page independent hash value pages reference section outline design topicoriented collaborative crawler uses text classifier assign pages nodes given contents web page classifier assigns page one n distinct subject categories subject category associated local crawler classifier assigns page remote node local crawler transfers assigned node processing topicoriented collaborative crawler may viewed set broadtopic focused crawlers partition web breadth subject categories depends value n n range 1020 two subject categories might business sports larger n subject categories narrower investing football hockey implementation classifier used x4 discussed next section potential advantage replacing simple pagecontent hash function text classifier increased likelihood linked page mapped node linking page example link page classified sports may likely reference another sports page business page many potential benefits topicoriented collaborative crawling derive assumption topic locality pages tend reference pages general topic 17 one immediate benefit topic locality reduction bandwidth required transfer pages one local crawler another boundary pages assigned nodes retrieved transferred addition page quality metrics depend backlink information might accurately estimated pages grouped assigned category since complete linkage information may present additional advantage topicoriented collabora tion output local crawler meaningfully regarded independent subcollection information brokers 21 22 weight output dierent search systems according expected performance dierent query types may able take advantage topic focus subcollection topicoriented approach collaborative crawling disadvantage url may independently encountered downloaded multiple crawlers approach urls hashed always retained nodes encountered url encountered two nodes node independently download page transferring common node categorization property may appear represent serious limitation topicoriented collaborative crawling observe page encountered multiple nodes pages multiple categories reference situation topic locality assumption tends minimize benefits topicoriented collaboration depend accuracy classifier actual intra inter category linkage patterns found web experimental evaluation issues using x4 crawler reported section 6 provide context evaluation first describe evaluate simple classifier used x4 5 page categorization text categorization grouping text documents set predefined categories topics often categorization based probabilities generated training set preclassified documents containing examples topic document features words phrases structural relationships extracted preclassified documents used train classifier given unclassified document trained classifier extracts features document assigns highestprobability category text categorization heavily studied subject variety machine learning information retrieval techniques applied problem including rocchio feedback 25 support vector machines 26 expectationmaximization 34 boosting 35 yang liu 37 provide recent comparison five widely used methods many techniques applied categorize web pages cases extended exploit unique properties web data much work context focus crawlers discussed section 2 chakrabarti et al 8 take advantage webs link structure improve web page categorization using iterative relaxation labeling approach pages classified using categories assigned neighboring pages part feature set dumais chen 19 take advantage large collections hierarchically organized web pages provided organizations yahoo looksmart develop hierarchical categorization technique based support vector machines select page categorization technique use x4 several attributes available techniques consid ered first categorization based document contents contents neighboring documents considered classifier pages neighborhood would retrieved crawler encountering retrieving neighborhood page classified likely increase overlap crawlers something x4 seeks avoid second training completed classifier must remain static cannot learn new data since category assigned pages contents every local crawler must third classifier ecient enough categorize pages rate downloaded crawling major portion web requires minimum download rate several mbps classifier able match rate without requiring resources crawler finally accuracy classifier percent pages correctly clas sified high possible moreover minimize number boundary pages encountered probability linked page classified category linking page topic locality also high possible many available techniques choose three study basis simplicity potential ecient implementation basic naive bayes classifier 32 classifier based rocchio relevance feedback 25 probabilistic classifier due lewis 29 data open directory project 2 odp used train test classifiers odp selfregulated organization maintained volunteer experts categorize urls hierarchical class directory similar directories provided yahoo others top level 17 categories volunteers examine contents url determine category level hierarchy contains list relevant external links list links subcategories snapshot 673mb data obtained odp purpose experiments entire url directory tree collapsed top categories two categories given special treatment regional cate adult arts business computers games health recreation reference science world figure 1 odp categories used topic classifier gory encompasses pages specific various geographical areas eliminated entirely since believe represents much separate topic alternative organi zation pages world category written languages english also ignored initial classifier selection phase final x4 classifier nonenglish pages handled separate language iden tifier 16 toplevel categories including world excluding regional listed figure 1 ultimately became target categories used x4 classifier categorization pages preprocessed first removing tags scripts numerical information remaining text tokenized terms based whitespace punc tuation terms converted lower case resulting terms treated document features required classifiers odp category 500 documents randomly selected training 500 testing html documents containing 50 words preprocessing considered candidates selection performance classifiers shown figure 2 overall naive bayes classifier achieved accuracy 607 lewis classifier 582 rocchiotfidf classifier 437 topicoriented crawling important statistic topic locality measure proportion linked pages classified category linking page test topic locality 100 web pages category randomly selected training data five random links page links page five retrieved classified results shown figure 3 overall naive bayes classifier achieved topic locality 624 lewis classifier 623 rocchiotfidf classifier 489 test speed classifiers 30000 web pages average length 7738 bytes fed naive bayes classifier achieved throughput 1950 pagessecond lewis classifier 289 pagessecond rocchiotfidf classifier 161 pagessecond although naive bayes lewis classifiers perform comparably terms categorization accuracy outperforming rocchio tfidf classifier six times greater speed naive bayes classifier recommended use x4 crawler topic categorization natural language page written must determined languagespecific classifiers may used language identification partition pages two languagespecific topics division nodes natural languages languagespecific topics depends number local crawlers required mix web resources targeted crawl purpose experiments reported paper group nonenglish pages single category world following odp organization number simple statistical language identification techniques shown good performance 3 7 15 x4 identified englishlanguage pages proportion common english words appearing train test identifier selected 500 random pages cat egory including world resulting language identifier accuracy 961 902 identifying english nonenglish pages respectively 6 experimental evaluation x4 crawler implemented extension multitext web crawler originally developed part multitext project university waterloo gather data webbased question answering 14 crawler since used number external groups sharing design goals mercator 23 mul titext crawler designed highly modular config urable used generate collections 1tb size ordinary pc crawler maintain download rates millions pages day including pre postprocessing pages core crawler provides dataflow scripting language coordinates activities independent software components perform actual operations crawl individual component responsible specific crawling task address resolution page download url extraction url filtering duplicate content handling core crawler also provides transaction support allowing crawler actions rolled back restarted system failure x4 created adding two new components multitext crawler one component topic classifier component data transfer utility data transferred remote nodes queued locally topic classifier periodically every 30 minutes data transfer utility polls remote nodes downloads queued data scp used perform actual transfer apart changes standard crawl script add calls classifier data transfer utility changes multitext crawler required experimental evaluation x4 pending queue maintained breadthfirst order one exception breadthfirst ordering would place excessive load single host defined 02 total crawler activity time period roughly one hour urls associated host removed requeued anticipated load dropped acceptable level experimental evaluation x4 used naive bayes topic classifier trained odp data described previous section preprocessing retrieved page first checked determine unclassifiable defined containing fewer 50 terms removal tags scripts preprocessing arbitrarily assigned unclassifiable pages category 0 adult unless unclassifiable page language identifier applied page assigned world category language identifier topic classifier applied assign page final category experiments local crawler associated 16 categories figure 1 local crawlers mapped onto six nodes workstation cluster assigning three local crawlers five nodes single adu art bus com gam hea hom kid new rec ref sci shp soc spt1030507090na bayes lewis rocchiotfidf category figure 2 accuracy classifiers odp data adu art bus com gam hea hom kid new rec ref sci shp soc spt1030507090na bayes lewis rocchiotfidf category topic locality figure 3 topic locality classifiers odp data category world one nodes although multiple local crawlers assigned node testing purposes local crawlers acted respects executing distinct nodes generated 137gb experimental crawl june 2001 due limitations available bandwidth general internet crawlers group limited download rate 256kbsecond local crawler limited 16kbsecond due diculty controlling download rate low speeds actual download rates varied 11 kbsecond 16 kbsecond total 89 million pages downloaded classified figure 4 plots distribution data across local crawlers showing volume downloaded data retained locally categorization volume data imported local crawlers topic locality achieved local crawler plotted figure 5 generally topic locality achieved crawl slightly lower seen odp data figure 3 relative topic locality achieved topic roughly main exception local crawler 0 adult topic locality aected arbitrary decision assign unclassifiable pages associated category comparison equivalent value hashbased collaboration p l 116 625 hashbased collaborative crawling urls hashed exchanged mapping url unique local crawler preventing multiple crawlers downloading url case topicoriented collaborative crawling since page contents exchanged topicoriented collaboration multiple crawlers download url url referenced pages one category shows log number pages retrieved exactly local crawlers number pages decreases rapidly increases pages retrieved local crawlers generally home pages major organizations products wwwnytimescom wwwmicrosoftcomie order examine properties subcollections generated local crawlers ordered pages subcollection using pagerank algorithm permit direct comparison hashbased collaboration redistributed pages 16 dierent subcollections using pagecontent hash function computed pagerank ordering finally gathered pages single collection computed global pagerank ordering figure 7 compare pagerank ordering computed subcollection global pagerank ordering computed combined collection subcol lection figure reports kendall rank correlation pagerank ordering computed subcol ported category size figure 4 distribution crawled data lection pagerank ordering pages computed global collection higher correlation coe cient indicates accurate local estimate global pagerank ordering might expected use uniform hash function coecients hashbased subcollections nearly identical cases coecients topicoriented subcollections greater co ecients hashbased subcollections 7 conclusions future work paper propose concept topicoriented collaborative crawling method implementing generalpurpose distributed web crawler demonstrate feasibility implementation experimental evalu ation contrast url hostbased hashing approach evaluated cho 1011 approach allows duplicate page content recognized generates subcollections pages related topic rather location x4 could extended improved several ways current implementation pending queues maintained breadthfirst order instead focused crawling technique might used order pending queues placing urls likely reference ontopic resources closer front queue technique could substantially increase proportion ontopic pages retrieved decrease number transfers local crawlers x4 transfers contents boundary page node retrieves node classifier assigns design decision taken consequence desire map contents page uniquely local crawler order facilitate duplicate detection postprocessing alternative design would transfer urls links appearing boundary pages contents boundary pages would transferred disadvantage approach contents url uniquely associated local crawler every local crawler encounters url boundary page retain copy contents advantage approach communication overhead local crawlers reduced communication local crawlers transfer urls extracted boundary pages retention boundary page contents multiple nodes may advantages topic locality implies pages likely related topics local crawlers encountered high backlink count associated boundary page encountered many crawlers implies high quality current work primary interest web page categorization text categorization methods could explored use x4 techniques feature selection 36 might used improve eciency accuracy problems associated incremental crawling dynamically changing content considered examined future work evaluation based relatively small crawl 137gb thorough evaluation based multiterabyte crawl might reveal issues obvious current experi ments finally x4 tested ability scale larger number nodes correspondingly larger number categories 8 r intelligent crawling world wide web arbitrary predicates authority language trees zipping anatomy largescale hypertextual web search engine crawling towards eternity enhanced hypertext categorization using hyperlinks martin van den burg crawling web discovery maintenance largescale web data parallel crawling finding replicated web collections exploiting redundancy question answering autonomous learning construct knowledge bases world wide web topical locality web focused crawling using context graphs hierarchical classification web content adaptive model optimizing performance incremental web crawler intelligent fusion multiple methods information server selection mercator scalable performance limitations java core libraries probabilistic analysis rocchio algorithm tfidf text classification statistical learning model text classification support vector machines authoritative sources hyperlinked environment scaling question answering web evaluation phrasal clustered representations text categorization task building domainspecific search engines machine learning techniques evaluating topicdriven web crawlers machine learning wtms system collecting analyzing topicspecific web information text classification labeled unlabeled documents using em boosting rocchio applied text filtering fast categorisation large document collections tr evaluation phrasal clustered representations text categorization task enhanced hypertext categorization using hyperlinks syntactic clustering web boosting rocchio applied text filtering methods information server selection anatomy largescale hypertextual web search engine efficient crawling url ordering performance limitations java core libraries reexamination text categorization methods focused crawling authoritative sources hyperlinked environment finding replicated web collections hierarchical classification web content topical locality web myampersandldquoauthoritymyampersandrdquo mean quality predicting expert quality ratings web documents text classification labeled unlabeled documents using em learning construct knowledge bases world wide web intelligent crawling world wide web arbitrary predicates adaptive model optimizing performance incremental web crawler scaling question answering web statistical learning learning model text classification support vector machines evaluating topicdriven web crawlers exploiting redundancy question answering machine learning mercator probabilistic analysis rocchio algorithm tfidf text categorization focused crawling using context graphs crawling web ctr antonio badia tulay muezzinoglu olfa nasraoui focused crawling experiences real world project proceedings 15th international conference world wide web may 2326 2006 edinburgh scotland jos exposto joaquim macedo antnio pina albano alves jos rufino geographical partition distributed web crawling proceedings 2005 workshop geographic information retrieval november 0404 2005 bremen germany weizheng gao hyun chul lee yingbo miao geographically focused collaborative crawling proceedings 15th international conference world wide web may 2326 2006 edinburgh scotland