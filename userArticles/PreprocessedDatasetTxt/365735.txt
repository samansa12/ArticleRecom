framework symmetric band reduction develop algorithmic framework reducing bandwidth symmetric matrices via orthogonal similarity transformations framework includes reduction full matrices banded tridiagonal form reduction banded matrices narrower banded tridiagonal form possibly multiple steps framework leads algorithms require fewer floatingpoint operations standard algorithms eigenvalues required addition allows spacetime tradeoffs enables increases use blocked transformations b introduction reduction tridiagonal form major step eigenvalue computations symmetric matrices matrix full conventional householder tridiagonal ization approach eg golub van loan 1989 block variant thereof dongarra et al 1989 usually considered method choice hovever banded matrices approach optimal semibandwidth b number outmost nonzero offdiagonal small compared matrix dimension n since matrix reduced completely filled steps well known algorithms rutishauser 1963 schwarz work supported advanced research projects agency contracts dm28e04120 p95006 bischof also received support mathematical information computational sciences division subprogram office computational technology research us department energy contract w31109eng38 lang also received support deutsche forschungsgemeinschaft geschaftszeichen fr 75561 la 73421 work partly performed x sun postdoctoral associate mathematics computer science division argonne national laboratory argonne preprint anlmcsp5860496 submitted acm trans math software x figure 1 rutishausers tridiagonalization householder transformations 1968 called rsalgorithms rest paper economical standard approach b n algorithms elements annihilated one time givens rotations rutishausers algorithm annihilates elements diagonals whereas schwarzs algorithm proceeds columns givens rotation generates fillin element outside current band fillin chased sequence givens rotations fillin introduced rutishauser 1963 also suggested another band reduction scheme based householder transformations annihilates elements current column instead one rutishauser used analogous scheme chase triangular bulge generated reduction sequence qr factorizations shown figure 1 however significant work involved chasing triangular bulges algorithm competitive rotationbased rsalgorithms schwarz algorithm basis band reduction implementations eispack smith et al 1976 garbow et al 1977 vectorized along diagonal kaufman 1984 variant basis band reduction algorithm lapack anderson et al 1995 rsalgorithms require storage one extra subdiagonal rutishausers householder approach requires storage subdiagonals assess storage requirements various algorithms introduce concept working semibandwidth working semibandwidth algorithm symmetric band matrix number subsuperdiagonals accessed reduction instance working semibandwidth rsalgorithms rutishausers second algorithm householder transformations algorithmic approaches reduction step two parts annihilation one several elements bulge chasing restore banded form either way bulk computation spent bulge chasing tridiagonaliza tion algorithm described murata horikoshi 1975 parallel comput ers lang 1993 called mhlalgorithm following improves bulge chasing strategy employs householder transformations eliminate subdiagonal entries current column instead chasing whole triangular bulge reappear next step chases first column bulges columns removed would increase working semibandwidth next step working semibandwidth algorithm also 2b gamma 1 leaving rest bulges place algorithm requires roughly number floatingpoint operations flops rsalgorithms latter implemented givens rotations 50 flops rsalgorithms based fast givens rotations using householder transformations considerably improves data locality compared rota tionbased algorithms hand rsalgorithms require less storage may still preferable storage tight paper generalize ideas behind rsalgorithms mhl algorithm develop band reduction algorithm eliminates subdiagonals symmetric banded matrix semibandwidth b b fashion akin mhl tridiagonalization algorithm like rutishauser algorithm band reduction algorithm repeatedly used reduced matrix tridiagonal mhlalgorithm used reduction step results rutishauser algorithm however need chosen way deed exploiting freedom choosing leads class algorithms banded reduction tridiagonalization favorable computational properties particular derive algorithms 1 minimum algorithmic complexity 2 minimum algorithmic complexity subject limited storage 3 enhanced scope employing level 3 blas kernels blocked orthogonal reductions setting results nonblocked householder tridiago nalization full matrices alternatively first reduce matrix banded form tridiagonalize resulting band matrix latter approach significantly improves data locality almost computations done level 3 blas contrast blocked tridiagonalization garra et al 1989 half computation spent matrixvector products paper extends work bischof sun 1992 paper organized follows next section introduce framework band reduction symmetric matrices section 3 show several known tridiagonalization algorithms interpreted instances frame work derive new algorithms optimal respect either computational cost space complexity section 4 discuss several techniques blocking update orthogonal matrix u required eigenvector computations section 5 present experimental results section 6 sums findings 2 framework band reduction section describe framework band reduction symmetric matrices basic idea repeatedly remove sets offdiagonals therefore first present algorithm peeling diagonals banded matrix suppose nbyn symmetric band matrix semibandwidth b n reduced band matrix semibandwidth b want eliminate outermost b nonzero subsuperdiagonals pre sym post post pre sym pre post sym figure 2 annihilation chasing first sweep band reduction algorithm qr stands performing qr decomposition pre post denote pre postmultiplication q q resp sym indicates symmetric update pre post last picture shows block partitioning second sweep symmetry suffices access either upper lower triangle matrix focus latter case algorithm based annihilate chase strategy similar rs mhlalgorithms tridiagonalization mhlalgorithm householder transformations used annihilate unwanted elements case b 1 able aggregate n b b transformations wy compact wy representation bischof van loan 1987 schreiber van loan 1989 thus data locality improved first outmost subdiagonals annihilated first n b columns step done qr decomposition h theta n b upper trapezoidal block shown first picture figure 2 wy representation transformation matrix generated complete similarity transformation must apply block transform left right requires applying q left h theta gamma n b block pre sides h theta h lower triangular block sym right b theta h block post post transformation generates fillins diagonals band first n b columns fillin removed another qr decomposition second picture figure 2 pre sym post complete similarity transformation process repeated newly generated fillin step amounts chasing n b columns fillin along diagonal b diagonal elements pushed matrix start second annihilate chase sweep sweep starts matrix following properties remainder matrix ie current trailing matrix block tridiag onal diagonal blocks last one order b last one order b see last picture figure 2 every subdiagonal block upper triangular first matrix banded semibandwidth b every similarity transformation within sweep maintains working semibandwidth b involves number h rows columns restores form described one subdiagonal block destroying next subdiagonal block arrive following algorithm algorithm 1 onestep band reduction bandr1n b n b input n theta n symmetric matrix semibandwidth b n number subdiagonals eliminated block size n b 1 output n theta n symmetric matrix semibandwidth b gamma qr perform qr decomposition block b replace b pre replace block b sym replace block b post replace block b end working semibandwidth algorithm bd given onestep band reduction algorithm derive framework band reduction straightforward fashion peeling subdiagonals chunks algorithm 2 multistep band reduction bandrn b fd input n theta n symmetric matrix semibandwidth b sequence positive integers fd g k b sequence fn block sizes n output n theta n symmetric matrix semibandwidth b gamma call bandr1 n end working semibandwidth multistep algorithm b j b 1 satisfy also class systolic array algorithms use givens rotations remove several outer diagonals time bojanczyk brent 1987 ipsen 1984 schreiber 1990 chosen large b 2 bojanczyk brent 1987 order increase parallel scope per systolic operation hence minimize total number systolic operations 3 instances framework section discuss several instances algorithm 2 including non blocked householder tridiagonalization rutishausers algorithm mhl algorithm tridiagonalizing symmetric band matrices addition multistep framework allows new tridiagonalization methods featuring lower flops count andor better data locality 31 onestep tridiagonalization full symmetric matrices full symmetric matrix tridiagonalized bandrn 1g note 1 qr decomposition sym step algorithm 1 reduce determining applying suitable householder transformation pre post steps vanish thus arrive nonblocked standard householder tridiagonalization 32 twostep tridiagonalization full symmetric matrices another way tridiagonalize full symmetric matrix twostep sequence intermediate semibandwidth n 1 b first reduce banded form tridiagonalize resulting banded matrix contrast blocked householder tridiagonalization dongarra et al 1989 one half approximately 43n 3 flops reduction confined matrixvector products almost operations reduction banded form done within level 3 blas b n first reduction constitutes vast majority flops tridiagonalizing banded matrix requires roughly 6bn 2 flops done level 2 blas therefore twostep tridiagonalization may superior machines distinct memory hierarchy see table section 51 33 onestep tridiagonalization symmetric band matrices full matrices simplest way tridiagonalize matrix semibandwidth b onestep sequence bandrn b fd 1g qr decomposition sym steps reduce determining applying single householder transformations pre post steps vanish onestep sequence equivalent mhlalgorithm 34 tridiagonalization peeling single diagonals sequence bandrn b fd tridiagonalizes banded matrix repeatedly peeling single diagonals corresponds rutishausers algorithm except rotations replaced length2 householder transformations figure 3 nonzero structure matrices w 35 optimal tridiagonalization algorithms band matrices following derive tridiagonalization algorithms minimum flops count given working semibandwidth minimize number flops tridiagonalize n theta n banded matrix semibandwidth b subject working semibandwidth 2 reduction banded matrices blocking transformations always significantly increases flops count figure 3 shows nonzero pattern result aggregating n b lengthd1 householder transformations blocked householder transformation nonzeros form parallelogram w upper trapezoidal multiplying w columns theta takes approximately n b 2d flops multiplication costs another 2n b even multiplication routine gemm able take full advantage zeros thus banded context using wy representation increases overall flops count two ways first w factors must generated second applying blocked householder transform requires 4n b d1m operations would needed n b single lengthd transformations w argument applies also compact wy representation schreiber van loan 1989 addition blocking technique requires one matrix multiplication cost additional multiplication negligible banded case contrast reduction full matrices average length householder vectors significantly exceeds n b therefore prefer standard wy representation blocking householder transformations section focuses minimizing flops count consider nonblocked reduction algorithms ie n machines distinct memory hierarchy however higher performance level 3 blas may compensate overhead introduced blocking transforma tions thus flops count blas performance taken account order minimize execution time machines clarity analy sis omitted resulting weighting various algorithmic components ideas presented easily extended general analysis nonblocked case arithmetic cost algorithm 1 flops costn b 4d band difference sequence fd g algorithm 2 therefore requires flops given limit use dynamic programming aho et al 1983 determine optimal sequence fd g cost function given 3 taking storage requirements account allow space tradeoffs best use available memory 351 storage constraints first consider problem 2 2b gamma 1 storage constraints since maximum working semibandwidth algorithm 2 2b gamma 1 however optimal sequence fd g quite different onestep sequence mhlalgorithm example 50 000by50 000 symmetric matrix semibandwidth 300 optimal sequence reduction requires 348delta10 12 floatingpoint operations working semiband width 310 contrast mhlalgorithm requires working semibandwidth 599 also note constant sequence requires flops working semibandwidth 316 another constant sequence requires working semibandwidth 332 hence constant stride sequence seems good choice optimal one practical point view saves dynamic programming overhead 352 minimum storage consider another extreme case constraint problem 1 space one subdiagonal even small b mhlalgorithm among candidates since working semibandwidth candidate dsequence case satisfy condition 1 1 sequence satisfies condition instead suggest call doublingstride sequence since bandwidth reduction size doubles round example preceding subsection rsalgorithms require doublingstride algorithm requires 390 flops row update rank1 column update symmetric rank2 update x figure 4 data area visited mhlalgorithm 353 understanding optimality rutishausers algorithm peels subdiagonal one one requires minimum storage optimal among algorithms minimum storage hand although mhlalgorithm eliminates subdiagonals one step also optimal b large following give intuitive explanation complexity scheme superior approaches first let us compare mhlalgorithm algorithm 2 sequence fd b gamma 1g bulge chasing following reduction column 1 data area accessed mhlalgorithm rank1 row andor column updating symmetric rank2 updating shown figure 4 data area accessed algorithm 2 shown figure 5 comparing different bandreduction sequences take rank1 updating basic unit computation examine total data area visited algorithm purpose rank1 updating since update applied theta n matrix requires approximately 4mn flops area involved rank1 update good measure complexity symmetric rank2 update triangular n theta n matrix expensive rank1 update full matrix size require 4n 2 flops mhlalgorithm total area visited rank1 updates two successive band reductions denoting 1b obtain ratio amhl takes minimum ae 1112 ffi therefore twostep reduction save 8 flops compared mhlalgorithm course idea applied recursively tridiagonal reduction matrix reduced semibandwidth b gamma bd bd figure 5 data area visited two steps bandr1n b bandr1n algorithm 2 x x figure 6 data area visited rutishausers algorithm elimination bulge chasing rounds three outmost elements 91 81 71 first column light medium dark grey shading resp let us consider often row column repeatedly involved reduction chasing step data area visited rutishausers algorithm annihilating 3 elements first column 3 rounds shown figure 6 see particular total area visited first b theta b block almost twice big one algorithm 1 result revisiting first row column last visited area thus comparison rutishausers mhlalgorithm algorithm interpreted balancing counteracting goals decreasing number times column revisited use householder transformations decreasing area involved updates peeling subdiagonals several chunks 36 bandwidth reduction contexts necessary fully reduce banded matrix tridiagonal form example invariant subspace decomposition approach isda eigensystem computations lederman et al 1991 spectrum matrix condensed two narrow clusters repeatedly applying function f matrix f polynomial degree 3 application f roughly triples bandwidth therefore full moderate number iterations countermeasures taken prevent situation bandwidth periodically reduced reasonable value applications f bandwidth reduction also done using algorithm 2 either one multiple reduction steps 4 aggregating transformations eigenvectors matrix required transformations algorithm 1 must also applied another matrix say n theta n matrix u banded matrices update u dominates floatingpoint complexity 2n 3 updating u versus 6bn 2 reduction therefore strive maximize use blas 3 kernels update u decrease total time section discuss several techniques updating u blocked householder transformations methods tailored cases transformations cannot blocked 41 onthefly update update easily incorporated reduction inserting lines aggregate transformation matrix u required replace u qr pre steps algorithm 1 transformation applied u soon generated applied onthefly update matrix reduced tridiagonal form ie work u done blocked householder transformations thus enabling use level 3 blas 42 backward accumulation tridiagonalization however means work cannot done blocked fashion first glance fact seems also preclude use level 3 blas update u fortunately however obstacle circumvented decoupling work u reduction let us first consider tridiagonalization full symmetric matrix lapack routine sytrd update u delayed reduction completed householder transformations aggregated blocked householder transforms arbitrary n u b applied u addition enabling use level 3 blas decoupling update reduction allows reducing flop count reverting order transformations backward accumulation complexity reasons backward accumulation technique also used reduction full banded form even onthefly update also done blocked householder transforms 43 update tridiagonalization banded matrices full matrix reduced either banded tridiagonal form householder vectors stored conveniently zeroedout portions additional vector tridiagonalization banded matrices strategy longer possible eliminating one lengthb gamma 1 column band requires nb length b householder vectors bulge chasing therefore impractical delay update u completely tridiagonalized use another technique bischof et al 1994 case denote kth householder transformation generated jth sweep algorithm 1 h j 1 eliminates first column remaining band h j 3 generated bulge chasing reduction transformations sweep must determined applied canonical order h j 3 h j depends data modified post step h j transformations known dependence longer exists since transformations one sweep involve disjoint sets u columns may applied u order see figure 7 however entirely free mix transformations different sweeps must preceded since affects columns modified two transformations sweep sweep gammapsi gammapsi gammapsi gammapsi gammapsi gammapsi gammapsi gammapsi gammapsi gammapsi gammapsi gammapsi gammapsi gammapsi gammapsi sweep oe oe oe oe oe oe oe oe oe oe oe oe oe oe oe gammapsi gammapsi gammapsi gammapsi gammapsi gammapsi gammapsi gammapsi gammapsi gammapsi gammapsi gammapsi gammapsi gammapsi gammapsi figure 7 interdependence householder transformations h j work left picture u right picture h h cannot determined applied cannot applied u h applied u make use additional freedom delay work u certain number n u b reduction sweeps completed update u done bottom applying transformations order figure 8 columns u affected transformation h j first four reduction sweeps example 6 transformations hatching pattern aggregated blocked householder transformation h j 1 transformations figure 7 would applied order h 1 6 5 h 1 1 order preserves intersweep dependence mentioned addition n b transformations h k index k aggregated blocked householder transformation figure 8 transformations contributing block transformation hatched identically similar technique also used example qr algorithm computing eigensystem symmetric tridiagonal matrix lang 1995 allows updating eigenvector matrix matrixmatrix products instead single rotations 5 experimental results numerical experiments performed single nodes ibm sp parallel computer located highperformance computing research facility mathematics computer science division argonne national laboratory single nodes intel paragon located zentralinstitut fr angewandte mathe matik forschungszentrum julich gmbh timings computations double precision matrices random entries chosen 0 1 since none algorithms sensitive actual matrix entries following timings considered representative five codes used experiments dsytrd lapack routine blocked tridiagonalization full symmetric matrices dongarra et al 1989 table timings seconds one node ibm sp onestep lapack routine dsytrd twostep reduction routines dsyrdb dsbrdt full symmetric matrices order tridiagonal form timings include update u intermediate semibandwidth twostep reduction always b onestep reduction twostep reduction total time full gamma banded banded gamma tridiagonal dsbtrd lapack routine tridiagonalizing banded matrices using kaufmans modification schwarzs algorithm schwarz 1968 kaufman 1984 called sk algorithm following improve vectorization dsyrdb blocked reduction full symmetric matrices banded form algo rithm 1 dsbrdb blocked reduction banded matrices narrower banded form algo rithm 1 dsbrdt tridiagonalization banded matrices mhlalgorithm technique section 43 updating u latter three codes described detail bischof et al 1996 programs written fortran 77 ibm sp node purposes viewed 66 mhz ibm rs6000 workstation codes compiled xlf o3 qstrict linked lessl vendorsupplied blas intel paragon node compilation done if77 mvect o4 nx blas linked lkmath data presented section demonstrate may advantageous consider multistep reductions instead conventional direct methods particular machines memory hierarchies clear priori approach superior given problem 51 tridiagonalization full matrices first compared onestep tridiagonalization routine dsytrd lapack twostep reduction routines dsyrdb dsbrdt section 32 table shows large matrices twostep reduction make better use level 3 blas direct tridiagonalization onehalf operations confined matrixvector products reduction banded form runs mflops close peak performance ibm sp node note tridiagonalization banded matrix cannot blocked therefore block size n b affects reduction banded form bad performance reduction due fact routine dsyrdb provide optimized code nonblocked case transformation matrix u required direct tridiagonalization always superior significantly lower flops count matrix size sp1 update matrix size sp1 update matrix size paragon update 200 400 600 800 1000 1200 matrix size paragon update figure 9 speedup mhlalgorithm routine dsbrdt tridiagonalizing banded matrices skalgorithm lapack routine dsbtrd without updating matrix u left right pictures resp update n u transformations aggregated block transform 52 onestep tridiagonalization banded matrices next compared skalgorithm lapack routine dsbtrd mhl algorithm dsbrdt onestep tridiagonalization algorithms band matri ces mhlalgorithm update u done using blocked householder transformations default block size n u figure 9 shows results various matrix dimensions semibandwidths machines lang 1993 already noted lapack implementation optimal except small semibandwidths reason bulk computations must done explicit fortran loops since appropriate blas routines cover therefore routine runs fortran speed whereas mhlalgorithm rely level 2 blas reduction level 3 blas update u sp node mhlalgorithm runs 45 mflops reduction 53 mflops update dsbtrd reaches 20 30 mflops respectively addition skalgorithm requires 50 flops u updated situation may different vector machines dsbtrd features vector operations higher average vector length albeit nonunit stride table ii timings seconds one node ibm sp tridiagonalizing banded symmetric matrices order 1200 intermediate semibandwidth twostep reduction onestep reduction dsbrdt 83 150 250 twostep reduction dsbrdb dsbrdt 113 136 229 53 twostep tridiagonalization banded matrices section 353 showed peeling diagonals two equal chunks requires fewer flops direct tridiagonalization mhlalgorithm table ii shows time twostep approach lower semiband width large enough contrast theoretical results however intermediate semibandwidth b2 always optimal example took 2003 seconds first reduce semibandwidth tridiagonalize matrix explanation case b work done bandwidth reduction rely blocked householder transformations whereas final tridiagonalization cannot therefore higher performance level 3 blas compensates slighly higher flops count compared b small semibandwidths b lower flops count twostep scheme outweighed lower overhead eg fewer calls blas larger subma trices onestep reduction reduction full matrices onestep tridiagonalization always superior u required 54 doublingstride tridiagonalization banded matrices mhlalgorithm clearly superior machines blas performance significantly exceeds pure fortran code may applicable storage tight therefore also compared two algorithms need one additional subdiagonal working space skalgorithm routine dsbtrd lapack doublingstride sequence section 352 multiple calls dsbrdb reduction update u one call dsbrdt results given figure 10 show doublingsequence tridiagonalization well outperform skalgorithm machines 6 conclusions introduced framework band reduction generalizes ideas underlying householder tridiagonalization full matrices rutishausers algorithm mhlalgorithm banded matrices peeling subdiagonals chunks arrived algorithms require fewer floatingpoint operations less storage also provided intuitive explanation approach eliminates subdiagonals groups lower computational complexity matrix size sp1 update matrix size sp1 update 200 400 600 800 1000 1200 matrix size paragon update 200 400 600 800 1000 1200 matrix size paragon update figure 10 speedup doublingsequence tridiagonalization routines dsbrdb dsbrdt skalgorithm lapack routine dsbtrd without updating matrix u left right pictures resp previous algorithms banded matrices eliminated subdiagonals either one one successive bandreduction sbr approach improves scope block operations particular update transformation matrix u always done blocked householder transformations also presented results show sbr approaches provide better performance either using less memory achieve almost speed achieving higher performance experience suggests hard provide rule thumb selecting parameters optimal bandreduction algorithm flops count minimized using cost function 3 actual performance implementation depends machinedependent issues floatingpoint versus memory access cost experience developing realistic performance model advanced computer architectures difficult see example bischof lacroute 1990 even simpler problems another paper bischof et al 1996 describes implementation issues publicdomain sbr toolbox enabling computational practioners experiment sbr approach problems interest r data structures algorithms parallel tridiagonalization twostep band reduction wy representation products householder matrices adaptive blocking strategy matrix factorizations sbr toolbox software successive band reduction framework band reduction tridiagonaliza tion symmetric matrices tridiagonalization symmetric matrix square array meshconnected processors block reduction matrices condensed forms eigenvalue computations matrix eigensystem routines eispack guide extension matrix computations 2nd singular value decompositions systolic arrays banded eigenvalue solvers vector machines parallel algorithm reducing symmetric banded matrices tridiagonal form using level 3 blas rotation based algorithms parallelizable eigensolver real diagonalizable matrices real eigenvalues new method tridiagonalization symmetric band matrix jacobi rotation patterns bidiagonalization symmetric tridiagonalization systolic arrays journal vlsi signal processing storageefficient wy representation products householder transformations tridiagonalization symmetric band matrix matrix eigensystem routines eispack guide 2nd ed tr wy representation products householder matrices solution large dense symmetric generalized eigenvalue problems using secondary storage storageefficient wy representation products householder transformations adaptive blocking strategy matrix factorizations parallel algorithm reducing symmetric banded matrices tridiagonal form matrix computations 3rd ed using level 3 blas rotationbased algorithms banded eigenvalue solvers vector machines band reduction algorithms revisited algorithm 807 data structures algorithms automatically tuned linear algebra software ctr daniel kressner block algorithms reordering standard generalized schur forms acm transactions mathematical software toms v32 n4 p521532 december 2006 christian h bischof bruno lang xiaobai sun algorithm 807 sbr toolboxsoftware successive band reduction acm transactions mathematical software toms v26 n4 p602616 dec 2000