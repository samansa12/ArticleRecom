efficient learning equilibrium introduce efficient learning equilibrium ele normative approach learning noncooperative settings ele learning algorithms required equilibrium addition learning algorithms must arrive desired value polynomial time deviation prescribed ele becomes irrational polynomial time prove existence ele desired value expected payoff nash equilibrium paretoele objective maximization social surplus repeated games perfect monitoring also show ele always exist imperfect monitoring case finally discuss extension results generalsum stochastic games b introduction reinforcement learning context multiagent interaction attracted attention researchers cognitive psychology experimental economics machine learning articial intelligence related elds quite time 13 6 much work uses repeated games 5 8 stochastic games 16 15 12 2 models interactions literature learning games game theory 8 mainly concerned understanding learning procedures adopted dierent agents converge end equilibrium corresponding game game may known idea show simple dynamics lead rational behavior prescribed nash equilibrium learning algorithms required satisfy rationality requirement converge adopted agents equilibrium facing uncertainty game played gametheorists adopt bayesian approach typical assumption approach exists probability distribution possible games commonknowledge notion equilibrium extended context games incomplete information treated appropriate solution concept context agents assumed rational agents adopting corresponding bayes nash equilibrium learning issue major claim gametheoretic approach line goals multiagent reinforcement learning research ai must modied first bayesian approach used model partial information line common approach theoretical computer science computational learning dealing uncertainty second descriptive motivation underlying learning research gametheory diers considerably normative motivation learning research ai dierences important ramications explain issues detail first consider bayesian model partial information date work machine learning particular work singleagent reinforcement learning taken dierent approach motivated largely work online algorithms computer science distribution assumed uncertain entities instead goal approach behavior agent complete information closely quickly indeed ai researchers adopted nonbayesian approach work learning games looking algorithms converge appropriate equilibrium game class relevant games follow suit however researches multiagent reinforcement learning choose adopt assumptions made gametheorists despite fact dierences much fundamental work learning games started descriptive motivation mind goal show people use simple heuristic rules updating behavior multiagent setting ie game eventually adopt behavior corresponds appropriate equilibrium behavior case economic models based equilibria concepts sense justied assumption agents use learning rule justied fact agents involved people ie designed similarly ai concerned descriptive models human behavior interested designing articial agents except case cooperative systems reason believe agents designed dierent designers employ learning algorithms moreover one view designers choice learning algorithm agent fundamental decision follow normative criteria indeed ai perspective choice learning algorithm basic action take game play agent designers another related point gametheorists adopting descriptive stance concerned quickly learning rule leads convergence ages evolve behavior agent designer wants agent learn quickly care agents osprings thus ai speed convergence paramount importance better align research methodology multiagent reinforcement learning ai perspective present paper nonbayesian normative approach learning games approach makes assumptions distribution possible games may played making ective setting studied machine learning ai spirit work online algorithms computer science treats choice learning algorithm game specically adopt framework repeated games view learning algorithm strategy agent repeated game strategy takes action stage based previous observations initially information identity game played given following natural requirements learning algorithms provided agents 1 individual rationality learning algorithms equilibrium irrational agent deviate learning algorithm long agents stick algorithms regardless actual game 2 eciency deviation learning algorithm single agent others stick algorithms become irrational ie lead situation deviators payo improved polynomially many stages b agents stick prescribed learning algorithms expected payo obtained agent within polynomial number steps close value could obtained nash equilibrium agents known game outset tuple learning algorithms satisfying properties given class games said ecient learning equilibrium ele notice learning algorithms satisfy desired properties every game given class despite fact actual game played initially unknown assumptions typical work machine learning borrow game theory literature criterion rational behavior multiagent systems take individual rationality associated notion equilibrium also take equilibrium actual initially unknown game benchmark success wish obtain corresponding value although initially know game played idea constitutes major conceptual contribution paper remaining sections formalize notion ecient learning equilibrium show devoid content ie prove existence ele general class games class repeated games perfect monitoring also show classes games ele exist generalize results context paretoele wish obtain maximal social surplus also discuss extension results generalsum stochastic games technically speaking results prove rely novel combination socalled folk theorems economics novel ecient algorithm punishment deviators games initially unknown ecient learning equilibrium denition section develop denition ecient learning equilibrium context twoplayer repeated games generalization nplayer repeated games immediate requires additional notation presented extension stochastic games appears section 6 game model multiagent interaction game set players chooses action perform given set actions result players combined choices outcome obtained described numerically form payo vector ie vector values one players common description twoplayer game bimatrix called game strategic form rows matrix correspond player 1s actions columns correspond player 2s actions entry row column j game matrix contains rewards obtained players player 1 plays th action player 2 plays j th action make simplifying assumption size action set players identical extension sets dierent sizes trivial repeated game rg players play given game g repeatedly view repeated game respect game g consisting innite number iterations players select action game g playing iteration players receive appropriate payos dictated games matrix move next iteration ease exposition normalize players payos game g nonnegative reals 0 positive constant r max denote interval possible payos perfect monitoring setting set possible histories length set possible histories h union sets possible histories 0 empty history namely history time consists history actions carried far corresponding payos obtained players hence perfect monitoring setting player observe actions selected payos obtained past know game matrix start imperfect monitoring setup player observe following performance action payo obtained action selected player player cannot observe players payo even constrained setting strict imperfect monitoring player observe action payo alone denitions possible histories agent imperfect monitoring settings follow naturally given rg policy player mapping h set possible histories set possible probability distributions hence policy determines probability choosing particular action possible history notice learning algorithm viewed instance policy dene value player 1 policy prole policy player 1 policy player 2 using expected average reward criterion follows given rg natural number denote expected iterations undiscounted average reward player 1 players follow policy prole u 1 denition player 2 similar dene u 2 policy prole learning equilibrium every game matrix dened set actions possible payos rst requirement learning algorithms treated strategies order individually rational best response one another addition rapidly obtain desired value identity desired value may parameter take natural candidate nash equilibrium game another appealing alternative discussed later assume consider games k actions g every repeated game nash equilibrium oneshot game associated denote nv ng expected payo obtained agent equilibrium policy prole ecient learning equilibriumele every 0 0 1 exists 0 polynomial 1 k every game matrix g corresponding rg u nash equilibrium ng player 1 deviates 0 iteration l probability failure similarly player 2 notice deviation considered irrational increase expected payo spirit equilibrium game theory done order cover case expected payo nash equilibrium equals probabilistic maximin value 1 cases denition replaced one requires deviation lead decreased value obtaining similar results chosen order remain consistent gametheoretic literature equilibrium stochastic contexts notice also deviation considered irrational detrimental eect deviating players average reward manifest near future exponentially far future captures insight normative approach learning noncooperative setting assume initially game unknown agents learning algorithms rapidly lead values players would obtained nash equilibrium known game moreover mentioned earlier learning algorithms equilibrium 1 probabilistic maximin value player 1 dened set policies players 1 2 respectively denition player 2 similar 3 ecient learning equilibrium existence denition ele lesser interest cannot provide interesting examples ele instances section prove following constructive result theorem 1 exists ele perfect monitoring setting describe concrete algorithm property said earlier based combination socalled folk theorems economics novel ecient punishment mechanism ensures eciency approach folk theorems eg see 9 extended discussion 10 basic idea strategy prole leads payos greater equal security level probabilistic maximin values agents guarantee obtained directing agents use prescribed strategies telling agent punish agent turns deviate behavior punishment remains threat followed equilibrium result desired strategy prole executed order use idea setting need however use technique punishing without initially knowing payo matrix moreover need devise ecient punishment procedure setting recall consider repeated game iteration g played follows often use term agent denote player using algorithm question term adversary denote player players set possible actions consider following algorithm termed ele algorithm ele algorithm player 1 performs action one time k times parallel player 2 performs sequence actions times players behave according nash equilibrium revealed game computed players behave according corresponding strategies point several nash equilibria exist one selected using shared selection algorithm one players refer adversary deviates player refer agent acts follows agent replaces payos g complements r max adversary payos hence agent treat game constantsum game aim minimize adversarys payo notice payos might unknown use g refer modied game describe agent go minimizing adversarys payo initialize construct following model 0 repeated game game g replaced game g 0 entries game matrix assigned rewards r addition associate boolean valued variable jointaction fassumedknowng variable initialized value assumed repeat compute act compute optimal probabilistic maximin g 0 execute observe update following joint action follows let action agent performed let 0 adversarys action 0 performed rst time update reward associated 0 g 0 observed mark known 1 ele algorithm adopted players indeed ele much proof theorem nontrivial rests showing agents ability punish adversary quickly details presented appendix 4 imperfect monitoring ele algorithm previous section uses agents ability view adversarys actions payos natural question whether ability required existence ele section show general perfect monitoring required special classes games ele exists imperfect monitoring start general case theorem 2 ele always exist imperfect monitoring setting proof order see consider following games 1 g1 2 g2 payos obtained joint action g1 g2 identical player 1 dierent player 2 equilibrium g1 players play second action leading 1500 equilibrium g2 players play rst action leading 69 unique equilibria since obtained removal strictly dominated strategies assume ele exists look corresponding policies players equilibrium notice order ele must visit entry 69 times game g2 visit entry 1500 times game otherwise player 1 resp player 2 obtain high enough value g2 resp g1 since payos g2 resp g1 lower given rational player 2 deviate pretend game always g1 behave according suggested equilibrium policy tells case since game might actually g1 player 1 tell dierence player 2 able lead playing second action players times also game g2 increasing payo 9 10 contradicting ele approach bayesian exclude possibility agent knows participating game particular class thus may classes repeated games ele exists particular consider class repeated commoninterest games repeated games underlying game g commoninterest game ie game players always receive identical payos setting denition imperfect perfect monitoring denote setting player knows payo knows adversarys payo well thus examine case strict imperfect monitoring recall setting player knows action payo theorem 3 exists ele class commoninterest games strict imperfect monitoring proof idea quite simple surprisingly proposed complex less ecient approaches proposed require knowledge number actions available agent polynomial bound algorithm works follows first agents go series random play suciently many times ensure probability jointactions played greater 1 phase agent maintains information best payo obtained far action used payo rst obtained exploration phase agent plays best action repeatedly learning equilibria average reward learning strategy leads maximal average reward every agent thus agent motivation deviate ele polynomial number steps required attain average reward see deviation immediately reduce average reward agent need polynomial number steps approximately obtain maximal average reward need ok 4 log k 2 steps random play ensure jointactions played probability least 1 follows following given large enough k get probability k 2 trials agents play previously unplayed joint action approximated e hence get ok 2 log k 2 probability learn outcome associated new joint action approximated repeating process k 2 times get desired result 5 pareto ele previous sections dealt ele perfect imperfect monitoring settings cases interested learning procedure enable agents obtain expected payos ones would obtained nash equilibrium known game ambitious objective following let p b denote payo player game question player 1 plays player 2 plays b say pair actions b economically ecient total payo agents maximized easy see agents choose action g general way guarantee agents behave economically ecient manner due fact may case although b economically ecient behavior performing resp b agent 1 resp 2 irrational may lower probabilistic maximin value agent 1 resp 2 guarantee classical approach economics dealing economic ineciency introducing side monetary payments formally part strategy agent function agent instructed pay certain amount money agent part strategy 2 agents reward p paid c c positive negative zero utility assumed type utility function termed quasi linear sum agents monetary payments always 0 result agents turn using strategies maximize u 1 u 2 also economically ecient dene notion pareto ele pareto ele similar nash ele aim agents behavior economically ecient therefore two distinctive aspects pareto ele 1 require agents able get close ecient outcome 2 allow side payments part agents behavior 2 denition perfect monitoring case denition imperfect monitoring case similar suppose considering games k actions every repeated game let economically ecient joint action oneshot game associated denote pv payo obtained agent joint action policy prole also allows side payments pareto ecient learning equilibrium every 0 0 1 exists 0 polynomial k every game matrix g dened actions corresponding rg u 1 player 1 deviates 0 iteration l u 1 probability failure similarly player 2 theorem 4 exists paretoele perfect monitoring setting proof consider following algorithm denes policies side payments agents player 1 performs k iterations action parallel player 2 performs sequence actions times game known agents compute probabilistic maximin values agent 1 agent 2 denote probabilistic maximin value agent payo gets economically ecient solution e without loss choose r paid rby player 1 ecient solution played players total payo least high probabilistic maximin easy see examining two cases e 2 agents adopt ecient behavior sidepayments following iterations several economically ecient behaviors exist predetermined selection algorithm used case one players adversary deviates either exploration stage following state player agent punish case nash ele play payos game complements r max adversary payos proof follows steps proof existence ele case imperfect monitoring result respect nashele hold theorem 5 pareto ele always exist imperfect monitoring setting 6 stochastic games stochastic games provide general model repeated multiagent interactions stochastic game players may one nitely many states state associated game strategic form joint action state determines payos also determine stochastically identity next state agents reach formally let set actions available agents state game associated associates payo p agent j joint action b addition every 2 probability next state j joint action b denoted p multiple games policy agent associates possibly mixed action every state potentially payment agent policy function history states agent visited payos observed throughout section assume perfect monitoring setting since impossibility result imperfect monitoring repeated games immediately rules existence ele general context stochastic games stochastic games provide realistic also technically challenging setting first let us try understand issues involved rst obstacle face lack general results existence nash equilibrium averagereward stochastic games thus restrict attention case paretoele conceptually required generalization straightforward learning algorithm quickly lead economically ecient policy agents ie policy maximizes average sum rewards deviations quickly lead lower reward however case repeated games equated quick polynomial size game approximation parameters situation stochastic games complicated parameter typically used assess speed convergence learning algorithm stochastic games return mixing time 14 3 intuitively return mixing time policy expected time would take agent uses policy converge value close value policy ideally would like learning algorithm attain optimal value time polynomial return mixing time optimal policy formally assume xed stochastic game let policy prole denote step average reward policy prole agent starting state return mixing time minimal states us thus policy prole executed steps longer agents expected average sum rewards close longterm average sum rewards let policy prole maximizes min us let mix return mixing time denition pareto ecient learning equilibrium stochastic games identical repeated games except must polynomial mix well note game irreducible ie xed policy prole induced markov chain ergodic us depend show following theorem 6 following assumptions paretoele stochastic games exists 1 agents perfect monitoring 2 mix known proof intuitive idea behind algorithm identical case repeated games elaborate new issues first agents run algorithm nding policy prole maximize u next run algorithm nding best accomplish ie assuming agent trying minimize average payo point run policy prole adjusted appropriate side payments agent receives best accomplish much like case repeated games point agent deviates agent plays goal minimize agents average reward first note learning algorithm paretooptimal longterm average sum rewards algorithm close optimal average sum rewards desired agent incentive deviate stage sidepayment structure guarantees attain least value could attain show algorithm paretoele also need show value attained eciently punishment performed eciently resorting recent results ecient learning xedsum stochastic games commoninterest stochastic games first compute policies maximizes u use algorithm described 4 refer reader details need note algorithm learns required policy prole polynomial time next compute values agent attain use rmax 3 rmax appropriate learning xedsum game first xed sum game rewards based agent 1s rewards respect xedsum game rewards based agent 2s rewards note given value 0 input rmax learn 0 step policy time polynomial 0 game parameters policy optimal among policies mix time 0 shall take average reward policy used compute side payments structure case repeated games case average reward policy prole suitably modied include side payments lower value agent receive thus agent deviate know within mix steps attain lower average reward ie punishment carried eciently finally note standard though imperfect technique removing knowledge mix simply guess progressively higher values mix refer reader 3 implications approach previous work learning games ts one following two paradigms 1 study learning rules lead nash equilibrium solution concept game 8 2 study learning rules predict human behavior noncooperative inter actions ones modeled repeated games 6 approach taken 2 signicant merit descriptive purposes normative approach learning go beyond recommending behavior eventually lead desired solution major issues one needs face 1 learning algorithms agents individually rational 2 learning algorithms eciently converge desired values employed agents 3 deviation desired learning algorithm become irrational short period time concepts introduced paper address issues ele paretoele provide new basic tools learning noncooperative settings moreover able show constructive existence results ele paretoele context repeated games perfect monitoring also able show relax perfect monitoring assumption desired properties impossible obtain general case paretoele appealing concept context stochastic games well able extend results context together concepts results provide rigorous normative approach learning general noncooperative interactions useful contrast approach important line related work features algorithms guarantee agent using attain value approximately equal value would attained known advance adversary would play algorithms along line appear eg 11 7 special attention given issue eciency latter result truly spirit online algorithms goal much online would able line case attempt react online adversarys behavior manner would similar terms average payo best could done known adversarys behavior hand results highly valuable many readers may notice subtle crucial point treat adversarys policy xed sequence mixed strategies probabilistic actions contrary spirit gametheory reality adversary adjust policy response agents behavior imagine example following instance wellknown prisoners dilemma game consider following two adversary policies 1 agent initially plays row 1 denoted cooperate adversary always play column 1 denoted cooperate agent initially plays row 2 denoted defect adversary always play column 2 denoted defect 2 agent initially plays cooperate adversary always play defect agent initially plays defect adversary always play cooperate clear agent guarantee bestresponse value adversary approach limited view adversary using predetermined sequence mixed strategies bottom line despite practical theoretical importance results cannot replace concepts based notion equilibrium another related work normative guidelines design learning algorithms 1 bowling veloso suggest two criteria learning algorithms rst call rationality stipulates players policies converge stationary policy learning algorithm converge bestresponse policy second call convergence stipulates learner necessarily converge stationary policy criteria attractive work notion nashequilibrium learning strategies deeper notion rationality bestresponse upon conver gence convergence though denitely desirable ignores issue convergence rate moreover convergence specied well dened indeed work bowling veloso consider special welldened case convergence selfplay ie agents use algorithm standard notion convergence adopted work learning games uses fact particular context selfplay investigated bowling veloso requirements equivalent requirement algorithm converge correlated equilibrium common property pursued learning algorithms gametheory literature concept ele provides rigorous notion individually rational learning strategies moreover believe ecient ie polynomial time convergence rate integral part denition rationality many settings happens exponential number iterations great interest applies judgment irrationality well agent makes irrational choice leads increased reward near future decreased reward exponential number steps seem irrational r rational covergent learning stochastic games learning coordinate eciently model based approach dynamics reinforcement learning cooperative multiagent systems predicting people play games reinforcement learning games unique strategy equilibrium adaptive game playing using multiplicative weights theory learning games folk theorem repeated games discounting incomplete information game theory reinforcement procedure leading correlated equilibrium reinforcement learning survey markov games framework multiagent reinforcement learning stochastic games tr dynamics reinforcement learning cooperative multiagent systems distributed algorithmic mechanism design friendorfoe qlearning generalsum games multiagent reinforcement learning nearoptimal reinforcement learning polynominal time using redundancy improve robustness distributed mechanism implementations rmax general polynomial time algorithm nearoptimal reinforcement learning ctr jeffrey shneidman david c parkes specification faithfulness networks rational nodes proceedings twentythird annual acm symposium principles distributed computing july 2528 2004 st johns newfoundland canada david c parkes jeffrey shneidman distributed implementations vickreyclarkegroves mechanisms proceedings third international joint conference autonomous agents multiagent systems p261268 july 1923 2004 new york new york yevgeniy vorobeychik michael p wellman satinder singh learning payoff functions infinite games machine learning v67 n12 p145168 may 2007 rob powers yoav shoham thuc vu general criterion algorithmic framework learning multiagent systems machine learning v67 n12 p4576 may 2007 guido boella leendert van der torre enforceable social laws proceedings fourth international joint conference autonomous agents multiagent systems july 2529 2005 netherlands vincent conitzer tuomas sandholm awesome general multiagent learning algorithm converges selfplay learns best response stationary opponents machine learning v67 n12 p2343 may 2007