sparse approximate inverse preconditioner nonsymmetric linear systems paper concerned new approach preconditioning large sparse linear systems procedure computing incomplete factorization inverse nonsymmetric matrix developed resulting factorized sparse approximate inverse used explicit preconditioner conjugate gradienttype methods theoretical properties preconditioner discussed numerical experiments test matrices harwellboeing collection tim daviss collection presented results indicate new preconditioner cheaper construct approximate inverse preconditioners furthermore new technique insures convergence rates preconditioned iteration comparable obtained standard implicit preconditioners b introduction paper consider solution nonsingular linear systems form 1 coefficient matrix 2 ir nthetan large sparse particular concerned development preconditioners conjugate gradienttype methods wellknown rate convergence methods solving 1 strongly influenced spectral properties therefore natural try transform original system one solution favorable spectral properties preconditioner matrix used accomplish transformation g nonsingular dipartimento di matematica universita di bologna italy cerfacs 42 ave g coriolis 31057 toulouse cedex france benzicerfacsfr work supported part grant scientific cooperation agreement cnr czech academy sciences institute computer science academy sciences czech republic pod vodarenskou vezi 2 tumauivtcascz work author supported part grants ga cr 201930067 ga cr 230401 nsf grant number int9218024 michele benzi miroslav tuma matrix approximates linear system 2 solution system 1 convergence rate iterative methods applied 2 may much higher problem 2 preconditioned left right preconditioning also possible preconditioning right leads transformed linear system solution 3 obtained solution 1 given choice left right preconditioning often dictated choice iterative method also possible use forms preconditioning split preconditioning see 3 details note practice required compute matrix product ga ag explicitly conjugate gradienttype methods necessitate coefficient matrix form matrixvector multiplies therefore applying preconditioner within step gradienttype method reduces computing action g vector loosely speaking closer g exact inverse higher rate convergence iterative methods choosing yields convergence one step course constructing preconditioner equivalent solving original problem practice preconditioner g easily computed applied total time preconditioned iteration less time unpreconditioned one typically cost applying preconditioner iteration conjugate gradienttype method order cost matrixvector multiply involving sparse implies preconditioner also sparse density nonzeros roughly order clearly effectiveness preconditioning strategy strongly problem architecture dependent instance preconditioner expensive compute may become viable reused many times since case initial cost forming preconditioner amortized several linear systems situation occurs stance dealing timedependent nonlinear problems whose numerical solution gives rise long sequences linear systems coefficient matrix slowly varying one different righthand sides furthermore preconditioners efficient scalar computing environment may show poor performance vector parallel machines conversely approximate inverse preconditioning 3 number preconditioning techniques proposed literature see eg 23 references therein generally agreed construction efficient generalpurpose preconditioners possible still considerable interest developing methods perform well wide range problems wellsuited stateoftheart computer architectures introduce new algebraic preconditioner based incomplete triangular factorization gamma1 paper natural continuation 8 focus restricted symmetric positive definite systems preconditioned conjugate gradient method see also 57 paper organized follows x2 give quick overview implicit explicit preconditioning techniques considering relative advantages well limitations two approaches x3 summarize recent work popular approach approximate inverse preconditioning based frobenius norm minimization x4 introduce new incomplete inverse triangular decomposition technique describe theoretical properties graphtheoretical characterization fillin inverse triangular factorization presented x5 x6 consider use preconditioning matrices reduced block triangular form implementation details results numerical experiments discussed xx7 8 concluding remarks indications future work given x9 experiments suggest new preconditioner cheaper construct preconditioners based optimization approach moreover good rates convergence achieved preconditioner comparable insured standard ilutype techniques 2 explicit vs implicit preconditioning existing preconditioners broadly classified either implicit explicit kind preconditioner implicit application within step chosen iterative method requires solution linear system nonsingular matrix implicitly defines approximate applying g requires solving linear system coefficient matrix course chosen solving system matrix easier solving original problem 1 perhaps important example provided preconditioners based incomplete lu ilu decomposition u l u sparse triangular matrices approximate exact l u factors applying preconditioner requires solution two sparse triangular systems forward backward solves notable examples implicit preconditioners include ilq ssor adi preconditioners see 3 4 michele benzi miroslav tuma contrast explicit preconditioning matrix g gamma1 known possibly product sparse matrices preconditioning operation reduces forming one matrixvector product instance many polynomial preconditioners belong class 37 explicit preconditioners described subsequent sections implicit preconditioners intensively studied successfully employed number applications spite last years increasing amount attention devoted alternative forms preconditioning especially explicit kind far two main reasons recent trend first place shortly usage modern highperformance architectures became widespread realized straightforward implementation implicit preconditioning conjugate gradientlike methods could lead severe degradation performance new machines particular sparse triangular solves involved ilutype preconditioning found serial bottleneck due recursive nature computation thus limiting effectiveness approach vector parallel computers mentioned considerable effort devoted overcoming difficulty result architectures types problems possible introduce nontrivial parallelism achieve reasonably good performance triangular solves means suitable reordering strategies see eg 13854 however triangular solves remain problematic aspect computation shared memory 33 distributed memory 10 computers many problems efficient application implicit preconditioner parallel environment still represents serious challenge another drawback implicit preconditioners ilutype possibility breakdowns incomplete factorization process due occurrence zero exceedingly small pivots situation typically arises dealing matrices strongly unsymmetric andor indefinite even pivoting applied see 1149 general may even occur definite problems unless exhibits degree diagonal dominance course always possible safeguard incomplete factorization process always runs completion producing nonsingular preconditioner also guarantee resulting preconditioner acceptable quality fur thermore shown 23 problems standard ilu techniques produce unstable incomplete factors resulting useless preconditioners explicit preconditioning techniques based directly approximating gamma1 developed attempt avoid mitigate difficulties applying explicit preconditioner requires sparse matrixvector products easier parallelize approximate inverse preconditioning 5 sparse triangular solves cases construction preconditioner wellsuited parallel implementation addition construction approximate inverse sometimes possible even matrix stable incomplete lu decomposition moreover mention sparse incomplete inverses often used constructing approximate schur complements pivot blocks use incomplete block factorization twolevel preconditioners see 231215 course explicit preconditioners far completely troublefree even sparse approximate inverse g computed care must exercised ensure g nonsingular nonsymmetric problems matrix g could good approximate inverse used left preconditioning poor one used right preconditioning see 36 p 9645 p 6648 furthermore explicit preconditioners sometimes effective implicit ones reducing number iterations sense problems require higher number nonzeros order achieve rate convergence insured implicit preconditioners one reasons limitation explicit preconditioner attempts approximate gamma1 usually dense sparse matrix thus explicit preconditioner likely work well gamma1 contains many entries small magnitude favorable situation exhibits form diagonal dominance problems implicit preconditioning also likely effective hence problems type explicit preconditioners competitive implicit ones explicitness fully exploited finally explicit preconditioners usually expensive compute implicit ones although difference may become negligible common situation several linear systems coefficient matrix different righthand sides solved case time computing preconditioner often fraction time required overall computation also worth repeating construction certain sparse approximate inverses done least principle highly parallel manner whereas scope parallelism construction ilutype preconditioners limited 3 methods based frobenius norm minimization good deal work devoted explicit preconditioning based following approach sparse approximate inverse computed matrix g minimizes ki gamma gak ki gamma agk right preconditioning subject sparsity constraint see 4 ch 8 21643 4432311130 matrix norm usually frobenius norm weighted variant computational reasons choice constrained minimization problem decouples n independent linear least squares problems one row 6 michele benzi miroslav tuma column g number unknowns problem equal number nonzeros allowed row column g immediately follows identity ith unit vector g ith column g clearly considerable scope parallelism approach resulting sparse least squares problems solved principle independently either direct methods 44 3130 iteratively 1142 early papers eg 43243 sparsity constraint imposed priori minimizer found relative class matrices predetermined sparsity pattern instance band matrix good degree diagonal dominance banded approximation gamma1 justified see 18 however general sparse matrices difficult guess good sparsity pattern approximate inverse several recent papers addressed problem adaptively defining nonzero pattern g order capture large entries inverse 3130 indeed monitoring size residual possible decide whether new entries g retained discarded dynamic fashion moreover information residuals utilized derive rigorous bounds clustering singular values preconditioned matrix therefore estimate condition number 31 also possible formulate conditions norm residuals insure approximate inverse nonsingular unfortunately conditions appear dubious practical value trying fulfill could lead dense approximate inverse 1611 disadvantage approach symmetry coefficient matrix cannot exploited symmetric positive definite spd sparse approximate inverse symmetric general even preset symmetric sparsity pattern enforced guarantee approximate inverse positive definite could lead breakdown conjugate gradient acceleration reason kolotilina yeremin 4344 propose compute explicit preconditioner form lower triangular preconditioned matrix glag spd conjugate gradient method applied matrix gl solution constrained minimization problem frobenius norm gamma lgl l cholesky factor 43 shown problem solved without explicit knowledge entries l using entries coefficient matrix technique also used compute factorized approximate inverse nonsymmetric matrix approximate inverse preconditioning 7 separately approximating inverses l u factors stands however technique requires sparsity pattern approximate inverse triangular factors specified advance therefore suitable matrices general sparsity pattern additional reasons considering factorized approximate inverses clearly approximate inverse g expressed product two triangular factors trivial insure g nonsingular another argument favor approach given 11 observed factorized forms general sparse matrices contain information storage single product stored serial cost construction type preconditioner usually high although theoretical parallel complexity quite moderate 4430 results numerical experiments reported 44 demonstrate factorized sparse approximate inverse preconditioners insure rapid convergence preconditioned conjugate gradient iteration applied certain finite element discretizations 3d pde problems arising elasticity theory however experiments preconditioning strategy applied coefficient matrix directly rather reduced system schur comple ment better conditioned considerably less sparse original problem approximate inverse preconditioner applied directly original stiffness matrix rate convergence pcg iteration rather disappointing comparison frobenius normbased sparse approximate inverse preconditioner ilu0 preconditioner number general sparse matrices made 30 reported results show explicit preconditioner insure rates convergence comparable achieved implicit ilutype approach observed construction approximate inverse often costly amenable parallelization factorized sparse approximate inverses also considered authors instance kaporin 394041 whose approach also based minimizing certain matrix functional closely related kolotilina yeremin next sections present alternative approach factorized sparse approximate inverse preconditioning grounded optimization based instead direct method matrix inversion shall see serial cost forming sparse approximate inverse technique usually much less optimization approach convergence rates still comparable average obtained ilutype preconditioning 8 michele benzi miroslav tuma 4 method based inverse triangular factorization optimization approach constructing approximate inverses possible one section consider alternative procedure based direct method matrix inversion performed incompletely order preserve sparsity results factorized sparse g gamma1 incomplete matrix factorization method procedure resembles classical ilu type implicit techniques indeed draw experience accumulated years use ilutype preconditioning implementation stage deriving theoretical properties preconditioner g paper continues work 8 symmetric positive definite case studied see also 57 construction preconditioner based algorithm computes two sets vectors fz g n abiconjugate ie w 6 j given nonsingular matrix 2 ir nthetan close relationship problem inverting computing two sets abiconjugate vectors fz g n fw g n matrix whose ith column z matrix whose ith column w follows w z necessarily nonsingular hence inverse known two complete sets abiconjugate vectors known note infinitely many sets matrices w z whose columns biconjugate explicitly computed means biconjugation process applied columns two nonsingular matrices w 0 z 0 2 ir nthetan computationally convenient choice let w biconjugation process applied unit basis vectors order describe procedure let c denote ith row approximate inverse preconditioning 9 respectively ie c ith column basic abiconjugation procedure written follows biconjugation algorithm z z igamma1 return algorithm essentially due l fox see ch 6 25 closely related methods also considered hestenes stiefel 35 pp 42642734 stewart 52 general treatment given recent paper 14 geometrically procedure regarded generalized gramschmidt orthogonalization oblique projections nonstandard inner products see 614 several observations regarding algorithm order first place note formulation contains redundancy since exact arithmetic therefore step computation dot product q igamma1 may replaced assignment q igamma1 another observation fact procedure stands vulnerable breakdown division zero occurs whenever michele benzi miroslav tuma quantities p igamma1 happens zero shown exact arithmetic biconjugation algorithm break leading principal minors nonzero see nonsingular matrix exists permutation matrix q procedure applied pa aq break lu decomposition pivoting permutation matrices represent row column interchanges performed needed course computation biconjugation process carried completion without interchanges resulting z w matrices upper triangular 1 diagonal entries equal one satisfy identity recognize 5 familiar ldu decomposition ldu l unit lower triangular u unit upper triangular diagonal matrix pivots main diagonal factorization unique biconjugation algorithm explicitly computes matrix exactly 5 ldu hence process produces inverse triangular decomposition equivalently triangular decomposition udl type gamma1 p returned algorithm pivots ldu factorization denote delta ith leading principal minor 1 n let identity 5 implies showing biconjugation algorithm performed without breakdowns leading principal minors nonvanishing finite precision arithmetic pivoting may required promote numerical stability z w available solution linear system form 1 computed 4 1 note necessarily true matrix identity used outset ie approximate inverse preconditioning 11 practice direct method solving linear systems used account cost dense n theta n matrix biconjugation process requires twice work lu factorization notice cost solve phase using 6 forward backward solves l u factors symmetric number operations biconjugation algorithm halved observing w must equal z hence process carried using rows zvectors p igamma1 columns resulting z form set conjugate directions spd breakdown occur exact arithmetic pivoting required algorithm computes l dl factorization gamma1 method first described 26 geometrically amounts gramschmidt orthogonalization inner product hx yi x ay applied unit vectors e sometimes referred conjugate gramschmidt process method still impractical direct solver requires twice work cholesky dense matrices however explained 5 6 algorithm also applied nonsymmetric systems resulting implicit ldu factorization computed indeed possible compute solution 1 righthand side using z part entries method arithmetic complexity gaussian elimination applied dense problems combined suitable sparsitypreserving strategies method useful sparse direct solver least types problems see 56 sparse symmetric positive definite z matrix produced algorithm tends dense see next section observed experimentally often entries z small magnitude fillin z matrix reduced removing suitably small entries computation zvectors algorithm computes sparse matrix z diagonal matrix ie factorized sparse approximate inverse hence g used explicit preconditioner conjugate gradient method detailed study preconditioning strategy spd problems found 8 proven incomplete inverse factorization exists hmatrix analogously ilutype factorizations numerical experiments 8 show approach insure fast convergence pcg iteration almost good implicit preconditioning incomplete cholesky type construction preconditioner somewhat expensive computation incomplete cholesky factorization still quite cheap contrast michele benzi miroslav tuma least squares approach described previous section construction approximate inverse usually time consuming least sequential environment remainder paper consider explicit preconditioning strategy based biconjugation process described sparsity z w factors gamma1 preserved removing small fill z wvectors possibility would drop newly added fillin elements outside preset sparsity pattern main diagonal z w however general sparse matrices difficult guess reasonable sparsity pattern drop tolerance used instead trivial extension results 8 shows incomplete biconjugation process incomplete inverse factorization cannot break exact arithmetic hmatrix general matrices necessary safeguard computation order avoid breakdowns requires pivot modifications perhaps form pivoting postpone details x7 incomplete biconjugation algorithm computes sparse unit upper triangular matrices w w nonsingular diagonal matrix factorized sparse approximate inverse used explicit preconditioner conjugate gradienttype methods solution 1 conclude section remarks properties approximate inverse preconditioner g described hmatrix already mentioned construction preconditioner could break due occurrence zero extremely small pivots however following 46 note always exists ff 0 ffi diagonally dominant hence hmatrix therefore incomplete bicon jugation algorithm breaks one could try select ff 0 reattempt process shifted matrix large enough insure existence incomplete inverse factorization also small enough 0 close approach several drawbacks illconditioned matrices quality resulting preconditioner typically poor furthermore breakdown prompts shift may occur near end biconjugation process preconditioner may recomputed several times satisfactory value ff found better strategy perform diagonal modifications need arises shifting pivots away zero magnitude less specified threshold see x7 details mmatrix follows results 8 g nonnegative matrix approximate inverse preconditioning 13 moreover easy see componentwise following inequalities hold da diagonal part furthermore g 1 g 2 two approximate inverses mmatrix produced incomplete biconjugation process drop tolerance used g 1 greater equal drop tolerance used g 2 true sparsity patterns used determine nonzero structure z w patterns g 2 include patterns g 1 monotonicity property shared sparse approximate inverses see example ch 8 2 note property 7 important approximate inverse used within incomplete block factorization mmatrix insures intermediate matrices produced course incomplete factorization preserve mmatrix property see 2 pp 263264 finally discussing similarities point difference incomplete inverse factorization ilutype factorization matrix incomplete factorization mmatrix induces splitting regular splitting therefore convergent aei gamma denotes spectral radius 4755 true general incomplete factorization one considers induced splitting splitting need convergent example given symmetric mmatrix b matrix incomplete inverse factorization drop tolerance intermediate fillin dropped smaller absolute value produces approximate inverse g aei gamma ga 1215 1 shows approximate decomposition cannot obtained general incomplete factorization sense incomplete inverse factorization algebraically equivalent incomplete ldu factorization performed 14 michele benzi miroslav tuma 5 fillin biconjugation algorithm section give characterization fillin occurring factorized inverse obtained biconjugation algorithm results may serve guideline predict structure factorized approximate inverse impact certain aspects implementation wellknown structural nonzeros inverse matrix gamma1 characterized paths graph original matrix see 2429 following lemma states necessary sufficient conditions new entry fillin added one zvectors ith step biconjugation algorithm similar result holds wvectors make use standard nocancellation assumption lemma 51 let 1 z igamma1 l z igamma1 li time least one two following conditions holds proof suppose z igamma1 0 directly update formula zvectors see z igamma1 li 6 0 l since z igamma1 lj becomes nonzero ith step clearly p igamma1 j must nonzero z igamma1 kj ik get result opposite implication trivial 2 figures 51 56 provide illustration previous lemma figure 51 shows nonzero structure matrix fs760 1 order 760 harwellboeing collection 21 figures 526 show structure factor z different stages biconjugation algorithm pictures show initial steps entries z still zero nonzeros z induced nonzeros corresponding positions similar situation occurs course process computes w figure 57 show entries z larger absolute figure 58 show incomplete factor z obtained drop tolerance seen well incomplete process able capture large entries complete factor z figures generated using routines plotting sparse matrix patterns sparskit 50 approximate inverse preconditioning 15 figure 512 structure matrix fs760 1 left factor z right 20 steps biconjugation process figure 534 structure z 70 steps left 200 steps right biconjugation process michele benzi miroslav tuma figure 556 structure z 400 steps left 760 steps right biconjugation process figure 578 structure entries z larger 10 gamma10 left structure incomplete factor z drop tolerance approximate inverse preconditioning 17 sufficient condition fillin matrix z steps biconju gation algorithm given following lemma lemma 52 let e bipartite graph indices l path b z p proof use induction p let 0 course z 1 gamma1 lemma 51 get z 1 suppose lemma 52 true l p z using nocancellation assumption also z p following theorem gives necessary sufficient condition nonzero entry appear position l j l j inverse triangular factor theorem 53 let 1 p 1 proof first show stated conditions sufficient lemma 51 nonzeros imply z 1 l 1 j also nonzero done otherwise z 2 gamma1 0 taking account z l 2 get z 2 nonzero repeating arguments inductively finally get z p l consequently z assume z lj 6 0 lemma 51 implies least one following two conditions holds either exists li 0 6 0 exist indices 00 k 6 0 z 00 gamma1 li 00 6 0 former case necessary conditions latter case apply lemma 51 inductively z 00 gamma1 j inductive steps obtain conditions 2 clearly characterization fillin inverse triangular factorization less transparent necessary sufficient condition characterize nonzeros non factorized inverse michele benzi miroslav tuma 6 preconditioning block triangular matrices many sparse matrices arising realworld applications may reduced block triangular form see ch 6 20 section discuss application preconditioning techniques linear systems block lower triangular coefficient matrix closely following 30 reduction block triangular form usually obtained twostep procedure outlined 20 first step rows permuted bring nonzero entries main diagonal producing matrix pa second step symmetric permutations used find block triangular form 53 resulting matrix represented k1 k2 delta delta delta kkc c c c diagonal blocks ii assumed irreducible nonsingular diagonal blocks ii must also nonsingular suppose compute approximate inverses diagonal blocks incomplete biconjugation algorithm gamma1 z ii ii ii inverse approximated follows cf 30 22 k1 k2 delta delta qp preconditioning step conjugate gradienttype method requires evaluation action g vector ie computation z gd given vector step preconditioned iterative method done backsubstitution z b z b approximate inverse preconditioning 19 partitioning z induced block structure qpaq computation required certain preconditioned iterative methods accomplished similar way approach fillin confined approximate inverses diagonal blocks often resulting sparse preconditioner notice also approximate inverses g ii computed parallel price pay loss part explicitness approximate inverse preconditioner applied noted 30 comparison purposes apply scheme ilu preconditioning specif ically approximate 21 k1 k2 diagonal block ii approximated ilu decomposition u ii applying preconditioner requires solution linear system step preconditioned iteration done backsubstitution partitioning z use transposed ilu preconditioning similar type ilu block preconditioning introduce explicitness application preconditioner note ilu factorizations diagonal blocks performed parallel see section numerical experiments reduction block triangular form influences behavior preconditioned iterations different ways depending whether approximate inverse techniques ilutype preconditioning used michele benzi miroslav tuma 7 implementation aspects possible implement incomplete inverse factorization algorithm x4 least two distinct ways first implementation similar spirit classical submatrix formulation sparse gaussian elimination represented instance 1957 approach relies sparse incomplete rankone updates matrices z applied form outer vector products updates timeconsuming part computation course updates new fillin elements whose magnitude less prescribed drop tolerance dropped approach dynamic data structures used z matrices note step incomplete inverse factorization ith row ith column c required matrix stored static data structures rows columns course single array needed numerical values entries implementation efficient additional elbow room necessary instance computation incomplete z factor elbow room twice space anticipated storing nonzeros factor looking preconditioner number nonzeros original matrix estimated number nonzeros z half number nonzeros original matrix column z give initial prediction fillin based results x5 thus initial structure z given structure upper triangular part course w handled similarly space initially allocated given column enough situation solved way standard working dynamic data structures looking block free space end active part dynamic data structure large enough contain current column garbage collection see 57 fillin z w appears late steps biconjugation process able keep amount dynamic data structure manipulations relatively low levels following implementation referred dds implementation despite efforts minimize amount symbolic manipulations dds im plementation disadvantages nonlocal character computations high proportion nonfloatingpoint operations still remain important drawback submatrix rightlooking undelayed algorithms using dynamic data structures useful structural prediction known efficient block strategy used even operations performed incore work row column lists step outer cycle rather irregular therefore larger problems operations still scattered around memory outofcache consequence difficult achieve high efficiency code attempt parallelize approximate inverse preconditioning 21 computation preconditioner form face serious problems see 57 discussion difficulties parallelizing sparse rankone updates reasons considered alternative implementation hereafter referred sds makes use static data structures based leftlooking delayed update version biconjugation algorithm amounts rearrangement computations shown simplicity consider z factor assume breakdown occurs 1 let z 0 e 1 z 0 j gamma1 z j gamma1 z j z j gamma1 j gamma1 j gamma1 z j gamma1 z igamma1 procedure implemented static data structures cost increasing number floatingpoint operations indeed implementation found necessary recompute dot products p j gamma1 z j gamma1 used updating subsequent columns increase arithmetic complexity less pronounced depending problem density preconditioner hand formulation greatly decreases amount irregular data structure manipulations also appears better suited parallel implementation dot products vector updates innermost loop done parallel notice sds longer true single row column used step outer loop worth mentioning numerically dds sds implementations incomplete biconjugation process completely equivalent sds implementation straightforward suppose first steps completed order determine columns already determined part z play 22 michele benzi miroslav tuma role rankone updates used form jth column z need linked list scanning structure columns linked list coded similarly mechanism determines structure jth row cholesky factor l numerical factorization sparspak see 2713 addition approximate inverse preconditioner also coded standard row implementation classical ilu0 preconditioner see eg 50 chose nofill implicit preconditioner mostly interested comparing preconditioners nonzero density close original matrix input codes computation preconditioners check whether coefficient matrix zerofree diagonal row reordering matrix used permute nonzeros diagonal ilu0 approximate inverse factorization introduced simple pivot modification avoid breakdown whenever diagonal element algorithms compute preconditioner found small case less absolute value ieee machine precision ffl 22 increased 10 gamma3 special reasons choice worked well practice mentioned numerical experiments safeguarding measure required often ilu0 approximate inverse factorization experiments matrices nontrivially reduced block triangular form used routine mc13d ma28 19 get block triangular form 8 numerical experiments section present results numerical experiments range problems harwellboeing collection 21 tim davis collection 17 matrices used rescaled dividing elements absolute value largest nonzero entry scaling used righthand side linear system computed solution vector x ones choice used eg 57 experimented several iterative solvers conjugate gradient type present results three selected methods found sufficiently representative van der vorsts bicgstab method denoted bst tables qmr method freund nachtigal saad schultzs gmres restarted every 20 steps denoted g20 tables householder orthogonalization 56 see 3 description methods report 9 experiments solvers approximate inverse preconditioning 23 matrices used experiments come reservoir simulation ors pores2 saylr sherman chemical kinetics fs5414 network flow hor131 circuit simulation jpwh991 memplus add petroleum engineering watt matrices incompressible flow computations raefsky swang1 order n number nnz nonzeros test problem given table 1 together number iterations computing times unpreconditioned iterative methods means convergence attained 1000 iterations bicgstab qmr 500 iterations gmres20 time table 1 test problems n order matrix nnz nonzeros matrix convergence results iterative methods without preconditioning michele benzi miroslav tuma tests performed sgi crimson workstation risc processor r4000 using double precision arithmetic codes written standard fortran 77 compiled optimization option o4 cpu time given seconds measured using standard function dtime initial guess iterative solvers always x stopping criterion used jjr k jj unpreconditioned updated residual note r 1 jjr 0 jj 1 nzr nzr denotes maximum number nonzeros row following tables present results experiments ilu0 preconditioner approximate inverse preconditioner based biconjugation process referred aibc observe number nonzeros ilu0 preconditioner equal number nnz nonzeros original matrix whereas aibc preconditioner fillin given total number nonzeros factors w tables number nonzeros aibc denoted f ill right preconditioning used experiments comparison implicit explicit preconditioner based amount fill rate convergence measured number iterations two parameters realistically describe scalar behavior preconditioned iterative methods course important advantage inverse preconditioner explicitness captured description accuracy aibc preconditioner controlled value drop tolerance smaller drop tolerances result dense preconditioner often always higher convergence rate preconditioned iteration experiments consider relatively sparse preconditioners cases able adjust value obtain inverse preconditioner nonzero density close hence ilu0 preconditioner due scaling matrix entries choice often right one also give results approximate inverse obtained somewhat smaller value drop tolerance order show number iterations reduced allowing fillin preconditioner problems could find value number nonzeros aibc close nnz cases approximate inverse preconditioner tended either dense sparse approximate inverse preconditioning 25 table 2 give timings preconditioner computation iteration counts timings three iterative solvers preconditioned ilu0 information given table 3 approximate inverse preconditioner aibc aibc give two timings construction preconditioner first dds implementation using dynamic data structures second sds implementation using static data structures ilu ilu time matrix ptime bst qmr g20 bst qmr g20 raefsky1 2457 table 2 time form ilu0 preconditioner ptime number iterations time bicgstab qmr gmres20 ilu0 preconditioning 26 michele benzi miroslav tuma ptime aibc aibc time matrix fill dds sds bst qmr g20 bst qmr g20 5204 jpwh991 7063 031 026 15 27 28 024 067 078 48362 068 263 33 43 64 261 539 863 26654 089 245 table 3 time form aibc preconditioner ptime using dds sds implemen tations number iterations time bicgstab qmr gmres20 aibc approximate inverse preconditioning 27 appears results ilu0 aibc preconditioners roughly equivalent point view rate convergence ilu0 slight edge many problems two preconditioners give similar results cases like pores2 ilu0 much better aibc others like memplus situation reversed problems necessary allow relatively high fill approximate inverse preconditioner order convergence rate comparable insured ilu0 cf saylr4 cases sparse aibc gives excellent results see add raefsky matrices follows timings iterative part solution process pretty close average two preconditioners also notice using dense approximate inverse preconditioner obtained smaller value nearly always reduces number iterations although necessarily mean reduced computing time since takes longer compute preconditioner cost iteration increased concerning matrix pores2 method gives poor results observed fillin w factor high tried use different drop tolerances two factors one larger one used z help observed 31 finding sparse right approximate inverse pores2 hard left approximate inverse approximated instead unfortunately method produces exactly approximate inverse transposition therefore able cope problem effectively experienced similar difficulty w factor matrix sherman2 hand sherman3 face problems reported 30 convergence aibc preconditioner smooth time required compute preconditioners obvious ilu0 computed quickly hand computation aibc preconditioner prohibitive problems computing aibc two three times expensive computing ilu0 important experiments aibc show overall solution time almost always dominated iterative part unless convergence extremely rapid case iteration part takes slightly less time computation preconditioner observation suggests approximate inverse preconditioner much cheaper construct sequential environment approximate inverse preconditioners based 28 michele benzi miroslav tuma frobenius norm approach described x3 indeed look results presented 30 see sequential time required construct preconditioner accounts huge portion often excess 90 overall computing time worth emphasizing approach based frobenius norm minimization one propose seem produce preconditioners similar quality sense comparable ilu0 point view fillin rates convergence least average different implementations aibc see results table 3 larger problems effect additional floatingpoint operations sds implementation dds implementation actually faster nevertheless already observed implementation using static data structures may better suited parallel architec tures paper consider scalar implementation remaining experiments limit timings dds implementation aibc experiments excluding ones performed measure timings presented tables monitored also true residual jjb gamma ax k jj 2 general found discrepancy norm updated residual small however found illconditioned matrices harwellboeing collection included tables difference may large instance lns west matrices found jjr k jj final value r k happened ilu0 approximate inverse preconditioner regarded failure preconditioned iterative method present tables 4 5 results experiments matrices reduced block lower triangular form compared number iterations preconditioned iterative methods timings block approximate inverse preconditioner block ilu0 preconditioner described x6 since matrices trivial block lower triangular form one block two blocks one blocks dimension one matrices excluded experiments table 4 give matrix number nbl blocks results experiments ilu0 table 5 give analogous results aibc preconditioner amount fillin denoted f ill aibc computed fillin approximate inverses diagonal blocks plus number nonzero entries offdiagonal blocks approximate inverse preconditioning 29 block ilu block ilu time table 4 time compute block ilu preconditioner ptime number iterations time bicgstab qmr gmres20 block ilu0 preconditioning block aibc block aibc time matrix fill ptime bst qmr g20 bst qmr g20 table 5 time compute block aibc preconditioner ptime number iterations time seconds bicgstab qmr gmres20 block aibc preconditioning clear general reduction block triangular form lead noticeable improvement timings least sequential implementation observe block form used results ilu0 sometimes worse michele benzi miroslav tuma probably attributed permutations known cause cases degradation rate convergence preconditioned iterative method 22 notable exception matrix watt2 number iterations greatly reduced hand results block approximate inverse preconditioner mostly unchanged somewhat better matrix watt2 represents exception problem greatly benefits reduction block triangular form case permutations adversely affect rate convergence preconditioned iterative method fact suggests perhaps approximate inverse preconditioner robust ilu0 respect reorderings gain insight permutations original matrix influence quality types preconditioners experiments matrix permuted using minimum degree algorithm structure see 28 applied resulting permutation symmetrically get pap order preserve nonzero diagonal tables 6 7 present results test matrices trivial block triangular form corresponding preconditioners denoted ilu0md aibcmd respectively ilumd ilumd time matrix ptime bst qmr g20 bst qmr g20 26 43 47 218 457 645 table time compute ilu0 preconditioner ptime permuted according minimum degree algorithm number iterations time bicgstab qmr gmres20 ilu0md preconditioning approximate inverse preconditioning 31 aibcmd aibcmd time matrix fill ptime bst qmr g20 bst qmr g20 7152 031 43 48 038 123 138 19409 058 104 95 291 414 table 7 time compute aibc preconditioner ptime permuted minimum degree algorithm number iterations time bicgstab qmr gmres20 aibcmd preconditioning results table 6 show problems especially coming pdes minimum degree reordering detrimental effect convergence iterative solvers preconditioned ilu0 cases see dramatic increase number iterations analogy observed fact see eg 22 minimum degree ordering used nofill incomplete cholesky decomposition spd michele benzi miroslav tuma matrix poor approximation coefficient matrix least problems arising discretization 2d pdes convergence conjugate gradient method preconditioner iccg0 much slower natural ordering unknowns used observe similar phenomenon nonsymmetric linear systems note rather striking behavior matrix add20 benefits greatly minimum degree reordering matrix arises circuit model discretization pde also observed 22 negative impact minimum degree rate convergence pcg disappears incomplete cholesky factorization computed means drop tolerance rather position natural ask whether holds true approximate inverse preconditioner aibc computed using drop tolerance results table 7 show indeed case test problems number iterations nearly unaffected better addition note minimum degree ordering helps preserving sparsity incomplete inverse factors usually enough decrease computing times fact possible reduce storage demands approximate inverse preconditioner without negatively affecting convergence rates might become important large problems conclude section observations concerning choice drop tolerance experiments used fixed value throughout incomplete biconjugation process however relative drop tolerances whose value adapted step step could also considered see 57 thorough discussion issues related choice drop tolerances context ilu observed amount fillin distributed rather unevenly course approximate inverse factorization large proportion nonzeros usually concentrated last several columns z w problems large fill may preferable switch larger drop tolerance columns incomplete factors start fillingin strongly conversely suppose computed approximate inverse preconditioner certain value find preconditioned iteration converging slowly provided enough storage available one could try recompute least columns z using smaller value unfortunately general sparse matrices guarantee result preconditioner improved quality indeed allowing nonzeros preconditioner always result reduced number iterations approximate inverse preconditioning 33 finally worthwhile observe dual threshold variant incomplete inverse factorization could adopted see 51 approach drop tolerance applied maximum number nonzeros per column specified enforced computation preconditioner way possible control maximum storage needed preconditioner important automated implementation approach tried yet hope near future 9 conclusions future work paper developed sparse approximate inverse preconditioning technique nonsymmetric linear systems approach based procedure compute two sets biconjugate vectors performed incompletely preserve sparsity algorithm produces approximate triangular factorization gamma1 guaranteed exist hmatrix similar ilu factorization factorized sparse approximate inverse used explicit preconditioner conjugate gradienttype methods applying preconditioner requires sparse matrixvector products considerable interest use parallel computers new preconditioner used enhance convergence different iterative solvers based extensive numerical experiments found preconditioner insure convergence rates comparable average standard preconditioner approximate inverse factorization timeconsuming compute ilu0 cost prohibitive typically dominated time required iterative part contrast approximate inverse preconditioners based frobenius norm minimization produce similar convergence rates expensive compute possible parallel environment situation reversed since preconditioner construction frobenius norm approach inherently parallel ever scope parallelization also inverse factorization method based instance approximate inverse factors z w computed largely independent clearly point requires research conclusion drawn parallel versions approximate inverse preconditioners implemented tested results point fact quality approximate inverse preconditioner greatly affected reorderings coefficient matrix important practice suggests may use permutations increase potential parallelism reduce amount fill preconditioner without spoiling rate convergence 34 michele benzi miroslav tuma theoretical results fillin x5 provide guidelines use pivoting strategies enhancing sparsity approximate inverse factors topic deserves research based results experiments conclude technique introduced paper potential become useful tool solution large sparse nonsymmetric linear systems modern highperformance architectures work parallel implementation new preconditioner currently way future work also include dual threshold implementation preconditioner computation acknowledgments would like thank one referees helpful comments suggestions professor miroslav fiedler providing reference 24 first author gratefully acknowledges hospitality excellent research environment provided institute computer science czech academy sciences r parallel implementation preconditioned conjugate gradient methods solving sparse systems linear equations iterative solution methods templates solution linear systems parallel algorithms solution certain large sparse linear systems direct rowprojection method sparse linear systems direct projection method sparse linear systems explicit preconditioner conjugate gradient method sparse approximate inverse preconditioner conjugate gradient method sparse approximate inverse preconditioner nonsymmetric linear systems krylov methods preconditioned incompletely factored matrices cm2 approximate inverse preconditioners general sparse matrices approximate inverse techniques blockpartitioned matrices users guide sparspaka waterloo sparse linear equations package block preconditioning conjugate gradient method approximate inverse preconditionings sparse linear systems sparse matrix collection decay rates inverses band matrices direct methods sparse matrices users guide harwellboeing sparse matrix collection effect ordering preconditioned conjugate gradients stability analysis incomplete lu factorizations inversion bigraphs connection gauss elimination introduction numerical linear algebra notes solution algebraic linear simultaneous equations computer solution large sparse positive definite systems evolution minimum degree algorithm predicting structure sparse matrix computations approximateinverse preconditioners parallel preconditioning sparse approximate inverses parallel preconditioning approximate inverses connection machine parallel preconditioned conjugate gradient package solving sparse linear systems cray ymp inversion matrices biorthogonalization related results method conjugate gradients solving linear systems theory matrices numerical analysis polynomial preconditioning conjugate gradient calculations efficient parallel iterative solution large sparse linear sys tems explicitly preconditioned conjugate gradient method solution unsymmetric linear systems new convergence results preconditioning strategies conjugate gradient method factorized sparse approximate inverse fsai preconditionings solving 3d fe systems massively parallel computers ii iterative construction fsai preconditioners factorized sparse approximate inverse preconditioning theory factorized sparse approximate inverse preconditioning ii solution 3d fe systems massively parallel computers krylov methods numerical solution initialvalue problems differentialalgebraic equations incomplete factorization technique positive definite linear systems iterative solution method linear systems coefficient matrix symmetric mmatrix properties approximate inverses matrices preconditioning techniques nonsymmetric indefinite linear systems sparskit basic tool kit sparse matrix computations ilut dual threshold incomplete lu factorization conjugate direction methods solving systems linear equations high performance preconditioning matrix iterative analysis implementation gmres method using householder transformations computational methods general sparse matrices tr ctr kai wang jun zhang multigrid treatment robustness enhancement factored sparse approximate inverse preconditioning applied numerical mathematics v43 n4 p483500 december 2002 claus koschinski new methods adapting approximating inverses preconditioners applied numerical mathematics v41 n1 p179218 april 2002 stephen barnard luis bernardo horst simon mpi implementation spai preconditioner t3e international journal high performance computing applications v13 n2 p107123 may 1999 n guessous souhar multilevel block ilu preconditioner sparse nonsymmetric mmatrices journal computational applied mathematics v162 n1 p231246 1 january 2004 matthias bollhfer volker mehrmann convergence estimates algebraic multilevel preconditioners contemporary mathematics theory applications american mathematical society boston 2001 michele benzi miroslav tma parallel solver largescale markov chains applied numerical mathematics v41 n1 p135153 april 2002 mansoor rezghi mohammad hosseini ilu preconditioner nonsymmetric positive definite matrices using conjugate gramschmidt process journal computational applied mathematics v188 n1 p150164 1 april 2006 h koulaei f toutounian computing block ilu preconditioner block tridiagonal systems journal computational applied mathematics v202 n2 p248257 may 2007 michele benzi preconditioning techniques large linear systems survey journal computational physics v182 n2 p418477 november 2002