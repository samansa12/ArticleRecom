bounding cacherelated preemption delay realtime systems abstractcache memory used almost computer systems today bridge ever increasing speed gap processor main memory however use multitasking computer systems introduces additional preemption delay due reloading memory blocks replaced preemption cacherelated preemption delay poses serious problem realtime computing systems predictability utmost importance paper propose enhanced technique analyzing thus bounding cacherelated preemption delay fixedpriority preemptive scheduling focusing instruction caching proposed technique improves upon previous techniques two important ways first technique takes account relationship preempted task set tasks execute preemption calculating cacherelated preemption delay second technique considers phasing tasks eliminate many infeasible task interactions two features expressed constraints linear programming problem whose solution gives guaranteed upper bound cacherelated preemption delay paper also compares proposed technique previous techniques using randomly generated task sets results show improvement worstcase response time prediction proposed technique previous techniques ranges 5 percent percent depending cache refill time task set utilization 06 results also show cache refill time increases improvement increases indicates accurate prediction cacherelated preemption delay proposed technique becomes increasingly important current trend widening speed gap processor main memory continues b introduction realtime computing system tasks timing constraints terms deadlines must met correct operation guarantee timing constraints extensive research performed schedulability analysis 1 2 3 4 5 6 studies various assumptions usually made simplify analysis one simplifying assumption cost task preemption zero assumption however hold general actual systems invalidating result schedulability analysis example task preemption incurs costs process interrupts 7 8 9 10 manipulate task queues 7 8 10 actually perform context switches 8 10 many direct costs addressed number recent studies schedulability analysis focus practical aspects task scheduling 7 8 9 10 addition direct costs task preemption introduces form indirect cost due cache memory used almost computer systems today computer systems cache memory task preempted large number memory blocks 1 belonging task displaced cache memory time task preempted time task resumes execution task resumes execution spends substantial amount execution time reloading cache memory blocks displaced preemption cache reloading greatly increases preemption delay may invalidate result schedulability analysis overlooks indirect cost two ways address unpredictability resulting cacherelated preemption delay first way use cache partitioning cache memory divided disjoint partitions one partitions dedicated realtime task 12 13 14 15 cache partitioning techniques task allowed access partition thus cacherelated preemption delay avoided however cache partitioning 1 block minimum unit information either present present cachemain memory hierarchy 11 assume without loss generality memory references made block units number drawbacks one drawback requires modification existing hardware software another drawback limits amount cache memory used individual tasks second way address unpredictability resulting cacherelated preemption delay take account effects schedulability analysis 16 basumallick nilsen propose one technique technique uses following schedulability condition set n tasks extends wellknown liu laylands schedulability condition 4 condition u total utilization task set c worst case execution time wcet period respectively 2 additional term fl upper bound cacherelated preemption cost imposes preempted tasks one drawback technique suffers pessimistic utilization bound approaches 0693 large n 4 many task sets total utilization higher bound successfully scheduled 3 rectify problem busquetsmataix et al 17 propose technique based response time approach 2 6 technique incorporated response time equation follows e theta c j r worst case response time hpi set tasks whose priorities higher recursive equation solved iteratively resulting worst case response time r task compared deadline determine schedulability notations used throughout paper along denotes deadline assume without loss generality higher priority main memory cache memory preempt preempt preempt preempt cache mapping b worst case preemption scenario response time preempt fig 1 overestimation cacherelated preemption delay used techniques computed multiplying number cache blocks used task time needed refill cache block estimation based pessimistic assumption cache block used replaces cache memory block needed preempted task pessimistic assumption leads overestimation cacherelated preemption delay since possible replaced memory block one longer needed one replaced without rereferenced even preemptions overestimation addressed lee et al 18 use concept useful cache blocks computing cacherelated preemption delay useful cache block defined cache block contains memory block may rereferenced replaced another memory block technique consists two steps first step analyzes task estimate maximum number useful cache blocks task based results first step second step computes upper bound cacherelated preemption delay using linear programming technique busquets mataix et als technique upper bound incorporated response time equation compute worst case response time although lee et als technique accurate techniques consider usefulness cache blocks still subject number overestimation sources explain sources using example fig 1 example three tasks assume without loss generality higher priority highest priority task 3 lowest priority suppose main memory regions used three tasks mapped cache fig 1a also suppose maximum number useful cache blocks 2 3 5 2 respectively time needed refill cache block single cycle setting linear programming method used lee et als technique would give solution preempted three times 1 3 preempted twice 2 3 response time r 3 resulting preemption delay 3 theta 5 solution however suffers two types overestimation first task preempted useful cache blocks replaced cache example 2 preempted 1 small portion 2 useful cache blocks replaced cache corresponding conflict cache blocks used 1 ie cache blocks framed thick borders fig 1a second worst case preemption scenario given solution may feasible actual execution example cannot preempted three times 1 since 2 wcet 20 thus first invocation 2 certainly completed second invocation 1 rectify problems paper proposes novel technique incorporates following two important features first proposed technique takes account relationship preempted task set tasks execute preemption calculating maximum number useful cache blocks reloaded preemption second technique considers phasing tasks eliminate many infeasible task interactions two features expressed constraints linear programming problem whose solution bounds cacherelated preemption delay paper focus cacherelated preemption delay resulting instruction caching analysis data cacherelated preemption delay equally important research issue handled method explained 18 paper also compares proposed technique previous techniques results show proposed technique gives 60 tighter prediction worst case response time previous techniques results also show cache refill time increases gap worst case response time prediction made proposed technique previous techniques increases finally results show cache refill time increases cacherelated preemption delay takes proportionally large percentage worst case response time indicates accurate prediction cacherelated preemption delay becomes increasingly important current trend widening speed gap processor main memory continues 11 rest paper organized follows next section describes detail lee et als technique serves basis proposed technique section iii describe overall approach proposed technique along constraints needed incorporate scenariosensitive preemption cost advanced constraints take account task phasing discussed section iv section v discuss optimization aims reduce amount computation needed proposed technique section vi presents results experiments assess effectiveness proposed technique finally conclude paper section vii ii linear programmingbased analysis cacherelated preemption delay section describe detail lee et als linear programmingbased technique analyzing cacherelated preemption delay technique response time equation given follows pc r term guaranteed upper bound cacherelated preemption delay given response time r term includes delay due preemptions also delay due preemptions higher priority tasks response time equation solved iteratively follows r 0 r k1 r k iterative procedure terminates r converged r value compared deadline determine schedulability compute pc r k iteration technique uses two step approach first step task analyzed estimate maximum number useful cache blocks task may execution estimation uses data flow analysis technique 19 generates following two types information execution point p cache block c 1 set memory blocks may reside cache block c execution point p 2 set memory blocks may first reference cache block c execution point p cache block c defined useful point p two sets common element means may execution memory block cache block c p rereferenced total number useful cache blocks p determines additional cache reloading time incurred task preempted p obviously worst case preemption occurs task preempted execution point maximum total number useful cache blocks case gives worst case cacherelated preemption cost task final result first step table called preemption cost table gives task worst case preemption cost f 3 second step uses preemption cost table linear programming technique derive upper bound pc r required iteration response time calculation step first defines g j number preemptions j r g j values give worst case preemption scenario among tasks known worst case cacherelated preemption delay interval r ie pc r calculated follows total cacherelated preemption delay includes delay due preemptions higher priority tasks note highest priority task 1 included summation since never preempted general however exact g j values give worst case preemption delay cannot determined thus analysis safe scenario guaranteed worse actual preemption scenario assumed conservative scenario derived following two constraints valid g j combination satisfy 3 technique defines general preemption cost f ij cost task pays worst case jth preemption j gamma 1th preemption however since cases execution point maximum total number useful cache blocks contained within loop nest generalized preemption cost little effect f product iteration bounds containing loops first total number preemptions r cannot larger total number invocations second total number preemptions j r cannot larger number invocations j r multiplied maximum number times single invocation preempted higher priority tasks e theta note since technique computes worst case response times highest priority task lowest priority task worst case response times r 1 available r computed summarize lee et als technique problem computing safe upper bound formulated linear programming problem objective function value maximized satisfying two constraints side note linear programming increasingly used realtime research area strong theoretical ground example used bound worst case execution time task 20 bound interference program execution dma operation 21 bound number retries lockfree realtime systems 22 iii overall approach one problem lee et als technique preemption cost task fixed regardless tasks execute tasks preemption may result severe overestimation cacherelated preemption delay cache blocks shared among tasks example cache blocks used preempted task used tasks execute preemption disjoint preemption cost particular preemption would zero nevertheless lee et als technique assumes preemption cost still time needed reload useful cache blocks preempted task address problem technique proposed paper takes account relationship preempted task set tasks execute preemption computing preemption cost purpose proposed technique categorizes preemptions task number disjoint groups according tasks execute preemption number disjoint groups k higher priority tasks example three higher priority tasks 1 2 3 lower priority task 4 number possible preemption scenarios 4 7 corresponding f 1 g f 2 g f 3 g f according set tasks execute 4 preemption task j denote p j gamma1 set possible preemption scenarios higher priority tasks set p j gamma1 equal power set 23 set f excluding empty set since task j preempted least one higher priority task must involved addition denote p j h preemption task j tasks set h execute example denotes preemption 4 tasks 1 preemption costs tasks different preemption scenarios given following augmented preemption cost table example blocks u u u u u u u u u u c 6 c 7 execution points cache blocks u fig 2 calculation scenariosensitive preemption cost compute f j h preemption cost scenario p j h following three steps taken based information set useful cache blocks obtained analysis explained 18 first execution point task j compute intersection set useful cache blocks j execution point set cache blocks used tasks h second determine execution point j largest elements ie useful cache blocks intersection finally compute worst case preemption cost preemption scenario multiplying number useful cache blocks intersection cache refill time example consider fig 2 shows set useful cache blocks denoted us figure execution points lower priority task 4 sets cache blocks used higher priority tasks 1 2 3 example worst case preemption cost 4 case tasks 1 3 execute preemption ie f 4 f multiplied cache refill time preemption cost determined execution point shaded figure largest number useful cache blocks conflict cache blocks used 1 3 since preemption scenarios k higher priority tasks need compute number preemption costs worst case may require enormous amount computation k large computational requirement reduced substantially noting need consider higher priority tasks whose cache blocks conflict cache blocks used task preemption cost computed example fig 2 since none cache blocks used conflict used 4 need consider preemption scenarios include 2 computing preemption costs 4 instead preemption costs scenarios include 2 derived include 2 noting problem formulation formulate problem computing safe upper bound pc r linear programming problem based augmented preemption costs f j hs define new variable j h denotes number preemptions j task set h number preemptions scenario p j h corresponding objective function objective function states cacherelated preemption delay r sum delay due preemptions higher priority tasks r delay due preemptions task defined sum counts mutually disjoint preemption scenarios task multiplied corresponding preemption costs lee et als technique cannot determine exact g j h values give worst case preemption delay thus use various constraints g j hs bound objective function value next subsection give two constraints extensions lee et als original constraints section c discuss advanced constraint relates invocations higher priority task preemptions lower priority tasks higher priority task involved section iv give advanced constraints consider phasing tasks eliminate many infeasible task preemption scenarios finally section v discuss optimization reduces computational requirement proposed technique b extensions lee et als constraints subsection describes extensions based scenariosensitive preemption cost two constraints used lee et als technique extended constraints given terms g j hs see later subsume lee et als original two constraints first constraint lee et als technique states total number preemptions r cannot larger total number invocations r straightforwardly extended using g j hs follows g k h note since k h equal g k constraint equivalent first constraint lee et als technique similarly second constraint originally states total number preemptions j r cannot larger number invocations j r multiplied maximum number times single j invocation preempted higher priority tasks extended follows e theta r j constraint states number preemptions j higher priority executes bounded number invocations j multiplied maximum number times single j invocation preempted k show constraint subsumes second constraint lee et als technique sum sides constraints e theta j h j h e theta shows new constraint subsumes second constraint lee et als technique addition since number preemptions j higher priority task k executes bounded number k invocations combining constraints 4 5 e theta since new constraints described subsection either equivalent stringent original two constraints lee et als technique f j h always less equal f j h resulting objective function value always smaller equal objective function value lee et als tech cache mapping b preemption cost table c task invocations task r 4 fig 3 example task set nique yielding tighter prediction cacherelated preemption delay example consider task set fig 3 consists four tasks 1 2 3 highest priority task 4 lowest one assume tasks mapped cache memory shown fig 3a useful cache blocks tasks 1 denoted numbers 1 2 3 4 respectively cache mapping distribution useful cache blocks tasks give preemption cost table fig 3b assuming cache refill time single cycle 4 assume interested computing cacherelated preemption delay response time task 4 denoted r 4 fig 3c also assume r 4 4 example simplify explanation assume set useful cache blocks task shown fig 3a includes set useful cache blocks execution point task assumption hold general example fig 2 illustrates four invocations 1 three invocations 2 two invocations 3 whose response times denoted figure r 1 r 2 r 3 respectively note response times available compute r 4 since calculate response times highest priority task lowest priority task first constraint ie constraint 3 following three inequalitiesx r 4 similarly second constraint ie constraint 6 following inequalities e theta r 4 g 3 e theta r 3 g 3 e theta r 3 g 4 mind r 4 e theta r 4 g 4 mind r 4 e theta r 4 g 4 mind r 4 e theta r 4 2 maximum objective function value satisfies two sets constraints values g j hs give maximum follows comparison purposes lee et als technique used instead maximum objective function value would 54 significantly larger given proposed technique maximum objective function value derived preemption costs f determined number useful cache blocks tasks shown fig 3a note solution corresponds case nine invocations tasks 1 2 3 preempt task largest preemption cost constraints used arex r 4 r 4 e theta r 4 e theta r 3 e theta r 4 c advanced constraints relationship task invocations preemption although new constraints stringent lee et als technique cannot eliminate infeasible preemption scenarios fact even combination g j h values gives maximum objective function value previous example infeasible since requires least eight invocations 1 whereas four invocations 1 example cf fig 3c among eight required invocations four invocations g 3 f 1 meaning four preemptions 3 1 executes four required invocations g 4 f 1 four preemptions 4 1 executes reason earlier constraints cannot eliminate infeasible preemption scenario cannot relate invocations higher priority task preemptions lower priority tasks higher priority task involved example fig 3 trivially shown sum number preemptions 2 3 1 executes bounded number 1 invocations giving following constraint eliminates infeasible preemption scenario first sight appears problem solved bounding number preemptions lower priority tasks higher priority task executes number invocations higher priority task j expressed following constraint g k h r constraint cast example fig 3 translated following constraint 1 higher priority task involved particular constraint constraint 7 general however safe meaning valid preemption scenario may satisfy single invocation higher priority task involved one preemption lower priority tasks thus counted multiple times summation lefthand side constraint 7 example 3 preempted 2 2 turn preempted 1 invocation doubly counted first g 2 f 1 g second g 3 f general invocation k doubly counted g j h g capture observation symmetric relation 23 call dc standing doubly counted relation denoted dc relation associates two preemption scenarios g example four tasks 1 2 3 4 possible pairs preemption scenarios related dc follows using relation safe constraints derived follows consider combination preemption scenarios higher priority task involved pair preemption scenarios combination related dc sum number preemptions combination bounded number invocations higher priority task example following constraint eliminated infeasible preemption scenario safe since pairs preemption scenarios appear lefthand side related dc hand following constraint safe p 3 f related dc e set possible safe constraints derived rule follows higher priority task involved r 4 r 4 e maximum number preemptions preemptions b minimum number r r fig 4 example infeasible task phasing constraints cases higher priority task involved 2 3 derived similarly iv advanced constraints task phasing among two problems lee et als technique explained introduction first problem addressed previous section introducing scenariosensitive preemption cost section addresses second problem namely problem technique consider phasing among tasks thus may allow many infeasible preemption scenarios example technique assumes number preemptions lower priority task higher priority task involved potentially range zero number invocations higher priority task however fig 4a illustrates invocations higher priority task denoted j figure cannot involved preemption lower priority task denoted k figure even assume worst case response time denoted r k figure lower priority task similarly fig 4b illustrates invocations higher priority task inevitably involved preemptions lower priority task even assume best case response time b k lower priority task section incorporate constraints others task phasing framework developed previous section first define following four numbers two tasks j k j priorities higher worst case response time r jk n 0 jk let set intervals length r hyperperiod formed j k lcmt least common multiple number jk maximum number preemptions lower priority higher priority task j executes intervals similarly n jk minimum number preemptions lower priority task k higher priority task j executes set intervals hand 0 jk maximum number times instances lower priority task k overlapped instance higher priority task j technically maximum number levelk busy periods 24 j k intervals finally n 0 jk minimum number times instances lower priority task k overlapped instance higher priority task j ie minimum number levelk busy periods j k intervals assume worst case response times j k r j r k respectively available compute r likewise assume best case response times j k b j b k respectively best case execution times used four numbers given follows min x1 min x1 derivation lengthy presented interested readers referred extended version paper 25 first two numbers jk n jk used bound number preemptions two numbers 0 jk n 0 jk used bound number preemptions certain types first 0 jk used bound number preemptions execute course without multiply counted using technique explained previous section hand n jk used bound number preemptions either j k executes example number preemptions j executes k bounded r jk likewise number preemptions k executes j bounded jk following give examples constraints use 0 jk n 0 jk assume four tasks 1 2 3 4 1 2 correspond j k constraints respectively example ten possible preemption scenarios 3 among three preemption scenarios 1 2 execute related dc thus subject multiply counted one participate summation number preemptions restriction leads following two inequalities similarly three preemption scenarios 1 executes 2 related dc following two inequalities finally three preemption scenarios 2 executes 1 related dc following two inequalities v optimization based task set decomposition one potential problem proposed technique requires large amount computation large number tasks since number variables used o2 n n number tasks task set section discusses simple optimization based task set decomposition drastically reduce amount fig 5 example task decomposition computation required consider example fig 5 shows cache blocks used four tasks 4 figure notice although cache blocks shared 1 2 also 3 4 overlap cache blocks used 1 2 used 3 4 means neither 1 2 affects cacherelated preemption delay either 3 4 vice versa based observation decompose given task set collection subsets way two tasks two different subsets share cache block tasks subset analyzed independently tasks subsets using constraints given previous two sections example fig 5 given task set decomposed two subsets g calculate worst case response time lowest priority task 4 using iterative procedure explained section ii tasks one subset analyzed independently tasks subset two results combined follows r k1 r k r k r k 4 cacherelated preemption delay 4 due interactions shared cache blocks computed maximizex constraints involving 1 2 similarly pc 2 r k 4 cacherelated preemption delay 4 due interactions 3 4 computed maximizex constraints involve 3 4 maximize benefit optimization explained number subsets analyzed independently large interesting topic future research devise scheme allocates main memory tasks resulting cache mapping gives large number subsets vi experimental results section compare worst case response time prediction proposed technique previous techniques using sample task set target machine idt7rs383 board 20 mhz r3000 risc cpu r3010 fpa floating point accelerator instruction cache data cache 16 kbytes caches direct mapped block sizes 4 bytes sram static ram used target machines main memory cache refill time 4 cycles task set specification task period wcet unit cycles experiment used sample task set whose specification given table table first column lists tasks task set four tasks used experiments fft lud lms fir fft task performs fft inverse fft operations array 8 floating point numbers using cooleytukey algorithm 26 simultaneous linear equations doolittles method lu decomposition 27 fir implements 35 point finite impulse response fir filter 28 generated signal finally lms 21 point adaptive fir filter filter coefficients updated input signal 28 table also gives period wcet task second third columns respectively since target machine uses sram main memory cache refill time 4 cycles much smaller current computer systems range 8 cycles 100 cycles dram used main memory 11 obtain wcet task realistic cache refill times divide wcet two components first component execution time task memory references cache hits independent cache refill time measured target machine executing task code data preloaded cache second component time needed service cache misses occur tasks execution dependent cache refill time component computed multiplying total number cache misses cache refill time ref ill experiment total number cache misses obtained following procedure 1 two different execution times measured task one code data preloaded cache without preloading denoted 2 dividing difference 1 2 4 cycle cache refill time target machine computed total number cache misses tasks execution used three different cache mappings code used four tasks shown lud lms fft cache mapping 1 cache mapping 2 cache mapping 3 fig 6 three different cache mappings tasks fig 6 first mapping code used task mapped cache region hand second mapping cache regions used tasks overlapped 70 finally third mapping code used task mapped disjoint region cache speculate three mappings represent reasonably well spectrum possible overlap among cache regions used tasks ii gives preemption cost tables three mappings note preemption costs tasks decrease overlapping cache regions decrease less useful cache blocks displaced preemption eventually cache regions disjoint preemption costs zero used publicdomain linear programming tool called lp solve michel berkelaar url ftpftpeseletuenlpublp solve solve linear programming problem posed proposed technique total number constraints task set took less 3 minutes user cpu time 5 minutes system cpu time compute data points presented section proposed technique axil ii preemption cost tables three cache mappings preemption cost table cache mapping 1 unit cycles preemption cost table cache mapping 2 unit cycles preemption cost table cache mapping 3 unit cycles workstation running sunos 54 50 mhz supersparc cpu ti tms390z80 128 mbyte main memory experiments also implemented simple fixedpriority scheduler based tick scheduling explained 7 implementation scheduler invoked every 160000 cycles take account overhead associated scheduler used analysis technique explained 7 technique scheduler overhead response time r given number scheduler invocations r number times scheduler moves task delay queue tasks wait next invocations run queue r ffl c int time needed service timer interrupt measured 413 cycles target machine ffl c ql time needed move first task delay queue run queue measured 142 cycles target machine ffl c qs time needed move additional task delay queue run queue measured 132 cycles target machine detailed explanation equation beyond scope paper interested readers referred 7 figs 7a b show predicted worst case response time lowest priority task 4 percentage cacherelated preemption delay worst case response time respectively cache refill time increases 4 cycles 200 cycles three different techniques used predict worst case response time first technique proposed paper 1 2 3 predictions three different cache mappings explained earlier second c technique explained 17 assumes cache block used preempting task replaces cache memory block needed preempted task finally p lee et als technique presented 18 preemption cost assumed time needed reload useful cache blocks note unlike proposed technique worst case response time predictions c p insensitive cache mapping since preemption costs assumed independent cache mapping cache refill time10000000worst case response time m3 cache refill time05cacherelated preemption worst case response m3 b fig 7 worst case response time cacherelated preemption delayworst case response time vs cache refill time results fig 7a show proposed technique gives significantly tighter prediction worst case response time previous techniques example cache refill time 100 cycles second cache mapping used proposed technique gives worst case response time prediction 60 tighter best previous approaches 5323620 cycles 2 vs 13411402 cycles p superior performance proposed technique becomes evident cache regions used tasks become less overlapped move 1 3 fig 7a jumps worst case response time predictions three techniques jumps occur increase worst case response time due increased cache refill time causes additional invocations higher priority tasks resulting number bumps fig 7b results fig 7a also show cache refill time increases gap increases worst case response time prediction two tech cache mapping 1 cache mapping 2 cache mapping 36000000worst case response time cache refill cache mapping 1 cache mapping 2 cache mapping 36000000worst case response time cache refill b fig 8 impact different constraint groups accuracy worst case response time prediction niques eventually task set deemed unschedulable c p cache refill time 90 100 cycles respectively hand task set schedulable even cache refill time 200 cycles cache mapping 3 used finally results fig 7b show cache refill time increases cacherelated preemption delay takes proportionally large percentage worst case response time result even method cacherelated preemption delay takes 30 worst case response time cache refill time 100 cycles cache mapping 2 used indicates accurate prediction cacherelated preemption delay becomes increasingly important cache refill time increases current trend widening speed gap processor main memory continues 11 assess impact various constraints used proposed technique accuracy resultant worst case response time prediction classified constraints two groups calculated reduction worst case response time prediction group constraint sets classified follows three constraints section iii deal scenariosensitive preemption cost classified group 1 whereas section iv eliminate infeasible task phasing classified group 2 figs 8a b show reduction worst case response time prediction two constraint groups applied cache refill times 60 cycles 80 cycles respectively comparison purposes also give worst case response time prediction technique results show cache refill times cache regions used tasks completely overlapped ie cache mapping 1 reduction comes constraints group 2 since case scenariosensitive preemption cost degenerates preemption cost used technique p however cache regions used tasks become less overlapped impact constraints group 1 becomes significant eventually cache regions disjoint reduction comes constraints group 1 alone since case scenariosensitive preemption costs zero performed experiments using number task sets results similar given section interested readers referred 25 results task sets presented vii conclusion paper proposed enhanced schedulability analysis technique analyzing cacherelated preemption delay required cache memory used multitasking realtime systems proposed technique uses linear programming following two novel features expressed terms constraints linear programming first technique takes account relationship preempted task set tasks execute preemption calculating number memory blocks reloaded cache preempted task resumes execution second technique considers phasing tasks eliminate many infeasible task interactions experimental results showed incorporation two features yields 60 accurate prediction worst case response time compared prediction made previous techniques results also showed cache refill time increases gap increases worst case response time prediction proposed technique previous techniques finally results showed cache refill time increases cacherelated preemption delay takes proportionally large percentage worst case response time indicates accurate prediction cacherelated preemption delay becomes increasingly important current trend widening speed gap processor main memory continues acknowledgments authors grateful sam h noh helpful suggestions comments earlier version paper r results earliest deadline scheduling al gorithm finding response times realtime system rate monotonic scheduling algorithm exact characterization average case behavior scheduling algorithms multiprogramming hard realtime environment dynamic scheduling hard realtime tasks realtime threads extendible approach analyzing fixed priority hard realtime tasks effective analysis engineering realtime fixed priority schedulers impact ada runtime systems performance characteristics scheduling models accounting interrupt handling costs dynamic priority task systems engineering analysis fixed priority schedulers computer architecture quantitative approach smart strategic memory allocation realtime cache design oscontrolled cache predictability realtime systems compiler support softwarebased cache partitioning softwarebased cache partitioning realtime applications cache issues realtime systems adding instruction cache effect schedulability analysis preemptive realtime systems analysis cacherelated preemption delay fixedpriority preemptive scheduling efficient microarchitecture modeling path analysis realtime software method bounding effect dma io interference program execution time framework implementing objects scheduling tasks lockfree realtime systems science research associates fixed priority scheduling periodic task sets arbitrary dead lines bounding cacherelated preemption delay realtime systems dftfft convolution algorithm theory elementary numerical analysis c algorithms realtime dsp tr compilers principles techniques tools results earliest deadline scheduling algorithm dynamic scheduling hard realtime tasks realtime threads extendible approach analyzing fixed priority hard realtime tasks c language algorithms realtime dsp compiler support softwarebased cache partitioning analysis cacherelated preemption delay fixedpriority preemptive scheduling computer architecture 2nd ed scheduling algorithms multiprogramming hardrealtime environment elementary numerical analysis engineering analysis fixed priority schedulers effective analysis engineering realtime fixed priority schedulers impact ada runtime systems performance characteristics scheduling models adding instruction cache effect schedulability analysis preemptive realtime systems oscontrolled cache predictability realtime systems efficient microarchitecture modeling path analysis realtime software analysis cacherelated preemption delay fixedpriority preemptive scheduling method bounding effect dma io interference program execution time framework implementing objects scheduling tasks lockfree realtime systems ctr jaudelice c de oliveira caterina scoglio ian f akyildiz george uhl new preemption policies diffservaware traffic engineering minimize rerouting mpls networks ieeeacm transactions networking ton v12 n4 p733745 august 2004 accounting cacherelated preemption delay dynamic priority schedulability analysis proceedings conference design automation test europe april 1620 2007 nice france hemendra singh negi tulika mitra abhik roychoudhury accurate estimation cacherelated preemption delay proceedings 1st ieeeacmifip international conference hardwaresoftware codesign system synthesis october 0103 2003 newport beach ca usa jan staschulat rolf ernst scalable precision cache analysis preemptive scheduling acm sigplan notices v40 n7 july 2005 chanik park jaeyu seo sunghwan bae hyojun kim shinhan kim bumsoo kim lowcost memory architecture nand xip mobile embedded systems proceedings 1st ieeeacmifip international conference hardwaresoftware codesign system synthesis october 0103 2003 newport beach ca usa jan staschulat rolf ernst multiple process execution cache related preemption delay analysis proceedings 4th acm international conference embedded software september 2729 2004 pisa italy