selecting examples partial memory learning paper describes method selecting training examples partial memory learning system method selects extreme examples lie boundaries concept descriptions uses examples new training examples induce new concept descriptions forgetting mechanisms also may active remove examples partial memory irrelevant outdated learning task using implementation method conducted lesion study direct comparison examine effects partial memory learning predictive accuracy number training examples maintained learning experiments involved stagger concepts synthetic problem two realworld problems blasting cap detection problem computer intrusion detection problem experimental results suggest partial memory learner notably reduced memory requirements slight expense predictive accuracy tracked concept drift well learners designed task b introduction partial memory learners online systems select maintain portion past training examples use together new examples subsequent training episodes systems learn memorizing selected new facts using selected facts improve current concept descriptions derive new concept descriptions researchers developed partial memory systems less susceptible overtraining learning concepts change drift compared learners use memory models salganicoff 1993 maloof 1996 widmer kubat 1996 widmer 1997 key issues partial memory learning systems select relevant examples input stream maintain use future learning episodes decisions affect systems predictive accuracy memory requirements ability cope changing concepts selection policy might keep training example arrives maintenance policy forgets examples fixed period time policies strongly bias learner toward recent events consequence system may forget important rarely occurring events alternatively system may attempt select proto maloof ryszard michalski typical examples keep indefinitely case learner strongly anchored past may perform poorly concepts change drift paper presents method selecting training examples partial memory learner approach extends previous work using induced concept descriptions select training examples lie extremities concept boundaries thus enforcing boundaries system retains uses examples subsequent learning episodes approach stores nonconsecutive collection past training examples needed situations important training events occur necessarily reoccur input stream forgetting mechanisms may active remove examples partial memory longer enforce concept boundaries become irrelevant learning task new training examples arrive boundaries current concept descriptions may change case training examples lie boundaries change result contents partial memory change continues throughout learning process surveying relevant work present general framework partial memory learning describe implementation learner called aqpartial memory aqpm based aq15c inductive learning system wnek kaufman bloedorn michalski 1995 present results lesion study kibler langley 1990 examined effects partial memory learning predictive accuracy memory requirements also make direct comparison ib2 aha kibler albert 1991 since similar spirit aq pm applying method stagger concepts schlimmer granger 1986 synthetic problem two realworld problemsthe problems blasting cap detection xray images maloof michalski 1997 computer intrusion detection maloof michalski 1995experimental results showed significant reduction number examples maintained learning expense predictive accuracy unseen test cases aqpm also tracks drifting concepts comparably stagger schlimmer granger 1986 flora systems widmer kubat 1996 2 partial memory learning online learning systems must memory model dictates treat past training examples three possibilities exist reinke michalski 1988 1 full instance memory learner retains past training examples 2 partial instance memory retains past training examples 3 instance memory retains none researchers studied described learning systems type memory model example stagger schlimmer granger 1986 winnow lit tlestone 1991 learning systems instance memory gem reinke michalski 1988 ib1 aha et al 1991 learners full instance mem ory systems partial instance memory appear least studied examples include lair elio watanabe 1991 hillary iba woogulis langley 1988 ib2 aha et al 1991 darling salganicoff 1993 aqpm maloof michalski 1995 flora2 widmer kubat 1996 metalb widmer 1997 online learning systems must also policies deal concept memory refers store concept descriptions researchers investigated variety strategies conjunction different models instance memory example ib1 aha et al 1991 maintains past training examples store generalized concept descriptions contrast gem reinke michalski 1988 keeps past training examples addition set concept descriptions form rules id5 utgoff 1988 iti utgoff berkman clouse 1997 store training examples leaf nodes decision trees also examples systems full instance memory actually id5 stores subset examples attribute values leaves interesting special case full instance memory finally example system instance memory id4 schlimmer fisher 1986 uses new training example incrementally refine decision tree discarding instance systems concept memory learning occur either incremental mode temporal batch mode incremental learners modify adjust current concept descriptions using new examples input stream learner also maintains instance memory uses examples augment arriving environment flora2 flora3 flora4 widmer kubat 1996 examples systems learn incrementally aid partial instance memory temporal batch learners hand replace concept descriptions new ones induced new training examples input stream held instance memory darling salganicoff 1993 aqpm examples temporal batch learners partial instance memory batch learning algorithm c45 quinlan 1993 cn2 clark niblett 1989 used conjunction full instance memory however choice depends greatly problem hand figure 1 displays classification selected learning systems based concept memory various types instance memory described role instance concept memory learning discuss partial instance memory learning systems appeared literature following sections focus learning systems instance memory thus sake brevity drop term instance referring systems example use term partial memory mean partial instance memory 21 survey partial memory learning systems appears one first partial memory systems sense minimal partial memory model 4 marcus maloof ryszard michalski partial instance memory full instance memory memory instance partial instance memory full instance memory memory instance partial instance memory full instance memory concept memory concept memory online learning systems temporal batch incremental gem aq15c aq15c winnow hillary id4 iti figure 1 learning systems classified concept instance memory system keeps first positive example consequently always learns one positive example partial memory arriving training examples hillary iba et al 1988 maintains collection recent negative examples partial instance memory positive examples may added concept description disjuncts generalized subsequent learning steps hillary retains negative examples concept description covers otherwise specializes concept description negative examples retained may forgotten later covered positive concept description ib2 aha et al 1991 instancebased learning method uses scheme like aqpm keeps nonconsecutive sequence training examples memory ib2 receives new instance classifies instance using examples currently held memory classification correct instance discarded conversely classification incorrect instance retained intuition behind instance correctly classified gain nothing keeping scheme tends retain training examples lie boundaries concepts ib3 extends ib2 adding mechanisms cope noise darling salganicoff 1993 uses proximitybased forgetting function opposed timebased frequencybased function algorithm initializes weight new example one decays weights examples within neighborhood new example examples weight falls thresh old removed darling also example partial memory learning system since forgets examples maintains portion past training examples flora2 system widmer kubat 1996 selects consecutive sequence training examples input stream uses timebased scheme forget examples partial memory older threshold set adaptively system designed track drifting concepts periods system performing well increases size window keeps examples change performance presumably due change target concepts system reduces size window forgets old examples accommodate new examples new target concept systems concept descriptions begin converge toward target concepts size window increases number training examples maintained partial memory flora3 extends flora2 adding mechanisms cope changes con text change seasons instance changing context concept warm different summer winter temperature contextual variable governs warm concept appropriate flora4 extends flora3 adding mechanisms coping noise similar used ib3 aha et al 1991 finally metalb metalib systems widmer 1997 based naive bayes instancebased learning algorithms respectively systems like flora3 widmer kubat 1996 cope changes context use partial memory mechanisms maintain linear sequence training examples fixed window time algorithm identifies context uses examples window relevant context metalib uses additional mechanisms exemplar selection exemplar weighting concentrate relevant examples window favorit system krizakova kubat 1992 kubat krizakova 1992 extends unimem lebowitz 1987 uses mechanisms aging forgetting nodes decision tree although favorit instance memory include discussion aging forgetting mechanisms important partial memory learners system uses third type forgetting frequencybased forgetting favorit uses incoming training examples add nodes strengthen existing nodes decision tree time aging mechanisms gradually weaken strengths nodes incoming training examples reinforce nodes presence tree nodes score decays falls threshold point algorithm forgets removes node conversely incoming training examples continue reinforce revise node score increases score surpasses upper threshold nodes score fixed remains 22 general framework partial memory learning based analysis systems design aqpm developed general algorithm inductive learning partial instance memory presented 6 marcus maloof ryszard michalski 1 learnpartialmemorydata 2 concepts 0 3 partialmemory 4 n 5 missed findmissedexamplesconcepts data partialmemory missed 7 concepts 8 trainingset 0 concepts 9 partialmemory concepts 11 end learnpartialmemory table 1 algorithm partial memory learning table 1 algorithm begins data source supplies training examples distributed time represented data temporal counter generalize usual assumption single instance arrives time placing restrictions cardinality data allowing consist zero training examples criterion important ultimately determines structure time learner 1 allowing data empty learner track passage time since passage time longer associated explicit arrival training examples allowing data consist one training examples learner model arbitrary periods time eg days weeks without requiring specific number training examples arrive interval intuitively may day system learns one thing simply learns something else mean another day passed initially learner begins concepts training examples partial memory steps 2 3 although may possess arbitrary amount background knowledge first learning step 1 partial memory learner behaves like batch learning system since concepts examples partial memory training set consists examples data 1 uses set induce initial concept descriptions step 7 subsequently system must determine training examples retain partial memory steps 8 9 subsequent time steps 1 system begins determining new training examples misclassifies step 5 system uses examples examples partial instance memory learn new concept descriptions step 7 seen review related systems several ways accomplish system could simply memorize new examples training set could also induce new concept descriptions examples finally could use examples training set modify alter existing concept descriptions form new concept descriptions precise way particular learner determines misclassified examples step 5 learns step 7 selects examples retain step 8 maintains examples step depends concept description language learning meth ods employed task hand therefore ground discussion describe aqpm learning system 3 description aqpm learning system aqpm online learning system maintains partial memory past training examples implement aqpm extended aq15c inductive learning system wnek et al 1995 begin describing system delving details aqpm aq15c represents training examples using restricted version attributional language vl 1 michalski 1980 rule conditions form attribute reference attribute attribute used represent domain objects reference list attribute values rule condition true attribute value instance condition matched reference decision rules form expression form rule condition assigns decision decision variable implication operator c conjunction rule conditions conditions conjunction true expression evaluated returned decision also represent training instances vl 1 restricting cardinality reference one view training instances vl 1 rules conditions references consisting single values performance element aq15c consists routine flexibly matches instances vl 1 decision rules decision rules carve regions representation space leaving space uncovered instance falls uncovered region space using strict matching technique system would classify instance unknown important applications flexible matching involves computing degree match instance decision rule compute metric variety ways experiments discussed computed degree match follows 2 decision class consisting n conjunctions c j degree match instance given 1 number conditions c j satisfied instance fi ij total number conditions c j yields number range 0 1 expresses proportion conditions rule instance matches value zero means match value one means complete match flexible matching routine 8 marcus maloof ryszard michalski returns decision label class highest degree match degree match falls certain threshold routine may report unknown match learn set decision rules aq15c uses aq algorithm michalski 1969 covering algorithm briefly aq randomly selects positive training example known seed algorithm generalizes seed much possible given constraints imposed negative examples producing decision rule default mode operation positive training examples covered rule removed consideration process repeats using remaining positive examples covered implement aqpm extended aq15c incorporating features outlined partial memory algorithm table 1 aqpm finds misclassified training examples flexibly matching current set decision rules examples data step 5 missed examples grouped examples currently held partial memory step passed learning algorithm step 7 like aq15c aqpm uses aq algorithm induce set decision rules training examples meaning aqpm operates temporal batch mode form new contents partial memory step 8 aqpm selects examples current training set using syntactically modified characteristic decision rules derived new concept descriptions discuss section 31 finally aqpm may use variety maintenance policies step 9 like timebased forgetting aging inductive support activated setting parameters 31 selecting examples one key issues partial memory learners deciding new training examples select retain mechanisms maintain examples also important examples held partial memory may longer useful could due fact concepts changed drifted system initially thought crucial concept longer important represent explicitly since current concept descriptions implicitly capture information returning aqpm used scheme selects training examples lie boundaries generalized concept descriptions call examples extreme examples aqpm decision rule axisparallel hyperrectangle discrete ndimensional space n number attributes used represent domain objects therefore extreme examples could lie surfaces edges corners hyperrectangle covering study chose middle ground retained examples lay edges hyperrectangle although considered implemented schemes retaining examples maloof 1996 referring figure 2 see portion discrete version iris data set 1936 took original data set uci machine learning repository blake keogh merz 1998 produced discrete version using scale implementation bloedorn wnek michalski kaufman 1993 setosa example versicolor example figure 2 visualization setosa versicolor training examplesplsetosa example versicolor example010101010101010101010101010101010101010101010101010101010101010101010101010101010100110101010101 sw setosa concept versicolor concept figure 3 visualization setosa versicolor concept descriptions overlain training examples chimerge algorithm kerber 1992 shown examples versicolor setosa classes example represented four attributes petal length pl petal width pw sepal length sl sepal width sw find extreme boundary training examples aqpm uses characteristic decision rules specify common attributes domain objects class michalski 1980 rules consist domain attributes maloof ryszard michalskiplsw setosa example versicolor example figure 4 visualization setosa versicolor extreme examples values objects represented training set form tightest possible hyperrectangle around cluster examples returning example figure 3 shows characteristic rules induced training examples pictured figure 2 aqpm syntactically modifies set characteristic rules match examples lie boundaries uses strict matching technique select extreme examples although aqpm uses characteristic rules select extreme examples use types decision rules eg discriminant rules classification figure 4 shows examples retained selection algorithm examples lie edges hyperrectangles expressed characteristic decision rules theorem 1 states upper bound number examples retained aq pm lesioned counterpart lesioned version aqpm describe formally next section equivalent temporal batch learning system full instance memory best case partial memory learner retain fewer training examples lesioned counterpart multiplicative factor worst case lower bound number examples maintained partial memory learner equal lesioned learner follows proof theorem 1 occurs training set consists examples lie edges characteristic concept description theorem 1 characteristic decision rule c induced training examples drawn ndimensional discrete representation space number training examples retained partial memory learner jreference k lesioned counterpart retain jreference k j proof let c characteristic decision rule induced training examples drawn ndimensional discrete representation space let c k kth condition c definition following three numerically equivalent 1 dimensionality n 2 number conditions c 2 c 3 number attributes forming partial memory learner c k expresses kth dimension hyperrectangle match jreference k j training examples along edge kth dimension furthermore c k corresponds 2 ngamma1 edges kth dimension hyperrectangle realized c therefore number training examples matched c k compute number would overcount training examples lie corners hyperrectangle therefore must subtract two training examples lie endpoints edge hyperrectangle yielding jreference k undercounts number training examples excludes training examples lie corners since 2 n corners ndimensional hyperrectangle total number examples matched lesioned learner attribute value training example map corresponding value condition c k definition characteristic concept description set training examples attribute result condition c k number attribute values conditions reference equal number unique values attribute takes therefore number training examples maintained lesioned learner equal jreference k j 12 marcus maloof ryszard michalski 1 aqbldata 2 concepts 0 3 trainingset 4 n 5 trainingset 6 concepts 7 8 end aqbl table 2 algorithm lesioned version aqpm aqbaseline aqbl 32 forgetting mechanisms forgetting mechanisms important partial memory learners two reasons first learner selects examples lie boundaries concept descrip tions aqpm boundaries change reason retain old boundary examples new extreme examples important work enforcing concept boundary learner forget old ones second learner must deal concept drift forgetting mechanisms crucial removing irrelevant outdated examples held partial memory see experimental section concepts change suddenly learner must cope examples held partial memory previous target concept context new target concept many examples contradictory forgetting imperative aqpm two types forgetting explicit implicit explicit forgetting occurs examples partial memory meet specific userdefined cri teria current implementation aqpm uses timebased forgetting function remove examples partial memory older certain age implicit forgetting occurs examples partial memory evaluated deemed useless longer enforce concept boundaries computing partial memory basic algorithm table 1 evaluates training examples currently held partial memory misclassified current concept descriptions step 8 consequently repeatedly evaluates extreme examples determines still fall concept boundary gives rise implicit forgetting process learning algorithm generalizes concept description particular extreme example longer lies concept boundary forgets example call implicit forgetting process explicit criterion removing examples eg remove examples older fifty time steps r r r size red green blue shape color r r r size red green blue shape color r r r size red green blue shape color target concept time steps 139 b target concept time steps 4079 c target concept time steps 80120 figure 5 visualization stagger concepts 4 experimental results section present series experimental results lesion study ki bler langley 1990 used aqpm three problems produce lesioned version aqpm simply disabled partial memory mechanisms resulting system equivalent temporal batch learner full instance mem ory present learner formally table 2 refer aqbaseline aqbl also included ib2 aha et al 1991 sake comparison instancebased learner partial memory model first problem synthetic problem referred stagger con cepts schlimmer granger 1986 become standard benchmark testing learning algorithms track concept drift derived remaining two data sets realworld problems first problem entails detecting blasting caps xray images airport luggage maloof michalski 1997 second involves using learned profiles computing behavior intrusion detection maloof michalski 1995 chose realworld problems require online learning likely involve concepts change time ex ample computing behavior changes individuals move project project semester semester appearance visual objects also change due deformations objects changes environment ex periments independent variable learning algorithm dependent variables predictive accuracy number training examples maintained measures computed 95 confidence intervals also presented detailed results learning time concept complexity problems found elsewhere maloof 1996 14 marcus maloof ryszard michalski406080100 predictive accuracy time step aqbl figure 6 predictive accuracy aqpm aqbl ib2 stagger concepts 41 stagger concepts stagger concepts schlimmer granger 1986 synthetic problem target concept changes time three attributes describe domain ob jects size taking values small medium large color taking values red green blue shape taking values circle triangle rectangle conse quently 27 possible object descriptions ie events representation space presentation training examples lasted 120 time steps target concept changing every 40 steps target concept first 39 steps red next 40 time steps target concept final 40 time steps target concept size medium large visualization target concepts appears figure 5 time step single training example 100 testing examples generated randomly 3 results presented conducted 60 learning runs using ib2 aqpm aqbl lesioned version aqpm referring figure 6 see predictive accuracy results ib2 aqpm aqbl stagger concepts ib2 performed poorly first target concept worse final two 53sigma27 62sigma40 respectively conversely aqpm aqbl achieved high predictive accuracies first target concept 99sigma10 100sigma00 respectively however target concept changed time step 40 aqbl never able match partial memory learners predictive accuracy former burdened examples irrelevant new target concept experiment illustrates importance forgetting mechanisms aqpm less burdened past examples kept fewer examples memory forgot held memory fixed period time aqpms predictive accuracy second target concept examples maintained time step aqbl figure 7 memory requirements aqpm aqbl ib2 stagger concepts 89sigma338 aqbls 69sigma30 third target concept aqpm achieved 96sigma18 predictive accuracy aqbl achieved 71sigma382 predictive accuracy results aqpm comparable stagger schlimmer granger 1986 flora systems widmer kubat 1996 following exceptions first target concept aqpm converge quickly flora systems ultimately achieved similar predictive accuracy second target concept aqpms convergence like flora systems performed 10 worse test cases performance ie slope asymptote third target concept similar turning memory requirements shown figure 7 see partial memory learners aqpm ib2 maintained far fewer training examples aqbl without partial memory mechanisms baseline learner aqbl simply accumulated examples intuitively inefficient inadequate policy learning changing concepts yet ib2s predictive accuracy showed selection mechanisms alone enough taking closer look memory requirements aqpm ib2 figure 8 see number examples learner maintained increases example selection mechanisms overall ib2 maintained fewer training examples aqpm savings cannot mitigate poor predictive accuracy first 40 time steps instance learners accumulated examples achieved acceptable predictive accuracies number examples maintained stabilized concept changed time step 40 learners increased number examples held partial memory retain information new concept increases ib2s memory requirements occurred adds new examples misclassified examples currently held mem ory target concept changed new examples misclassified consequently added memory ib2 kept examples related previous target concept predictive accuracy suffered final examples maintained time step figure 8 memory requirements aqpm ib2 stagger concepts blasting caps figure 9 example xray image used experimentation target concept although aqpm also increased number examples held memory used explicit forgetting process remove outdated irrelevant training examples fixed period fifty time steps proved crucial learning concepts cannot compare aqpms memory requirements staggers since latter maintain past training examples indirectly compare one run flora2 widmer kubat 1996 recall size representation space stagger problem 27 examples time step 50 flora2 maintained 24 examples 89 representation space time step aqpm maintained 1011 examples average 37 representation space entire learning run flora2 kept average 15 examples 56 representation space aqpm hand maintained 66 examples average 24 space 42 blasting cap detection problem blasting cap detection problem involves detecting blasting caps xray images airport luggage maloof michalski 1997 66 training examples experiment derived 5 images varied amount clutter luggage position bag relative xray source figure 9 shows typical xray image collection positive negative examples blasting caps represented using 27 intensity shape positional attributes maloof duric michalski rosenfeld 1996 computed eleven attributes blob produced heavy metal explosive near center blasting cap also computed eleven attributes rectangular region produced blasting caps metal tube finally used five attributes capture spatial relationship blob rectangular region realvalued attributes scaled discretized using scale implementation bloedorn et al 1993 chimerge algorithm kerber 1992 4 15 relevant attributes selected using promise measure baim 1988 resulting attributes blob maximum intensity average intensity length bounding rectangle three measures compactness rectangular region selected attributes length width area standard deviation intensity three measures compactness finally remaining spatial attributes distance centroids blob rectangle component distance parallel major axis fitted ellipse randomly set aside 10 original data testing set remaining 90 partitioned randomly evenly 10 data sets ie data conducted experimental comparison ib2 aqpm lesioned version system aqbl learning run presented learners data tested resulting concept descriptions testing set making note predictive accuracy memory requirements conducted 100 learning runs randomly generated new test set new data sets data averaging performance metrics 100 runs figure shows predictive accuracy results blasting cap detection problem aqpms predictive accuracy consistently lower aqbls learned available training data time step learning stops time step 10 aqpms predictive accuracy 7 less lesioned learner aqbl 81sigma34 vs 88sigma28 ib2 perform well task ultimately achieved predictive accuracy 73sigma38 notable decrease memory requirements measured aqpms loss predictive accuracy shown figure 11 learning ceased time step 10 baseline learner maintained entire training set 61sigma00 examples partial memory learner kept 18sigma05 training examples average roughly 30 total number examples ib2 retained slightly examples partial memory aqpm 25sigma06 maloof ryszard michalski5565758595 predictive accuracy time step aqbl figure 10 predictive accuracy aqpm aqbl ib2 blasting cap detection examples maintained time step aqbl figure 11 memory requirements aqpm aqbl ib2 blasting caps detection problem 43 computer intrusion detection problem computer intrusion detection problem must learn profiles users computing behavior use profiles authenticate future behavior learning descriptions intrusion behavior problematic since adequate training data difficult impossible collect consequently chose learn profiles user assuming misclassification means users profile inadequate unauthorized person masquerading user question existing intrusion detection systems make assumption data experiment derived 11200 audit records collected 9 users 3 week period first parsed users computing activity predictive accuracy time step aqbl figure 12 predictive accuracy aqpm aqbl ib2 intrusion detection problem output unix acctcom command frisch 1995 sessions segmenting logouts periods idle time twenty minutes longer resulted 239 training examples selected seven numeric audit metrics cpu time real time user time characters transferred blocks read cpu factor hog factor next represented seven numeric measures session time series using maximum minimum average values following davis davis 1981 21 real integer attributes scaled discretized using scale implementation bloedorn et al 1993 chimerge algorithm kerber 1992 finally using promise measure baim 1988 selected 13 relevant attributes average maximum real time average maximum system time average maximum user time minimum average characters transferred average blocks transferred average maximum cpu factor average maximum hog factor experimental design problem identical one used blasting cap problem referring figure 12 see predictive accuracy results aqpm aqbl ib2 intrusion detection problem aqpms predictive accuracy slightly lower aqbls learning stopped time step 10 aqpms accuracy 88sigma16 aqbls 93sigma12 difference 5 ib2 fared much better problem previous ones learning ceased ib2s predictive accuracy slightly better aq pms 89sigma13 although result statistically significant p 05 figure 13 shows memory requirements learner problem aqpm maintained notably fewer training examples lesioned counterpart learning ceased time step 10 baseline learner aqbl maintained examples aqpm maintained 64sigma10 training examples roughly 29 total number examples ib2 maintained even fewer examples aqpm learning stopped ib2 held roughly 52sigma08 examples partial memory slightly fewer number held aqpm maloof ryszard michalski50150250 examples maintained time step aqbl figure 13 memory requirements aqpm aqbl ib2 intrusion detection problem 44 summary lesion study comparing aqpm aqbl suggested mechanisms selecting extreme examples notably reduced number instances maintained partial memory expense predictive accuracy concepts changed aq pm relied forgetting mechanisms remove outdated irrelevant examples held memory recall aqpm use two types forgetting implicit explicit explicit mechanisms proved crucial stagger concepts implicit forgetting mechanisms general little effect issue explore next section direct comparison ib2 using stagger concepts illustrated importance forgetting policies apparent example selection mechanisms alone guarantee acceptable predictive accuracy concepts changed hand concepts stable case computer intrusion detection blasting cap detection problems forgetting mechanisms played less important role selection mechanisms moreover predict differences performance aqpm ib2 problems due inductive bias rather limitation ib2s example selection method would explain ib2 performed well intrusion detection problem performed poorly blasting cap detection problem indeed aqpm ib2 used similar selection methods experimental results showed maintained roughly number examples memory regarding indirect comparison flora systems aqpm performed well two three stagger concepts appears maintained fewer training examples partial memory difference memory requirements due learners selected examples input stream flora systems kept sequence examples varying length input stream result partial memory likely contained duplicate examples would partial memory learning 21 especially true problem like stagger concepts randomly draw 120 examples representation space consisting 27 domain objects conversely aqpm retained examples lay boundaries concept descriptions consequently would retain duplicate examples examples interior concept claim aqpm able achieve comparable accuracy maintaining fewer examples partial memory selected examples enforced concept boundaries hence high utility two systems use different concept description languages aqpm uses vl 1 capable representing dnf concepts whereas flora systems use conjunctive description language however upon analyzing stagger concepts concluded unlikely representational bias accounted differences similarities predictive accuracy noted aqpm fare well flora systems second three stagger concepts transitioning concept descriptions first target concept second difficult change representation space overlap old negative concept new positive concept depicted figure 5 aqpm advantage incremental learning system situation operates temporal batch mode since aqpm replaces old concept descriptions new ones would burdened information old concepts encoded concept descriptions aqpm operates temporal batch mode cause fair performance second concept examples held partial instance memory discussed aqpm used simple forgetting policy removed examples older fifty time steps flora systems hand used adaptive forgetting window case efficiently discarded examples concept changed may account difference performance second concept making transition second third stagger concepts easier transitioning first second overlap old positive concept description new positive concept description see figure 5 aqpms static forgetting policy worked better transition previous one learner achieved predictive accuracies comparable flora systems 5 discussion intelligent systems need induced hypotheses reasoning generalize systems experiences anticipate manipulating concept descriptions cope changing concepts slow systems reactivity keeping extreme examples addition concept descriptions learner maintains rough approximation current concept descriptions consequently able reason react efficiently 22 marcus maloof ryszard michalski learning stable concepts expect slight changes positions concept boundaries extreme examples case document past provide stability hand examples arrive radically change concept boundaries examples held memory longer fall concept boundaries removed replaced examples process actually happening situations different degrees extreme examples provide stability needed yet hinder learner forgetting mechanisms ensure stability result low reac tivity systems succeed nonstationary environments must find balance stability reactivity sections follow examine variety issues related study globally partial memory learning nonstationary concepts particular examine experimental results aspects study mal oof 1996 learning time concept complexity methods example selection incremental learning discussing current limitations work consider directions future 51 learning time experimental results lesion study showed example selection method greatly reduced number training examples maintained compared baseline learner number training examples affects run time algorithms investigated reducing number training examples maintained resulted notable decreases learning time intrusion detection problem example time step 10 aqpms learning time 367 seconds aqbls 556 seconds 5 meaning aqbl 52 slower aqpm problem 52 complexity concept descriptions also examined complexity induced concept descriptions terms conditions rules aqpm produced concepts descriptions complex simpler produced aqbl degree descriptions induced aq pm simpler notable measures learning time memory requirements table 3 shows decision rules intrusion detection problem aqpm induced two computer users daffy coyote 6 first rule daffy consists one condition involving average system time attribute must fall high range 2535253 6391466 7 class label daffy assigned decision variable average system time one daffys sessions falls range therefore daffys computing use characterized considerable consumption system time weights appearing end rules strength measures weight indicates many total training examples rule covers uweight indicates many unique training examples rule covers rules may overlap table 3 examples aqpm rules daffy coyotes computing behavior tweight 10 uweight 10 tweight 7 uweight tweight 4 uweight two rules cover training example rule daffys computing use strong since alone covers available training examples next two rules characterize coyotes behavior whose use computing resources low especially compared daffys 53 example selection methods selection method used study retained examples lay edges hyperrectangle expressed decision rule alluded similar methods keep examples lie corners surfaces hyperrectangles experimental results previous study maloof 1996 blasting caps intrusion detection problems showed keeping examples lie corners hyperrectangle opposed edges resulted slightly lower predictive accuracy slightly reduced memory requirements anticipate method retaining examples lie surfaces hyperrectangle slightly improve predictive accuracy slightly increase memory requirements compared edges method results conclude aqpm keeps examples partial memory predictive accuracy converge full memory learner 54 adding examples partial memory paper discussed reevaluation strategy maintaining examples partial memory using scheme aqpm uses new concept descriptions test misclassified examples examples partial memory lie concept boundaries retains examples removes discussed gives rise implicit forgetting process alternative scheme accumulate examples computing partial memory using misclassified examples adding resulting extreme examples already partial memory therefore example placed partial memory remains removed explicit forgetting process problems discussed elsewhere maloof 1996 see notable differences performance reevaluation policy accumulation policy example one would expect reevaluation policy would work best maloof ryszard michalski dynamic problems like stagger concepts accumulation policy would work best stable problems like blasting cap problem date experimental results supported intuition 55 incremental learning basic algorithm used temporal batch learning method table 1 step 7 also examined variants using incremental learning algorithms maloof 1996 meaning system learns new concept descriptions modifying current set descriptions using new training examples examples partial memory investigated notion using two incremental algorithms gem algorithm reinke michalski 1988 full instance memory technique aq11 algorithm michalski larson 1983 instance memory tech nique chose algorithms inductive biases similar aqpm use vl 1 representation language michalski 1980 aq induction algorithm michalski 1969 experimental results computer intrusion detection blasting cap detection problems show evidence incremental learning variants aqpm lose less predictive accuracy aqpm using temporal batch learning method infer incremental learning variants perform better concepts encode information lost using temporal batch method incremental learning methods take advantage information whereas temporal batch method moreover intend evaluate incremental learning methods using stagger problem determine perform may find incremental methods perform worse encode much information past reduce learners ability react changing environments 56 current limitations many current limitations approach stem assumptions system makes example system assumes given representation space adequate learning currently incapable constructive induction michalski 1983 also assumes context training examples presented stationary hence cannot learn contextual cues widmer 1997 although implement explicit mechanisms handle noise work mechanisms contexts similar widmer kubat 1996 general selection methodology works best ordered attributes taking advantage inherent structure consequently purely nominal domains method selects training examples since training example exists projection representation space example lies concept boundary 57 future work much current research assumes representation space concepts drift contexts change adequate learning environment nonsta tionary representation space could also change learners typically detect concept change sudden drop predictive accuracy learner subsequently unable achieve acceptable performance may need apply constructive induction operators effort improve representation space learning end one may use program automatically invokes constructive induction like aq18 bloedorn michalski 1998 kaufman michalski 1998 another interesting problem future research detect good bad types change consider problem intrusion detection need systems flexible enough track changes users behavior otherwise changes occur systems false negative rate increase yet intrusion detection systems flexible may perceive crackers behavior change true users behavior adapt accordingly envision twolayer system learns historical profile users computing behavior learns historical profile changed time users computing behavior longer matches historical profile system would determine type change occurred plausible user system would issue alert systems prove robust perform lower false negative rates standpoint methodology would like investigate policies let learner function instances arrive without feedback producing decisions without feedback necessarily problematic feedback arrive period time system may realize many past decisions wrong naturally simplest policy forget past decisions case learner would never realize made mistakes certain applications like intrusion detection require systems accountable however even though system may remember past decisions realizes wrong perhaps issue alert alternatively system may seek feedback events led incorrect decisions relearn perspective implementation fruitful exercise would implement example selection method using another concept representation like decision trees nothing inherent method limits decision rules could apply method symbolic representation uses linear attributes could also implement example selection maintenance schemes well mechanisms coping noise contextual changes latter areas commented wellstudied elsewhere finally several opportunities additional experimental studies investigated concepts change suddenly changes concepts could also occur gradually think concepts geometric objects space could change shape position size consequently concepts could grow ie change size position shape move ie change position 26 marcus maloof ryszard michalski shape size deform ie change shape position size although synthetic data sets like provide opportunities investigate specific research hypotheses also interested concept drift realworld applications like intrusion detection agent applications eg agent prioritizing email 6 conclusion partial memory learning systems select maintain portion past training examples use examples future learning episodes paper presented selection method uses extreme examples enforce concept bound aries method extends previous work using induced concept descriptions select nonconsecutive sequence examples input stream reevaluating examples held partial memory removing longer enforce concept boundaries results implicit forgetting process used conjunction explicit forgetting mechanisms remove examples satisfying userdefined criteria experimental results lesion study suggested method notably reduces memory requirements small decreases predictive accuracy two realworld problems computer intrusion detection blasting cap detection xray images stagger problem aqpm performed comparably stagger flora systems finally direct comparison ib2 revealed aqpm provided comparable memory requirements often higher predictive accuracy problems considered acknowledgments would like thank eric bloedorn renee elio doug fisher wayne iba pat langley anonymous reviewers provided suggestions improved work earlier versions paper would also like thank department computer science georgetown university institute study learning expertise center study language information stanford university support work research conducted machine learning inference laboratory george mason university laboratorys research supported part national science foundation grants iis9904078 iri9510644 part advanced research projects agency grant n0001491j1854 administered office naval research grant f4962092j0549 administered air force office scientific research part office naval research grant n0001491j1351 notes 1 structure time crucially important paper feel issue warrants sophisticated treatment 2 aq15c methods computing degree match based empirical analysis found method worked best problems study 3 first time step generated two random examples one class 4 ran ib2 using unscaled continuous data 5 conducted experiments using c implementation aqpm running sun 2 6 attribute values expressed using original real ranges 7 units case seconds r method attribute selection inductive learning systems uci repository machine learning databases cn2 induction algorithm convart program constructive induction time dependent data masters thesis incremental deductive strategy controlling constructive induction learning examples use multiple measurements taxonomic problems essential system administration second trading simplicity coverage incremental concept learning chimerge discretization numeric attributes machine learning experimental science favorit concept formation ageing knowledge forgetting aging knowledge concept formation experiments incremental concept formation unimem redundant noisy attributes progressive partial memory learning method partialmemory incremental learning application computer intrusion detection learning symbolic descriptions shape object recognition xray images quasiminimal solution general covering problem pattern recognition ruleguided inductive inference theory methodology inductive learning ca morgan kaufmann incremental generation vl 1 hypotheses underlying methodology description program aq11 technical report department computer science incremental learning concept descriptions method experimental results case study incremental concept induction beyond incremental processing tracking concept drift id5 incremental id3 decision tree induction based efficient tree restructuring guiding constructive induction incremental learning examples tracking context changes metalearning learning presence concept drift hidden contexts selective induction learning system aq15c method users guide reports machine learning inference laboratory tr method attribute selection inductive learning systems incremental learning concept descriptions method experimental results instancebased learning algorithms redundant noisy attributes attribute errors linearthreshold learning using winnow essential system administration incremental deductive strategy controlling constructive induction learning examples italicfavorititalic forgetting aging knowledge concept formation c45 programs machine learning learning presence concept drift hidden contexts tracking context changes metalearning progressive partial memory learning datadriven constructive induction cn2 induction algorithm experiments incremental concept formation method partialmemory incremental learning application computer intrusion detection ctr chichun huang novel graybased reduced nn classification method pattern recognition v39 n11 p19791986 november 2006 steffen lange gunter grieser variants iterative learning theoretical computer science v292 n2 p359376 27 january steffen lange gunter grieser power incremental learning theoretical computer science v288 n2 p277307 16 october 2002 antonin rozsypal miroslav kubat association mining timevarying domains intelligent data analysis v9 n3 p273288 may 2005 marcus maloof ryszard michalski incremental learning partial instance memory artificial intelligence v154 n12 p95126 april 2004 miquel montaner beatriz lpez josep llus de la rosa taxonomy recommender agents theinternet artificial intelligence review v19 n4 p285330 june