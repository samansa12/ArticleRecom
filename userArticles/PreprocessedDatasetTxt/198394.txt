distributed network computing local atm networks communication processors long bottleneck distributed network computing however recent progress switchbased highspeed local area networks lans may changing situation asynchronous transfer mode atm one widelyaccepted emerging highspeed network standards potentially satisfy communication needs distributed network computing paper investigate distributed network computing local atm networks first study performance characteristics involving endtoend communication environment includes several types workstations interconnected via fore systems asx100 atm switch compare communication performance four different application programming interfaces apis four apis fore systems atm api bsd socket programming interface suns remote procedure call rpc parallel virtual machine pvm message passing library api represents distributed programming different communication protocol layer evaluate parallel matrix multiplication local atm network experimental results show network computing promising local atm networks b introduction distributed network computing offers great potential increasing amount computing power communication resources available largescale applications distributed environment interested cluster workstations interconnected local area communication network combined computational power cluster workstations connected high speed lan applied solve variety scientific engineering problems likely combined power integrated heterogeneous network workstations may exceed standalone highperformance supercomputer since early 1970s computers interconnected networks ethernet token ring etc communication bandwidth networks limited tens megabits per second mbitssec bandwidth shared computers communication resources required collection cooperating distributed processors often bottleneck network computing limiting potential even higher speed fiber distributed data interface fddi provides bandwidth 100 mbitssec saturated data traffic computers therefore one design goals distributed network computing reduce amount communication computers however based nature application certain degree communication computers may necessary even communication channel saturated relatively long communication delay may degrade overall computing performance recent introduction asynchronous transfer mode atm may change situation atm emerging highspeed network technology may satisfy communication needs required many distributed network computing applications atm proposed international standards organizations uses small 53 bytes cells transmit data multiples oc1 rates 5184 mbitssec popular data transfer rates atm oc3 15552 mbitssec oc12 62208 mbitssec 7 10 atm initially developed standard widearea broadband networks fact local atm networks appearing advance longhaul atm networks makes atm attractive alternative traditional lans atm networks characterized switchbased network architecture computers connected switch communication pair computers established switch contrast situation computers share one communication medium case traditional lans ethernet switched network capable supporting multiple connections simultaneously aggregate bandwidth atm switch may several gbitssec within predesigned limit available aggregate bandwidth atm switch increases number ports increases paper discuss performance distributed network computing local atm networks consider endtoend communication latency achievable band width also computational performance distributed network applications endto communication latency primarily affected hardware software overhead hardware overhead includes host interface overhead switch signal propagation delay bus architecture host computer hardware technology improves impact overhead decrease software overhead includes delay caused interactions host operating system device driver higher layer protocols device driver overhead mainly caused design host interface bus architecture host computer overhead highlevel protocols delay caused interactions host operating system varied using different application programming interfaces apis available different communication layers several recent papers found significant portion communication overhead occurring due interactions 15 22 17 primary goal study performance tradeoffs choosing different apis local atm environment test environment several workstations interconnected via fores asx100 atm switch details local atm environment discussed section 2 least four possible apis available ffl fores api 13 ffl bsd socket programming interface 20 21 ffl suns remote procedure call rpc 21 ffl parallel virtual machine pvm message passing library 14 fores api provides several capabilities normally available apis guaranteed bandwidth reservation selection different atm adaptation layers aal multicasting atm specific features bsd socket interface provides facilities interprocess communication ipc first introduced 42bsd unix operating system rpc popular clientserver paradigm ipc processes different computers across network widely used communication mechanism distributed systems kernel 11 amoeba distributed operating system 4 pvm developed oak ridge national laboratory software package allows heterogeneous network parallel serial vector computers appear single computational resource pvm adopted communication primitives cray t3d massive parallel supercomputer 8 interprocess communication four apis chosen however performance application may affected decision made api may also represent communicating different protocol layer combinations apis also possible figure 1 shows protocol hierarchy different apis instance suns rpc uses external data representation xdr encapsulate application messages ensuring architecture independent data format sockets communicating underlying transport layers socket interface applications choose different transport protocol combinations transmission control protocolinternet protocol tcpip user datagram protocolinternet protocol udpip even raw sockets interprocess communication finally atm use either atm adaptation layer 34 5 ip choosing fitting combination specific distributed application addition communication efficiency several factors must also considered special interface restrictions ease use paper focus communication efficiency atm rpc3application programming interface message passing library pvm bsd socket programming interface sun rpcxdr programming interface atm api fore systems xdr interface ip socket interface atm api atm atmapplication atm switch local switch clouds figure 1 protocol hierarchy echo program used measure endtoend communication characteristics ie communication latency short messages communication throughput large messages explore underlying communication capabilities different apis local atm ethernet fddi networks two wellknown distributed applications parallel partial differential equations pde parallel matrix multiplication mm programs used measure performance different apis parallel mm coarsegrain distributed application requires data distribution independent computation module frequently used area scientific computation contrast parallel mm parallel pde typically finegrain distributed application within iteration pde computation node exchanges boundary conditions four neighbors requires frequent communication parallel mm parallel pde used diverse set applications fluid modeling weather forecasting supersonic flow modeling studying performance characteristics different apis compared according various aspects functionality userfriendliness semantics goal provide general programming guidelines programmers use developing distributed applications local atm networks several related works studied pointtopoint atm connections regard wolman 25 discussed performance tcpip showed pointtopoint atm connection approximately twofold performance increase ethernet lans thekkath 23 showed overhead lightweight rpc 170 sec however measurements include overhead incurred within atm switches thekkath 24 also implemented distributed shared memory system atm reported took 37 sec perform remote write operation atm switch result raw performance clear endtoend applicationtoapplication overhead paper organized follows section 2 briefly review atm standard describe hardware software test environment give overview api section 3 endtoend communication characteristics presented four apis various configurations performance two wellknown distributed applications parallel pde parallel mm presented section 4 finally close conclusion discussion future work 2 overview system environment section give overview atm technology discuss configuration experimental hardware software finally present description four different apis 21 asynchronous transfer mode atm method transporting information using fixedlength cells 53 octets 7 10 based virtual circuitoriented packet cell switching cell includes 5byte header 48byte information payload connection identifier consists virtual circuit identifier vci virtual path identifier vpi placed cell header vpi vci used multiplexing demultiplexing switching cells network communication model atm layered structure includes physical layer atm layer atm adaptation layer aal ffl physical layer physical layer transport method atm cells two atmentities provides particular physical interface format eg synchronous optical network sonet block coded format sonet defines standard set optical interfaces network transport hierarchy optical signals multiples basic signal rate 5184 mbitssec called oc1 optical carrier level 1 oc3 15552 mbitssec oc12 62208 mbitssec designated customer access rates bisdn block coded transmission sublayer based physical layer technology developed fiber channel standard 3 functions sublayer involve generating processing overhead atm cell header case oc3 baud rate 19440 mbaud payload rate 15552 mbitssec 14976 mbitssec available user data ffl atm layer atm layer performs multiplexing demultiplexing cells belonging different network connections translation vci vpi atm switches transmission cell information payload aal functions flow control traffic policing ffl atm adaptation layer atm adaptation layer divided two sub layers segmentation reassembly sublayer sar convergence sublayer cs sar sublayer performs segmentation typically large data packets higher layers atm cells transmitting end inverse operation receiving end cs servicedependent may perform functions like message identification timeclock recovery according specific services purpose atm adaptation layer aal provide link services required higher network layers generic atm cells used atm layer five service class standardized provide services ccitt recommendation atm specifies five aal protocols 19 listed follows 1 aal type 1 provides constant bit rate services traditional voice transmission 2 aal type 2 transports variable bit rate video audio information keeps timing relation source destination 3 aal type 3 supports connectionoriented data service signaling 4 aal type 4 supports connectionless data services combined aal type 3 5 aal type 5 provides simple efficient atm adaptation layer used bridged routed protocol data units pdu although atm network technology originally developed public telecommunication networks metropolitan wide areas recent interest focused applying technology interconnect computing resources within local area 6 paper investigate feasibility performing network computing atm local area 22 network environment experiments described performed variety host network architectures different host architectures tested include sparc 1 sparc 2 4690 sun microsystems possible architecture tested variety network interfaces case atm without presence local area switch atm environment provided magic multidimensional applications gigabit internetwork consortium 18 project fore systems inc host adapters local area switches used host adapters included series100 series200 interface sun sbus physical media series100 series200 adapters 100 mbitssec taxi interface fddi fiber plant signal encoding scheme local area switch fore asx100 sba100 interface figure 2 12 fores firstgeneration host adapter interfaced host cell level series100 adapter capable performing atm cell header crc generationverification aal 34 crc generationverification however host responsible multiplexingdemultiplexing vpivcis segmentation reassembly sar adaptation layers nonaal 34 crc generationverification tasks cpu intensive thus network throughput becomes bounded cpu performance host contrast series200 host adapter figure 3 16 fores second generation interface uses intel i960 onboard processor i960 takes aal cell related tasks including sar functions aal 34 aal 5 cell multiplexing series200 adapter host interfaces packet level feeds lists outgoing packets incoming buffers i960 i960 uses local memory manage pointers packets uses dma direct memory access move cells host memory cells never stored adapter memory asx100 local atm switch figure 4 12 based 24 gbitssec gigabit per second switch fabric risc control processor switch supports four network modules module supporting 622 mbitssec modules installed magic switches include two fourport 100 mbitssec taxi modules one twoport ds3 45 mbitssec module one twoport oc3c 155 mbitssec sonet module connections exist asx bus interface fifo fifo atm transmit engine atm receive engine parallelserial serialparallel receive odl transmit odl figure 2 series100 host interface transmit odl receive odl network control status seal receive queue transmit queue transmit engine receive engine bus control status fifo fifo board control 960 boot prom memory processor figure 3 series200 host interface 100 switches within metropolitanarea widearea environment asx100 supports fores spans signaling protocol series100 series200 adapters establish either switched virtual circuits svcs permanent virtual circuits pvcs experiments conducted ignored circuit setup time thus atm circuits used viewed pvcs host environment provided magic army high performance computing research center ahpcrc two sun 4690s provided ahpcrc connected via local ethernet subnet local fddi ring magic local atm network magic provided two sun sparc 1s connected via local ethernet subnet magic local atm network sun 4690s sparc 1s used characterize various aspects endtoend network communication section 3 machines also connected point topoint manner atm portion order characterize effect fores asx100 network module queue module vpivci translation control risc control processor vme bus interface3 network module queue module atm links bits 20 mhz ethernet switch fabric 24 gbs network module queue module network module queue 100 mbps taxi 100 mbps taxi mbps 155 mbps oc3c figure 4 magic asx100 atm switch functional overview switch local atm network characterize network used distributed applications section 4 ahpcrc provided four sun sparc 2 machines machines connected via magic asx100 atm switch via local ethernet subnet experiments run ethernet network general sniffer used measure artificial load created clientserver socket program ethernet subnet loads 0 percent used simulate typical subnet might look like access equipment necessary generate loading atm network 23 application programming interfaces programmers choose wide variety apis section briefly review four fores api bsds socket interface suns rpcxdr pvms message passing library 231 fore systems atm api support underlying device driver userlevel library routines provide portable interface atm data link layer depending platform subroutine library uses either system v streams interface socketbased interface device driver details implementation hidden programmer interface described portable across platforms atm library routines provide connectionoriented client server model data transmitted connection svc pvc established client server connection setup client server network makes best effort deliver atm cells destination transmission cells may dropped depending available resources remaining endtoend flow control hosts cell retransmissions left applications library routines provide socketlike interface applications first use atm open open file descriptor bind local application service access point asap file descriptor atm bind asap unique given endsystem comprised atm switch identifier port number switch connections established using atm connect within client process combination atm listen atm accept within server process operations allow data transfer specified simplex duplex multicast atm vpi vci allocated network connection establishment device driver associates vpivci asap turn associated file descriptor bandwidth resources reserved connection specified bandwidth greater capability fabric connection request refused due lack communication resources applications select type atm aal used data exchange selected aal treated argument atm connect client side fore systems implemen tation aal type 0 1 2 currently supported series200 interfaces type 3 4 treated identically atm send atm recv used transfer user messages one protocol data unit pdu transferred call maximum size pdu depends aal selected connection constraints underlying socketbased streambased device driver implementation local atm network also supports tcpip either aal 34 aal 5 used encapsulate ip packets receiving side packet implicitly demultiplexed host uses identity vc determine whether ip packet bandwidth specified ip connections zero interpreted switch control software lower priority connection nonzero reserved bandwidth 232 bsd socketbased programming interface 42bsd kernel introduced interprocess communication ipc mechanism sockets flexible unix pipes socket endpoint communication referred descriptor like file pipe two processes create socket connect two endpoints establish reliable byte stream unreliable datagram connected descriptors sockets read written user processes similar regular file operations transparency sockets allows kernel redirect output one process input another process residing another machine 20 sockets typed according communications semantics socket types defined subset properties socket supports properties inorder delivery data unduplicated delivery data reliable delivery data preservation message boundaries support outofband messages connectionoriented communication connection mechanism used avoid transmit identity sending socket packet data instead identity endpoint communication exchanged prior transmission data maintained end referred time sending receiving messages datagram socket models potentially unreliable connectionless packet communication stream socket models reliable connectionbased byte streams may support outofband data transmission sequenced packet socket models sequenced reliable unduplicated connectionbased communication preserves message boundaries sockets exist within communication domains communication domain abstraction introduced bind common properties communications bsd socket supports unix domain internet domain ns domain environment limited use internet domain communication local atm networks internet domain stream sockets datagram sockets use tcpip udpip 21 underlying protocols respectively especially focus communication performance stream socket since provides reliable data transmission 233 sun remote procedure call remote procedure call rpc fundamental approach interprocess communication based simple concept known procedure call rpc model follow client sends request blocks remote server sends response back similar wellknown wellunderstood mechanism known procedure call various rpc extensions available broadcasting nonblocking batching sun rpc uses udpip tcpip underlying protocols supports three rpc features blocking batching broadcasting transport protocol altogether five types rpc calls broadcast rpc use connectionless transport protocols like udpip batching rpc nonblocking call expect response order flush previous calls last call must normal blocking rpc call broadcast rpc calls servers support broadcast respond calls successfully completed otherwise silent sun rpc provides two types interfaces application programmers one available via library routines consist three layers second interface uses rpc specification language rpcl stub generator rpcgen rpcl extension xdr specification 234 pvm message passing library pvm de facto standard distributed computing uses basic message passing library pvm software system allows heterogeneous network computers used single parallel computer thus large computational problems solved using aggregate power many computers pvm collection serial parallel vector computers appears one large distributedmemory computer peruser distributed environment must setup running applications pvm daemon process runs participating machines used exchange network configuration information applications written fortran c implemented using pvm message passing library similar libraries found distributedmemory parallel computers sending message pvm composed three steps first send buffer must initialized call pvm initsend pvm mkbuf second message must packed buffer using number pvm pk routines pvm pk routines packs array given data type active send buffer calls pvm unpk routines unpack active receive buffer array given data type third message sent another process calling pvm send routine pvm mcast multicasting routine message received calling either blocking receive using pvm recv nonblocking receive using pvm probe pvm recv pvm daemon task process 1 machine 1 networks task process 2 pvm daemon machine 2 pvm daemon task process 1 machine 1 networks task process 2 pvm daemon machine 2 udp socket pvm advise mode pvm normal mode udp socket figure 5 comparison pvm normal pvm advise modes dynamic process group implemented top pvm version 3 implemen tation process belong multiple named groups groups changed dynamically time computation functions logically deal groups tasks broadcast barrier synchronization use users explicitly defined group names argu ments routines provided processes join leave named group pvm two communication modes pvm advise mode pvm normal mode advise mode sets direct tcp connection two communicating processes see figure 5 normal mode uses existing udp connections among pvm daemon processes application process creates tcp connection local daemon process therefore two tcp connections two udp connections required bidirectional communication two application processes see figure 5 direct communication link application processes pvm normal mode advantage advise mode provides efficient communication path normal mode observed twofold increase communication performance see section 33 drawback advise mode small number direct links allowed unix systems makes use unscalable terms advise mode normal mode used explicitly mentioned pvm manual 3 endtoend communication characteristics section present endtoend communication characteristics four apis simple clientserver echo algorithm implemented using api measure endtoend performance two performance measurements used characterize communication ca pabilities one communication latency latency especially important transmitting short messages maximum achievable communication throughput maximum achievable throughput important applications may require transmission large volume data api represents programming different communication protocol layer see figure 1 fores api implemented top atm adaptation layer provides way choose either aal 34 aal 5 underlying communication protocol internet domain bsd socket interface built top either tcpip udpip raw socket stream socket tcpip employed default ip atm either aal 34 aal 5 used encapsulate ip packets however environment aal 5 default adaptation layer transmitting ip packets atm networks pvm message passing library uses bsd sockets underlying communication facility pointed section 234 pvm two communication modes advise normal protocol combinations needed application using pvm totally depends assumed pvm mode sun rpcxdr also built top bsd socket interface either tcpip udpip chosen underlying communication protocol however evaluated suns rpcxdr protocol using tcpip bsd socket interface used pvm suns rpcxdr underlying interprocess communication mechanism communication performance socket interface significant impact performance pvm sun rpcxdr therefore carefully examined endtoend characteristics stream sockets see section 32 also studied performance two pvm communication modes section 33 performance fores api using aal 34 aal 5 protocols presented section 34 although many possible protocol combinations indicated figure 1 specifically interested five protocol combinations listed 1 fore systems atm api atm aal 34 2 fore systems atm api atm aal 5 3 bsd stream socket tcpip atm aal 5 4 pvm advise mode using stream sockets atm aal 5 5 sun rpcxdr using stream sockets atm aal 5 performance comparison five different protocol combinations presented section 34 convenience presentation defined three performance metrics capture endtoend performance characteristics since stream sockets available various net works good candidate used compare performance networks section 35 compare performance stream sockets fddi ethernet atm net works performance atm may different different host machines interface cards therefore section 36 compare performance atm aal 5 protocol several different host machines interface cards echo client process begin initial setup prepare buffer begin start timer end processing timing statistics end echo server process forever begin initial setup prepare buffer end begin end transmission send bytes packet receive bytes packet send bytes packet receive bytes packet actual message iterations figure codes echo server client processes 31 echo program figure 6 shows pseudo code client server echo processes client sends mbyte message server waits receive byte message back clientserver interaction iterates n times gather round trip timing iteration client process timing starts client sends byte message server ends client receives bytes response message total roundtrip time affected protocol stack device driver host interface signal propagation switch routing echo program used measuring endtoend communication latency avoid problem synchronizing clocks two different machines communication latency sending mbyte message estimated half total roundtrip time communication throughput calculated dividing 2 theta roundtrip time since 2 theta bytes message physically transmitted environment two sun 4690s physically connected local atm network fddi ring ethernet several experiments conducted sun 4690s section 32 36 suns sparc 1 sparc 2 computers also used measure communication performance bsd sockets fores api exception several experiments section 36 workstation fore systems series200 interface connected module 0 4 ports 100 mbitssec taxi interface magic asx100 atm switch unless explicitly mentioned operating system used sun workstations sun os 412 atm connections length multimode 625 micron fiber optic cable less 20 meters timing information collected echo program however found time required send receive first message takes much longer subsequent transmissions case aal 34 aal 5 timing difference first others around 10 15 milliseconds stream socket around milliseconds roundtrip timing seconds message size timing distribution 100 echo samples aal 5 maximum minimum mean 90 confidence interval mean maximum minimum figure 7 variation roundtrip timing vs packet size 100 samples using atm aal 5 study required fully understand causes effect included first echo timing calculations endtoend communication latency described section 100 timing samples collected use three statistics represent communication characteristics sample maximum sample mean sample minimum sample maximums minimums represent worst best collected timings respectively sometimes used characterize communication performance however experiments timing samples collected close calculated sample mean example figure 7 shows distributions maximum mean minimum aal 5 echo program timing samples varied message sizes follows 4 bytes 4 8 kbytes 96 byte message equal length data payload two atm aal 5 cells given message size echo program iterates 100 times gathered timing iteration collected maximum minimum mean computed 90 confidence interval 100 echo timing samples figure 7 90 confidence interval close mean samples therefore chose present mean timing samples remaining experiments 32 bsd socket interface local atm networks bsd ipc basic building block communication socket socket communication endpoint internet communication domain stream socket built top tcpip socket interface provides way manipulate options associated socket underlying protocols option represents special property socket underlying protocol among options two interesting change sending receiving achievable throughput mbytessec message size throughtput comparison stream socket disable enable enable 51k enable roundtrip timing seconds message size timing abnormality stream socket 32 kbyes socket buffer enable disable b disable tcpnodelay enable tcpnodelay figure 8 endtoend performance measurement bsd stream socket two sun 4690s local atm networks buffer sizes enabling disabling tcp nodelay socket buffer size socket layer option default socket buffer size depends systems initial configuration buffer size could range 51 kbytes environment operating systems provide socket buffer size greater 64 kbytes choice socket buffer size affect time required assemble disassemble message tcp nodelay tcp layer option enabled tcp queue small packet smaller low water mark tcp flow control form larger packet protocol uses low high water marks ensure appropriate flow control two communicating processes tcp nodelay option disabled default echo program implemented using bsd sockets used measure achievable throughput roundtrip echo time stream sockets performance different combinations two options presented figure 8a shows achievable throughput varying message size 4 bytes 1 mbyte 3 message size doubled time first studied performance stream socket tcp nodelay enabled following three socket buffer sizes kbytes 51 kbytes sun 4690 significant performance difference among three socket buffer sizes another experiment examined effect disabling enabling tcp nodelay fixed socket buffer size 32 kbytes choice 32 kbytes intentional since socket buffer size pvm advise mode set 32 kbytes achievable throughput tcp nodelay enabled better one tcp nodelay disabled shown figure 8a achievable throughput drops dramatically two places around message sizes 8 kbytes 64 kbytes tcp nodelay disabled order explore detailed timing information message sizes around 8 kbytes greater detail also ran experiment message sizes varied 4 bytes 16 kbytes 96 byte increments results presented figure 8b known tcp timing abnormality reported crowcroft 9 others observed timing abnormality local atm network however enabling tcp nodelay timing abnormality disappears performed experiments tcp echo program fddi ethernet message sizes less 16 kbytes found similar abnormalities also exist fddi ethernet fddi timing abnormality occurred message sizes range 4072 12800 bytes ethernet timing abnormality ranged 4064 6976 bytes 8416 9872 bytes 11328 12768 bytes timing abnormality caused tcp protocol however range message sizes abnormality occurred frequency timing abnormality affected physical network tcp used concurrent server model new socket created server accepts connection request client would like point newly created socket inherit options socket layer options lower layer protocols tcp udp ip set defaults therefore tcp nodelay option explicitly enabled server ensure abnormality occur bytes throughput figures used mbytessec equal maximum achievable throughput mbytessec socket buffer size effect socket buffer size different hosts sun 4690 sun 4690 figure 9 maximum achievable throughput different socket buffer sizes host machines understand effect socket buffer size different host machines also examined tcp echo program suns sparc 1 sparc 2 tcp nodelay option enabled experiment maximum achievable throughput three socket buffer sizes shown figure 9 found larger socket buffer size better maximum achievable throughput machines sun 4690 33 pvm characteristics local atm networks pvm provides normal advise modes described section 234 advise mode creates direct tcp connection two communicating application processes normal mode uses existing udp connection pvm daemon processes application process creates tcp connection local daemon process therefore two tcp connections two udp connections required bidirectional communication two application processes versions 324 325 326 pvm used pvm advise mode tcp version 324 tcp nodelay option disabled thus timing abnormalities similar stream socket observed abnormality fixed pvm version 325 enabling tcp nodelay sending side also sets socket buffer size 32 kbytes figure shows performance effect increasing message size normal advise modes pvm advise mode provides least twofold performance jump pvm normal mode echo timing consists time spent packing message pvm buffer time required transmitting message network shown figure total time dominated message transmission time roundtrip timing seconds message size timing comparison pvm 326 pvm normal mode pvm advise mode total pvm normal mode sending pvm normal mode packing pvm normal mode total pvm advise mode total pvm advise mode sending pvm advise mode packing figure 10 performance two pvm communication modes local atm networks 34 four apis local atm networks subsection compare performance following five protocol combinations 1 atm aal 34 fore systems atm api atm aal 34 2 atm aal 5 fore systems atm api atm aal 5 3 stream socket tcp stream sockets atm aal 5 4 pvm advise mode tcp pvm advise mode using stream sockets atm aal5 sun rpcxdr tcp sun rpcxdr using stream socket atm aal 5 experiments socket buffer size set 32 kbytes tcp nodelay option enabled tcpip connections figure 11 shows times required exchange message 4 bytes using atm aal 5 atm aal 34 socket interface 1738 2068 3920 respectively time required either pvm rpc least three times atm aal 5 maximum achievable throughput half atm aal 5 expected fores api exhibited better communication performance others major causes long latency low throughput pvm rpc protocol processing overhead tcpip communication overhead pvm daemon rpc daemon processes heavy interaction host operating system latency atm aal 5 still large communication intensive application believed modifications atm interface device driver could reduce overhead less one hundred sec maximum achievable throughput mbsec message size throughtput comparison five protocol combinations aal 34 sun rpc atm aal 34 layer pvm advise mode tcpip roundtrip timing seconds message size roundtrip timing comparison five protocol combinations sun rpc sun rpcxdr tcpip pvm advise mode tcpip atm aal 34 layer figure five protocol combinations table 1 echo measurements r different protocol combinations mbytessec bytes sec atm aal 34 407 6823 1034 pvm advise 152 3853 2766 sun rpcxdr 159 5407 2957 characterize experimental results using performance metrics three performance metrics introduced ffl r max maximum achievable throughput maximum achievable throughput obtained experiments transmitting large messages message size needed achieve half maximum achievable throughput number may compared corresponding numbers different hardware software configurations since maximum achievable throughputs may different different configurations required send message minimum size set half time required echo program sending message 4 bytes three performance metrics provide quick reference communication characteristics different protocol combinations maximum achievable throughput maximum throughput could observed applications different software hardware combinations important applications may require large volume data transmission startup latency minimum required time send messages especially important transmitting short messages half performance length provides reference point reach half maximum achievable throughput table 1 characterize five protocol combinations using three metrics r max n 12 0 startup latency atm aal 5 869 sec atm aal 34 yields largest maximum achievable throughput 407 mbytessec significant difference communication overhead pvm advise mode sun rpcxdr 35 bsd stream socket different networks experiment compared performance stream sockets tcpip local atm ethernet fddi networks atm fddi ethernet interface assigned different ip addresses giving desired ip address ip protocol choose corresponding network interface transmit messages maximum achievable throughput mbytessec message size throughtput comparison stream socket three networks fddi atm ethernet atm fddi roundtrip timing seconds message size roundtrip timing comparison stream socket three networks ethernet fddi atm atm fddi ethernet figure 12 performance comparison stream socket different networks table 2 echo measurements r different networks mbytessec bytes sec networks 209 3204 1960 fddi ring 215 6818 1833 ethernet 105 6482 1053 table 3 echo measurements r using aal 5 echo program various pointto point connection end host interface operating io bus r card system standard mbytessec bytes sec sun sparc 1 series100 411 sbus 096 838 734 sun sun sparc 1 series200 412 sbus 260 3784 811 sun 4690 series200 412 sbus 440 4237 858 sun sparc2 series200 412 sbus 576 6650 469 figure 12 shows time required echo program short messages achievable throughput long messages stated previously experiments performed absence network traffic ethernet shows lowest latency message sizes less 500 bytes believed firmware code ethernet finetuned better communication latency communication latencies atm fddi similar atm fddi sustain around 2 mbytessec throughput network utilization around mbitssec 100 mbitssec tcpip indicates possibility improvement tcpip atm ethernet sustain 105 mbytessec throughput figure represents 84 network utilization ethernet table 2 summarizes results 36 performance comparisons different hardware configurations table 4 echo measurements r using aal 5 echo program various configuration via asx100 switch local area end host interface operating r card system mbytessec bytes sec sun sparc 1 series100 411 094 748 736 sun sun sparc 1 series200 412 282 6675 742 sun 4690 series200 412 396 5134 869 sun previous endtoend communication performance measurements done two sun 4690 machines subsection experiments variety host machines different host interfaces tested host machines tested include sun sparc 1 sparc 2 4690 two atm interface cards include fores series100 series200 performance metrics obtained tabulated table 3 pointtopoint connection without going atm switch table 4 communication via local atm switch list performance comparison follows ffl effect host interfaces fores series200 interface much better communication throughput series100 interface example sun sparc 2 achieve 576 mbytessec maximum throughput using sba200 interface 144 mbytessec using sba100 interface ffl effect host machines faster machine cpus yield higher throughput lower latency one special case unexpected sun 4690 larger latency sparc 1 study required ffl effect switch component signal propagation delay switch measured timing differences pointtopoint direct connection connection via asx100 switch using series200 interface delay switch 9 sec 11 sec sparc 2 sun 4690 respectively ffl sun sparc 2 lowest communication latency largest user throughput next section investigate performance pvm bsd sockets fores api carrying distributed applications ethernet local atm networks performance evaluation distributed applications distributed network computing one possible application areas may benefit use highspeed local area networks atm computers distributed local area used together cooperatively solve large problems previously supercomputer would required solve problems echo program used previous sections provided latency achievable throughput different protocol combinations several networks section consider impact using different protocol combinations local atm networks distributed applications especially want understand performance two popular distributed programs parallel partial differential equations pde parallel matrix multiplication atm lans ethernet partial differential equations matrix multiplication examples chosen represent two typical types communication computation patterns parallel matrix multiplication consists several phases including distribution phase computation phase result collecting phase distribution phase result collecting phase high volume data transferred processing nodes matrix multiplication used compare throughput different protocol combinations networks parallel partial differential equations characterized communication intensive application execution processing node repeatedly exchanges boundary values immediate neighbors since boundary values need exchanged messages short parallel pde used compare latency impact different protocol combinations networks present brief description pde matrix multiplication later subsections hardware environment used included four sun sparc 2 workstations four workstations exclusively used experiments sparc 2 equipped ethernet adapter connected 10 mbitssec ethernet fore systems sba200 atm adapter connect fore systems asx100 switch parallel pde matrix multiplication applications implemented using masterslave programming model masterslave model master program spawns directs number slave programs perform computations master program also responsible recording timing information collecting computation results one four sparc 2 workstations used execute master slave time running distributed programs atm lan atm switch dedicated experiments ethernet experiment executed distributed programs network two different background traffic loads silent 30 loaded since bandwidth ethernet network shared additional load realistic ethernet experiments network general sniffer ethernet sniffer used monitor traffic ethernet secure fully controlled testing environment would like point important difference local atm network ethernet local atm network scalable ethernet scalable atm switch n ports capable supporting n parallel channels therefore meshconnected distributed application processor needs communicate four immediate neighbors means channel connected processor shared four processors long switch capable supporting n ports n increases always four processors sharing channel however case ethernet number processors increases total traffic amount increases well reason consider extra traffic loads local atm network experiments due availability equipment unable investigate issue scalability following two experiments communication apis networks compared ffl bsd stream socket interface ethernet atm networks tcp nodelay option enabled sockets send receive buffers set 32 kbytes execution distribution applications ffl pvm ethernet atm networks pvm advise mode used processing nodes could communicate direct tasktotask links striped partition b distribution c computation result collected figure 13 simple parallel implementation matrix multiplication ffl fores api atm aal 5 used due lower communication latency 41 parallel matrix multiplication many possible parallel algorithms matrix multiplication used straightforward approach address problem parallel multiplication two n theta n square matrices b yield product matrix b cluster sun sparc 2 workstations viewed simple 2d mesh 2 theta 2 distribution phase matrix partitioned rowstripping processing node sparc 2 workstation leftmost column 2d mesh half number rows matrix matrix b partitioned columnstripping similar way processing node topmost row mesh half number columns matrix b shown figure 13a distribution phase consists two steps first step processing nodes leftmost column transmit partitions matrix processing nodes row second step processing nodes topmost row transmit partitions matrix b nodes located column distribution phase figure 13b processing node computes submatrix c using appropriate partitions matrices figure 13c result submatrix c sent back master designate processing node result collecting phase shown figure 13d table 5 shows total execution time three matrix sizes ie 32 theta 32 128 theta 128 256 theta 256 timing information table 5 mean value 50 executions distributed program atm lan environment performance pvm bsd socket similar fores api since required computation becomes dominant part execution table 5 see fore systems api best throughput uses aal 5 directly 393 speedup achieved running 256 theta 256 matrix multiplication atm also conducted experiments ethernet two background traffic loads silent 30 load traffic ethernet monitored sniffer execution sniffer capable capturing traffic network performance pvm bsd socket silent ethernet comparable atm table 5 execution time matrix multiplication unit second protocol hierarchy matrix size network 32theta32 128theta128 256theta256 sequential 00988 66205 640001 ethernet silent 00134 19693 169130 ethernet 30 loaded 00341 20355 172416 bsd socket ethernet silent 00627 19136 167187 ethernet 30 loaded 00714 19932 169256 fores api setup 30 loaded ethernet used two sparc 2 workstations ethernet generate background traffic used sniffer verify traffic load one workstations periodically sent 1460 byte udp packets workstation 1460 byte udp packet packed single ethernet packet transmitted used sniffer adjust interval udp packets achieve desired background traffic load 128 byte udp packet shorter interval also used created amount background traffic however observed similar effect previous approach 42 parallel partial differential equations pde widely used many applications largescale scientific computing weather forecasting modeling supersonic flow elasticity studies one parallel algorithms uses 2d mesh topology briefly described detailed description refer 2 one class pde problems represented uniform mesh n lines unit square shown figure 14a n positive number intersections lines called mesh points desired function uxy mesh point iterative process used obtain approximate value uxy computing approximate value uxy need values four neighboring mesh points except boundary mesh points less four neighbors let e k denote absolute value difference approximate value u k xy exact value u x iterative process continues e k e 0 10 v shown iterative process converges g n iterations required accuracy example 10 gamma6 accuracy v 6 g 2 implementation parallel pde cluster sun sparc 2 workstations used 2dmesh mesh points unit square partitioned equally checkerboard mesh points inside unit square b partitioned 2d mesh figure 14 mesh points mesh partition parallel pde table execution time partial differential equation unit second protocol hierarchy mesh size network 16theta16 64theta64 256theta256 sequential 00868 01713 51483 102823 3307060 6614495 ethernet silent 03326 06472 32719 64996 1383927 2767819 ethernet 30 loaded 03519 06750 34066 66960 1402437 2791801 bsd socket ethernet silent 01432 02824 26505 51868 1347947 2687918 ethernet 30 loaded 01943 03651 26854 54361 1359601 2717504 fores api atm 01222 02208 24506 48273 1332512 2660706 style mapped processing nodes shown figure 14b dash lines mesh points represent crossmachine communications iteration processing node sends values boundary mesh points neighbors waits data four neighboring nodes recomputes approximate values mesh points reside inside partition table 6 shows time spent executing parallel version partial differential equations different mesh sizes protocol hierarchy networks since pde example one communication intensive distributed applications overhead different apis become important atm lan fores api lowest protocol overhead compared apis speedup 249 achieved bsd socket api provided good performance reliable communication interface pvm message passing library worst performance scenario pvm provides additional support distributed programming results additional overhead performance gets even worse running pvm normal mode instead advise mode ethernet environment first executed pde silent ethernet according sniffer observed consistent traffic load ethernet processing node needs exchange data neighbors every iteration running 64 theta 64 mesh size pde measured 6 traffic load 256 theta 256 meshsize pde 3 traffic load case 64 theta 64 processing node 32 theta 32 partition mesh points 256 theta 256 meshsize 128 theta 128 mesh points ratio communication time computation time former larger later thus 64 theta 64 meshsize pde generates traffic 256 theta 256 meshsize pde executing pde silent ethernet extreme unusual example ethernets silent usually traffic x windows network file system nfs remote printing telnet file transfer performance measured running distributed applications ethernet degree background traffic closer real world therefore used approach mentioned generate background traffic 30 load performances pvm bsd sockets degraded several issues need pointed ffl small number dedicated workstations silent ethernet distributed network computing ethernet accomplish performance comparable atm lans without support dedicated communication channels like atm links scalability ethernet becomes problem number processing nodes increases performance distributed programs local atm networks scalable example case parallel pde processing node uses four bidirectional links communicate four neighbors number links still fixed employ processing nodes solve problem hand ethernet saturate quickly number processing nodes increases ffl limitations imposed current implementation fores api include 4 kbytes maximum transfer size concurrent server model support machine dependencies ffl two previous experiments include time connection management application topology setup resource reservation since tasks related implementation communications api ffl restricted using unique facilities individual communications apis example multicasting barrier synchronization pvm singleclient multipleserver multicasting model fores api fair performance comparison distributed programs used experiments used common facilities api 5 conclusions future work paper studied feasibility carrying distributed programs cluster workstations interconnected local atm network endtoend performance several table 7 functional efficient comparison four available apis property fores api bsd socket pvm interface sun rpcxdr communication model message passing message passing message passing rpc underlying mechanism device driver transport protocols socket socket maximum transfer unit 4 kbytes 8 kbytes udp 8 kbytes udp tcp tcp protocol selection aal 345 tcpudp advisenormal tcpudp bufferred remote process spawn noz noz yes noz concurrent server dynamic process group authentication reliability protocoldependent yes protocoldependent application complexity high high low medium throughput good fair fair fair latency short medium long long property implementationdependent z supported unix system calls protocol combinations based four different apis presented also studied performance speedup parallel pde parallel matrix multiplication programs executed four sun sparc 2 workstations local atm network experimental results demonstrated executing communicationintensive distributed programs local atm networks appears promising focused discussion mainly communication performance aspect designing implementing distributed programs many factors need considered table 7 shows functional performance comparison four apis fores api bsd socket interface pvm provide general message passing capability users ie processes different machines communicate via send receive commands depending underlying communication protocol connection set actual data transmission sun rpcxdr uses remote procedure call invoke remote services basically message passing rpc provide similar communication capability comprehensive comparison message passing rpc mechanism refer chapter 5314 goscinsks book 1 api discussed paper represents distributed programming environment different communication layer protocol hierarchy distributed program using api lower layer like fores api take advantage better communication performance however usually lacks distributed programming support available higher layers without distributed programming support api extra effort required users develop distributed applications hand higher layer apis provide convenient distributed programming environment versatile facilities like remote process spawn process synchronization multicasting consequence degree overhead incurred order provide convenient programming environment fores api provided best performance among four apis studied paper ever maximum transfer unit fores api 4 kbytes user level message segmen tationreassemble required unreliable data transmission fores api forces application programs process message loss retransmission explicitly communication interface fores api similar socket interface current implementation fores api support multiple clients communicating specific server makes much complicated implement distributed applications fores api apis bsd sockets wellaccepted interprocess communication protocol however provide machine transparent access sun microsystems claimed rpc library use transport layer interface tli api 21 instead sockets future operating system releases sun rpcxdr suitable clientserver applications remote database access remote file access transaction processing however clear whether rpc programming paradigm good implementing high performance computing applications cluster networked workstations general pvm provides higher level programming support fores api support includes datatype encapsulation process group communication remote process spawn dynamic process control also introduces protocol overhead fores api order provide userfriendly distributed programming interface good performance one possibility implement pvm message passing library using fores api atm could become good candidate providing kind communication capability needed distributed network computing multicasting capability atm utilized pvm multicasting subroutines atm signaling protocol q93b 5 fores spans could used maintain application specific topologies 2d mesh hypercube tree pvm fores api potential become appropriate api running distributed programs local atm network one disadvantage implementing pvm fores atm api fores api standard porting pvm atm lan another vendor would require significant amount work project implement pvm atm using fores api instead bsd sockets currently investigation also developing new device driver fores series200 host interface reduce internal overhead streambased device driver another ongoing project study improve performance tcpip local atm networks acknowledgement authors wish express sincere gratitude thomas pfenning university cologne germany eric lampland network services university minnesota timothy j salo jeffrey kays minnesota supercomputer center inc ronald j vetter university north carolina colleague james schnepf rose tsang valuable comments support r distributed operating systems logical design design analysis parallel algorithms distributed operating system atm usernetwork interface specification internetworking atm wans cray research layering harmful goal atm v distributed system designing practical atm lan fore systems pvm 3 users guide reference manual protocol structure highspeed communication broadband isdn fore systems 200series atm adapters highspeed workstations pcs minnesota supercomputer center inc request comment 1483 design implementation 43bsd unix operating system network programming guide active messages mechanism integrated communication computation limits lowlatency communication highspeed networks efficient support multicomputing atm net works latency analysis tcp atm network tr distributed operating systems v distributed system active messages limits lowlatency communication highspeed networks distributed operating systems