skeleton trees efficient decoding huffman encoded texts new data structure investigated allows fast decoding texts encoded canonical huffman codes storage requirements much lower conventional huffman trees olog2 n trees depth olog n decoding faster part bitcomparisons necessary decoding may saved empirical results large reallife distributions show reduction 50 number bit operations basic idea generalized yielding savings b introduction importance usefulness data compression information retrieval ir systems today wellestablished many authors commented 1 17 31 27 large fulltext ir systems indeed voracious consumers storage space realtive size raw textual database text kept also various auxiliary files like dictionaries concordances usually adjoined system make retrieval process efficient moreover certain data structures decoding tables trees resident ram large systems require powerful machines therefore quite natural efforts made compress text necessary files thereby reducing demand storage ram equivalently fixed machine given resources effectively increasing size data base still handled efficiently popular compression methods based works lempel ziv 29 30 adaptive methods always suitable ir applications context fulltext retrieval large number small passages accessed simultaneously eg producing kwic keywordincontext index response submitted query text fragments decodable regardless exact location adaptive coding method used would force us start decoding beginning text logical block contains retrieved passage would either decode much needed may imply increased processing time prepare priori smaller blocks would cost us compression efficiency cases advantage using adaptive methods often yield better compression static ones may lost huffman coding 14 still one best known popular static data compression methods certain applications data transmission communication channel coding decoding ought fast applications like ir scenario focus paper compression decompression symmetrical tasks compression done building system whereas decompression needed processing every query directly affects response time thus special interest fast decoding techniques see eg 15 data structures needed decoding huffman encoded file huffman tree lookup table generally considered negligible overhead relative large texts however texts large huffman coding applied connection markov model 2 required huffman forest may become storage problem moreover alphabet encoded necessarily small may eg consist different words text huffman trees thousands even millions nodes uncommon 23 try paper reduce necessary internal memory space devising efficient ways encode trees addition new suggested data structure also allows speedup decompression process reducing number necessary bit comparisons manipulation individual bits indeed main cause slow decoding huffman encoded text method based large tables constructed preprocessing stage suggested 5 help entire decoding process performed using byte oriented commands see also 26 however internal memory required storage tables may large another possibility avoid accessing individual bits using 256ary instead optimal binary huffman codes obviously reduces compression efficiency de moura et al 6 report degradation significant next section recall necessary definitions canonical huffman trees used section 3 presents new suggested data structure includes experimental results section 4 main idea extended yielding yet smaller trees even faster decoding 2 canonical huffman codes figure 1 canonical huffman code given probability distribution might quite large number different huffman trees since interchanging left right subtrees internal node result different tree whenever two subtrees different structure weighted average path length affected interchange often also optimal trees cannot obtained via huffmans algorithm one may thus choose one trees additional properties preferred choice many applications canonical tree defined schwartz kallick 25 recommended many others see eg 15 27 denote p assume length bits codeword assigned huffmans procedure element probability depth leaf corresponding p huffman tree tree called canonical scanning leaves left right appear nondecreasing order depth equivalently nonincreasing order 22 idea huffmans algorithm used generate lengths f g codewords rather codewords latter easily obtained follows ith codeword consists first bits immediately right binary point infinite binary expansion many properties canonical codes mentioned 15 3 following used running example paper consider probability distribution implied zipfs law defined weights nth harmonic number law believed govern distribution common words large natural language text 28 canonical code represented string hn 1 called source k denotes length longest codeword depth tree n number codewords length k source corresponding zipfs distribution 74i code depicted figure 1 shall assume ease description source holes ie three integers true many reallife distributions particular examples hand distribution one alphabets used compressing set sparse bitmaps 8 techniques suggested herein easily adapted general case using vector succi giving codeword length next larger codeword length j make exposition clearer shall suppress reference succi since distributions without holes one properties canonical codes set codewords length comprises binary representations consecutive integers example case codewords length 9 bits binary integers range 110011100 111011010 fact exploited enable efficient decoding relatively small overhead codeword bits detected one get relative index within sequence codewords length simple subtraction following information thus needed let length shortest codeword let basei integer value first codeword length denote standard sbit binary representation integer k leading zeros necessary jth codeword length let seqi sequential index first codeword length suppose detected codeword w length iw integer value binary string w ie relative index w within block codewords length thus seq relative index w within full list codewords rewritten iw gamma diff thus one needs list integers diff table 1 gives values n basei seqi diffi example 9 table 1 decode values canonical huffman code zipf200 suggest next section new representation canonical huffman codes spaceefficient may also speed decoding process permitting times decoding single bit one iteration 3 skeleton trees fast decoding following small example using data shows savings possible suppose decoding detect next codeword starts 1101 information enough decide following codeword ought length 9 bits thus able detected first 4 bits codeword read following 5 bits block without check bit end codeword reached goal construct efficient datastructure permits similar decisions soon possible fourth bit earliest possible example since also codewords length 8 starting 110 31 decoding sktrees suggested solution binary tree called sktree skeletontree structure induced underlying huffman tree generally significantly fewer nodes tree traversed like regular huffman tree start pointer root tree another pointer first bit encoded binary sequence sequence scanned read zero resp 1 proceed left right child current node regular huffman tree leaves correspond full codewords scanned decoding algorithm outputs corresponding item resets treepointer root proceeds scanning binary string case however visit tree depth necessary identify length current codeword leaves sktree contain lengths corresponding codewords f tree pointer gamma root start length string f string else tree pointer gamma right tree pointer value tree pointer 0 f codeword string start output tree pointer gamma root start else figure 2 decoding procedure using sktree formal decoding process using sktree depicted figure 2 variable start points index bit beginning current codeword encoded string stored vector string node sktree consists three fields left right pointer null node leaf valuefield zero internal nodes contains length bits current codeword node leaf actual implementation use fact internal node either zero two children store valuefield rightfield space serving flag use right pointer procedure also uses two tables table giving jth element nonincreasing order frequency encoded alphabet diff defined varying k length shortest length longest codeword procedure passes one level tree one according bits encoded string leaf reached rest current codeword read one operation note bits input vector individually scanned yields possible time savings 9 91010 figure 3 sktree zipf200 distribution figure 3 shows sktree corresponding zipfs distribution 200 tree tilted 45 ffi left right children indicated arrows pointing right framed leaves correspond last codewords indicated length sktree example consists 49 nodes opposed 399 nodes original huffman tree idea similar sktree based tables rather trees suggested moffat turpin 22 instead identifying roots subtrees codewords depth essentially form complete tree fixed depth less depth code tree extending shorter branches examine code tree nodes depth determine minimum codeword length subsidiary subtree find length codeword fixedsized window compressed bitstream considered binary value compared leftjustified base values sequence hardcoded cascading ifstatements comparison equivalent transition left right child sktree replacement bit comparisons equivalent byte word based comparisons reminiscent mechanism suggested 5 32 construction sktrees traversing standard canonical huffman tree decode given codeword one may stop soon one gets root full subtree depth h h 1 ie subtree depth h 2 h leaves since stage known exactly h bits needed complete codeword one way look sktrees therefore standard huffman trees full subtrees depth h 1 pruned direct much efficient construction follows onetoone correspondence codewords paths root leaves huffman tree extended define binary string path p induced tree given root r 0 path consist e e 0 r left resp right child r example figure 3 p111 consists four nodes represented bullets top line skeleton sktree consist paths corresponding last codeword every length let codewords denoted l example etc idea p l serves demarcation line node left resp right p l ie left resp right child one nodes p l corresponds prefix codeword length first approximation construction procedure thus takes tree obtained clearly need include longest codeword l k always string k 1s adjoins missing children turn complete tree internal node left right child label new leaf set equal label closest leaf following inorder traversal words creating path l one first follows nodes already existing tree one branches creating new nodes labeling missing right child node path labeled basing assumption holes missing left children new node path labeled closer look implies following refinement suppose codeword l zero rightmost position ie l string ff length gamma 1 first codeword length follows getting ith bit one decide length current codeword 1 l terminates string 1s l 0 jfij first codeword length length codeword deduced already read bit following fi follows one always need full string l sktree prefix including rightmost zero let l prefix revised version procedure starts tree obtained nodes tree depicted bullets figure 3 path p l leaf tree left child leaf new terminal node represented figure 3 box containing number additional leaves filled explained 33 space complexity evaluate size sktree count number nodes added path p l k since codewords canonical code ordered corresponding frequencies also alphabetically sorted suffices compare l l igamma1 let empty string let fli longest common prefix l l string 10 example number nodes sktree given since summation alone number internal nodes bullets figure 3 maximum function comes prevent extreme case difference might negative example l longest common prefix since consider bits including rightmost zero l case indeed new nodes added p l immediate bound number nodes sktree ominn k 2 since one hand 2 hand cannot exceed number nodes underlying huffman tree 2n gamma 1 get tighter bound consider nodes upper levels sktree belonging full binary tree f leaves root sktree depth f 1e leaves level gamma 1 tree f part sktree paths p l must overlapping account nodes f separately 2k gamma 1 nodes f k gamma 1 disjoint paths path p l extending f log yields bound number nodes sktree savings worst case eg one codeword length except longest always least two generally depth huffman tree omegagamma n savings might significant trees optimal skewed distributions many applications like distributions characters character pairs words natural languages depth huffman tree olog n large n even constant c depth c log 2 n must quite small suppose huffman tree leaf depth 16 theorem 1 probability element corresponding leaf f j jth fibonacci number get 18 exercise 1214 52 golden ratio thus c log 2 n give numeric example section 4 one huffman trees corresponds different words english leaves probability tree size leaf level 3 log 2 n less 44 theta 10 gamma12 meaning word occurs every 4400 billion words existence rare word puts lower limit size text case must large enough fill 35000 cdroms distributions given table 2 experiments ratio depth huffman tree log 2 n 131 261 even original huffman tree would deeper sometimes convenient impose upper limit olog n depth often implies negligible loss compression efficiency 10 case given logarithmic bound depth size sktree log n log 34 time complexity decoding based standard huffman tree average number comparisons per codeword sum taken leaves depth tree times probability get similar sum holds sktrees difference leaf correspond single element several consecutive codewords length let w prefix codeword corresponding leaf sktree labeled denote codewords corresponding leaf sktree correspond using notations section 2 indices range diff average number comparisons per codeword using sktree thus evaluated i2fleaves sktreeg w binary string corresponding leaf depth tree shortcut labeli probj probability element index j approximation assume probability element level tree 2 gammai corresponds dyadic probability distribution probabilities integral powers 1 cannot great difference actual probability distribution dyadic one since yield huffman tree see 20 bounds distance distributions given model eqn 2 becomes i2fleaves sktreeg similar sum taken leaves original huffman tree gives average codeword length dyadic distribution therefore large savings whenever number nodes sktree much smaller underlying full huffman tree 35 experimental results test effectiveness use sktrees following reallife distributions used data french collected tresor de la langue francaise database 680 mb french language texts 115 million words 17 th 20 th centuries 4 english source 500 mb 87 million words wall street journal 24 hebrew part responsa retrieval project 100 mb hebrew aramaic texts 15 million words written past ten centuries 7 first set alphabets consists bigrams three languages source english distribution 13 next set elements encoded different words yields large alphabets final set contains distribution trigrams french completeness zipf200 distribution used examples also added total average number average relative source k number codeword nodes number savings elements length sktree comparisons comparisons bigrams french 29 2192 7784 285 4620 406 hebrew 24 743 8037 127 4183 480 english 26 289101 11202 425 5726 489 words french 27 439191 10473 443 5581 467 hebrew trigrams french 28 25781 10546 381 5026 523 table 2 time space requirements reallife distributions table 2 displays results first three columns give statistics various distributions depth k huffman tree size n encoded alphabet weighted average length codeword measured bits equals average number comparisons standard huffman tree used next two columns bring number nodes sktree given eqn 1 average number comparisons per codeword decoding based sktree given eqn 2 final column shows relative savings number comparisons measured percent see large distributions roughly half comparisons may saved note savings spite fact highprobability symbols short codewords relatively bits common weighted average takes account bits saved shorter codewords savings multiplied higher probabilities bits saved longer codewords even probabilities small large number cumulative effect note also cost storing sktree several percent cost full huffman tree 4 reduced sktrees wish explore might gained pruning sktree internal node one would thus get leaves yet possible deduce length current codeword partial information already available example figure 3 bits already processed 111 corresponding internal node rightmost upper corner know already length current codeword either 9 10 therefore need one comparison know exact length concatenate following seven bits three already processed get 10bit string w binary value w smaller base10 next codeword must length 9 otherwise length 10 used original sktree explained previous section would least one comparison possibly even eg bits 111 0110 would performed four comparisons still know length 9 reflection leads idea reduced sktree obtained sktree pruning branches one hand reduced tree obviously smaller saw may also decrease number comparisons formally define node v sktree two values lowerv upperv v leaf lowerv v internal node lowerv lowerleftv node v codewords corresponding leaves subtree rooted v lengths interval lowerv upperv terms earlier notation define reduced sktree smallest subtree sktree leaves w correspond range two consecutive codeword lengths ie 56 figure 4 reduced sktree zipf200 distribution figure 4 reduced sktree obtained sktree figure 3 leaves also indicated bullets corresponding range underneath recall original huffman tree 399 nodes sktree 49 reduced sktree left 13 note leaves original sktree deleted also entire subtrees nodes corresponding part p l overlapping p l eqn 3 since seek minimal tree path node highest tree need kept rest branch offsprings pruned generalized view regular reduced sktrees would follows consider full canonical huffman tree assign node values lower upper delete nodes starting leaf proceeding parent nodes get smallest tree every leaf w satisfies lowerw upperw ie corresponding codewords length sktree process continued nodes deleted codewords corresponding new leaves lengths get reduced sktree henceforth adopt notation sk 1 tree sk 2 tree original reduced sktrees respectively subscript referring maximal size set codewordlengths associated leaves tree cannot use equality eqn 3 would impose range exactly two codeword lengths leaf sk 2 tree example figure 4 leaves equality last elements codeword blocks 56 67 12corresponding sk 2 tree figure 5 example sk 2 tree special leaves eqn 3 examples leaves may exist parent nodes correspond already ranges 3 codeword lengths case original leaf sk 1 tree must kept let us call leaves sk 2 tree special leaves example distributions special leaves exist distribution hebrew bigrams first elements source h0 0 0 0 left part figure 5 last codewords l codeword lengths 13 listed right part figure 5 corresponding section sk 2 tree special leaves w indicated rectangles containing value lowerw equal upperw leaves depicted bullets example see codewords length 8 prefix 011 parent node corresponding leaf associated prefix 01 may extended codewords lengths 6 7 8 similarly prefix 11110 implies codeword length 11 1111 prefix codewords lengths 11 24 decoding procedure sk 2 trees similar sk 1 trees given figure 2 ifblock replaced one figure 6 use f lag field leaf w f otherwise value field w stores lowerw w leaf 0 w internal node value tree pointer 0 f len gamma value tree pointer codeword string start f lag tree f codeword string start len output tree pointer gamma root start figure decoding using sk 2 tree leaf w reached current codeword initialized length lowerw correct setting w special leaf next codeword indeed length lowerw w special leaf f lagw 1 check appending zero right end codeword get integer value larger equal first codeword length lowerw 1 update current codeword include also following bit construction sk 2 trees similar underlying sk 1 tree consider paths nodes nodes appear least two different paths internal nodes sk 2 tree leaves added filling missing left right children may special leaves space complexity sk 2 trees note principle several special leaves may emanate single branch p l leaves upper bound number nodes ominn k 2 sk 1 trees practice special leaves rare appear particular case p l contained either p l former case special leaves right children nodes p l latter left children example referring tree figure 5 l prefix l generates special leaf right child whereas l contains l prefix generates special leaves left children given huffman tree special leaves associated sk 2 tree case examples beside one figure 5 number nodes clearly exactly one leaf range sk 2 tree complete tree ie internal node exactly two children sizes sk 2 trees earlier example distributions listed table 3 seen even huge huffman trees hundreds thousands nodes size reduced several tens 7090 reduction even relative sizes sk 1 trees number savings average savings source nodes rel number rel sk2 tree sk1 tree comparisons sk1 tree english 15 78 3444 180 bigrams french 47 84 3757 187 hebrew english 41 90 4842 154 words french 41 91 4725 153 hebrew 33 90 4715 172 trigrams french 43 89 4157 173 table 3 time space requirements sk 2 trees evaluate average number comparisons take sum similar eqn 2 leaves sk 2 tree special leaves formula eqn 2 applies others let w prefix corresponding codeword assume leaf labeled jwj codewords corresponding leaf sk 2 tree w0 first length following ones length 1 exact cutoff point important codewords correspond consecutive indices range iw0 1 probability codewords multiplied number necessary comparisons detect jwj 1 since need additional comparison decide length 1 yields using notations eqn 2 following formula i2fleaves sktreeg table 3 gives resulting averages examples reallife examples give reduction 5064 relative regular huffman decoding algorithm 1519 relative algorithm using sk 1 trees 5 final remarks pruning skeleton tree turned profitable terms time space shouldnt climb even higher define sk trees accordingly 2 associate value rangesize node v sk 1 tree giving size set corresponding codeword lengths leaves sk 1 tree rangesize sk 2 tree rangesize 2 consider path starts leaf moves parent pointers towards root rangesize values nodes path form nondecreasing sequence first value 1 followed possibly several 2s etc fixing paths last node value 2 exists new leaf yields sk 2 tree similarly proceeding even last node value 3 would yield sk 3 tree etc however savings incurred passing sk 1 tree sk 2 tree caused fact several consecutive nodes paths rangesize value 2 new leaves several levels higher accordingly several comparisons could saved parent node node rangesize value 3 also rangesize value 3 child parent node must rangesize value 1 means special leaf argued earlier cases rare therefore whenever special leaf involved passing lowest node rangesize value 2 lowest node rangesize value 3 would let us climb one level save one comparison hand need additional comparison within range 3 values cases nothing gained course price two additional comparisons could process using binary search ranges size 4 3 generally need r additional comparisons reaching leaf sk 2 r tree pushing idea extreme would skeletontree would find correct length codeword using sequence binary search steps list first last codewords every codeword length suggested 22 standard binary search search code maximal codeword length k takes exactly dlog 2 ke comparisons would 4 5 example distributions note average number comparisons sk 1 trees threshold examples table 2 corresponding values table 3 therefore seem necessarily worthwhile pass sk trees 2 moffat turpin 22 suggest use biased binary search since probability distribution codeword lengths skewed first bits code word approaches linear search skeletontrees introduced paper convenient data structure perform similar search efficiently r huffman coding dead ziff perl fast searching compressed text allowing errors responsa retrieval project always wanted know afraid ask rabinowitz j moore e information retrieval method construction minimum redundancy codes nemetz art computer programming application informational divergence huffman codes implementation minimum redundancy prefix codes text compression dynamic document databases kallick b fast decoding huffman codes psychobiology language universal algorithm sequential data compression compression individual sequences via variablerate coding adding compression fulltext retrieval system tr ctr dana shapira ajay daptardar adapting knuthmorrispratt algorithm pattern matching huffman encoded texts information processing management international journal v42 n2 p429439 march 2006