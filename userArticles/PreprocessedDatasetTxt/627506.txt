generalization neural networks authors discuss requirements learning generalization traditional methods based gradient descent limited success stochastic learning algorithm based simulated annealing weight space presented authors verify convergence properties feasibility algorithm implementation algorithm validation experiments described b introduction neural networks applied wide variety applications speech generation1 handwriting recognition2 last decade seen great advances design neural networks class problems called recognition problems design learning algorithms35 57 learning weights neural network many recognition problem longer difficult task however designing neural network generalization problem well understood domains neural network applications classified two broad categories recognition generalization 1 8 classes first train neural network set inputoutput pairs 2 n n recognition problems trained network tested previously seen input j 1 j n corrupted noise shown fig1 trained network expected reproduce output j corresponding j spite noise shape recognition 9 10 handwriting recognition2 examples recognition prob lems hand generalization problems trained neural network tested input n1 distinct inputs 1 2 n used training network shown fig1 network expected correctly predict output n1 input n1 model learned training typical examples generalization problems bond rating11 robotics12 neural networks generalization problems important since many applications enormous importance real world1315 would benefit work many applications difficult successfully apply either conventional mathematical techniques eg statistical regression standard ai approaches eg rule based systems neural network generalization ability useful domains11 require priori specification functional domain model rather attempts learn underlying domain model training inputoutput examples r n e 1 2 1 2 n n r n e 1 2 1 2 n n n1o n1 n2o n2 figure 1 classes problems learning algorithm generalization problems different learning algorithm recognition problems recognition problems network expected reproduce one previously seen puts network may remember outputs inputs fitting curve used train ing remember outputs one often uses large networks many nodes weights however memorization learning samples suited generalization problems since lead worse performance prediction outputs unseen inputs furthermore generalization problems allow small amount error output predicted network hence fitted curve need pass pair used training networks addressing generalization problem may instead fit simple curve eg low degree polynomial basic analytical functions like logx sinex tangentx etc inputoutput pairs rather fitting crooked curve neural network used generalization problems tend simpler small number hidden nodes layers interconnection edges weights enabling one use computationally sophisticated algorithms earlier work neural networks 4 9 related recognition problems little research towards developing neural network models generalization problems 16 17 present new learning algorithm stochastic backpropagation generalization problems verify convergence algorithm provide theoretical arguments towards capability proposed algorithm discover optimal weights also describe implementation algorithm experience algorithms solving generalization problems 2 problem formulation generalization problems neural networks formulated three different ways analytical constructive function learning 2527 c symbolic semantic network8 analytical formalism focuses existence networks capability generalize also provide worst case time complexity discover networks solve arbitrary generalization problems provide way discover networks constructive function learning formalism approaches generalization problems complementary fashion studies algorithms discover networks solve class generalization problems aims discovering function f mapping input domain output domain set learning examples function discovered may defined boolean numbers real numbers inputs outputs assumed numbers symbolic meaning function network represent symbolic meaning beyond numeric computation third approach symbolic semantic network associates symbolic meaning network generalization occurs attaching new node appropriate parent node network inherit properties parent classification formulations generalization problems include special cases example signal detection problem considered special case task signal detection problem learn recognize parameter function f rather learn function recognizing frequency given sinusoidal function example signal detection problem backpropagation neural networks applied successfully problem28 focus constructive function learning formulation terms learning numeric function simple neural network described directed graph e vertex set v three kinds nodes input nodes leaves b hidden nodes internal nodes c output nodes roots edge e e associated weight w j shown fig2 network used compute output set inputs node computes function weighted sum input signals g inset output function g maps 11 given input network would compute output finput inverse problem discovering function f ie set edge weights given set inputoutput pairs referred learning problem stated follows given set example input output pairs 1 1 n n find weights edge e j e neural network network maps j j j12n closely possible represent possibly infinite domain inputs represent possibly infinite range outputs dimensional feature space f 1 f k describing input input j considered ktuple cartesian space f 1 f 2 f k given learning sample per sample error function e real 1 2 n 1 2 n generalization involves finding mapping function f neural network example symbols nodes edges weights output input i2 input i1 w36 figure 2 feedforward neural network minimize error function e entire domain particular f learning sample reducing error function entire domain looking small subset difficult arbitrary learning samples arbitrary input domain generalization problem often simplified making assumptions domain learning sample assume represents entire domain adequately value e estimated value e function f assumed smooth continuous well behaved domain range function f boolean sets set real numbers usually one function fit given set learning samples inputoutput pairs makes generalization problem harder example learning specific boolean function subset domain difficult 25 since several boolean functions domain fit learning samples little consensus criteria prefer one candidate boolean functions rest break tie restrict attention functions set real numbers draw upon notion simplicity functions real numbers choose one function among set possible functions fit learning samples simplicity intuitively defined term number maxima minima function simplicity reduces notion degree polynomials polynomial functions 3 stochastic backpropagation general idea behind algorithm use simulated annealing weight space weight space defined collection configurations w connection weights neural network simulated annealing procedure searches weight space configuration w opt minimizes errortofit function ew search procedure based monte carlo method 29 given current configuration w network characterized values weights small randomly generated perturbation applied small change randomly chosen weight difference error de current configuration w slightly perturbed one negative ie perturbation results lower error fit process continued new state de 0 probability acceptance new configuration given exp dek b occasional transitions higher error configuration help search process get local minimas acceptance rule new configurations referred metropolis criteria following criteria probability distribution configurations approaches boltzman distribution given eqa1 zt normalization factor known partition function depending temperature boltzman constant factor exp ek b known boltzman factor denotes control parameter called temperature due historic reasons starting high value temperature decreased slowly execution algorithm temperature decreases boltzman distribution concentrates configurations lower error finally temperature approaches zero minimum error configurations nonzero probability occurring way get globally optimal weights network minimizing error fit training examples provided maximum temperature sufficiently high cooling carried sufficient slowly algorithm description one define configurations cost function generation mechanismor equivalently neighborhood structure describing algorithm assume weight takes discrete values set sdd0d2dsd restriction weights discrete values limit learning ability neural networks generalization problems configurations defined ntuple weights n number weights network configuration space constructed allowing weight take values cost function defined error desired outputs network outputs learning examples shown j c refers jth network output input cth training example j c refers jth desired output cth training example indices c refers different outputs network various training examples respectively generate neighboring configurations change one randomly chosen weight element configuration use uniform probability distribution chose weight changed thus probability generating neighboring configuration current configuration uniformly distributed neighbors one always choose large value scale input output data value small range achieve better accuracy procedure begin stopcriterion system frozen repeat accept false w lm changed perturb else exp c accept updateconfiguration j equilibriumisapproachedsufficientlyclosely c m1 fc fig3 stochastic backpropagation algorithm psuedopascal psuedopascal description stochastic backpropagation shown fig3 initialize routine assigns default values variables particular current configuration temperature outeriterationcount loops correspond simulated annealing weight space inner loop represents simulated annealing fixed value control parameter executes probability distribution current configuration possible configuration becomes stable helps us achieve boltzman distribution outer loop changes control parameter slowly final value near 0 corresponds slow cooling achieve configurations globally minimum error steps inside innermost loop combine backpropagation transitions simulated annealing use backprop backpropagation algo rithm 4 subroutine compute error derivatives e w lm respect various weights derivatives help us estimate change error function particular weight changed perturb produces neighboring configuration changing randomly chosen weight change error function due perturbation estimated using error derivatives obtained backpropagation algorithm new configuration accepted unconditionally iff lower error current one otherwise new configuration accepted probability finally program variables updated update state newly chosen configuration note acceptance criterion implemented drawing random number uniform distribution 01 comparing exp eit basic algorithm shown fig3 made efficient one changes function backprop procedure notice backpropagation produces partial derivatives e respect weights whereas use one derivatives subsequent computation better modify backpropagation algorithm compute one derivative required comparison existing algorithms existing learning algorithms eg hopfield net works30 31 based memorizing learning examples accurately cannot used prediction generalization problems two flexible learning methods include backpropagation 9 32 boltzman machine learning 33 iterative algorithms based gradient descent error surface backpropagation error fit given set weights defined eq b1 j c actual state unit j inputoutput training example c j c desired state backpropagation algorithm computes gradient error respect weight hidden unit j layer j affects error via effects units k next layer k derivative error e j given eq b2 index c suppressed clarity dy k weights changed direction reduce error output one may change weights simultaneously avoid conflicting local weight adjustments boltzman machine stochastic nature aim learning weights achieve probability distribution inputoutput mapping boltzman machine learning 5 based series simulated annealing state space network state space network characterized defining state net work state node described output state network ntuple vector one component node learning algorithm aims achieve certain probability distribution states gradient descent minimize error probability distribution use simulated annealing weight space quite different boltzman machine 33 simulated annealing carried state space network learning methods work costerror surface concave since algorithms based simple heuristic gradient descent get stuck local minima furthermore get stuck plateaus gradient small shown fig4 algorithms cannot guarantee optimality discovered weights learning problem npcomplete general 34 remains npcomplete several restrictions surprising heuristic learning algorithms like backpropagation always work cannot trusted find globally optimal weights generalization problems two ways approaching npcomplete problems approximation methods 35 b stochastic enumeration method simulated annealing 36 since difficult formulate general approximation method neural network learning use simulated annealing method extend backpropagation algorithm stochastic weight changes learning weights algorithm convergence properties achieve globally optimal weights simple network generalization implementation performance studies show algorithm performs well many generalization problems global minima figure 4 two bad cases gradient descent 4 modeling analysis stochastic backpropagation given neighborhood structure stochastic backpropagation viewed algorithm continuously attempts transform current configuration one neighbors mechanisms mathematically described means markov chain sequence trials outcome trial depends outcome previous one 37 case stochastic backpropagation trials correspond transitions clear outcome transition depends outcome previous one ie current confi guration markov chain described means set conditional probabilities pair outcomes probability outcome kth trial j given outcome k 1th trial let k denote probability outcome kth trial k obtained solving recursion l k1p li k1k k12c1 sum taken possible outcomes hereinafter xk denotes outcome kth trial hence conditional probabilities depend k corresponding markov chain called homogeneous otherwise called inhomogeneous case stochastic backpropagation conditional probability denotes probability kth transition transition configuration configuration j thus xk configuration obtained k tran sitions view called transition probability r r matrix pk1k transition matrix r denotes size configuration space transition probabilities depend value control parameter temperature thus kept constant corresponding markov chain homogeneous transition matrix defined l 1li g il il forallji ie transition probability defined product following two conditional probabilities generation generating configuration j configuration acceptance probability ij accepting configuration j generated configuration corresponding matrices gt called generation acceptance matrices respectively result definition eqc4 pt stochastic matrix ie 1 gt represented uniform distribution neighborhoods since transitions implemented choosing random neighboring configuration j current configuration computed metropolis criteria ie min1 expdek b stochastic backpropagation algorithm attains global minimum possibly large number transitions say k following relation hold r opt set globally optimal configurations minimum error fit shown eqc5 holds asymptotically ie lim prx k r opt 1 certain conditions matrix l gt l satisfied 2 3 certain additional conditions matrix k rate convergence sequence k faster log k proof carried three steps showing existence stationary distribution homogeneous markov chains b showing convergence innerloop stochastic backpropagation stationary distribu tion c showing stationary distribution final frozen system nonzero probabilities optimal configurations proof last step contingent cooling rate provides us bound rate 41 existence stationary distribution following theorem establishes existence stationary distribution theorem 1 feller 37 stationary distribution q finite homogeneous markov chain exists markov chain irreducible aperiodic furthermore vector q uniquely determined following equation note q left eigenvector matrix p eigenvalue 1 markov chain irreducible pairs configurations ij positive probability reaching j finite number transitions ie markov chain aperiodic configurations r greatest common divisor integers n1 equal 1 case stochastic backpropagation matrix p defined eq c4 since definition guarantee forall ijc sufficient irreducibility check markov chain induced gt irreducible 38 ie g lk lk1 0 k01p1 c10 establish aperiodicity one uses fact irreducible markov chain aperiodic following condition satisfied 38 thus aperiodicity sufficient assume using inequality eq c12 fact forall 1 prove following l 1lit r l g l l 1lit jt r l g l l 1lit jt r g l l 1lit r g l l 1 r g l thus p l 1lit r l g l 0 c14 thus eq c11 holds ii summarizing following result homogeneous markov chain conditional probabilities given eq c4 stationary distribution matrices gt satisfy eqs c10 c12 respec tively note stochastic backpropagation acceptance probabilities defined hence eq c12 always satisfied setting forall 0 r opt j r opt 42 convergence stationary distribution impose conditions matrices gt ensure convergence qt distribution p given eq c5 theorem 2 38 two argument function yei e opt taken i0i arbitrary configuration depend stationary distribution qt given i0i provided matrices g satisfy following conditions proof theorem discussed elsewhere 39 implicitly assumed acceptance probabilities depend cost values configuration configurations hence i0i depend particular choice 0 since ensure eq c5 following condition sufficient 38 thus conditions c19c23 guarantee convergence easily checked matrices gt stochastic backpropagation meet conditions 43 cooling rate certain conditions matrices gt stochastic backpropagation algorithm converges global minimum probability 1 value l control parameterl 012 corresponding markov chain infinite length l eventually converges 0 l ie validity following equation shown limq r opt however cooling rate constraint sequence l control parameterl 012 satisfy certain properties assure convergence globally optimal configurations particular k form g one guarantee convergence globally optimal configurations 40 44 convergence results listed conditions simulated annealing converges globally optimal configurations formulation stochastic backpropagation uses similar acceptance probabilities generation probabilities cooling schedule simulated annealing algorithm 38 use metropolis criteria acceptance cri teria configuration generating mechanism uniform distribution neighbors generation mechanism across two neighboring configuration symmetric one generate arbitrary configuration given configuration finite number steps thus satisfy conditions theorem 1 2 guaranteed convergence global minima follow cooling schedule n logn g satisfy conditions cooling rate guarantees convergence global minima provided g greater depth local minima 5 implementation carried complete implementation stochastic backpropagation conducted validation stu dies implemented algorithm unix platform sequential machine eg sun 360 since generalization often takes large amounts computation plan reimplement algorithm vector processor eg cray xmp speedup current prototype based source code public domain software implementing backpropagation algorithm41 studied software reuse pertinent modules implement stochastic backpropagation implementation comprises 5000 lines c code required approximately seven man months design code debug large fraction effort directed towards reading understanding backpropagation software reuse effort rewarded reduced time designing coding debugging code provided g maximal depth local minima error surface verified walkthrough method extensive usage prototype used seminar courses research backpropagation package 41 three modules user interface learning module testing module user interface implements commands associating commands internal functions via table commands allow users examine modify state software choose options specify type speed computing displays learning module implements backpropagation algorithm computing error derivatives weight adjustments tune weights iteratively training repeats weight adjustments fixed number times till total squared error reaches value set user learning algorithm provides option adjusting weights examining pattern examining patterns testing module computes outputs given inputs neural network also computes total squared error output produced network desired output specified input pattern augmented user interface module adding commands examine modify parameters stochastic backpropagation routines process commands installed table associating commands processing routines command enable user choose alternative learning algorithms also added implemented stochastic backpropagation c simplified cooling schedule cooling schedule based expotential cooling simplicity efficiency cooling schedule used many applications 42 main routine learning module namely trial modified adjust weight based weight randomly choosing neighbor accepting metropolis criteria testing module altered implementation 6 validation evaluated stochastic backpropagation learning algorithm generalization two types functions monotonic functions b non monotonic functions experimental setup consisted four modules data set generator neural network simulator data collection data analysis shown fig 5 data set generator module uses four functions linear quadratic logarithmic trigonometric shown table 1 neural network simulator implements alternative learning algorithms backpropagation stochastic backpropagation takes network configuration data sets module simulates learning algorithm produces outputs well weights data collection module comprises set routines sample state neural network simulator periodically say every 100 epochs learning sample weights collect termination learning data analysis module produces graphs statistics algorithms monitored learning phase well testing phase performance algorithm learning phase measured total square error learning set inputoutput pairs performance algorithm testing phase measured per pattern error new set inputoutput pairs distinct learning set behavior alternative algorithms learning shown figures 6 7 figure 6 shows change total square error along learning steps monotonic functions ie linear logarithmic quadratic functions stochastic backpropagation backpropagation yield comparable total square error tested trained network independent set samples stochastic backpropagation trained network yielded 17 error per pattern backpropagation trained network yielded 09 error per pattern networks predict outputs samples within 5 desired output figure 7 shows change total square error epochs learning nonmonotonic function ie trigonometric function stochastic backpropagation network configuration monotonic non monotonic experimental setup data sets backpropagation stochastic backprop neural network simulator outputs inputs finat weights experiment per datafiles plots statistics plots statistics etc analysis module weights input outpputs weight samples periodic intermediate data collection problems fig 5 experiment design performance comparison study table 1 functions controlled study yields better total square error learning well testing network trained backpropagation network yields per pattern error 15 predicting output 14 samples 50 within 5 desired put network trained stochastic backpropagation yields per pattern error 11 predicts output samples 50 within 5 desired output initial stochastic backpropogation stochastic backpropogation backpropogation2epoch figure learning monotonic functions initial stochastic backpropogation backpropogation2epoch figure 7 learning nonmonotonic functions 7 conclusions stochastic backpropagation provides feasible learning algorithm generalization problems reduces error fit training examples critical generalization problem stochastic backpropagation performs well monotonic functions using simple networks fewer hidden nodes performs better backpropagation algorithm nonmonotonic functions stochastic backpropagation learning algorithm theoretical property convergence also provides stochastic guarantee finding optimal weights however experiments confirm one needs tune parameters implementation stochastic backpropagation get better results 8 acknowledgements acknowledge useful help cs 8199 class spring 1991 urop university minnesota backpropagation package41 9 r handwritten numeral recognition multilayered neural network improved learning algorithm learning learning internal representations back propagating errors learning algorithm boltzman machine linear function neurons structure training art adaptive pattern recognition selforganizing neural network brain style computation learning generalization learning recognize shapes parallel network learning translation invariant recognition massively parallel network design intelligent robot federation geometric mchines neural computers using artificial neural nets statistical discovery observations using backpropagation beyond regression new tools prediction analysis behaviorial sciences connectionist expert systems material handling conservative domain neural connectivity propagation representation continuous functions several variablesby superposition continuous functions one variable addition meaning generalization ch mapping abilities three layered training 3node neural net npcomplete complexity loading shallow networks neural network design complexity learning scaling generalization learning algorithm generalization problems memorization generalization ch creating artificial equation state calculations fast computing machines neural networks physical systems emergent collective computing abilities neural computation decisions optimization problems parallel distributed processing explorations microstructure cognition constraint stisfaction machines learn complexity connectionist learning various node functions computers interactability guide theory npcompleteness optimization simulated annealing introduction probability theory applications probabilistic hill climbing algorithms properties applications theory applications cooling schedules optimal annealing explorations parallel distributed processing handbook models john wiley tr linear function neurons structure training connectionist expert systems art adaptive pattern recognition selforganizing neural network cooling schedules optimal annealing complexity loading shallow neural networks simulated annealing boltzmann machines stochastic approach combinatorial optimization neural computing neurocomputer applications explorations parallel distributed processing neural network design complexity learning training 3node neural network npcomplete parallel distributed processing explorations microstructure cognition vol 1 creating artificial neural networks generalize design intelligent robots federation geometric machines brain style computation computers intractability learning translation invariant recognition massively parallel networks complexity connectionist learning various node functions ctr v kumar shekhar b amin scalable parallel formulation backpropagation algorithm hypercubes related architectures ieee transactions parallel distributed systems v5 n10 p10731090 october 1994