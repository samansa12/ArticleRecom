algorithms modelbased gaussian hierarchical clustering agglomerative hierarchical clustering methods based gaussian probability models recently shown promise variety applications approach maximumlikelihood pair clusters chosen merging stage unlike classical methods modelbased methods reduce recurrence relation simplest case corresponds classical sum squares method show structure gaussian model exploited yield efficient algorithms agglomerative hierarchical clustering b introduction multivariate gaussian models proposed quite time basis clustering algorithms recently methods type shown promise number practical applications 9 examples geophysical sciences include seismic data processing biological sciences classification cell types based chemical responses social sciences classification based attachment theory psychology also used clustering various types industrial financial data imageprocessing applications include unsupervised texture image segmentation tissue classification biomedical images identification objects astronomy analysis images molecular spectroscopy recognition classification surface defects manufactured products agglomerative hierarchical clustering murtagh raftery 8 banfield raftery 1 em algorithm related iterative techniques celeux govaert 3 combination dasgupta raftery 4 effective computational techniques obtaining partitions models subject efficient computation context however received little attention aim fill gap case agglomerative hierarchical clustering although iterative computation involved issue efficiency nevertheless important since practical value methods limited growth time complexity least quadratic number observations paper organized follows remainder section gives necessary background modelbased clustering hierarchical agglomeration section 2 propose computational techniques four simplest common gaussian models compare performance method appropriate benchmark finally extension complex gaussian models discussed section 3 11 modelbased cluster analysis relevant probability model follows population interest consists g different subpopulations density pdimensional observation x kth subpopulation f k x unknown vector parameters given observations denote identifying labels classification comes kth subpopulation classification likelihood approach clustering parameters labels fl chosen maximize likelihood g focus case f k x multivariate normal gaussian mean vector k variance matrix sigma k overall approach much general restricted multivariate normal distributions 1 however experience date suggests clustering based multivariate normal distribution useful great many situations interest 8 1 9 3 f k x multivariate normal likelihood 1 form g 2 set indices corresponding observations belonging kth group replacing k 2 maximum likelihood estimator number elements k yields concentrated loglikelihood w sample crossproduct matrix kth group loglikelihood 3 maximized classifications fl minimize tr wellknown sum squares criterion example suggested ward 11 possible metric proposed agglomerative hierarchical method clustering alternative allows different variance group k fl chosen minimize 1 sigma k groups otherwise structural constraints values fl minimize maximize loglikelihood 5 sigma k allowed vary completely groups loglikelihood maximized whenever fl minimizes equivalent criteria minimized corresponding four parameterizations sigma k criterion g tr criterion g g table 1 four parameterizations covariance matrix sigma k gaussian model corresponding criteria minimized 12 hierarchical agglomeration agglomerative hierarchical clustering ward 11 stagewise procedure opti mal pairs clusters successively merged stage merging corresponds unique number clusters unique partition data classifications differ according criterion optimality strategy choosing single pair one optimal modelbased hierarchical clustering maximumlikelihood pair merged stage although resulting partitions suboptimal agglomerative hierarchical clustering methods common use often yield reasonable results relatively easy compute modelbased clustering another advantage hierarchical agglomeration associated bayesian criterion choosing best partition hence optimal number clusters among defined hierarchy 1 hierarchical clustering accomplished splitting rather agglomeration complexity algorithms combinatorial unless severe restrictions allowed subdivisions applicable process hierarchical agglomeration usually assumed start observation cluster proceed observations single cluster however could well started given partition proceed form larger clusters value returned consists classification tree list pairs clusters merged possibly optimal value change criterion stage classical agglomerative methods e g sum squares nearest farthest neighbor single complete link 7 metric cost based geometric considerations associated merging pair clusters particular pair cost remains fixed long neither clusters pair involved merge time complexity hierarchical agglomeration significantly reduced cost merging pairs retained updated course algorithm overall memory usage proportional square initial number clusters usually initial number observations could severe limitation large data sets one possible strategy apply hierarchical agglomeration subset data partition remaining observations via supervised classification discriminant analysis banfield raftery 1 used 522 26000 pixels initial hierarchical phase successfully classify tissue mri brainscan image via gaussian modelbased techniques classical method simple recurrence relation updating cost merging pairs sumofsquares method recurrence deltai j cost merging groups j hi ji represents group formed merging groups j initial cost merging pair obtained computation proceed without reference data size group must retained updated amount space needed deltai decreases number groups increases memoryefficient scheme maintaining deltai j follows assume without loss generality observation number k observation smallest index group k initial classification group j 1 values deltaj stored j easy recover space course computation assuming j highest index particular merge l largest current index space associated group j used group l thereby freeing larger space associated group l original indexes groups easily recovered end programming languages fortran 77 memory allocation static values deltaj stored sequentially order delta2 1 delta3 1 delta3 2 delta4 1 delta4 2 delta4 scheme described leaves contiguous free space used classification tree return values languages c allow dynamic memory allocation separate list values deltaj maintained j space associated list largest value j freed stage scheme modelbased methods generally require computational resources classical meth ods advantage storing cost merging pairs require relatively expensive computations determinants crossproduct matrices object paper show relatively efficient methods agglomerative hierachical clustering based gaussian models efficient algorithms four basic models clearly structure exploited various criteria w k symmetric positive semidefinite matrix see table 1 section 11 moreover since two groups merged stage hierarchical agglomeration close relationship criteria successive stages fact sample crossproduct matrix merged group obtained sum sample crossproduct matrices two component groups means symmetric rank1 update k denotes sum observations group k derivation given appendix remainder section show relation leads efficient algorithms methods table 1 assume input consists n theta p matrix whose rows correspond individual observations vector length n indicating initial classification observation 21 sigma k covariance matrix constrained diagonal uniform across groups criterion minimized stage tr g tr sumofsquares criterion long known heuristic relationship gaussian model recognized tr w k sum squares observations group k group mean subtracted classical methods one known underlying statistical model view recurrence relation 4 required start hierarchical clustering procedure set values deltai j number observations group first value deltai pair observations computed absence information individual observations usually constitute initial partition data nothing need done coarser initial partitions recurrence relation could used obtain initial values hierarchical clustering given deltai pair observations merges initialization determined given partition rather minimum value deltai j stage process described however requires storage proportional square number observations n undesirable n groups begin update formula 5 leads better initialization procedure deltai j since tr assuming k smallest index associated observations group k overwrite kth observation sum k observations group kth element classification vector n k accomplished onp time requires additional storage since input overwritten 8 6 used initialize deltai j total storage required would onp input om 2 deltai j 22 sigma k convariance group constrained diagonal otherwise allowed vary groups criterion minimized stage g g tr sumofsquares classical methods deltai remains unchanged stage stage unless either group group j involved merge storing deltai results gain time efficiency hierarchical clustering update formula 5 tr tr unlike classical methods simple recurrence relation deltahi ji deltai j deltai k deltaj k however reasonably efficient update required maintain values n k tr addition k vector k overwrite kth observation done sum squares time group k one element index k 0 next observation group stored kth element classification vector number elements group stored k 0 th entry classication vector trace sample cross product matrix corresponding term criterion overwrite first two elements k 0 th observation scheme additional storage necessary p 2 beyond required deltai j input issue terms tr w k remains resolved includes terms corresponding groups consisting single observation well groups observations coincide hence first stages hierachical clustering arbitrary without sort initialization procedure replace 9 modified criterion order handle cases transparently g tr tr w w sample cross product matrix group consisting observations default value 1 factor tr w np attempt take account scaling data since resulting criterion scale dependent 23 constant sigma k covariance matrix uniform across groups otherwise structural constraints criterion minimized stage g contrast methods discussed point change criterion caused merging two groups affected merges among groups current classification hence advantage store deltai j duration computation nev ertheless 5 leads efficient update change merged represented instead w maintain lower triangular cholesky factor l w see e g 6 since determinant easily computed square product diagonals l diagl noting since observation group either build cholesky factor coarser initial partition else merge optimal groups hierarchical agglomeration follows theta theta theta theta theta theta theta theta theta theta thetac c c c c givens theta theta theta theta theta theta theta theta theta givens theta theta theta theta theta theta theta theta givens theta theta theta theta theta theta theta theta theta givens theta theta theta theta theta theta theta theta symbol givens gamma stands application givens rotation elementary orthogonal transformation allows selective numerically stable introduction zero elements matrix marked entries values changed last transformation time efficiency cholesky update op 2 contrast op 3 forming new cholesky factor updated p theta p matrix w details cholesky update via givens rotations see e g 6 although criterion defined possible partitions remains problem initialization rank less p particular first stages hierachical clustering using criterion arbitrary initially observation cluster since merging pair observations j result singletons p 2 circumvent use sumofsquares criterion tr w determine merges value jw j positive maintaining cholesky factor w computation proceeds k overwrites data n k overwrites classification vector information needed recover classification tree optimal values criterion stored portions structures longer needed algorithm necessary op 2 storage maintaining l additional storage size om number stages needed p 4 store merge indexes order completely reconstruct classification tree 24 unconstrained sigma k covariance matrix allowed vary completely groups criterion minimized stage g like criteria discussed sections 21 22 group contributes separate additive term 13 time efficient save values deltai j l k denotes cholesky factor w k view 5 l hiji computed efficiently l l j w ij applying givens rotations see section 23 composite matrix composite matrix never explicitly formed instead w ij row l treated separate updates l although time efficiency form updated cholesky factor order magnitude forming new cholesky factor updated w hiji use 14 advantage storage efficiency maintaining upper triangle w k updating directly via 5 would require storage since w k p rows regardless n k whereas l k minn overwrite data entries corresponding lower triangle l k used necessary pointers values updated kth term 13 besides required data deltai j additional op 2 storage needed cholesky factors updating deltai j finally jw k even greater ambiguity criterion either 11 9 reason use g tr w place 13 gives hybrid modified criterion sigma 10 13 default value 1 2 1 25 benchmark comparisons section compare algorithms developed sections 2124 approaches use update formula 5 otherwise efficient benchmark algorithms leave data intact keep track composition groups classification vector used transmit initial partition sigma sigma updated merge first forming column means combined group forming sum squares observations groups column mean subtracted constant sigma k upper triangle merge updated using first part 12 column sums w w j computed list observations group added form column sum w hiji upper triangle w formed using symmetric rank one operations quantity jw j merge computed cholesky decomposition described section 23 unconstrained sigma k upper triangle w hiji formed scratch merge involving j cholesky decomposition used obtain determinant addition op 2 storage used sample crossproduct matrices storage allocated benchmarks return values number groups cluster facilitate updating deltai j term contributed cluster criterion methods section 3 use considerably less storage constant variance requires additional storage recover results constant unconstrained variance require op 2 additional storage maintaining cholesky factors figure shows marked gains time efficiency methods section 2 benchmarks randomly generated observations dimension used default initial partition singleton observation constitutes cluster basic methods written fortran splus interface time shown average nine different data sets using splus version 33 1 silicon graphics iris workstation irix 52 operating system solid line represents performance algorithms based update formula 5 dashed line represents performance algorithms necessary quantities obtained without updating effect dramatic unconstrained case extrapolated results ignoring effects increased memory usage show improvement factor around 15 compared factor around 4 500 results sigma also point comparison since case solid line represents classical sum squares approach via wellknown recurrence relation 4 note time scale constantvariance method differs methods use memory exchange improved time efficiency 3 extension complex gaussian models banfield raftery 1 developed modelbased framework subsumes parameterizations table 1 resulting clustering methods include criteria general sigma constant sigma k still constraining structure sigma k accomplished means reparameterization covariance matrix sigma terms eigenvalue decomposition k orthogonal matrix eigenvectors k diagonal matrix whose elements proportional eigenvalues sigma k k scalar proportional volume ellipsoid orientation principal components sigma k determined k k determines shape density contours paradigm particularly useful two threedimensional data geometric features often identified visually may also applicable higherdimensional data multivariate visualization analysis structure example banfield raftery 1 able closely match clinical classification biomedical data set using gaussian hierarchical clustering analyzing geometric features parameterization selected allow characteristics orientation volume shape distributions vary groups constraining others analysis model leads sum squares criterion sigma 16 suggests likely appropriate groups spherical approximately size constantvariance assumption k k k groups otherwise unconstrained favors clusters ellipsoidal 1 splus 33 version 33 unix mathsoft inc seattle wa 1995 number observations time 100 200 300 400 500103050diagonal uniform number observations time 100 200 300 400 500103050diagonal varying number observations time 100 200 300 400 500103050unconstrained number observations time 100 200 300 400 50050015002500constant variance figure 1 cpu time vs number observations four basic models solid line represents methods proposed paper orientation shape volume elements sigma k allowed vary groups resulting classification likely contain elliptical groups differing geometric features metrics appropriate various intermediate situations also formulated example assuming sigma oe 2 implies underlying densities spherical variation k groups allows volumes differ celeux govaert 2 analyzed criterion showed give classification performance much better traditional methods one example successfully apply method astronomical image one tightly clustered galaxy contained within another dispersed one table shows relationships orientation volume shape discussed 1 criteria based combinations factors also possible 3 software hierarchical clustering based models available public domain see 1 used variety applications success 9 revision based techniques described paper currently progress efficient computational methods first four models table 2 given section distribution volume shape orientation reference spherical fixed fixed na 115 10 8 1 3 spherical variable fixed na 1 3 dad elliptical fixed fixed fixed 5 10 1 3 elliptical variable variable variable 10 1 3 fixed fixed variable 8 1 3 elliptical variable fixed variable 1 3 table 2 parameterizations covariance matrix sigma k gaussian model geometric interpretation models shown discussed banfield raftery 1 2 conclude section showing techniques applied remaining relevant criteria gamma1omega respectively k diagonal matrix eigenvalues w k cases efficient algorithms possible k l maintained updated storage provided original data described section 24 instead information pertaining terms 15 kth term sum appropriate criterion stored space corresponding lower triangle l k methods section 3 matrix w hiji never explicitly formed instead nonzero eigenvalues obtained squares singular values l minn rows p columns sigma include additive term inside logarithm appears 10 sigma oe 2 k order accommodate cases w k vanishes concluding remarks paper made several contributions toward computational efficiency agglomerative hierarchical clustering first gave memoryefficient scheme suitable method stores change criterion merged pair second showed sample cross product matrix union two gaussian clusters formed rankone update sum samplecross matrices constituent clusters described used obtain efficient algorithms modelbased clustering included memoryefficient initialization strategy sum squares method corresponds simplest gaussian model well time memory efficient algorithms three gaussian models counterpart classical hierarchical agglomeration time gave strategies resolve inherent ambiguities models finally showed techniques easily extended two additional gaussian models based sophisticated parameterization covariance matrix recently shown promise practical applications derivation update formula following holds group hi ji formed merging groups denotes sample crossproduct matrix group k k sum observations n k cardinality group k proof let x k matrix observations corresponding group k denotes vector length n k every element equal 1 g x k x k mean subtracted column w hence since follows f hiji since x hiji matrix consisting observations groups j hence gamman gamman r comparison mixture classification maximum likelihood cluster analysis gaussian parsimonius clustering models detecting features spatial point processes clutter via modelbased clustering invariant criteria grouping data matrix computations finding groups data fitting straight lines point patterns transitions onr contract n0001491j1074 time series image analysis clustering methods based likelihood ratio criteria hierarchical groupings optimize objective function tr ctr pigeau gelgon building tracking hierarchical geographical temporal partitions image collection management mobile devices proceedings 13th annual acm international conference multimedia november 0611 2005 hilton singapore jun liu jim p lee lingjie li zhiquan luo k max wong online clustering algorithms radar emitter classification ieee transactions pattern analysis machine intelligence v27 n8 p11851196 august 2005 bin tao tao kevin chenchuan chang organizing structured web sources query schemas clustering approach proceedings thirteenth acm international conference information knowledge management november 0813 2004 washington dc usa peter meinecke helge ritter resolutionbased complexity control gaussian mixture models neural computation v13 n2 p453475 february 2001 marina meil david heckerman experimental comparison modelbased clustering methods machine learning v42 n12 p929 januaryfebruary 2001 j robinson f murtagh p basheer gaussian segmentation bse images assess porosity concrete proceedings sixth conference computational structures technology p253254 september 0406 2002 diansheng guo donna j peuquet mark gahegan iceage interactive clustering exploration large highdimensional geodata geoinformatica v7 n3 p229253 september nevin l zhang hierarchical latent class models cluster analysis eighteenth national conference artificial intelligence p230237 july 28august 01 2002 edmonton alberta canada nevin l zhang hierarchical latent class models cluster analysis journal machine learning research 5 p697723 1212004 huidong jin manleung wong k leung scalable modelbased clustering large databases based data summarization ieee transactions pattern analysis machine intelligence v27 n11 p17101719 november 2005 shi zhong joydeep ghosh unified framework modelbased clustering journal machine learning research 4 p10011037 1212003 fionn murtagh clustering massive data sets handbook massive data sets kluwer academic publishers norwell 2002