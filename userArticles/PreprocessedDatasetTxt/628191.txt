finding localized associations market basket data abstractin paper discuss technique discovering localized associations segments data using clustering often aggregate behavior data set may different localized segments cases desirable design algorithms effective discovering localized associations expose customer pattern specific aggregate behavior information may useful target marketing present empirical results show method indeed able find significantly larger number associations discovered analysis aggregate data b introduction market basket data consists sets items bought together customers one set items called transaction recent years considerable amount work done trying find associations among items large groups transactions 3 4 considerable amount work also done finding association rules beyond traditional supportconfidence framework provide greater insight process finding association rules traditional notions support confidence 2 7 8 12 21 however methods try find associations aggregate data opposed finding associations small localized segments take advantages natural skews correlations small portions data recent results 1 24 shown thereare considerable advantages using concepts data locality localized correlations problems clustering indexing paper builds upon flavor techniques paper focus segmenting market basket data generate extra insight discovering associations localized small segments data considerable impact deriving association rules data since patterns cannot recognized aggregate basis often discovered individual segments associations referred personalized associations applied useful target marketing moreover algorithm directly adapted segment categorical data categorical data set associated set attributes attribute takes finite number nonnumerical values record data consists set values one attribute record transformed transaction simple manner creating item categorical value however method specifically applicable case discovering useful associations market basket data opposed finding well partitioned clusters categorical data problem clustering widely studied literature 5 6 9 10 11 13 14 15 recent years importance clustering categorical data received considerable attention researchers 13 14 16 17 17 clustering technique proposed clusters items used order cluster points merit approach recognized fact connection correlation among items clustering data points concept also recognized gibson 14 uses approach based nonlinear dynamical systems effect technique called rock 16 recently proposed uses number common neighbors two data points order measure similarity thus method uses global knowledge similarity data points order measure distances tends make decision points merge single cluster robust time algorithm discussed 16 take account global associations individual pairs items measuring similarities transactions hence good similarity function space transactions take account item similarities moreover approach increase number item associations reported end data segmentation initial motivation considering clustering data recently fast summarization based algorithm called cactus proposed 13 categorical data clustering techniques classified two categories partitional hierarchical 19 partitional clustering set objects partitioned clusters objects cluster similar one another clusters many methods work cluster representatives used anchor points assigning objects examples methods include well known kmeans kmedoid techniques advantage class techniques even large databases possible work memory efficient way small number representatives periodic disk scans actual data points agglomerative hierarchical clustering start placing object cluster merge atomic clusters larger clusters bottom fashion desired number clusters obtained direct use agglomerative clustering methods practical large databases since performance methods likely scale least quadratically number data points paper discuss method data clustering uses concepts agglomerative partitional clustering conjunction random sampling make robust practical scalable large databases primary focus paper slightly different categorical clustering algorithms past wish use technique tool finding associations small segments data provide useful information localized behavior cannot discovered otherwise paper organized follows remainder section discuss definitions notations similarity measures algorithm clustering discussed section 2 corresponding time complexity section 3 empirical results contained section 4 finally section 5 contains conclusions summary 11 intuitive understanding localized correlations order provide intuitive understanding importance localized correlations let us consider following example data set drawn database supermarket cutomers database may contain customers many types example transactions drawn extremely cold geographical regions alaska may contain correlations corresponding heavy winter apparel whereas may present aggregate data often present high degree rest database correlations cannot found using aggregate analysis since present great degree overall basis attempt find correlations using global analysis lowering support result finding large number uninteresting redundant correlations created simply chance throughout data set 12 definitions notations introduce main notations definitions need presenting method let u denote universe items transaction set drawn items u thus transaction contains information whether item bought customer however results easily generalized case quantities associated item metatransaction set items along integer weights associated item shall denote weight item metatransaction wtm transaction also metatransaction integer weight 1 associated item define concatenation operation two metatransactions 0 following way take union items 0 define weight item concatenated metatransaction adding weight item 0 item present metatransaction weight assumed zero thus meta transaction may easily obtained set transactions using repeated concatenation constituent transactions concept metatransaction important since uses cluster representatives note metatransaction somewhat similar concept metadocument used information retrieval applications order represent concatenation documents correspondingly vectorspace model 23 may also used order represent metatransactions vector space model metatransaction vector items weight associated given entry equal weight associated corresponding item projection metatransaction defined new metatransaction 0 obtained removing items interested removing items smallest weight metatransaction intuitively projection metatransaction created set items corresponds process ignoring least frequent thus least significant noisy items set 13 similarity measures support set items 3 defined fraction transactions contain items support pair items indication level correlation presence set items often used association rule literature order identify groups closely related items market basket data shall denote aggregate support set items x supx support relative subset transactions c denoted sup c x order measure similarity pair transactions first introduce measure similarity pair items call affinity items affinity two items j denoted ai j ratio percentage transactions containing j percentage transactions containing least one items j formally similarity pair transactions defined average affinity items formally similarity pair metatransactions defined analogue manner except weigh terms summation products corresponding item weights two metatransactions similarity 0 defined j q ai j interesting observation similarity measure two transactions item common constituent items highly correlated high similarity greatly desirable since transaction data sparse often closely correlated transactions may share many items 2 clustering algorithm section describe clasd clustering association discovery algorithm overall approach uses set cluster representatives seeds used order create partitions finding good set cluster representatives around partitions built critical success algorithm would like total k clusters k parameter specified user assume k users indication approximate number clusters desired initial number seeds chosen method denoted startsize always larger final number clusters k expect many initial set cluster representatives may belong cluster may belong cluster reason start larger number representatives target iteratively remove outliers merge representatives belong closest cluster iteration reduce number cluster representatives factor ff picked closest pair representatives iteration merged thus two representatives belong natural cluster would expect merged automatically process expensive hierarchical agglomeration straightforward computation requires 2 time merge n total number representatives discuss detail later section found precomputation maintainance certain amount information nearest neighbors seed effective option implementing operation effectively overall algorithm illustrated figure 1 individual merge operation illustrated figure 2 merge project resulting metatransaction described figure 3 helps removal noisy items closely related cluster also helps us speed subsequent distance computations use partitioning techniques order include information entire database iteration order ensure final representatives created tiny random sample startsize seeds however expensive since process partitioning full database startsize partitions require considerable time iteration therefore pick option performing random sampling database order assign transactions seeds increase value samplesize factor ff iteration factor number representatives iteration decreased thus later phases clustering benefit larger samplesizes useful since robust computations desirable later iterations addition becomes computationally feasible pick larger samplesizes later iterations since assignment process dependant current number cluster representatives process performing database sample partitions indicated figure 4 points outliers would points assigned points removed automatically clustering algorithm process accomplished procedure killdelta described figure 5 value threshold found effective purpose experiments 20 average number transactions assigned representative process continues one two conditions met either k clusters set k parameter specified user largest k clusters assignment phase contain significant percentage transactions parameter denoted threshold 0 updatecriterion procedure figure 6 purpose algorithm picked threshold 0 33 total transactions assigned representatives thus unlike many clustering methods whose computation output strictly determined input parameter k equal number output clusters clasd relaxed dependency k thus number final clusters smaller k correspond natural grouping data regard k users estimation granularity partitioning would generate localized item associations granularity sufficient provide significant new information localized behavior whereas restricted enough partition statistical meaning let n total number transactions database size initial set seeds randomly chosen transactions two constants experiments use 30 reason choosing value n dependent n ensure running time merge operation linear number transactions time need ensure number representatives atleast linearly dependent number output clusters k correspondingly choose value n discussed parameters appearing figure 1 set following values experiments report next sections 100 parameter fi used figure 3 10 algorithm begin precompute affinities item pairs currentsize g initial set startsize seeds terminationcriterion begin construct r randomly picking samplesize transactions database f final partitioning database g let r entire database transactions report c figure 1 clustering algorithm algorithm mergecluster representative integer mergecount begin mergecount begin replace b ab concatenation b figure 2 merging procedure algorithm projectcluster representative number items fi fixed constant begin sort items decreasing order weights first item order item items weight largest weights set remaining weights 0 figure 3 projecting least important items procedure assigncluster representatives transactions begin 2 begin cluster representative assign cluster c 2 redefine concatenating transactions c returnm c end figure 4 assigning transactions clusters procedure killcluster representatives clusters c integer threshold begin 2 c contains less threshold points discard figure 5 removing outlier representatives procedure updatecriterioncluster representatives clusters c k threshold 0 begin jmj largest k clusters c threshold 0 transactions figure updating termination criterion 21 implementing merging operations effectively finally discuss issue merge cluster representatives effectively idea precompute nearest neighbors representative beginning merge phase use precomputed neighbors order find best merge merge operation metatransaction nearest neighbor denoted henceforth nnm need delete nnm nearest neighbor lists representatives occured added merged metatransaction 0 appropriate nearest neighbor lists list becomes empty recompute scratch implementations discussed rock 16 ordered list distances clusters maintained cluster heap data structure results olog n time per update delta log n time finding best merge however space complexity quadratic terms initial number representatives n implementation precomputed constant number nearest neighbors cluster representative implementation uses linear space updating lists takes time per iteration list becomes empty solution also better always recomputing single nearest neighbor list size 1 representative maintaining larger list size reduces likelihood many lists becoming empty corre spondingly likelihood spend 2 one iteration highly decreased experiments maintain 5 closest neighbors representative general least one merged representatives nnm appeared list representative n 2 resulting metatransaction 0 one 5 nearest neighbors n experiments performed list ever became empty thus effectively achieving linear space time maintenance lists 22 reporting item correlations discussed figure 1 transactions assigned cluster representative final pass database integrated application correlations reported output clustering algorithm support counting parallelized assignment process final pass assigning transaction cluster representative also update information support sup c fi jg 2item respect corresponding cluster c end report pairs items whose support user specified threshold least one cluster 3 time space complexity discussed clasd space requirements linear n maintain constant number neighbors cluster also need store affinity ai pair items denote total number items data overall space required clasd let q time required calculate similarity function thr running time may computing summing times required merge assignment operations nearest neighbor lists need initialized log ff nk sequences merge operations requires 2 delta q delta log ff nk time let n r denote total number recomputations nearest neighbor lists merges entire algorithm requires time determining optimum pair merged iteration takes time assignment procedure requires size random sample increases factor number clusters decreases last pass data assigns entire data set rather random sample requires n delta k delta q time summing everything obtain overall running time 2 delta q log ff choosing different values initsamplesize allows us tradeoff accuracy running time method discussed earlier value n r turned small runs contribute significantly finally report item correlations spend number disk accesses access entire data set thrice first access compute affinity matrix choose random sample initial seeds also choose independent random samples r appropriate sizes used various iterations refine clustering information many samples need sizes easily computed know n k ff initsamplesize random sample stored separate file since sizes random samples geometrically increasing order largest random sample 1ff times full database size accessing files requires equivalent one pass data ff 2 last pass data used final assignment transactions cluster representatives 4 empirical results simulations performed 200mhz ibm rs6000 computer 256mb memory running aix 43 data stored 45gb scsi drive report results obtained real synthetic data cases interested evaluating extent segmentation method helps us discover new correlation among items well scalability algorithm first explain data generation technique report experiments also implemented rock algorithm see 16 tested synthetic real data sets method distance two transactions proportional number common neighbors transactions two transactions 1 2 neighbors number clusters k value main parameters method 41 synthetic data generation synthetic data sets generated using method similar discussed agrawal et al 4 generating data sets two stage process 1 generating maximal potentially large itemsets first step generate potentially large itemsets potentially large itemsets capture consumer tendencies buying certain items together first picked size maximal potentially large itemset random variable poisson distribution mean l successive itemset generated picking half items current itemset generating half randomly method ensures large itemsets often common items itemset weight w associated chosen exponential distribution unit mean 2 generating transaction data large itemsets used order generate transaction data first size transaction chosen poisson random variable mean transaction generated assigning maximal potentially large itemsets succession itemset assigned transaction chosen rolling l sided weighted die depending upon weight w assigned corresponding itemset itemset fit exactly assigned current transaction half time moved next transaction rest time order capture fact customers may often buy items potentially large itemset together added noise process corrupting added itemsets itemset decide noise level 2 0 1 generated geometric random variable g parameter n adding potentially large itemset transaction dropped minfg jijg random items transaction noise level n itemset chosen normal distribution mean 05 variance 01 shall also briefly describe symbols used order annotate data three primary factors vary average transaction size size average maximal potentially large itemset l number transactions considered data set transactions denoted t10i4d100k overall dimensionality generated data ie total number items always set 1000 42 synthetic data results remainder section denote ais aggregate itemsets set 2itemsets whose support relative aggregate database least also denote cis cluster partitioned itemsets set 2itemsets support relative least one clusters c generated clasd algorithm ie pair items exists cluster c k 2 c support c k least shall denote cardinality sets ais cis respectively easy see ais cis itemset satisfies minimum support requirement respect entire database must also satisfy respect least one partitions follows ais cis order evaluate insight gain item correlations using clustering method consider following issues 1 2itemsets cis required smaller support relative entire data set ones ais since jc j n cluster c 2 c thus compare output ais also ais 0 values 2 mentioned ais cis thus method always discovers correlations among pairs items due relaxed support requirements however want estimate discriminative process finding correlations words want know much gain clustering method versus random assignment transactions k groups number 2itemsets aggregate data itemsets aisk partition itemsets cis aggregate data itemsets ais figure 7 comparison clustered itemsets aggregate itemsets experiments reported size random set seeds clustering algorithm 30k figure 7 used t20i6d100k dataset experiments run value 0 chosen sk reason k output clusters size required support 2itemset reported clustering algorithm would decrease factor k hence want know would happen didnt clustering instead would simply lower support factor k expected graph representing cis function lies ais ais 0 however note ratio ais 0 cis large 3 means clustering helps us prune 75 82 2itemsets would reported lowered support thus get benefit discovering new correlations among items without overwhelmed large output mixes useful irrelevant information next tested number 2itemsets reported end clustering procedure compare number 2itemsets would obtain randomly grouped transactions k groups used t20i6d100k set set first experiment assigned transactions uniformly random one k groups let ris denote set itemsets support least relative least one partitions corresponding cardinality denoted ris second experiment incorporated knowledge gained clustering method exactly created partition data k groups size ith number 2itemsets partition itemsets cis clustersize random partition itemsets ris equal random partition itemsets ris figure 8 comparison clustering random partitions group equal size ith cluster obtained algorithm transactions randomly assigned groups subject size condition corresponding set denoted ris results shown figure 8 since ris ris values considered restrict attention ris used dataset cisris varies 27 shows clustering method significantly outperforms indiscriminate grouping data finally figure 9 show number clusters k influences number 2item sets report t20i6d100k set used compare ais constant case ais 0 use 00025 fact cis increases k corresponds intuition grouping clusters implies lowering support requirement examples illustrate itemsets found different depending upon whether use localized analysis aggregate analysis perspective target marketing customer segmentation application correlations may much greater value obtained analysis full data set next two figures illustrate method scales number clusters k size dataset n figure 10 fix figure 11 fix separately graph running time spent finding cluster representatives performing final partitioning procedure entire database clusters using number clusters k number 2itemsets aggregate data itemsets aisk parition itemsets cis aggregate data itemsets ais figure 9 comparison clustered itemsets aggregate itemsets representatives clearly running time final phase requires ok delta n distance computations something cannot hope easily outperform algorithm attempts put n points k clusters show running time entire algorithm phase small compared running times close optimal analysis previous section shows time quadratic size random sample turn linear k hand time spent last step method assign transactions respective clusters linear k n noted figures 10 11 last step clearly dominates computation overall method scales linearly k n comparison rock ran rock synthetically generated t20i6d100k dataset used setting minimum overlap required two neighbor clusters wanted report 2itemsets support 00025 least one generated clusters rock obtained clustering one big cluster contained 86 transactions second cluster 3 transactions sizes remaining clusters varied around 1 data result surprising take account fact overall dimensionality data space 1000 average number items transaction 20 thus likelihood two transactions sharing significant percentage items low means number pairs neighbors database quite small turn implies number links two transactions even smaller recall number links number clusters k running time seconds total time finding cluster representatives figure 10 running time scalability number clusters 5100030005000number transactions n running time seconds total time finding cluster representatives figure 11 running time scalability number transactions output clusters edible poisonous output clusters edible poisonous 9 150 124 19 232 0 figure 12 correspondence output clusters original labeling two transactions number common neighbors moreover algorithm runs random sample computes number links transactions relative sample decreasing even likelihood nonzero number links two transactions since distance two clusters proportional number links pairs transactions two clusters result cluster distances almost always equal zero hence data agglomerates first cluster time discover cluster distance zero nearest neighbor thus choose different candidate merging quality results could improved either increasing size random sample experiment random sample size random sample clasd run decreasing increasing size random sample limited option however since rock quadratic storage requirements changing note two transactions size 20 current value required 4 items common order considered neighbors smaller value indeed increase number neighbor pairs notion neighbors tends lose meaning since required overlap insignificant ran two experiments follows first experiment doubled size random sample second one set cases generated clusters achieved balance sizes yet number 2itemsets reported discovered clasd 28983 first experiment 38626 second experiment compared 56861 discovered clasd moreover methods described sensitive changes overall dimensionality data conclude due implicit assumption large number transactions data set reasonably well correlated rock perform well data assumptions hold synthetically generated data designed 4 resemble real market basket data example input tested clasd rock mushroom adult data sets ml repository 1 43 mushroom data set mushroom data set contained total 8124 instances entry 22 categorical attributes eg capshape odor etc labeled either edible poisonous transformed record transaction manner used 16 attribute value v domain introduce item av record r transformed transaction contains item av r value v attribute ran rock parameters indicated 16 ie able confirm results reported authors data set minor differences number 2itemsets discovered clustering 1897 support level test clasd data set must take account fact attribute values domain fv wg items av aw never appear transaction want make distinction situation situation two items appear together transaction database yet exclude one another hence define negative affinity every pair items av aw present results figure 12 exception cluster number 9 draws half records two classes mushrooms clusters clearly belong either edible poisonous categories although small percentage records category eg clusters 1 2 believe latter due fact distance function heuristic designed maximize number 2itemsets enough support cluster may induce absorption poisonous mushrooms group edible mushrooms example increases support many pairs items number 2itemsets discovered 2155 support level superior reported rock also computed number 2itemsets using original labels group data two clusters edible poisonous found 1123 2itemsets enough support relative least one cluster hence segmentation method proves useful discovering interesting correlations items even previous labeling exists aside also found interesting correlations cannot found looking labels support level could find correlation convex caps pink gills since pair characteristics appears together 750 species mushrooms 92 data correlation pair useful enough recognized reported also could discover c orrelation treating original labels two clusters 406 edible species two characteristics 96 edible entries 344 poisonous species 87 poisonous entries however technique creates structured segmentation correlation found 44 adult data set also tested algorithm adult data set uci machine learning respository data set extracted 1994 census data contained information data people data set total 32562 instances among instances 8 categorical valued first transformed categorical attributes binary data resulted total 99 binary attributes one observation data set significant number attributes data corresponded particular value example records united states one fields private large fraction time data largely consisted whites much information particular attributes rock algorithm often built linkages based attributes resulted random assignments points clusters applied clasd algorithm data set several interesting 2itemsets local clusters could discovered aggregate data set example found clusters contained disproportionate number females either unmarried status notinfamily clusters support female unmarried female notinfamily 30 behavior reflected clusters disproportionately high number men case men largest share tended husbands thus asymmetry men women terms localized 2itemsets discovered asymmetry may function data picked census specifically data corresponds behavior employed people analysis indicates segments population large fractions employed women unmarried families correlation cannot discovered aggregate support model since required support level 81 designed catch 2itemsets low results extraodinarily high number meaningless 2itemsets along useful 2itemsets meaningless 2itemsets create difficulty distinguishing useful information data noise another interesting observation cluster found following 2itemsets craftrepair male craftrepair hsgrad tended expose segment data containing males relatively limited education involved job craftrepair similar segmented 2itemsets corresponding males limited educational level observed professions transportmoving farmingfishing another interesting 2itemset discovered one segments data set doctorate profspecialty probably corresponded segment population highly educated involved academic activities professorships also discovered small cluster 2itemset amerindianeskimo hsgrad support 50 also entries segment corresponded americanindian eskimos tended educational level ranged 9th grade high school graduate exposes interesting demographic segment people shows particular kind association note absolute support association relative entire data set extremely low since 311 instances american indian eskimos data base 32562 instances less 1 2itemsets cases support low discovered aggregate analysis turned quite dominant interesting particular segments population 5 conclusions summary paper discussed new technique clustering market basket data technique may used finding significant localized correlations data cannot found aggregate data empirical results illustrated algorithm able find significant percentage itemsets beyond random partition transactions information may prove useful target marketing applications algorithm also generalized easily categorical data showed cases algorithm performs better rock finding localized associations r finding generalized projected clusters high dimensional spaces new framework itemset generation mining association rules sets items large databases fast algorithms mining association rules large databases automatic subspace clustering high dimensional data data mining applications optics ordering points identify clustering structure mining surprising patterns using temporal description length beyond market baskets generalizing association rules correlations incremental clustering mining data warehousing environment database interface clustering large spatial databases density based algorithm discovering clusters large spatial databases noise data mining knowledge discovery overview cactus clustering categorical data using summaries clustering categorical data approach based dynamical systems cure efficient clustering algorithm large databases rock robust clustering algorithm categorical attributes clustering based association rule hypergraphs snakes sandwiches optimal clustering strategies data warehouse algorithms clustering data finding groups data introduction cluster anal ysis finding interesting rules large sets discovered association rules efficient effective clustering methods spatial data mining introduction modern information retrieval local dimensionality reduction new approach indexing high dimensional spaces distributionbased clustering algorithm mining large spatial databases comparative study clustering methods birch efficient data clustering method large databases tr ctr antonin rozsypal miroslav kubat association mining timevarying domains intelligent data analysis v9 n3 p273288 may 2005 hua yan keke chen ling liu efficiently clustering transactional data weighted coverage density proceedings 15th acm international conference information knowledge management november 0611 2006 arlington virginia usa alexandros nanopoulos apostolos n papadopoulos yannis manolopoulos mining association rules large clustered domains information systems v32 n5 p649669 july 2007