measuring software dependability robustness benchmarking abstractinability identify weaknesses quantify advancements software system robustness frequently hinders development robust software systems efforts made develop benchmarks software robustness address problem suffer significant shortcomings paper presents various features desirable benchmark system robustness evaluates existing benchmarks according features new hierarchically structured approach building robustness benchmarks overcomes many deficiencies past efforts also presented approach applied building hierarchically structured benchmark tests part unix file virtual memory systems resultant benchmark successfully used identify new response class structures detected similar situation less organized techniques b introduction given current scarcity tools measure robustness software system operating system developers lack means focus attention issues affecting system robustness system developers long used suites performance tests aid development high performance machines application programs believe suite robustness tests would similarly useful gauging development robust systems providing means compare robustness among various systems throughout paper many examples presented context evaluating robustness operating system however issues examined arise evaluating robustness complex software system robustness benchmark suite robustness tests stimuli benchmark address issues general enough apply wide range systems yet specific enough provide basis differentiation according system robustness essentially robustness benchmark aims stimulate system ways likely trigger internal errors thereby expose design errors error detection recovery mechanisms differentiation amongst systems reflect number errors uncovered attempting design useful benchmark general applicability several issues must considered example benchmark simulates memory faults via fault injection supervisor code operating system kanawati92 kao93 likely easily portable operating systems perhaps even operating systems similar applications point view similar operating system interfaces often backed different bodies code makes difficult inject faults supervisor code way results meaningfully compared across systems paper documents several goals benchmark robustness strive achieve goals considered light constraints presented unixlike operating systems present choices made initial efforts develop suite robustness tests machines running unixlike operating system section 2 describes motivation robustness benchmarking section 3 presents several characteristics benchmark robustness attempt achieve computer system section 4 subsequently examines constraints opportunites arise design benchmark used solely evaluating robustness unixlike systems later document initial efforts development robustness benchmarks evaluate examples design philosophies presented herein 2 background motivation development computer systems traditionally motivated desire achieve higher performance need measure progress towards goal prompted development performance benchmarks grown complexity sophistication since inception original performance measures computer system reflected attempts compute average instruction execution time system later focus shifted attempts measure overall system performance scenarios designed reflect common uses system latter approach led first synthetic benchmarks whetstonecurnow76 dhrystoneweicker84 applications oriented benchmarks specspec90 suite measures performance prototypical workloads built collection real applications similarly advent reliable computing systems spurring development robustness benchmarks quantify improvements system reliability robustness benchmarking grows focus also shifting simple measures hardware characteristics measures reflect overall reliability computing environment ie hardware together supporting software date much effort building robust systems devoted building robust hard ware efforts evaluate robustness software systems become common recently exemplified studies miller90 suh93 studies concentrate like robustness benchmarks studying behavior produced system subjected unusual rather commoncase stimuli studies perform evaluations via collection isolated tests draw conclusions collected results unfortunately often difficult evaluate relative significance individual results collected test suites example miller90 examines behavior unix utilities supplied randomly generated input data crash single utility must taken seriously crash utility even though weighting may reflect reality benchmark lacks knowledge underlying systems structure cannot know two utilities related way thus several utilities crash due bug underlying shared system library robustness system measured might perceived unduly low robustness benchmark affected lack knowledge systems structure miller90 studies similar synthetic benchmarks performance arena validity benchmarks depends accuracy constructed emulate normal workload system case robustness tests emulating normal workload refers maintaining frequency distribution exception conditions occurs systems normal use distribution must maintained obtain accurate assessment systems robustness normal use order allow accurate evaluations overall system robustness robustness benchmarks must evolve towards structure embodies dependency hierarchy system benchmarks allow testing multiple levels abstraction order facilitate isolation sources failures help evaluate severity failures encountered robustness benchmarks also easy adapt new facilities added existing system example module added software system conceptually similar older module test procedures developed older module easily adaptable new module ensure expertise lost present suites robustness tests address issues paper consider goals may achieved without precluding desirable properties found existing benchmarks robustness 3 general design issues several issues must addressed designer suite robustness benchmarks section presents many desirable properties mentions tradeoffs may necessary attain first foremost must possible use robustness benchmarks compare different operating environments computer systems thus benchmarks portable across platforms goal often restricts range tests performed notably require knowledge specific one environment often case faultinjection based tests coverage ideally benchmark test possible uses every system module tested often however space stimuli large permit exhaustive testing completely deterministic benchmark may choose test frequent uses module experience shows common case uses often properly debugged benchmark functions solely verification suite alternatively deterministic benchmark may focus solely unusual uses module thereby providing better assessment robustness module however problems usually occur intersections rarely occurring events taken together produce unexpected state set possible event intersections often large explore systematically thus benchmark remains limited coverage likely useful mainly aide debugging uncommon cases realistic estimate robustness obtained use randomized stimulation randomness randomized stimulation attempts uniformly cover space possible uses module randomized tests usually higher serendipity ie ability uncover previously unknown errors deterministic counterparts note however nondeterminism introduced randomized stimulation may lead loss repeatability sometimes randomized stimulation required order adequately emulate systems computational model explained next section degree randomness without loss repeatability one motivation extensible set benchmarks extensibility extensible benchmark provides means extend set stimuli consistent manner ie stimuli added benchmark way produces results directly comparable results generated prior addition extensibility necessary allow addition stimuli different nature used testing existing system module also allow existing benchmark extended apply new system modules extensibility ensures benchmark consistent measure progress rather simply verification suite isolated module simple form extensibility achieved use parameterized stimuli example benchmark suite might consist group stimuli whose behavior completely determined input string random numbers new sets tests generated thereby increasing coverage varying input string whereas every test set generated maintains peatability extensibility benchmark rather restricted however limited variation achieved tests varying input parameters greater extensibility requires ability add completely new testing code maintaining consistency result processing general extensibilty benchmark determined degree test control structure benchmark extended without affecting result processing hierarchically structured benchmarks provide general means achieving extensibility hierarchy complexity tests within set benchmarks organized order increasing complexity complexity inversely proportional number modules exercised test simplest tests often applicable across multiple system modules eg tests proper resource allocation deallocation applicable system module manages resource whereas complicated tests usually highly specific particular module combination modules organization may reflected tests complicated tests assume simpler tests passed designing benchmark suite tests several system modules may desirable develop hierarchical interface system modules organized reflect hierarchy functions simple tests written objectoriented fashion without requiring code duplication possibly leading divergence permodule basis example approach benchmarking documented section 54 note arrangement tests hierarchy complexity may lead higher initial implementation cost effort required define structure benchmark unlike construction benchmark composed collection ad hoc tests however initial investment usually worthwhile due desirable properties hierarchical structure resultant ease code reuse benchmark extended reporting results several characteristics desirable results reported test robustness mentioned test result repeatable results also amenable comparisons different machines indeed rather simply indicating whether test robustness passed desirable report failed tests using scale reflects severity failure closely related issue amount localization triggering events sullivan91 reflected reported results ie extent results pinpoint errors detected possible causes good localization especially valuable system designers trying focus improving depenability operating system note detection failure ie failed test raises number difficulties many stimuli exercise system ways may anticipated creators golden standard correctness often absent developers robustness benchmark might choose overcome problem defining standard correctness even measure incorrectness example scale ranking errors terms severity ranging unanticipated error code returns complete system crashes could serve yardstick incorrectness alternatively benchmark could augmented ability learn record correct result example developers benchmark may define correct result result commonly produced particular abnormal scenario 1 benchmark used determine common results third possible approach involves defining possible incorrect results assuming anything else correct example effects system call invoked garbage parameters could defined correct long operating system crash file system left intact executing processes unaffected benchmark suite may also elect compute index robustness individual test results result serves provide high level means comparing two machines generally weighted average individual test results set weights used generally 1 approach measures behavioral consistency modules across domain test reflects perceived severity errors detected may depend upon number system modules affected error likelihood occurrence daily operation range applications affected specific type error involved whether system able detect error whether detected corrected error 4 benchmarks unixlike systems unixlike operating systems available wide spectrum hardware platforms ranging personal computers supercomputers operating systems attempt provide similar interfaces functionality benchmarks written purposes comparing robustness section describes opportunities limitations constrain development benchmark suite designed specifically test unixlike operating systems issues raised addition presented previous section note many multiuser multitasking timeshared operating systems present similar constraints attempt point features unix relevant robust benchmarking efforts although restrict applicability work nonunix operating systems 41 goal unix benchmark unixlike operating systems primarily used support applications require support multiple processes multiple users even single user setups processes owned least two different users usually present system processes owned system administrator owned one users function operating system manage access hardware resources ensure running processes affect adversely suggests robustness operating system reflect systems ability successfully contain fault conditions generated one process ie reflect ability operating system prevent faults affecting processes thus system crash considered extreme case failure localize fault means processes affected unix benchmark suite thus attempt measure ability module contain errors module module basis several testable modules interfaces existing systems 42 benchmark structure 421 gross structure unix systems provide least following modules together interfaces file system virtual memory process management signal handling also provide network support window management simple benchmark suite might consist series independent test programs exercises one module however test accurately reflect fact normal use system module must support simultaneous interaction several programs thus multithreading support running monitoring several simple programs simultaneously required representative test fault handling scenarios may arise regular use note multithreaded benchmark also able test systems ability handle propagation multiple faults simultaneously occurring distinct modules thus robustness benchmark unixlike systems multiple threads feature also proves convenient measuring extent fault propagates described later another disadvantage designing benchmark suite made one test program per system module system may several modules incremental cost adding new widely applicable test test suite high new test must implemented every module involved however new test simply manipulates modules manner abstracted beyond standardized module interface test need coded modulespecific code interface handles interaction system modules unchanged motivation hierarchical structuring benchmarks unix systems support enough modules justify implementing hierarchically structured system interface allow tests coded objectoriented manner without code duplication example implementation approach see section 54 note also hierarchical approach enforces consistency result reporting simply eliminating multiple copies functionally similar testing reporting code test implemented exactly abstract level thereby guaranteeing compatibility test reports across modules finally noted portability considerations often require code benchmark suite limited userlevel implementation unixlike kernels often differ substantially implementation case would presumably little difference robustness among various flavors unix thus benchmark requires kernellevel support likely easily portable across wide range unix platforms 422 measurement faults following criteria might used evaluate seriousness fault condition increasing order severity 1 fault affect process causing 2 2 fault affect executing processes 3 fault crash operating system 4 fault crash operating system microkernel 3 uncontained fault may affect another executing process one several ways may cause process crash without crashed entire operating system produce incorrect output simply execute slowly otherwise would note process causing fault may affected similar manner however causing process benchmark benchmark able detect fault without aid external monitoring agent watchdog started beginning tests practice measure limited benchmarks ability detect changes made state 3 course applies systems built top microkernel barton cristian suh response late timing earlylate timeout late response invalid output response valuestate failure incorrect answer crash crash partialtotal amnesia pause halt crash task stop process crash abort crash w error message table 1 comparison failure classifications possible effects fault may classified according several taxonomies barton90 cristian91 suh93 summarized table 1 taxonomies necessitate means measuring effect processes owned benchmark may done observing sacrificial program executed concurrently benchmark suite checking see affected faults generated benchmark suite note almost possible resultant states sacrificial program enumerated table 1 detected mechanically watchdog program previously calibrated expected behavior sacrificial program fault produce complete system crash however watchdog may unable observe unless executing separate processor isolated one used execute tests separate processor available human intervention may required event system crash sacrificial program course make widespread use system increase probability reflect effects uncontained faults robustness benchmark suite might elect provide synthetic program serve sacrificial program may choose make use several performance benchmark suites specmarks advantage widespread availability system evaluated ability run one particular application without failure application might well serve sacrificial program pointed use sacrificial program marked effect properties benchmark particular robustness benchmark employing sacrificial workload multithreaded benchmark suffers disadvantages multithreaded eg resultant benchmark likely lose determinism repeatability however mentioned earlier multithreading also representative application computing model supported operating system use multithreading discussed later section 423 recording results unixlike systems provide stable data repository robustness benchmark must implement means recording results face adverse conditions produced testing possible effects test often unknown rather difficult implement buffered output channel susceptible data loss operating system crash output channels available userlevel process fall category simple nonautomated way overcome problem involves printing results unbuffered crt printer port monitored human similar effect might also achieved use unbuffered serial line output communicate results second watchdog processor serial line available arrangement greater chance losing small amount data approximated communicating results second computer local area network last case logging variable granularity ie test causes crash test repeated synchronously writing log disk frequently may help reduce amount data loss although cannot completely overcome effects data buffering kernel 424 randomness extensibility order provide good coverage test space robustness benchmark may opt use tests whose behavior dependent upon output random number generator however test may execute exactly actions different systems output random number generator changes unix systems provide random number generator interface random identical across implementations guaranteed produce exactly stream random numbers machine machine 4 brings question value comparing randomized runs made two different machines problem resolved controlled randomness whereby streams random numbers pregenerated stored file advance pregenerated numbers fed benchmark run time thereby ensuring benchmark runs two different machines behave manner determined identical sets random numbers note unix random number generator produces stream random numbers run run given machine given seed thus affect repeatability results single machine nevertheless repeatability often difficult achieve realistic robustness benchmark unix system unix systems usually provide deterministic process schedulers thus test presents system multithreaded workload order evaluate systems ability handle multiple simultaneous requests introduce randomness result robustness test consequently results test may always repeatable scheduling order likely vary run run problem cannot usually resolved portable manner robustness suite wants test multitasking environment unfortunately unix schedulers normally provide hooks allow repeatable deterministic process scheduling multithreaded benchmark scheduling threads may reduce severity problem provide complete solution benchmark whole remains subject scheduling actions systemwide scheduler 43 summary unix environments provide level programming support adequate benchmarks ranging simple tests based perturbation input string complex hierarchical tests extensible abstract modulespecific levels requirements robustness benchmarks better understood unix may well provide testbed development practice implementations produce identical streams system vendor might choose change portability coverage extensibility consistency results initial menta tion cost localization triggering events repeatability randomness theta theta theta theta theta multithreading theta theta theta theta theta hierarchical structure theta theta theta theta logging theta theta theta theta theta kernel theta theta table 2 effects implementation choices benchmark characteristics postive correlation inverse correlation independence indicated thetarespectively one relationship indicated actual relationship depends specific implementation crashme cmu crashme modular hierarchical portability unix unix similar modules implementations similar modules coverage high system calls variablelocal module variable serendipity high highlimited syscalls variablelocal module variable extensibility none none difficult easy localization none possible via sentries high high repeatability low low variable variable table 3 properties various example benchmarks properties marked variable unconstrained design benchmark vary individual tests benchmark language robustness tests expressed minimal effort present unix environments provide modular interfaces organized hierarchy permits hierarchically organized benchmarks new benchmarks derived small amount effort via inheritance initial approach implementation described section 54 table 2 summarizes relationships properties benchmark implementation choices made constructing seen table implementation choices affect important characteristics robustness benchmark next section provides several examples benchmarks exhibiting tabulated relationships 5 examples robustness benchmarks section presents initial efforts producing benchmarks system robustness example evaluated respect design issues presented section 3 properties hardware operating system time crash approx ibmrt mach 25 3 sec ibmrt mach 26 i486 mach 25 5 sec i486 mach 30 mk76 4 sec i486 mach 30 mk82 50 sec table 4 time taken crashme crash machines benchmarks summarized table 3 described detail 51 crashme crashme simple publicly available test robustness unix systems program allocates array fills array random data subsequently spawns several child processes try execute array data code parent crashme process observes children spawns replacements children take exceptions die crashme run unix system subjected large number varied exception conditions short period time result error detection handling capabilities operating system severely tested crashme succeeds crashing large number unix systems albeit small sample machines observed amount time operating system stayed crashme appears correlated degree observed reliability operating system day day use refer table 4 crashme good test systems ability handle high error rate many ways good general purpose benchmark robustness although portable good coverage stimulus space correspondingly high serendipity results crashme limited either system crashes crash system crashes difficult determine cause crash determination could result error logging external crashme error logging provided operating system crashme also lacks repeatability high degree randomness introduced due scheduling large number child processes created program test run twice system crashes times cannot determined whether crashes shared common cause due difficulty interpreting results crashme limited usefulness measure progress building robust system 52 cmu crashme observed aforementioned problems crashme attempted remedy restricting coverage test hoping gain repeatability better localization triggering events spawned child processes constrained exercising single well hardware operating system time crash approx ibmrt mach 25 ibmrt mach 26 crash i486 mach 25 crash i486 mach 30 i486 mach 30 mk82 crash became unusably sluggish table 5 time taken cmu crashme crash machines defined system interface namely unix system calls anticipated error checking parameters passed system calls would sufficient guarantee system calls made randomly generated parameters would able crash operating system much surprise many systems tested vulnerable limited test refer table 5 although modified version crashme still exhibits high degree nondeterminism offers better localization triggering events original version localization improved restricting tests subset unix system calls restriction together monitoring system calls via sentry mechanism described russinovich92 used successfully identify errors mach 30 unix server 53 modular benchmarks another approach modular benchmarking modular benchmarks separate tests individual system modules benchmarks constructed regarding system collection isolated modules writing one tests exercise module independently one example modular benchmark documented suh93 another example set robustness benchmarks recently constructed cmu test robustness advanced spaceborne computer module ascmdingman93 although embedded system running realtime operating system related unix regarded collection system modules much way operating system ascm test suite consisted distinct tests exercise various system modules file system memory system external communication locking support multiprogram operations together watchdog program similar parent crashme process monitor collect results tests example modular benchmark file module benchmark ascm test suite serves good example modular test benchmark stresses seven calls file module create file open file close file delete file read file write file move file pointer systematically constructing tests file handles buffer addresses number words closed start 16 byte buffer 1 opened readonly start 256 byte buffer open readwrite middle 256 byte buffer 256 deleted end 16 byte buffer 1024 altered beyond end 16 byte buffer 4090 allocated memory null pointer 4100 address deleted buffer 6 file handles theta 7 data buffers theta 6 table various possibilities input parameters read file write file tests operation number tested tests correct unexpected error bad success terminated warm restart cold restart read file 252 175 77 table 7 results running read file write file tests ascm system dingman93 test case classed produced correct result correct returned unexpected error code unexpected error indicated success spite given invalid input parameters bad success caused operating system terminate benchmark terminated caused warm restart system warm restart necessitated cold restart system cold restart call interface definition call example read file call takes 3 parameters file handle starting address buffer data read number words read file benchmark chooses value parameters set values based parameters type example file handle might point valid file closed valid file opened readonly mode deleted file among possibilities choosing possible test input combinations parameters read file call benchmark generates 252 test cases shown table 6 results 252 tests divided six groups increasing order severity produced expected result correct returned error code one would expected given input parameters unexpected error returned indicating success spite given invalid input parameters bad success caused operating system terminate benchmark terminated caused warm restart system warm restart caused cold restart system cold restart results 252 tests read file write file calls take identical sets inputs thus generate test parameter combinations shown table 7 advantages modular benchmarking approach include relatively low complexity individual tests intermodule interactions usually considered ability guarantee determinism unfortunately modular benchmarks also several disadvantages although modular benchmark approach applies well hardware system components manufactured separately often designed independent testability approach scale well large bodies operating systems software whose modules often closely intertwined making independent testing difficult testing paradigm matched well system evaluated different problem may occur modularly written system seemingly well suited modular testing evaluated case modular decomposition benchmark suite restricts coverage individual tests eliminates possibility stimulating interactions system modules addition modular benchmarks also unable take advantage similarities system modules quite likely similarities exist many modules system modular benchmarking requires similar tests applicable multiple modules coded applicable module appropriate test thus similarities modules hidden within individual tests significant loss similarities key extensibility explained next section modular benchmarks offer guarantee results similar tests even two different revisions test comparable effect comparisons external benchmark ie done human automated postprocessor collects results individual tests information determining comparability abstract nature tests completely hidden evaluator problem limits usefulness measure progress development system modular benchmark goes beyond simple interface verification example improvements system module may render modular benchmark incompatible test simply implementation details changed original test may still apply abstract level modular benchmarks enforce separation abstract test apply interfacing test module tested consequently modular test often needs adapted response significant change module adaptation made great care must taken results adapted test remain directly comparable results original version order illustrate last point consider changes benchmark might necessitated incompatible upgrade system module eg change version 10 version 11 x window system old new versions module provide function ality thus tests old system applicable new module however part benchmark code must rewritten accomodate interface change rewriting involves module interface code interfaces system module tested benchmarks test routines results updated benchmark remain comparable original benchmark care must taken modify code module interface code eg code purposes result gathering processing changed modular benchmarks require separation module interface code testing code necessary modifications may complex must performed great care 54 hierarchical approach given shortcomings modular benchmarks would seem decomposition system multiple unrelated modules best approach organizing suite robustness tests consider following set identical tests taken ascm benchmarking suite tests appear benchmarks file system memory buffer system object used denote either file memory buffer list ffl reference object created reference object deleted ffl delete active object ffl write past end object ffl read beginning object ffl allocate objects resources exhausted tests represent examples stimuli applicable multiple modules hidden within individual modular benchmarks result similar tests performed results potentially comparable one another may apparent results one way remedy problem abstract tests associated result processing separating implementation details various modules clearly defined interface layer however onestep decomposition sufficient delineate range applicability given test tests may applicable modules others might apply subset etc believe correct way decompose software system order test use class hierarchy systems features organized hierarchy classes test specified apply one particular classes one possible class hierarchy might used organize testing unix system shown figure 1 note modular ascm benchmarks described earlier actually represent example simple hierarchy one level abstraction ascm benchmark generates tests module looking interface module sets test input parameters selected correct data types chosen call implemented module input parameter given type predetermined list inputs may used instantiate parameter thus one think interface call module class inheriting set base classes base class corresponds one input parameter type class corresponding particular call inherits input parameter types describe arguments test applicable class composition tests applicable base classes 541 proposed hierarchy management various resources primary function operating system figure 1 one possible starting point construction benchmark robustness note resource sharedresource namespace sharednamespace file descriptor virtual memory communication namespace file namespace storage object virt mem image file shared segment process log writeonce socket sharedlog xwindow figure 1 part one possible hierarchy abstract classes representing abstractions boldface ordinary nonabstract classes representing system modules data types hierarchy contains many abstract classes eg storage object correspond single system module rather serve fundamental means grouping modules similarity explicit grouping basis organization hierarchically structured benchmark suite also provides mechanism orderly extension benchmark suite order construct hierarchical benchmark hierarchically structured interface operating system must first implemented unix provide interface hierarchical interface library must developed support subsequent construction robustness tests construction interface proceeds follows decided upon hierarchy im plemented designer must choose appropriate set methods defined interface class example methods allocate deallocate might defined toplevel class resource shared resource inherits resource might add methods lock unlock already defined resource great care must taken define methods sufficient generality apply modules descendants instance particular class eg file required implement methods abstract classes ancestors whenever method implemented completely module independent way defined general abstract class applies subclasses instances class may chose redefine default implementation specific one note operating systems future likely provide objectoriented hierarchical interfaces facilities thereby eliminate need construct interface library note also hierarchical interfaces benchmarks easily written language supports objectoriented programming c also written traditional languages c extra effort part programmer experimental hierarchy written c 542 using hierarchy build benchmark hierarchical interface operating system built operating system test requiring modulespecific knowledge written abstract manner test implemented abstract possible level class hierarchy applies example test resource exhaustion repeated allocation mentioned section 54 requires allocate method module testing coded take parameter class resource use allocate method provided resources test coded particular class automatically applied descendants class case resource overallocation test test applied system modules base hierarchy including files memory buffers contrast consider test checks correct system behavior upon writing past end object test requires notion object able store information beginning end case test applies class storage object subclasses test encoded appropriate level abstraction result processing associated test encoded level thus result processing allowed knowledge specificity test eg might want use knowledge scale results test according weight assigned severity failure class importantly however test result processing test coded exactly abstract way guarantees comparability results obtained applying test multiple modules exactly one copy test used multiple applications way inconsistencies arise might occur multiple copies present eg modular benchmarks hierarchical benchmarks also easily extensible consistent manner due organi zation benchmark suite extended include new module interface module encoded subclass appropriate class within hierarchical interface operating system new module related way existing modules placement reflects relationships tests developed existing modules applied new module immediately possibility duplicating test code new module positioned correctly hierarchy consistency testing result processing guaranteed maintained across extension finally note hierarchical structuring offers superior extensibility reduction coding costs code reuse potential better organization result reporting antagonistic achieving goals desirable benchmark portability coverage localization triggering events etc thus hierarchical benchmarks offer special benefits without sacrificing desirable qualities offered benchmark styles 543 hierarchical testing using c hierarchically structured benchmarks easily implemented language provides support inheritance section presents trivial tests demonstrate use c building hierarchy constructing tests using hierarchy tests presented intended solely illustrate programming paradigm readers familiar c may wish skip next section presents complicated example hierarchical test actually implemented given hierarchy figure 1 significant parts declaration class resource might look follows class resource public virtual int allocate int n 0 virtual int deallocate int id int n 0 resource abstract class ie correspond single system module implementations methods allocate deallocate provided hence marked 0 storage object abstract class inherits resource supplies additional methods class storageobject public resource public virtual void setmode objectmode mode virtual objectmode getmode virtual int readdata char buf int len 0 virtual int writedata char buf int len 0 protected enum objectmode modespec mode object storage object class provides two methods read data write data implemented classes inheriting default implementations also provided two methods manage mode storage object classes inheriting storage object may override definitions set mode get mode desire methods declared virtual finally file object class declared follows class fileobject public storageobject public resource methods int allocate int n int deallocate int id int n storageobject methods void setmode objectmode mode int readdata char buf int length int writedata char buf int length file object class must implement unimplemented methods inherited ancestors also chooses override default implementation set mode replacing enhanced version manipulates mode bits underlying file disk calls original default implementation declared implemented methods hierarchical interface simple test applicable resource might implemented follows void resourcetestresource r allocating forever test attempts exhaust supply resource hopes stimulating anomalous behavior similarly specific test applicable storage objects might attempt stimulate error conditions writing unusually large segments data void storageobjteststorageobject write big data block finally two tests applied objects appropriate types fileobject f processobject resourcetest p resourcetest f storageobjtest f resource test applied file object process object storage object test applicable file object process object storage object 544 example hierarchical test simple illustration hierarchical benchmark hierarchical analog part ascm benchmark constructed unix system observe read file write file tests described section 53 require capabilities defined class storage object hierarchical unix benchmark implements tests storage object class making use read data write data methods respectively file object memory object classes defined hierarchical interface library subclasses storage object evaluated using benchmark testspecific analysis modulespecific analysis correct unexpected error unexpected termination bad success correct correct correct correct bad success unexpected error correct unexpected error impossible unexpected error unexpected termination correct impossible unexpected termination impossible bad success bad success unexpected error impossible bad success table 8 matrix showing modulespecific testspecific analyses test result combined determine ultimate classification result categories correct unexpected error bad success correspond ascm result classes similar names unexpected termination class supersedes ascm terminated class derivation matrix discussed length text implementation read data write data benchmarks composed two parts first part generates objects ie file objects memory objects used testing part embodies modulespecific knowledge must know least data type object generated may need know internal details object generated example may need know object encapsulating closed file required second part benchmark embodies abstracted test routine part test usually written highest possible level abstraction embody module specific knowledge example read data write data tests written apply storage object require knowledge object testing thus code testing file object looks like storageobject obj routine test storage object applied storage object file piece virtual memory particular case conducts tests reading writing using possible combinations test buffer addresses io request sizes hand get test obj moduledependent routine implemented modules tested makes use local modulespecific knowledge generate objects tested example shown returns file object passed test storage object treated like storage object processing hierarchical benchmark implemented maintains six result classifications defined ascm modular benchmark seemingly slight important change ascm error class terminated replaced unexpected termination class terminations expected outcome test instead counted correct reason change discussed later structure result processing code resembles testing code test results also processed moduledependent moduleindependent routine modulespecific analysis routine supplied module tested evaluates outcome test view characteristics particular object tested system without systemwide integrated error handler modulespecific routine also able examine modulespecific error return mechanisms incorporate feedback analysis unlike module specific routine moduleindependent routine also known testspecific routine detailed knowledge object tested detailed knowledge test applied aware tested object level abstraction test testspecific routine applicable across modules tested evaluates outcomes based systemwide error handling information observed behavior tested object note information may available resultprocessing routines used analyzing outcome test given goal classifying test results one six classes described two analysis routines return correct unexpected error unexpected termination bad success evaluations correspond first four possible result classifications respectively modulespecific testspecific evaluations combined yield final classification outcome falls one four classes situations either remaining two possible outcomes warm restart cold restart occur occurrence detected external monitoring agent human two cases outcome indisputable modulespecific testspecific analysis need performed consider example write data test attempts write 1024 bytes data storage object encapsulates closed file passing buffer points 256 bytes data test run job modulespecific routine aware internal details storage object check whether result indicates invalid attempt write closed file modulespecific routine aware file object written actually closed error code returned agrees predicted outcome modulespecific routine indicates correct return otherwise indicates bad success unexpected error appropriate moduleindependent routine hand aware dealing storage object knows test attempted write data actually supplied write data call therefore checks see much data test claimed written successfully classifies outcome one four aforementioned categories two evaluations combined according following principles employed produce matrix table 8 ffl call succeeds ie didnt return error code result analyses must agree success expected outcome order result deemed correct error stimulus introduced test either modulespecific object generation abstract test code initialization without knowledge thus either one analyses expects error correct outcome correct outcome must error eg write data called file open writing passed null buffer asked write 1024 bytes testspecific routine predict error correct outcome decision use null buffer would made testspecific initialization code executing test trigger error predicted outcome correctly classified bad success conversely call fails error one analyses need accept error correct outcome order result deemed correct possible likely one two parts testing code aware stimulus expected cause error condition example suppose call failed return code indicated invalid data passed data call moduleindependent routine would sufficient information classify correct outcome moduledependent independent routine knowing valid storage object written error return would look like unexpected error return ffl call produces termination one result analysis need accept termination correct ie expected order outcome considered correct justification case analogous handling failure call ffl occurrence unexpected termination precludes possibility unexpected error bad success analysis routines may disagree whether benchmark terminated ffl analysis routines may disagree whether call failed may seem counter intuitive common occurrence however may occur presence error return channels 5 available analyses examination exam ple modulespecific errorreturn channel might indicate modulespecific routine test failed alternatively testspecific routine might conclude test failed observing behavioral data available modulespecific routine cases disagreement one analysis may return bad success returns unexpected error correct combination two results yields unexpected error analysis detected error error indication channels available available routine indicate error note analysis routines implementation observe analyze test termination operating system done order classify termination correct unexpected termination original ascm classification retained classifying terminations group unto benchmark termination would required analysis terminations could simply monitored handled way cold restarts warm restarts however implementation hierarchical benchmark exposed problem ascm result classification renders simpler scheme unusable hierarchical benchmark initially implemented run file object module interfacing part mach 30 filesystem minimal effort restricted extending hierarchical os interface library test later run memory object module interfacing virtual memory system results running benchmark shown table 9 results serve demonstrate one valuable benefits hierarchical testing namely enforcing consistency across modules result processing especially valuable benchmark extended hierarchical benchmark initially implemented using ascm result classification scheme works reasonably well context file management process termination expected outcome file operation however process termination often correct means signalling error interactions virtual memory 5 error return channel means operating system indicate error condition application examples include return codes global status flags signals system traps operation number tested tests correct unexpected error bad success unexpected termination warm restart cold restart read data file object 210 108 0 0 57 data file object 210 92 0 9 91 read data memory object 210 115 data memory object 210 101 table 9 evaluation file object memory object modules using hierarchical bench mark result classes used defined ascm benchmark described section 53 modified terminated class replaced class containing unexpected ter minations terminations expected outcome test counted correct number correct outcomes due expected terminations included parentheses table represents number results would misclassified ascm scheme system eg case attempt reference invalid address benchmark extended test memory objects addition file objects limitations ascm scheme became apparent many otherwise correct outcomes simply classified terminated seen table 9 large numbers test outcomes misclassified ascm scheme might lead conclusion virtual memory system low quality due misclassifications less 10 tests would appeared produced correct outcome however analysis revealed result classification system virtual memory system fault led modified result classifications wherein terminations classed either correct unexpected termination original shortcoming result analysis made obvious hierarchical structure benchmark enforced comparability results across two modules ad hoc modular approach problem might easily obscured addition noted case study exemplifies one potential pitfalls robustness benchmark design difficult define error classes without knowing outcomes possible tests classes may applied ascm benchmark example defined terminated class absence enough data evaluate suitability ascm evaluations file module see table 7 show occurrences terminated tests occurrences reported modules similarly tested cases hierarchical testing serves enforcing consistency result processing maintain placeholder result classes well characterized data vague classification become available hierarchical framework serves guide towards correct characterization new result class 6 conclusions future work paper current robustness benchmark efforts examined approaches fail address certain issues critical longterm success robustness test suite several issues delineated proposed benchmarking organization overcomes many problems outlined proposed hierarchical benchmarking organization adversely affect desirable properties attained benchmarks date particular proposal imposes hierarchical extensible structure upon test suites structure may mandate higher initial implementation effort promises improve lifespan maintainability robustness benchmark use long period time remain several opportunities improvement explored discussion extensible hierarchy example hierarchically structured testing facilitates combination individual tests results overall index robustness also suggests system determining relative importance individual results addition possibility extending hierarchical structure include benchmarks operate via fault injection explored mainly simple fault injection violates notion testing procedures abstracted implementation details whenever possible possibility reconciling difference via abstracted form fault injection explored much work presented herein carried context measuring robustness unixlike operating system kernel operating system regarded opaque monolithic entity however advent microkernel based operating systems presents opportunity lowest levels hierarchical benchmark focus robustness layers beneath operating system server ie microkernel hierarchically structured benchmarks easily extended incorporate notion benchmarking one level deeper simply adding one layers bottom benchmark hierarchy finally approach measuring robustness applied easily large software system written modular manner application promises extensibility low maintenance cost consistency obtained similar benchmark operating system robustness detailed investigation applications remains pursued r fault injection experiments using fiat understanding faulttolerant distributed systems synthetic benchmark measuring robustness fault tolerant aerospace system ferrari tool validation system dependability properties fine fault injection monitoring environment tracing unix system behavior faults empirical study reliability unix utilities open system fault management fault tolerant mach standard performance evaluation corporation development benchmark measure system robustness software defects impact system availability study field failures operating systems dhrystone synthetic systems programming benchmark hartstone synthetic benchmark requirements hard realtime applications tr ctr jean arlat yves crouzet johan karlsson peter folkesson emmerich fuchs gnther h leber comparison physical softwareimplemented fault injection techniques ieee transactions computers v52 n9 p11151133 september