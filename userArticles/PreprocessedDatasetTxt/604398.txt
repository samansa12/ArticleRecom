high performance computations large scale simulations subsurface multiphase fluid heat flow tough2 widely used reservoir simulator solving subsurface flow related problems nuclear waste geologic isolation environmental remediation soil groundwater contamination geothermal reservoir engineering solves set coupled mass energy balance equations using finite volume method contribution presents design analysis parallel version tough2 parallel implementation first partitions unstructured computational domain time step set coupled nonlinear equations solved newton iteration newton step jacobian matrix calculated illconditioned nonsymmetric linear system solved using preconditioned iterative solver communication required convergence tests data exchange across partitioning borders parallel performance results cray t3e900 presented two real application problems arising yucca mountain nuclear waste site study execution time reduced 7504 seconds two processors 126 seconds 128 processors 2d problem involving 52752 equations larger 3d problem 293928 equations time decreases 10055 seconds 16 processors 329 seconds 512 processors b introduction subsurface flow related problems touch many important areas days society natural resource development nuclear waste underground storage environmental remediation groundwater contami nation geothermal reservoir engineering complexity model domains physical processes involved numerical simulation play vital roles solutions problems contribution presents design analysis parallel implementation widely used tough2 software package 9 10 numerical simulation flow transport porous fractured media contribution includes descriptions algorithms methods used parallel implementation performance evaluation present address department computing science high performance computing center north umea university se901 87 umea sweden c 2000 kluwer academic publishers printed netherlands high performance computations subsurface simulations 3 parallel simulations 512 processors cray t3e900 two real application problems although implementation analysis made cray t3e use standard fortran 77 programming language mpi message passing interface makes software portable platform fortran 77 mpi available serial version tough2 transport unsaturated groundwater heat version 2 used 150 organizations 20 countries see 11 examples major application areas include geothermal reservoir simulation environmental remediation nuclear waste isolation tough2 one official codes used us department energys civilian nuclear waste management evaluation yucca mountain site repository nuclear wastes context arises largest demanding applications tough2 far scientists lawrence berkeley national laboratory currently developing 3d flow model yucca mountain site involving computational grids 10 5 blocks related coupled equations water gas flow heat transfer radionuclide migration subsurface 3 considerably larger difficult applications anticipated near future analysis solute transport ever increasing demands spatial resolution comprehensive description complex geolog ical physical chemical processes high performance capability tough2 code essential applications early results project presented 5 2 tough2 simulation tough2 simulation package solves mass energy balance equations describe fluid heat flow general multiphase multicomponent systems fundamental balance equations following dt z z z integration arbitrary volume v bounded surface k denotes mass kth component water gas heat etc f k flux fluids heat surface q k source sink inside v general form flow mass parameters arbitrary nonlinear functions primary thermodynamic variables density pressure saturation etc given computational geometry space discretized many small volume blocks integral block becomes variable leads naturally finite volume method resulting following ordinary differential equations dt volume block n anm interface area bordering blocks n fnm flow note flow terms usually contain spatial derivatives replaced simple difference variables defined blocks n divided distances block centers see figure 1 illustration lefthand side 3dimensional grid block illustrated arrows illustrating flow throw interface areas neighboring grid blocks lefthand side two neighboring blocks n illustrated 2dimensional picture block center marked cross included also variables vm v n volumes dm n distance grid block centers interface area nm f nm figure 1 space discretization geometry data time implicitly discretized first order difference equation deltat nm vector x consists prime variables time flow sourcesink terms right hand side evaluated deltat high performance computations subsurface simulations 5 initialization setup time step advance newton iteration calculate jacobian matrix solve linear system output figure 2 sketch main loops tough2 simulation numerical stability multiphase problems lead coupled nonlinear algebraic equations solved using newtons method 3 computational procedure main solution procedures schematically outlined figure 2 reading data setting problem time consuming parts main loops time stepping newton iteration iterative linear solver time step nonlinear discretized coupled algebraic equations solved newton method within newton iteration jacobian matrix first calculated numerical differentiation implicit system linear equations solved using sparse linear solver preconditioning several newton iterations convergence checked control parameter measures maximum component residual newton iterations newton iterations converge time advance one time step process repeats predefined total time reached newton procedure converge preset max newtoniteration current time step reduced usually half newton procedure tried reduced time step converged time advance otherwise time step reduced another round newton iteration follows procedure repeated convergence newton iteration reached system linear equation usually illconditioned requires robust solvers dynamically adjusted time step size key overcome combination possible convergence problems newton iteration linear solver highly dynamic 6 elmroth ding wu system trajectory sensitive variations convergence parameters computationally major part 65 execution time spent solving linear systems second major part 30 assembly jacobian matrix 4 designing parallel implementation aim work develop parallel prototype tough2 demonstrate ability efficiently solve problems significantly larger problems previously solved using serial version software problems larger number blocks number equations per block target computer system prototype version parallel tough2 696 processor cray t3e900 nersc lawrence berkeley national laboratory following sections give overview design main steps including grid partitioning grid block reordering assembly jacobian matrix solving linear system well details parallel implementation 41 grid partitioning grid block reordering given finite domain described section 2 following consider dual mesh grid obtained representing block volume element centroid representing interfaces blocks connections words blocks connections used consistency original tough2 documentation 10 physical properties blocks interfaces represented data associated blocks connections respectively tough2 computational domain defined set connections given input data information adjacency matrix constructed ie matrix nonzero entry element j connection blocks j current implementation value 1 always used nonzero elements different weights may used adjacency matrix stored compressed row format called crs format slight modification harwellboeing format see eg 2 descriptions crs harwellboeing formats actual partitioning grid p almost equalsized parts performed using three different partitioning algorithms implemented metis software package version 40 8 three algorithms denoted kway vkway recursive partitioning algorithm consistency metis documentation kway multilevel version traditional graph partitioning algorithm minimizes number edges straddle partitions vkway modification kway instead minimizes actual total communication volume recursive recursive bisection algorithm objective minimize number edges cut partitioning grid processors blocks specifically vector elements matrix rows associated blocks reordered processor local ordering blocks processor computes results denoted update set processor update set partitioned internal set border set border set consists blocks edge block assigned another processor internal set consists blocks update set blocks included update set needed read computations defines external set figure 3 illustrates blocks distributed pro cessors vertices graph represent blocks edges represent connections ie interface areas pairs blocks table shows blocks classified update external sets update sets divided internal border sets table elements placed local order global numbering illustrates reordering11314135 processor 0 processor 2processor 11 figure 3 grid partitioning 3 processors table example block distribution local ordering internal border external sets internal processor 0 7 processor 1 2 3 j processor 2 6 14 j 5 10 13 k12 4 order facilitate communication elements corresponding borderexternal blocks local renumbering nodes made particular way blocks update set precede blocks external set update set internal blocks precede border blocks finally external blocks ordered internally blocks assigned specific processor placed consecutively one possible ordering given example table processor 0 example grid blocks numbered 7 11 internal blocks ie blocks updated processor 0 dependencies blocks blocks assigned processors grid blocks 8 12 border blocks processor 0 ie blocks updated processor 0 dependencies blocks assigned processors finally blocks 1 13 external blocks processor 0 ie blocks updated processor 0 data associated blocks needed readonly computations amount data processor send receive computations approximately proportional number border external blocks respectively consecutive ordering external blocks reside processor makes possible receive data corresponding blocks appropriate vectors without use buffers need reordering provided sending processor access ordering information however possible general order border blocks transformations avoided sending basically blocks border set may sent one processor 42 jacobian matrix calculations new jacobian matrix calculated newton step ie several times time step algorithm parallel al gorithm processor responsible computing rows high performance computations subsurface simulations 9 jacobian matrix correspond blocks processors update set derivatives computed numerically jacobian matrix stored distributed variable block row format dvbr 7 matrix blocks stored row wise diagonal blocks stored first block row scalar elements matrix block stored column major order use dense matrix blocks enable use dense linear algebra software eg optimized level 2 level subproblems dvbr format also allows variable number equations per block computation elements jacobian matrix basically performed two phases first phase consists computations relating individual blocks beginning phase processor already holds information necessary perform calculations second phase include computations relating interface quan tities ie calculations using variables corresponding pairs blocks performing computations exchange relevant variables required number variables processor sends elements corresponding border blocks appropriate processors receives elements corresponding external blocks 43 linear systems nonsymmetric linear systems solved generally illconditioned difficult solve therefore parallel implementation tough2 made different iterative solvers preconditioners easily tested results presented obtained using stabilized biconjugate gradient method bicgstab 14 aztec software package 7 3 theta 3 block jacobi scaling domain decomposition based preconditioner possibly overlapping subdomains ie additive schwarz see eg 13 using ilut 12 incomplete lu factorization domain decomposition based procedure performed different levels overlapping case procedure turns another variant block jacobi preconditioner order distinguish 3 theta 3 block jacobi scaling full subdomain block jacobi scaling obtained chosing domain decomposition preconditioning procedure refer former block jacobi scaling latter domain decomposition based preconditioner though course preconditioners illustration difficulties arising linear systems would like mention small problem yucca mountain simulations mentioned introduction nonsymmetric problem includes 45 blocks 3 equations per block 64 connections solving linear system jacobian matrix size 135 theta 135 1557 nonzero elements first jacobian generated first newton step first time step ie matrix involved first linear system solved largest smallest singular values 248theta10 32 227theta10 gamma12 respectively giving condition number 11 theta 10 44 applying block jacobi scaling block row multiplied inverse 3 theta 3 diagonal block condition number significantly reduced scaling reduces largest singular value 769 theta 10 3 smallest increased 983 theta 10 gamma5 altogether reducing condition number 78 theta 10 7 however still illconditioned problem therefore domain decomposition based preconditioner incomplete lu factorization mentioned applied block jacobi scaling procedure shown absolutely vital convergence problems significantly larger 44 parallel implementation section outline parallel implementation describing major steps important routines parallel tough2 includes 20000 lines fortran code excluding metis aztec packages numerous subroutines using mpi message passing 6 however order understand main issues parallel implementation sufficient focus couple routines course several routines also modified compared serial version software details would distracting cycit initially processor 0 reads data describing problem solved essentially way serial version software processors call routine cycit contains main loops time stepping newton iterations routine also initiates grid partitioning data distribution partitioning described section 41 defines input data distributed processors distribution performed several routines called cycit five categories data distributed possibly reordered vectors elements corresponding grid blocks distributed according grid partitioning reordered local order internal border external elements described section 41 vectors elements corresponding connections distributed adjusted local grid block numbering high performance computations subsurface simulations 11 processor determined connections involved local partition vectors elements corresponding sinks sources replicated full processor extracts reorders parts needed addition number scalars small vectors matrices fully replicated ie data structures sizes depend number grid blocks connections finally processor 0 constructs data structure storing jacobian matrix distributes appropriate parts processors include integer vectors defining matrix structure large array holding floating point numbers matrix elements problem distributed time stepping procedure begins brief description routine cycit given figure 4 description lots details omitted clarity calls included couple routines require description exchangeexternal routine exchangeexternal particular interest parallel implementation main loop routine outlined figure 5 called processors vector scalar noel argu ments exchange vector elements corresponding external grid blocks performed neighboring processors parameter noel number vector elements exchanged per external grid block additional parameters defines current partition eg information neighbors etc need also passed routine clarity chosen include figure though details omitted chosen include full mpi syntax using fortran interface communication primitives routine pack called exchangeexternal copies appropriate elements vector consecutive work array external elements given processor specified sendindex remark elements stored directly appropriate vector received since external blocks ordered consecutively neighbor whereas border elements sent need packed consecutive work space sent note use nonblocking mpi routines sending receiving data use blocking routines would assure messages sent received appropriate order avoid deadlock using nonblocking primitives sends receives made arbitrary order minor inconvenience use nonblocking routines work space used store elements sent need large enough store elements processor send neighbors cycit initialization grid partitioning data distribution etc set first time step first newton step number secondary variables par per grid block time endtime newton converged call multi newton converged result convergence test newton converged update primary variables increment time define new time step set else iter maxiter solve linear system call eos call exchangeexternalpar num sec vars iter maxiter physical properties range time step decreased many times stop execution print message failure solve problem else reduce time step call eos call exchangeexternalpar num sec vars figure 4 outline routine cycit executed processors exchangeexternalvector noel neighbors call mpi irecvvectorrecvstart rlen mpi double precision proc tagmyid mpi comm world req2i1 ierr call packi vector sendindex slen workiw noel call mpi isendworkiw slen mpi double precision proc tagproc mpi comm world req2i ierr call mpi waitall2num neighbors req stat ierr figure 5 outline routine exchangeexternal simultaneously called processors performs exchange noel elements per external grid block data vector routine multi called set linear system ie main part computations multi computing elements jacobian matrix computationally multi performs three major steps first performs computations depend individual grid blocks followed computations terms arising sinks sources far computations made independently processors last computational step multi interface quantities ie computations involving pairs grid blocks performing last step exchange external variables required vectors x primary variables dx last increments newton process delx small increments x values used calculate incremental parameters needed numerical calculation derivatives 14 elmroth ding wu r residual number elements sent per external grid block equals number equations per grid block four vectors operation performed calling exchangeexternal performing computations involving interface quantities eos3 eos routines thermophysical properties fluid mixtures needed assembling governing mass energy balance equations provided routine called eos equations state main task eos routine provide values secondary thermophysical variables functions primary variables though also performs additional important tasks see 10 pages 1726 details several eos routines available tough2 new eos routines become available however eos3 one used parallel implementation order provide maximum flexibility strive minimize number changes needs done eos routine moving serial parallel implementation done organizing data assigning appropriate values certain variables calling eos routine current parallel implementation eos3 routine serial code used unmodified exception statements though still needs verified practice believe current parallel version tough2 handle also eos routines exception write statements needing adjustments 45 cray t3ethe target parallel system parallel implementation tough2 made portable use standard fortran 77 programming language mpi message passing interface interprocessor communication development analysis however performed 696 processor cray t3e900 system t3e distributed memory computer processor local memory together network interface hardware processor known digital ev5 alpha local memory form processing element pe sometimes called node 696 pes connected network arranged 3dimensional torus see eg 1 details performance cray t3e system 5 performance analysis parallel performance evaluation performed 2d 3d real application problem arising yucca mountain nuclear waste site study results obtained 512 processors cray t3e900 nersc lawrence berkeley national laboratory linear systems solved using bicgstab 3 theta 3 block jacobi scaling domain decomposition based preconditioner ilut incomplete lu factorization different levels overlapping tried procedure though results presented nonoverlapping tests general shown give good performance stopping criteria used linear solver denote residual right hand side respectively test problems require simulated times 10 4 10 5 years would require significant execution time also good parallel performance large number processors order investigate parallel performance therefore limited simulated time 10 years 2d problem 01 year 3d problem still require enough time steps perform analysis parallel performance shorter simulated time course give initialization phase unproportionally large impact performance figures initialization phase therefore excluded timings tests performed using kway vkway recursive partitioning algorithms metis see later different orderings grid blocks lead variations time discretization following unstructured nature problem turn lead variations number time steps required thereby total amount work performed trying three partitioning algorithms chosing one leads best performance problem number processors reduce somewhat artificial performance variations resulting differences number time steps required results presented indicate partitioning algorithm used 51 results 2d 3d real application problems 2d problem consists 17584 blocks 3 components per block 43815 connections blocks giving total 52752 equations jacobian matrix linear systems solved newton step size 52752 theta 52752 946926 nonzero elements topmost graph figure 6 illustrates reduction execution time increasing number processors execution time number processors execution time 2h 5m 4s 2m 6s number processors figure 6 execution time parallel speedup 2d problem 2 4 8 16 32 64 128 processors cray t3e900 high performance computations subsurface simulations 17 reduced 7504 seconds ie 2 hours 5 minutes 4 seconds two processors 126 seconds ie 2 minutes 6 seconds 128 processors parallel speedup 2d problem presented second graph figure 6 since problem cannot solved one processor parallel code speedup normalized 2 two pro cessors ie speedup p processors calculated 2t 2 p denote wall clock execution time 2 p processors respectively completeness also report execution time original serial code 8245 seconds 2d problem 3d problem consists 97976 blocks 3 components per block 396770 connections blocks giving total 293928 equa tions jacobian matrix linear systems solved newton step size 293928 theta 293928 8023644 nonzero elements topmost graph figure 7 illustrates reduction execution time 3d problem increasing number processors memory batch system time limits prohibits tests less 16 processor results therefore presented 16 32 64 128 256 512 pro cessors execution time significantly reduced number processors increased way 512 processors reduced 10055 seconds ie 2 hours 47 minutes 35 seconds processors 329 seconds ie 5 minutes 29 seconds 512 processors ability efficiently use larger number processors even better illustrated speedup shown second graph figure 7 speedup defined 16t 16 p since performance result available smaller number processors results clearly demonstrate good parallel performance large number processors problems observe speedups 1191 128 processors 2d problem 4893 512 processors 3d problem repeatedly doubling number processors 2 4 4 8 etc 128 processors 2d problem obtain speedup factors 158 285 219 191 196 162 3d problem corresponding speedup factors repeatedly doubling number processors 16 512 processors 270 228 195 150 169 200 would ideal speedup time number processors doubled speedup eg 270 228 3d problem often called superlinear speedup present explanations later sections number processors execution time 2h 47m 35s 5m 29s number processors figure 7 execution time parallel speedup 3d problem 16 32 64 128 256 512 processors cray t3e900 high performance computations subsurface simulations 19 overall parallel performance satisfactory complete analysis providing insights explaining superlinear speedup 52 unstructured problem ideal case problem evenly divided among processors approximately number internal grid blocks per processor also roughly number external blocks per processor problems however unstructured means partitioning made even aspects leads example imbalances number external elements per processor internal blocks evenly dis tributed 3d problem 512 processors average number external grid blocks 234 maximum number external blocks processor 374 follows least one processor 60 higher communication volume average processor assuming communication volume proportional number external blocks note average number internal grid blocks 191 case means average processor actually external blocks internal blocks finally average number neighboring processors 1259 maximum number neighbors processor 25 altogether indicates communication pattern irregular amount communication becoming significant terms number messages total communication volume time amount computations performed without external elements becoming fairly small despite difficulties parallel implementation shows ability efficiently use large number processors every half second wall clock time 512 processors new linear system size 293928 theta 293928 8023644 nonzero elements generated solved includes time numerical differentiation elements jacobian matrix 3 theta 3 block jacobi scaling block row ilut factorization domain decomposition based preconditioner number bicgstab iterations 53 analysis work load variations several issues need considered analyzing performance number processors increased first sizes individual tasks performed different processors decreased giving increased communication computation ratio relative load imbalance also likely increase addition may find variations time discretization performed number time steps number iterations newton process linear solver order conduct detailed study present summary iteration counts timings two test problems table ii table shows average number newton iterations per time step average number iterations linear solver per time step per newton step well total number time steps newton iterations iterations linear solver recall linear system solve time consuming operation computation jacobian matrix second largest time consumer operations performed newton step problems note variations occur time discretization problem solved different number pro cessors similar behavior observed example using different linear solvers serial version tough2 variations time discretization leads variations number time steps needed number newton iterations required notably 4 processors execution 2d problem requires 15 time steps 35 newton steps 91 iterations linear solver compared execution 2 processors increase work fully explains low speedup 4 processors similar variations amount work also contribute good speedup cases however figures table ii alone fully explain superlinear speedup observed cases therefore continue study looking performance linear solver however show examples motivates continued study ie cases speedup actually higher would expect looking iteration counts example 2d problem speedup 8 processors 124 larger maximum expected ie 899 vs 800 compared execution two processors 8 processor execution actually requires slightly newton iterations iterations linear solver number time steps tests doubling number processors 8 16 see another factor 219 speedup even though reduction number newton iterations iterations linear solver 24 96 respectively speedup 3d problem 16 processors 270 32 64 processors 228 also higher would expected looking table ii alone far summarize following observations two problems unstructured nature problem naturally leads variations work load different tests alone explains table ii iteration counts execution times 2d 3d test problems 2d problem partitioning algorithm vk vk k rec rec rec k rec time steps 104 120 104 104 104 94 94 103 total newton iterations 645 869 669 653 663 697 620 637 newton itertime step 620 724 643 628 638 741 660 618 total lin solv iterations 8640 16528 10934 9888 11011 11282 11894 19585 lin solv iternewton step 1340 1902 1634 1514 1661 1846 1918 3075 lin solv itertime step 831 1371 1051 951 1059 1200 1265 1901 time spent lin solv time spent total time 3d problem partitioning algorithm k rec k k rec rec time steps 154 149 143 137 185 166 total newton iterations 632 606 585 561 708 646 newton itertime step 410 407 409 409 383 389 total lin solv iterations 8720 10275 9357 10362 14244 14487 lin solv iternewton step 1380 1696 1599 1847 2012 2243 lin solv itertime step 566 690 654 756 770 873 time spent lin solv time spent total execution time 22 elmroth ding wu speedup anomalies observed couple cases evident issues investigated therefore continue study focusing performance linear solver preconditioner 54 performance preconditioner linear solver breakup speedup one part linear solver including preconditioner one computations mainly assembly jacobian matrix presented problems figure 8 figure illustrates superlinear speedup whole problem follows superlinear speedup linear solver note results presented total time spent parts ie different number linear systems solved difference number iterations required solve linear system affects numbers speedup parts close p tests problems also indication part computation may show good performance also larger number processors slight decrease 256 512 processors 3d problem due increased number time steps conclude performance parts satisfying needs explanations continue study superlinear speedup linear solver 541 effectiveness preconditioner preconditioner crucial number iterations per linear system solved domain decomposition based process expected become less efficient number processors increases best effect preconditioner expected whole matrix used factorization order achieve good parallel performance size preconditioning operation processor restricted local subdomain average matrix used preconditioning processor n size whole global matrix p number processors reduced effectiveness follows naturally smaller subdomains ie decreased size matrices used preconditioner since diagonal blocks used calculate approximate solution number iterations required per linear system two test problems confirms theory see table ii test problems number iterations required per linear system increases number processors exceptions going 4 8 8 processors 2d problem 32 64 3d problem number processors linear solver parts number processors linear solver parts figure 8 breakup speedup 2d 3d problems one part linear solver marked r one part computations marked 4 ideal speedup defined straight line also included results 256 processors 2d problem table ii note increase number iterations per linear system 50 compared 128 processors clear preconditioner perform good job number processors increased 256 introducing one level overlapping additive schwarz domain decomposition based preconditioner 256 processors number iterations linear solver reduced order smaller number proces sors however done additional cost performing overlapping overall time roughly unchanged main observation despite overall increasing number iterations linear solver increasing number processors speedup linear solver higher would normally expected certain number processors increased number iterations per linear system larger number processors obviously following reduced effectiveness preconditioner following sections conclude performance analysis investigating parallel performance actual computations performed preconditioning linear iteration processes 542 performance preconditioner another effect decreased sizes subdomains domain decomposition based preconditioner total amount work perform incomplete lu factorizations becomes significantly smaller number processors increased example number processor doubled size processors local matrix ilut factorization decreased factor 4 average n p n 2p theta n 2p hence amount work per processor reduced factor 2 8 depending sparsity structure hence amount work per processor reduced faster normally expect assume ideal speedup 2 figure 9 gives breakup speedup speedup linear solver separated one part preconditioner ie ilut factorization one parts linear solver preconditioner shows dramatic improvement performance number processors increases following naturally decreased work ilut factorization continue turn single important explanation sometimes superlinear speedup number processors parts preconditioner linear solver excluding preconditioner number processors parts preconditioner linear solver excluding preconditioner figure 9 breakup speedup 2d 3d problems one part linear solver excluding ilut factorization preconditioner marked r one part ilut factorization marked one part computations marked 4 ideal speedup defined straight line 26 elmroth ding wu 543 performance linear iterations explained superlinear speedup preconditioning part linear solver presented figure 9 also observe modest speedup parts computation low speedup parts partly explained increased number iterations seen previous section increased communication computation ratio slightly increased relative load imbalance factors already figures presented section 52 showing large number external elements per processor imbalance number external elements per processor presented 3d problem 512 processors indicated communication computation ratio would eventually become large performance obtained iteration process linear solver supports observation 544 impacts overall performance order fully understand total combined effect superlinear behavior preconditioner moderate speedup parts linear solver need investigate large proportion spent preconditioning total time required solving linear systems illustrated table iii table iii time spent preconditioning percentage total time spent linear solver 2d problem processors percentage 835 771 739 692 616 499 364 3d problem processors percentage 884 789 730 660 586 394 number processors becomes large amount time spent preconditioning becomes small compared time spent iterations whereas relation opposite 2 processors 2d problem 16 processors 3d problem example 16 processors 3d problem 884 time linear solver spent factorization preconditioner whereas corresponding number 394 512 processors long preconditioner consumes large portion time superlinear speedup significant effects overall performance implementation evident certain number processors superlinear speedup incomplete lu factorization domain decomposition based preconditioner sufficient give superlinear speedup whole application number processor becomes large factorization consumes smaller proportion execution time hence superlinear behavior less impact overall per formance instead issues become critical large number processors increased number iterations linear solver 6 conclusions contribution presents design analysis parallel prototype implementation tough2 software package parallel implementation shows efficiently use least 512 processors crayt3e system implementation constructed flexibility use different linear solvers preconditioners grid partitioning algorithms well alternative eos modules solving different problems computational experiments real application problems show high speedup 128 processors 2d problem 512 processors 3d problem results accompanied analysis explains good parallel performance observed also explains minor variations performance following unstructured nature problem superlinear speedups following decreased work preconditioning process results also illustrate tradeoff time spent preconditioning effect result objective minimize wall clock execution time note particular problems smaller subdomains could used least small number processors seen variations performance tests using three different partitioning algorithms variations clearly follow variations amount work required eg due differences time discretization analysis required order determine whether variations follow particular pattern result unpredictable circumstances 28 elmroth ding wu problems targeting near future larger terms number blocks number equations per block moreover simulation time significantly longer increased problem size expect able efficiently use even larger number processors available longer simulations directly affect parallel performance future investigations include studies alternative nonlinear solvers studies interplay time stepping proce dure nonlinear systems linear systems evaluations different linear solvers preconditioners parameter settings would general interest may help improve performance particular implementation related study partitioning algorithms recently completed 4 acknowledgements thank karsten pruess author original tough2 software valuable discussions work anonymous referees constructive comments suggestions work supported director office science office laboratory policy infrastructure us department energy contract number deac0376sf00098 research uses resources national energy research scientific computing center supported office science us department energy r aztec users guide tough users guide domain decomposition tr bicgstab fast smoothly converging variant bicg solution nonsymmetric linear systems domain decomposition parallel implementation tough2 software package large scale multiphase fluid heat flow simulations performance cray t3e multiprocessor