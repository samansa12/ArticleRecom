bayesian classification gaussian processes abstractwe consider problem assigning input vector one classes predicting pcschmi x twoclass problem probability class one given schmi x estimated yschmi x gaussian process prior placed yschmi x combined training data obtain predictions new schmi x points provide bayesian treatment integrating uncertainty parameters control gaussian process prior necessary integration carried using laplaces approximation method generalized multiclass problems 2 using softmax function demonstrate effectiveness method number datasets b introduction consider problem assigning input vector x one classes predicting p cjx classic example method logistic regression twoclass problem probability class 1 given x estimated oew x b gammay however method flexible ie discriminant surface simply hyperplane xspace problem overcome extent expanding input x set basis functions foexg example quadratic functions components x highdimensional input space large number basis functions one associated parameter one risks overfitting training data motivates bayesian treatment problem priors parameters encourage smoothness model putting priors parameters basis functions indirectly induces priors functions produced model however possible would argue perhaps natural put priors directly functions one advantage functionspace priors impose general smoothness constraint without tied limited number basis functions regression case task predict realvalued output possible carry nonparametric regression using gaussian processes gps see eg 25 28 solution regression problem gp prior gaussian noise model place kernel function training data point coefficients determined solving linear system parameters describe gaussian process unknown bayesian inference carried described 28 gaussian process method extended classification problems defining gp input sigmoid function idea used number authors although previous treatments typically take fully bayesian approach ignoring uncertainty posterior distribution given data uncertainty parameters paper attempts fully bayesian treatment problem also introduces particular form covariance function gaussian process prior believe useful modelling point view structure remainder paper follows section 2 discusses use gaussian processes regression problems essential background classification case section 3 describe application gaussian processes twoclass classification problems extend multipleclass problems section 4 experimental results presented section 5 followed discussion section 6 paper revised expanded version 1 gaussian processes regression useful first consider regression problem ie prediction real valued output new input value x given set training data ng relevance strategy transform classification problem regression problem dealing input values logistic transfer function stochastic process prior functions allows us specify given set inputs x distribution corresponding outputs denote prior functions p similarly p joint distribution including also specify p tjy probability observing particular values actual values ie noise model z z z hence predictive distribution found marginalization product prior noise model note order make predictions necessary deal directly priors function space n n 1dimensional joint densities however still easy carry calculations unless densities involved special form p tjy p gaussian p jt gaussian whose mean variance calculated using matrix computations involving matrices size n theta n specifying p multidimensional gaussian values n placements points x means prior functions gaussian process formally stochastic process collection random variables fy xjx 2 xg indexed set x case x input space dimension number inputs gp stochastic process fully specified mean function covariance function cx x finite set variables joint multivariate gaussian distribution consider gps x j 0 assume noise model p tjy gaussian mean zero variance oe 2 predicted mean variance x given eg 25 21 parameterizing covariance function many reasonable choices covariance function formally required specify functions generate nonnegative definite covariance matrix set points modelling point view wish specify covariances points nearby inputs give rise similar predictions find following covariance function works well l x l lth component x vector parameters needed define covariance function note analogous hyperparameters neural network define parameters log variables equation 4 since positive scaleparameters covariance function obtained network gaussian radial basis functions limit infinite number hidden units 27 w l parameters equation 4 allow different length scale input dimension irrelevant inputs corresponding w l become small model ignore input closely related automatic relevance determination ard idea mackay 10 neal 15 v 0 variable specifies overall scale prior v 1 specifies variance zeromean offset gaussian distribution gaussian process framework allows quite wide variety priors functions example ornsteinuhlenbeck process covariance function cx x rough sample paths meansquare differentiable hand squared exponential covariance function equation 4 gives rise infinitely ms differentiable process general believe gp method quite generalpurpose route imposing prior beliefs desired amount smoothness reasonably highdimensional problems needs combined modelling assumptions ard another modelling assumption may used build covariance function sum covariance functions one may depend input variables see section 33 details 22 dealing parameters given covariance function straightforward make predictions new test points however practical situations unlikely know covariance function use one option choose parametric family covariance functions parameter vector either estimate parameters example using method maximum likelihood use bayesian approach posterior distribution parameters obtained calculations facilitated fact log likelihood l log p dj calculated analytically log 2 5 kj denotes determinant k also possible express analytically partial derivatives log likelihood respect parameters l see eg 11 given l derivatives respect straightforward feed information optimization package order obtain local maximum likelihood general one may concerned making point estimates number parameters large relative number data points parameters may poorly determined may local maxima likelihood surface reasons bayesian approach defining prior figure 1 x obtained yx squashing sigmoid function oe distribution parameters obtaining posterior distribution data seen attractive make prediction new test point x one simply averages posterior distribution z gps possible integration analytically general numerical methods may used sufficiently low dimension techniques involving grids space used highdimensional difficult locate regions parameterspace high posterior density gridding techniques importance sampling case markov chain monte carlo methods may used work constructing markov chain whose equilibrium distribution desired distribution p jd integral equation 7 approximated using samples markov chain two standard methods constructing mcmc methods gibbs sampler metropolishastings algorithms see eg 5 however conditional parameter distributions amenable gibbs sampling covariance function form given equation 4 metropolishastings algorithm utilize derivative information available means tends inefficient randomwalk behaviour parameterspace following work neal 15 bayesian treatment neural networks williams rasmussen 28 rasmussen 17 used hybrid monte carlo hmc method duane et al 4 obtain samples p jd hmc algorithm described detail appendix 3 gaussian processes twoclass classification simplicity exposition first present method applied twoclass problems extension multiple classes covered section 4 using logistic transfer function produce output interpreted x probability input x belonging class 1 job specifying prior functions transformed specifying prior input transfer function shall call activation denote see figure 1 twoclass problem use logistic function gammay denote probability activation corresponding input x respectively fundamentally gp approaches classification regression problems similar except error model ny oe 2 regression case replaced bernoey choice v 0 equation 4 affect hard classification ie x hovers around 05 takes extreme values 0 1 previous related work approach discussed section 33 regression case two problems address making predictions fixed parameters b dealing parameters shall discuss issues turn 31 making predictions fixed parameters make predictions using fixed parameters would like compute r requires us find p new input x done finding distribution activation using appropriate jacobian transform distribution formally equations obtaining p jt identical equations 1 2 3 however even use gp prior p gaussian usual expression classification data ts take values 0 1 means marginalization obtain p jt longer analytically tractable faced problem two routes follow use analytic approximation integral equations 13 ii use monte carlo methods specifically mcmc methods approximate consider analytic approximation based laplaces approximation approximations discussed section 33 laplaces approximation integrand p yjt approximated gaussian distribution centered maximum function respect inverse covariance matrix given gammarr log p yjt finding maximum carried using newtonraphson iterative method allows approximate distribution calculated details maximization procedure found appendix 32 integration parameters make predictions integrate predicted probabilities posterior p jt p tjp saw 22 regression problem p tj calculated exactly using p r p tjyp yjdy integral analytically tractable classification problem let using log log 2 8 using laplaces approximation maximum find log log 2 denote righthand side equation log p tj stands approximate integration space also cannot done analytically employ markov chain monte carlo method following neal 15 williams rasmussen 28 used hybrid monte carlo hmc method duane et al 4 described appendix use log p tj approximation log p tj use broad gaussian priors parameters 33 previous related work work gaussian processes regression classification developed observation 15 large class neural network models converge gps limit infinite number hidden units computational bayesian treatment gps easier neural networks regression case infinite number weights effectively integrated one ends dealing hyperparameters results 17 show gaussian processes regression comparable performance stateoftheart methods nonparametric methods classification problems seen arise combination two different strands work starting linear regression mccullagh nelder 12 developed generalized linear models glms twoclass classification context gives rise logistic regression strand work development nonparametric smoothing regression problem viewed gaussian process prior functions traced back least far work kolmogorov wiener 1940s gaussian process prediction well known geostatistics field see eg 3 known kriging alternatively considering roughness penalties functions one obtain spline methods recent overviews see 25 8 close connection gp roughness penalty views explored 9 combining glms nonparametric regression one obtains shall call nonparametric glm method classification early references method include 21 16 discussions also found texts 8 25 two differences nonparametric glm method usually described bayesian treatment firstly fixed parameters nonparametric glm method ignores uncertainty hence need integrate described section 31 second difference relates treatment parameters discussed section 22 given parameters one either attempt obtain point estimate parameters carry integration posterior point estimates may obtained maximum likelihood estimation crossvalidation generalized crossvalidation gcv methods see eg 25 8 one problem cvtype methods dimension large computationally intensive search regiongrid parameterspace looking parameters maximize criterion sense hmc method described similar search using gradient information 1 carrying averaging posterior distribution parameters defence gcv methods note wahbas comments eg 26 referring back 24 methods may robust unrealistic prior one difference kinds nonparametric glm models usually considered method exact nature prior used often roughness penalties used expressed terms penalty kth derivative yx gives rise power law power spectrum prior yx also differences parameterization covariance function example unusual find parameters like ard introduced equation 4 nonparametric glm models hand wahba et al 26 considered smoothing spline analysis variance ssanova decomposition gaussian process terms builds prior sum priors functions decomposition ff ff important point functions involving orders interaction univariate functions give rise additive model included sum full interaction term one using bayesian point view questions kinds priors appropriate interesting modelling issue also recent work related method presented paper section 31 mentioned necessary approximate integral equations 13 described use laplaces approximation following preliminary version paper presented 1 gibbs mackay 7 developed alternative analytic approximation using variational methods find approximating gaussian distributions bound marginal likelihood p tj approximate distributions used predict p jt thus x parameters gibbs mackay estimated maximizing lower bound p tj also possible use fully mcmc treatment classification problem discussed recent paper neal 14 method carries integrations posterior distributions simultaneously works generating samples p jd two stage process firstly fixed n individual updated sequentially using gibbs sampling sweep takes time 2 matrix k gamma1 computed time 3 actually makes sense perform quite gibbs sampling scans update parameters probably makes markov chain mix faster secondly parameters updated using hybrid monte carlo method make predictions one averages predictions made would possible obtain derivatives cvscore respect knowledge used practice 4 gps multipleclass classification extension preceding framework multiple classes essentially straightforward although notationally complex throughout employ oneofm class coding scheme 2 use multiclass analogue logistic functionthe softmax functionto describe class probabilities probability instance labelled class c denoted c upper index denotes example number lower index class label similarly activations associated probabilities denoted c formally link function relates activations probabilities c automatically enforces constraint 1 targets similarly represented c specified using oneofm coding log likelihood takes form c softmax link function gives c two class case shall assume gp prior operates activation space specify correlations activations c one important assumption make prior knowledge restricted correlations activations particular class whilst difficulty extending framework include interclass correlations yet encountered situation felt able specify correlations formally activation correlations take form hy c 12 k ii 0 c element covariance matrix cth class individual correlation matrix k c form given equation 4 twoclass case shall use separate set parameters class use independent processes perform classification redundant forcing activations one process say zero would introduce arbitrary asymmetry prior simplicity introduce augmented vector notation twoclass case c denotes activation corresponding input x class c notation also used define similar manner define excluding values corresponding test point x let definition augmented vectors gp prior takes form ae oe equation 12 covariance matrix k block diagonal matrices k individual matrix k c expresses correlations activations within class c twoclass case use laplaces approximation need find mode p jt procedure described appendix c twoclass case make predictions x averaging softmax function gaussian approximation posterior distribution present simply estimate integral using 1000 draws gaussian random vector generator class represented vector length zero entries everywhere except correct component contains 1 5 experimental results using newtonraphson algorithm initialized time entries 1m iterated mean relative difference elements w consecutive iterations less 10 gamma4 hmc algorithm step size used parameters large possible keeping rejection rate low used trajectory made leapfrog steps gave low correlation successive states priors parameters set gaussian mean gamma3 standard deviation 3 simulations step size produced low rejection rate 5 parameters corresponding w l initialized gamma2 v 0 0 sampling procedure run 200 iterations first third run discarded burnin intended give parameters time come close equilibrium distribution tests carried using rcoda package 3 examples section 51 suggested indeed effective removing transients although note widely recognized see eg 2 determining equilibrium distribution reached difficult problem although number iterations used much less typically used mcmc methods remembered iteration involves leapfrog steps ii using hmc aim reduce random walk behaviour seen methods metropolis algorithm autocorrelation analysis parameter indicated general low correlation obtained lag iterations matlab code used run experiments available ftpcsastonacukneuralwillickigpclass 51 two classes tried method two well known two class classification problems leptograpsus crabs pima indian diabetes datasets 4 first rescale inputs mean zero unit variance training set matlab implementations hmc simulations tasks take several hours sgi challenge machine 200mhz r10000 although good results obtained much less time also tried standard metropolis mcmc algorithm crabs problem found similar results although sampling method somewhat slower hmc results crabs pima tasks together comparisons methods 20 18 given tables 1 2 respectively tables also include results obtained gaussian processes using estimation parameters maximizing penalised likelihood found using 20 iterations scaled conjugate gradient optimiser b neals mcmc method details setup used neals method given appendix e leptograpsus crabs problem attempt classify sex crabs basis five anatomical attributes optional additional colour attribute 50 examples available crabs sex colour making total 200 labelled examples split training set 20 crabs sex colour making 80 training examples 120 examples used test set performance gp method equal best methods reported 20 namely 2 hidden unit neural network direct input output connections logistic output unit trained maximum likelihood network1 table 1 neals method gave similar level performance also found estimating parameters using maximum penalised likelihood mpl gave similar performance less minute computing time pima indians diabetes problem used data made available prof ripley trainingtest split 200 332 examples respectively 18 baseline error obtained simply classifying record coming diabetic gives rise error 33 neals gp methods comparable best alternative performance error around 20 encouraging results obtained using laplaces approximation neals method similar 5 also estimated parameters using maximum penalised likelihood rather monte carlo integration performance case little worse 217 error 2 minutes computing time 3 available comprehensive r archive network httpwwwcituwienacat 4 available httpmarkovstatsoxacukpubprnn 5 performance obtained gibbs mackay 7 similar method made 4 errors crab task colour given 70 errors pima dataset method colour given colour given neural network1 3 3 neural network2 5 3 linear discriminant 8 8 logistic regression 4 4 pp regression 4 ridge gaussian process laplace 3 3 approximation hmc gaussian process laplace 4 3 approximation mpl gaussian process neals method 4 3 table 1 number test errors leptograpsus crabs task comparisons taken ripley 1996 ripley 1994 respectively network2 used two hidden units predictive approach ripley 1993 uses laplaces approximation weight network local minimum method pima indian diabetes neural network 75 linear discriminant 67 logistic regression 66 pp regression 4 ridge functions 75 gaussian mixture 64 gaussian process laplace 68 approximation hmc gaussian process laplace 69 approximation mpl gaussian process neals method 68 table 2 number test errors pima indian diabetes task comparisons taken ripley 1996 ripley 1994 respectively neural network one hidden unit trained maximum likelihood results worse nets two hidden units ripley 1996 analysis posterior distribution w parameters covariance function equation informative figure 51 plots posterior marginal mean 1 standard deviation error bars seven input dimensions recalling variables scaled zero mean unit variance would appear variables 1 3 shortest lengthscales therefore variability associated 52 multiple classes due rather long time taken run code able test relatively small problems mean hundred data points several classes furthermore found full bayesian integration possible parameter settings beyond computational means therefore satisfied maximum penalised likelihood approach rather using potential gradient hmc routine simply used inputs scaled conjugate gradient optimiser based 13 instead attempting find mode class posterior rather average posterior distribution tested multiple class method forensic glass dataset described 18 dataset 214 examples 9 inputs 6 output classes dataset small performance figure 2 plot log w parameters pima dataset circle indicates posterior marginal mean obtained hmc run burnin one standard deviation error bars square symbol shows log wparameter values found maximizing penalized likelihood variables 1 number pregnancies 2 plasma glucose concentration 3 diastolic blood pressure 4 triceps skin fold thickness 5 body mass index 6 diabetes pedigree function 7 age comparison wahba et al 1995 using generalized linear regression found variables 1 2 5 6 important estimated using 10fold cross validation computing penalised maximum likelihood estimate multiple gp method took approximately 24 hours sgi challenge gave classification error rate 233 see table 3 comparable best methods performance neals method surprisingly poor may due fact allow separate parameters processes constrained equal neals code also small perhaps significant differences specification prior see appendix e details 6 discussion paper extended work williams rasmussen 28 classification problems demonstrated performs well datasets tried believe kinds gaussian method forensic glass neural network 4hu 238 linear discriminant 36 pp regression 5 ridge functions 35 gaussian mixture 308 decision tree 322 gaussian process la mpl 233 gaussian process neals method 318 table 3 percentage test error forensic glass problem see ripley 1996 details methods process prior used easily interpretable models neural networks priors parameterization function space example posterior distribution ard parameters illustrated figure 51 pima indians diabetes problem indicates relative importance various inputs interpretability also facilitate incorporation prior knowledge new problems quite strong similarities gp classifiers supportvector machines svms 23 svm uses covariance kernel differs gp approach using different data fit term maximum margin optimal found using quadratic programming comparison two algorithms interesting direction future research problem methods based gps require computations trace determinants linear solutions involving n theta n matrices n number training examples hence run problems large datasets looked methods using bayesian numerical techniques calculate trace determinant 22 6 although found techniques work well relatively small size problems tested methods computational methods used speed quadratic programming problem svms may also useful gp classifier problem also investigating use different covariance functions improvements approximations employed acknowledgements thank prof b ripley making available leptograpsus crabs pima indian diabetes forensic glass datasets work partially supported epsrc grant grj75425 novel developments learning theory neural networks much work carried aston university authors gratefully acknowledge hospitality provided isaac newton institute mathematical sciences cambridge uk paper written thank mark gibbs david mackay radford neal helpful discussions anonymous referees comments helped improve paper appendix maximizing case describe find iteratively vector p jt maximized material also covered 8 x533 25 x92 provide completeness terms equation 9 welldefined complete set activations bayes theorem log log p depend normalizing factor maximum p jt found maximizing psi respect using log log 2 14 k covariance matrix gp evaluated x psi defined similarly equation 8 k partitioned terms n theta n matrix k n theta 1 vector k scalar k viz enters equation 14 quadratic prior term data point associated maximizing respect achieved first maximizing psi respect quadratic optimization determine find maximum psi use newtonraphson iteration new differentiating equation 8 respect find noise matrix given results iterative equation avoid unnecessary inversions usually convenient rewrite form note gammarrpsi always positive definite optimization problem convex given converged solution easily found using w zero appended n diagonal position given mean variance easy find r mean distribution p jt order calculate gaussian integral logistic sigmoid function employ approximation based expansion sigmoid function terms error function gaussian integral error function another error function approximation fast compute specifically use basis set five scaled error functions interpolate logistic sigmoid chosen points 6 gives accurate approximation desired integral small computational cost justification laplaces approximation case somewhat different argument usually put forward eg asymptotic normality maximum likelihood estimator model finite number parameters dimension problem grows number data points however consider infill asymptotics see eg 3 number data points bounded region increases local average training data point x provide tightly localized estimate x hence yx reasoning parallels formal arguments found 29 thus would expect distribution p become gaussian increasing data appendix b derivatives log p tj wrt hmc mpl methods require derivative l log p tj respect components example k derivative involve two terms one due explicit dependencies l log 2 k also change cause change however chosen rpsiyj l gamma2 log jk dependence jk gamma1 w j arises dependence w hence differentiating one obtains hence required derivative calculated appendix maximizing multipleclass case gp prior likelihood defined equations 13 11 define posterior distribution activations jt appendix interested laplace approximation posterior therefore need find mode respect dropping unnecessary constants multiclass analogue equation 14 c exp 6 detail used basis functions erfx used interpolate oex principle appendix define psi analogy equation 8 first optimize psi respect afterwards performing quadratic optimization psi respect order optimize psi respect make use hessian given k mn theta mn blockdiagonal matrix blocks k c although form two class case equation 17 slight change definition noise matrix w convenient way define w introducing matrix pi mn theta n matrix form using notation write noise matrix form diagonal matrix outer product twoclass case note gammarrpsi positive definite optimization problem convex update equation iterative optimization psi respect activations follows form given equation 18 advantage representation noise matrix equation 23 invert matrices find determinants using identities blockdiagonal inverted blockwise thus rather requiring determinants inverses mn theta mn matrix need carry expensive matrix computations n theta n matrices resulting update equations form given equation 18 noise matrix covariance matrices multiple class form essentially results needed generalize method multipleclass problem although mentioned time complexity problem scale 3 rather due identities equations 24 25 calculating function gradient still rather expensive experimented several methods mode finding laplace approximation advantage newton iteration method fast quadratic convergence integral part newton step calculation inverse matrix acting upon vector ie gamma1 b order speed particular step used conjugate gradient cg method solve iteratively corresponding linear system b repeatedly need solve system w changes updated saves time run cg method convergence time called experiments cg algorithm terminated 1n calculation derivative log p tj wrt multipleclass case analogous twoclass case described appendix b appendix hybrid monte carlo hmc works creating fictitious dynamical system parameters regarded position variables augmenting momentum variables p purpose dynamical system give parameters inertia randomwalk behaviour space avoided total energy h system sum kinetic energy potential energy e potential energy defined pjd expgammae ie sample joint distribution p given p p expgammae gamma k marginal distribution required posterior sample parameters posterior therefore obtained simply ignoring momenta sampling joint distribution achieved two steps finding new points phase space nearidentical energies h simulating dynamical system using discretised approximation hamiltonian dynamics ii changing energy h gibbs sampling momentum variables hamiltons first order differential equations h approximated using leapfrog method requires derivatives e respect given gaussian prior log p straightforward differentiate derivative log p tj also straightforward although implicit dependencies hence must taken account described appendix b calculation energy quite expensive new need perform maximization required laplaces approximation equation 9 proposed state accepted rejected using metropolis rule depending final energy h necessarily equal initial energy h discretization appendix e simulation setup neals code used fbm software available httpwwwcsutorontocaradfordfbmsoftwarehtml example commands used run pima example modelspec pima1log binary gpgen pima1log fix 05 1 mcspec pima1log repeat 4 scanvalues 200 heatbath hybrid 6 05 gpmc pima1log 500 follow closely example given neals documentation gpspec command specifies form gaussian process particular priors parameters v 0 ws see equation 4 expression 00505 specifies gammadistribution prior v 0 x02051 specifies hierarchical gamma prior ws note jitter 01 also specified prior covariance function improves conditioning covariance matrix mcspec command gives details mcmc updating procedure specifies 4 repetitions 200 scans values followed 6 hmc updates parameters using stepsize adjustment factor 05 gpmc specifies sequence carried 500 times aimed rejection rate around 5 exceeded stepsize reduction factor reduced simulation run r statistics spatial data hybrid monte carlo bayesian data analysis efficient implementation gaussian processes variational gaussian process classifiers nonparametric regression generalized linear models correspondence bayesian estimation stochastic processes smoothing splines bayesian methods backpropagation networks maximum likelihood estimation models residual covariance spatial regression generalized linear models scaled conjugate gradient algorithm fast supervised learning monte carlo implementation gaussian process models bayesian regression classification bayesian learning neural networks automatic smoothing regression functions generalized linear models evaluation gaussian processes methods nonlinear regres sion pattern recognition neural networks statistical aspects neural networks flexible nonlinear approaches classification density ratios bayesian numerical analysis nature statistical learning theory comparison gcv gml choosing smoothing parameter generalized spline smoothing problem spline models observational data classification computing infinite networks gaussian processes regression comparison kriging nonparametric regression methods tr ctr christopher k williams connection kernel pca metric multidimensional scaling machine learning v46 n13 p1119 2002 keerthi k b duan k shevade n poo fast dual algorithm kernel logistic regression machine learning v61 n13 p151165 november 2005 mrio figueiredo adaptive sparseness supervised learning ieee transactions pattern analysis machine intelligence v25 n9 p11501159 september koji tsuda propagating distributions hypergraph dual information regularization proceedings 22nd international conference machine learning p920927 august 0711 2005 bonn germany w addison r h glendinning robust image classification signal processing v86 n7 p14881501 july 2006 hyunchul kim daijin kim zoubin ghahramani sung yang bang appearancebased gender classification gaussian processes pattern recognition letters v27 n6 p618626 15 april 2006 yasemin altun alex j smola thomas hofmann exponential families conditional random fields proceedings 20th conference uncertainty artificial intelligence p29 july 0711 2004 banff canada wei chu zoubin ghahramani preference learning gaussian processes proceedings 22nd international conference machine learning p137144 august 0711 2005 bonn germany balaji krishnapuram alexander j hartemink lawrence carin mario figueiredo bayesian approach joint feature selection classifier design ieee transactions pattern analysis machine intelligence v26 n9 p11051111 september 2004 wei chu sathiya keerthi chong jin ong bayesian trigonometric support vector classifier neural computation v15 n9 p22272254 september yasemin altun thomas hofmann alexander j smola gaussian process classification segmenting annotating sequences proceedings twentyfirst international conference machine learning p4 july 0408 2004 banff alberta canada hyunchul kim jaewook lee clustering based gaussian processes neural computation v19 n11 p30883107 november 2007 bart bakker tom heskes task clustering gating bayesian multitask learning journal machine learning research 4 p8399 1212003 mark girolami simon rogers variational bayesian multinomial probit regression gaussian process priors neural computation v18 n8 p17901817 august 2006 liefeng bo ling wang licheng jiao feature scaling kernel fisher discriminant analysis using leaveoneout cross validation neural computation v18 n4 p961978 april 2006 volker tresp generalized bayesian committee machine proceedings sixth acm sigkdd international conference knowledge discovery data mining p130139 august 2023 2000 boston massachusetts united states malte kuss carl edward rasmussen assessing approximate inference binary gaussian process classification journal machine learning research 6 p16791704 1212005 lehel csat manfred opper sparse online gaussian processes neural computation v14 n3 p641668 march 2002 manfred opper ole winther gaussian processes classification meanfield algorithms neural computation v12 n11 p26552684 november 2000 michael lindenbaum shaul markovitch dmitry rusakov selective sampling nearest neighbor classifiers machine learning v54 n2 p125152 february 2004 volker tresp scaling kernelbased systems large data sets data mining knowledge discovery v5 n3 p197211 july 2001 charles micchelli massimiliano pontil learning vectorvalued functions neural computation v17 n1 p177204 january 2005 michael e tipping sparse bayesian learning relevance vector machine journal machine learning research 1 p211244 912001 balaji krishnapuram lawrence carin mario figueiredo alexander j hartemink sparse multinomial logistic regression fast algorithms generalization bounds ieee transactions pattern analysis machine intelligence v27 n6 p957968 june 2005 van gestel j k suykens g lanckriet lambrechts b de moor j vandewalle bayesian framework leastsquares support vector machine classifiers gaussian processes kernel fisher discriminant analysis neural computation v14 n5 p11151147 may 2002 zhihua zhang james kwok dityan yeung modelbased transductive learning kernel matrix machine learning v63 n1 p69101 april 2006 gavin c cawley nicola l c talbot preventing overfitting model selection via bayesian regularisation hyperparameters journal machine learning research 8 p841861 512007 matthias seeger pacbayesian generalisation error bounds gaussian process classification journal machine learning research 3 p233269 312003 arnulf b graf felix wichmann heinrich h blthoff bernhard h schlkopf classification faces man machine neural computation v18 n1 p143165 january 2006 ralf herbrich thore graepel colin campbell bayes point machines journal machine learning research 1 p245279 912001 liam paninski jonathan w pillow eero p simoncelli maximum likelihood estimation stochastic integrateandfire neural encoding model neural computation v16 n12 p25332561 december 2004 alexander j smola bernhard schlkopf bayesian kernel methods advanced lectures machine learning springerverlag new york inc new york ny