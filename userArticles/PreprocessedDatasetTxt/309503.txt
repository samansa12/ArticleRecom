winnowbased approach contextsensitive spelling correction large class machinelearning problems natural language require characterization linguistic context two characteristic properties problems feature space high dimensionality target concepts depend small subset features space conditions multiplicative weightupdate algorithms winnow shown exceptionally good theoretical properties work reported present algorithm combining variants winnow weightedmajority voting apply problem aforementioned class contextsensitive spelling correction task fixing spelling errors happen result valid words substituting casual causal evaluate algorithm winspell comparing bayspell statisticsbased method representing state art task find 1 run full unpruned set features achieves accuracies significantly higher bayspell able achieve either pruned unpruned conditionsemi 2 compared systems literature winspell exhibits highest performancesemi 3 several aspects winspells architecture contribute superiority bayspell primary factor able learn better linear separator bayspell learnssemi 4 run test set drawn different corpus training set drawn winspell better able bayspell adapt using strategy present combines supervised learning training set unsupervised learning noisy test set b introduction large class machinelearning problems natural language require characterization linguistic context problems include lexical disambiguation tasks partofspeech tagging wordsense disambiguation grammatical disambiguation tasks prepositionalphrase attachment documentprocessing tasks text classification context usually whole document problems two distinctive properties first richness linguistic structures must represented results extremely highdimensional feature spaces problems second given target concept depends small subset features leaving huge balance features irrelevant earlier version work appeared icml96 particular concept paper present learning algorithm architecture properties suitable class problems algorithm builds recently introduced theories multiplicative weightupdate algorithms combines variants winnow littlestone 1988 weighted majority littlestone warmuth 1994 extensive analysis algorithms colt literature shown exceptionally good behavior presence irrelevant attributes noise even target function changing time littlestone 1988 littlestone warmuth 1994 herbster warmuth 1995 properties make particularly wellsuited class problems studied theoretical properties winnow family algorithms well known recently people started test claimed abilities algorithms applications address claims empirically applying winnowbased algorithm largescale realworld task aforementioned class problems contextsensitive spelling correction contextsensitive spelling correction task fixing spelling errors result valid words id like peace cake peace typed piece intended errors account anywhere 25 50 observed spelling errors kukich 1992 yet go undetected conventional spell checkers unix spell flag words found word list contextsensitive spelling correction involves learning characterize linguistic contexts different words piece peace tend occur problem multitude features one might use characterize contexts features test presence particular word nearby target word features test pattern parts speech around target word tasks consider number features ranges hundred 10000 1 feature space large however target concepts context piece occur depend small subset features vast majority irrelevant concept hand contextsensitive spelling correction therefore fits characterization presented provides excellent testbed studying performance multiplicative weightupdate algorithms realworld task evaluate proposed winnowbased algorithm winspell compare bayspell golding 1995 statisticsbased method among successful tried problem first compare winspell bayspell using heavilypruned feature set bayspell normally uses typically 10 1000 features winspell found perform comparably bayspell condition full unpruned feature set used however winspell comes achieving substantially higher accuracy achieved pruned condition better accuracy bayspell achieved either condition calibrate observed performance bayspell winspell compare methods reported literature winspell found significantly outperform methods tried using comparable feature set core winspell bayspell linear separators given fundamental similarity algorithms ran series experiments understand winspell nonetheless able outperform bayspell several aspects winspell architecture found contribute superiority principal factor winspell simply learned better linear separator bayspell attribute fact bayesian linear separator based idealized assumptions domain winnow able adapt via mistakedriven update rule whatever conditions held practice address issue dealing test set dissimilar training set arises contextsensitive spelling correction well related disambiguation tasks patterns word usage vary widely across documents thus training test documents may quite different first confirming experimentally performance indeed degrade unfamiliar test sets present strategy dealing situation strategy called combines supervised learning training set unsupervised learning noisy test set find using strategy bayspell winspell able improve performance unfamiliar test set winspell however found particularly well achieving comparable performance using strategy unfamiliar test set achieved familiar test set rest paper organized follows next section describes task contextsensitive spelling correction present bayesian method used winnowbased approach problem introduced experiments winspell bayspell presented final section concludes 2 contextsensitive spelling correction widespread availability spell checkers fix errors result non words teh predominant type spelling error become kind results real unintended word example typing intended fixing kind error requires completely different technology used conventional spell checkers requires analyzing context infer word likely intended call task contextsensitive spelling correction task includes fixing classic types spelling mistakes homophone errors eg peace piece typographic errors eg form also mistakes commonly regarded grammatical errors eg among errors cross word boundaries eg maybe may problem started receiving attention literature within last halfdozen years number methods proposed either contextsensitive spelling correction directly related lexical disambiguation tasks methods include word trigrams mays et al 1991 bayesian classifiers gale et al 1993 decision lists yarowsky 1994 bayesian hybrids golding 1995 combination partofspeech trigrams bayesian hybrids golding schabes 1996 recently transformationbased learning mangu brill 1997 latent semantic analysis jones martin 1997 differential grammars powers 1997 research systems gradually achieving higher levels accuracy believe winnowbased approach particularly promising problem due problems need large number features characterize context word occurs win nows theoreticallydemonstrated ability handle large numbers features 21 problem formulation cast contextsensitive spelling correction word disambiguation task ambiguity among words modelled confusion sets confusion set means word w set ambiguous word thus c fhear hereg see occurrence either hear target document take ambiguous hear task decide context one actually intended acquiring confusion sets interesting problem right work reported however take confusion sets largely list words commonly confused back random house dictionary flexner 1983 includes mainly homophone errors confusion sets random house added representing grammatical typographic errors bayesian winnowbased methods contextsensitive spelling correction described terms operation single confusion set say disambiguate occurrences words w 1 wn methods handle multiple confusion sets applying technique confusion set independently 22 representation target problem contextsensitive spelling correction consists sentence ii target word sentence correct bayesian winnow based algorithms studied represent problem list active features active feature indicates presence particular linguistic pattern context target word use two types features context words collocations contextword features test presence particular word within sigmak words target word collocations test pattern contiguous words andor partofspeech tags 2 around target word experiments reported k set 10 2 examples useful features confusion set fweather whetherg include 1 cloudy within sigma10 words 2 verb feature 1 contextword feature tends imply weather feature 2 collocation checks pattern verb immediately target word tends imply whether dont know whether laugh cry intuition using two types features capture two im portant complementary aspects context context words tell us kind words tend appear vicinity target word lexical atmo sphere therefore capture aspects context widescope semantic flavor discourse topic tense collocations contrast capture local syntax around target word similar combinations features used related tasks accent restoration yarowsky 1994 word sense disambiguation ng lee 1996 use feature extractor convert initial text representation sentence list active features feature extractor preprocessing phase learns set features task thereafter convert sentence list active features simply matching set learned features sentence preprocessing phase feature extractor learns set features characterize contexts word w confusion set tends occur involves going training corpus time word confusion set occurs generating possible features context namely one contextword feature every distinct word within sigmak words one collocation every way expressing pattern contiguous elements gives exhaustive list features found training set statistics occurrence features collected process well point pruning criteria may applied eliminate unreliable uninformative features use two criteria make use aforementioned statistics occurrence 1 feature occurred practically none training instances specifically fewer 10 occurrences fewer 10 nonoccurrences 2 presence feature significantly correlated identity target word determined chisquare test 005 significance level 3 bayesian approach various approaches tried contextsensitive spelling cor rection bayesian hybrid method call bayspell among successful thus method adopt benchmark comparison winspell bayspell described elsewhere golding 1995 briefly reviewed however version uses improved smoothing technique described given sentence target word correct bayspell starts invoking feature extractor section 22 convert sentence set f active features bayspell normally runs feature extractor pruning enabled first approximation bayspell acts naive bayesian classifier suppose moment really applying naive bayes would calculate probability word w confusion set correct identity target word given features f observed using bayes rule conditional independence assumption probability righthand side calculated maximumlikelihood estimate 3 mle training set would pick answer w highest p w jf bayspell differs naive approach two respects first assume conditional independence among features rather heuristics detecting strong dependencies resolving deleting features left reduced set f 0 relatively independent features used place f equation procedure called dependency resolution second estimate p f jw terms bayspell use simple mle tends give likelihoods 00 rare features abundant task hand thus yielding useless answer 00 posterior probability instead bayspell performs smoothing interpolating mle p f jw mle unigram probability p f means incorporating lowerorder model way generally regarded essential good performance chen goodman 1996 use set probability presence feature f independent presence word w extent independence holds p f accurate robust estimate p f jw calculate chisquare probability observed association f w due chance enhancement smoothing minor extent dependency resolution greatly improve performance bayspell naive bayesian approach effect enhancements seen empirically section 54 4 winnowbased approach various ways use learning algorithm winnow littlestone 1988 task contextsensitive spelling correction straightforward approach would learn confusion set discriminator distinguishes specifically among words set drawback approach however learning applicable one particular discrimination task pursue alternative approach learning contextual characteristics word w individually learning used distinguish word w word well perform broad spectrum natural language tasks roth 1998 following briefly present general approach concentrate task hand contextsensitive spelling correction approach developed influenced neuroidal system suggested valiant 1994 system consists large number items range 5 correspond highlevel concepts humans words well lowerlevel predicates highlevel ones composed lowerlevel predicates encode aspects current state world input architecture outside highlevel concepts learned functions lowerlevel predicates particular highlevel concept learned cloud ensemble classifiers classifiers within cloud learn clouds highlevel concept autonomously function lowerlevel predicates different values learning parameters outputs classifiers combined output cloud using variant weighted majority algorithm littlestone warmuth 1994 within classifier variant winnow algorithm littlestone 1988 used training occurs whenever architecture interacts world example reading sentence text architecture thereby receives new values lowerlevel predicates turn serve example training highlevel ensembles classifiers learning thus online process done continuous basis 4 valiant 1995 figure 1 shows instantiation architecture contextsensitive spelling correction particular correcting words fdesert dessertg bottom tier network consists nodes lowerlevel predicates application correspond features domain clarity five nodes shown thousands typically occur practice highlevel concepts application correspond words confusion set desert dessert highlevel concept appears cloud nodes shown set overlapping bubbles suspended box output clouds activation level word confusion set comparator selects word highest activation final result contextsensitive spelling correction sections elaborate use winnow weighted majority followed discussion properties architecture 41 winnow job classifier within cloud winspell decide whether particular word w confusion set belongs target sentence classifier runs winnow algorithm takes input representation target sentence set active features returns binary decision whether word w belongs sentence let f set active features feature weight arc connecting f classifier hand winnow algorithm returns classification 1 positive iff threshold parameter experiments reported set 1 initially classifier connection feature network training however establishes appropriate connections learns weights connections training example consists sentence represented set active features together word w c confusion set correct sentence example treated positive example classifiers w c negative example classifiers words confusion set training proceeds online fashion example presented system representation classifiers updated example discarded first step training classifier example establish appropriate desert desert weighted majority words hot within arid within words desert activation desert comparator cake within words desertdessert decision dessert dessert weighted majority dessert activation dessert figure 1 example winspell network fdesert dessertg five nodes bottom tier network correspond features two higherlevel clouds nodes shown overlapping bubbles suspended box correspond words confusion set nodes within cloud run winnow algorithm parallel different setting demotion parameter fi copy input arcs weights arcs overall activation level word confusion set obtained applying weighted majority algorithm nodes words cloud word highest activation level selected connections classifier active features f example active feature f 2 f already connected classifier sentence positive example classifier classifier corresponds target word w c occurs sentence add connection feature classifier default weight 01 policy building connections asneeded basis results sparse network connections demonstrated occur real examples note build new connections sentence negative example classifier 5 one consequence different words confusion set may links different subsets possible features seen figure 1 second step training update weights connections done using winnow update rule updates weights mistake made classifier predicts 0 positive example ie 1 correct classification weights promoted promotion parameter classifier predicts 1 negative example ie 0 correct classification weights demoted parameter experiments reported ff set 15 fi varied 05 09 see also section 42 way weights nonactive features remain unchanged update time algorithm depends number active features current example total number features domain use sparse architecture described coupled representation example list active features reminiscent infinite attribute models winnow blum 1992 42 weighted majority rather evaluating evidence given word w using single classifier combines evidence multiple classifiers motivation discussed weighted majority littlestone warmuth 1994 used combination basic approach run several classifiers parallel within cloud try predict whether w belongs sentence classifier uses different values learning parameters therefore makes slightly different predictions performance classifier monitored weight derived reflecting observed prediction accuracy final activation level output cloud sum predictions member classifiers weighted abovementioned weights specifically used clouds composed five classifiers differing values winnow demotion parameter fi values 05 06 07 08 used weighting scheme assigns jth classifier weight total number mistakes made classifier far essential property weight classifier makes many mistakes rapidly disappears start decrease value number examples seen avoid weighing mistakes initial hypotheses heavily 6 total activation returned cloud c j classification either 1 0 returned jth classifier cloud denominator normalization term rationale combining evidence multiple classifiers twofold first running mistakedriven algorithm even known good behavior asymptotically guarantee current hypothesis point time better previous one common practice therefore predict using average last several hypotheses weighting hypothesis example length mistakefree stretch littlestone 1995 cesabianchi et al 1994 second layer winspell ie weightedmajority layer partly serves function though online fashion second motivation weightedmajority layer comes desire algorithm tunes parameters task contextsensitive spelling correction selftuning used automatically accommodate differences among confusion sets particular differences degree words confusion set overlapping usages fweather whetherg example words occur essentially disjoint contexts thus training example gives one word classifier predicts almost surely wrong hand famong betweeng numerous contexts words acceptable thus disagreement training example necessarily mean classifier wrong following mistake therefore want demote weights former case latter updating weights various demotion parameters parallel allows algorithm select best setting parameters confusion set addition using weighted majority layer strictly increases expressivity architecture plausible cases linear separator would unable achieve good prediction twolayer architecture would able 43 discussion multiplicative learning algorithms studied extensively learning theory community recent years littlestone 1988 kivinen warmuth 1995 winnow shown learn efficiently linear threshold function little stone 1988 mistake bound depends margin positive negative examples functions f exist real weights w wn real threshold fx iff particular functions include boolean disjunctions conjunctions k n variables rofk threshold functions 1 r k n key feature winnow mistake bound grows linearly number relevant attributes logarithmically total number attributes n using sparse architecture keep variables beginning rather add variables necessary number mistakes made disjunctions conjunctions logarithmic size largest example seen linear number relevant attributes independent total number attributes domain blum 1992 winnow analyzed presence various kinds noise cases linear threshold function make perfect classifications littlestone 1991 proved assumptions type noise winnow still learns correctly retaining abovementioned dependence number total relevant attributes see kivinen warmuth 1995 thorough analysis multiplicative update algorithms versus additive update algorithms exact bounds depend sparsity target function number active features examples algorithm makes independence assumptions attributes contrast bayesian predictors commonly used statistical nlp condition recently investigated experimentally simulated data littlestone 1995 shown redundant attributes dramatically affect bayesian pre dictor superfluous independent attributes less dramatic effect number attributes large order 10000 winnow mistakedriven algorithm updates hypothesis mistake made intuitively makes algorithm sensitive relationships among attributes relationships may go unnoticed algorithm based counts accumulated separately attribute crucial analysis algorithm shown crucial empirically well littlestone 1995 one advantages multiplicative update algorithms logarithmic dependence number domain features property allows one learn higherthanlinear discrimination functions increasing dimensionality feature space instead learning discriminator original feature space one generate new features conjunctions original features learn linear separator new space likely exist given modest dependency winnow dimensionality worthwhile increase dimensionality simplify functional form resulting discriminator work reported regarded following path define collocations patterns words partofspeech tags rather restricting tests singleton elements increases dimensionality adds redundancy among features time simplifies functional form discriminator point classes almost linearly separable new space similar philosophy albeit different technically followed work support vector machines cortes vapnik 1995 theoretical analysis shown winnow able adapt quickly changing target concept herbster warmuth 1995 investigate issue experimentally section 55 feature winspell prune poorly 2performing attributes whose weight falls low relative highest weight attribute used classifier pruning way greatly reduce number features need retained representation important observe though tension compacting representation aggressively discarding features maintaining ability adapt new test environment paper focus adaptation study discarding techniques tradeoff currently investigation 5 experimental results understand performance winspell task contextsensitive spelling correction start comparing bayspell using pruned set features feature extractor bayspell normally uses evaluates winspell purely method combining evidence multiple features important claimed strength winnowbased approach however ability handle large numbers features tested essentially disabling pruning resulting tasks 10000 features seeing winspell bayspell scale first experiment showed winspell bayspell perform relative outside reference calibrate performance compared two algorithms methods reported literature well baseline method success winspell previous experiments brought question able outperform bayspell methods investigated ablation study stripped winspell simple nonlearning algorithm gave initial set weights allowed emulate bayspells behavior exactly restored missing aspects winspell one time observing much contributed improving performance bayesian level preceding experiments drew training test sets popula tion following traditional paclearning assumption assumption may unrealistic task hand however system may encounter target document quite unlike seen training check whether fact problem tested acrosscorpus performance methods found indeed significantly worse withincorpus performance address problem tried strategy combining learning training set unsupervised learning noisy test set tested well winspell bayspell able perform unfamiliar test set using strategy sections describe experimental methodology present ex periments interleaved discussion 51 methodology experiments follow training test sets drawn two corpora 1millionword brown corpus kucera francis 1967 34 millionword corpus articles wall street journal wsj marcus et al 1993 note particular annotations needed corpora task contextsensitive spelling correction simply assume texts contain contextsensitive spelling errors thus observed spellings may taken gold standard algorithms run 21 confusion sets taken largely list words commonly confused back random house dictionary 1983 confusion sets selected basis frequentlyoccurring brown wsj include mainly homophone confusions eg pieceg several confusion sets random house added representing grammatical errors eg famong betweeng typographic errors eg may beg results reported percentage correct classifications confusion set well overall score gives percentage correct confusion sets pooled together comparing scores tested significance using mcnemar test dietterich 1998 possible data individual trials available system comparison comparison across different test sets withinacross study instead used test difference two proportions fleiss 1981 tests reported 005 significance level 52 pruned versus unpruned first step evaluation test winspell conditions bayspell normally runs ie using pruned set features feature extractor used random 8020 split sentence brown training test sets results running algorithm 21 confusion sets appear pruned columns table 1 although confusion sets one algorithm better overall winspell performs comparably bayspell preceding comparison shows winspell credible method task test claimed strength winnow ability deal large numbers features test modified feature extractor minimal pruning features features pruned occurred exactly training set features extremely unlikely afford good generalizations extremely numerous hope considering full set features pick many minor cases holte et al called small disjuncts normally filtered pruning process results shown unpruned columns table 1 algorithms better unpruned condition winspell improves almost every confusion set sometimes markedly result outperforms bayspell unpruned condition every confusion set except one results focus behavior algorithms unpruned case table 1 pruned versus unpruned performance bayspell winspell pruned condition algorithms use pruned set features feature extractor unpruned condition use full set excluding features occurring training set algorithms trained 80 brown tested 20 first two columns give number features two conditions bar graphs show differences adjacent columns shading indicating significant differences using mcnemar test 005 level confusion set pruned unpruned pruned unpruned features features bayspell winspell bayspell winspell accept except 78 849 880 878 920 960 affect effect 36 842 980 1000 980 1000 among 145 2706 753 758 780 860 amount number 68 1618 748 732 805 862 begin 84 2219 952 897 952 979 cite sight site 24 585 765 647 735 853 fewer less 6 1613 960 944 920 933 1161 11625 978 982 983 985 180 4679 945 964 959 973 lead led 33 833 898 875 857 918 maybe may 86 1639 906 844 958 979 passed past 141 1279 905 905 905 959 peace piece 67 992 740 720 920 880 principal principle 38 669 853 848 853 912 quiet quite 41 1200 955 954 894 939 raise rise 24 621 795 743 872 897 857 6813 936 969 934 957 theyre 946 9449 948 966 945 985 weather whether 61 1226 934 984 984 1000 youre 103 2738 904 936 909 973 53 system comparison previous section shows winspell bayspell perform relative evaluate respect external standard compared methods reported literature two recent methods use test sets thus readily compared rules transformationbased learner mangu brill 1997 method based latent semantic analysis jones martin 1997 also compare baseline canonical straw man task simply identifies common member confusion set training guesses every time testing results appear table 2 scores lsa taken jones martin 1997 based different 8020 breakdown brown used systems scores rules version system uses feature set comparison shows winspell significantly higher performance systems interestingly however mangu brill able improve ruless overall score 885 933 almost level winspell making various clever enhancements feature set table 2 system comparison algorithms trained 80 brown tested 20 except lsa used 8020 breakdown version rules one uses feature set bayspell winspell run unpruned condition first column gives number test cases bar graphs show differences adjacent columns shading indicating significant differences using test difference two proportions raggedended bars indicate difference 15 percentage points three overall lines pool results different sets confusion sets confusion set test baseline lsa rules bayspell winspell cases accept except 50 700 823 880 920 960 affect effect 49 918 943 979 980 1000 among 186 715 808 731 780 860 amount number 123 715 566 780 805 862 begin 146 932 932 953 952 979 cite sight site 34 647 781 735 853 country county 62 919 813 952 919 952 fewer less 75 907 920 933 1225 830 983 985 366 913 928 959 973 lead led maybe may 96 875 958 979 passed past 74 689 803 837 905 959 peace piece 50 440 839 900 920 880 principal principle 34 588 912 882 853 912 quiet quite 66 833 908 924 894 939 raise rise 39 641 806 846 872 897 514 634 905 926 934 957 theyre 850 568 739 945 985 weather whether 61 869 851 934 984 1000 youre 187 893 914 909 973 including using tagger assign word possible tags context rather merely using words complete tag set suggests winspell might get similar boost adopting enhanced set features note lsa system lsa reported best confusion sets words part speech since hold confusion sets lsas overall score adversely affected 54 ablation study previous sections demonstrate superiority winspell bayspell task hand explain winnowbased algorithm better core winspell bayspell linear separators roth 1998 winnow multiplicative update rule able learn better linear separator one given bayesian probability theory nonwinnow enhancements winspell particularly weightedmajority voting provide leverage address questions ran ablation study isolate contributions different aspects winspell study based observation core computations winnow bayesian classifiers essentially isomorphic winnow makes decisions based weighted sum observed features bayesian classifiers make decisions based sum product likelihoods prior proba bility taking logarithm functional form yields linear function understanding start full bayspell system strip bayesian essence map taking log simplified nonlearning version winspell performs identical computation add back removed aspects winspell one time understand much contributes eliminating performance difference equivalent bayesian essence full winspell system experiment proceeds series steps morph bayspell winspell bayspell full bayspell method includes dependency resolution interpolative smoothing simplified bayspell like bayspell without dependency resolution means matching features even highly interdependent ones used bayesian calculation strip bayspell way naive bayes would use mle likelihoods performance would poor unrepresentative bayspell would undermine experiment seeks investigate winspell improves bayspell pale imitation thereof simplified winspell minimalist winspell set emulate computation simplified bayspell 1layer architecture ie weighted majority layer uses full network sparse initialized bayesian weights explained momentarily learning ie update bayesian weights bayesian weights simply log simplified bayspells likelihoods plus constant make nonnegative required winnow occasionally likelihood 00 case smooth loglikelihood gamma1 large negative constant used gamma500 addition add pseudofeature winnows representation active every example corresponds prior weight feature log prior 1layer winspell like simplified winspell adds learning lets us see whether winnows multiplicative update rule able improve bayesian feature weights ran learning 5 cycles training set 2layer winspell like 1layer winspell adds weightedmajority voting layer architecture replaces full network 2layer winspell sparse network yields complete winspell algorithm although performance affected fact started bayesian uniform weights table 3 ablation study training 80 brown testing 20 algorithms run unpruned condition bar graphs show differences adjacent columns shading indicating significant differences using mcnemar test 005 level confusion set bayspell simplified 1layer 2layer bayesian bayspell winspell winspell winspell accept except 920 920 940 900 960 affect effect 980 959 980 980 1000 among 780 796 774 909 892 amount number 805 780 846 886 854 begin 952 884 966 986 993 cite sight site 735 735 794 765 882 country county 919 806 919 935 968 fewer less 920 947 933 960 973 983 979 986 991 995 959 945 959 984 978 lead led 857 918 878 878 939 maybe may 958 969 958 990 990 passed past 905 932 919 878 932 peace piece 920 840 880 840 880 principal principle 853 853 824 853 912 quiet quite 894 970 924 909 939 raise rise 872 795 821 821 897 934 957 953 971 967 theyre 945 927 973 981 982 weather whether 984 967 984 1000 1000 youre 909 893 968 979 989 ablation study used 8020 breakdown brown previous section unpruned feature set results appear table 3 simplified winspell omitted table results identical simplified bayspell primary finding three measured aspects winspell contribute positively improvement bayspell ranking strongest weakest benefit 1 update rule 2 weightedmajority layer 3 sparse networks large benefit afforded update rule indicates winnow able improve considerably bayesian weights likely reason bayesian weights already optimal bayesian assumptions conditional feature independence adequate data estimating likelihoods hold fully practice winnow update rule surmount difficulties tuning likelihoods via feedback fit whatever situation holds imperfect world feedback obtained training set used set bayesian likelihoods incidentally interesting note use sparse network improves accuracy fairly consistently across confusion sets reason improves accuracy omitting links features never cooccurred given target word training effectively sets weight features 00 apparently better accuracy setting weight log bayesian likelihood case smoothed version 00 mle probability second observation concerns performance winspell starting bayesian weights overall score 972 compared 964 starting uniform weights see table 2 suggests performance winnow improved moving hybrid approach bayes used initialize network weights hybrid approach also improvement bayes present experiment pure bayesian approach scored 931 whereas updates performed bayesian weights score increased 951 final observation experiment intended primarily ablation study winspell also serves miniablation study bayspell difference bayspell simplified bayspell columns measures contribution dependency resolution turns almost negligible first glance seems surprising considering level redundancy unpruned set features used instance features include collocation treaty also include collocations det treaty noun sing nevertheless two reasons dependency resolution little benefit first features generated systematically feature extractor thus tend overcount evidence equally words second naive bayes known less sensitive conditional independence assumption ask predict probable class opposed asking predict exact probabilities classes duda hart 1973 domingos pazzani 1997 contribution interpolative smoothing enhancement bayspell naive bayes addressed table 3 however investigated briefly comparing performance bayspell interpolative smoothing performance mle likelihoods naive method well number alternative smoothing methods table 4 gives overall scores overall score bayspell interpolative smoothing 938 dropped 858 mle likelihoods also lower alternative smoothing methods tried shows dependency resolution improve bayspell much naive bayes interpolative smoothing sizable benefit 55 acrosscorpus performance preceding experiments assumed training set representative test set contextsensitive spelling correction however assumption may overly strong word usage patterns vary widely one author another even one document another instance algorithm may trained one corpus discriminate desert dessert tested article persian gulf war unable detect misspelling desert operation dessert storm check whether fact problem tested acrosscorpus performance algorithms trained 80 brown tested randomlychosen 40 sentences table 4 overall score bayspell using different smoothing methods last method interpolative smoothing one presented training 80 brown testing 20 using mle likelihoods broke ties choosing word largest prior ties arose words probability 00 katz smoothing used absolute discounting ney et al 1994 goodturing discounting resulted invalid discounts task kneserney smoothing used absolute discounting backoff distribution based marginal constraint interpolation fixed katz kneserney set necessary parameters separately word w using deleted estimation smoothing method reference overall mle likelihoods 858 interpolation fixed ney et al 1994 898 laplacem kohavi et al 1997 909 kohavi et al 1997 910 katz smoothing katz 1987 916 kneserney smoothing kneser ney 1995 934 interpolative smoothing section 3 938 wsj results appear table 5 algorithms found degrade significantly first glance magnitude degradation seems small 938 912 overall score bayspell 964 952 however viewed increase error rate actually fairly serious bayspell error rate goes 62 88 42 increase winspell 36 48 33 increase section present strategy dealing problem unfamiliar test sets evaluate effectiveness used winspell bayspell strategy based observation test document though im perfect still provides valuable source information word usages returning desert storm example suppose system asked correct article containing 17 instances phrase operation desert storm 1 instance erroneous phrase operation dessert storm treat test corpus training document start running feature extractor generate among others collocation 3 operation storm algorithm whether bayspell winspell able learn training test qua training corpus feature 3 typically cooccurs desert thus evidence favor word algorithm use feature fix one erroneous spelling phrase test set important recognize system cheating looking test set would cheating given answer key along test set system really enforcing consistency across test set detect sporadic errors systematic ones writing operation dessert storm every time however possible pick least systematic errors also regular supervised learning training set leads strategy call supunsup combining supervised learning training set unsupervised learning noisy test set table 5 within versus acrosscorpus performance bayspell winspell training 80 brown cases testing withincorpus case 20 brown acrosscorpus case 40 wsj algorithms run unpruned condition bar graphs show differences adjacent columns shading indicating significant differences using test difference two proportions 005 level raggedended bars indicate difference 15 percentage points confusion set test cases test cases bayspell winspell within across within across within across accept except 50 affect effect among 186 256 780 793 860 871 amount number 123 167 805 695 862 737 begin 146 174 952 891 979 989 cite sight site 34 country county 62 71 919 944 952 958 fewer less 75 148 920 946 973 973 1225 328 983 979 979 925 366 1277 959 955 933 959 lead led maybe may 96 67 958 925 918 899 passed past 74 148 905 959 959 980 peace piece 50 19 920 789 880 737 principal principle 34 quiet quite 66 20 894 650 939 750 raise rise 39 118 872 720 897 822 514 637 934 965 957 984 theyre 850 748 945 917 985 981 weather whether 61 95 984 947 1000 968 youre 187 74 909 851 973 959 learning training set supervised benevolent teacher ensures spellings correct establish simply assumption learning test set unsupervised teacher tells system whether spellings observes right wrong ran winspell bayspell supunsup see effect acrosscorpus performance first needed test corpus containing errors generated one corrupting correct corpus varied amount corruption 0 20 p corruption means altered randomlychosen p occurrences confusion set different word confusion set supunsup strategy calls training training corpus corrupted test corpus testing uncorrupted test corpus purposes experiment however split test corpus two parts avoid confusion training testing data trained 80 brown plus corrupted version 60 wsj tested uncorrupted version 40 wsj results 5 level corruption shown table 6 level corruption corresponds typical typing error rates 7 table compares across table 6 acrosscorpus performance bayspell winspell using supunsup strategy performance compared supervised learning training supunsup case 80 brown plus 60 wsj 5 corrupted supervised case 80 brown testing cases 40 wsj algorithms run unpruned condition bar graphs show differences adjacent columns shading indicating significant differences using mcnemar test 005 level raggedended bars indicate difference 15 percentage points confusion set test cases bayspell winspell supunsup sup supunsup accept except affect effect 66 879 909 955 939 among 256 793 812 871 906 amount number 167 695 784 737 874 begin 174 891 943 989 994 cite sight site country county 71 944 958 958 972 fewer less 148 946 932 959 980 328 979 985 985 991 1277 955 956 973 978 lead led 69 797 754 899 884 maybe may 67 925 910 925 970 passed past 148 959 966 980 980 peace piece 19 789 842 737 895 principal principle quiet quite 20 650 750 750 900 raise rise 118 720 873 822 898 637 965 962 984 983 theyre 748 917 908 981 985 weather whether 95 947 958 968 968 youre 74 851 878 959 973 corpus performance algorithm without additional boost unsupervised learning part test corpus bayspell winspell benefit unsupervised learning amount difference winspell suffered considerably less bayspell moving within acrosscorpus condition result winspell unlike bayspell actually able recover withincorpus performance level using supunsup strategy acrosscorpus condition borne mind results table 6 depend two factors first size test set larger test set information provide unsupervised learning second factor percentage corruption test set figure 2 shows performance function percentage corruption representative confusion set famount numberg one would expect improvement unsupervised learning tends decrease percentage corruption increases bayspells performance famount numberg 20 corruption almost enough negate benefit unsupervised learning winspell sup bayspell supunsup bayspell sup percentage corruption performance15595857565 figure 2 acrosscorpus performance bayspell dotted lines winspell solid lines supunsup strategy supervised learning curves show performance function percentage corruption test set training supunsup case 80 brown plus 60 wsj corrupted supervisedonly case 80 brown testing cases 40 wsj algorithms run confusion set unpruned condition 6 conclusion theoretical analyses winnow family algorithms predicted excellent ability deal large numbers features adapt new trends seen training properties remained largely undemonstrated work reported presented architecture based winnow weighted majority applied realworld task contextsensitive spelling correction potentially huge number features 10000 experiments showed algorithm winspell performs significantly better methods tested task comparable feature set comparing winspell bayspell bayesian statisticsbased algorithm representing state art task found winspells mistakedriven update rule use weightedmajority voting sparse architecture contributed significantly superior performance found exhibit two striking advantages bayesian ap proach first winspell substantially accurate bayspell running full unpruned feature sets outscoring bayspell 20 21 confusion sets achieving overall score 96 second winspell better bayspell adapting unfamiliar test corpus using strategy presented 3that combines supervised learning training set unsupervised learning test set work represents application techniques developed within theoretical learning community recent years touches upon important issues still active research first demonstrates ability winnowbased algorithm successfully utilize strategy expanding space features order simplify functional form discriminator done generating collocations patterns words partofspeech tags use strategy winnow shares much philosophy none technical underpinnings support vector machines cortes vapnik 1995 second twolayer architecture used related various voting boosting techniques studied recent years learning community freund schapire 1995 breiman 1994 littlestone warmuth 1994 goal learn combine simple learners way improves overall performance system focus work reported learning online fashion many issues still investigate order develop complete understanding use multiplicative update algorithms realworld tasks one important issues work raises need understand improve ability algorithms adapt unfamiliar test sets clearly crucial issue algorithms used real systems related issue size comprehensibility output representation mangu brill 1997 using similar set features one used demonstrate massive feature pruning lead highly compact classifiers surprisingly little loss accu racy clear tension however achieving compact representation retaining ability adapt unfamiliar test sets analysis tradeoff investigation winnowbased approach presented paper developed part research program trying understand networks simple slow neuronlike elements encode large body knowledge perform wide range interesting inferences almost instantaneously investigate question context learning knowledge representations support language understanding tasks light encouraging results presented contextsensitive spelling correction well recent results dagan et al 1997 reddy tadepalli 1997 roth zelenko 1998 extending approach tasks acknowledgments would like thank neal lesh grace ngai stan chen reviewers editors helpful comments paper second authors research supported feldman foundation grant israeli ministry science arts done partly harvard university supported nsf grant ccr9200884 darpa contract afosrf496292j0466 notes 1 tested successfully 40000 features results reported use 11000 2 word sentence tagged set possible partofspeech tags obtained dictionary tag match word tag must member words tag set 3 maximumlikelihood estimate p f jw number occurrences f presence w divided number occurrences w 4 purpose experimental studies presented update knowledge representation testing done provide fair comparison bayesian method batch approach 5 interfere subsequent updating weights conceptually treat nonconnection link weight 00 remain 00 multiplicative update 6 exact form decreasing function unimportant interpolate quadratically 10 067 decreasing function number examples 7 mays et al 1991 example consider error rates 001 10 task r learning boolean functions infinite attribute space bagging predictors empirical study smoothing techniques language modeling approximate statistical tests comparing supervised classification learning algorithms optimality simple bayesian classifier zeroone loss pattern classification scene analysis statistical methods rates proportions random house unabridged dictionary decisiontheoretic generalization online learning application boosting method disambiguating word senses large corpus bayesian hybrid method contextsensitive spelling correction combining trigrambased featurebased methods contextsensitive spelling correction tracking best expert concept learning problem small disjuncts estimation probabilities sparse data language model component speech recognizer exponentiated gradient versus gradient descent linear predictors improved backingoff mgram language modeling acoustics improving simple bayes techniques automatically correcting words text computational analysis presentday american english brown university press learning quickly irrelevant attributes abound new linearthreshold algorithm redundant noisy attributes comparing several linearthreshold learning algorithms tasks involving superfluous attributes weighted majority algorithm automatic rule acquisition spelling correction building large annotated corpus english penn treebank context based spelling correction structuring probabilistic dependences stochastic language modelling integrating multiple knowledge sources disambiguate word sense exemplarbased approach learning application differential grammars active learning committees text categorization learning resolve natural language ambiguities unified approach national conference artificial intelligence part speech tagging using network linear separators colingacl 98 circuits mind decision lists lexical ambiguity resolution application accent restoration spanish french tr ctr paisarn charoenpornsawat virach sornlertlamvanich thatsanee charoenporn improving translation quality rulebased machine translation coling02 machine translation asia p16 september 01 2002 derrick higgins use error tags artfls encyclopdie good error identification lead good error correction proceedings workshop student research p3034 april 29may brill robert c moore improved error model noisy channel spelling correction proceedings 38th annual meeting association computational linguistics p286293 october 0306 2000 hong kong filip ginter jorma boberg jouni jrvinen tapio salakoski new techniques disambiguation natural language application biological text journal machine learning research 5 p605621 1212004 michele banko eric brill mitigating paucityofdata problem exploring effect training corpus size classifier performance natural language processing proceedings first international conference human language technology research p15 march 1821 2001 san diego patternbased disambiguation natural language processing proceedings 2000 joint sigdat conference empirical methods natural language processing large corpora held conjunction 38th annual meeting association computational linguistics p18 october 0708 2000 hong kong xin li dan roth learning question classifiers proceedings 19th international conference computational linguistics p17 august 24september 01 2002 taipei taiwan jianhua li xiaolong wang combining trigram automatic weight distribution chinese spelling error correction journal computer science technology v17 n6 p915923 november 2002 yair evenzohar dan roth classification approach word prediction proceedings first conference north american chapter association computational linguistics p124131 april 29may rayid ghani rosie jones learning monolingual language model multilingual text database proceedings ninth international conference information knowledge management p187193 november 0611 2000 mclean virginia united states adam j grove dan roth linear concepts hidden variables machine learning v42 n12 p123141 januaryfebruary 2001 hema raghavan james allan matching inconsistently spelled names automatic speech recognizer output information retrieval proceedings conference human language technology empirical methods natural language processing p451458 october 0608 2005 vancouver british columbia canada yoshimasa tsuruoka junichi tsujii training naive bayes classifier via em algorithm class distribution constraint proceedings seventh conference natural language learning hltnaacl 2003 p127134 may 31 2003 edmonton canada cong li hang li word translation disambiguation using bilingual bootstrapping proceedings 40th annual meeting association computational linguistics july 0712 2002 philadelphia pennsylvania michele banko eric brill scaling large corpora natural language disambiguation proceedings 39th annual meeting association computational linguistics p2633 july 0611 2001 toulouse france hiyan alshawi online multiclass learning kway limited feedback application utterance classification machine learning v60 n13 p97115 september 2005 leslie g valiant neuroidal architecture cognitive computation journal acm jacm v47 n5 p854882 sept 2000 phrasebased statistical model sms text normalization proceedings colingacl main conference poster sessions p3340 july 1718 2006 sydney australia rayid ghani rosie jones dunja mladeni mining web create minority language corpora proceedings tenth international conference information knowledge management october 0510 2001 atlanta georgia usa hang li cong li word translation disambiguation using bilingual bootstrapping computational linguistics v30 n1 p122 march 2004 rayid ghani rosie jones dunja mladenic building minority language corpora learning generate web search queries knowledge information systems v7 n1 p5683 january 2005 rocco servedio computational sample complexity attributeefficient learning proceedings thirtyfirst annual acm symposium theory computing p701710 may 0104 1999 atlanta georgia united states cucerzan david yarowsky augmented mixture models lexical disambiguation proceedings acl02 conference empirical methods natural language processing p3340 july 06 2002 adam j grove nick littlestone dale schuurmans general convergence results linear discriminant updates machine learning v43 n3 p173210 june 2001 adam r klivans rocco servedio toward attribute efficient learning decision lists parities journal machine learning research 7 p587602 1212006 mirella lapata frank keller webbased models natural language processing acm transactions speech language processing tslp v2 n1 p3es february 2005 dan roth minghsuan yang narendra ahuja learning recognize threedimensional objects neural computation v14 n5 p10711103 may 2002 dan roth learning natural language theory algorithmic approaches proceedings 2nd workshop learning language logic 4th conference computational natural language learning september 1314 2000 lisbon portugal graeme hirst alexander budanitsky correcting realword spelling errors restoring lexical cohesion natural language engineering v11 n1 p87111 march 2005 mike thelwall text characteristics english language university web sites research articles journal american society information science technology v56 n6 p609619 april 2005 russell greiner adam j grove dan roth learning costsensitive active classifiers artificial intelligence v139 n2 p137174 august 2002 julie weeds david weir cooccurrence retrieval flexible framework lexical distributional similarity computational linguistics v31 n4 p439475 december 2005 hans van halteren walter daelemans jakub zavrel improving accuracy word class tagging combination machine learning systems computational linguistics v27 n2 p199229 june 2001