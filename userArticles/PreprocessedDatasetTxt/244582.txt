strict time lower bound optimal schedules parallel prefix resource constraints abstractprefix computation basic operation core many important applications eg grand challenge problems circuit design digital signal processing graph optimizations computational geometry1 paper present new strict timeoptimal parallel schedules prefix computation resource constraints concurrentreadexclusivewrite crew parallel random access machine pram model prefix n elements p processors p independent n n pp 12 derive harmonic schedules achieve strict optimal time steps leftlceil 2left n1 right mathordleft vphantom 2left n1 right left p1 right right kernnulldelimiterspace left p1 right rightrceil also derive pipelined schedules better programspace efficiency harmonic schedule yet require small constant number steps optimal time achieved harmonic schedule harmonic schedules pipelined schedules simple easy implement prefix n elements p processors p independent n npp 12 harmonic schedules timeoptimal cases establish optimization method determining key parameters timeoptimal schedules based connections structure parallel prefix pascals triangle using derived parameters devise algorithm construct schedules restricted class values n p prove constructed schedules strictly timeoptimal also give strong empirical evidence algorithm constructs strict timeoptimal schedules cases npp b introduction given computation evaluates 0 ffi associative operation ffi prefix sum firstorder linear recurrence coefficients 1 special case prefix computation stated simple loop refer problem interchangeably recurrence prefix sums prefix computation since results prefix sum readily applied prefix computation associative operations prefix computation fundamental operation core many key applications grand challenge problems circuit design digital signal processing graph optimizations computational geometry1 8 15 addition also important tool loop parallelization traditional automatic loop parallelization techniques4 respect loopcarried dependences thus unable generate scalable parallel code loops containing loopcarried dependences understand parallelize loops loopcarried dependences beyond techniques essential understand simplest case loops loopcarried dependence namely prefix sums optimal schedules technique used derive paper could appliedwith extensions parallelizing many sequential algorithms containing loop carried dependences since practical applications amount resources ie functional units processors fixed priori independent problem size desirable devise scheme performs prefix computation optimal time given set resource constraints since interested extending technique handle general forms loops loopcarried true dependence also concerned properties optimal schedules clarity simplicity implementation extendibility prefix n elements p processors p independent n n pp 12 derive harmonic schedules achieve strict optimal time steps 1e also derive pipelined schedules better programspace efficiency harmonic schedule yet take small constant number steps optimal time achieved harmonic schedule harmonic schedules pipelined schedules simple thus easy implement prefix n elements p processors p independent n n pp 12 exist kind structure parallel prefix computation n pp 12 establish optimization method determining key parameters timeoptimal schedules based connections structures parallel prefix pascals triangle using parameters devise algorithm construct schedules restricted class cases terms n p derive strict timeoptimal schedules subject optimization condition cases give strong empirical evidence algorithm constructs strict timeoptimal schedules conjecture schedules n pp achieve strict optimal time paper organized follows section 2 presents assumptions definitions section 3 reviews related work section 4 presents harmonic schedule n pp shows strict time optimality harmonic schedule properties section 6 gives pipelined schedule section 7 establishes optimization method determining key parameters timeoptimal schedules n pp gives algorithm construct schedules n pp strict timeoptimal schedules within optimization restricted class cases terms n p provides strong empirical evidence algorithm constructs strict timeoptimal schedules cases results assume parallel random access machine pram concurrentreadconcurrentread crew model 15 analysis pram consists p autonomous processors executing synchronously access globally shared memory processor computes operation one time step unit schedules timeoptimal terms number parallel time steps results computed crew model multiple processors may read simultaneously memory location ie broadcasting data exclusive access required writing memory location facilitate study computational complexity comparison previous work area previous results pram crew assume memory access data transfer communication take zero time steps define terms frequently used paper schedule used perform computation 1 denote time ie number time steps run schedule machine p p unambiguous p refers number processors machine refer p interchangeably time time steps steps time compute sequentially denoted 1 speedup machine p processors uniprocessor schedule denoted simply unambiguous let p number operations executed computation using p processors define operation redundancy r since operations assumed take one time step number operations sequential program 1 defines run time finally define utilization u maximum number operations p processors perform p steps three types datadependences defined literature 11 purposes need concern true flow dependences defined follows operation uses result another operation said true flow dependent simply dependent unambiguous loopcarried dependence type true dependence loop operation iteration uses value produced previous iteration dependence operation represented directed arc dependence graph consists set nodes representing operations input values dependence arcs connecting binary dependence graph dependence graph operations take two operands operand may either input value value output operation dependence graph prefix computation scheduled using treeheight reduction thr 11 referred thr graph final values prefix sums values computed definition prefix sums final operations prefix sums operations assign final values redundant operations auxiliary intermediate operations operations compute auxiliary values effort speed final value computation redundant values auxiliary intermediate values refer auxiliary values computed redundant operations figure 1 shows dependence graphs sequential schedule parallel schedule prefix computation eight inputs illustrated two graphs nodes final operations top fringe nodes redundant operations inner ones identified square inside nodes given values ones without incoming arcs placed bottom note dependence graph sequential schedule prefix sums tree one parallel schedule define j particular ah0 prefix sum first j elements note distinguish algorithm schedule algorithm used produce schedulefor given prefix computation run pram execute actual prefix computation 3 related work results parallel prefix computation divided two categories resource constraints usually number processor p function problem size n resource constraints p independent n comprehensive survey parallel computing using prefix problem given 14 special forms prefix circuits previously known ofman early 1963 whose carry circuit18 form carry circuits later discussed ladner fischer 13 muraoka16 showed simplest linear recurrence computed log n n2 processors used name treeheight reduction technique kogge stones recursive doubling12 firstorder linear recurrences essentially equivalent treeheight reduction applied prefix problem chen kuck11 showed linear recurrences computed 1 log n p n2 ladner fischer13 found class circuits prefix size assuming enough processors available unlimited resources fich9 gave upper bounds lower bounds circuit size ie number gates operations prefix circuits n inputs depth log n dlog ne extra depth cases unbounded bounded fanout fich constructed circuits using complex recursive procedure ladner fischers unbounded fanout lower bounds prefix circuits input n upper bounds fanout bound f 2 also obtained finally lower bounds fanout two k2 1 bilgory gajski5 gave different algorithm generates suffixthe term used solutions minimum cost given length n depth initial value availability e fanout f cost defined minimum number operation nodes along pair corresponding input output lower bound cost therefore maximum number nodes path input corresponding output instead circuit size chose cost measurement fit better consideration silicon layout cole vishkin6 gave algorithm solves prefix sums problem olog n log log n using log log n log n processors provided ai represented olog n bits different problem address resource constraints wellknown algorithm described 3 computes prefix problem 2np log p time given p processors algorithm divides problem p partitions conquers local computation partitions one processor combines results algorithm takes 2npp steps optimal time snir 19 gave algorithm dynamically constructing parallel prefix circuits fixed number processors crew machines depth resulting circuit p processors 2np 1 o1 close strict timeoptimal depth given problem size n snirs algorithm works find right partition problem minimize depth resulting schedule must recompute partition time new n given overhead associated problem size n either compile time run time n known run time frequently overhead incurred finding right partition impact performance parallel prefix computation also studied erew pram model kruskal rudolph snir 10 shown prefixes n elements computed using p processors 2np steps egecioglu koc 7 studied tradeoffs parallel arithmetic steps required perform associative operations parallel routing steps required transfer operands one processor another prefix computation presented prefix algorithm using 2p routing steps 4 harmonic schedule a3 a4 a5 a6 a7 x3 x2 x3 sequential full thr i1 figure 1 sequential schedule vs full treeheight reduction thr prefix sums given prefix problem size n shown figure 1 number operations sequential schedule cannot reduced definition required outputs left drawing figure 1 gives computation tree sequential schedule prefix sums size 8 tree height ie dependence distance critical path tree sequential schedule equals n gamma 1 minimum number time steps compute problem sequentially way parallelize computation use associativity operator reduce tree height time drawing right figure 1 gives full treeheight reduction thr schedule prefix sums size 8 shown use associativity compute redundant values ahead final value computation compute multiple final values parallel thus trade multiple processors redundant operations speedup parallelism speed prefix schedule thought measured average number final values produced step given fixed number processors want introduce many redundant operations reduce average number results produced per step example fixed number processors full thr shown figure 1 slows computation farther away optimal problem size n increases order produce many final results possible given fixed number processors schedule redundant operations possible however one cannot reduce number redundant operations certain threshold would leave processors idle slow computation example removing redundant operations schedule would degenerate sequential schedule shown figure 1 one processor would stay idle therefore fastest parallel schedules fixed number processors would use minimum number redundant operations simultaneously achieving full processor utilization idea harmonic schedule remainder section describe detail derivation properties harmonic schedules shall prove next section harmonic schedule indeed timeoptimal example 41 figure 2 illustrates iteration harmonic schedule processors nodes top fringe figure represent final values prefix computed final operations inner nodes represent redundant values computed redundant operations bottom ends vertical lines represent given elements prefix sums first step iteration one final operation operations second step two final operations redundant operations kth step k final operations redundant operations step one final operation one fewer redundant operation immediate preceding step p processors full utilization p steps iterations ieloop body final redundant computations meet end iteration end iteration redundant values used computing final values another nice property number operations iteration sum final operations redundant operations sp hence called schedule harmonic schedule 2 numbers correspond steps loop body computed redundant operations top nodes final values computed final operations inner nodes output redundant values figure 2 iteration schedule h processors schedule 41 harmonic schedule let multiple sp cases discussed shortly harmonic schedule computes prefix sums n elements p processors denoted h decribed recall ahi ji represents partial sum elements ai throught aj refer section 2 translate schedule one using array gamma 1 straightforwardly shown figure 3 k f li gthe inner loop k harmonic schedule consists p iterations iteration inner loop executed single parallel step parallel step consists p operations shown inner loop body executed p processors parallel first group p gamma operations compute look ahead redundant values prefix values second group k operations compute final values refer inner loop period computation harmonic schedule also easily verify schedule h memory location written multiple operations simultaneously schedule 41 instantiated given number processors compilation negligible time note unlike 19 runtime scheduling overhead since full instantiation schedule depends p depend n array subscript calculation instantiated h schedule ie actual value p becomes simple reduced addition using strength reduction2 step parallel 7 processors 2 a28i 3 4 a28i 5 a28i 7 figure 3 instantiated schedule h processors illustration instantiate harmonic schedule processors figure 3 28 21 inner nodes height l figure 2 computed operations lth step instantiated schedule figure 3 note operations step execute parallel use array store redundant values distinguish redundant values final values given elements size array reduced sp gamma 1 since contents produced iteration used iteration thus reused iteration theorem 41 prefix sums size processors harmonic schedule computes given prefix sums computes final values yielding speedup p 12 processor utilization u p proof prove correctness harmonic schedule suffices show produces correst results first period sp elements plus starting element 0 since periods identical computation structures note period harmonic schedule corresponds iteration program template observe harmonic schedule computes prefix sums sp elements p parallel time steps follows schedule computes k th step use induction number parallel steps assume end k th step first sk prefix sums ah1 1i ah1 well available end p th step harmonic schedule computes number time steps taken schedule h compute msp final values iterations determined follows number iterations n elementsnumber time steps per iteration shown schedule h computes msp final values multiple p steps next show performance stated theorem 41 also true n pp subject ceiling effect theorem 42 extended harmonic schedule schedule h extended compute prefix sums size n sp time 1e yielding speedup proof previous theorem proved statement 1 suffice show statement true n sp n 6 msp 1 proof proceeds two cases case 1 sp last q elements called trailing elements final operations compute q trailing elements called trailing final operations redundant operations whose values used trailing final operations called trailing redundant operations without loss generality let us look example figure 413a1 m1 p7 new delayed ops step 3 74746755511811913 use delayable slots step 3 7 compute intermediate values use idle slots step 8 13 compute figure 4 amortizing scheme extended schedule h odd p mp 1th step want compute p final operations p gamma 12 redundant operations given redundant tree height 3 takes exactly 3 steps compute solution m1 p6 compute intermediate values use delayed operations slots idle slots delayed ops step 4 6 use idle slots step 7 11 compute new4667551081197108 figure 5 amortizing scheme extended schedule h even p amortize computation ie delay final operations steps use freed processor time slots compute redundant trees incurred q steps compute delayed operations idle processor slots steps mp 1 example figure 4 redundant tree used step 8 computed step 5 6 7 causes three final operations one step 5 7 delayed three idle slots step 8 used compute three delayed operations step 5 7 question whether feasible amortizing scheme delayed operations idle processor time slots ie whether always enough delayable operations match tailing redundant operations fashion note final operations iteration delayed operations depend hence enough delayable operations match trailing redundant operations similar fashion always find feasible amortizing scheme steps mp1 1p gamma 1 arguments true odd p 2 therefore extended schedule h computes prefix sums size n sp time processors case 2 p 2 even let n case 1 amortizing scheme shown figure 5 note even p heights trailing local trees turn bp subject number trailing elements odd p height local trees always p subject number trailing elements rest proof similar case 1 hence extended schedule h computes prefix sums size n sp time theorem 42 construct extended part schedule h follows unwind last iteration minimum number steps number nonredundantly scheduled elements number redundant trees schedule needs least 12 steps complete nonredundantly scheduled elements redundantly scheduled elements redundant tree redundant tree redundant tree figure redundant trees redundantly scheduled elements nonredundantly scheduled elements schedules unextended schedule h use amortizing scheme delayable operations last iteration idle processor slots q steps thus obtain generic extended schedule h epilogues handles length trailing steps 1 q p note although precise q depends n generic extended schedule h precomputed installed library routine q known run time corresponding epilogue chosen run 5 optimality harmonic schedule section show schedule h computes strict optimal time steps prefix sums size n sp processors lemma 51 presents lower bound time parallel schedule p processors lemma 53 shows schedules r r h p h given number processors p theorem 51 proves optimality schedule h consider elements given prefix problem stored array call element array redundantly scheduled element used redundant operation otherwise call nonredundantly scheduled element nonredundantly scheduled element participate redundant operations directly used final operation ie direct arc nonredundantly scheduled element final operation figure 6 elements nonredundantly scheduled elements redundantly scheduled set redundant operations said form redundant computation cluster redundant cluster short iff dependences operations form connected graph removing final operations dependence graph use results produced redundant operations without considering directedness resulting graph redundant cluster provides redundant lookahead values used final operations parallel step schedule shown figure 6 note dependence graph redundant cluster schedule restricted tree nonleaf nodes redundant cluster redundant values produced redundant operations cluster leaves redundant cluster redundantly scheduled elements cluster redundant cluster covers redundantly scheduled elements leaves ie uses compute redundant values lemma 51 schedule computes prefix sums number nonredundantly scheduled elements number redundant clusters proof single parallel step one nonredundantly scheduled element alone redundant values one redundant cluster alone used compute final values particular schedule prefix sums redundant clusters takes least completelemma 52 parallel schedule prefix sums size n number redundantly scheduled elements smaller equal sum number redundant operations number redundant clusters ie number redundantly scheduled elements r anumber redundant clusters proof redundant cluster c r c redundant operations uses h redundantly scheduled elements array least c number redundant clusters schedule note also redundant clusters disconnected ie dependence pair redundant clusters thus number redundantly scheduled elements number redundantly scheduled elements redundant cluster c j r anumber redundant clusters alemma 53 prefix sums size multiple p schedule r r h tp schedule compute given prefix fewer steps schedule h proof lemma 52 number nonredundantly scheduled elements array sizegammanumber redundantly scheduled elements anumber redundant clusters lemma 51 number redundant clusters number nonredundantly scheduled elements gamma 1 number redundant clusters anumber redundant clusters a1 r theorem 42 p r two statements given r r h r t109100918273645546101938881746760534673676155494337312519615651464136312621162624222018164945413733292521171393532292623221916131074336298577696153453737221141412108642242220181614121086428642t time steps p number processors n array size figure 7 maximum size n prefix sums done time using p processors 2 2 theorem 51 optimality schedule h given p processors prefix problem size schedule must time proof number final operations schedule computing prefix sums size n n gamma 1 hence suffices show 1 schedule redundant operations h cannot compute prefix size n fewer steps h 2 schedule redundant operations h cannot compute prefix size n fewer steps h statement 1 true schedule h maximum possible processor utilization p steps schedule redundant operations h steps produce fewer final results h regardless redundant operations arranged assuming fully utilizes p processors statement 2 holds lemma 53 2 corollary 51 optimality extended schedule h given p processors prefix problem size strict optimal time p proof theorem 51 proves statement multiple sp multiple sp theorem 42 states schedule h achieves time p 1e amortization scheme optimality time shown similar way proof theorem 51 2 corollary 52 ratio timeoptimal schedule prefix sums ratio final redundant operations p multiple p proof follows theorem 42 theorem 51 2 corollary 52 used deriving simpler concise programspace efficient optimal schedules illustrate figure 7 show maximum size n prefix sums computed time processors number diagonal equal 1 sp starting number diagonal p larger preceding number diagonal implies optimalitygiven one step p final values computed row odd p starting increases p12 increases one row even p starting increases one n increases turn bp 12e n increases average p 6 pipelined schedule schedule h strict timeoptimal prefix sums size n sp schedule p iterations inner loop body wondered whether simpler shorter schedules comparable performance exist corollary 52 ratio final redundant operations optimal schedule p h achieves ratio every iteration p steps schedule achieve ratio fewer steps p smaller loop body h still optimal applying idea software pipelining indeed derive simpler concise programspace efficient schedule h call pipelined schedule schedule p schedule p loop body single parallel step p final p gamma 12 redundant operations yielding speedup schedule h except small constant number steps startup odd p achieve ratio p single parallel step even p amortizing needed achieve ratio remainder section give complete schedule p odd number processors p schedule p even number processors p derived similarly prefix sums size n computed pipelined schedule odd number processors p first p steps schedule set actual iterations iteration step computes final valuesusing preceding final values previously computed redundant values redundant values future iterations array subscripts instantiated schedule p simplified compile time p given note operations loop body single step thus execute parallel schedule 61 pipelined schedulep step parallel p processors forin iteration p 1 final values array computed first p 1 expressions p values computed last p comparison schedule h schedule p choose proper epilogue either compile run time also schedule p may extended handle loop multiple statements easily schedule h 13 initial steps 4 iteration steps figure 8 pipelined schedule 7 5 iteration steps 14 initial steps figure 9 pipelined schedule example 61 let us instantiate schedule p 7 schedule given illustrated figure 8 operation step done parallel one processors step 3 step parallel p processors figure 9 illustrates schedule derived general schedule obtained even number processors p amortized schedule odd iterations computes bp12c final dp gamma 12e redundant operations even iterations computes dp final bp gamma 12c redundant operations theorem 61 prefix sums size n computed schedule p p 2 processors time d2np schedule p speedup utilization u infinity proof proof straightforward 2 7 formulation strict timeoptimal schedules derived strict timeoptimal schedules prefix sums size n sp schedules n sp useful parallelizing loops containing loopcarried dependences schedules n sp equally useful designing highspeed circuits13 5 section present new formulation finding strict timeoptimal schedules terms combinatorial optimization unifies single framework optimal schedules prefix problems n sp sp thus new approach yields general scheme optimal schedules prefix sums n p establish connections structures prefix computation pascals triangle thus formulate problem finding strict timeoptimal schedules solving system inequalities based algorithm constructing optimal schedules derived first give intuition solution characterize different costs final results full thr graph finally find minimum cost producing n final results p processors particular form foundation algorithm presented next section builds timeoptimal schedules prefix problem size n p processors next section strict time lower bound n sp1 schedule given n sp p 1 finding optimal p equivalent finding maximum size n prefix sums computed p steps shall find optimal schedules n sp following line thinking lemma 51 optimal schedule steps clusters thus processors optimal schedule must compute maximum number final values clusters way make redundant cluster produce within maximum allowable height many redundant results possible used compute final results want make redundant cluster cover many elements array possible within maximum allowable height maximum allowable height redundant cluster determined relative position cluster among redundant clusters nonredundantly scheduled elements graph redundant cluster exceeds maximum allowable height critical path computation lengthened neither heights redundant clusters lowered would make redundant clusters cover fewer original elements ie produce fewer final results could otherwise computation tree height k maximum number original elements covered 2 k enough processors redundant clusters schedule grow full binary dependence graphs schedule saturated ie final results computed processors steps basic idea try achieve dual goals maximum utilization processors minimum number redundant operations however n sp harmonic schedule section 4 longer applicable intuitively enough prefix elements make single periodthe minimum number prefix elements make single period spthe period length plus starting element hence given regular pattern computation exists n sp difficulty lies determining many redundant operations used redundant clusters organized dual goals accomplished definition 71 node n 1 binary dependence graph said right depend node n 2 iff n 1 uses second operand value produced n 2 given value represented n 2 node n 1 said right depend figure 10 rightdependences computation tree node n k1 distance k iff exist distinct nodes n right depends k concept left dependence left dependence distance k defined similarly notation 71 cost vector let top fringe operations computation graph height cost vector j rightdependence distance operation aj value j cost vector equal number operations jth partial sum graph right depends thus sum j 0 j equal number operations whole graph shown figure 10 cost vectors graphs left right 0 respectively total numbers operations graphs equal 0 respectively shall see soon dependence graph constructed cost vector determined theorem 71 corollary characterize different costs final values full thr graph show relevant properties help us minimize total cost problem size number processors given theorem 71 full graph height k number nodes top fringe rightdependence distance j proof prove theorem induction height k full graph base graph height one node top fringe rightdependence distance show one case facilitate understanding graph height two nodes top fringe rightdependence distance 0 1 respectively illustrated figure 11 left leaf graph height 1 also node top fringe rightdependence distance 0 root node top fringe rightdependence distance 1 thus induction assumption assume theorem holds show theorem holds 1 intuition shown figure 11 number nodes rightdependence distance j top fringe full graph height k1 sum number height k height k height 0 height 1 full graph full graph full graph full graph height k1 full graph figure 11 number nodes top fringe graph height k rightdependence distance topfringe operation nodes first subgraph height k rightdependence distance j number topfringe operation nodes second subgraph height k rightdependence distance since new layer operation nodes top second subgraph height k preceding argument induction assumption corollary 71 1 total number operation nodes top fringe full graph height k k 0 2 given n full graphs height 0 sum numbers operation nodes top fringes rightdependence distance j 3 total number operation nodes top fringes n full graphs height 0 equal 2 n gamma 1 proof claim 1 true total number operations top fringe full graph height k equal sum numbers operations top fringe rightdependence distance j ie sum g kj k theorem 71 note truth claim 1 also found observing number array elements covered full graph proof claim 2 requirement claim theorem 71 43 44 g g g g g 1 3 4 figure 12 correspondence theorem 61 corollary 61 pascals triangle 3 follows claim 1 summing g k figure 12 shows correspondence theorem 71 corollary pascals triangle corollary 72 cost vector full thr graph let 1 idimensional vector elements equal 1 let dimc dimension vector c cost vector c k full graph height k determined recursively follows proof proof straightforward 2 figure 10 cost vector full graph height 3 given following recursive sequence follows try find minimum cost dependence graph prefix size n p processors theorem 72 gives system inequalities determining minimum time steps required compute prefix problems using p processors theorem 72 lower bound time required compute prefix problem size n processors determined using following system inequalities number topfringe operations ie number final results rightdependence distance k equal l k maximum rightdependence distance used schedule proof theorem 71 delta number operations ie number final results top fringe graph rightdependence distance iwhich seen cost full graph height number final operations rightdependence distance k full graph height goal choose use schedule final operations minimum sum cost ie right dependence distances simultaneously maximally utilize p processors solving first inequality k means finding minimum graph height minimum rightdependence distance k given prefix computed time final operations rightdependence distance k since number processors p involved first inequality may multiple pairs solutions k ie pair k gives minimum number operations used compute prefix respect particular number processors given number processors p unique pair k determined computing results given prefix minimum number operations second inequality used choose k respect p ie pair k choose enable us construct schedules p processors maximally utilized second inequality equals number operations required redundant clusters height 0 means sufficiently many processor slots steps p processors compute operations required clusters cover n prefix elements pt gamma means minimum number steps produce final results p processors thus second inequality states lower bound time gives sufficiently many processor slots compute minimum number operations required produce prefix results p processors 2 solutions inequalities implies widths schedules ensure timeoptimality goal achieve minimum cost number operations maximum resource utilization simultaneously width schedule prefix computation paper defined maximum number operations computed single step schedule many cases width schedule must larger number resources p order ensure existence legal schedule given resources shown example 81 restricting width schedule equal number processors p could way lead findings strict timeoptimal schedules schedules n sp theorem 72 suggest construct schedules starting full thr graph 2 minimum number smaller n cutting expensive columns graph optimization produces precisely harmonic schedule proven strictly timeoptimal example 83 illustrate derivation n sp prove optimization general constructs strict timeoptimal schedules need prove 1 legal schedules devised using optimization n p n sp 2 schedules strictly timeoptimal show next section class n p n sp optimization enables us derive legal schedules optimality schedules follows theorem 72 subject condition mentioned schedules derived full thr graph cutting expensive columns devising schedules n sp based theorem 72 construct schedule computing prefix n elements p processors using algorithm 81 given algorithm 81 input prefix computation n elements number processors p output dependence graph making parallel schedule performs prefix computation construct schedule ppcn p f 1 find min time min pn p 2 find min cost vectort k l 3 construct graphc tn gwe describe three procedures algorithm 81 remainder section complete section examples built dependence graph parallel schedule given prefix completed allocating operations graph p processors shall discuss processor allocation end section procedure 81 find min time input prefix size n number processors p output procedure find min time min pn p solves inequalities theorem 72 minimum time minimum number processors p 0 required maximum rightdependence distance k resulting schedule number final results l rightdependence distance k given p greater derived p 0 procedure completes resulting schedule constructed using since remaining processors would use achieving minimum time given p smaller derived p 0 procedure find min time pn p continues search minimum time n p procedure sped starting better k one outer loops procedures optimization omitted favor readability find min timen p f find min time min pn p find min time min pn p f 1for 2 3 4 5 ffind 2 6 p 7 0 pt gamma 8 p else return g g g find min time pn p f 1 2 3 0 return k lg gprocedure 82 construct cost vector cutting scheme procedure takes input solutions l procedure 81 generates output cost vector computation graph evaluation given prefix constructed dimension resulting vector equal size n given prefix cost vector full graph derived using technique given corollary 72 construct cost vectort k l f 1 derive cost vector c full thr graph height 2 remove c elements greater k 3 remove c l equal k follows group means group consecutive elements equal k c 31 point rightmost group let 32 remove last element group pointed shift pointer group left point rightmost group leftmost group operated ie pointer moves clockwise circular fashion 4 return resulting cost vector c gnote schemes choosing c l elements equal k chose use simple scheme stated step 3 facilitate understanding cutting schemes discussed 20 procedure 83 construct graph procedure establishes dependence arcs nodes dependence graph placed matrix tn described next matrix encodes dependence graph evaluates given prefix problem size n p processors store operations graph theta n sparse matrix tn called construction matrix graph organized using cost vector c follows elements given prefix problem stored 0th row tn j operations jth partial sum graph right depends stored jth column operation rightdependence distance placed ith four fields associated operation oi j tn left source right source right dep distance number depended fields oi jleft source oi jright source point left right source nodes respectively field oi jright dep distance stores rightdependence distance node field oi jnumber depended number nodes node oi j depends including oi j construct 1 2 oi jright dep distance 1 oi jright sourceright dep distance 3 column left sourcej gamma 1oi jright sourcenumber depended 4 row left source column left source gamma 5 oi jleft source orow left source column left source 6 jnumber depended sourcenumber depended oi jright sourcenumber depended gexample 81 find minimum time optimal schedule prefix size processors procedure find min time min p determines 22 results computed processors steps ie dependence graph height results rightdependence distance procedure construct cost vector generates cost vector cost vector procedure construct graph constructed schedule shown figure 13 theorem 72 fact valid schedule constructed using cost vector schedule figure 13 achieves strict optimal time prefix size 23 10 processorsexample 82 find minimum time optimal schedule prefix size processors example shows procedure produces full treeheightreduction schedule n power 2 given enough processors procedure find min time min p determines results computed processors steps ie dependence graph height rightdependence distance obtained full thr schedule height procedure construct cost vector generates cost vector c procedure construct graph constructs schedule shown figure 14 well known strict timeoptimal schedule prefix 2 processors cost vector32 22 figure 13 optimal schedule prefix input size 23 p processors determined procedure cost vector figure 14 procedure generates full treeheight reduction schedule find minimum time optimal schedule prefix size processors example illustrates procedure produces precisely harmonic schedule procedure find min time min p decides minimum time required compute 28 prefix sums 5 steps minimum number processors accomplish p find min time p therefore continues finds minimum time required compute 28 prefix sums processors 7 steps 28 results computed processors steps ie dependence graph height results rightdependence distance 2 procedure construct cost vector generates cost vector construct graph creates precisely computation graph harmonic schedule one shown figure 2 shown achieve strict optimal time section 5 2 algorithm 81 constructs dependence graph computing prefix n elements p processors whether dependence graph would make valid schedule depends solely existence legal processor allocation onto computation graph arbitrary n p n sp turns difficult prove existence since one would need characterize certain properties dependence graphs n p n sp able characterize class dependence graphs show existence legal processor allocations ie existence class strict timeoptimal schedules theorem 72 time implied dependence graphs lower bound theorem 71 columns operation nodes show allocations exist ie strict timeoptimal schedules n p found following lemma says number processor slots ie number operation nodes dependence graph always multiple n lemma 81 proof observe thus delta integer 2 theorem 81 prefixes n elements computed steps p processors class n p strict optimal time steps computing prefix n elements p processors proof consider set columns exactly 1s number 1s row set equal consider set columns k 1s number 1s row set corresponds number operation nodes time step equal total number processor slots required compute prefixes lemma 81 thus prefixes n elements computed steps p processors since dependence graphs n p characterized theorem also constructed using algorithm 81 theorem 72 algorithm 81 based strict optimal time prefix size n p processors 2 dependence graphs class n p theorem 81 nice structure exactly p operation nodes time step thus p minimum number processors required carry computation steps general values n p n sp dependence graphs seem much less apparent clean characterization class n p theorem 81 thus existence legal processor allocations dependence graphs n p n sp considered theorem 81 remains open strict timeoptimality theorem 81 subject optimization condition implied theorem 72 schedules n sp constructed full thr graph cutting expensive columns however conjecture optimization constructs strictly timpoptimal schedules n p n sp subject optimization condition supported facts 1 algorithm 81 constructs precisely harmonic schedules proven strictly timeoptimal 2 prefix problems n sp1 constructed strict timeoptimal schedules using algorithm 81 9 conclusion presented strictly timeoptimal schedules parallel prefix computation resource constraints divided parallel schedules prefix computation size n two areas according number processors p schedules n sp n sp1 2 prefix schedules n sp useful parallelizing loops containing loopcarried dependences schedules n sp equally useful designing highspeed circuits13 5 prefix n elements p processors p independent n n sp pp 12 derived harmonic schedules showed harmonic schedules achieve strict optimal time steps d2n gamma harmonic schedules prefix computation also constructed using algorithm 81 1e also derived pipelined schedules optimal schedules time take constant overhead dp gamma 12e time steps strict optimal time harmonic schedules pipelined schedules expressed program templates parameterized number processors p ie generated negligible time compile time p known harmonic schedules pipelined schedules exhibit simple nice patterns loop structure makes easy implement main advantage pipelined schedules harmonic schedules former single parallel step loop body ie small program size prefix n elements p processors p independent n n sp harmonic schedules timeoptimal n large enough accommodate repeating pattern n sp cases established optimization method theorem 72 determining key parameters timeoptimal schedules based connections theorem 71 corallaries dependence graphs parallel prefix pascals triangle found using derived parameters devised algorithm 81 construct schedules restricted class cases terms n p proved constructed schedules strictly timeoptimal subject optimization condition discussed end section 7 also give strong empirical evidence conjecture algorithm 81 constructs strict timeoptimal schedules case fact optimization methods also applied derive strict timeoptimal schedules shown example 83 existence legal schedules n p n sp optimization strict timeoptimality schedules n p n sp remain open except open end concluded search strict optimal time schedules achieve strict optimal time parallel prefix computation resource constraints crew pram model acknowledgements authors wish thank anonymous reviewers valuable comments earlier draft paper r computational fluid dynamics parallel processors compilersprinciples techniques tools chapter 4 automatic program parallelization heuristic suffix solutions faster optimal parallel prefix sums list ranking parallel prefix computation processors chapter new bounds parallel prefix circuits power parallel prefix structure computers computations parallel algorithm efficient solution general class recurrence equations parallel prefix computation parallel computing using prefix problem introduction parallel algorithms architectures arraystreeshypercubes parallelism exposure exploitation programs optimal schedules parallel prefix computation bounded resources algorithmic complexity discrete functions depthsize tradeoffs parallel prefix computation parallelization programs containing loopcarried dependences resource constraints tr ctr sukanya suranauwarat hideo taniguchi design implementation initial evaluation advanced knowledgebased process scheduler acm sigops operating systems review v35 n4 p6181 october 2001 yenchun lin chaocheng shih new class depthsize optimal parallel prefix circuits journal supercomputing v14 n1 p3952 july 1999 armita peymandoust giovanni de micheli symbolic algebra timing driven dataflow synthesis proceedings 2001 ieeeacm international conference computeraided design november 0408 2001 san jose california yenchun lin yaohsien hsu chunkeng liu constructing h4 fast depthsize optimal parallel prefix circuit journal supercomputing v24 n3 p279304 march yenchun lin jiannan chen z4 new depthsize optimal parallel prefix circuit small depth neural parallel scientific computations v11 n3 p221236 september yenchun lin junwei hsiao new approach constructing optimal parallel prefix circuits small depth journal parallel distributed computing v64 n1 p97107 january 2004 jin hwan park h k dai reconfigurable hardware solution parallel prefix computation journal supercomputing v43 n1 p4358 january 2008 yenchun lin chinyu su faster optimal parallel prefix circuits new algorithmic construction journal parallel distributed computing v65 n12 p15851595 december 2005