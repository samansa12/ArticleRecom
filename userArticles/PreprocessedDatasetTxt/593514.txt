scaling kernelbased systems large data sets form support vector machine gaussian processes kernelbased systems currently popular approaches supervised learning unfortunately computational load training kernelbased systems increases drastically size training data set systems ideal candidates applications large data sets nevertheless research direction active paper review current approaches toward scaling kernelbased systems large data sets b introduction kernelbased systems support vector machine svm gaussian processes gp powerful currently popular approaches supervised learning kernelbased systems demonstrated competitive performance several applications data sets kernelbased systems also great potential kdd appli cations since degrees freedom grow training data size therefore capable modeling increasing amount detail increasing data set size unfortunately least three problems one tries scale systems large data sets first training time increases dramatically size training data set second memory requirements increase data third prediction time proportional number kernels equal least increases size training data set number researchers approached issues developed solutions scaling kernelbased systems large data sets review approaches paper focus computational complexity training paper organized follows section 2 brief introduction kernelbased systems implementation issues section 3 presents various approaches scaling kernelbased systems large data sets first section 31 present approaches based idea com 2001 kluwer academic publishers printed netherlands volker tresp mittee machines section 32 covers data compression approaches kernel systems applied compressed subsampled versions original training data set section 33 discusses ecient methods linear svm based modications optimization problem solved training regimes kernelbased systems require solution system linear equations size training data set section 34 presents various approaches eciently solving system equations section 35 projection methods discussed solutions based nitedimensional approximation kernel system section 4 provides conclusions 2 kernelbased systems supervised learning 21 kernelbased system kernelbased systems response fx input x calculated superposition kernel functions kx w weight ith kernel kernels either dened input patterns subset training data case svm sometimes bias term b included regression corresponds estimate regression function binary svm classication estimated class label sign cases kernel functions kx required positive denite func tions typical examples linear kernels kx x positive integer q gaussian kernels special case since equivalent linear representation weight vector computationally ecient input dimension smaller size training data set rows design matrix input vectors training data set kernelbased systems dier weights w determined based training data scaling kernelbased systems 3 22 gaussian process regression gpr gpr one assumes priori function fx generated innitedimensional gaussian distribution zero mean covariance dened input points x x j furthermore assume set training data targets generated according independent additive gaussian noise variance 2 optimal regression function fx takes form equation 1 kernel determined covariance function based assumptions maximum posterior map solution cost function2 w w 0 w 4 n n gram trix 1 optimal weight vector solution system linear equations matrix form becomes vector targets n ndimensional unit matrix mackay 1997 provides excellent introduction gaussian processes 23 generalized gaussian process regression ggpr gaussian processes nd wider range applications permitting exible measurement process particular might assume 2 f0 1g class label ith pattern posterior probability class 1 corresponding cost function log values f location training data using identity cost function also written as2 f 4 volker tresp case optimal weights found iteratively using iterated reweighted least squares algorithm approach generalized density exponential family distributions discussion ggpr found tresp 2000b williams barber 1998 fahrmeir tutz 1994 24 support vector machine svm svm classies pattern according sign equation 2 commonly used svm cost function is2 w nndimensional diagonal matrix ones minus ones diagonal corresponding class label respective patterns e ndimensional vector ones c 0 constant negative components vector within bracket zero note similarity svm ggpr cost function dierence measurement process svm derived exponential family distributions ggpr common following equivalent denition dieren tiable cost function constraints weights svm minimize cost function 1 slack variables 0 subject constraint rest paper meant applied component wise introducing ndimensional vector lagrange multipliers enforcing constraints one obtains dual problem maximize constraints weights w lagrange multipliers related via optimal weight vector sparse weight w many kernels zero training remaining kernels nonzero weights dene support vectors shown solution minimizes bound generalization error vapnik 1998 vapnik scaling kernelbased systems 5 1998 scholkopf burges smola 1999 christianini shawetaylor 2000 muller mika ratsch tsuda scholkopf 2001 excellent sources information svm 25 training solving system linear equations equation 5 gpr requires operations iterated reweighted least squares algorithm used training ggpr typically requires systems similar linear equations must solved repeatedly support vector machine dual optimization problem requires solution quadratic programming qp problem linear constraints due large dimension optimization problem general qp routines unsuitable even relatively small problems hundred data points common interior point methods require repeated solution linear equations dimensionality number variables solve highdimensional qp problems svm special algorithms developed important ones chunk ing decomposition sequential minimal optimization smo methods iteratively solve smaller qp problems chunking decomposition require qp solver inner loops decomposition scales better size training data set since dimensionality qp problem xed whereas chunking algorithm dimensionality increases reaches number support vectors chunking one earliest approaches used optimizing svm machine decomposition introduced osuna freund girosi 1997 developed joachims 1998 smo extreme case decomposition algorithm since two variables lagrange multipliers optimized time done ana lytically smo require qp optimizer inner loop decomposition smo general faster chunking experimental evidence suggests computational complexity decomposition smo scales approximately square size training data set alternative qp several gradient optimization routines developed addition perezcruz alarcondiana naviavazquez artesrodr iguez 2001 used fast iterated reweighted least squares procedure recently train svm 6 volker tresp 26 kernels regularization networks smoothing splines relevance vector machine kernel fisher discriminant possible discuss currently popular kernelbased learning systems detail limited space allowed regularization networks poggio girosi 1990 essentially identical gpr kernels greens functions derived appropriate regularization problem similarly smoothing splines closely related wahba 1990 relevance vector machine tipping 2000 achieves sparseness pruning away dimensions weight vector using evidence framework finally kernel fisher discriminant wellknown linear fisher discriminant approach transformed highdimensional feature space means kernel functions muller et al 2001 3 scaling kernelbased systems large data sets section heart paper discuss various approaches toward scaling kernelbased systems large data sets 31 committee machines 311 introduction committee machines dierent data sets assigned dierent committee members ie kernel systems training predictions committee members combined form prediction committee want discuss two committee approaches particularly suitable applied kernelbased systems 312 bayesian committee machine bcm bcm approach tresp 2000a data partitioned data sets approximately size learning systems trained respective training data set bcm calculates unknown responses number test points time let f nq vector response variables nq test points underlying assumption bcm assumption data sets independent given f q good assumption f q contains many points since points dene scaling kernelbased systems 7 map make data independent approximation also improves number data set large increases independence data sets average based assumption one obtains practical algorithm obtained assume probability distributions approximately gaussian p f q jd also approximately gaussian ef q qq nq nq prior covariance matrix test points equation 8 form committee machine predictions committee members nq inputs used form prediction committee inputs prediction module weighted inverse covariance prediction intuitive appealing eect weighting predictions committee members inverse covariance modules uncertain predictions automatically weighted less modules certain predictions applied bcm gpr tresp 2000 gpr mean covariance posterior gaussian densities readily computed subsequently bcm applied ggpr tresp 2000b svm schwaighofer tresp 2001 ggpr svm posterior distributions approximately gaussian tresp 2000 shown nq dimension f q least large eective number parameters 2 case bcm generalizations give excellent results furthermore possible derive online kalman lter versions bcm require one pass data set storage matrix dimension number test points tresp 2000a training prediction additional test points requires resources dependent number test points independent 2 since latter closely related kernel bandwidth also close connection kernel bandwidth nq 8 volker tresp size training data set several data sets found dierence performance bcm approximation optimal gpr prediction based inversion full covariance matrix bcm applied training data sets size full inversion clearly unfeasible 313 boosting boosting committee members trained sequentially training particular committee member dependent training performance previously trained members boosting reduce variance bias prediction reason training committee member weight put data misclassied previously trained committee members schapire 1990 developed original boosting approach boosting ltering three learning systems used existence oracle produce arbitrary quantity training data assumed rst learning system trained k training data second learning system trained quantity data training data generated half classied correctly half classied incorrectly rst learning system third learning system trained data learning systems one two disagreed majority vote three learning systems determines classication note second learning system obtains 50 patterns training dicult rst learning system third learning system obtains critical patterns sense learning rst second learning systems disagree patterns original boosting algorithm particularly useful large data sets since large number training data ltered directly used training recent years focus interest shifted toward boosting algorithms also applied smaller data sets boosting resampling boosting reweighting recently pavlov mao dom 2000 used form boosting resampling boost smo context training svm large data sets used small fraction data 2 4 train committee member using smo algorithm train subsequent committee member chose data higher probability dicult previous committee member classify way portion complete training data set used training overall prediction weighted combination predictions committee members experiments boostsmo faster factor 10 smo providing essentially prediction accuracy implementation probability data reweighted prior scaling kernelbased systems 9 training new committee member multiple passes data required nevertheless online versions approach also conceivable 32 data compression data compression generally applicable solution dealing large data sets idea train learning system smaller data set either generated subsampling preclustering data latter case cluster centers used training patterns idea preclustering data extended interesting direction applying concept squashing training linear svm pavlov chudova smyth 2000 training smo algorithm used clustering performed using metric derived likelihood prole data first small percentage original training data set randomly chosen cluster centers linear svm typically 50100 random weights v oset b generated following prior distributions probabilistic version svm used data point loglikelihood weight vector calculated producing ldimensional vector likelihood prole data point data point assigned cluster center closest likelihood prole finally weight cluster center proportional number data assigned cluster procedure leads considerable reduction training data taking account statistical properties data training time using smo squashing comparable training time boostsmo see previous section providing comparable prediction accuracy 33 fast algorithms linear svms svm originally formulated linear classier kernels introduced order able obtain nonlinear classication bound aries training linear svm considerably faster training kernel svm following section discusses approaches lead even faster training algorithms linear svm modifying cost function 331 active support vector machine asvm asvm developed mangasarian musicant 2001 optimization problem svm reformulated modied cost function is2 v constraints design matrix design matrix input vectors training data set represented rows note cost function contains square bias b margins respect orientation v location relative origin b maximized furthermore 2norm slack vector minimized based modications dual problem formulated contains nonnegativity constraints equality constraints due modications simple iterative optimization algorithm derived iteration step system linear equations size input dimension plus one needs solved number iteration steps nite example data set 7 million points required 5 iterations needed 95 cpu minutes 332 lagrange support vector machine lsvm variation asvm lsvm mangasarian musicant 2000 based reformulation optimization problem leads dual problem dierence lsvm works directly karushkuhntucker necessary sucient optimality condition dual problem algorithm quires prior optimization iterations inversion one matrix q size input dimension plus one iteration simple form vector lagrange multipliers step positive constant lsvm asvm comparable speed although asvm faster problems great advantage lsvm denitely simplicity algorithm lsvm also applied nonlinear kernels matrix size number data points needs inverted examples fast linear svmvariants see mangasarian musicant 1999 34 approximate solutions systems linear equations gaussian processes variants svm see section 332 require solution large system linear equations scaling kernelbased systems 11 341 method skilling gaussian processes also variants svm necessary solve linear system equation form matrix solution linear set equations identical minimum cost function minimized iteratively using conjugate gradient method ok n 2 steps k number iterations conjugate gradient procedure often k set much smaller n without signicant loss performance particularly large number small eigenvalues approach due skilling one earliest approaches speeding training gpr systems gibbs mackay 1997 note computational complexity approach quadratic size training data set approach therefore well suited massive data sets 342 nystrom method nystrom method introduced williams seeger 2001 applicable particular gpr lets assume decomposition gram matrix form uu 0 diagonal matrix u n matrix typically n case solve system linear equations see equation 5 using woodbury formula press teukolsky vetterling flannery 1992 obtain form matrix size mm needs inverted w still dimension number training data n example appropriate decomposition gram matrix would eigenvalue decomposition computational complexity full eigenvalue decomposition scales 3 much gained unless gram matrix large number small eigenvalues particularly interesting approximation introduced williams seeger 2001 performed eigendecomposition ran domly chosen submatrix based p eigenvectors eigenvalues decomposition smaller matrix corresponding decomposition larger matrix approximated using nystrom method nystrom method method numerically solving integral equations computational complexity approach om 2 n approach scales linear n authors veried excellent quality approximate method using training set size 7291 obtained good results approach applicable regression classication 35 projection methods leading finitedimensional representation general degrees freedom kernel system grow number kernels ie data points approaches discussed section project basically innitedimensional problem nite dimensional representation problem reduces estimating parameters nitedimensional problem approaches solution assumes form system xed basis functions bcm approach section 312 also considered specic projection approach basis functions kernels dened test points 351 optimal projections problem formulation given input data distributions p x size training data n known best linear combination mdimensional set basis functions provides best approximation gpr kernelbased system projected bayes regression zhu williams rohwer morciniec 1998 problem solved based innitedimensional principle component analysis n 1 result one use rst eigenfunctions covariance function describing gaussian process asymptotically computational complexity onm 2 trecate williams opper 1998 described improved variational approximation nite data size approach smaller test set error compared projected bayes regression computational complexity scales 2 approach quadratic n whereas projected bayes regression linear n representation increases data size csato opper 2001 presented online variant based ideas leads sparse representation limiting representation small number well chosen kernel functions 352 projection subset kernels lets select subset training data set size n let mm denote corresponding kernel matrix approximate scaling kernelbased systems 13 vector covariances functional values x data subset approximation equality either x x j elements subset approximation otherwise approximation regression function superposition kernel functions optimal weight vector minimizes cost nm contains covariance terms n training data subset data chosen kernel functions although used xed number kernels training data contributed determine weight vector w methods described following subsections based decomposition connection decomposition gram matrix section bcm approximation discussed appendix 353 reduced support vector machines rsvm rsvm lee mangasarian 2000 uses nonstandard svm cost function form2 w compare equation original svm cost function equation 6 notice cost term weights simplied previous section w denotes kernel weights randomly selected kernels case asvm section 331 square bias b included 2norm transformed slack variable included nm dened previous subsection log1 exp x applied componentwise function g soft twicedierentiable version typically large positive number advantages modications constraints needed formulation equation 6 b due modications cost function quadratically converging newton algorithm used training c due projection nite number kernels time complexity optimization routine scales linearly number data points 14 volker tresp experiment using adult data set 3 n32600 training data rsvm needed 16 minutes whereas smo algorithm needed two hours surprisingly smaller data sets rsvm performed even better full svm explained smaller tendency toward overtting rsvm experimental results showed random selection training data signicantly increase variance prediction 354 sparse greedy matrix approximation previous two subsections expansion terms nite number kernels dened random subset training data sought contrast bcm approximation kernels dened test points smola scholkopf 2000 expansion based subset training data kernels selected randomly goal nd kernels best represent n kernels training data proximity dened reproducing kernel hilbert space ie feature space simplies calculations drastically since inner product two kernels dened x x j simply equal kx authors describe greedy algorithm step best kernel randomly selected candidate kernels added set already selected kernels computational cost version ol n 2 n total number training data size selected subset training data smola scholkopfs paper motivated theoretical considerations authors showed approximation based kernels close approximation based optimal basis vectors optimal also closely related eective number parameters discussed section 312 therefore dependent kernel bandwidth variant approach applicable gpr described smola bartlett 2001 paper derived stopping criterion bounds approximation error using training data set size authors demonstrated less 10 training data used kernels obtained statistically insignicant dierences performance gpr based inversion full covariance matrix approximation 4 conclusions summarized important approaches scaling kernelbased systems large data sets nonlinear kernels various authors 3 retrievable httpwwwicsuciedu mlearn scaling kernelbased systems 15 achieved considerable reduction training time makes nonlinear kernel systems applicable data sets maybe 100000 data points approaches assume representation based nite subset training data respectively kernels sucient reasonable assumption kernel bandwidth low nev ertheless goal stated introduction able model increasing amount detail sucient data become available would require kernel bandwidth scaled increasing data size thus increasing degrees freedom appropriately possible limited degree methods presented combining kernelbased systems hierarchical partitioning map local algorithms might interesting direction future research finally kernel systems using linear kernels training time considerably faster kernel systems using nonlinear kernels systems million data points trained witin reasonable time appendix lets assume one interested prediction set query points dened subset training data section 352 test data section 312 let f q denote unknown functional values query points let f w expansion terms kernel functions dened query points qq covariance matrix dened subset points weight vector leads optimal predictions subset points covariance training data given f q also nq covariance training data query points nn covariance matrix dened training data note inverse covyjf q needs calculated n n matrix approximation used section 352 simply sets covyjf q unit matrix bcm uses block diagonal approximation covyjf q calculation optimal weight vector w requires inversion matrices block size approximation improves blocks used smaller number elements set zero dimension f q large volker tresp since two terms right side equation 11 cancel detailed discussion found tresp schwaighofer 2001 acknowledgements salvatore ingrassia universita della calabria stefano ricci universita di pavia provided extensive comments earlier version paper comments helped improve paper considerably addition valuable discussions alex smola chris williams john platt lehel csato anton schwaighofer gratefully acknowledged r support vector machines multivariate statistical modeling based generalized linear models making largescale support vector machine learning practical rsvm reduced support vector chines introduction gaussian processes massive support vector regression lagrangian support vector machine active support vector machine classi scaling support vector machines using boosting algorithm towards scalable support vector machines using squashing fast training support vector classi fast training support vector machines using sequential minimal optimization networks approximation learning numerical recipes c strength weak learnability bayesian committee support vector machine sparse greedy gaussian process regression relevance vector machine scalable kernel systems statistical learning theory spline models observational data bayesian classi gaussian regression optimal tr ctr paul bradley johannes gehrke raghu ramakrishnan ramakrishnan srikant scaling mining algorithms large databases communications acm v45 n8 august 2002 navneet panda edward chang gang wu concept boundary detection speeding svms proceedings 23rd international conference machine learning p681688 june 2529 2006 pittsburgh pennsylvania daniel schneega steffen udluft thomas martinetz kernel rewards regression information efficient batch policy iteration approach proceedings 24th iasted international conference artificial intelligence applications p428433 february 1316 2006 innsbruck austria jiantao sun benyu zhang zheng chen yuchang lu chunyi shi weiying gecko method optimize composite kernels web page classification proceedings 2004 ieeewicacm international conference web intelligence p299305 september 2024 2004 ying lu jiawei han cancer classification using gene expression data information systems v28 n4 p243268 june