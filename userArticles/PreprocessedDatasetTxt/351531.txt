priori sparsity patterns parallel sparse approximate inverse preconditioners parallel algorithms computing sparse approximations inverse sparse matrix either use prescribed sparsity pattern approximate inverse attempt generate good pattern part algorithm paper demonstrates pde problems patterns powers sparsified matrices psms used priori effective approximate inverse patterns additional effort adaptive sparsity pattern calculations may required psm patterns related various approximate inverse sparsity patterns matrix graph theory heuristics concerning pdes greens function parallel implementation shows psmpatterned approximate inverses significantly faster construct approximate inverses constructed adaptively often giving preconditioners comparable quality b introduction sparse approximate inverse approximates inverse usually sparse matrix sparse matrix accomplished example leastsquares method minimizing matrix residual norm 1 f constraint sparse general degrees freedom problem nonzero values well locations minimization considers variables simultaneously however complex thus simple approach prescribe set nonzeros sparsity pattern performing minimization objective function 11 decoupled sum squares 2norms n individual columns ie e j j jth columns identity matrix matrix respectively leastsquares matrix small number columns equal number nonzeros corresponding j nonsingular leastsquares matrices full rank thus approximate inverse constructed solving n leastsquares problems parallel however sparse approximate inverses attractive parallel preconditioning primarily preconditioning operation sparse matrix vector product cost constructing approximate inverse large matrix usually high especially adaptive pattern selection strategies described competitive constructed parallel diagonally dominant entries 1 decay rapidly away diagonal 17 banded pattern produce good approximate inverse work performed auspices us department energy lawrence livermore national laboratory contract w7405eng48 center applied scientic computing lawrence livermore national laboratory l560 box 808 livermore ca 94551 echowllnlgov 1 form right approximate inverse used notationally slightly clearer matrix distributed parallel processors rows left approximate inverse computed rowwise general setting without applicationspecic information clear best choose sparsity pattern algorithms developed rst compute approximate inverse initial pattern updated new minimization problem solved either exactly inexactly process repeated threshold residual norm satised maximum number nonzeros reached 12 15 22 refer adaptive procedures one procedure use iterative method minimal residual starting sparse initial guess 12 approximately minimize 12 ie nd sparse approximate solutions suppose sparse initial guess j used rst iterates sparse maintain sparsity strategy drop small elements usually used either search direction iterates prescribed pattern necessary since sparsity pattern emerges automatically method ecient sparsesparse operations must used product sparse matrix sparse vector p nonzeros involves p columns sparse matrix another adaptive procedure called spai 15 22 uses numerical test determine nonzero locations added current sparsity pattern jth column numerical test adding nonzero location k form tolerance residual given sparsity pattern column j j current approximation test 14 lower bound improvement square residual norm pattern j augmented entry k added tolerance satised lefthand side 14 large compared values k cost performing test sparse dot product r column k location test interprocessor communication needed test k corresponding column local processor adaptive algorithms utilize additional degrees freedom minimizing aorded locations nonzeros allowed much general problems solved adaptive methods however tend expensive thus paper focuses problem selecting preprocessing step sparse approximate inverse computed immediately minimizing 11 section 2 rst examines sparsity patterns produced nonadaptive adaptive schemes section 3 tests idea using patterns powers sparsied matrices psms priori sparsity patterns sparse approximate inverses numerical tests presented section 4 comparisons sequential parallel versions current methods finally section 5 draws conclusions 2 graph interpretations approximate inverse sparsity patterns 21 use pattern variants structure sparse matrix order n directed graph ga whose vertices integers whose edges j correspond nonzero odiagonal entries notation usually implies matrices nonzero diagonal entries subset ga directed graph vertices subset edges ga priori patterns sparse approximate inverses 3 graph ga representation sparsity pattern clear context distinguish structure vector x order n subset ng corresponds nonzero entries x associated matrix order n often refer structure x subset vertices associated matrix notice structure column j matrix set vertices ga edges pointing vertex j plus vertex j structure row j vertex plus set vertices pointed vertex j inverse matrix shows unknown linear system depends unknowns structure matrix shows immediate dependencies suggests structure 1 edge whenever directed path vertex vertex j ga 21 nonsingular ignoring coincidental cancellation structure called transitive closure ga denoted g irreducible matrix result says inverse full matrix suggest possibility truncating transitive closure process approximate inverse sparse matrix heuristic often employed vertices closer vertex j along directed paths important retained approximate inverse sparsity pattern idea supported decay elements observed tang 30 discrete greens function many problems sparsity patterns rst used benson frederickson 4 symmetric case also dened matrices patterns qlocal matrices given graph ga structurally symmetric matrix full diagonal structure jth column qlocal matrix consists vertex j qth level nearestneighbors ga 0local matrix diagonal matrix 1local matrix sparsity pattern sparsity pattern common priori pattern used approximate 1 gives good results many problems usually improved fails many problems one improvement use higher levels q unfortu nately storage preconditioners grows quickly q increased even impractical many cases 20 huckle 24 proposed similar patterns may eective nonsymmetric include patterns corresponding graphs ga density former particular grows quickly increasing k primarily patterns useful envelope patterns adaptive spai algorithm select pattern gives upper bound interprocessor communication required parallel implementation 23 cosgrove daz 14 proposed augmenting pattern without going full 2local matrix suggested adding nonzeros j way minimizes number new rows introduced jth leastsquares matrix expression 12 augmented structure determined structure kolotilina yeremin 28 proposed similar heuristics augmenting sparsity pattern factorized sparse approximate inverses sparsication instead augmenting pattern also possible diminish pattern relatively full accomplished spar sication dropping small elements matrix using resulting pattern introduced kolotilina 26 computing sparse approximate inverses dense matrices see also 10 32 kaporin 25 sparse matrices factorized approximate inverses sparsication combined use higher level neighbors tang 30 showed sparsifying matrix prior applying adaptive spai algorithm eective anisotropic problems observation storage therefore operation count required preconditioners produced way much smaller technique generate patterns powers sparsied matrices idea explicitly combining sparsication use higher level neighbors used alleon et al 1 attributes technique cosnuau 16 approximating inverse dense matrices electromagnetics however tests showed higher levels warranted tang wan 31 also used sparsi cation applying qlocal matrix pattern q 1 approximate inverses used multigrid smoothers showed sparsication cause deterioration convergence rate problems work alleon et al tang wan represent rst uses psm patterns instead applying sparsication also appropriate cases apply sparsication sparse approximate inverse computed 27 31 useful reduce cost using approximate inverse relatively full 22 insights adaptive schemes adaptive schemes generate patterns dierent pattern example generated patterns much sparser nevertheless patterns produced adaptive schemes interpreted using graph consider rst approximate minimization method described section 1 following algorithm nds sparse approximate solution minimal residual iterations dropping strategy elements search direction r encapsulated function drop step 4 may depend current pattern algorithm 21 sparse approximate solution 1 sparse initial guess 2 r e j 3 loop krk tol reached max iterations 4 dropr 5 q ad 6 rq 7 8 r r q 9 endloop elements r already candidates new elements vector r generated essentially product thus structure r set vertices edges pointing vertices structure initial guess consists single nonzero element location j structure grows outward vertex j ga iteration algorithm search direction iterative method r instead r kth entry search direction r ae k dropping strategy based size entries similar one based test 14 attempts minimize updated residual norm method candidates new elements rst second level neighbors vertices nonsymmetric directions edges important priori patterns sparse approximate inverses 5 exactly graph interpretation spai algorithm see huckle 24 computing column j vertices far vertex j enter pattern least initially algebraically means nonzero locations r ae k intersect value test zero ecient implementation spai uses graph ideas narrow indices k need checked early parallel implementation spai 18 tested rst level neighbors vertex rather rst second levels good approximation many cases implementation also assumed structurally symmetric onesided interprocessor communication necessary recent parallel implementation spai 2 3 implements algorithm exactly code implements onesided communication message passing interface mpi uses dynamic load balancing case processors nish computing rows earlier others 3 patterns powers sparsied matrices 31 graph interpretation section 2 observed prescribed sparsity patterns patterns generated adaptive methods generally subsets pattern low powers given full diagonal typically increase accuracy higher powers clearly methods related neumann series characteristic polynomial 24 structure column j approximate inverse subset vertices level sets directed edges vertex j ga good vertices choose level sets neighborhood vertex j algorithms dier vertices selected convectiondominated anisotropic problems upstream vertices vertices preferred directions greater uence others column j inverse figure 31 shows discrete greens function point pde convection nonsymmetry function shows upstream nonzeros row column exact inverse greater magnitude others without additional physical information direction ow however often possible use sparsication identify preferred upstream directions51525010203001030507 fig 31 greens function point pde convection examined sparsity patterns produced adaptive algorithms tried determine could generated simpler graph algorithms simple examples turned structures produced exactly close transitive closures subset ga ie structure sparsied 6 edmond chow matrix figure 32 show structures several matrices orsirr2 harwellboeing collection b 0 sparsication original matrix c transitive closure g structure produced spai algorithm latter gure selected 2 shows example eective sparse approximate inverse pattern problem however bothersome features example approximate inverse four independent diagonal blocks note approximate adaptively generated pattern well pattern c generated using transitive closure b sparsied orsirr2 c transitive closure b pattern spai fig 32 adaptively generated pattern approximated transitive closure c 32 sparsify simplest method sparsify matrix retain entries matrix greater global threshold thresh example figure used important however make sure diagonal elements retained otherwise structurally singular matrix would resulted case general diagonal always retained priori patterns sparse approximate inverses 7 one strategy choosing threshold choose one retains example onethird original nonzeros matrix fewer nonzeros retained powers sparsied matrix numbers nonzeros grow quickly thresholds chosen small problems tested section 4 number levels used may increased preconditioner reaches target number nonzeros best choices parameters problemdependent special problems strategy may eective example matrix contains unique values matrix sparsied using global threshold matrix scaled becomes important often case matrix contains many dierent types equations variables scaled way example consider matrix rst row column scaled large number z threshold z chosen diagonal matrix retained third row sparsied matrix become independent rows thus apply thresholding matrix symmetrically scaled ones diagonal eg matrix scaled matrix threshold less equal 1 guarantee diagonal retained scaling also makes easier choose threshold method scaling foolproof avoid simple problems graph sparsied matrix 0 vertex connections vertices accomplished sparsifying matrix one retains least xed number edges vertex example ones corresponding largest matrix values given parameter implemented column j selecting diagonal jth element plus 1 largest odiagonal elements column j original matrix guarantees 1 vertices edges vertex j applied rowwise guarantees 1 vertices edges emanating vertex j choose explicitly keep diagonal matrix thus column row least nonzeros choosing may simpler meaningful choosing threshold matrix values dierent values may used dierent vertices depending vertexs initial degree number incident edges denote structure matrix 0 sparsied denote structure set vertices edge whenever path distance less 0 structure 1 subset matrix form ignoring coincidental cancellation called level set expansions sparsied matrix patterns powers sparsied matrix psm patterns heuristic 3 tested alleon et al 1 equivalent using variable level perform sparsication mention also possible perform sparsication every level set expansion denote variant variant values need computed propose following stresses larger elements drop denotes sparsication process dene note 1 generally subset complicated strategies possible thresholds dierent level note determining much dicult determining since values need computed 33 factorized forms approximate inverse sparse approximate inverses cholesky lu factors often used analogue leastsquares method minimization 11 factorized sparse approximate inverse fsai technique kolotilina yeremin 28 implemented parallel field 19 normal equations method used solve leastsquares systems cholesky lu factors required compute approximate inverse means however adaptive pattern selection schemes cannot used since matrix whose inverse approximated available priori sparsity patterns must used instead given sparse approximate inverse approximates u 1 sparse matrices g h respectively patterns g h chosen pattern gh close sense good patterns approximating 1 supposing good pattern 1 upper lower triangular parts good patterns g h since pattern gh includes pattern patterns tested section 4 may also possible use patterns powers exact approximate l u known l u factors discretizations pdes inverses often banded elements decaying rapidly away main diagonal technique may appropriate approximate l u available example sparse incomplete lu factorization opposed inverses irreducible matrices inverses cholesky lu factors often sparse ordering thus applied gives factors whose inverses well approximated sparse matrices experimentally fewer nonzeros exact inverse factors translates lower construction cost better performance factorized approximate inverses computed incomplete biconjugation process 7 8 transitive closure used compute number nonzeros exact inverse cholesky factor based height nodes elimination tree lead reordering strategies approximately minimize height elimination tree thus number nonzeros inverse factors allows prediction well approximate inverses might perform given problem 7 8 34 approximate inverse schur complement determine good pattern schur complement matrix notice schur complement thus good sparsity pattern 1 determined good sparsity pattern 1 simply 22 priori patterns sparse approximate inverses 9 block good sparsity pattern 1 b small order compared global matrix else method overly costly code may possible compute approximate inverse extract approximation 1 compute partial approximate inverse ie rows columns approximate inverse correspond 1 11 almost order 35 parallel computation computation equivalent structural sparse matrixmatrix products sparsied matrices computation also viewed n level set expansions one row column performed parallel vertices near vertices dierent processor communication necessary communication reduced partitioning graph sparsied matrix among processors number edgecuts reduced unfortunately general onesided communication required compute sparse approximate inverses processors need request rows processors processor cannot predict rows need send onesided communication may implemented mpi processor occasionally probe messages processors latency probes critical performance factor multithreaded environment possible dedicate threads local processing node servicing requests rows server threads remaining threads compute row make requests rows necessary worker threads consider matrix approximate inverse computed partitioned way rows across several processors algorithm 31 describes one organization parallel computation processor computes level set expansion rows continuing next level level requests replies processor coalesced allowing fewer larger messages used like 3 external rows cached processor case needed compute rows communication numerical phase values computed algorithm implemented using occasional probing onesided communication algorithm 31 parallel level set expansions computing communicate rows 1 initialize set vertices v empty 2 sparsify rows local processor 3 merge structures locally sparsied rows v 4 level 5 nonlocal k 2 v request receive row k 6 sparsify received rows 7 merge structures new sparsied rows v 8 endfor 9 nonlocal k 2 v request receive row k compute structure row 10 local row j 11 initialize v j single entry location j 12 level 13 new sparsied structure row k v j 14 endfor 15 endfor compute values row 16 local row j nd pattern v j also implemented second parallel code following features multithreaded take advantage multiple processors per sharedmemory node symmetric multiprocessor computers uses server worker threads easily implement onesided commu nication uses simpler algorithm algorithm 31 computes row performs associated communications continuing next row multiple threads used avoids worker threads needing synchronize coordinate rows request nodes smaller messages used communication also spread entire execution time algorithm scalable use memory thus also slower rst version used directaddress tables traded memory faster computation timings second code reported next section limited timings rst code also shown also working factorized implementation symmetric matrices guarantee preconditioner also symmetric implementation makes simple change step 16 algorithm 31 require onesided communication full matrix stored 4 numerical tests 41 preconditioning quality first test quality sparsity patterns generated powers sparsied matrices small problems harwellboeing collection particular chose problems tested spai 22 order make comparisons performed tests exactly conditions solve linear systems using gmres20 relative residual tolerance 10 8 zero initial guess report number gmres steps needed convergence indicate convergence using symbol right preconditioning used tables 41 45 show test results unfactored column 2 factored column 3 forms approximate inverse compare results leastsquares ls method using pattern original matrix fsai leastsquares method nonsymmetric factored form 28 using pattern original matrix unfactored form also display result spai method reported 22 using choice parameters adaptive methods factored forms also available 5 6 29 tested global thresholds shown table scaled matrix used perform sparsications tables also show number nonzeros nnz unfactored preconditioners entry lsfsai number nonzeros results show preconditioners almost quality adaptive spai achieved using patterns cases even better preconditioners result sometimes even less storage table 41 results also show using pattern generally give good preconditioner problems sherman2 relatively hard problem sparse approximate inverses result reported 22 shows spai could reduce residual norm 10 5 preconditioner 26327 nonzeros 7 steps results similar patterns full residual norm reduction achieved approximate inverse denser note case sparsication priori patterns sparse approximate inverses 11 table iteration counts orsirr2 pattern unfactored factored nnz lsfsai 335 383 5970 spai 84 5318 table iteration counts sherman1 pattern unfactored factored nnz lsfsai 145 456 3750 spai threshold applied original matrix rather diagonally scaled matrix although matrix values 27 orders magnitude diagonal scaling foolproof factorized approximate inverses eective problem patterns saylr4 relatively hard problem gmres grote huckle 22 report spai could solve problem gmres could bicgstab also true patterns results table 45 bicgstab course many problems dicult psmpatterned approximate inverses include nnc666 gre1107 harwellboeing collection fidap problems navierstokes simulations problems however solved using adaptive methods 12 problems pose diculties psm patterns greens function heuristic invalid problems either pde problems modied eg fidap problems used penalty formulation tables 46 47 show test results unfactored form approximate inverse 0 ls patterns problems since generally superset guarantee better pattern terms norm r show also display matrix residual norms parameter shown table used sparsify matrices level set expansion results show priori methods approach quality adaptive methods closely 42 parallel timing results results show preconditioning quality pde problems signicantly degraded using nonadaptive schemes based powers sparsied matrices section illustrate main advantage preconditioners low construction costs compared adaptive schemes section 43 show results using multithreaded version code parasails parallel sparse approximate inverse least squares like parallel version spai 3 make comparisons parasails implemented table iteration counts sherman2 pattern unfactored factored nnz lsfsai 23094 spai 26327 table iteration counts pores3 pattern unfactored factored nnz lsfsai 3474 spai 599 16745 preconditioner object isis solver library 13 codes generate sparse approximate inverse partitioned across processors rows thus left preconditioning used codes leastsquares problems arose solved using lapack routines qr decomposition problems relatively full approximate inverses solving leastsquares problems takes majority computing time tests run multiple nodes ibm rs6000 sp supercomputer lawrence livermore national laboratory node contains four 332 mhz powerpc cpus timings performed using userspace mode much ecient internetprotocol mode however nonthreaded codes use one processor per node userspace mode tested spai one processor per node parasails four processors per node iterative solver matrixvector product codes also nonthreaded used one processor per node rst problem tested nite element model three concentric spherical shells dierent material properties matrix order 16881 nonzeros spai algorithm using default parameters target residual norm row 04 produced much sparser precondi tioner 171996 nonzeros solved problem using gmres50 tolerance steps comparison purposes chose parameters parasails gave similar number nonzeros preconditioner particular 3 parameter 3 gave preconditioner 179550 nonzeros solved problem 331 steps figure 41 shows two resulting sparsity patterns table 48 reports various numbers nodes npes wallclock times preconditioner setup phase precon iterative solve phase solve total time total time constructing preconditioner code includes time determining sparsity pattern due relatively small size next problem one worker one server thread used per node ie two processors per node parasails runs one processor used spai runs comparison table 49 show results using rst nonthreaded occasional mpi probes onesided communication version code code priori patterns sparse approximate inverses 13 table iteration counts saylr4 pattern unfactored factored nnz lsfsai 22316 spai 67 84800 table iteration counts sherman3 steps nnz krkf ls 20033 173620 spai 264 48480 much faster uses directaddress arrays quickly merge sparsity patterns rows directaddress arrays length global size matrix scalable parasails b spai fig 41 structure sparse approximate inverses concentric shells problem tested second larger problem models workhardening metal squeezing make pancakelike particular example pattern sparsied matrix 0 parameter 3 lead good preconditioner parasails produced preconditioner 141848 nonzeros solved problem 142 steps spai produced preconditioner 120192 nonzeros 14 edmond chow table iteration counts sherman4 steps nnz krkf ls 199 3786 62503 spai 86 9276 table timings concentric shells problem parasails spai npes precon solve total precon solve total solved problem 139 steps results varying numbers processing nodes shown table 410 results show nonadaptive parasails algorithm implemented many times faster adaptive spai algorithm 43 implementation scalability section experimentally investigate implementation scalability constructing sparse approximate inverses parasails let n p time construct approximate inverse order n parallel computer using p processors dene scaled eciency en p n 1t pn p en implementation perfectly scalable ie one could double size problem number processors without increasing execution time however long en p bounded away zero xed n p increased say implementation scalable consider 3d constant coecient pde discretized using standard nite dierences uniform mesh anisotropic parameters problem used test scalability multigrid solvers 9 problems constant size per compute node 10 3 local problem sizes node topologies 1 3 5 3 used thus largest problem cube 60 unknowns node used four processors 4 worker threads 1 server thread problem preconditioner construction phase ie 500 processors largest conguration threshold parasails chosen nonzeros along strongest z direction retained 3 pattern used although symmetric problem preconditioner symmetric used gmres50 iterative solver zero initial guess convergence tolerance 10 6 priori patterns sparse approximate inverses 15 table concentric shells problem timings preconditioner setup using direct addressing npes precon table timings workhardening problem parasails spai npes precon solve total precon solve total table 411 shows results including wallclock times constructing preconditioner iterative solve phase number iterations required convergence average time one iteration solve phase e p e scaled ecien cies constructing preconditioner one step solve phase figures 42 43 graph scaled eciencies e p e respectively implementation seems scalable values p may encountered comparison table 412 show results spai using 40 3 local problem size larger problems led excessive preconditioner construction times one processor per node used spai 5 conclusions paper demonstrates eectiveness patterns powers sparsied matrices sparse approximate inverses pde problems opposed many existing methods prescribing sparsity patterns psm patterns use values structure original matrix sparse patterns produced psm patterns allow simpler direct methods constructing sparse approximate inverse preconditioners used comparable preconditioning quality adaptive methods signicantly less computational cost numerical tests show additional eort adaptive sparsity pattern calculations always required acknowledgments author indebted weipai tang one rst use sparsication computing approximate inverse sparsity patterns john gilbert instrumental directing attention transitive closure matrix motivating possibility nding good patterns priori michele benzi made helpful comments directed author 24 author also grateful ongoing support robert clay andrew esmond ng ivan otero yousef saad alan williams nally cogent comments anonymous referees table timings iteration counts eciencies parasails local problem size npes n precon solve iter solveiter ep es local problem size npes n precon solve iter solveiter ep es local problem size npes n precon solve iter solveiter ep es local problem size npes n precon solve iter solveiter ep es 50 50 50 local problem size npes n precon solve iter solveiter ep es local problem size npes n precon solve iter solveiter ep es table timings iteration counts eciencies spai local problem size npes n precon solve iter solveiter ep es priori patterns sparse approximate inverses 17 1200103050709number nodes scaled efficiency fig 42 implementation scalability parasails preconditioner construction phase 1200103050709number nodes scaled efficiency fig 43 implementation scalability one step iterative solution r mpi implementation spai preconditioner t3e portable mpi implementation spai preconditioner isis iterative solution large sparse linear systems arising certain multidimensional approximation problems ordering method factorized approximate inverse pre conditioner semicoarsening multigrid distributed memory machines class preconditioning methods dense linear systems boundary ele ments approximate inverse techniques blockpartitioned matrices etude dun pr decay rates inverses band matrices parallel implementation sparse approximate inverse preconditioner predicting structure sparse matrix computations parallel preconditioning sparse approximate inverses preconditioned conjugate gradient method solving discrete analogs explicit preconditioning systems linear algebraic equations dense matrices factorized sparse approximate inverse preconditionings factorized sparse approximate inverse precondition ings iterative methods sparse linear systems towards e sparse approximate inverse smoother multigrid preconditioning boundary integral equations tr ctr kai wang sangbae kim jun zhang kengo nakajima hiroshi okuda global localized parallel preconditioning techniques large scale solid earth simulations future generation computer systems v19 n4 p443456 may robert falgout jim e jones ulrike meier yang conceptual interfaces hypre future generation computer systems v22 n1 p239251 january 2006 robert falgout jim e jones ulrike meier yang pursuing scalability hypres conceptual interfaces acm transactions mathematical software toms v31 n3 p326350 september 2005 kai wang jun zhang chi shen parallel multilevel sparse approximate inverse preconditioners large sparse matrix computations proceedings acmieee conference supercomputing p1 november 1521 chi shen jun zhang parallel two level block ilu preconditioning techniques solving large sparse linear systems parallel computing v28 n10 p14511475 october 2002 dennis c smolarski diagonallystriped matrices approximate inverse preconditioners journal computational applied mathematics v186 n2 p416431 15 february 2006 edmond chow parallel implementation practical use sparse approximate inverse preconditioners priori sparsity patterns international journal high performance computing applications v15 n1 p5674 february 2001 oliver brker marcus j grote sparse approximate inverse smoothers geometric algebraic multigrid applied numerical mathematics v41 n1 p6180 april 2002 luca bergamaschi giorgio pini flavio sartoretto computational experience sequential parallel preconditioned jacobidavidson large sparse symmetric matrices journal computational physics v188 michele benzi miroslav tma parallel solver largescale markov chains applied numerical mathematics v41 n1 p135153 april 2002 anwar hussein ke chen fast computational methods locating fold points power flow equations journal computational applied mathematics v164165 n1 p419430 1 march 2004 p k jimack domain decomposition preconditioning parallel pde software engineering computational technology civilcomp press edinburgh uk 2002 e flrez garca l gonzlez g montero effect orderings sparse approximate inverse preconditioners nonsymmetric problems advances engineering software v33 n710 p611619 29 november 2002 michele benzi preconditioning techniques large linear systems survey journal computational physics v182 n2 p418477 november 2002