modified adaptive algorithms well known adaptive algorithm simple easy program results fully competitive nonlinear methods free knot spline approximation modify algorithm take full advantages nonlinear approximation new algorithms approximation order nonlinear methods proved characterizing approximation spaces one algorithms implemented computer numerical results illustrated figures tables b introduction common knowledge nonlinear approximation methods better general linear counterparts case splines nonlinear approximation puts knots function approximated changes rapidly results dramatic improvements approximating functions singularities various satisfactory results free knot spline approxi mation knots chosen ones related theorems proved showing existence certain balanced partitions accurate description given later may cause diculties practice since often numerically expensive find balanced partitions socalled adaptive approximation piecewise polynomial pp functions dyadic intervals used partition adaptive approximation draws great attention simplicity nature price pay simplicity approximation power slightly lower free knot counterpart moreover known exactly kind functions approximated prescribed order characterization adaptive approximation spaces point say adaptive algorithms paper mean approximate given univariate function pp functionssplines kinds adaptive algorithms characterized literature see 10 example paper shall modify existing adaptive algorithms two ways resulting algorithms approximation power free knot spline approximation largely keeping simplicity adaptive approximation next section shall state known results free knot spline approximation describing algorithms section 3 section 4 shall give main results parallel free knot spline approximation given next section numerical implementation examples contents last section received editors march 17 1999 accepted publication revised form february 9 2000 published electronically august 29 2000 httpwwwsiamorgjournalssinum38335356html department mathematics computer science georgia southern university statesboro ga 30460 yhugasouedu department mathematics university manitoba winnipeg mb canada r3t 2n2 kkopotunmathvanderbiltedu author supported nsf grant dms 9705638 department mathematics southwest missouri state university springfield mo 65804 xmy944fmailsmsuedu 1014 yk hu k kopotun x yu emphasize consider univariate case paper idea merging cubes initially introduced used cohen et al recent paper 11 multivariate adaptive approximation resulting partition consists rings cubes possibly empty subcube removed algorithm produces near minimizers extremal problems related space bv r 2 authors explored algorithm 21 particular able obtain results extremal problems related spaces v p r functions bounded variation besov spaces b r algorithm ready implement settings depending value p l p norm chosen order local polynomials dicult r 1 though bookkeeping may messy hand algorithm designed multivariate case univariate version would much complex necessary would also produce onedimensional rings unions subintervals necessarily neighboring unpleasant turned unnecessary modified algorithms take advantage simplicity real line topology simply merging neighboring intervals thus resulting partitions consisting intervals algorithms cannot easily generalized multivariate setting since procedure emerging neighboring cubes may generate complicated undesirable sets partition also makes much dicult establish jackson inequalities local ap proximants refer interested reader 21 one find proof jackson inequality ring already dicult enough reasons strongly believe simpler ecient univariate algorithms necessary 2 preliminaries throughout paper say f function approximated belongs l p mean f l p 0 p f ci r integer 0 r 0 p q besov space set functions f l p semiquasinorm sup finite r usual rth modulus smoothness quasinorm defined also define short notation special case used frequently theory potential confusion especially case 0 1 interval omitted notation sake simplicity example l p stands l p 0 1 quasinormed complete linear spaces continuously embedded hausdor space x kfunctional f defined kf x 0 generalized replace x1 quasiseminorm x1 kf x 0 modified adaptive algorithms 1015 interpolation space x 0 consists functions f x0x1 q sup studying approximation method revealing know approximation spaces define let functions quasinormed linear space x approximated elements subsets n necessarily linear required satisfy assumptions 0 depend n v n0 n dense x vi f x best approximation n approximant sets paper satisfy assumptions denoting define approximation space set f x e n f order n sense following seminorm finite f sup general theorem enables one characterize approximation space merely proving corresponding jackson bernstein inequalities see 13 sections 75 79 9 15 theorem let 0 linear space semiquasinorm continuously embedded x n satisfies six assumptions satisfies jackson inequality bernstein inequality 0 0 q approximation space partition interval 0 1 mean finite set subintervals whose union n nonlinear yk hu k kopotun x yu spaces nr pp functions order r 0 1 n 0 pieces defined ip x x p n p p r1 space polynomials degree r characteristic functions 0r defined 0 assumptions ivi n see p 3 degree best approximation function f elements nr denoted nr f p ef nr p remark authors use notation n1rr place nr since pp functions viewed special kinds splines interior break point x knot multiplicity r also use pp nr following general notation nonlinear approximation use first subscript number coecients approximant see 13 14 17 26 strictly speaking n piece pp function order r form proper subset free knot spline space n1rr subset approximation power l p whole space see theorem 1242 13 1988 paper 23 also see 24 13 section 128 petrushev characterized approximation space using besov spaces see following theorem theorem b let 0 p n 0 0 r therefore 0 q 0 r particular inequality 24 proved finding balanced partition according function f x dsdt sense see 13 details proof fact many jacksontype inequalities proved showing existence balanced partition see eg theorems 1243 5 6 13 theorem 11 19 parts theorems 21 41 17 state theorem 1246 13 given burchard 8 1974 case see also de boor 3 modified adaptive algorithms 1017 theorem c let r n positive integers let r monotone function p space functions f l p 0 1 variation f v p ip finite sup taken finite partitions p 0 1 following 17 see also brudnyi 7 bergh peetre 1 define modulus smoothness f 0ht sup following theorem due devore yu 17 provides characterization using interpolation spaces involving v p theorem let 0 p 0 r approximation elements nr 0 jackson inequality bernstein inequality therefore particular p jackson inequality 212 follows definition f p existence f v p nr n proved see 17 showing existence balanced partition defining p best l p approximations f space p r1 yk hu k kopotun x yu 3 adaptive algorithms 31 original adaptive algorithm likely hard find exactly balanced partition numerically algorithm sort hu 20 instance uses two nested loops another level loop increases number knots probably one reasons much attention paid adaptive approximation selects break points repeatedly cutting intervals two equal halves produces pp functions dyadic break points represented finite binary numbers form 2 k denote spaces pp functions nr approximation errors ef nr f p describe original adaptive algorithms univariate setting let e nonnegative set function defined subintervals 0 1 satisfies ei ej j uniformly 0 given prescribed tolerance 0 say interval good ei otherwise called bad want generate partition g g e 0 1 good intervals 0 1 good desired partition otherwise put 0 1 b temporary pool bad intervals proceed b divide every interval two equal pieces test whether good case moved g bad case kept b procedure terminates resulting intervals good g guaranteed happen 32 set function ei usually depends function f approximated measures error approximation f gx dx 29 thus called error measure throughout paper simplest case ei taken local approximation error f 0 1 polynomials degree r ei corresponding approximant g defined 215 gives error g number intervals g one estimate dierent ways n f p n f e p inf g 1p infimum taken 0 solomjak 2 devore 12 estimates functions f sobolev spaces estimates found rice 25 de boor rice 6 devore yu 18 references therein mention following two results theorem e see 18 theorem 51 let f c r 0 1 f r x x l monotone function modified adaptive algorithms 1019 c 1 absolute constant n f cn r note compared theorem c theorem extra requirement 35 theorem f see 18 corollary 33 let 0 p 0 q l q see 37 weaker 24 free knot spline approximation reason hard see adaptive algorithms select break points smaller set numbers set finite binary numbers also special order consider example good free knot approximant knots close 0 see examples 20 table 52 later paper however adaptive algorithm needs least n 1 knots 2 put one 2 n thus needs knots free knot spline algorithm although one classifies adaptive approximation special kind free knot spline approximation since knots sequence depends function approximated one far free choosing knots considered restrictive devore popov 14 free knot spline approximation point theorems mentioned subsection jacksontype socalled direct theorems bernstein inequalities closely related inverse theorems sometimes referred also inverse theorems free knot splines 25 213 valid splines including pp functions produced adaptive algorithms problem jackson inequalities original adaptive algorithms strong enough match bernstein inequalities sense theorem point view theorems e f weaker look know exactly kind functions approximated original adaptive algorithms prescribed order characterize approximation spaces q fully exploit power nonlinear approximation sometimes generate many intervals many may error measure much smaller mentioned two major aspects adaptive approximation dierent free knot spline approximation smaller set numbers choose knots b special restrictive way select knots set turns b reason drawback although also reason adaptive approximation simple want keep way mean keep knots produces paper modify usual adaptive algorithm two ways idea splitting merging intervalscubes used recent paper cohen et al 11 two new algorithms generate partitions 0 1 fewer dyadic knots nearly balanced sense section 4 prove approximation order free knot splines 32 algorithm start original adaptive procedure 0 generates partition 0 1 good intervals number n may much larger decrease merge intervals begin 1 check union 1 2 still good interval measure ei add 3 union 1020 yk hu k kopotun x yu check whether ei 3 proceed find largest good union k sense ei name 1 k 1 k n continue k1 find next largest good union 2 end procedure obtain modified partition consisting n n good intervals union j i1 bad partition considered nearly balanced size n 33 algorithm ii second algorithm generates nearly balanced partition another way make heavy use prescribed tolerance rather merges intervals relatively small measures dividing large ones ordinary adaptive algorithms start dividing 0 1 two intervals 1 2 equal length however similarity ends compare measures ei 1 ei 2 divide interval larger measure two equal pieces case equal measure divide rather randomly one left three intervals ready threestep loop step 1 assume currently partition k j largest measure among ei j1 fixed parameter check union j1 j2 see whether measure ei j1 j2 add next interval j3 union check measure continue get largest union jm1 whose measure less replace union intervals contains j 1 k find next largest union jm1m2 manner replace intervals union furthermore intervals left j keep j intact way obtain new partition old j still largest measure partition nearly balanced sense measure union two consecutive new intervals less new intervals largest unions old intervals end step renumber new intervals update value k step 2 check whether new partition produced step 1 satisfactory using applicationspecific criterion instance whether k reached prescribed value n error reduced certain level continue step 3 otherwise define final spline 215 terminate algorithm step 3 divide interval largest measure two equal pieces renumber intervals update values k go back step 1 remark step 1 l l1 two newest intervals two brothers equal length one needs check l1 l l 1 l j andor l1 l2 since unions two consecutive intervals measures modified adaptive algorithms 1021 less value previous iteration turn less current stated way shows purpose step clearly pointed one needs careful stopping criterion algorithm ii example applied characteristic function two iterations always break point 22 example replaced number 0 1 finite binary representation 04 k used sole stopping criterion algorithm fall infinite loop fortunately error example still tends 0 therefore infinite loop avoided adding error checking criterion next lemma shows case general lemma 31 let e interval function satisfying 31 32 let prescribed criterion terminate algorithm ii proof show k never exceeds n number iterations goes let 0 1 fixed let max taken one moment fix denote group subintervals partition large errors g step 1 changing iteration iteration first make observations since interval currently largest measure always g iteration cuts member g however algorithm merge member g another interval ei union another interval would even larger measure 31 32 exists 0 g note intervals partition disjoint thus total length intervals g larger 1 cardinal number g 1 observations conclude following iteration cuts member g two children equal length one three cases happen neither child belongs g thus removed total length g exactly one children belongs hence length child length 2 removed g c children belong case decreases g 1 b keeps unchanged c increases 1 one see 3 1 iterations empty g since least one third cases b keep g 1 remove total length g thus emptying reduces maximum error factor 1 repeat enough times maximum error eventually tend 0 although 32 say anything convergence rate ei 0 proof lemma may make sound extremely slow one expect fairly fast convergence cases example case f generalized lipschitz space lip p lip l p b 0 r f lip f lip p sup 1022 yk hu k kopotun x yu b f lip feel safe say functions applications belong lip p reasonably away 0 least subintervals containing singularities thus halving interval often reduces error factor 2 natural question may arise complex new algorithms give brief comparisons answer question algorithm straight ward original adaptive algorithm second merging phase added phase consists merging attempts n number subintervals original algorithm generates n final subintervals algorithm ii two major dierences original version first one mentioned remark algorithm description two merging attempts made cutting interval one bookkeeping original version vector needed record errors intervals indicate intervals bad algorithm ii keeps index interval largest error ei scalar variable addition vector containing errors requires search largest element vector cutting merging operation one see new algorithms much complex terms programming steps added cpu time terms number results mainly evaluations error measure ei required merging operations estimate either algorithm uses two three times much cpu time original algorithm information cpu time given section 5 together numerical details 4 approximation power algorithms show modified adaptive algorithms full power nonlinear approximation precisely prove produce piecewise polynomials satisfying jackson inequalities free knot spline approximation possibly larger constants righthand side since partitions exactly balanced mentioned earlier corresponding bernstein inequalities hold true splines therefore really proving approximation spaces modified adaptive algorithms free knot spline approximation state results three main theorems parallel theorems b c respectively fact prove results kind algorithms kahanes theorems generalization 13 theorems 1243 5 proofs would similar ones recall throughout paper j denotes interval largest measure among partition union two consecutive intervals j measure ej ei j j called bad algorithm pp functions resulting partitions defined 215 theorem 41 let n r positive integers let 0 p 0 r two modified adaptive algorithms defined 28 ii functions 215 satisfy jackson inequality 4 modified adaptive algorithms 1023 theorem obtain approximation space product turns surprising since nr dense nr surprising part one get approximant using simple adaptive algorithm corollary 42 let 0 p 0 q 0 r approximation pp functions nr particular proof theorem 41 proofs theorem cases ii similar consider remark case ii inequality plays major role pp approximants produced algorithm let ei gx dx g 28 claim number n intervals produces greater 2n1 indeed 38 rest proof 41 similar 24 cf section 128 p 386 13 sketch completeness proved 13 f b 0 1 equivalent f b 0 1 constants equivalence depending r f define approximant 215 fifth step used equality last step used equivalence f b pp approximants produced algorithm ii let ei use 39 stopping criterion step 2 algorithm terminates due 1024 yk hu k kopotun x yu thus giving less n pieces situation algorithm otherwise n pieces terminates 41 follows cn theorem 43 conditions theorem c modified adaptive algorithms ei x dx n produce pp approximants nr satisfy jackson inequality proof theorem 43 pp approximants produced algorithm let ei taylor polynomial f degree r 1 point x i1 best see equation 415 chapter 12 13 using 46 place 44 45 p proved arguments similar proof theorem 41 algorithm also refer reader proof theorem c 13 estimate n need replace n pp approximants produced algorithm ii let ei use 39 stopping criterion step 2 algorithm terminates situation algorithm otherwise p modified adaptive algorithms 1025 used inequality 46 second step last one make similar changes algorithm theorem 44 let n r positive integers let 0 p 0 r two modified adaptive algorithms ei p produce pp functions 215 satisfy jackson inequality using theorems 44 following characterization corollary 45 approximation pp functions nr particular p proof theorem 44 suces show 214 since 47 immediately follows 0 n see end section 2 prove p case verified making changes similar proof l case previous theorem pp approximants produced algorithm let ei p p 38 number n intervals algorithm produces estimated indeed n 2n otherwise done p cn p p used definition 211 f p since 1 p gives cn 214 follows since p pp approximants produced algorithm ii set ei e r f use p stopping criterion 39 stops exactly situation algorithm 1026 yk hu k kopotun x yu partition otherwise n intervals terminates latter case 5 numerical implementation examples theoretically two algorithms approximation power however comes numerical implementation prefer algorithm ii since directly controls number polynomial pieces n algorithm neither power n tolerance though closely related implemented algorithm ii computer using fortran 90 mainly 2 error measure used code 2 unless better one use f r square root function first example section l 2 norm f interval estimated composite simpson rule integral l norm estimated equally distributed nodes n p program parameter roughly set 6 times r best l 2 polynomial approximant discretized 51 overdetermined n p r system linear equations least squares method calculated either qr decomposition singular value decomposition calling linpack subroutines sqrdc sqrsl ssvdc double precision counterparts latter takes longer see dierence first four five digits local approximation errors computed thus test extensively l version algorithm basically except use estimated 52 local polynomials p global smooth splines still obtained least squares method still best l 2 approximants common literature justified fact best l 2 polynomial approximant also nearbest l polynomial interval see lemma 32 devore popov 16 number polynomial pieces used main termination criterion 39 set small value mainly protect program falling infinite loops rather sophisticated ones proofs previous section turned infinite loop problem nonfull rank matrix least squares method problem happens far falls infinite loop small machine diculties distinguishing n p modified adaptive algorithms 1027 points needed 51 therefore added third condition protect program failing stop program also added second part code namely finding l 2 smooth spline approximation function knot sequence nr i2r interior knots 2 3 n b break points pp function obtained algorithm ii used single knots auxiliary knots set b despite fact partitions guaranteed good pp functions usually work well smooth splines de boor gave theoretical justification discussion subroutine newnot 4 chapter xii least square objective function finding smooth spline set 5r1 number equal pieces cut subinterval points resulted cutting weights w j chosen 54 becomes composite trapezoidal rule integral b fx dx actual calculation bspline coecients bsplines knot sequence scaled done de boors subroutine l2appr 4 chapter xiv used source code subroutines book package pppack internet tested code sun ultrasparc clock frequency 167mhz 128mb ram running solaris 251 speed fast issue finding break points somewhere 0015 second 01 second printing minimum amount messages screen less 10 computing smooth splines also tested code 300 mhz pentium ii machine 64 mb ram running windows nt 40 speed least three times fast none problems tested used 01 second reason great dierence speed may sun used file server ideal numerical computation still room improvement eciency example one use value n p larger use beginning decrease n increases error subinterval decreases value n related n reason main cost cpu time evaluation error measure ei subinterval use estimated qr decomposition exam ple problem involves n p function evaluations n p r operations required qr decomposition plus estimating error resulting matrices cutting intervals requires two ei evaluations 1028 yk hu k kopotun x yu table approximation order merging attempt requires one numerical experiments show typical run resulting n subintervals cuts intervals 2n time cutting results two attempts merging subintervals gives 8n least squares problems involves n p function evaluations plus n p r 2 arithmetic operations view approximation order proved previous section fact n p roughly multiple r think pays use relatively large r least 4 5 5 error reach machine epsilon single precision n somewhere 30 70 cases use square root function test pp function approximation order function lipschitz space lip 1 thus approximation order 12 splines equally spaced knots l norm matter order r theorem 43 e n f n p cn r n function consisting n polynomial pieces computed algorithm ii using f r x dx combined theorem constant c knot sequence found qr decomposition used end program subinterval estimate e n since error decreases fast double precision used qr decomposition large values n assume actually obtain code e approximation order since log e plot points plane form line since plot zigzags much calculated least squares line fitting find order table 51 gives values dierent r using l 2 l norms mention points values n low ruin obvious line pattern formed larger n thus give two values one points seen table latter values right around even exceed r remark tried power e r f p ei felt view 46 would yield better balance subintervals thus higher order orders obtained well r 446 eg reason might additive power e r f p illustrate advantage interval merging compare original adaptive algorithm modified ones function log 2 function c decreasing convex 0 1 note since f decreasing 0 1 table 52 shows comparison numbers knots produced approximation error original adaptive algorithm algorithm ii programs try put first knots near graph steep original algorithm pointed early lay knots 2 one one reaching error modified adaptive algorithms 1029 table comparison numbers interior knots produced original modified adaptive algorithms error approximating original alg algorithm ii trying knots one time merging last interval puts first knot 2 23 interesting watch algorithm ii moves knot toward better position successive iterations without increasing total number pieces following screen output shows iterations 1 2 program moves break point 05 025 0125 error decreases form 05 047 iterations 322 moves break point way 2 23 error decreased 027 happened internally iteration 1 eg cuts interval 0 05 0 025 025 05 since error union 025 05 05 1 smaller 0 025 merges two intervals 025 1 net eect steps moving break point 2 1 2 2 iteration 0 errors linfty error iteration 1 errors linfty error iteration 2 errors linfty error many lines deleted iteration 22 errors 270000e01 230000e01 linfty error yk hu k kopotun x yu table approximation errors runge function 5 5 9 consider infamous runge function also c hand hard interpolate approximate lyche mrken 22 approximated knot removal algorithm hu 20 approximated balancing rth derivative function subintervals two nested loops rest paper use 4 table 53 compare results lyche mrken lm 22 hu 20 number knots list errors measured 2 b pp function n smooth spline also measured l norm divide l 2 norm since comparable l norm lm hu used errors lm estimated figures 22 simple nature algorithm expected compete results splines two three times many knots turns approximation errors almost good produced sophisticated methods reader may begin wonder eect parameter algorithm ii used lemma 39 guarantee termination algorithm ii tried functions tested worked excellently except number polynomial pieces went times square root function using dx case used instead true theory might get infinite loop since goal find nearly balanced partition better aspect provided infinite loop happen matter fact sometimes feel need value slightly larger 1 eg symmetric functions runge function happens 1 two subintervals largest measure moment symmetric center interval outcome next iteration processes subinterval left often interfere processing subinterval right later may make approximation error worse least much knot sequence becomes unsymmetrical thus unnatural unpleasant furthermore algorithms literature produce symmetric knots symmetric functions would hard compare results minor reasons used preparation table 53 next example consider pp function jump 22 mentioned discussion lemma 31 since 22 finite binary representation function never approximated exactly pp function dyadic break points program cutting merging around jump since number pieces always 3 two iterations stopped criterion 53 resulting modified adaptive algorithms 1031 500 600 700 800 900 1000 110006114182204 temperature fig 51 titanium heat data circles final spline solid line 15 interior knots errors preapproximation dotted final spline dashed use scales right 070710754 pp function matches f exactly computer screen since two points indistinguishable one well combine single break point thus virtually reproducing f original adaptive algorithm contrast would put many many knots around jump trying narrow subinterval containing jump 05 075 0625 06875 071875 knots useless except newest two practice one often wants approximate discrete data points known functions previous examples case preapproximate points spline many parameters wish use apply algorithm spline smoothlooking data interpolate data c 1 cubic spline knots data points using de boors subroutine cubspl 4 worked well produced sample data points runge function square root function applied approach resulted virtually knot sequences generated directly approximating original functions real world however likely data contain errors data points interpolated one see small wiggles graph tricks program laying knots areas curve otherwise flat one example titanium heat data experimentally determined see 4 chapter xiii also lm 22 hu 20 figure 51 reader see wiggles left right de boor 4 chapter xiv suggests data approximated less smooth spline absolutely agree reason used fewer knots preapproximating spline flat parts ends near high 1032 yk hu k kopotun x yu peak around 900 trying ignore wiggles fact used almost knot sequence preapproximating spline figure 4 20 table approximation errors titanium heat data obtained order knots error alg ii 4 11 0070 alg ii 4 15 0031 since de boor lm hu used l norm approximating data also used l version program figure 51 shows cubic spline approximation titanium data obtained method 15 interior knots error 0031 table 54 gives comparison results others data acknowledgments deeply indebted professor ron devore inspired us discussing excellent ideas 11 visit university south carolina want thank professors pencho petrushev albert cohen providing us drafts manuscript 11 credit also due professor dietrich braess editor paper referees whose opinions suggestions helped much improving manuscript matter fact reshaped last section communication r space vp 0 piecewise polynomial approximation functions classes w good approximation splines variable knots practical guide splines least squares cubic spline approximation iivariable knots adaptive algorithm multivariate approximation giving optimal convergence rates spline approximation functions bounded variation splines optimal knots better jackson bernsteintype inequalities families commutative operators banach spaces adaptive wavelet methods elliptic operator equationsconvergence rates nonlinear approximation space bv r 2 note adaptive approximation function spaces applications interpolation besov spaces degree adaptive approximation convexity preserving approximation free knot splines algorithm data reduction using splines free knots multivariate adaptive approximation data reduction strategy splines applications approximation functions data direct converse theorems spline rational approximation besov spaces rational approximation real functions basic theory tr