parallelizing algorithms symbolic computation using maple maple speak parallel maple portable system parallel symbolic computation system built interface parallel declarative programming language strand sequential computer algebra system maple thus providing elegance strand power existing sequential algorithms maple implementation different parallel programming paradigms shows fairly easy parallelize even complex algebraic algorithms using system sample applications among algorithms solving multivariate nonlinear equation systems implemented various parallel architectures example straightforward parallelization complex important problem real root isolation parallelized using generic strand program fewer 20 lines code slight modification 5 lines original sequential maple source even simple modification gained speedup 5 times better reported others literature b introduction symbolic computation important part applied thematics physics engineering areas nowadays sequential computer algebra systems like maple mathematica reduce widely used currently possible solve small examples inherent complexity problems symbolic computation particular lot problems involving nonlinear equation supported austrian science foundation fwf espritiii posso project systems cannot solved symbolically within reasonable amount time using todays computers furthermore lot algorithms even possible predict runtime memory demands given input deal problems obvious solution parallelization todays fast parallel computers theoretically able solve huge problems solving large problems face following situation ffl sequential computer algebra systems big sequential libraries fast implementations basic algorithms yet suitable parallel computers ffl parallel programming languages well suited parallel programming libraries solving problems symbolic computation thus goal reuse huge library existing computer algebra systems additionally get easy access power todays parallel computers several attempts parallelizing maple made watt 1986 unix fork join primitives used parallelism programmer develop parallel algorithm low level improvement done char 1990 linda used linda hardware independent programmers still worry creation parallel processes communication system implemented shared memory machine still problems communication job creation overhead also several parallelizations computer algebra systems available c based sac algebra library parallelized shared memory machines implementation kuechlin 1990 called parsac uses mach thread routines parallelization parallelization hong et al 1992 schreiner hong 1993 called includes several high level functions communication process generation systems provide best efficiency available distributed memory machines contrast kmaplek siegl 1993 especially designed distributed memory architectures kept absolutely portable kmaplek programs may run different hardware without modification recompilation necessary communication done automatically system without additional programming effort since kmaplek uses implicit parallelism allows writing parallel programs without expert knowledge parallel programming disadvantage compared shared memory systems design introduces higher overhead communication additionally bigger memory overhead due need copying required data maple strand heaps achieve goal developed interface maple char et al 1983 parallel programming language strand foster taylor 1989 result parallel programming system full functionality maple parallel power strand method allows easy writing porting parallel algorithms give several sample programs describing different techniques quick parallelization algorithms kmaplek experiments done 20 processor sequent shared memory machine tested network sun workstations well 16processor transputer distributed memory machine showed good speedup minimal programing effort 2 kmaplek system kmaplek system two layers figure 1 top layer parallel declarative programming language strand controls parallel execution algorithm performing sequential tasks may call arbitrary maple functions sequences maple statements underlying maple system method combines advantages strand well maple 21 strand strand parallel programming language guarded horn clause type means basic construct clause bodies b j executed parallel soon guards g fulfilled strand pattern matching used instead unifica tion assignments used describe output bindings increase performance data dependencies expressed dataflow synchronization stream commu nication additionally strand allows use arbitrary sequential subroutines written c fortran guards body calls describe basic communication features give short example program type producer consumer parallelism request interested reader may find details strand foster taylor 1989 main producerchannel consumerchannelfwd producerxxs get empty variable x createx generate value x producerxs outxxs generate empty var x usex use value x consumerxs mode create generate value mode use take action mode declaration procedure says argument annotated used input matching arguments annotated used output bin dings declaration must used pattern used output allow dataflow synchronization fwd annotation means annotated goal executed next processor starting main procedure generates two parallel subtasks producer consumer connected via shared stream variable channel consumer process sends request data instantiating first cons cell stream tells producer create value stream element x soon finished consumer process use value producer consumer 22 maple maple well known computer algebra system one biggest libraries available known algorithms symbolic computation already implemented maple maple programming language supports functional well procedural programming styles pascallike syntax maple kernel written c linked subroutine c programs input output may handled via strings representing maple expressi ons information may found char et al 1983 char et al 1988 23 kmaplek kernel main task kernel system find minimal set interface routines combining syntactic strand parallel programming system iointerface maple iointerface iointerface maple maple figure 1 structure kmaplek system semantic features parallel declarative programming language strand imperative sequential language maple finally came interface consisting one new strand guard two bodies ffl maple body call takes arbitrary maple function arguments input returns result computed maple procedure covers sequential calls maple used parallelized program using call sequential kmaplek program symbolic integration example would look like intfxresult mapleintfxresult ffl extend nondeterministic guard check maple expressions need test guard takes boolean maple function succeeds function evaluates true fails otherwise able write strand program finding maximum arbitrary maple data structures statements since also use guard operation solving arbitrary constraints guard kmaplek may also used constraint logic programming language ffl complex sequential operations involving several input output arguments use inline operation allowing inclusion sequence arbitrary maple statements additionally may specify variables used input ones output tuple form inmvarsvar means content strand variable svar assigned maple variable mvar maple program executed contrary outmvarsvar tuple stands assignment maple expression denoted mvar strand variable svar computation procedure collects output expressions returns list result one finally finds symbolic expression result wants generate executable c program evaluating result kmaplek program looks like inlineintmpexpr readlibcctmpoptimizedcprog critical part interface data type used communication possible manipulate data within strand well maple additionally efficient communication parallel tasks require structure may packed array easily communication primitiva distributed systems may transfer arrays single step decided use strings containing maple expressions legible form communication strand maple experiments found method allows easy porting developing parallel algorithms used symbolic computation 3 parallel programming kmaplek describe important programming concepts necessary parallelize algorithms symbolic computation kmaplek reader basic knowledge parallel logic programming understand given programming examples 31 generating parallel tasks beginning parallel program split several tasks single incoming expression kmaplek done help inline function example consider problem integrating sum expression symbolically integral may easily parallelized computing individual terms parallel kmaplek generate independent tasks following procedure splitintsumexprtermlist inlineinsumsumexpr term sum term odtermlist inline call assigns sumexpr maple variable sum inlined maple statement print individual terms collect termlist result list understandable strand part parallel manipulation left list independent terms according integration rules may compute individual terms independently 32 parallel evaluation simplest parallel program consider case may compute tasks independently parallel continue integration example give program distributing data parallel shell simplicity assume integral solved variable x mode parintinputoutput parint parintttsyys mapleinttxy parinttsysfwd first step split list single elements return list unbound variables result immediately individual values variables known stage integrate terms input list parallel result values bound corresponding variables result list soon individual processes integrating single terms finished 33 combining result finally combine individual subexpressions single expression example done following procedure combineintresultlistresultexpr listtomapleresultlistmaplelist mapleconvertmaplelistresultexpr builtin kmaplek call list maple synchronizes resultlist converts elements maple list expression finally convert list back sum expression using corresponding maple function whole parallel integration program looks like sumintsumexprresult combineintresultlistresult improve performance may use dynamic load balancing scheme based managerworker concept see section 43 experimenting example shows higher amount garbage collection sequential system allows superlinear speedup low number processors maple ability remember previously computed functions need compute function twice fact parallel system cannot utilize feature unbalanced nature problem restricts speedup achievable high number processors 34 meta programming methods maple ability return partially evaluated functions use fact already sequential program available would like parallelize minimum effort instead immediately evaluating functions return evaluated arguments control process picks partially evaluated functions distributes parallel processors example consider parallelization following maple procedure procedure generates value var returns list three elements computed independent sub procedures p13 parallelizing procedure use generic parallel evaluation procedure takes list maple statements input returns list results mode parlistinputoutput parlist parlistexprexprlistyys inlineexpry parlistexprlistysfwd parallelized maple program looks like maple evaluates variable var returns independent sub procedures p13 unevaluated complete program combine strand maple part exampleresult mapletolistresultlistresult efficient parallelization complicated maple programs also implemented generic versions parallel evaluation generalized divide conquer see section 41 method based dynamic load balancing using managerworker scheme see section 43 4 main parallelization techniques describe several programing techniques used parallelizing algorithms kmaplek technique explained example important algorithm used symbolic computation 41 divideconquer real root isolation important problem symbolic computation real root isolation problem goal procedure finding rational intervals around real roots polynomials real coefficients interval contains single root maple done uspensky procedure collins loos 1982 algorithm divide conquer style idea parallelizing divideandconquer algorithm clear input split independent subproblems type subproblems solved different nodes recursively solve subproblem base case reached leads tree structure parallel processes uspensky procedure splits range roots two pieces calls recursively found isolating interval since roots normally equally balan ced one half worth search fact expect high speedup problem parallelizing algorithm used generic divide conquer algorithm figure 2 based meta programming technique 34 modified original maple algorithm way maple returns recursive calls unevaluated instead computing result calls distributed parallel processors computation detail maple routine mcall called generic divide conquer algorithm divconq simply print recursive calls unevaluated collected strand list parts additionally global maple variable compose fun assigned function composing evaluated parts case base case compose fun assigned 0 base case yet reached split comp procedure collects results recursive calls split procedure transfers maple list mlist finally composition function compose fun returning result executed divconqmcallresult inlineoutcompose funcomp mcallparts divide split compcomppartsresult mode split comp split comp0xx base case 0 function split compcomppartsresult splitpartsresults wait subproc comp list mapleresultsmlist compose partial results inlineinin argsmlistcompres strip listresresult mode split splitxjxsyjys divconqxy splitxsysfwd split figure 2 generic divideconquer algorithm function access results recursive calls maple list variable args improved version algorithm uses dynamic load balancing algorithm based managerworker scheme recursive calls give results instances chebyshev polynomials 20 processor sequent shared memory machine times given seconds total execution time including time garbage collection loading required modules numbers parentheses denote speedup gained maple v kmaplek expected analysis program speedup gained high compared paral lelizations reported literature char 1990 maximum speedup 17 examples achieved found rather successful better performance comes mainly lower communication programming overhead system 42 pipelining grobner bases computation grobner bases algorithm invented buchberger 1965 one important time intensive algorithm symbolic computation used solving many problems polynomial ideal theory particular used symbolic solution arbitrary nonlinear multivariate polynomial equations parallel implementation grobner bases algorithm described siegl 1990 siegl 1991 uses pipeline principle polynomial reduc tion give sketch overall method details proofs given siegl 1990 pipeline algorithm test modify input respect set properties test modify part may like well known concurrent prolog implementation sieve eratosthenes shapiro 1987 append tested values set order use testing rest input cases result test used generating new input values typical examples critical pair completion procedures set critical pairs built tested respect given set properties pairs canceled appended test set new critical pairs built parallelizing method assign element test set one parallel process connected pipe input values driven subsequently pipe intermediate process takes first element previous neighbor tests modifies cancels result sent next process waits next input computing result whenever value reaches end pipeline possible extend test set simply creating new process holding new value end pipe thus future input values tested respect new value necessary send computed values back pipe generating new input intermediate pipe process generate new input values send back front end front end process takes new values coming back pipe sends pipe sequential grobner bases algorithm critical pair completion algorithm given set input polynomials initial basis one computes critical polynomials reduces respect basis polynomials resulting polynomials added basis new critical pairs formed parallel version pipeline processes reducing input polynomials respect increasing set basis polynomials irreducible input values appended reduction processes generating new critical polynomials called spolynomials input new polynomials joining basis sent back pipe building spolynomials earlier computed basis polynomials process front pipe needs collect computed spolynomials sends reduction processes result get bidirectional pipe forming dyna mode maininpolybasis maininpolybasis main loop append spolinpolyspoltestpoly top end sievetestpolyspolbasisfwd back end mode sievetestpolysspolysbasispolys poly filterbptpsspsrpssps1 sieverpsspsbpsfwd extend pipe reduction sievetpsspsb inpoly append spol oe oe spolys testpolys basispolys f iltern oe sieve basis oe figure 3 organization pipe computing grobner bases mically expandable ring organization pipe compute grobner basis shown figure 3 detail main procedure generates top append spol back end sieve process pipe top end process collects newly generated polynomials spol pipe appends initial input polynomials inpoly sends whole pipe intermediate filter processes take po lynomial reduces respect basis polynomial returns result next pipe process soon back end receives non zero polynomial extends pipe new intermediate filter process sends new basis polynomial bp back pipe generating new spolynomials polynomials reducing zero canceled soon polynomials flowing pipe algorithm terminates benchmarks done standard example literature boege et al 1986 computed different term orderings total degree pure lexicographic term order example order kmaplek trinks2 tdeg 125 53 24 28 45 katsura3 plex 147 38 39 11 134 katsura3 tdeg 196 51 38 14 140 grobner bases algorithm nondeterministic havior possible decide advance order reduction chosen implementation polynomials flowing pipe allowed overtake reduced faster general turns using faster generated therefore simpler polynomials earlier reduces overall computation time may lead superlinear speedup 43 managerworker parallel interactive user interface kmaplek may used developing parallel algorithms even possible parallelize maples interactive user interface way several user queries fullfiled simultaneously user interface kmaplek internally parallelized using manager worker scheme parallelism shelln shell n workers streams init workernworkers spawn n workers shellinputwork get user request mode init worker init worker0 init workernmergerequestjrs n0 j workerrequestn place worker n1 n1 init workern1rs managerfmexprresgjworkmexprmresjrequest exprmexpr send expr worker resmres send result user managerworkrequest mode worker workerxmmexprresultjrequest instance per proc workerresultrequest figure 4 parallel user shell managerworker approach used parallelizing algorithms big set independent parts limited number nodes used computing time various parts may vary load equally distributed among nodes many problems may parallelized methods may also parallelized using managerworker approach practice method often used avoiding load balancing problems shell user request processed different node computing time request vary greatly parallelizing user interface figure 4 use set worker processes subsequently solve one query worker requests piece work manager pro cess manager responds request next user query whenever one worker finished job sends result back manager receives new query allows user make queries without wait results detail spawn set workers init worker manager process whenever worker idle sends request new work mmexprresult tuple empty variables merger combines individual requests workers single stream connected mana ger whenever manager gets new input compute fmexprmresg finds worker requesting new work sends work corresponding worker exprmexpr returns location result ioprocess resmres worker may compute user quest soon ready result sent directly ioprocess new request work sent manager actual implementation kmaplek user terface may also describe dependencies parallel tasks parallelized user interface advantage user may type one query never blocked waiting results one give objective values performance gained user interface soon one type queries faster whole system solve one gain linear speedup subjective performance due non blocking behavior much higher measurements tell gives significant speedup problem solving capacity 5 conclusion showed usage system parallelizing algorithms symbolic computation simple elegant way several programming techniques given developing parallelizing algorithms found even quick parallelization possible get linear speedup long enough inherent parallelism grain size large enough due high level approach used kmaplek could keep algorithms portable across different parallel architectures since kmaplek available many parallel computers ranging shared memory systems several distributed memory systems workstation networks able develop algorithms one machine architecture test different others without modification utilizing whole sequential library maple enabled us parallelize rather complicated algorithms various tests found main problem achieving high speedup find proper division strategy algorithm exploit inherent parallelism algorithms symbolic computation efficiency 50 percent may successful result utilizing coarse grain parallelism actual communication time system minor influence performance currently interface uses strings communication introduces small linear overhead parsing parsing transferring example factorial 1000 2568 digits long number strand maple back requires approximately 03 sec i386 pro cessor whereas squaring number maple requires around 3 seconds independent operations sequential maple kernel large enough cover overhead parsing expressions strings suitable inter later releases use linearized form internal data structure communication allow finer granularity also investigate various algorithms detail try find better parallelization strategies better efficiency r examples solving systems algebraic equations calculating grobner bases ein algorithmus zum auffinden der basiselemente des restklassenringes nach einem nulldimensionalen polynomideal algorithm finding basis residue class ring zerodimensional polynomial ideal german design maple compact progress report system generalpurpose parallel symbolic algebraic compu tation real zeros polynomials ian foster stephen tay lor paclib user manual parallel sac2 based threads concurrent prolog parallel grobner bases computation kmaplek bounded parallelism computer algebra tr real zeroes polynomials examples solving systems algebraic equations calculating groebner concurrent prolog collected papers progress report system generalpurpose parallel symbolic algebraic computation new concepts parallel programming design maple ctr wolfgang schreiner christian mittermaier karoly bosa distributed maple parallel computer algebra networked environments journal symbolic computation v35 n3 p305347 march