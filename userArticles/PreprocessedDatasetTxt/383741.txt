recursive formulation cholesky factorization matrix packed storage new compact way store symmetric triangular matrix called rpf recursive packed format fully described novel ways transform rpf standard packed format included new algorithm called rpc recursive packed cholesky operates rpg format presented algorithm rpc basd level3 blas requires variants algorithms trsm syrk work rpf call rptrsm rpsyrk find work calling gemm follows execution time rpc lies gemm advantage storage scheme compared traditional packed full storage demonstrated first rpc storage format uses minimal amount storage symmetric triangular matrix second rpc gives level3 implementation cholesky factorization whereas standard packed implementations level 2 hence performance rpc implementation decidedly superior third unlike fixed block size algorithms rpc requires block size tuning parameter present performance measurements several current architectures demonstrate improvements traditional packed routines also msp parallel computations ibm smp computer made graphs attached section 7 show rpc algorithms superior factor 16 74 order around 1000 19 103 order around 3000 traditional packed algorithms architectures rpc performance results almost even better traditional fullstorage algorithms results b introduction important class linear algebra problems coefcient matrix symmetric positive denite 5 11 23 symmetry necessary store either upper lower triangular part matrix lower triangular caseb 7 14 21 28 upper triangular caseb 43 9 figure 1 mapping 7 7 matrix lapack cholesky algorithm using full storage lda 7 fortran77 lower triangular caseb 3 9 14 upper triangular caseb figure 2 mapping 7 7 matrix lapack cholesky algorithm using packed storage 11 lapack potrf pptrf subroutines lapack library3 oers two dierent kind subroutines solve problems instance potrf 1 pptrf factorize symmetric positive four names spotrf dpotrf cpotrf zpotrf used lapack real symmetric complex hermitian matrices3 rst character indicates precision arithmetic versions single precision double precision c complex z double complex lapack95 uses one name la potrf versions7 potrf andor pptrf express paper precision arithmetic language version po andor pp matrix factorization algorithms denite matrices means cholesky algorithm dierence way triangular matrix stored see gures 1 2 potrf case matrix stored one lower left upper right triangles full square matrix16 page 64 2 triangle wasted see gure 1 uniform storage scheme blocking level 3 blas8 subroutines employed resulting high speed solution pptrf case matrix kept packed storage 1 16 page 74 75 means columns lower upper triangle stored consecutively one dimensional array see gure 2 triangular matrix occupies strictly necessary storage space nonuniform storage scheme means use full storage blas impossible level 2 blas20 9 packed subroutines employed resulting low speed solution summarize choice high speed waste memory versus low speed waste memory 12 new way storing real symmetric complex hermitian either case positive denite matrice together new recursively formulated linear algebra subroutines propose new way storing lower upper triangular matrix solves dilemma14 24 words obtain speed potrf amount memory used pptrf new storage scheme named rpf recursive packed formatsee gure 4 explained benet recursive formulations cholesky factorization lu decomposition described works gustavson 14 toledo 22 symmetric positive denite matrix cholesky case kept full matrix storage emphasis works better data locality thus better utilization computers memory hierarchy recursive formulations oer however recursive packed formulation also property provide short introduction computer memory hierarchy basic linear algebra subprograms blas describing recursive packed cholesky rpc recursive packed format rpf 13 rationale behind introducing new recursive algorithm rpc new recursive data format rpf computers several levels memory ow data memory computational units important factor governing performance engineering scientic computations object keep functional units running peak capacity use memory hierarchy fortran column major c row major system see gure 3 high performance achieved using locality reference within program present context called blocking registers cache level 1 cache level 2 cache level 3 shared memory distributed memory secondary storage 1 secondary storage 2 faster smaller expensive slower larger cheaper figure 3 computer memory hierarchy top hierarchy central processing unit cpu communicates directly registers number registers usually small level 1 cache directly connected registers computer run almost peak performance able deliver data level 1 cache way cpu permanently busy several books describing problems associated computer memory hier archy literature 10 5 11 adequate numerical linear algebra specialists memories near cpu registers caches faster access cpu memories away fast memories expensive one reason small register set tiny cache memories much larger set registers however l1 cache still large enough solving scientic problems even subproblem like matrix factorization cache order matrix large special set basic linear algebra subprograms blas developed address computer memory hierarchy problem area numerical linear algebra blas documented 20 9 8 6 blas well summarized explained numerical linear algebra specialists 10 5 three levels blas level 1 blas shows vector vector opera tions level 2 blas shows vector matrix andor matrix vector operations level 3 blas shows matrix matrix operations cholesky factorization one make following three observations respect blas 1 level 3 implementations using full storage format run fast 2 level 3 implementations using packed storage format rarely exist level 3 implementation previously used 16 however great programming cost conventional level 2 implementations using packed storage format run large problem sizes considerably slower full storage implementations 3 transforming conventional packed storage rpf using rpc algorithm produces level 3 implementation using amount storage packed storage 14 overview paper section 2 describes new packed storage data format data transformations conventional packed storage section 21 describes conventional lower upper triangular packed storage section 22 discusses transform place either lower upper trapezoid packed data format recursive packed data format vice versa section 23 describes possibility transpose matrix reordered packed recursive packed format vice versa finally section 24 recursive aspects data transformation described four subsections describe place transformation pictorially via several gures sections 31 32 recursive trsm syrk work rpf described routines almost required oating point operations calling level 3 blas gemm finally section 33 rpc algorithm described terms using recursive algorithms sections 31 32 section 2 three algorithms described pictorially via several gures note rpc algorithm uses one level 3 blas subroutine namely gemm usually gemm routine specialized highly tuned done computer manufacturer atlas25 gemm used section 4 explains rpc algorithm numerically stable section 5 describes performance graphs packed storage lapack3 algorithms recursive packed algorithms several computers typical computers like compacq hp ibm sp ibm smp intel pen tium sgi sun considered gures results show recursive packed cholesky factorization rp pptrf solution rp pptrs 4 9 times faster traditional packed sub routines three graphs one demonstrates successful use openmp17 18 parallelizing directives gure 18 second graph shows recursive data format also eective complex arithmetic g ure 19 third one shows performance three algorithms factorization potrf pptrf rp pptrf solution potrs pptrs rp pptrs gure 20 section 6 discusses important developments paper 2 recursive packed storage new way store triangular matrices packed storage called recursive packed presented storage scheme right way explain describe conversion packed recursive packed storage vice versa see gures 2 4 lower triangular caseb 22 upper triangular caseb 9 12 15 19 2022 24 26 271 figure 4 mapping 7 7 matrix cholesky algorithm using recursive packed storage recursive block division illustrated 21 lower upper triangular packed storage symmetric complex hermitian triangular matrices may stored packed storage form see lapack users guide 3 ibm essl library manual16 pages 6667 gure 2 columns triangle stored sequentially one dimensional array starting rst column mapping positions full storage packed storage triangular matrix size ij ap ij 12m j2 lthe advantage storage saving almost half 4 memory compared full storage 3 upper triangular lower triangular stored 4 least 12 formulae function lda leading dimension fortran77 saving fortran77 22 reordering lower upper trapezoid packed storage packed storage memory map buer pp 12 words recursive packed storage memory map figure 5 reordering lower packed matrix first last p 1 columns leading triangle copied buer place columns accentuated rectangle assembled bottom space trapezoid last buer copied back top trapezoid assumed matrices stored column major order concepts paper fully applicable also matrices stored row major order intermediate step transform packed triangular matrix recursive packed matrix matrix divided two parts along column thus dividing matrix trapezoidal triangular part shown g 5 6 triangular part remains packed form trapezoidal part reordered consists triangle packed form rectangle full storage form reordering demands buer size triangle minus longest column reordering lower case g 5 takes following steps first columns triangular part trapezoid moved buer note rst column correct place columns rectangular part trapezoid moved consecutive locations nally buer copied back correct location reordered array p gure 5 chosen bm2c rectangular submatrix square deviate square single column buer size pp 12 addresses lead 8packed storage packed storage memory map buer pm p 12 words recursive packed storage memory map figure reordering upper packed matrix first rst columns trailing triangle copied buer place columns accentuated rectangle assembled top space trapezoid last buer copied back bottom trapezoid ing triangle rectangular submatrix trailing triangle given reordering leading trailing triangles lower upper packed storage scheme original triangular matrix reordering implemented subroutines subroutine tpz trm p ap subroutine tr tpz p ap tpz tr means reordering trapezoidal part packed format triangularrectangular format described tr tpz opposite reordering 23 transposition rectangular part rectangular part reordered matrix kept full matrix storage desired oers excellent opportunity transpose matrix transformed recursive packed format rectangular submatrix square transposition done completely inplace deviates square column buer size columns necessary transposition purpose reuse buer used reordering 24 recursive application reordering method reordering applied recursively leading trailing triangles still packed storage nally originally triangular packed matrix divided rectangular submatrices decreasing size full storage implementation complete transformation packed recursive packed format p rp compare gures 2 4 recursive subroutine p rpmap 1 call tpz trm p ap call p rpp ap call p rpm inverse transformation recursive packed packed rp p recursive subroutine rp pmap 1 call rp pp ap call tr tpz p ap call rp pm examples shown concerns lower triangular matrix upper triangular transformation transformation transposition follows pattern gure 7 illustrates recursive division small lower upper triangular matrices figure 7 lower upper triangular matrices recursive packed storage data format 20 rectangular submatrices shown gures kept full storage column major order array containing whole matrices recursive formulation cholesky algorithm necessary blas two blas6 operations triangular solver multiple right hand sides trsm 5 rank k update symmetric matrix syrk needed recursive cholesky factorization solution rp pptrf 6 rp pptrs2 section rp trsm rp syrk rp pptrf rp pptrs formulated recursively use recursive packed operands explained trsm syrk pptrf pptrs operate various cases depending operands order operands following consider single specic cases deduction cases follows guidelines computational work recursive blas routines rp trsm rp syrk also rp trmm done non recursive matrixmatrix multiply routine gemm19 25 attractive property since gemm usually highly optimized current computer architectures gemm operation well documented explained 12 13 6 speed computation depends much speed good gemm good gemm implementations usually developed computer manufacturers model implementation gemm obtained 5 naming trsm syrk herk gemm see footnote potrf page 1 6 prex rp indicates subroutine belongs recursive packed library example rp pptrf recursive packed cholesky factorization routine netlib 6 works correctly slowly innovative computing laboratory university tennessee knoxville developed automatic system called usually produce fast gemm subroutine another automatic code generator scheme gemm developed berkeley4 essl see 1 gemm blas produced via blocking high performance kernel routines example essl produces single kernel routine datb function atlas chip gemm kernel principles underlying production kernels similar major dierence essls gemm code written hand whereas atlas gemm code parametrized run parameter settings best parameter setting found particular machine 31 recursive trsm based nonrecursive gemm fig 8 shows splitting trsm operands operation consists three suboperations based splitting algorithm programmed follows recursive subroutine rp trsm n n 1 else call rp trsm call gemm 0 n call rp trsm n 32 recursive syrk based nonrecursive gemm fig 9 shows splitting syrk operands operation consists three suboperations 21 11 22 figure 8 recursive splitting matrices rp trsm operation case sideright uplolower transatranspose based splitting algorithm programmed follows recursive subroutine rp syrk n 1 else call rp syrk p n call gemm 0 n call rp syrk 33 recursive pptrf pptrs based recursive trsm recursive syrk fig 10 shows splitting pptrf operand operation consists four suboperations mp c 22 mp bc 22 mp 11 21 mp figure 9 recursive splitting matrices rp syrk operation case uplolower transno transpose 22 rp pptrf based splitting algorithm programmed follows recursive subroutine rp pptrf ap 1 else call rp pptrf p ap call rp trsm call rp syrk call rp pptrf solution subroutine rp pptrs performs consecutive triangular solutions transposed nontransposed cholesky factor routine explicitly recursive calls recursive rp trsm twice 4 stability recursive algorithm paper 24 shows recursive cholesky factorization algorithm equivalent traditional algorithms books5 11 23 whole theap 21 mp 22 mp mp mp figure 10 recursive splitting matrix rp pptrf operation case uplolower ory traditional cholesky factorization blas trsm syrk algorithms carries recursive cholesky factorization blas trsm syrk algorithms described section 3 error analysis stability algorithms well described book nicholas j higham15 dierence lapack algorithms po pp rp 7 inner products accumulated case dierent order used mathematically equivalent stability analysis shows summation order stable 5 performance results sun ultrasparc ii 400 mhz sgi r10000 195 mhz compaq alpha ev6 500 mhz hp pa8500 440 mhz intel pentium iii 500 mhz table 1 computer names new recursive packed blas rp trsm rp syrk new recursive packed cholesky factorization solution rp pptrf rp pptrs routines compared traditional lapack subroutines concerning results performance comparisons made seven 7 full packed recursive packed dierent architectures listed table 1 result graphs attached appendix paper double precision arithmetic fortran9021 used cases ibmppc essl 3100 lesslsmp ibmpw2 essl 2220 lesslp2 sun sun performance library 20 lsunperf sgi standard execution environment 73 lblas compaq dxml v35 ldxmp ev6 hp hpux pa20 blas library 1030 lblas intel atlas 30 table 2 computer library versions following procedure used carrying performance tests machine recursive traditional routines compiled compiler compiler ags call vendor optimized otherwise optimized blas library blas library versions seen table 2 compared recursive traditional routines received input produced output time measurement time spent reordering matrix 8 recursive packed format included run time rp pptrf rp pptrs traditional routines data transformation cost cpu time measured timing function etime except powerpc machine 4 way smp machine run time measured wall clock time means special ibm utility function called rtc except operating system programs running test runs machine timings made sequence matrix sizes ranging steps case hp intel machines matrix size starts start resolution etime utility coarse number right hand sides taken nrhs n10 due memory limitations actual hp machine test series could range operation counts cholesky factorization solution 8 however necessary perform transformation rp pptrf transformation rp pptrs get correct results n number equations nrhs number right hand sides counts used convert run times flop rates ten gures gure show performance graph comparisons new rpc algorithms traditional lapack algorithms rpc algorithms use rpf data format comparisons mentioned cost transforming packed format rpf rpf packed format included recursive packed factor solve routines subroutines dpptrf zpptrf dpptrs zpptrs use packed data format dpotrf dpotrs use full data format figure 20 compares three algorithms rpc lapack full storage lapack packed storage every gure two subgures one caption upper subgure shows comparison curves cholesky factorization lower subgures show comparison curves forward backward substitutions captions describe details performance gures rst seven gures describe comparison performance several dierent computers 51 ibm smp powerpc figure 11 shows performance ibm 4way powerpc 604e 332 mhz computer lapack routine dpptrf upper subgure performs 100 mflops performance u graph little better l graph performance remains constant order matrix increases performance rpc factorization routine increases n increases u graph increases 50 mflops almost 600 mflops l graph 200 mflops 650 mflops u graph performance better l graph performance relative u l rpc algorithm performance 49 72 times better dpptrf algorithm large matrix sizes performance rpc solution routine lower subgure l u graphs almost equal dpptrs routine performs 100 mflops matrix sizes rpc algorithm curve increases 250 mflops almost 800 mflops relative u l performance rpc algorithm 57 55 times faster dpptrs algorithm large matrix sizes matrix size varies 300 3000 subgures 52 ibm power2 figure 12 shows performance ibm power2 160 mhz computer lapack routine dpptrf upper subgure u graph performs 200 mflops l graph performs 150 mflops increase graphs size matrix grows performance graphs rpc factorization routine increase u graph 300 little 400 mflops l graph 200 mflops 450 mflops l graph better u graph matrix sizes 750 3000 u graph better l graph matrix sizes 300 750 graphs grow rapidly matrix sizes 300 500 relative u l rpc algorithm performance 19 31 times faster dpptrf algorithm large matrix sizes performance rpc solution routine lower subgure l u graphs almost equal performance dpptrs algorithm stays constant 250 mflops decreasing slightly n ranges 300 3000 performance rpc algorithm increases 350 500 mflops relative u l rpc algorithm performance 23 23 times faster dpptrs algorithm large matrix sizes matrix size varies 300 3000 subgures 53 compaq alpha ev6 figure 13 shows performance compaq alpha ev6 500 mhz computer lapack routine dpptrf upper subgure u graph performs better l graph dierence 50 mflops performance starts 300 mflops increases 400 mflops drops 200 mflops performance rpc factorization routine increases n increases graphs u l graphs almost equal u graph little higher matrix sizes 300 450 relative u l rpc algorithm performance 34 50 times faster dpptrf algorithm large matrix sizes routine dpptrs shape solution performance curves lower subgure l u graphs almost equal performance dpptrs routine decreases 450 250 mflops n increases 300 3000 rpc performance curves increases 450 mflops 750 mflops performance u l rpc algorithm 33 33 times faster dpptrs algorithm large matrix sizes matrix size varies 300 3000 subgures 54 sgi r10000 figure 14 shows performance sgi r10000 195 mhz computer one processor lapack routine dpptrf upper subgure u graph performs better l graph matrix sizes 300 2000 u l graphs dpptrf performance slowly decreases performance rpc factorization routine u l graphs increases 220 300 mflops n increases 300 1000 stays constant n increase 3000 relative u l rpc algorithm performance 49 49 times faster dpptrf algorithm large matrix sizes routine dpptrs shape solution performance curves lower subgure l u graphs almost equal performance dpptrs routine decreases 130 mflops 60 mflops n increases 300 3000 performance rpc solution routine increases beginning runs constantly 300 mflops performance u l rpc algorithm 51 52 times faster dpptrs algorithm large matrix sizes matrix size varies 300 3000 subgures 55 sun ultrasparc ii figure 15 shows performance sun ultrasparc ii 400 mhz computer lapack routine dpptrf upper subgure u l graphs show almost equal performance n 1500 functions start 200 225 mflops decrease 50 mflops rpc factorization routine performance u l graphs also almost equal whole interval function values start quickly rise 350 mflops slowly increase 450 mflops rpc factorization u l algorithm 97 102 times faster dpptrf algorithm large matrix sizes performance rpc solution routine lower subgure l u graphs almost equal dpptrs performance graphs decreases 225 50 mflops performance rpc solution graphs increases 330 almost 450 mflops rpc solution u l algorithm 100 times faster dpptrs algorithm large matrix sizes matrix size varies 300 3000 subgures 56 hp pa8500 figure shows performance hp pa8500 440 mhz computer lapack routine dpptrf upper subgure u l graphs decreasing functions u graph function values go 370 100 mflops l graph function goes 280 180 mflops performance rpc factorization graphs increasing functions matrix size increases 1000 3000 performance varies matrix sizes 500 1500 u graph function values range 700 mflops almost 800 mflops l graph function values range 600 mflops little 700 mflops rpc algorithm u l 47 times faster dpptrf algorithm large matrix sizes performance rpc solution routine lower subgure l u graphs almost equal dpptrs routine performance decreases 300 mflops 200 mflops rpc algorithm curve increases 550 mflops almost 810 mflops rpc algorithm u l 52 50 times faster dpptrs algorithm large matrices solution case matrix size varies 500 2500 subgures 57 intel pentium iii figure 17 shows performance intel pentium iii 500 mhz computer lapack routine dpptrf upper subgure u l graphs decreasing functions u graph function ranges 100 80 mflops l graph function ranges less 50 25 mflops rpc factorization routine u l graphs almost equal graphs increasing functions 200 310 mflops rpc factorization algorithm u l 42 92 times faster dpptrf algorithm large matrices performance rpc solution routine lower subgure l u graphs almost equal dpptrs performance graphs decreases 80 50 mflops rpc algorithm curves increases 240 330 mflops rpc algorithm u l 59 60 times faster dpptrs algorithm large matrices matrix size varies 500 3000 subgures 58 ibm smp powerpc openmp directives figure shows performance ibm 4way powerpc 604e 332 mhz computer graphs demonstrate successful use openmp17 18 parallelizing directives curves lapackl lapacku recursivel recur identical corresponding curves gure 11 compare curves recursivel recursiveu recparl recparu recparl recparu curves result double parallelization rpc algorithms call parallelized dgemm parallelized openmp directives recparl curve much faster recursivel sometimes slower recparu fastest specially large size matrices doubly parallelized rpc algorithm recparu 100 mflops faster ordinary rpc algorithm recursiveu relative u l rpc factorization algorithm performance 56 76 times faster dpptrf algorithm large matrices rpc double parallelization algorithm solution lower subgure exceeds 800 mflops relative u l rpc solution algorithm performance 65 66 times faster dpptrf algorithm large matrices matrix size varies 300 3000 subgures 59 intel pentium iii running complex arithmetic figure 19 shows performance intel pentium iii 500 mhz computer gure demonstrate successful use rpc algorithm hermitian positive denite matrices performance measured complex mflops compare usual real arithmetic mflops complex mflops multiplied 4 lapack routine zpptrf upper subgure u graph performs little better l graph routine performs 80 mflops rpc hermitian factorization routine u graph performs better l graph rpc performance graphs increasing functions go 240 320 mflops rpc hermitian factorization algorithm u l 38 43 times faster zpptrf algorithm large size matrices performance rpc solution routine lower subgure l u graphs almost equal zpptrs performance decreases 108 80 mflops rpc solution algorithm increases 240 320 mflops rpc algorithm u l 39 37 times faster zpptrs algorithm large hermitian matrices matrix size varies 500 3000 subgures 510 intel pentium iii three cholesky algorithm figure 20 shows performance intel pentium iii 500 mhz computer graphs gure depict three cholesky algorithms lapack full storage dpotrf dpotrs algorithms lapack packed storage dpptrf dpptrs algorithms rpc factorization solution algorithms lapack packed storage algorithms dpptrf dpptrs previously explained gure 17 dpotrf routine upper subgure u l cases performs better rpc factorization routine smaller matrices larger matrices rpc factorization algorithm performs equally well slightly better dpotrf algorithm performance dpotrs algorithms u l graphs better rpc performance computer however potrf potrs storage requirement almost twice storage requirement rpc algorithms matrix size varies 500 3000 subgures 6 conclusion summarize emphasize important developments described paper recursive packed cholesky factorization algorithm based blas level 3 operations developed rpc factorization algorithm works almost speed traditional full storage algorithm occupies data storage traditional packed storage algorithm also see bullet 4 user interface new packed recursive subroutines rp pptrf rp pptrs exactly traditional lapack subroutines pptrf pptrs user see identical data formats however new routines run much faster two separate routines described rp pptrf rp pptrs data format always converted lapack packed data format recursive packed data format routine starts operation converted back lapack data format afterwards rp ppsv subroutine exists package equivalent lapack ppsv routine rp ppsv subroutine data converted factorization solution new recursive packed level 3 blas rp trsm rp syrk written fortran9021 developed call gemm routine gemm subroutine developed either computer manufacturer generated atlas system25 atlas generated gemm subroutine usually compatible manufacturer developed routine acknowledgements research partially supported lawra project unic collaboration ibm tj watson research center yorktown heights last two authors also supported danish natural science research council grant epos project ecient parallel algorithms optimization simulation r exploiting functional parallelism power2 design highperformance numerical algorithms applied numerical linear algebra blas basic linear algebra subprograms extended set fortran basic linear algebra subroutines matrix computations recursion leads automatic variable blocking dense linearalgebra algorithms accuracy stability numerical algorithms basic linear algebra subprograms fortran usage fortran 9095 explained locality reference lu decomposition partial pivoting numerical linear algebra automatically tuned linear algebra software atlas tr extended set fortran basic linear algebra subprograms set level 3 basic linear algebra subprograms exploiting functional parallelism power2 design highperformance numerical algorithms matrix computations 3rd ed optimizing matrix multiply using phipac applied numerical linear algebra locality reference lu decomposition partial pivoting recursion leads automatic variable blocking dense linearalgebra algorithms gemmbased level 3 blas fortran 9095 explained 2nd ed basic linear algebra subprograms fortran usage accuracy stability numerical algorithms numerical linear algebra high performance computers recursive formulation cholesky algorithm fortran 90 superscalar gemmbased level 3 blas ongoing evolution portable highperformance library recursive blocked data formats blass dense linear algebra algorithms ctr chan enrique quintanaorti gregorio quintanaorti robert van de geijn supermatrix outoforder scheduling matrix operations smp multicore architectures proceedings nineteenth annual acm symposium parallel algorithms architectures june 0911 2007 san diego california usa steven gabriel david wise opie compiler rowmajor source mortonordered matrices proceedings 3rd workshop memory performance issues conjunction 31st international symposium computer architecture p136144 june 2020 2004 munich germany dror irony gil shklarski sivan toledo parallel fully recursive multifrontal sparse cholesky future generation computer systems v20 n3 p425440 april 2004 bjarne andersen john gunnels fred g gustavson john k reid jerzy waniewski fully portable high performance minimal storage hybrid format cholesky algorithm acm transactions mathematical software toms v31 n2 p201227 june 2005 f g gustavson highperformance linear algebra algorithms using new generalized data structures matrices ibm journal research development v47 n1 p3155 january