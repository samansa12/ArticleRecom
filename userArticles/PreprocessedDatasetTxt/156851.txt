weighted nearest neighbor algorithm learning symbolic features past nearest neighbor algorithms learning examples worked best domains features numeric values domains examples treated points distance metrics use standard definitions symbolic domains sophisticated treatment feature space required introduce nearest neighbor algorithm learning domains symbolic features algorithm calculates distance tables allow produce realvalued distances instances attaches weights instances modify structure feature space show technique produces excellent classification accuracy three problems studied machine learning researchers predicting protein secondary structure identifying dna promoter sequences pronouncing english text direct experimental comparisons learning algorithms show nearest neighbor algorithm comparable superior three domains addition algorithm advantages training speed simplicity perspicuity conclude experimental evidence favors use continued development nearest neighbor algorithms domains ones studied b introduction learning classify objects fundamental problem artificial intelligence fields one attacked many angles despite many successes domains task proven difficult due either inherent difficulty domain lack sufficient data learning example instancebased learning programs also called exemplarbased salzberg 1990 nearest neighbor cover hart 1967 methods learn storing examples points feature space require means measuring distance examples aha 1989 aha kibler 1989 salzberg 1989 cost salzberg 1990 example usually vector feature values plus category label features numeric normalized euclidean distance used compare examples however feature values symbolic unordered values eg letters alphabet natural interletter distance nearest neighbor methods typically resort much simpler met rics counting features match towell et al 1990 recently used metric nearest neighbor algorithm comparative study simpler metrics may fail capture complexity problem domains result may perform well paper present sophisticated instancebased algorithm designed domains feature values sym bolic algorithm constructs modified value difference tables style stanfill waltz 1986 produce noneuclidean distance metric introduce idea exception spaces result weights attached individual examples combination two techniques results robust instancebased learning algorithm works domain symbolic feature values describe series experiments demonstrating algorithm pebls performs well three important practical classification problems comparisons given show algorithms accuracy comparable back propagation decision trees learning algorithms results support claim nearest neighbor algorithms powerful classifiers even features symbolic 11 instancebased learning versus models power instancebased methods demonstrated number important real world domains prediction cancer recurrence diagnosis heart disease classification congressional voting records aha kibler 1989 salzberg 1989 experiments demonstrate instancebased learning ibl applied effectively three mains features unordered symbolic values 1 prediction protein secondary structure 2 word pronunciation 3 prediction dna promoter sequences domains received considerable attention connectionist researchers employed back propagation learning algorithm sejnowski rosenberg 1986 qian sejnowski 1988 towell et al 1990 addition word pronunciation problem subject number comparisons using machine learning algorithms stanfill waltz 1986 shavlik et al 1989 dietterich et al 1990 domains represent problems considerable practical im portance symbolic feature values makes difficult conventional nearest neighbor algorithms show nearest neighbor algorithm pebls based stanfill waltzs 1986 value difference method produce highly accurate predictive models domains intent compare ibl learning methods three respects classification accuracy speed training perspicuity ie ease algorithm representation understood comparable performance first respect superiority latter two argue ibl often preferable learning algorithms types problem domains considered paper instancebased learning shown compare favorably algorithms eg decision trees rules wide range domains feature values either numeric binary eg aha 1989 aha kibler 1989 aha et al 1991 salzberg 1989 paper presents similar evidence terms classification accuracy domains symbolic feature values however describing domains consider advantages instancebased learning algorithms provide training time neural net learning algorithms requires vastly time training machine learning methods training normally performed repeatedly presenting network instances training set allowing gradually converge best set weights task using example back propagation algorithm weiss kapouleas 1989 mooney et al 1989 shavlik et al 1989 report back propagations training time many orders magnitude greater training time algorithms id3 frequently factors 100 addition neural net algorithms number parameters eg momentum parameter need tweaked programmer may require much additional time weiss kapouleas experiments required many months experimenters time produce results back propagation algorithms typically required hours parameter might adjusted algorithm value r distance metric see consider two possible values 2 nearest neighbor algorithm requires little training time terms experimenters time processing time present two versions pebls algorithm one slightly complex simpler version called unweighted version experimental section requires odn number examples number features dimensions per example v number values feature may general n much larger v 2 hence complexity usually odn complex version pebls incrementally computes weights exemplars requires training instances 1 training classification 1 reference system takes minutes real time decstation 3100 train 17142 instances experimenters time limited minutes defining time nearest neighbor system worst odn admittedly slow compared algorithms nearest neighbor methods lend well parallelization produce significantly faster classification times example assigned separate processor parallel architecture enough processors examples divided among number processors large training set classification time may reduced od log n implemented system set four looselycoupled transputers well conventional architecture recent efforts fgp fertig gelernter 1991 use larger numbers parallel processors mbrtalk system waltz stanfill 1986 implemented form knearestneighbor learning using tightlycoupled massively parallel architecture 64000processor connection machine tm perspicuity instancebased learning algorithm also transparent operation learning methods algorithm nothing straightforward define measure distance values compare new instance instances memory classify according category closest decision tree algorithms perhaps equally straightforward classification time pass example tree get decision neural net algorithms fast classification time transparent weights must propagated net summed passed filter eg threshold layer complicated part method computation distance ta data set bles computed via fixed statistical technique based frequency occurrence values however certainly complicated entropy calculations decision tree methods eg quinlan 1986 weight adjustment routines used back propagation tables provide insight relative importance different features noticed instance several tables protein folding data almost identical indicating features similar instance memory ibl system readily accessible examined numerous ways example human wants know particular classification made system simply present instance memory used classification human experts commonly use explanations example asked justify prediction economy experts typically produce another similar economic situation past neural nets yet provide insight made classification although recent efforts explored new methods understanding content trained network hanson burr 1990 also relatively easy modify algorithm include domain specific knowledge relative importance features known features may weighted accordingly distance formula salzberg 1989 taken together advantages listed make clear ibl algorithms number benefits respect competing models however order considered realistic practical learning technique ibl must still demonstrate good classification accuracy problem domain symbolic features obvious distance metric ibl counting number features differ work well metric called lap metric experimental results show modified valuedifference metric process symbolic values exceptionally well results taken together results domains numeric features eg aha 1990 aha et al 1991 show ibl algorithms perform quite well wide range problems learning algorithms back propagation widely used understood neural net learning algorithm used basis comparison experiments paper although earlier approaches notably perceptron learning model unable classify groups concepts linearly separable back propagation overcome problem back propagation gradient descent method propagates error signals back multilayer network described many places eg rumelhart et al 1986 rumelhart mcclelland 1986 readers interested detailed description look use decision tree algorithm id3 quinlan 1986 basis comparison decision tree algorithms addition compared performance algorithm methods used data domains described section 3 appropriate present results domainspecific classification methods 21 instancebased learning instancebased learning algorithm like algorithms stores series training instances memory uses distance metric compare new instances stored new instances classified according closest exemplar memory algorithm implemented program called stands parallel exemplarbased learning system 2 clarity use term example mean training test example shown system first time use term exemplar following usage salzberg 1991 refer specifically instance previously stored computer memory exemplars may additional information attached eg weights term stance covers examples exemplars pebls designed process instances symbolic feature val ues heart pebls algorithm way measures distance two examples consists essentially three compo nents first modification stanfill waltzs 1986 value difference metric vdm defines distance different values given feature call method mvdm modified value difference metric second component standard distance metric measuring distance two examples multidimensional feature space fi nally distance modified weighting scheme weights instances memory according performance history salzberg 1989 1990 components distance calculation described sections 22 2 parallelization algorithm developed speed experimentation theoretical importance learning model 23 pebls requires two passes training set first pass feature value difference tables constructed instances training set according equations stanfill waltz vdm second pass system attempts classify instance computing distance new instance previously stored ones new instance assigned classification nearest stored instance system checks see classification correct uses feedback adjust weight old instance weight described detail section 23 finally new instance stored memory testing examples classified manner modifications made memory distance tables 22 stanfillwaltz vdm 1986 stanfill waltz presented powerful new method measuring distance values features domains symbolic feature val ues applied technique english pronunciation problem impressive initial results stanfill waltz 1986 value difference metric vdm takes account overall similarity classification instances possible value feature using method matrix defining distance values feature derived statistically based examples training set distance ffi two values eg two amino acids specific feature defined equation 1 1 equation v 1 v 2 two possible values feature eg protein data would two amino acids distance values sum n classes example protein folding experiments section 41 three categories number times v 1 classified category c 1 total number times value 1 occurred k constant usually set 1 using equation 1 compute matrix value differences feature input data interesting note value difference matrices computed experiments quite similar overall different features although differ significantly value pairs idea behind metric wish establish values similar occur relative frequency classifications term c 1 represents likelihood central residue classified given feature question value v 1 thus say two values similar give similar likelihoods possible classifica tions equation 1 computes overall similarity two values finding sum differences likelihoods classifications consider following example say pool instances examine single feature takes one three values b c two classifications ff fi possible data construct table 1 table entries represent number times instance table 1 number occurrences value class feature values ff fi 4 3 table 2 value difference table feature values 0000 0571 0191 given feature value classification information construct table distances follows frequency occurrence class ff 571 since 4 instances classified ff 7 instances value similarly frequencies occurrence b c 286 667 respectively frequency occurrnce class fi 429 find distance b use equation 1 yields 0571 complete table distances shown table 2 note construct different value difference table feature 10 features construct 10 tables equation 1 defines geometric distance fixed finite set values property value distance zero positive distance values distances symmetric distances obey triangle inequality summarize properties follows ii iii iv stanfill waltzs original vdm also used weight term w g f makes version ffi nonsymmetric eg ffia b 6 ffib major difference metric vdm mvdm omit term makes ffi symmetric total distance delta two instances given x represent two instances eg two windows protein folding domain x exemplar memory new example variables x values th feature x example n features wx w weights assigned exemplars described following section new example w domains numeric features manhattan distance produces euclidean distance experiments used however used protein secondary structure task summary four major differences mvdm stanfillwaltz vdm 1 omit weight term w g f makes stanfillwaltz vdm nonsymmetric formulation ffi delta symmetric 2 stanfill waltz 1986 used value version equation 1 preliminary experiments indicated equally good performance achieved chose value reasons simplicity 3 added exemplar weights distance formula described section 23 4 stanfill waltz used 10 closest exemplars classification whereas pebls uses nearest neighbor really difference learning algorithms rather value difference 23 weighted exemplars exception spaces stored instances reliable classifiers others intuitively one would like trustworthy exemplars drawing power others final difference mvdm metric original vdm capacity metric treating reliable instances differently accomplish weight wx distance formula reliable exemplars given smaller weights making appear closer new example weighting scheme first adopted system salzberg 1989 1990 assigned weights exemplars according performance history wx ratio number uses exemplar number correct uses exemplar thus accurate exemplars wx 1 unreliable exemplars wx 1 making appear away new example unreliable exemplars may represent either noise exceptions small areas feature space normal rule apply times exemplar incorrectly used classification larger weight grows alternative scheme handling noisy exceptional instances ibl framework discussed aha kibler 1989 elaborated aha 1990 scheme instance used nearestneighbor computation proven acceptable classifier acceptable instances whose classification accuracies exceed baseline frequency class fixed amount example baseline frequency class 30 instance correct 80 time would acceptable whereas baseline frequency 90 instance would accept able note technique designed primarily filter noisy instances rather identify exceptional instances difference noisy instances probably ignored discarded whereas exceptional instances retained used relatively infrequently differ salzbergs original exemplar weighting scheme one significant aspect way exemplars points weighted initially original scheme stored points initial weights 11 effect feature space significant consider instance space containing two points classified ff fi unweighted two points define hyperplane divides ndimensional space ff fi region shown figure 1 point located left side plane classified ff likewise fi figure 1 two unweighted points instance space pebls computes distance new instance weighted ex emplar distance multiplied exemplars weight intuitively makes less likely new instance appear near exemplar exemplars weight grows figure 2 shows geometrically use weights creates circular envelope around exemplar larger weight defining exception space shrinks weight difference increases points inside circle match point larger weight weights equal special case hyperplane given generally given space many exemplars exemplars smallest weights best classification performance partition space set hyperplanes weights best exemplars identical partitioning uses large circles exemplar effectively rule region space exemplars larger weights define exception spaces around figure 3 shows within exception space process may recur groups exemplars figure 2 two weighted points instance space figure 3 partitions exception spaces approximately equal weights ability partition space large general rules pockets exceptions important domains contain many exceptions without capability many points required learning necessary surround exceptions set nonexception points define edge space two points required define rule exception capability becomes even important ibl models store subset training examples reduces number points must stored cost salzberg 1990 given discussion clear instances initialized weights 1 consider system trained instances training n th hierarchy instance weights already constructed training represent structure domain instance entered weight 1 would immediately become one influential classifiers space found better strategy initialize new instance weight equal matching exemplar adopted weighting strategy experiments described weighting scheme completes modified value difference metric domains chose comparisons three domains received considerable attention machine learning research community word pronunciation task sejnowski rosenberg 1986 shavlik et al 1989 prediction protein secondary structure qian sejnowski 1988 holley karplus 1989 prediction dna promoter sequences towell et al 1989 domain symbolicvalued features thus mvdm applicable whereas standard euclidean distance sections 3133 describe three databases problems present learning 31 protein secondary structure accurate techniques predicting folded structure proteins yet exist despite increasingly numerous attempts solve problem techniques depend part prediction secondary structure primary sequence amino acids secondary structure information used construct final tertiary structure tertiary structure difficult derive directly requiring expensive methods xray crystallography primary sequence sequence amino acids constitute protein relatively easy discover attempts predict secondary structure involve classification residues three categories ff helix fi sheet coil three widely used approaches problem robson garnier et al 1978 chou fasman 1978 lim 1974 produce classification accuracies ranging 48 58 accurate techniques developed predicting tertiary secondary structure eg cohen et al 1986 lathrop et al 1987 accurate prediction secondary structure proven extremely difficult task learning problem described follows protein consists sequence amino acids bonded together chain sequence known primary structure amino acid chain one twenty different acids point two acids join chain various factors including chemical properties determine angle molecular bond angle purposes characterized one three different types fold ff helix fi sheet coil words certain number consecutive acids hereafter residues chain join manner call ff segment chain ff helix characterization fold types protein known secondary structure learning problem given sequence residues fixed length window protein chain classify central residue window ff helix fi sheet coil setup simply window z z gtpgksfnlnfdtg central residue qian sejnowski 1988 holley karplus 1989 formulated problem exactly manner studies found optimal window size approximately 17 residues 21 largest window tested either study separate statistical study cost 1990 found window size five six nearly sufficient uniquely identifying residues data set indicated table 3 table shows percentage sequences given size unambiguously entire data set determine fold classification protein segment example consider window size six centered residue classified found 9941 patterns data set unique addition found slightly information contained residues left point prediction right point prediction residue secondary structure must predicted skew top column table indicates left right shift pattern respect center window eg skew 2 means table 3 percent unique patterns window size skew window size 3 2 6 9935 9941 9941 9941 9936 9929 9923 7 9950 9953 9954 9952 9948 9942 9936 9 9962 9963 9963 9964 9962 9958 9954 14 9972 9973 9972 9972 9971 9971 9971 19 9976 9976 9975 99 pattern centered two positions left point prediction table shows quite clearly one stored patterns length 6 data set one could classify data set better 99 accuracy obstacles good performance domain include undersampling nonlocal effects considering window size five database using 21618 32 million possible segments represented database 068 also proteins solution form globular structures net result residues sequentially far may physically quite close significant effects reason secondary structure probably cannot completely determined primary structure qian claim method incorporating local information perform much better current results 6070 range nonhomologous proteins 32 promoter sequences promoter sequence database subject several recent experiments towell et al 1990 related protein folding task involves predicting whether given subsequence dna sequence promoter sequence genes initates process called transcription expression adjacent gene data set contains 106 examples 53 positive examples promoters negative examples generated larger dna sequences believed contain promoters see towell et al 1990 detail construction data set instance consists sequence 57 nucleotides alphabet c classification learning 57 nucleotides treated 57 features one four symbolic values 33 pronunciation english text word pronunciation problem presents interesting challenges machine learning although effective practical algorithms developed task given relatively small sequence letters objective learn sound stress required pronounce part given word sejnowski rosenberg 1987 introduced task learning community nettalk program nettalk used back propagation learning method performed well task pronouncing words continuous spoken text although could match performance current speech synthesis programs instance representation text pronunciation similar previous problems instances sequences letters make word task classify central letter sequence correct phoneme used fixed window seven characters experiments sejnowski rosenberg stanfill waltz 1986 used window size 15 classes include 54 phonemes plus 5 stress classifications phoneme stress predicted 5 theta 54 270 possible classes although 115 actually occur dictionary experiments emphasized prediction phonemes difficulties domain arise irregularity natural lan guage english language particular rules exist exceptions better performance data set obtained nonlearning rulebased approaches kontogiorgios 1988 however learning algorithms trouble finding best set rules 4 experimental results section describe experiments results three test domains comparison use previously published results learning methods order make comparisons valid attempted duplicate experimental design earlier studies closely possible used data used studies 41 protein secondary structure protein sequences used experiments originally brookhaven national laboratory secondary structure assignments ff helix fisheet coil made based atomic coordinates using method kabsch sander 1983 qian sejnowski 1988 collected database 106 proteins containing 128 protein segments called subunits used set proteins segments used parallel experiment sigillito 1989 using back propagation identical data reproduced classification accuracy results qian initial experiment divided data training set containing 100 protein segments test set containing 28 segments overlap two sets table 4 shows composition two sets table 4 shows percentages three categories table 4 composition training test sets protein segments residues ff fi coil train 100 17142 261 195 544 test 28 4476 218 231 551 approximately test set training set 3 protein segments separated main experiments ie instances drawn one segment resided together either training testing set pebls trained described training set using equation 2 found preliminary experiments produced slightly improved accuracy domain repeated main experiment variety different window sizes ranging 3 21 domain pebls included postprocessing algorithm based minimal sequence length restrictions used holley karplus 1989 restrictions stated fisheet must consist contiguous sequence fewer two residues ffhelix fewer four subsequences predicted conform restrictions individual residues reclassified coil qian sejnowski 1988 used different form postprocessing called cascaded neural net fed output one net another network attempted reclassify residues second network designed 3 qian sejnowski carefully balanced overall frequencies three categories training test sets attempted addition used training set 18105 residues slightly smaller although databases identical access specific partitioning training test sets used qian sejnowski table 5 classification accuracy window size window unweighted holley qian size pebls pebls karplus sejnowski 9 647 656 623 623 19 692 710 626 take advantage correlations neighboring secondary structure assignments results classification accuracy given table 5 un weighted pebls column shows results using pebls without weights wx exemplars entries table 5 percentages correct predictions test set table shows highest accuracy produced pebls achieved 710 window size 19 qian sejnowski obtained best result 643 using cascaded network ar chitecture used single network design similar holley karplus best result 627 best performance pebls without postprocessing 678 best conventional technique reported holley karplus produced accuracies 55 also performed experiments using overlap metric produced accuracies table comparison correlation coefficients algorithm correct c ff c fi c coil pebls 710 047 045 040 qian sejnowski 643 041 031 041 holley karplus 632 041 032 046 5560 range different window sizes matched pairs analysis reveals weighted version pebls performs significantly better unweighted version particular ttest shows weighted version better 9995 confidence level 9 thus exemplar weights improve performance significantly another frequently used measure performance domain correlation coefficients provide measure accuracy categories defined following equation mathews p ff number times ff correctly predicted n ff number times ff correctly rejected ff number false positives ff u ff number misses ff correct predicted similar definitions used c fi c coil coefficients pebls two back propagation experiments appear table 6 variations training set size third measure classification performance involves repeated testing randomly selected test sets table 7 shows table 7 training pebls varying percentages data set training set percent correct size test set 50 602 70 623 90 651 performance pebls weighted trained varying percentages randomly selected instances entire data set using window size 19 trial set examples chosen random train ing examples removed data set test phase used remaining examples since protein comprises many examples different parts single protein could appear training test set given trial classification accuracy table 7 averaged ten runs training set size note numbers reported table 7 reflect classification performance pebls without post processing minimal sequence length restrictions explained post processing part experiments holley karplus experiments thus performance compared weighted algorithm using window size postprocessing weighted pebls figure 678 post processing accuracy improves 710 thus see particular composition training testing sets experiment constructed mimic design earlier experiments improved accuracy learning algorithm 4 42 promoter sequences experiments run towell et al 1990 promoter sequence database leaveoneout trials methodology involves removing one element data training remaining data testing one element thus 106 instances database pebls trained 105 tested remaining 1 performed instance database entire procedure repeated 10 times time using different random order instances towell et al also repeated entire leaveoneout experiment 10 times using different randomized initial states neural nets time results shown table 8 compares pebls towell et als kbann algorithm addition report numbers obtained towell et al several machine learning algorithms including back propa gation id3 nearest neighbor overlap metric best method reported biological literature oneill 1989 recall overlap metric measures distance number features different values also worth noting 10 test runs pebls four instances caused errors three four negative 4 one likely source variation classification accuracy homologies training test sets homologous proteins structurally similar algorithm may much accurate predicting structure protein trained homologous one table 8 promoter sequence prediction algorithm pebls 4106 kbann 4106 pebls unweighted 6106 back propagation 8106 id3 19106 nearest neighbor overlap 13106 oneill 12106 instances towell notes negative examples database data used derived selecting substrings fragment e coli bacteriophage believed contain promoter sites towell et al 1990 p 865 would suggest based results four examples reexamined four examples might interesting exceptions general patterns dna promoters 43 english text pronunciation english pronunciation task used training set defined se jnowki rosenberg 1987 nettalk program set consists instances drawn brown corpus 1000 commonly used words english language unable discern difference training set somewhat restricted set shavlik shavlik et al 1989 one experimental design used training brown corpus pebls tested entire 20012 word merriam webster pocket dictionary results presented table 9 weighted unweighted versions pebls algorithm comparison table 9 english text pronunciation algorithm phoneme accuracy phonemestress pebls 782 692 pebls unweighted 791 672 back propagation 770 give results nettalk program used back propagation learning algorithm shavlik et al 1989 replicated sejnowski rosenbergs methodology part work although results differ sejnowski rosenbergs surprisingly since back propagation networks require much tuning make easier comparison property follows fact original sejnowski rosenberg study used distributed output encoding system produced 26bit sequence rather one bit 115 phonemestress combinations first 21 bits distributed encoding 51 phonemes remaining 5 bits local encoding stress types 21bit output vector matched closest descriptive vectors 51 phonemes shavlik et al explicitly compared encoding purely local encoding since output pebls always local ie output specific phoneme phonemestress combination appropriate compare methods produced output table 10 shows results 5 compared back propagation perceptron id3 quinlan 5 preliminary experiments used overlap metric database abysmal results desire improve results one reasons developed mvdm table 10 phonemestress accuracy output encoding algorithm local encoding distributed encoding correct correct back propagation 630 723 id3 642 693 perceptron 492 421 1986 latter three results shavlik et al 1989 table shows pebls performed slightly better learning methods output local encoding distributed encoding improved results id3 back propagation comparable experiments pebls would require significant changes output function yet performed shavlik et al also tested performance back propagation id3 perceptron learning function size training set performed similar experiment increasing show results table 11 graphically figure 4 comparison results averaged 10 runs different randomlychosen training sets run surprisingly performance improves steadily size training set increases surprising though good performance even small training sets table 11 pebls performance varying training set sizes percentage brown phonemes correct corpus training full dictionary 100 782 figure 4 classification accuracy function training set size 51 classification accuracy studies show classification accuracy pebls general equal slightly superior learning methods domains features notably protein structure prediction task pebls gives considerably better classification results back propaga tion without weighted exemplars noted considered fair test performance random selection residues varying percentages dataset performance figures algorithm slightly worse albeit still quite good would informative see similar experiment run neural network learning algorithm recently zhang waltz investigated hybrid learning method protein structure prediction combining nearest neighbor neural net learning statistical information figures yet published method also outperforms previous methods waltz 1990 although accuracy exceed pebls dna promoter sequence prediction towell et al 1990 report kbann technique integrates neural nets domain knowledge superior standard back propagation 9995 certainty df 18 kbann designed specifically show adding domain knowledge could improve performance neural net learning algorithm addition kbann outperformed id3 nearest neighbor nearest neighbor using overlap metric using experimental de sign pebls exactly matched performance kbann measures superior back propagation id3 oneill method data strong performance pebls data set demonstrates nearest neighbor perform well using large protein folding small promoter sequences training sets especially significant pebls using weak general method able match performance kbanns knowledgerich approach english pronunciation domain results mixed best result sejnowski rosenberg 77 superior phonemestress accuracy pebls 692 however shavlik et al replicated ex periment best result 723 note neural net results reflect distributed output encoding local encoding pebls produces back propagations classification accuracy 630 id3 642 somewhat lower pebls conclusion techniques perform similarly learning technique yet comes close performance good commercial systems much less native speakers english clearly still room considerable progress domain shavlik et al concluded based experiments classification accuracy versus number training examples see figure 4 nettalk data small amounts training data back propagation preferable decision trees constructed id3 however results indicate nearest neighbor algorithms also work well training set small performance curve figure 4 shows pebls needs examples achieve relatively good performance 52 transparency representation operation trained given domain pebls contains memory set information relatively perspicuous comparison weight assignments neural network exemplars provide specific reference instances case histories may cited support particular decision information may easily gathered training even testing phase shed additional light domain question instance consider attaching counter exemplar incrementing time exemplar used exact match comparing number times exemplar used get good idea whether exemplar specific exception part general rule examining weight wx attach exemplars determine whether instance reliable clas sifier distance tables reveal order set symbolic values apparent values alone hand derivation distances perspicuous derived global characteristics training data english pronunciation task distributed output encodings shown produce superior performance local encodings shavlik et al 1989 result points weakness pebls 1nearest neighbor method allow distributed output encodings neural nets handle encodings quite easily decision trees handle difficulty shavlik et al built separate decision tree 26 bits distributed encoding phonemestress pairs task raises question whether nearest neighbor methods handle encodings one possibility use knearest neighbor would allow one exemplar determine output bits eg exemplar contained 26bit encoding predicted value bit new example would determined majority vote k nearest neighbors bit experiments required determine strategy would advantageous general transparency operation learning classification algorithms nearest neighbor algorithm simple basic learning routine simply stores new examples memory pebls computation exemplar weights nothing simple recordkeeping based classification performance existing exemplars adding exemplars changing weights change way nearest neighbor algorithms partition feature space illustrated exception spaces although may hard visualize three dimensions nonetheless straightforward compare operation adjustment weights back propagation algorithm thus far researchers found difficult characterize classification performance changed connection weights changed one minor drawback pebls method nonincremental unlike back propagation versions decision tree methods incremental extension pebls would probably quite expensive since value difference tables might recomputed many times hand extending pebls handle mixed symbolic numeric data quite straightforward algorithm could use simple differences numeric fea tures value difference tables symbolic ones finally experiments protein domain demonstrated use weights attached exemplars improve accuracy nearest neighbor algorithms domains english pronunciation weights make significant difference based results earlier results realvalued domains salzberg 1990 1991 conclude exemplar weights offer real potential enhancing power practical learning algorithms 6 conclusion demonstrated series experiments instancebased learning algorithm perform exceptionally well domains features values symbolic direct comparisons implementation pebls performed well better back propagation id3 several domainspecific learning algorithms several difficult classification tasks addition nearest neighbor offers clear advantages much faster train representation relatively easy interpret one yet knows interpret networks weights learned neural nets decision trees somewhat easier interpret hard predict impact new example structure tree sometimes one new example makes difference times may radically change large portion tree hand neural nets fixed size decision trees tend quite small respect methods compress data way nearest neighbor addition classification time fast dependent depth net tree size input based classification accuracy though clear learning techniques advantage nearestneighbor methods respect nearest neighbor learning per se shown weighting exemplars improve performance subdividing instance space manner reduces impact unreliable examples nearest neighbor algorithm one simplest learning methods known yet algorithm shown outperform consistently taken together results indicate continued research extending improving nearest neighbor learning algorithms prove fruitful acknowledgements thanks joanne houlahan david aha numerous insightful comments suggestions thanks also richard sutton three anonymous reviewers detailed comments ideas research supported part air force office scientific research grant afosr890151 national science foundation grant iri9116843 r study instancebased algorithms supervised learning tasks prediction secondary structure proteins amino acid sequence turn prediction proteins using pattern matching approach nearest neighbor pattern classification certain aspects anatomy physiology cerebral cortex comparative study id3 backpropagation english texttospeech mapping fgp virtual machine acquiring knowledge cases empirical comparison id3 backpropagation analysis accuracy implication simple methods predicting secondary structure globular proteins connectionist models learn learning representation connectionist networks protein secondary structure prediction dictionary protein secondary struc ture pattern recognition hydrogenbonded geometric features automatic lettertophoneme transcription speech synthesis ariadne patterndirected inference hierarchical abstraction protein structure recogni tion algorithms prediction ffhelical betastructural regions globular proteins comparison predicted observed secondary structure t4 phage lysozyme distributed model human learning memory context theory classification learning escherichia coli promoters computational geometry intro duction predicting secondary structure globular proteins using neural network models pattern recognition categorization learning representations backpropagating errors pdp research group nested hyperrectangles exemplarbased learning learning nested generalized exemplars nearest hyperrectangle learning method nettalk parallel network learns read aloud symbolic neural learning algorithms experimental comparison personal communication toward memorybased reasoning refinement approximate domain theories knowledgebased neural networks massively parallel ai empirical comparison pattern recognition tr ctr walter daelemans peter berck steven gillis unsupervised discovery phonological categories supervised learning morphological rules proceedings 16th conference computational linguistics august 0509 1996 copenhagen denmark david waltz simon kasif reasoning data acm computing surveys csur v27 n3 p356359 sept 1995 vronique hoste walter daelemans iris hendrickx antal van den bosch dutch word sense disambiguation optimizing localness context proceedings acl02 workshop word sense disambiguation recent successes future directions p6166 july 11 2002 l mangasarian j b rosen e thompson convex kernel underestimation functions multiple local minima computational optimization applications v34 n1 p3545 may 2006 tomuro question terminology representation question type classification coling02 computerm 2002 second international workshop computational terminology p17 august 31 2002 rafael alonso jeffrey bloom hua li chumki basu adaptive nearest neighbor search parts acquisition eportal proceedings ninth acm sigkdd international conference knowledge discovery data mining august 2427 2003 washington dc michael pazzani daniel billsus learning revising user profiles identification ofinteresting web sites machine learning v27 n3 p313331 june 1997 luca cazzanti maya r gupta local similarity discriminant analysis proceedings 24th international conference machine learning p137144 june 2024 2007 corvalis oregon perrizo amal perera parameter optimized vertical nearestneighborvote boundarybased classification acm sigkdd explorations newsletter v8 n2 p6369 december 2006 ping zhang brijesh verma kuldeep kumar neural vs statistical classifier conjunction genetic algorithm based feature selection pattern recognition letters v26 n7 p909919 15 may 2005 steven salzberg arthur l delcher david heath simon kasif bestcase results nearestneighbor learning ieee transactions pattern analysis machine intelligence v17 n6 p599608 june 1995 paul losiewicz douglas w oard ronald n kostoff textual data mining support science technology management journal intelligent information systems v15 n2 p99119 septoct 2000 v hoste hendrickx w daelemans van den bosch parameter optimization machinelearning word sense disambiguation natural language engineering v8 n4 p311325 december 2002 gerard escudero llus mrquez german rigau empirical study domain dependence supervised word sense disambiguation systems proceedings 2000 joint sigdat conference empirical methods natural language processing large corpora held conjunction 38th annual meeting association computational linguistics p172180 october 0708 2000 hong kong kai ming ting discretisation lazy learning algorithms artificial intelligence review v11 n15 p157174 feb 1997 jorn veenstra antal van den bosch singleclassifier memorybased phrase chunking proceedings 2nd workshop learning language logic 4th conference computational natural language learning september 1314 2000 lisbon portugal piotr indyk rajeev motwani prabhakar raghavan santosh vempala localitypreserving hashing multidimensional spaces proceedings twentyninth annual acm symposium theory computing p618625 may 0406 1997 el paso texas united states amir ahmad lipika dey feature selection technique classificatory analysis pattern recognition letters v26 n1 p4356 1 january 2005 david w patterson mykola galushka niall rooney characterisation novel indexing technique casebased reasoning artificial intelligence review v23 n4 p359393 june 2005 jianping zhang yeesat yim jumming yang intelligent selection instances prediction functions lazylearning algorithms artificial intelligence review v11 n15 p175191 feb 1997 hwee tou ng hian beng lee integrating multiple knowledge sources disambiguate word sense exemplarbased approach proceedings 34th annual meeting association computational linguistics p4047 june 2427 1996 santa cruz california pedro domingos michael pazzani optimality simple bayesian classifier zeroone loss machine learning v29 n23 p103130 novdec 1997 naoki abe hiroshi mamitsuka predicting protein secondary structure using stochastic tree grammars machine learning v29 n23 p275301 novdec 1997 stephan raaijmakers learning distributed linguistic classes proceedings 2nd workshop learning language logic 4th conference computational natural language learning september 1314 2000 lisbon portugal christopher j merz using correspondence analysis combine classifiers machine learning v36 n12 p3358 julyaugust 1999 eyal kushilevitz rafail ostrovsky yuval rabani efficient search approximate nearest neighbor high dimensional spaces proceedings thirtieth annual acm symposium theory computing p614623 may 2426 1998 dallas texas united states philip k chan salvatore j stolfo experiments multistrategy learning metalearning proceedings second international conference information knowledge management p314323 november 0105 1993 washington dc united states anandeep pannu using genetic algorithms inductively reason cases legal domain proceedings 5th international conference artificial intelligence law p175184 may 2124 1995 college park maryland united states aristides gionis piotr indyk rajeev motwani similarity search high dimensions via hashing proceedings 25th international conference large data bases p518529 september 0710 1999 belur v dasarathy data mining tasks methods classification nearestneighbor approaches handbook data mining knowledge discovery oxford university press inc new york ny 2002 amir ahmad lipika dey method compute distance two categorical values attribute unsupervised learning categorical data set pattern recognition letters v28 n1 p110118 january 2007 charles x ling hangdong wang computing optimal attribute weight settings nearest neighboralgorithms artificial intelligence review v11 n15 p255272 feb 1997 grzegorz gra arkadiusz wojna riona new classification system combining rule induction instancebased learning fundamenta informaticae v51 n4 p369390 december 2002 mark stevenson yorick wilks interaction knowledge sources word sense disambiguation computational linguistics v27 n3 p321349 september 2001 ting liu andrew w moore alexander gray new algorithms efficient highdimensional nonparametric classification journal machine learning research 7 p11351158 1212006 wai lam chikin keung danyu liu discovering useful concept prototypes classification based filtering abstraction ieee transactions pattern analysis machine intelligence v24 n8 p10751090 august 2002 jihoon yang vasant g honavar feature subset selection using genetic algorithm ieee intelligent systems v13 n2 p4449 march 1998 jakub zavrel walter daelemans memorybased learning using similarity smoothing proceedings eighth conference european chapter association computational linguistics p436443 july 0712 1997 madrid spain ronny kohavi daniel sommerfield case studies public domain multiple mining tasks systems mlc handbook data mining knowledge discovery oxford university press inc new york ny 2002 xin dong alon halevy jayant madhavan ema nemes jun zhang similarity search web services proceedings thirtieth international conference large data bases p372383 august 31september 03 2004 toronto canada philip k chan salvatore j stolfo accuracy metalearning scalable data mining journal intelligent information systems v8 n1 p528 janfeb 1997 antal van den bosch sabine buchholz shallow parsing basis words case study proceedings 40th annual meeting association computational linguistics july 0712 2002 philadelphia pennsylvania arkadiusz wojna centerbased indexing vector metric spaces fundamenta informaticae v56 n3 p285310 august fedro domingos controlsensitive feature selection lazy learners artificial intelligence review v11 n15 p227253 feb 1997 arkadiusz wojna centerbased indexing vector metric spaces fundamenta informaticae v56 n3 p285310 august filippo neri lorenza saitta exploring power genetic search learning symbolic classifiers ieee transactions pattern analysis machine intelligence v18 n11 p11351141 november 1996 ding liu strong lower bound approximate nearest neighbor searching information processing letters v92 n1 p2329 16 october 2004 harold somers bill black joakim nivre torbjrn lager annarosa multari luca gilardoni jeremy ellman alex rogers multilingual generation summarization job adverts tree project proceedings fifth conference applied natural language processing p269276 march 31april 03 1997 washington dc nivre mario scholz deterministic dependency parsing english text proceedings 20th international conference computational linguistics p64es august 2327 2004 geneva switzerland walter daelemans antal van den bosch jakub zavrel forgetting exceptions harmful language learning machine learning v34 n13 p1141 feb 1999 juan manuel gimeno illa javier bjar alonso miquel snchez marr nearestneighbours time series applied intelligence v20 n1 p2135 januaryfebruary 2004 xudong luo jimmy homan lee hofung leung nicholas r jennings prioritised fuzzy constraint satisfaction problems axioms instantiation validation fuzzy sets systems v136 n2 p151188 june 1 omer barkol yuval rabani tighter lower bounds nearest neighbor search related problems cell probe model journal computer system sciences v64 n4 p873896 june 2002 omer barkol yuval rabani tighter bounds nearest neighbor search related problems cell probe model proceedings thirtysecond annual acm symposium theory computing p388396 may 2123 2000 portland oregon united states chaolin liu chengtsung chang jimhow ho classification clustering casebased criminal summary judgments proceedings 9th international conference artificial intelligence law june 2428 2003 scotland united kingdom roumani b skillicorn mobile services discovery selection publishsubscribe paradigm proceedings 2004 conference centre advanced studies collaborative research p163173 october 0407 2004 markham ontario canada stergios papadimitriou seferina mavroudi liviu vladutu anastasios bezerianos generalized radial basis function networks trained instance based learning data mining symbolic data applied intelligence v16 n3 p223234 mayjune 2002 piotr indyk rajeev motwani approximate nearest neighbors towards removing curse dimensionality proceedings thirtieth annual acm symposium theory computing p604613 may 2426 1998 dallas texas united states terry r payne peter edwards claire l green experience rule induction knearest neighbor methods interface agents learn ieee transactions knowledge data engineering v9 n2 p329335 march 1997 fink alfred kobsa user modeling personalized city tours artificial intelligence review v18 n1 p3374 september 2002 dietrich wettschereck david w aha takao mohri review empirical evaluation feature weighting methods aclass lazy learning algorithms artificial intelligence review v11 n15 p273314 feb 1997 miquel montaner beatriz lpez josep llus de la rosa taxonomy recommender agents theinternet artificial intelligence review v19 n4 p285330 june sunil arya david mount nathan netanyahu ruth silverman angela wu optimal algorithm approximate nearest neighbor searching fixed dimensions journal acm jacm v45 n6 p891923 nov 1998 stergios papadimitriou seferina mavroudi liviu vladutu g pavlides anastasios bezerianos supervised network selforganizing map classification large data sets applied intelligence v16 n3 p185203 mayjune 2002 christopher g atkeson andrew w moore stefan schaal locally weighted learning artificial intelligence review v11 n15 p1173 feb 1997 alfred kobsa jrgen koenemann wolfgang pohl personalised hypermedia presentation techniques improving online customer relationships knowledge engineering review v16 n2 p111155 march 2001 francisco azuaje werner dubitzky norman black kenny adamson retrieval strategies casebased reasoning categorised bibliography knowledge engineering review v15 n4 p371379 december 2000 francisco azuaje werner dubitzky norman black kenny adamson retrieval strategies casebased reasoning categorised bibliography knowledge engineering review v15 n4 p371379 december 2000