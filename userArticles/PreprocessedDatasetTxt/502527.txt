proximal support vector machine classifiers instead standard support vector machine svm classifies points assigning one two disjoint halfspaces points classified assigning closest two parallel planes input feature space pushed apart far possible formulation also interpreted regularized least squares considered much general context regularized networks 8 9 leads extremely fast simple algorithm generating linear nonlinear classifier merely requires solution single system linear equations contrast standard svms solve quadratic linear program require considerably longer computational time computational results publicly available datasets indicate proposed proximal svm classifier comparable test set correctness standard svm classifiers considerably faster computational time order magnitude faster linear proximal svm easily handle large datasets indicated classification 2 million point 10attribute set 208 seconds computational results based 6 lines matlab code b introduction standard support vector machines svms 36 6 3 5 20 powerful tools data classification classify points assigning one two disjoint halfspaces halfspaces either original input space problem linear classifiers higher dimensional feature space nonlinear classifiers 36 6 20 standard svms require solution either quadratic linear program require specialized codes 7 contrast propose proximal svm psvm permission make digital hard copies part work personal classroom use granted without fee provided copies made distributed profit commercial advantage copies bear notice full citation first page copy otherwise republish post servers redistribute lists requires prior specific permission andor fee kdd 2001 san francisco ca usa classifies points depending proximity one two parallel planes pushed far apart possible fact geometrically motivated proximal formulation considered much general context regularization networks 8 9 results give extensive theoretical statistical justification proximal approach contain extensive computational implementation results given furthermore specific formulation leads strongly convex objective function always case 8 9 strong convexity plays key role simple proximal code provided well fast computational times obtained obtaining linear nonlinear psvm classifier requires nothing sophisticated solving single system linear equations efficient fast linear equation solvers freely available 1 part standard commercial packages matlab 26 solve large systems fast briefly summarize contents paper section 2 introduce proximal linear support vector machine give linear proximal algorithm 21 explicit expression leaveoneoutcorrectness terms problem data 16 section 3 introduce proximal kernelbased nonlinear support vector machine corresponding nonlinear classifier 28 nonlinear proximal algorithm 31 section 3 contains many numerical testing results linear nonlinear classifiers based extremely simple matlab 26 code 6 lines linear nonlinear psvm results surpass algorithms compared speed give comparable testing set correctness word notation background material vectors column vectors unless transposed row vector prime superscript vector x ndimensional real space r n step function stepx defined n scalar inner product two vectors x ndimensional real space r n denoted x 2norm x denoted x matrix r mn ith row row vector r n j jth column column vector ones arbitrary dimension denoted e r mn b r nk kernel kab maps r mn r nk r mk particular x column vectors r n kx real number kx row vector r kaa mm matrix base natural logarithm denoted make use following gaussian kernel 36 6 20 frequently used svm literature 1 r mn b r nk positive constant identity matrix arbitrary dimension denoted 2 linear proximal support vector consider problem depicted figure 1 classifying points ndimensional real space r n represented n matrix according membership point class specified given mm diagonal matrix plus ones minus ones along diagonal problem standard support vector machine linear kernel 35 6 given following quadratic program parameter 0 min wyr n1m 2 depicted figure 1 w normal bounding planes bound sets respectively constant determines location relative origin two classes strictly linearly separable error variable 2 case shown figure 1 plane x class points plane x class points follows consequently plane midway bounding planes 3 separating plane separates completely approximately depicted figure 1 quadratic term 2 twice reciprocal square 2norm distance 2 w two bounding planes 3 see figure 1 maximizes distance often called margin maximizing margin enhances generalization capability support vector machine 35 6 classes linearly inseparable case shown figure 1 two planes bound two classes soft margin ie bound approximately error determined nonnegative error variable 1norm error variable minimized parametrically weight 2 resulting approximate separating plane 5 depicted figure 1 plane acts linear classifier follows 0 x point departure similar 23 24 optimization problem 2 replaced following problem min wyr n1m note explicit nonnegativity constraint needed component negative objective function decreased setting still satisfying corresponding inequality constraint note 2norm error vector minimized instead 1norm margin bounding planes maximized respect orientation w relative location origin extensive computational experience 22 23 24 18 17 indicates formulation good classical formulation 2 added advantages strong convexity objective function key idea present paper make simple fundamental change formulation 8 namely replace inequality constraint equality follows min wyr n1m modification even though simple changes nature optimization problem significantly fact turns write explicit exact solution problem terms problem data show whereas impossible previous formulations combinatorial nature geometrically formulation depicted figure 2 interpreted follows planes x w 1 bounding planes anymore thought proximal planes around points class clustered pushed far apart possible term w objective function nothing reciprocal 2norm distance squared two planes w space r n1 x x x x x x x x psfrag replacements w separating plane x figure 1 standard support vector machine classifier wspace r n approximately bounding planes equation 3 soft ie error margin 2 w plane equation approximately separating x x x x x x x x x x x x psfrag replacements separating plane x w figure 2 proximal support vector machine classifier w space r n1 planes points sets cluster pushed apart optimization problem 9 note formulation 9 also interpreted regularized least squares solution 34 system linear equations daw e finding approximate solution w least 2norm similarly standard svm formulation 2 interpreted using linear programming perturbation theory 21 least 2 norm approximate solution system linear inequalities e neither interpretations however based idea maximizing margin distance parallel planes 3 key feature support vector machines 36 6 20 karushkuhntucker kkt necessary sucient optimality conditions 19 p 112 equality constrained problem obtained setting equal zero gradients respect w u lagrangian lw u lagrange multiplier associated equality constraint 9 setting gradients l equal zero gives following kkt optimality conditions first three equations 11 give following expressions original problem variables w terms lagrange multiplier u 12 substituting expressions last equality 11 allows us obtain explicit expression u terms problem data follows h defined u 13 explicit solution w problem 9 given 12 solution 13 u entails inversion possibly massive mm matrix make immediate use shermanmorrisonwoodbury formula 14 p 51 matrix inversion done 23 10 24 results expression well another simple expression 29 involve inversion much smaller dimensional matrix order n completely solves classification problem concreteness explicitly state simple algorithm algorithm 21 linear proximal svm given data points r n represented n matrix diagonal matrix 1 labels denoting class row generate linear classifier 7 follows define h 14 e 1 vector ones compute u 15 positive typically chosen means tuning validating set ii determine w 12 iii classify new x using 7 standard svms support vectors consist data points complement data points dropped problem without changing separating plane 5 36 20 thus standard svm formulation 2 support vectors correspond data points lagrange multipliers nonzero solving 2 data points give answer solving entire dataset proximal formulation however lagrange multipliers u merely multiple error vector given 12 con sequently components typically nonzero since none data points usually lie proximal planes x concept support vectors needs modified follows w r n1 given linear functions 11 follows basis theorem linear equations 13 theorem 21125 lemma 21 applied last equality 11 fixed value error vector n 1 linearly independent data points needed determine basic nonzero components w r n1 guided fact small number data points characterize specific w define concept support vectors data points error vector less absolute value typically pick small enough 1 data support vectors resolving proximal svm problem 9 data points adjusted typically upwards tuning set gives test set correctness essentially identical obtained using entire dataset note explicit expressions w u terms problem data given 12 15 able get also explicit expression leaveoneoutcorrectness looc 32 fraction correctly classified data points point turn left psvm formulation classified classifier 7 omitting algebra following leaveoneoutcorrectness step function defined introduction h defined 14 h denotes row h h denotes h row h removed h u defined 15 h replaced h similarly denotes row extend results nonlinear proximal support vector machines 3 nonlinear proximal support vector machines obtain nonlinear proximal classifier modify equality constrained optimization problem 9 20 18 replacing primal variable w dual equivalent du 12 obtain min e objective function also modified minimize weighted 2norm sums problem variables u replace linear kernel aa nonlinear kernel defined introduction obtain min using shorthand notation lagrangian 19 written similarly 10 lu v r lagrange multiplier associated equality constraint 19 setting gradients lagrangian respect u v equal zero gives following kkt optimality conditions first three equations 22 give following expressions u terms lagrange multiplier v 23 substituting expressions last equality 22 gives explicit expression v terms problem data follows e 24 g defined note similarity g h defined 14 similarity allows us obtain g expression replacing k 14 taken advantage matlab code 41 algorithm 21 written linear classifier 7 thus generate nonlinear classifier algorithm 31 merely replace k algorithm solution v 24 solution u problem 19 given 23 unlike situation linear kernels shermanmorrisonwoodbury formula useless kernel matrix square inversion must take place potentially highdimensional r however reduced kernel techniques 17 utilized reduce dimensionality kernel much smaller dimensionality rectangular kernel small 1 random submatrix reduced kernels make large problems tractable also often lead improved generalization avoiding data overfitting eectiveness reduced kernels demonstrated means numerical test problem next section paper nonlinear separating surface corresponding kernel equation 81 deduced linear separating surface 5 follows replace x corresponding kernel expression substitute 23 u obtain nonlinear separating surface corresponding nonlinear classifier nonlinear separating surface 0 x give explicit statement nonlinear classifier algorithm algorithm 31 nonlinear proximal svm given data points r n represented n matrix diagonal matrix 1 labels denoting class row generate nonlinear classifier 28 follows choose kernel function kaa typically gaussian kernel 1 ii define g 25 vector ones compute v 24 positive typically chosen means tuning set iii nonlinear surface 27 computed v constitutes nonlinear classifier 28 classifying new point x nonlinear classifier 28 direct generalization linear classifier 7 works quite eectively indicated numerical examples presented next section 4 numerical implementation comparison computations performed university wisconsin data mining institute locop1 machine utilizes 400 mhz pentium ii allows maximum 2 gigabytes memory process computer runs windows nt server 40 matlab 6 installed even though locop1 multiprocessor machine one processor used experiments since matlab single threaded application distribute load across processors 26 algorithms require solution single square system linear equations size number input attributes n linear case size number data points nonlinear case using rectangular kernel 18 size problem reduced k k nonlinear case simplicity algo rithm give actual matlab implementation used experiments consists 6 lines native matlab code 226 figure 3 spiral dataset consisting 97 black points 97 white points intertwined two spirals 2dimensional space psvm gaussian kernel generated sharp nonlinear spiralshaped separating code 41 psvm matlab code function psvmlinear nonlinear classification note command line matlab code computes directly factor 15 much economical stable computing inverse explicitly multiplying h e calculations h e involve transpose typically large matrices time consuming instead calculate rsumh wsa respectively transposes vectors note matlab code works linear classifier also nonlinear classifier well nonlinear case matrix kaa used input instead 20 equations 1 10 pair returned instead w nonlinear separating surface given 27 rectangular kernels 17 also handled code input rectangular matrix ka r mk k given output pair u u u associated reduced matrix final note regarding simplification psvm substitute expression 15 u 12 obtain algebra following simple expression w terms problem data e direct explicit solution psvm problem written following single line matlab code also perform explicit matrix inversion e 1 slightly faster matlab code according matlab commands diagd m1 vector generated diagonal matrix computational testing results using oneline matlab code slightly better obtained code 41 ones reported tables comment solution 29 also obtained directly 9 using equality constraint eliminate problem solving resulting unconstrained minimization problem variables w setting zero gradients respect w turn computations datasets used numerical tests following seven publicly available datasets uci machine learning repository 28 wpbc ionosphere cleveland heart pima indians bupa liver mush room tictactoe census dataset version us census bureau adult dataset publicly available silicon graphics website 4 galaxy dim dataset used galaxy discrimination neural networks 30 two large datasets 2 million points 10 attributes created using david musicants ndc data generator 29 spiral dataset proposed alexis wieland mitre corporation available cmu artificial intelligence repository 37 outline computational results five groups follows 1 table 1 comparison seven dierent methods adult dataset experiment compared performance seven dierent methods linear classification dierent sized versions adult dataset reported results sor 22 smo 31 svm light 16 22 results lsvm 24 results computed using locop1 whereas ssvm 18 rlp 2 18 smo experiments run 266 mhz pentium ii processor windows nt 4 using microsofts visual c 50 compiler sor experiments run 200 mhz pentium pro 64 megabytes ram also windows nt 4 using visual c 50 svm light experiments run hardware sor solaris 56 operating system bold type indicates best result dash indicates results available 22 although timing comparisons approximate dierent machines used indicate psvm distinct edge speed eg solving largest problem 74 seconds much faster method times tenfold testing correctness shown table 1 times tenfolds 2 table 4 comparative performances lsvm 24 psvm large dataset two large datasets consisting 2 million points attributes created using ndc data generator 29 one called ndceasy highly linearly separable around 90 one called ndchard since linear separability around 70 shown table 4 linear classifiers obtained using methods performed almost identically despite 2 million size datasets psvm solved problems 20 seconds compared lsvms times 650 seconds contrast svm light 16 failed problem 24 3 table 3 comparison psvm ssvm lsvm svm light using linear classifier experiment compared four methods psvm ssvm lsvm svm light seven publicly available datasets uci machine learning repository 28 30 shown table 3 correctness four methods similar execution time including tenfold cross validation psvm smaller much one order magnitude three methods tested since lsvm ssvm psvm based similar formulations classification problem value used svm light tradeo trading error margin represented parameter c value c chosen tuning paired ttest 27 95 confidence level performed compare performance psvm algorithms tested pvalues obtained show significant dierence psvm methods tested 4 figure 3 psvm spiral dataset used gaussian kernel order classify spiral dataset dataset consisting 194 black white points intertwined shape spiral synthetic dataset 37 however apparently di cult test case data mining algorithms known give neural networks severe problems 15 con trast sharp separation obtained using psvm seen figure 3 5 table 2 nonlinear classifier comparison using psvm ssvm lsvm experiment chose four datasets uci machine learning repository known nonlinear classifier performs significantly better linear classifier used psvm ssvm lsvm order find gaussiankernelbased nonlinear classifier classify data datasets tested three methods performed similarly far tenfold cross validation concerned however execution time psvm much smaller two methods note mushroom dataset consists points attributes square 8124 8124 kernel matrix fit memory order address prob lem used rectangular kernel r 2158124 instead described 17 general algorithm performed particularly well rectangular kernel since system solved size k k k k much smaller number rows contrast full square kernel matrix system solved size paired ttest 27 95 confidence level performed compare performance psvm algorithms tested pvalues obtained show significant difference psvm methods tested far tenfold testing correctness concerned 5 conclusion future work proposed extremely simple procedure generating linear nonlinear classifiers based proximity one two parallel planes pushed far apart pos sible procedure proximal support vector machine psvm requires nothing sophisticated solving simple nonsingular system linear equations either linear nonlinear classifier contrast standard support vector machine classifiers require costly solution linear quadratic program linear classifier needed psvm inversion small matrix order input space dimension typically order 100 less even millions data points clas sify nonlinear classifier linear system equations order number data points needs solved allows us easily classify datasets many thousand points larger datasets data selection reduction methods 11 17 12 utilized indicated numerical results subject future work computational results demonstrate psvm classifiers obtain test set correctness statistically comparable standard svm classifiers fraction time sometimes order magnitude less another avenue future research incremental classification large datasets appears particularly promising view simple explicit solutions 24 linear nonlinear psvm classifiers updated incrementally new data points come streaming sum principal contribution work ecient classifier requires specialized software psvm easily incorporated sorts data mining applications require fast simple eective classifier acknowledgements research described data mining institute report 0102 february 2001 supported national science foundation grants ccr9729842 cda9623632 air force oce scientific research grant f496200010085 microsoft corporation grateful professor cj lin national taiwan university pointed reference 33 upon reading original version paper least squares also used 33 construct svm explicit requirement mercers positive definiteness condition 35 needed fur thermore objective function quadratic program 33 strongly convex like important feature psvm influences speed evidenced many numerical comparisons given 33 6 r lapack users guide robust linear programming discrimination two linearly inseparable sets massive data discrimination via linear support vector machines us census bureau tutorial support vector machines pattern recognition learning data concepts cplex optimization inc regularization networks support vector machines regularization networks support vector machines interior point methods massive support vector machines data selection support vector machine classification theory linear economic models matrix computations data mining sparse grids making largescale support vector machine learning practical rsvm reduced support vector machines ssvm smooth support vector machine nonlinear programming generalized support vector machines nonlinear perturbation linear programs successive overrelaxation support vector machines active support vector machine classification lagrangian support vector machines lipschitz continuity solutions linear inequalities mathworks machine learning uci repository machine learning databases ndc normally distributed clustered datasets automated stargalaxy discrimination neural networks sequential minimal optimization fast algorithm training support vector machines advances large margin classifiers least squares support vector machine classifiers solutions illposed problems nature statistical learning theory nature statistical learning theory twin spiral dataset tr lipschitz continuity solutions linear inequalities programs complementarity problems nature statistical learning theory matrix computations 3rd ed making largescale support vector machine learning practical fast training support vector machines using sequential minimal optimization least squares support vector machine classifiers data selection support vector machine classifiers machine learning learning data tutorial support vector machines pattern recognition lagrangian support vector machines ctr wenye li kinhong lee kwongsak leung largescale rlsc learning without agony proceedings 24th international conference machine learning p529536 june 2024 2007 corvalis oregon soumen chakrabarti shourya roy mahesh v soundalgekar fast accurate text classification via multiple linear discriminant projections proceedings 28th international conference large data bases p658669 august 2023 2002 hong kong china tsong song hwang tsungju lee yuhjye lee threetier ids via data mining approach proceedings 3rd annual acm workshop mining network data june 1212 2007 san diego california usa simon hill arnaud doucet adapting twoclass support vector classification methods many class problems proceedings 22nd international conference machine learning p313320 august 0711 2005 bonn germany thorsten joachims training linear svms linear time proceedings 12th acm sigkdd international conference knowledge discovery data mining august 2023 2006 philadelphia pa usa glenn fung sathyakama sandilya r bharat rao rule extraction linear support vector machines proceeding eleventh acm sigkdd international conference knowledge discovery data mining august 2124 2005 chicago illinois usa hwanjo yu jiong yang jiawei han xiaolei li making svms scalable large data sets using hierarchical cluster indexing data mining knowledge discovery v11 n3 p295321 november 2005 kristin p bennett michinari momma mark j embrechts mark boosting algorithm heterogeneous kernel models proceedings eighth acm sigkdd international conference knowledge discovery data mining july 2326 2002 edmonton alberta canada hwanjo yu jiong yang jiawei han classifying large data sets using svms hierarchical clusters proceedings ninth acm sigkdd international conference knowledge discovery data mining august 2427 2003 washington dc tonatiuh pea centeno neil lawrence optimising kernel parameters regularisation coefficients nonlinear discriminant analysis journal machine learning research 7 p455491 1212006 yang ali r hurson contentaware search multimedia data ad hoc networks proceedings 8th acm international symposium modeling analysis simulation wireless mobile systems october 1013 2005 montral quebec canada bin li mingmin chi jianping fan xiangyang xue support cluster machine proceedings 24th international conference machine learning p505512 june 2024 2007 corvalis oregon dacheng tao xuelong li xindong wu weiming hu stephen j maybank supervised tensor learning knowledge information systems v13 n1 p142 september 2007 hwanjo yu xiaoqian jiang jaideep vaidya privacypreserving svm using nonlinear kernels horizontally partitioned data proceedings 2006 acm symposium applied computing april 2327 2006 dijon france brian whitman deb roy barry vercoe learning word meanings descriptive parameter spaces music proceedings hltnaacl workshop learning word meaning nonlinguistic data p9299 may 31 soumen chakrabarti shourya roy mahesh v soundalgekar fast accurate text classification via multiple linear discriminant projections vldb journal international journal large data bases v12 n2 p170185 august glenn fung murat dundar jinbo bi bharat rao fast iterative algorithm fisher discriminant using heterogeneous kernels proceedings twentyfirst international conference machine learning p40 july 0408 2004 banff alberta canada glenn fung l mangasarian multicategory proximal support vector machine classifiers machine learning v59 n12 p7797 may 2005 deepak k agarwal shrinkage estimator generalizations proximal support vector machines proceedings eighth acm sigkdd international conference knowledge discovery data mining july 2326 2002 edmonton alberta canada glenn fung olvi l mangasarian alexander j smola minimal kernel classifiers journal machine learning research 3 p303321 312003 w chaovalitwongse p pardalos time series support vector machine using dynamic time warping kernel brain activity classification cybernetics systems analysis v44 n1 p125138 january 2008 yuhjye lee wenfeng hsieh chienming huang epsilonssvr smooth support vector machine epsiloninsensitive regression ieee transactions knowledge data engineering v17 n5 p678685 may 2005 ryan rifkin aldebaro klautau defense onevsall classification journal machine learning research 5 p101141 1212004 rolando grave de peralta menendez quentin noirhomme febo cincotti donatella mattia fabio aloise sara gonzlez andino modern electrophysiological methods braincomputer interfaces computational intelligence neuroscience v2007 n2 p111 april 2007 rolando grave de peralta menendez quentin noirhomme febo cincotti donatella mattia fabio aloise sara gonzlez andino modern electrophysiological methods braincomputer interfaces computational intelligence neuroscience v7 n3 p18 august 2007