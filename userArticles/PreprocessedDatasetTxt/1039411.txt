loadbalancing scatter operations grid computing present solutions statically loadbalance scatter operations parallel codes run grids loadbalancing strategy based modification data distributions used scatter operations study replacement scatter operations parameterized scatters allowing custom distributions data paper presents 1 general algorithm finds optimal distribution data across processors 2 quicker guaranteed heuristic relying hypotheses communications computations 3 policy ordering processors experimental results mpi scientific code illustrate benefits obtained loadbalancing b introduction traditionally users developed scientific applications parallel computer mind assuming homogeneous set processors linked homogeneous fast network however grids 10 computational resources usually include heterogeneous processors heterogeneous network links orders magnitude slower parallel computer therefore execution grids applications designed parallel computers usually leads poor performance distribution workload take heterogeneity account hence need tools able analyze transform existing parallel applications improve performances heterogeneous environments loadbalancing ex ecution furthermore willing fully rewrite original applications rather seeking transformations modify original source code little possible research supported french ministry research acigrid program among usual operations found parallel codes scatter operation one collective operations usually shipped message passing libraries instance mostly used message passing library mpi 16 provides mpiscatter primitive allows programmer distribute even parts data processors mpi communicator less intrusive modification enabling performance gain heterogeneous environment consists using communication library adapted heterogeneity thus much work devoted purpose mpi numerous projects including magpie 15 mpistart 13 mpichg2 8 aim improving communications performance presence heterogeneous networks gain obtained reworking design collective communication primitives instance mpichg2 performs often better mpich disseminate information held processor several others mpich always use binomial tree propagate data mpichg2 able switch flat tree broadcast network latency high 14 making communication library aware precise network topology easy mpichg2 queries underlying globus 9 environment retrieve information network topology user may specified environment variables networkaware libraries bring interesting results compared standard communication libraries however improvements often sufficient attain performance considered acceptable users processors also heteroge neous balancing computation tasks processors also needed take benefit grids typical usage scatter operation spawn spmd computation section processors received piece data thereby computation load processors depends data received use scatter operation means loadbalance com putations provided items data set scatter independent mpi provides primitive mpiscatterv allows distribute unequal shares data claim replacing mpiscatter mpiscatterv calls parameterized clever distributions may lead great performance improvements low cost term source code rewriting transformation operations require deep source code reorganization easily automated software tool problem thus loadbalance execution computing data distribution depending processors speeds network links bandwidths section 2 present target application real scientific application geophysics written mpi ran raytrace full set seismic events year 1999 section 3 present loadbalancing techniques section 4 processor ordering policy derive case study section 5 experimental results section 6 related works conclude section 7 2 motivating example 21 seismic tomography geophysical code consider seismic tomography field general objective applications build global seismic velocity model earth inte rior various velocities found different points discretized model generally mesh reflect physical rock properties locations seismic waves velocities computed seismograms recorded cap tors located around globe analyzed wave type earthquake hypocenter captor locations well wave travel time determined data tomography application reconstructs event using initial velocity model wave propagation source hypocenter given captor defines path application evaluates given properties initial velocity model time wave propagate along evaluated path compared actual travel time final step new velocity model minimizes differences computed process accurate new model better fits numerous paths many locations inside earth therefore computationally demanding 22 example application outline application study exploits potential parallelism computations tasks distributed across processors recall input data set seismic waves characteristics described pair 3d coordinates coordinates earthquake source receiving captor plus wave type characteristics seismic wave modeled set ray paths represents wavefront propagation seismic wave characteristics sufficient perform raytracing whole associated ray path therefore ray paths traced independently existing parallelization application presented 12 assumes homogeneous set processors implicit target parallel computer following pseudocode outlines main communication computation phases raydata read n lines data file mpiscatterraydata rbuff root p number processors involved n number data items mpiscatter instruction executed root computation processors processor identified root performs send contiguous blocks bnp c elements raydata buffer processors group processors make receive operation respective data rbuff buffer sake simplicity remaining n mod p items distribution shown figure 1 shows potential execution communication operation p 4 root process time idle receiving sending computing figure 1 scatter communication followed computation phase 23 hardware model figure 1 outlines behavior scatter operation observed applications runs test grid described section 51 behavior indication networking capabilities root node send one destination node time singleport model 4 realistic grids many nodes simple pcs fullduplex network cards root processor sends data processors turn 1 receiving processor actually begins communication previous 1 mpich implementation order destination processors scatter operations follows processors ranks processors served leads stair effect represented figure 1 end times receive operations black boxes 3 static loadbalancing overall execution time loadbalancing rather small make assumption grid characteristics change computation consider static loadbalancing note also computed distribution necessarily based static parameters estimated whole execution monitor daemon process like 19 running aside application could queried scatter operation retrieve instantaneous grid characteristics 31 framework consider set p processors characterized 1 time comp x takes compute x data items 2 time tcomm x takes receive x data items root process want process data items look distribution n 1 n p data p processors minimizes overall computation time paper root processor last process p p start process share data items sent data items processors root processor sends data processors turn processor p begins communication processors served takes time tcomm j n j root takes tcomm n send p data p takes comp n process thus p ends processing time time taken system compute set n data items therefore 1ip 2 looking distribution n 1 n p minimizing duration 32 exact solution dynamic programming studying equation 2 remark time process n data processors 1 p equal maximum 1 time taken root send n 1 data p 1 plus time taken p 1 process 2 time processors 2 p process n n 1 data plus time root send n 1 data p 1 leads dynamic programming algorithm 1 distribution expressed list hence use list constructor cons algorithm 1 costd denotes cost processing data items processors p p p solutiond list describing distribution data items processors achieve minimal execution time costd algorithm 1 compute optimal distribution n data processors solution0 p cons0 nil 1 n solutiond p consd nil 1 n e 1 sol e costd min solutiond return costn 1 solutionn 1 algorithm 1 complexity op n 2 may prohibitive algorithm 1 assumes functions tcomm x comp x nonnegative present efficient heuristic valid simple cases 33 guaranteed heuristic using linear program ming section make hypothesis functions tcomm n comp n affine n increas ing nonnegative n 0 equation 2 coded following linear program minimize linear program must solved integer find integer solution however solve rational obtain optimal rational solution n 1 n p round obtain integer solution n 0 let 0 execution time solution time rational solution opt time optimal integer solution jn n 0 easily enforced rounding scheme described 1ip indeed hypothesis tcomm j x comp j x non negative increasing affine functions therefore tcomm j equivalent upper bound comp j n 0 using upper bounds overapproximate expression 0 given equation 5 obtain implies equation 4 knowing opt 0 rounding scheme rounding scheme trivial first round nearest integer n nearest integer obtain n 0 make approximation error jej 1 e negative underestimated resp overesti mated approximation round ceiling resp floor one remaining n j nearest ceiling dn j e resp floor bn j c obtain new approximation error jej 1 remains approximate one say n let n 0 e distribution n 0 n 0 thus integer n 0 differs n less one 34 choice root process make assumption originally n data items must processed stored single computer denoted c processor c may may used root processor root processor c whole execution time equal time needed transfer data c root processor plus execution time computed algorithm 1 best root processor processor minimizing whole execution time picked root result minimization p candidates 4 case study solving rational linear communication computation times section study simple theoretical case make hypothesis functions tcomm n comp n linear n words constants tcomm also look rational solution integer one case study enable us define policy order processors must receive data indeed simple case processor ordering leading shortest execution time quite simple show section 43 prove section 42 always optimal rational solution working processors ending time also show condition processor receive share whole work condition comes expression execution duration processors process share whole work finishes date begin studying case section 41 finally section 44 derive case study consequences general case 41 execution duration theorem 1 execution duration looking rational solution processor p receives non empty share n whole set n data items processors end computation date execution time processor p receives data process proof want express n functions n equation 2 states processor p ends processing time current hypotheses hypothesis processors end processing time find equation 8 express execution duration function n sum equation 8 values 1 p equivalent equation 7 rest paper note hypotheses theorem 1 42 simultaneous endings paragraph exhibit condition costs functions tcomm n comp n set processors necessary sufficient optimal rational solution processor receives nonempty share data processors end date tells us theorem 1 used find rational solution system theorem 2 simultaneous endings given p processors p p whose communication computation duration functions tcomm n comp n linear n exists optimal rational solution processor receives nonempty share whole set data processors end computation date proof proof made induction number processors one processor theorem trivially true shall next prove theorem true p processors also true p1 processors suppose optimal solution compute n data items obtained giving n items p 1 1 n items p 2 p p1 0 1 end date processor p 1 1 theorem supposed true p proces sors know exists optimal rational solution processors p 2 p p1 work finish work simultaneously 8i 2 2 p case theorem 1 time taken p data processors p 2 end date 2 1 k 1 strictly increasing 2 decreasing moreover 1 0 2 0 thus whole end date maxt 1 minimized unique 0 1 1 case processor data compute end date contrary 1 k 1 2 strictly increasing thus whole end date minimized case processor p 1 nothing compute end date 0 processors p 2 p p1 end date k n thus exists optimal rational solution receives nonempty share whole set data processors end computation date 8i 2 1 p proof theorem 2 shows processor p dp interesting problem using increase whole processing time therefore forget processors theorem 2 states optimal rational solution remaining processors working date 43 processor ordering policy stated section 23 root processor sends data processors turn receiving processor actually begins communication previous processors received shares data moreover mpich implementation mpi order destination processors scatter operations follows processors ranks defined programmer therefore setting processor ranks influence order processors start receive process share whole work equation 7 shows case overall computation time symmetric processors depends ordering therefore must carefully defines ordering order speedup whole computation appears current case best ordering quite simple theorem 3 processor ordering policy functions tcomm n comp n linear n 1 p 1 looking rational solution smallest execution time achieved processors root processor excepted ordered decreasing order bandwidth p 1 processor connected root process highest bandwidth p p 1 processor connected root processor smallest bandwidth last processor root processor proof consider ordering p 1 p p pro cessors except p p root processor explained section 31 consider permutation ordering words consider order p p processors exists denote resp best rational execution time processors ordered p 1 p p must show p k1 connected root processor higher bandwidth p k strictly smaller words must show implication therefore study sign difference replace expression stated equation 7 hypothesis 1 p 1 dp bit complicated 1 p 1 dp opposite exists value 1 p 1 dp optimal execution time cannot achieved solution processor receives nonempty share whole set data processors end computation date therefore solution processor receives nonempty share whole set data processors end computation date leads execution time strictly greater equations 10 11 summarized proving following implication prove equation 9 hence study sign expression denominators obviously strictly positive sign sign want simplify second sum equation 14 thus remark value order take advantage simplification proposed equation 15 decompose second sum equation four terms sum 1 k 1 terms k k 1 sum k 2 p k1 report result equation 16 equation 14 suppress terms common sides sign divide resulting equation strictly positive term way obtain sign k k1 equivalent therefore k1 k 0 equation 13 holds thus equation therefore inversion processors p k p k1 profitable bandwidth root processor processor higher bandwidth root processor processor p k 44 consequences general case general case going order processors exact study feasible even general case know computation communication characteristics processors indeed consider possible orderings p processors use algorithm 1 compute theoretical execution times chose best result theoretically possible prac tice large values p approach unrealistic furthermore general case analytical study course impossible cannot analytically handle function tcomm n comp n build previous result order processors decreasing order bandwidth connected root processor except root processor ordered last even without previous study policy surprising indeed time spent send share data items processor payed processors p p p first processor one less expensive send data course practice things bit complicated working integers however main idea roughly show suppose computation communication functions linear denote opt best execution time achieved rational distribution n data items whatever ordering processors opt best execution time achieved integer distribution n data items whatever ordering processors note rat opt int opt may achieved two different ordering processors take rational distribution achieving execution time rat opt round obtain integer solution following rounding scheme described section 33 way obtain integer distribution execution time 0 0 satisfying equation 1ip proof equation 4 however integer solution execution time obviously least equal int opt also integer solution rational solution int opt least equal rat opt hence bounds int 1ip 0 execution time distribution obtained rounding according scheme section 33 best rational solution processors ordered decreasing order bandwidth connected root processor except root processor ordered last computation communication functions linear ordering policy even guaranteed 5 experimental results 51 hardware environment experiment consists computation 817101 ray paths full set seismic events year 1999 processors machines run globus 9 use mpichg2 8 message passing library table 1 shows resources used experiment located two geographically distant sites processors 1 6 standard pcs intel piii amd athlon xp 7 8 two mips processors sgi origin 2000 premises whereas processors 9 16 taken sgi origin 3800 mips processors named leda end france input data set located pc named dinadan first site machine cpus type rating dinadan 1 piii933 pellinore 2 piii800 caseb 3 xp1800 sekhmet 4 xp1800 0004885 190 seven 7 8 r12k300 0016156 057 leda 916 r14k500 0009677 095 table 1 processors used computational nodes experiment table 1 indicates processors speeds observed series benchmarks performed application column indicates number seconds needed compute one ray lower better associated rating simply intuitive indication processor speed higher better inverse normalized respect rating 1 arbitrarily chosen pentium iii933 several identical processors present computer 5 6 916 average performance reported network links throughputs root processor dinadan nodes reported table 2 assuming linear communication cost column indicates time seconds needed receive one data element root processor machine dinadan 0 caseb pellinore sekhmet seven merlin table 2 measured network bandwidths sray sorted descending order notice merlin processors 5 6 though geographically close root processor smallest bandwidth connected 10 mbits hub experiment whereas others connected fastethernet switches 52 results experimental results section evaluate two aspects study first experiment compares unbalanced execution original program without source code modification predict best balanced execution second experiment evaluates execution performances respect two processors ordering policies bandwidths descending ascending order original application figure reports performance results obtained original program processor receives equal amount data choose ordering pro cessors conclusion given section 44 ordered processors descending bandwidth surprisingly processors end times largely differ exhibiting huge imbalance earliest processor finishing 259 latest 853 s1003005007009001000030000500007000090000110000 time data caseb pellinore sekhmet seven seven leda leda leda leda leda leda leda leda merlin merlin dinadan total time comm time amount data figure 2 original program execution uniform data distribution loadbalanced application second experiment evaluate loadbalancing strategy made assumption computation communication cost functions affine increasing assumption allowed us use guaranteed heuris tic simply replaced mpiscatter call mpiscatterv parameterized distribution computed heuristic large number rays algorithm 1 takes 15 minutes run celeron 12 ghz whereas heuristic execution using pipmp 7 17 instantaneous error relative optimal solution less 6 time data caseb pellinore sekhmet seven seven leda leda leda leda leda leda leda leda merlin merlin dinadan total time comm time amount data figure 3 loadbalanced execution nodes sorted descending bandwidth results experiment presented figure 3 execution appears well balanced earliest latest finish times 405 430 respectively represents maximum difference finish times 6 total du ration comparison performances original application gain significant total execution duration approximately half duration first experiment ordering policies compare effects ordering policy results presented figure 3 obtained descending bandwidth order execution processors sorted ascending bandwidth order presented figure 41003005007009001000030000500007000090000110000 time data merlin merlin leda leda leda leda leda leda leda leda seven seven sekhmet pellinore caseb dinadan total time comm time amount data figure 4 loadbalanced execution nodes sorted ascending bandwidth load balance execution acceptable maximum difference ending times 10 total duration earliest latest processors finish 437 486 predicted total duration longer 56 processors reverse order though load slightly less balanced first experiment peak load sekhmet ex difference comes idle time spent processors waiting actual communication begins clearly appears figure 4 surface bottom area delimited dashed line stair effect bigger figure 3 6 related work many research works address problem loadbalancing heterogeneous environments consider dynamic loadbalancing representative dynamic approach work 11 strongly related problem work library allows programmer produce per process load statistics execution information may used decide redistribute arrays one iteration however dynamic load evaluation data redistribution make execution suffer overheads avoided static approach static approach used various contexts ranges data partitioning parallel video processing 1 finding optimal number processors linear algebra algorithms 3 works closer distribution loops heterogeneous processors balance workload studied 6 particular case independent iterations equivalent scatter operation ever computation communication cost functions affine loadbalancing solution first presented heterogeneous processors network contentions occur contention taken account homogeneous processors framework apples project 5 discusses loadbalance iterative solver making stencil computations suggest linear programming techniques compute distribution actually use less precise though simplest solution solving linear equations another way loadbalance scatter operation implement following masterslave paradigm general framework studied 2 static loadbalancing could serve purpose code rewriting case becomes far complex 7 conclusion paper partially addressed problem adapting grid existing parallel applications designed parallel computers studied static loadbalancing scatter operations assumptions made processor speeds network links bandwidth presented two solutions compute loadbalanced dis tributions general exact algorithm heuristic far efficient simple cases affine computation communication times also proposed policy processor ordering order decreasing order network bandwidth root proces sor target application experiments showed replacing mpiscatter mpiscatterv calls used clever distributions leads great performance improvement low cost acknowledgments part computational resources used taken origin 3800 cines httpwwwcinesfr want thank letting us access machines r optimal scheduling algorithms communication constrained parallel processing scheduling strategies masterslave tasking heterogeneous processor grids linear algebra algorithms heterogeneous cluster personnal comput ers parametric integer programming globus metacomputing infrastructure toolkit grid blueprint new computing infrastructure dynamic loadbalancing dataparallel mpi programs parallel seismic raytracing global earth mesh delivering network performance numerical applications exploiting hierarchy parallel computer networks optimize collective operation perfor mance mpis collective communication operations clustered wide area systems mpi message passing interface standard pippiplib network weather service distributed resource performance forecasting service metacomputing tr distributed processing divisible jobs communication startup costs grid magpie network weather service mpistart scheduling divisible loads parallel distributed systems scheduling strategies masterslave tasking heterogeneous processor grids optimal scheduling algorithms communication constrained parallel processing source code transformations strategies loadbalance grid applications bandwidthcentric allocation independent tasks heterogeneous platforms linear algebra algorithms heterogeneous cluster personal computers exploiting hierarchy parallel computer networks optimize collective operation performance mpi messagepassing interface standard scheduling divisible workloads heterogeneous platforms seismic raytracing earth mesh modeling various parallel architectures divisible load theory