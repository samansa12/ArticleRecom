nearoptimal reinforcement learning polynomial time present new algorithms reinforcement learning prove polynomial bounds resources required achieve nearoptimal return general markov decision processes observing number actions required approach optimal return lower bounded mixing time optimal policy undiscounted case horizon time discounted case give algorithms requiring number actions total computation time polynomial number states actions undiscounted discounted cases interesting aspect algorithms explicit handling explorationexploitation tradeoff b introduction reinforcement learning agent interacts unknown environment attempts choose actions maximize cumulative payo sut ton barto 1998 barto et al 1990 bertsekas tsitsiklis 1996 environment typically modeled markov decision process mdp assumed agent know parameters process learn act directly experience thus reinforcement learning agent faces fundamental tradeo exploitation exploration bertsekas 1987 kumar varaiya 1986 thrun 1992 agent exploit cumulative experience far executing action currently seems best execute dierent action hope gaining information experience could lead higher future payos little exploration prevent agent ever converging optimal behavior much exploration prevent agent gaining nearoptimal payo timely fashion large literature reinforcement learning growing rapidly last decade many dierent algorithms proposed solve reinforcement learning problems various theoretical results convergence properties algorithms proven example watkins qlearning algorithm guarantees asymptotic convergence optimal values optimal actions derived provided every state mdp visited innite number times watkins 1989 watkins dayan 1992 jaakkola et al 1994 tsitsiklis 1994 asymptotic result specify strategy achieving innite exploration provide solution inherent exploitationexploration tradeo address singh et al 1998 specify two exploration strategies guarantee sucient exploration asymptotic convergence optimal actions asymptotic exploitation qlearning sarsa algorithms variant qlearning rum mery niranjan 1994 singh sutton 1996 sutton 1995 gullapalli barto 1994 jalali ferguson 1989 presented algorithms learn model environment experience perform value iteration estimated model innite exploration converge optimal policy asymptotically results best knowledge results reinforcement learning general mdps asymptotic nature providing guarantee either number actions computation time agent requires achieve nearoptimal performance hand nonasymptotic results become available one considers restricted classes mdps model learning modied standard one one changes criteria success thus saul singh 1996 provide algorithm learning curves convergence rates interesting special class mdps problem designed highlight particular exploitationexploration tradeo fiechter 1994 1997 whose results closest spirit considers discountedpayo case makes learning protocol easier assuming availability reset button allows agent return set startstates arbitrary times others provided nonasymptotic results prediction uncontrolled markov processes schapire warmuth 1994 singh dayan 1998 thus despite many interesting previous results reinforcement learning literature lacked algorithms learning optimal behavior general mdps provably nite bounds resources actions computation time required standard model learning agent wanders continuously unknown environment results presented paper void essentially strongest possible sense present new algorithms reinforcement learning prove polynomial bounds resources required achieve nearoptimal payo general mdps observing number actions required approach optimal return lower bounded algorithm mixing time optimal policy undiscountedpayo case horizon time discountedpayo case give algorithms requiring number actions total computation time polynomial number states undiscounted discounted cases interesting aspect algorithms rather explicit handling exploitationexploration tradeo two important caveats apply current results well prior results mentioned first assume agent observe state environment may impractical assumption reinforcement learning problems second address fact state space may large resort methods function approximation results available reinforcement learning function approximation sutton 1988 singh et al 1995 gordon 1995 tsitsiklis roy 1996 partially observable mdps chrisman 1992 littman et al 1995 jaakkola et al 1995 asymptotic nature extension results cases left future work outline paper follows section 2 give standard denitions mdps reinforcement learning section 3 argue mixing time policies must taken consideration order obtain nitetime convergence results undiscounted case make related technical observations denitions section 4 makes similar arguments horizon time discounted case provides needed technical lemma heart paper contained section 5 state prove main results describe algorithms detail provide intuitions proofs convergence rates section 6 eliminates technical assumptions made convenience main proofs section 7 discusses extensions main theorem deferred exposition finally section 8 close discussion future work preliminaries denitions begin basic denitions markov decision processes denition 1 markov decision process mdp states actions transition probabilities p action states j specify probability reaching state j executing action state thus state action payo distributions state mean rm r max rm 0 variance var var max distributions determine random payo received state visited simplicity assume number actions k constant easily veried k parameter resources required algorithms scale polynomially k several comments regarding benign technical assumptions make payos order first common assume payos actually associated stateaction pairs rather states alone choice latter entirely technical simplicity results paper hold standard stateaction pay os model well second assumed xed upper bounds r max var max means variances payo distributions restriction necessary nitetime convergence results third assumed expected payos always nonnegative convenience easily removed adding suciently large constant every payo note although actual payos experienced random variables governed payo distributions paper able perform analyses terms means variances exception section 55 need translate high expected payos high actual payos move standard denition stationary deterministic policy mdp denition 2 let markov decision process states actions policy mapping g later occasion dene use nonstationary policies policies action chosen given state also depends time arrival state mdp combined policy yields standard markov process states say ergodic markov process resulting ergodic welldened stationary distribu tion development exposition easiest consider mdps every policy ergodic socalled unichain mdps put erman 1994 unichain mdp stationary distribution policy depend start state thus considering unichain case simply allows us discuss stationary distribution policy without cumbersome technical details turns result unichains already forces main technical ideas upon us results generalize nonunichain multichain mdps small necessary change denition best performance expect learning algo rithm generalization multichain mdps given section 7 meantime however important note unichain assumption imply every policy eventually visit every state even exists single policy quickly thus exploitationexploration dilemma remains us strongly following denitions nitelength paths mdps repeated technical use analysis denition 3 let markov decision process let policy tpath sequence p probability p traversed upon starting state 1 executing policy pr dene two standard measures return policy denition 4 let markov decision process let policy let p path expected undiscounted return along r expected discounted return along p 1 discount factor makes future reward less valuable immediate reward tstep undiscounted return state u pr tstep discounted return state pr cases sum paths p start dene u unichain case u independent simply write u furthermore dene optimal tstep undiscounted return u fu similarly optimal tstep discounted return also u unichain case u independent simply write u existence limits guaranteed unichain case finally denote maximum possible step return g undiscounted case g discounted case g tr max 3 undiscounted case mixing times easy see seeking results undiscounted return learning algorithm nite number steps need take account notion mixing times policies mdp put simply undiscounted case move asymptotic return nitetime return may longer welldened notion optimal policy may policies eventually yield high return instance nally reaching remote highpayo state take many steps approach high return policies yield lower asymptotic return higher shortterm return policies simply incomparable best could hope algorithm competes favorably policy amount time comparable mixing time policy standard notion mixing time policy markov decision process quanties smallest number steps required ensure distribution states steps within stationary distribution induced distance distributions measured kullbackleibler divergence variation distance standard metric furthermore wellknown methods bounding mixing time terms second eigenvalue transition matrix p also terms underlying structural properties transition graph conductance sinclair 1993 turns state results weaker notion mixing requires expected return steps approach asymptotic return denition 5 let markov decision process let ergodic policy return mixing time smallest 0 ju suppose simply told policy whose asymptotic return u exceeds value r unknown mdp reward r reward 0 1 figure 1 simple markov process demonstrating nitetime convergence results must account mixing times return mixing time principle suciently clever learning algorithm instance one managed discover quickly could achieve return close u much steps conversely without assumptions reasonable expect learning algorithm approach return u many fewer steps simply may take assumed policy order steps approach asymptotic return example suppose two states one action see figure 1 state 0 payo 0 selfloop probability 1 probability going state 1 absorbing state 1 payo r 0 small return mixing time order 1 starting state 0 really require order 1 steps reach absorbing state 1 start approaching asymptotic return r relate notion return mixing time standard notion mixing time markov decision process n states let ergodic policy let smallest value 0 state probability state 0 steps within stationary probability return mixing time 3trmax proof lemma follows straightforward way linearity expectations omitted important point return mixing time polynomially bounded standard mixing time may cases substantially smaller would happen instance policy quickly settles subset states common payo takes long time settle stationary distribution within subset thus choose state results undiscounted return terms return mixing time always translate standard notion via lemma 1 notion return mixing time precise type result reasonable expect undiscounted case would like learning algorithm number actions polynomial return learning algorithm close achieved best policy among mix time motivates following denition denition 6 let markov decision process dene class ergodic policies whose return mixing time let opt denote optimal expected asymptotic undiscounted return among policies thus goal undiscounted case compete policies time polynomial 1 n eventually give algorithm meets goal every simultaneously interesting special case mixing time asymptotically optimal policy whose asymptotic return u time polynomial 1 n algorithm achieve return exceeding u high probability clear modulo degree polynomial running time result best one could hope general mdps 4 discounted case horizon time discounted case quantication policies learning algorithm competing straightforward since discounting makes possible principle compete policies time proportional horizon time words unlike undiscounted case expected discounted return policy 11 steps approaches expected asymptotic discounted return made precise following lemma markov decision process let policy state call value lower bound given horizon time discounted mdp proof lower bound v follows trivially denitions since expected payos nonnegative upper bound x innite path p let r 1 expected payos along path path prex innite path p solving yields desired bound since inequality holds every xed path also holds distribution paths induced policy discounted case must settle notion competing slightly dierent undiscounted case reason undiscounted case since total return always simply averaged learning algorithm recover youthful mistakes low return early part learning possible discounted case due exponentially decaying eects discounting factor ask time polynomial horizon time learning algorithm policy current state discounted return within asymptotic optimal state thus time reinitialized 0 current state start state learned policy would nearoptimal expected return goal algorithm achieve general mdps discounted case 5 main theorem ready describe new learning algorithms state prove main theorem namely new algorithms general mdp achieve nearoptimal performance polynomial time notions performance parameters running time mixing horizon times described preceding sections ease exposition rst state theorem assumption learning algorithm given input targeted mixing time optimal return opt achieved policy mixing within steps undiscounted case optimal value function v discounted case simpler case already contains core ideas algorithm analysis assumptions entirely removed section 6 theorem 3 main theorem let markov decision process n states undiscounted case recall class ergodic policies whose return mixing time bounded opt optimal asymptotic expected undiscounted return achievable exists algorithm taking inputs nt opt total number actions computation time taken polynomial 1 1 n r max probability least 1 total actual return exceeds opt discounted case let v denote value function policy optimal expected discounted return exists algorithm taking inputs n v total number actions computation time taken polynomial 1 1 n horizon time r max probability least 1 halt state output policy remainder section divided several subsections describing dierent central aspect algorithm proof full proof theorem rather technical underlying ideas quite intuitive sketch rst outline 51 highlevel sketch proof algorithms although dierences algorithms analyses undiscounted discounted cases easiest think single algorithm algorithm commonly referred indirect modelbased namely rather maintaining current policy value function algorithm actually maintain model transition probabilities expected payos subset states unknown mdp important emphasize although algorithm maintains partial model may choose never build complete model necessary achieve high return easiest imagine algorithm starting call balanced wandering mean algorithm upon arriving state never visited takes arbitrary action state upon reaching state visited takes action tried fewest times state breaking ties actions randomly state visits algorithm maintains obvious statistics average payo received state far action empirical distribution next states reached estimated transition probabilities crucial notion algorithm analysis known state intuitively state algorithm visited many times therefore due balanced wandering tried action state many times transition probability expected payo estimates state close true values important aspect denition weak enough many times still polynomially bounded yet strong enough meet simulation requirements outline shortly fact denition known state achieves balance shown section 52 states thus divided three categories known states states visited still unknown due insucient number visits therefore unreliable statistics states even visited important observation cannot balanced wandering indenitely least one state becomes known pigeonhole principle soon start accumulate accurate statistics state fact stated formally section 55 perhaps important denition knownstate mdp set currently known states current knownstate mdp simply mdp naturally induced full mdp brie transitions states preserved transitions redirected lead single additional absorbing state intuitively represents unknown unvisited states although learning algorithm direct access virtue denition known states approximation rst two central technical lemmas prove section 52 shows appropriate denition known state good simulation accuracy expected step return policy close expected step return either mixing time competing undiscounted case horizon time discounted case thus time partial model part algorithm knows well second central technical lemma section 53 perhaps enlightening part analysis named explore exploit lemma formalizes rather appealing intuition either optimal step policy achieves high return staying high probability set currently known states importantly algorithm detect replicate nding highreturn exploitation policy partial model optimal policy signicant probability leaving within steps algorithm detect replicate nding exploration policy quickly reaches additional absorbing state partial model thus performing two oline polynomialtime computations section 54 algorithm guaranteed either nd way get nearoptimal return quickly nd way improve statistics unknown unvisited state pigeonhole principle latter case cannot occur many times new state becomes known thus algorithm always making progress worst case algorithm build model entire mdp happen analysis guarantees happen polynomial time following subsections esh intuitions sketched providing full proof theorem 3 section 6 show remove assumed knowledge optimal return 52 simulation lemma section prove rst two key technical lemmas mentioned sketch section 51 namely one suciently accurate approximation another mdp actually approximate step return policy quite accurately step return eventually appeal lemma show accurately assess return policies induced knownstate mdp computing return algorithms approximation appeal lemma 4 using settings important technical point goodness approximation required depends polynomially 1t thus denition known state require polynomial number visits state begin denition approximation require denition 7 let markov decision processes state space say approximation state rm r states j action p state prove simulation lemma says provided suciently close sense dened step return policies similar lemma 4 simulation lemma let markov decision process states undiscounted ontg policy t2 1 state u discounted case let 11 let ontg approximation policy state 1 note lemma undiscounted case stated respect policies whose 2return mixing time opposed return mixing time however 2 return return mixing times linearly related standard eigenvalue arguments proof let us x policy start state let us say transition state 0 state j 0 action small p probability steps state following policy cross least one small transition nt total probability small transitions state 0 action 0 n independent opportunities cross transition implies total expected contribution either u walks cross least one small transition ntg similarly since p implies p approximation total contribution either u walks cross least one small transition thus bound dierence u u restricted walks eventually determine choice solve thus restrict attention walks length cross small transition note transition satisfying p convert additive approximation p multiplicative approximation 1 p thus path p cross small transitions path p approximation error payos yields since inequalities hold xed path traverse small transitions also hold take expectations distributions paths induced thus additive 4 terms account contributions paths traverse small transitions bounded equation 19 upper bounds use following taylor expansion complete analysis undiscounted case need two conditions hold 2 rst condition would satised solving obtain t2 8g 4t g value also implies constant therefore satisfying second condition would require recalling earlier constraint given equation 19 choose nd 4t g satised choice given lemma similar argument yields desired lower bound completes proof undiscounted case analysis discounted case entirely analogous except must additionally appeal lemma 2 order relate step return asymptotic return 2 simulation lemma essentially determines denition known state one visited enough times ensure high probability estimated transition probabilities estimated payo state within ontg values following lemma whose proof straightforward application cherno bounds makes translation number visits state desired accuracy transition probability payo estimates lemma 5 let markov decision process let state visited least times action executed least bmkc times let p ij denote empirical probability transition estimates obtained visits probability least 1 p states j actions rm rm var maximum variance random payos states thus get formal denition known states denition 8 let markov decision process say state known action executed least times 53 explore exploit lemma lemma 4 indicates degree approximation required sucient simulation accuracy led denition known state let denote set known states specify straightforward way known states dene induced mdp induced mdp additional new state intuitively represents unknown states transitions denition 9 let markov decision process let subset states induced markov decision process denoted states fs 0 g transitions payos dened follows state 2 rms payos deterministic zero variance even payos stochastic action p ms absorbing state states action p ms ij thus transitions states preserved state 2 action p ms 0 2s p ij thus transitions states redirected 0 denition 9 describes mdp directly induced true unknown mdp preserves true transition probabilities states course algorithm approximations transition probabilities leading following obvious approximation denote obvious empirical approximation natural approximation following lemma establishes simulation accuracy immediately lemma 4 lemma 5 lemma 6 let markov decision process let set currently known states probability least 1 undiscounted case policy t2 ms state u ms ms ms discounted case let 11 policy state ms v ms v ms 37that states simply states visited far transition probabilities observed transition frequencies rewards observed rewards let us also observe return achievable thus approximately achievable achievable real world lemma 7 let markov decision process let set currently known states policy state 2 u ms ms proof follows immediately facts identical expected payos nonnegative outside payo possible 2 heart analysis identied part unknown mdp algorithm knows well form approximation key lemma follows demonstrate fact thus simulation lemma must always provide algorithm either policy yield large immediate return true mdp policy allow rapid exploration unknown state lemma 8 explore exploit lemma let markov decision pro cess let subset states let induced markov decision process 2 1 0 either exists policy u ms respectively v ms exists policy probability walk steps following terminate proof give proof undiscounted case argument discounted case analogous let policy satisfying u u suppose u ms witnesses claim lemma may write u pr pr r pr sums respectively paths p start state paths q start state every state q paths r start state least one state keeping interpretation variables p q r xed may write pr pr ms qu ms q u ms equality follows fact path q every state pr ms q um inequality fact u ms takes sum paths avoid absorbing state 0 thus pr implies x r pr r pr r pr x r pr desired 2 54 oline optimal policy computations let us take moment review synthesize combination lemmas 6 7 8 establishes basic line argument time set current known states step return policy lower bounds step return extension time must either policy whose step return nearly optimal must policy quickly reach absorbing state case policy executed quickly reach state currently known set section discuss two oline polynomialtime computations nd policy highest return exploitation policy one highest probability reaching absorbing state steps exploration policy essentially follows fact standard value iteration algorithm dynamic programming literature able nd step optimal policies arbitrary mdp n states 2 computation steps discounted undiscounted cases sake completeness present undiscounted discounted value iteration algorithms bertsekas tsitsiklis 1989 optimal step policy may nonstationary denoted sequence optimal action taken state th step tstep undiscounted value iteration initialize u ms ms iju t1 j ms ms iju t1 j undiscounted value iteration works backwards time rst producing optimal policy time step optimal policy time step 1 observe nite policy maximizes cumulative step return also maximize average step return tstep discounted value iteration initialize ms ms ijv t1 j ms ms ijv t1 j discounted value iteration works backwards time rst producing optimal policy time step optimal policy time step 1 note total computation involved 2 discounted undiscounted cases use value iteration straightforward certain points execution algorithm perform value iteration oline twice using either undiscounted discounted version depending measure return second time denote computation use undiscounted value iteration regardless measure return transition probabilities dierent payos absorbing state 0 payo r max states payo 0 thus reward exploration represented visits 0 rather exploitation policy returned value iteration 0 policy returned value iteration guarantees either step return current known state approaches optimal achievable assuming know thus detect probability execution 0 reaches unknown unvisited state steps signicant probability also detect putting together technical pieces need place give detailed description algorithm tie loose ends section 6 remove assumption know optimal returns achieved sequel use expression balanced wandering denote steps algorithm current state known state algorithm executes action tried fewest times current state note state becomes known denition never involved step balanced wandering use known denote number visits required state becomes known state dierent undiscounted discounted cases given denition 8 call algorithm explicit explore exploit whenever algorithm engaged balanced wandering performs explicit oline computation partial model order nd step policy guaranteed either exploit explore description follows freely mix description steps algorithm observations make ensuing analysis easier digest explicit explore exploit initialization initially set known states empty balanced wandering time current state algorithm performs balanced wandering discovery new known states time state visited known times balanced wandering enters known set longer participates balanced wandering observation clearly nm known 11 steps balanced wan dering pigeonhole principle state becomes known worst case terms time required least one state become known generally total number steps balanced wandering algorithm performed ever exceeds nm known every state known even steps balanced wandering consecutive known state account known steps balanced wandering oline optimizations upon reaching known state 2 balanced wandering algorithm performs two oline optimal policy computations described section 54 attempted exploitation resulting exploitation policy achieves return least u 2 respec tively discounted case least v 2 algorithm executes next steps respectively halts outputs given 2mixing time given algorithm input respectively horizon time attempted exploration otherwise algorithm executes resulting exploration policy derived oline computation steps lemma 8 guaranteed probability least 2g leaving set balanced wandering time attempted exploitation attempted exploration visits state algorithm immediately resumes balanced wandering observation thus every action taken algorithm either step balanced wandering part step attempted exploitation attempted exploration concludes description algorithm wrap analysis one main remaining issues handling condence parameter statement main theorem undiscounted discounted case theorem 3 ensures certain performance guarantee met probability least 1 essentially three dierent sources failure algorithm known state algorithm actually poor approximation nextstate distribution action thus suciently strong simulation accuracy repeated attempted explorations fail yield enough steps balanced wandering result new known state undiscounted case repeated attempted exploitations fail result actual return near u handling failure probability simply allocate 3 sources failure fact make probability rst source failure bad known state controllably small quantied lemma 6 formally use lemma 6 meet requirement states known simultaneously second source failure failed attempted explorations standard cherno bound analysis suces lemma 8 attempted exploration viewed independent bernoulli trial probability least 2g least one step balanced wandering worst case must make every state known exploit requiring nm known steps balanced wandering probability fewer nm known steps balanced wandering smaller 3 number step attempted explorations og nish analysis discounted case discounted case ever discover policy whose return current state close v attempted exploitation algorithm nished arguments already detailed since high probability accurate approximation part must nearoptimal policy well lemma 7 long algorithm nished must engaged balanced wandering attempted explorations already bounded number steps high probability every state known set contain states actually accurate approximation entire mdp lemma 8 ensures exploitation must possible since exploration emphasize case eventually contains states worst case analysis algorithm may discover able halt nearoptimal exploitation policy long ever occurs using value known given discounted case denition 8 total number actions executed algorithm discounted case thus bounded times maximum number attempted explorations given equation 45 bound total computation time bounded 2 time required oline computations times maximum number attempted explo rations giving undiscounted case things slightly complicated since want simply halt upon nding policy whose expected return near u want achieve actual return approaching u third source failure failed attempted exploitations enters already argued total number step attempted explorations algorithm perform contains states polynomially bounded actions algorithm must accounted step attempted exploitations step attempted exploitations expected return least u 2 probability actual return restricted attempted exploitations less u 34 made smaller 3 number blocks exceeds o1 2 log1 standard cherno bound analysis however also need make sure return restricted exploitation blocks sucient dominate potentially low return attempted explorations dicult show provided number attempted exploitations exceeds og times number attempted explorations bounded equation 45 conditions satised total number actions bounded ot times number attempted explorations total computation time thus 2 times number attempted explorations thus bounded concludes proof main theorem remark serious attempt minimize worstcase bounds made immediate goal simply prove polynomial bounds straightforward manner possible likely practical implementation based algorithmic ideas given would enjoy performance natural problems considerably better current bounds indicate see moore atkeson 1993 related heuristic algorithm 6 eliminating knowledge optimal returns mixing time order simplify presentation main theorem made assumption learning algorithm given input targeted mixing time optimal return opt achievable mixing time undiscounted case value function v discounted case horizon time implied knowledge discounting factor section sketch straightforward way assumptions removed without changing qualitative nature results brie discuss alternative approaches may result practical version algorithm let us begin noting knowledge optimal returns opt v used attempted exploitation step algorithm must compare return possible current state best possible entire unknown mdp absence knowledge explore exploit lemma lemma ensures us safe bias towards exploration precisely time arrive known state rst perform attempted exploration oline computation modied knownstate mdp described section 54 obtain optimal exploration policy 0 since simple matter compute probability 0 reach absorbing state 0 steps compare probability lower bound long lower bound exceeded may 0 attempt visit state lower bound guarantees oline computation attempted exploitation step must result exploitation policy close optimal discounted case halt output undiscounted case execute continue note explorationbiased solution removing knowledge results algorithm always exploring states reached reasonable amount time ex ploitation although simple way removing knowledge keeping polynomialtime algorithm practical variants algorithm might pursue balanced strategy standard approach strong bias towards exploitation instead enough exploration ensure rapid convergence optimal performance instance maintain schedule 2 0 1 total number actions taken algorithm far upon reaching known state algorithm performs attempted exploitation execution attempted exploration execution choices analyses ensure still explore enough polynomial time contain policy whose return near optimal return enjoyed meantime may much greater explorationbiased solution given note approach similar spirit greedy method augmenting algorithms qlearning exploration component crucial dierence greedy exploration probability attempt single action designed visit rarely visited state proposing probability execute multistep policy reaching unknown state policy provably justied undiscounted case still remains remove assumption algorithm knows targeted mixing time indeed would like state main theorem value long run algorithm number steps polynomial parameters total return exceed opt probability easily accomplished ignoring parameters already algorithm given input runs p steps xed polynomial p meets desired criterion propose new algorithm 0 need input simply runs sequentially amount time 0 must run 0 executed still polynomial need run 0 suciently many steps rst steps dominate lowreturn periods took place p 0 steps similar analysis done undiscounted case towards end section 55 note solution sucient polynomial time far one would implement practice instance would clearly want modify algorithm many sequential executions shared accumulated common partial models 7 multichain case main issue extending results arbitrary multichain mdps asymptotic undiscounted return policy independent start state makes undiscounted case multichain mdps look lot like usual discounted case indeed results extend arbitrary multichain mdps discounted case without modication therefore one way deal undiscountedcase multichain mdps ask given polynomial time algorithm state policy expected return nearoptimal state another way modify expect compete policy instead expecting compete largest asymptotic return start state policy compete lowest asymptotic return start state policy thus modify denitions 5 6 follows markov decision process let policy return mixing time smallest ij definition 6 let arbitrary markov decision process dene class policies whose return mixing time let opt optimal expected asymptotic undiscounted return among policies rened denitions undiscountedcase results unichain mdps extend without modication arbitrary mdps 8 future work number interesting lines research practical implementation although polynomial bounds proven far large immediately claim practical relevance algorithm feel underlying algorithmic ideas promising eventually result competitive algorithm currently examining practical issues choices arise implementation discussed brie section 6 hope report implementation experiments soon modelfree version partially related last item would nice nd algorithm similar require maintaining partial model policy perhaps several currently investigating well large state spaces would interesting study applicability recent methods dealing large state spaces function approximation algorithm recently investigated context factored mdps kearns koller 1999 acknowledgements give warm thanks tom dean tom dietterich tommi jaakkola leslie kaelbling michael littman lawrence saul terry sejnowski rich sutton valuable comments satinder singh supported nsf grant iis9711753 portion work done university colorado boulder r sequential decision problems neural networks dynamic programming deterministic stochastic models parallel distributed compu tation numerical methods athena scienti expected mistake bound model online reinforcement learning stable function approximation dynamic program ming convergence indirect adaptive asynchronous value iteration algorithms convergence stochastic iterative dynamic programming algorithms reinforcement learning algorithm partially observable markov decision problems distributed asynchronous algorithm expected average cost dynamic programming stochastic systems estimation markov decision processes learning curve bounds markov decision processes undiscounted rewards worstcase analysis temporaldierence learning algorithms machine learning algorithms random generation counting markov chain approach analytical mean squared error curves temporal di reinforcement learning soft state aggregation convergence results singlestep onpolicy reinforcement learning algo rithms reinforcement learning replacing eligibility traces generalization reinforcement learning successful examples using sparse coarse coding reinforcement learning introduc tion role exploration learning control asynchronous stochastic approximation q learning learning delayed rewards tr ctr david wingate kevin seppi p3vi partitioned prioritized parallel value iterator proceedings twentyfirst international conference machine learning p109 july 0408 2004 banff alberta canada alexander l strehl michael l littman theoretical analysis modelbased interval estimation proceedings 22nd international conference machine learning p856863 august 0711 2005 bonn germany alexander l strehl lihong li eric wiewiora john langford michael l littman pac modelfree reinforcement learning proceedings 23rd international conference machine learning p881888 june 2529 2006 pittsburgh pennsylvania shie mannor duncan simester peng sun john n tsitsiklis bias variance value function estimation proceedings twentyfirst international conference machine learning p72 july 0408 2004 banff alberta canada reveliotis theologos bountourelis efficient pac learning episodic tasks acyclic state spaces discrete event dynamic systems v17 n3 p307327 september 2007 carlos diuk alexander l strehl michael l littman hierarchical approach efficient reinforcement learning deterministic domains proceedings fifth international joint conference autonomous agents multiagent systems may 0812 2006 hakodate japan eyal evendar shie mannor yishay mansour action elimination stopping conditions multiarmed bandit reinforcement learning problems journal machine learning research 7 p10791105 1212006 daniela pucci de farias nimrod megiddo combining expert advice reactive environments journal acm jacm v53 n5 p762799 september 2006 pieter abbeel andrew ng exploration apprenticeship learning reinforcement learning proceedings 22nd international conference machine learning p18 august 0711 2005 bonn germany daniela pucci de farias benjamin van roy costshaping linear program averagecost approximate dynamic programming performance guarantees mathematics operations research v31 n3 p597620 august 2006 amol deshpande zachary ives vijayshankar raman adaptive query processing foundations trends databases v1 n1 p1140 january 2007