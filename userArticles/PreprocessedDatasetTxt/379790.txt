parallel implementation central decomposition method solving largescale planning problems use decomposition approach solve three types realistic problems blockangular linear programs arising energy planning markov decision problems arising production planning multicommodity network problems arising capacity planning survivable telecommunication networks decomposition algorithmic device breaks computations several independent subproblems thus ideally suited parallel implementation achieve robustness greater reliability performance decomposition algorithm use analytic center cutting plane method accpm handle master program run algorithm two different parallel computing platforms network pcs running linux genuine parallel machine ibm sp2 approach well adapted coarse grain parallelism results display good speedups classes problems treated b introduction despite recent progresses optimization algorithms hardware model builders keep generating larger larger optimization models challenge best existing technology particularly true area planning problems quest realism description systems dynamics uncertainty naturally yields huge models one way increase computation power resort parallel compu tations however serious obstacles parallel implementations optimization algorithms first genuine parallel machines still expensive users dont access high technology equipment secondly adapting optimization softwares parallel computations often quite complicated real issue practitioners paper want demonstrate classes problems exist solutions require sophisticate hardware major adaptations existing software yet achieve interesting speedups allow solve problems would intractable standard sequential machines regular optimization software used three key ingredients first idea use decomposition break initial problem many independent smaller size problems simultaneously solved different processors second idea use clusters dedicated pcs get loose parallel environment finally overcome possible instability decomposition process base decomposition scheme analytic center cutting plane method second goal paper demonstrate parallel implementation central decomposition scheme scales well illustrate case consider three classes realistic problems problems classes large seem solvable direct sequential approach even powerful workstations first class problems pertains energy planning problems formulated blockangular linear programs solved via lagrangian relaxation class deals planning excess capacity achieve survivability telecommunication networks capacity problems formulated structured linear programs solved benders decomposition scheme finally considered blockangular linear programming problems arise area markov decision problems even though problem sizes excessive problems numerically challenging thus make interesting test problems achieve efficiency parallel computations one needs organize computations processors simultaneously busy decomposition approach computations alternate master problem subproblems thus unless one could implement parallel computations master problem processors one idle code deals master program consequently necessary condition efficient implementation work problems master program comparatively small respect subproblems applications present paper type idea use parallel computation via decomposition quite natural certainly new 7 11 however many reports successful implementations probably bad reputation standard decomposition schemes dantzigwolfe decomposition 9 dual counterpart benders decomposition 6 reputed unstable schemes viewed special implementation classical kelleycheneygoldstein 26 8 cutting plane algorithm though kelleys cutting plane method often performs well also known slow even fail instances nemirovskii yudin 35 devised illuminating example based resilient oracle confirms possibly slow convergence kelleys method remedy flaw various schemes proposed literature based regularized scheme 38 27 29 others use central query points levins center gravity center maximum inscribed ellipsoid volumetric center lastly analytic center paper use last method use analytic center suggested huard 23 celebrated linearized method centers many years later idea surfaced renegars 36 polynomial pathfollowing algorithm lp 1 sonnevend 42 suggested use analytic centers connection general convex programming several authors realized potential concept relation cutting plane approach nondifferentiable optimization 43 17 mixed integer programming 33 idea refined implemented sophisticate code 20 named accpm analytic center cutting plane method current implementation based projective algorithm 25 15 extensive testing accpm proved powerful robust 3 16 consider promising candidate parallel implementation paper address parallel treatment subproblems computations used two different parallel machines cluster 16 pentium pro pcs ibm sp2 university geneva 8 available nodes scientific parallel experimentations first system particularly attractive due low cost ease installation second system commercial machine optimized parallel computations paper organized follows section 2 briefly introduce three decomposable appli cations energy planning problems markov decision models well adapted telecommunications network survivability problems suit benders decomposition section 3 address issues decomposition approaches specialized two classes problems section 4 give unified view cutting plane methods solve decomposed problem two specialized algorithms classical kelleycheneygoldstein 26 8 cutting plane method analytic center cutting plane method section 5 address basic issues parallel implementation decomposition scheme applied three applications presented earlier section 61 present numerical results finally section 7 give conclusions application problems 21 energy planning co 2 abatement markal market allocation standard energy systems model developed aegis international energy agency currently implemented 40 countries 4 markal multiperiod linear programming model objective function lp discounted sum horizon considered usually years investment operating maintenance costs technologies plus cost energy imports minimization objective function subject constraints 1 lp acronym linear programming shall use throughout paper describing energy system gives optimal investment plan selected technologies extension consisted link existing markal models several countries belgium great britain germany switzerland joint constraint maximum co 2 emission accordance recommendation environment development conferences rio 1992 kyoto 1997 coupling models provides basis discussing impact tax pollution emittant also fair transfers countries 4 individual markal models linear programming problems 5000 10000 constraints 20000 variables linking models creates large blockstructured linear programs algebraic formulation class problems takes form maximize subject 1 dimensions matrices relate common resource co 2 emission matrices b country markal models structure global problem named primal blockangular made several independent blocks constraints small set constraints couple variables similar problems also studied 34 message models used linear programs even larger single problem size 40000 rows 73000 variables coupling 8 models yields linear program 330000 rows 600000 variables goes beyond limits standard stateoftheart software workstations multiregional planning problems suit quite well type parallelism want use loosely coupled 7 20 linking constraints small number 3 8 subproblems considerable size 40000 rows sequential implementation time spent master problem remains small fractionabout 2of overall solution time unfortunately subproblems may differ considerably sizes predicted computational effort solve 22 markov decision model second test problem class blockangular linear programs generated markov decision problem systems slow fast transition rates abbad filar 1 showed one reformulate problems blockangular problems filar haurie 12 applied approach manufacturing problems particular problems solved communicated us day haurie moresino problems subproblems 17 coupling constraints problems solved immense however numerically difficult unstable variables probabilities sometimes take small values mean time small values large impact global solution stateoftheart codes applied frontal manner ie without decomposition encounter serious difficulties even smaller instances problems problems solved real application problems included experiments particularly challenging 23 survivablity telecommunications network survivability critical issue telecommunications individual components telecommunications network prone failure events interrupt traffic cause severe damage customers network survivable cope failures raises number issues regarding topology network capacity message rerouting policy case failure present case interested planning capacity network fixed topology facet telecommunications survivability problem first presented minoux 31 formulated large linear programming model following assumption failure occurs entire traffic may changed restore feasibility planned excess capacity however telecommunications operators strong preference minimal changes traffic assignment 30 lisser et al considered model proviso reroutings restricted segment traffic interrupted failure solved optimization problem via lagrangian relaxation approach lead easy parallel implementation master program large computationally much demanding subproblems shall present subsequent sections alternative decomposition approach survey related survivability problems see 2 give formal description problem telecommunications network undirected graph loops parallel edges normal operational state traffic assignment meets regular demands origindestination pairs compatible capacity network consider initial traffic assignment normal condition given fixed component networkedge nodefails traffic component interrupted must rerouted alternative routes network consider one component fail time denote set failure states clearly cardinality cannot exceed 2 jej state 2 operational part network consists subnetwork g working components state originates failure node v 2 v gs subgraph g n v g originates failure arc 2 e gs subgraph g n g state one defines initial traffic assignment set new demands k 2 r pairs nodes conditional demands created traffic implicated failures described k 2 r vector single origindestination pair let k vector unused capacity normal operational state unused capacity utilized route conditional demands necessary capacity installed given positive unitary cost objective capacity planning problem consists finding least capacity investment cost allows rerouting conditional demands k 2 r state 2 standard flow model applies directed graphs case massbalance equations establish algebraic relation inflow outflow however telecommunications networks essentially undirected graphs messages travel link j either directions retrieve usual flow formulation directed graphs replace arc j pair opposite arcs must remember capacity 2 account possible case edge assigned traffic usage arc sum flows two opposite arcs basis problem formulation given arbitrary orientation graph g define 2 nodearc adjacency matrix n gs matrix n gamman adjacency matrix graph formed two graphs gs one opposite let x k ij flow component commodity k arc j mass balance equations involve vectors components definitions one state problem follows min st k2rs inequalities 2a capacity constraints impose operational state flow arc j exceed capacity equations 2b node balance equations demand k resulting operational state 2 3 problem reformulation via decomposition decomposition often means two different things firstly decomposition mathematical technique transform initial largescale problem one smaller dimension secondly decomposition refers algorithm used solve transformed problem section consider mathematical transformation usual transformation techniques decomposing largescale optimization problems lagrangian relaxation benders decomposition paraphrase lagrangian relaxation let us assume problem interest maximization one constraints split two sets simple constraints complicating ones method goes follows complicating constraints relaxed objective modified incorporate penalty relaxed constraint penalty parameters nothing else dual variables associated relaxed constraints solving problem yields optimal value function dual variables appropriate convexity regularity assumptions function convex minimum value coincides optimal one original problem defines new problem equivalent original one variable space smaller dimension linear programming case convexity regularity assumption hold besides method dantzigwolfe decomposition paraphrase benders decomposition let us assume problem interest minimization one variables split two sets fixing first set variables one obtains problem whose optimal solution depends variables first set transformed problem consists minimizing function appropriate convexity regularity assumptions function convex minimum value coincides optimal value original problem minimization function defines new problem equivalent original one dimension first set variables linear programming two schemes dual one another first set constraints dually variables small dimension transformation yields new equivalent problem much smaller size whose components objective function andor constraints implicitly defined components given optimal values objective variables appropriate optimization problems next subsections provide detailed examples transformations emphasize similarity transformed problem two cases even though new problem smaller dimension likely easy difficulties arise first nondifferentiability objective derivatives replaced subgra dients provide quality approximation second difficulty comes computation subgradients obtained solution auxiliary optimization problem achieve efficiency accuracy one must resort advanced algorithmic tools section 4 describes one tool 31 lagrangian relaxation blockangular programs natural class structured problems eligible decomposition largescale blockangular linear programs following form maximize subject dimensions associate 3 partial lagrangian function assuming problem 3 optimal solution may duality replace min u lu optimal value maximize gammaha ui subject b x usual parlance problem 4 master program problem 5 subprob lem note feasible set subproblem independent u since objective pointwise maximum linear functions u convex u let us observe subproblem 5 separable consequence partial lagrangian additive problem 7 always feasible either optimal solution unbounded moreover domain rewrite problem 4 let us say word oracle assume first u 2 doml optimal solution 7 optimality cut follows directly definition l 7 assume u 62 doml problem 7 unbounded let u ray along feasibility cut excludes u part domain l practical implementations might useful work relaxation subproblems precisely assume subproblems solved fflprecision ffl 0 generate point x feasible subproblems thus arbitrary v fflsubgradient inequality fflsubgradients used define weaker lp relaxation relaxation successfully used 21 sequential implementation decomposition also use parallel implementation application lagrangian relaxation two blockangular classes problems described sections 21 22 obvious chosen problems number coupling constraints small hence variable space transformed problem also small 32 benders decomposition linear programs addressed previous section display primal blockangular structure set complicating constraints links independent blocks similar formulation given subset variables links independent blocks consider problem subject dimensions constraint matrix linear program displays dual blockangular structure given x 0 let qx 0 optimal value subproblem minimize subject w x replace 10 subject x 0 0 12 function qx 0 additive value subject w x words subproblem 11 separable interesting consider dual maximize subject w objective value defined pointwise maximum linear forms x 0 thus convex note constraints 15 independent x 0 safely assume 15 feasible otherwise original problem either unbounded infeasible problem 14 may infeasible values x 0 assume first feasible optimal solution x dual optimal solution view 15 optimality cut writes assume 14 feasible assumed dual 15 feasible unbounded let ray along dual objective hb equivalently feasibility cut excludes part feasible domain 10 similarly lagrangian relaxation exact optimality requirement may replaced ffloptimality obtain weaker inequality cutting plane methods solve decomposed problems formulation transformed problem lagrangian relaxation benders decomposition generate problems usually difficult despite relatively small dimension variable space difficulty stems fact problems essentially nondifferentiable solving requires advanced algorithmic tools section concentrate solution method specifically present generic cutting plane method analytic center variant solve nondifferentiable problem interest cutting plane methods apply canonical convex problem x ae r n closed convex set x 0 ae r n compact convex set f r n 7 r convex function assume set x 0 defined explicitly function f set x defined following oracle given oracle answers either one statement 1 x x support vector 2 x x separation vector answers first type named optimality cuts whereas answers second type feasibility cuts successive answers oracle sequence query points x k 2 x 0 used define outer polyhedral approximation problem epigraph space let fx j g j 2 kg sequence query points set k partitioned k 1 k 2 correspond optimality cuts feasibility cuts respectively let oracle answer x j best recorded function value polyhedral approximation defines epigraph space x z set localization h clearly f k contains optimal solutions initial problem kth step generic cutting plane algorithm follows 1 pick 2 oracle returns generic cut hd cut optimality one 0 otherwise 3 update f k1 f k specific cutting plane algorithms differ choice query point x k let us focus two strategies kelleys strategy consists selecting x g strategy possibly simplest means alternative comprehensive discussion nondifferentiable optimization methods refer 28 18 define analytic center strategy let us introduce slacks associated potential log barrier function associated set x 0 analytic center strategy selects concise summary method convergence properties see 18 practice often case function f additive set x intersection many sets shown section 31 case lagrangian relaxation lagrangian dual turns sum several functions l one corresponding subproblem feasible set x intersection domains functions case oracle returns separate information function l also deal similar situation benders decomposition separable subproblem see section 32 let us reformulate problem 16 x 2 x 0 oracle associated formulation provides answers 1 x x support vectors 2 x x separation vectors oracle viewed collection independent oracles one function f one x feature two beneficial consequences firstly disaggregate formulation much improves performance kelleys cutting plane method 24 accpm 10 secondly importantly case disaggregation naturally allows parallel computations 5 parallel implementation cutting plane methods decomposition obvious candidate parallel computation 7 11 independent oracles examined parallel reduce time needed gather information oracle make sure scheme computationally advantageous ratio times spent solve consecutive master problems solve sequentially subproblems associated oracle small smaller ratio larger part work done parallel situation often appears decomposition reallife problems two different cases may render oracle expensive 1 relatively computationally involved independent oracles 2 many possibly cheap independent oracles cases benefit parallel computation machine relatively powerful parallel processors moreover expect decomposition working two abovementioned conditions made scalable implemented parallel machine roughly speaking scalability means ratio computing times sequential algorithm parallel algorithm increases linearly number subproblems long exceed number parallel processors available see 7 11 related notion used extensively performance evaluation parallel algorithms speedup ratio solution time problem solved one processor machine solution kprocessor parallel machine ideally scalable algorithms speedups keep close k kprocessor machine 51 blockangular application problems applications presented sections 21 22 relatively small number subproblems varying 3 16 subproblems may large size unfortunately may significantly differ size among consequently always possible achieve good load balancing parallel code 52 survivablity telecommunications network survivability problem 2 blockangular linear program two types coupling elements constraints 2a couple independent blocks 2b variable couples jsj blocks otherwise independent constraints 2a 30 choice apply lagrangian relaxation 2a note constraints rather numerous paper apply benders decomposition fixing primary variable obtain problem type 12 application variable space small dimension shall see subproblems rather involved since cost associated rerouting flows x subproblems type 14 take one two values 0 1 formally one write st k2rs problem essentially feasibility problem named compatible multicommodity flow problem although problem involves relatively commodities 3 dimension small owing block angular structure 18 one apply decomposition scheme solve case introduce auxiliary problem handle first phase min st k2rs problem optimal solution zero l takes positive finite value l solve first phase problem use lagrangian decomposition scheme dualizing constraints 19a optimal solution 19 positive optimal dual variables associated dualized constraints used build feasibility cut details compatible multicommodity flows see minoux 32 lagrangian relaxation compatible multicommodity flow problem 19 relatively easy solve kelleys cutting plane scheme performs well contrast problem 12 yspace difficult costly oracle indeed deal nested twolevel decomposition scheme subproblems higher level decomposition ie problems 19 decomposed reason analytic center cutting plane method method choice solve achieves greater stability efficiency let us turn attention load balancing among independent processors oracle made relatively many independent oracles independent compatible multicommodity flow problem number independent oracles jsj number may rather large hundred elements computations distributed 3 number commodities equal number different origindestination communications interrupted network component failure various computing units conceivably computation load varies one oracle ie one element use many computing units jsj computing load unbalanced parallel computation scalable however interested large parallel machines rather clusters workstations involving dozens face issue distributing evenly work among computing units psfrag replacements cpu time figure 1 load balancing proceed follows beginning subproblems cyclically distributed among nodes subproblems solved subproblems sorted according time needed solve time spent node computed well average time node tasks transferred nodes time spent average average illustrated figure 1 node tasks processed increasing order completion time done long total time tasks processed far time spent one node average procedure reaches point tasks left jobs transferred nodes time spent average note decomposition scheme progresses relative difficulty subproblems may vary reason choose rebalance load dynamically iteration 6 numerical results 61 computational resources perform numerical experiments used two different parallel machines cluster linked fast ethernet parallel supercomputer ibm sp2 latter expensive machine intended cover need large institution whereas former cheap form parallelism benefits recent advances fast ethernet technology briefly expose salient characteristics machines following idea beowulf project 5 37 cluster 16 pentium pro pcs runs linux operating system redhat 42 distribution cluster linked fast ethernet allows 100mbs transfer rate six machines 384 mb ram one master remaining 10 machines 64mb ram installed mpich public domain version mpi handle communications processors ibm rs6000 scalable power parallel system rs6000 sp2 university geneva 15 nodes complete rs6000 workstation cpu disk memory runs operating system aix 41 processors standard architecture rs6000 running 77mhz one wide node 66mhz fourteen thin nodes nodes divided three pools one dedicated interactive jobs another intensive inputoutput jobs last one parallelproduction work last pool used numerical tests contains eight processors thin nodes four nodes 192 mb ram memory remaining four nodes 128 mb ram sp2 machine belongs messagepassing family access switch made possible call library provided constructor standard libraries pvm 14 mpi 13 41 also available pvm mpi libraries portable platforms definite advantage view implementation accpm corresponds one described 20 code combines three programming languages c c fortran 77 cluster pcs compiled gnu compilers gcc g g77 respectively ibm sp2 machine compiled aix compilers mmxlc mmxlc mmxlf respectively cases compilations done default optimization level option 62 programming language parallel computations claimed earlier goal keep code general portable many parallel computing platforms thus implemented parallel communication message passing interface mpi library 22 13 41 important advantage parallel communication model implemented multiprocessor machine well cluster independent machines loosely coupled network ethernet family public domain implementation mpi available popular workstations 22 moreover mpi standard accepted commercial vendors implemented majority parallel computers particular sp2 ibm consequently one develop implementation using mpi test network workstations network pcs port sophisticate parallel machine 63 solving blockangular linear programs 631 problem characteristics solved set realistic blockangular linear programs number subproblems varying 3 16 markal multicountry multiperiod energy planning model global constraint limiting emission co 2 4 energy energy planning problems developed iiasa 34 two variants problems small large one named energys energyl respectively variants large model considered differ number subproblems vary 2 8 machines problems decentralized versions markov decision models 12 solve one small two medium one large model type problem statistics collected table 1 columns 2 3 4 specify every problem size master ie number coupling constraints 0 number rows active optimum number gub rows number subproblems p respectively following columns characterize subproblems report total number rows columns nonzero elements p subproblems several problems well balanced subproblems last two columns give insight balance specify minimum maximum column size subproblems problem master subproblems balance rows active gub rows cols nonz min cols max cols table 1 statistics blockangular decomposable problems problem calls cuts time markal 26 78 2530 energyl4 28 112 112391 machiness 11 176 172 machinesl 9 144 532 table 2 performance sequential accpm blockangular lps 632 numerical results solved problems relative precision problems considered master program small compared subproblem result time compute analytic centers negligible key element performance number times master program calls oracle concentrate element neglect internal iterations master program first results pertain sequential implementation decomposition code one node cluster pcs use code 21 analytic center cutting plane method 20 table 2 exhibits results problems listed table 1 report number outer iterations total number cuts handled accpm overall cpu time seconds required reach optimal solution subproblems solved hopdm higher order primaldual method code 19 benefit interior point warm start technique problems family energyl particularly demanding respect memory subproblem needs 80 mb storage solution time sequential code also considerable problem subpb 1 proc 2 processors 4 processors 8 processors machiness machinesl table 3 performance parallel decomposition blockangular lps parallel decomposition implemented cluster 16 pcs running linux problems could solved appropriate number processors 1 2 4 8 used table 3 report cpu times seconds required solve problem speedup respect solution time required sequential code include results running problems p subproblems parallel machine p processors put nr mark problems run clearly parallel decomposition approach one cannot expect speedups exceed p results collected table 3 show parallel decomposition reaches reasonable speedups 152 194 2 processors used 3 4 processors used 5 8processor parallel machine results seem satisfactory problems solved best speedups obtained energy problems well balanced subproblems cf table 1 markov decision problems scale well due discrepancy size difficulty subproblems yet speedups 5 8processor machine seem fairly satisfactory although problems large todays lp codes difficult due presence low probability transitions rates small coefficients matrix important global consequences attempt solving stright undecomposed formulation breaks commercial lp solvers use decomposition spreads numerical difficulties across subproblems overall problem easily solved 64 survivability telecommunications networks 641 problem characteristics tests performed problems different nature eight generated randomproblem generator see 40 reallife disguised problems t1 t2 t3 table 4 report characteristics network first give number nodes nodes number arcs arcs number rout mbasic data failure data nodes arcs routings failures cond demand table 4 characteristics survivability problems ings routings 4 latter characteristics concern network normal operational state next indicate features related failure states number failures 5 overall number conditional demands resulting failures cond demand let us observe problem involves large number routings give idea problem dimension possible reformulate survivability problem large lp 31 size problem compact lp formulation involving massbalance equations found table 5 size computed according formulas number rows k2rs number columns k2rs let us observe p239 largest dimension requires 15 million variables 2 hundred thousand constraints large dimension results important number failures conditional demands 4 recall routings fixed data problem 5 theoretical number failures equal jsj jn j however arcs may regular flow failure consequence consider failures failures force rerouting messages problem variables constraints p22 3676 1332 p53 92301 16061 table 5 problem characteristics 642 numerical results case blockangular linear programs time spent computing analytic center master program relatively small negligible respect time spent subproblems main difference previous set experiments concerns number subproblems far greater number also smaller involved solved via decomposition scheme key efficiency factor overall performance number calls oracle experiments conducted 8 processors 6 ibm sp2 mentioned pre viously subproblem needs solving multicommodity network flow problems kelleys cutting plane method performs well therefore needed efficient simplex solve subproblems unfortunatly numerical experiments made commercial simplex running operating system linux longer true consequently solve problems cluster pcs problem table 6 provide number calls oracle call corresponds solving several compatible multiflow problems cpu time seconds oracle master whole algorithm well time speedup one processor compared 8 processors note total time includes inputoutput operations add oracle master computing times let us first observe number iterations grows regularly number network arcs note time spent master accpm exceed 2 time spent oracle previously remarked problems even largest could solved reasonable time problems p227 p239 solved sequentially due time constraints conse 6 maximum number processors available parallel experiments machine quently speedups computed cases wrote nc corresponding column problems computed speedups figures run 38 smaller problem 7 larger ones note problems p22 much smaller instances yet speedups 38 53 respectively feel similar ratios could achieved larger problems processors giving hope much larger problems could solved practice ideally running time parallel application decrease linearly number processors increases figure 2 attempt reveal influence number processors overall running time two problems problem calls cuts cpu secs speedup p53 26 41 1377 035 138 442 pb5 26 282 12159 878 1225 585 table figure 2 concerns problem p145 figure 2 display overall running time according number processors used parallel observe large decrease running time 15190 less 2450 seconds figure 2 b translates speedup dashed line represents perfect scalability linear decrease time compared number processors whereas pure line represents observed speedup pure line stays close bold one stems figure scalability rather good example fact 8 processors speedup 62 able measure speedup larger problems p145 would required freezing parallel machine considerable time allowed however conjecture expect similarif betterspeed ups larger problems large number failures allows freedom load balancing intuitively quite clear easier obtain equal times dividing larger number tasks cpu time number processors a2468 improvement number processors observed optimal b figure 2 cpu time speedup function number nodes conclusions shown paper computational advantages resulting parallelisation oracle decomposition using central prices applied code solve two different types problems small number computationally expensive independent subproblems large number relatively cheap subproblems cases remarkable speedups obtained latter parallel code reaches scalability two realistic applications energy planning telecommunications well challenging markov decision problem shown take advantage parallel implementation analytic center cutting plane method interesting alternative modify original accpm method master program activated time idle subproblem solved another processor reciprocally solution subproblem starts soon processor becomes available similarly idea asynchronous parallel decomposition 39 dual prices used recent output master program way one may expect increase efficiency decomposition possibly improve speedups algorithm acknowledgment grateful jemery day alain haurie francesco moresino providing us challenging markov decision problems arising manufacturing r algorithms singularly perturbed limiting average markov control problems cutting plane method analytic centers stochastic programming multinational markal model study joint implementation carbon dioxide emission reduction measures beowulf parallel workstation scientific computation partitioning procedures solving mixedvariables programming prob lems parallel distributed computations newtons method convex programming tchebycheff approximation decomposition algorithm linear programming comparative behavior kel leys cutting plane method analytic center cutting plane method optimal ergodic control singularly perturbed hybrid stochastic systems mpi messagepassing interface standard pvm parallel virtual machine users guide tutorial networked parallel computing polynomial newton method linear programming solving nonlinear multicommodity flow problems analytic center cutting plane method decomposition nondifferentiable optimization projective algorithm interior point methods nondifferentiable optimiza tion hopdm version 212 fast lp solver based primaldual interior point method warm start users guide mpich resolution mathematical programming nonlinear constraints methods centers impact formulation decomposition new polynomialtime algorithm linear programming cutting plane method solving convex programs survey bundle methods nondifferentiable optimization handbooks operations research management science new variants bundle meth ods survivability telecommunication net works optimum synthesis network nonsimultaneous multicommodity flow requirements solving combinatorial optimization problems using karmarkars algorithm informational complexity efficient methods solution convex extremal problems based newtons method beowulf harnessing power parallelism pileofpcs regularized decomposition method minimizing sum polyhedral functions routing survivability optimization using central cutting plane method mpi complete reference centre potential reduction algorithm allowing column generation tr new polynomialtime algorithm linear programming regularized decomposition method minimizing sum polyhedral functions polynomialtime algorithm based newtons method linear programming parallel distributed computation numerical methods nondifferentiable optimization decomposition nondifferentiable optimization projective algorithm solving combinatorial optimization problems using karmakars algorithm parallel decomposition multistage stochastic programming problems multicommodity network flows pvm parallel virtual machine cutting plane method analytic centers stochastic programming new variants bundle methods solving nonlinear multicommodity flow problems analytic center cutting plane method mpi warm start myampersand949subgradients cutting plane scheme ctr laura di giacomo giacomo patrizi dynamic nonlinear modelization operational supply chain systems journal global optimization v34 n4 p503534 april 2006