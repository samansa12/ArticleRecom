assessing performance new ibm sp2 communication subsystem ibm recently launched upgrade communication subsystem sp2 parallel computer change affects hardware software elements highperformance switch message interface adapters new implementation mpi messagepassing library characterize extent changes affect execution times parallel applications authors run collection benchmarks sp2 old communication subsystem machine upgrade benchmarks include pointtopoint collective communication tests well set complete parallel applications performance indicators latency throughput exhibited basic communication tests execution time case real applications result indicate certain circumstances significant performance increase result b introduction long time passed since highperformance computing community realized highest computation speeds reasonable cost reached via massive parallelism eighties early nineties sort building euphoria led work done r beivide ja gregorio department electrical computer engineering university california irvine visiting researchers design use massively parallel processors mpps thousands even tens thousands processing elements however nowadays community seems think hundred processors represent upper limit feasibility parallel computing systems additionally mainly due lack stability supercomputing sector constructors making relatively conservative decisions regarding hardware software elements mpps way guaranteeing systems survive market reasonable period ibms sp2 parallel computer built around workstationbased hardware software represents one successful approaches mpps seems capable surviving difficult times hp96 current mpps including sp2 shown suitable running coarse grain parallel applications interchange large data structures many cases systems used simply increase throughput sequential jobs multiple users sharing machine however big challenge sp2 similar machines distributed memory parallel computers efficient running communicationdemanding medium fine grain parallel applications reducing execution times nowadays following principle allowing production portable software mpps programmed using conventional imperative languages enhanced communication libraries pvm mpi implement message passing synchronization among processes gei94 mpi94 environment efficiency parallel applications maximized workload evenly distributed among processors overhead introduced parallelization process minimized cost communication synchronization operations must kept low possible order achieve interconnection subsystem used support interchange messages must fast enough avoid becoming bottleneck traditionally latency throughput two parameters used indicate performance interconnection network mpps throughput increases one generation mpps next significant reduction latency seems tougher problem designers industry unfortunately performance parallel applications sensitive latency increase throughput help processing long messages help significantly interchanging reasonablysized messages tens thousands bytesie several orders magnitude smaller required reach maximum achievable throughput current mpps current attempts measure characterize different components latency modern mpps lead conclusion interconnection network routers wires overdimensioned compared nodes ability send receive messages words largest components latency network network interfaces communication software cause message passing overhead two elements io hardware software must greatly improved reduce message latency mpps thus achieve better performance running parallel applications importance issue highlighted several research development projects including cul96 myr96 recently ibm developed new version highperformance switch constitutes interconnection network sp2 upgrade affects switch also io adapters connecting processors switch ibm upgraded operating system aix 3 aix 4 latter includes new version mpi specially tailored sp2 aix 3 mpi available additional layer software mpl native ibm message passing library changes aimed increase parallel application performance paper report experimental data obtained running collection benchmark applications sp2 first old new communication subsystem information used make performance comparison two sp2 computers whose difference communication subsystem dont try report exhaustive evaluation new subsystem provide exact characterization latency componentsthat done future papers however important allow community know preliminary experimental data especially search machine run parallel applications designing new mpps redesigning existing ones paper structured follows next section brief introduction ibm sp2 parallel computer made including limited information available recent change communication subsystem describe collection programs used test machine section 3 results running programs analyzed section 4 paper ends conclusions section 5 acknowledgements section 6 reader find references access path parallel code used elaborate paper overview ibm sp2 sp2 distributed memory parallel computer processors nodes interconnected communication subsystem ibm offers several alternatives subsystem possible provide communication among processors using standard networking technology ethernet fddi atm however highend sp2 systems ibm offers highperformance switch provides better characteristics parallel computing figure 1 shows highlevel system structure sp2 age95 micro channel controller switch adapter system io bus memory processor highperformance switch adapters detail node nodes figure 1 sp2 system ibm also offers several alternatives nodes access sp2 c4 see acknowledgements section composed nodes powered 66 mhz processor attached 256 mb memory means 128bit wide bus nodes 32 kb instruction cache 128 kb data cache 2 mb l2 cache micro channel controller governs io bus connects processormemory subset devices disks ethernet networks highperformance switch means appropriate adapters sp2 communication subsystem composed highperformance switch plus adapters connect nodes switch elements need carefully designed allow cooperate without introducing bottlenecks adapter contains onboard microprocessor offload work passing messages node node memory provide buffer space dma engines used move information node adapters memory switch link highperformance switch described ibm anytoany packetswitched multistage indirect network similar omega network advantage network bisection bandwidth increases linearly size system contrast direct networks rings meshes tori guarantees system scalability core network crossbar chip offering 8 bidirectional ports used build small sp2 systems larger systems boards composed two stages 4 chips total bidirectional ports used systems always least one stage necessary order provide redundant paths least 4 pair nodes reader find age95 stu95 sketches configurations systems 16 48 64 128 nodes frame building block sp2 systems frame contains switch board nodes sp2 available c4 two frames 16 nodes one frames used run sequential batch jobs second one available running parallel programs one used experiments reported paper originally system communication subsystem following characteristics adapters built around intel i860 processor 8 mb ram theoretical peak transfer ability adapter 80 mbs although achievable maximum 52 mbs due overheads associated accessing managing micro channel links highperformance switch provided 40 mbs peak bandwidth direction 80 mbs bidirectional nodetonode latency 05 systems 80 nodes recently system upgraded information new system available writing report limited able confirm new adapters built around powerpc 601 processor allowing doubling peak transfer bandwidthreaching 160 mbs new switch offers 300 mbs peak bidirectional bandwidth latencies less 12 systems 80 nodes gar96 regarding software environment sp2 node runs full version aix ibms version unix includes unix features plus specific tools libraries programming executing parallel programs first tests c4s sp2 carried version 3 aix included mpl ibmdesigned library parallel programming using messagepassing paradigm sni95 mpl actually interface implemented several communication subsystems particular run software layer designed make best use highperformance switch alternatively mpl might also run ip thus almost communication media including switch environment mpi library mpich see bri95 also available additional software layer mpl introduction aix version 4 ibm decided adopt mpi native language programming sp2 systems replacing nonstandard mpl way applications programmed mpi could run less overhead compared previous version introduction new switch accompanied new version mpi implementation although aix remained without significant changes order take advantage characteristics new hardware mpl still available ease migration ensure usability old code mpi mpl used fortran 77 c programs paper report results obtained running experiments bechmarks described section 3 following three configurations c4s sp2 table 1 communication subsystem software old switch adapters aix v3 mpich implementation mpi top mpl old switch adapters aix v4 native version mpi nv4 new switch adapters aix v4 native version mpi table 1 sp2 configurations test 3 experiments benchmarks order make fastbut faircomparison two versions sp2 selected ran small collection test programs already used researchers others prepared us selecting programs aim perform progressive evaluation machines starting simple point point communications going collective communications numeric kernels finegrain nonnumeric applications measurements obtained experiment provided information latency throughput communication channels complete network information used firstorder indicator predicting change communication subsystem affect execution times parallel applications next subsections selected benchmarks briefly described 31 point point communication first group tests based code provided dongarra characterize point point communications using mpi dd95 two processors 0 1 engage sort ping pong processor 0 charge measurements processor reads value walltime clock invoking mpisend operation blocks mpirecv meanwhile processor 1 performs symmetric operations latter operation finishes processor 0 clock read thus delay twomessage interchange one direction measured latency computed one half time achieved throughput also computed considering latency message size operations done times avoid warmup effects another 1000 times average results message size provided input parameter program measures minimum maximum average latency throughput considered average values representative performance user obtain machine authors xh96 consider minimum values latency supposed free influence operating system users case minimum average values close cases 32 collective communications several mpi collective operations object detailed measurement similar taken point point communication pay special attention two broadcast reduction experiments performed others exhibit similar behavior addition random traffic test performed experiments several messages might compete network resources well accessing common destination next describe details tests broadcast test involves processors performing broadcast processor root operation including test performs 1000 iterations average values iteration processors perform broadcast mpibcast processor 0 designated root barriersynchronize mpibarrier compute broadcast delay root measures time moment broadcast starts time barrier finishes subtracts precomputed barrier delay obtained throughput figures indicate amount information received participants reduction test like previous one using mpireduce instead mpibcast case throughput figures consider amount information sent participants mpi offers collection predefined operations mpimax mpimin mpisum etc performed reduction also allows user define new operations test userdefined null operation performed last test group random2 sets separates available processors two groups even rank odd rank even processor sends data anyone odd processors randomly chosen responds immediately delay twoway interchange achieved throughput computed like point point case 33 parallel applications order assess behavior sp2 running complete applications selected four benchmarks mg lu sp bt nas parallel benchmarks npb version 2 bai95 shallow water modelling code swm parkbench suite benchmarks wf95 parallel simulator ps developed group mig95 next briefly describe programs version 2 npb obtained source code form contrast version 1 written fortran 77 plus mpi programs compiled executed without change source code benchmark operate several input problem sizes number grid points npb specify three classes b c depending problem size experiments used class b ie mediumsize problems mg uses multigrid method compute solution threedimensional scalar poisson equation lu simulated computational fluid dynamics application uses symmetric successive overrelaxation solve block lower triangularblock upper triangular system equations resulting unfactored implicit finitedifference discretization navierstokes equations three dimensions sp bt simulated computational fluid dynamics applications solve systems equations resulting approximately factored implicit finitedifference discretization navierstokes equations bt solves blocktriangular systems 5x5 blocks sp solves scalar pentadiagonal systems resulting full diagonalization approximately factored scheme bai95 swm parallel algorithm testbed solves nonlinear shallow water equations rotating sphere using spectral transform method programmed using messagepassing paradigm fortran 77 plus mpi forms part parkbench benchmark suite developers patrick h worley oak ridge national laboratory ian foster argonne national laboratory benchmark includes several input files select problem size algorithms use used mediumsize problem requires approximately 1 gb workspace run 64 bit precision 1000 gflop default parallel algorithms distributed fourier transform distributed legendre transform ps parallel discreteevent simulator developed group evaluate characteristics message passing networks 2d torus topology cutthrough flow control parameters simulator basically problem size terms number switching elements load network terms percentage maximum theoretical bandwidth network bisection number time steps appropriate makefiles distributed code simulate results reported paper torus 32x32 routers simulated 40000 cycles load varies 5 90 means number messages needed perform simulation varies 25 million nearly 9 million processes organized logical 4x4 torus process always communicates four logical neighbors communication follow particular temporal pattern messages short bytes code written c plus mpi performance evaluation latency throughput basic parameters characterize applications view performance communication subsystem conjunction determine adequacy given system execute given type parallel application first message latency imposes restrictions granularity applications second throughput imposes limit maximum amount information processes interchange per unit time reason following sections analyze results described benchmarks terms two parameters order show upgrade sp2 communication subsystem affected preliminary attempt evaluate performance improvement achieved new sp2 communication subsystem intend perform exhaustive analysis every mpi function results discussed minimum kernel aim offering first overview potential performance changes application might experience 41 point point communication first approach assess communication performance parallel computer measure minimum time required send message two processes located different nodes reason many performance evaluation studies concentrate point point communication order establish initial comparison point among different platforms cul96 dd95 hoc94 xh96 table 2 figure 2 show results obtained running point point test described previous section observed table introduction new version mpi implies reduction software overhead consequence startup times lower latency noticeably reduced messages short however messages 4 kb new version performs worse explanation phenomenon found fra95 deal later section maximum achievable throughput remains old new mpi implementation introduction new switch clearly brings reduction latency length ranges latency reduction specially significant medium long messages thus allowing achievement higher throughput latency throughput mbs bytes ov3 ov4 nv4 ov3 ov4 nv4 128 6454 5054 4958 198 253 258 512 9559 7816 6195 536 655 827 table 2 average values latency throughput point point communication l message length configurations ov3 ov4 nv4 described table latency message thov3 thov4 thnv4 throughput mbs message length figure 2 latency throughput point point communication data table 2 preliminary analysis results easily established characterization latency general latency message length l decomposed two components startup time h time used message header reach destination spooling time time required transmit remainder message path origin destination already established spooling time usually modelled linear function l thus tl expressed represents time required transmit byte path established parameter interest throughput defined general asymptotic behavior function l follows l thmax maximum asymptotic throughput achievable communication subsystem equations 3 4 see thmax calculated 1t b latencies table 1 fitted equation 2 obtained maximum throughputs unidirectional point point communications mbs system behavior short messages long messages see clearly detailed set experiments carried new switch results shown figure 3 case minimum latency values represented average values noisy produce clear graph curve fittings done using least squares latency message length kb103050700 throughput mbs message length kb figure 3 minimum latency maximum throughput new switch messages smaller 192 kb detailed analysis data indicates existence three different regions defined message length see figure 4 one 3 regions fitted equation 2 different parameters latency message length kb region 1200400600800 latency message length kb region 25001500250032 64 96 128 160 192 latency message length kb region 3 figure 4 message latency three different message length regions interesting observe h smaller regions correspond smaller messages b smaller regions correspond longer messages means system optimized minimize latency short messages maximizing throughput long messages mentioned reference fra95 explains change region 1 region 2 perfectly short messages eager protocol used ie message sent destination immediately longer messages rendezvous protocol used ie message sent receiver node agrees receive switching rendezvous protocol incurs higher startup costs reduces number times information copied thus reducing cost per byte change protocol comes ibms native version mpi discussion also valid experiments ov4 third region behavior latency linear regions 1 2 figures show clear sawedge effect jumps every 16 kb moment without information manufacturer unable offer reasonable hypothesis explain behavior set equations easy see maximum throughput achieved region 3 long messages computed parameter b equation 7 minimum latency obtained region 1 short messages dominated parameter h equation 5 going back comparison among several sp2 configurations consider example data table 1 bytes quite short message seen latency basically old new switch contrast long messages eg latency noticeably reduced throughput increased short effect communication subsystem upgrade hn ho thmax n 24 thmax therefore upgrade basically effect spooling time reduced less one half original however startup time used message header reach destination almost summarizing channel throughput doubled startup time remains results first approach tell us coarse grain parallel applications requiring interchange large data structures experiment significant performance improvement improvement also noticeable operating system level using system increase throughput batch taskswhen nodes run decoupled fashion contrast applications require frequent interchange short messages experiment significant reduction execution time upgrade fact happens message size required obtain reasonable performance increased change hockney defines l 12 length messages allows utilization one half maximum channel bandwidth hoc94 old subsystem l 12 aprox 3000 bytes see table 2 increased reach around 32 kb see figure 3 therefore minimum message size required efficiently run parallel applications increased order magnitude goal increase performance sp2 running applications requiring frequent interchange short messages imperative reduce startup time proportion even maximum throughput increased table 3 summarizes parameters h thmax l 12 several wellknown mpps including three considered sp2 configurations machine h thmax mbs convex spp1000 pvm 76 11 1000 convex spp1200 pvm 63 15 1000 cray t3d pvm 21 27 1502 intel paragon 29 154 7236 meiko cs2 83 43 3559 sp2ov4 44 35 3000 table 3 latency asymptotic throughput parallel computers data convex cray paragon ksr1 meiko taken dd95 42 collective communication measured characteristics representative communication patterns involving two processors several mpi collective operations random traffic two sets regarding broadcast collective operation experiments offered us information summarized figure 5 seen behavior much like point point case change software means change behavior latency curves use native mpi always reduce latency regarding hardware upgrade graphs show significant improvementalthough significant messages longer 4 kb even large enough messages latency reduction complete broadcast operation spectacular point point case therefore throughput increase impressive performance remaining collective operations including reduction described previous section measured applying strategy used broadcast show similar behavior therefore comments applicableso repeated latency message thov3 thov4 thnv4 throughput mbs message length figure 5 latency throughput broadcast operation results obtained running experiment random communication two sets 8 processors plotted figure 6 abovementioned broadcast case significant latency reductions achieved message sizes 4 kb2000600010000 latency message thov3 thov4 thnv4 throughput mbs message length figure 6 latency throughput random2 sets test 43 parallel applications commented section 3 execution times 6 benchmark applications measured order get insight changes communication subsystem may affect real parallel applications combine computation communication data analyzed previous sections reader infer reductions execution times marked cases parallel applications require processes interchange long messages5001500mg lu sp bt swm timeold timenew execution time benchmark2006001000 mg lu sp bt swm mflopsold mflopsnew mflops benchmark figure 7 results npb swm benchmarks graphs figure 7 show execution time seconds performance mflops bt lu mg sp swm mention native version mpi used tests therefore basically change hardware affects performance results expected improvements quite modest cases things better parallel simulator performance increase measured figure 8 shows execution times varying load simulated model indicator number messages application manages reference point comparison execution times experiment 16 nodes intel paragon 25 times achieved sp250150250350 old execution time load figure 8 execution times parallel simulator summarize section state change communication subsystem effective running parallel applications designers able achieve important reductions startup time ie latency short medium messages conclusions aim writing paper offer preliminary evaluation effect upgrade introduced ibm communication subsystem sp2 performance parallel applications clear dont pretend analysis upgraded system comprehensiveit mainly offer snapshot behavior collection experiments performed using sp2 frame running benchmarks measure performance specific communication operations plus others represent typical parallel applications purpose experiments analyze progressively effects upgrade overall system performance point point communication collective operations kernels numeric applications nas parallel benchmarks swm nonnumeric application fine grain characteristics analyzing obtained measurements conclude important improvement achieved bandwidth communication channels allows applications reach throughput values much higher achievable old networkalthough possible applications interchange long enough messages 10 kb 1 mb absolute terms point point communications asymptotic throughput new communication subsystem doubles previous one contrast messages short improvement bandwidth translate better performance startup time reduced words latency short medium messages changed significantly old new system consequently applications requiring frequent interchange massive amounts information experience clear reductions execution time results shown report confirm conclusions researchers studies real bottleneck mpp communication lies messagepassing software message interfaces attach nodes interconnection network therefore points mpp designers focus interest opinion 1 overhead message passing software minimized even means introducing changes architecture node processors 2 message interface located close processor possible connecting system bus even bus offchip cache processor acknowledgements want express grateful acknowledgement following institutions c4 centre de computaci comunicacions de catalunya providing access machine test technical support cicyt research done support comisin interministerial de ciencia tecnologa spain contract tic950378 dgicyt direccin general de investigacin cientfica tcnica grant allowed ja gregorio stay uci visiting associate researcher department electrical computer engineering university california irvine providing support equipment access r sp2 system architecture nas parallel benchmarks 20 users guide mpich assessing fast network interfaces mpi programming environment ibm sp1sp2 ibm power parallel division pvm users guide tutorial networked parallel computing communication challenge mpp intel paragon meiko cs2 computer architecture empirical evaluation techniques parallel simulation message passing networks message passing interface forum myricoms myrinet information communication software parallel environment ibm sp2 sp2 highperformance switch pstswm v4 modeling communication overhead mpi mpl performance ibm sp2 tr ctr sangman moh chansu yu ben lee hee young youn dongsoo han dongman lee fourary treebased barrier synchronization 2d meshes without nonmember involvement ieee transactions computers v50 n8 p811823 august 2001 jess labarta sensitivity performance prediction message passing programs journal supercomputing v17 n3 p291298 nov 2000 zoltan johasz analytical method predicting performance parallel image processing operations journal supercomputing v12 n12 p157174 janfeb 1998 j gregorio r beivide f vallejo modeling interconnection subsystems massively parallel computers performance evaluation v47 n2 p105129 february 2002 manuel prieto ignacio llorente francisco tirado data locality exploitation decomposition regular domain problems ieee transactions parallel distributed systems v11 n11 p11411150 november 2000