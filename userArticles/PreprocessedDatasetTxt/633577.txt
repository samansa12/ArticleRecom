geometric approach leveraging weak learners adaboost popular effective leveraging procedure improving hypotheses generated weak learning algorithms adaboost many leveraging algorithms viewed performing constrained gradient descent potential function iteration distribution sample given weak learner proportional direction steepest descent introduce new leveraging algorithm based natural potential function potential function direction steepest descent negative components therefore provide two techniques obtaining suitable distributions directions steepest descent resulting algorithms bounds incomparable adaboosts analysis suggests algorithm likely perform better adaboost noisy data weak learners returning low confidence hypotheses modest experiments confirm algorithm perform better adaboost situations b introduction algorithms like adaboost 7 able improve hypotheses generated weak learning methods great potential practical benefits call algorithm leveraging algorithm leverages weak learning method examples leveraging algorithms include bagging 3 arcx4 5 logitboost 8 one class leveraging algorithms follows following template construct master hypotheses given sample leveraging algorithm begins default master hypothesis h 0 constructs distribution sample function sample current master hypothesis h possibly trains weak learner using distribution sample obtain weak hypothesis h picks ff creates new master hypothesis authors supported nsf grant ccr 9700201 essentially arcing paradigm introduced breiman 5 4 skeleton adaboost boostbyresampling algorithms 6 7 although leveraging algorithms include arcing algorithms following template leveraging algorithms general section 2 introduce geolev algorithm changes examples sample well distribution paper consider 2class classification problems 1g however following schapire singer 15 allow weak learners hypotheses confidence rated mapping domain x real num bers sign numbers gives predicted label magnitude measure confidence master hypotheses produced template interpreted way although underlying goal produce hypotheses generalize well focus quickly leveraging algorithm decreases sample error variety results bounding generalization error terms performance sample 16 given sample margin hypothesis h instance x hx margin h entire sample vector hypothesis correctly labels sample margin vector whose components positive focusing margin vectors provides geometric intuition leveraging problem particular potential function margin space used guide choices ff distribution direction steepest descent ff value minimizes potential h leveraging algorithms viewed way perform feasible direction descent potential function amortized analysis using potential function often used bound number iterations required achieve zero sample error potential functions give insight strengths weaknesses various leveraging algorithms boosting algorithms property convert weak pac learning algorithms strong pac learning algorithms although theory behind adaboost algorithm elegant leads somewhat intriguing result minimizing normalization factor distribution reduce training error 14 15 search better understanding adaboost reduces sample error led geometric algorithms geolev geoarc although performance bounds algorithms poor show boosting property bounds incomparable adaboosts better weak hypotheses contain mostly lowconfidence predictions main contributions paper follows use natural potential function derive new algorithm leveraging learners called geolev geometric leveraging algorithm highlight relationship adaboost arcing feasible direction linear programming 10 use geometric interpretation prove convergence bounds algorithm geolev bound number iterations taken geolev achieve ffl classification error training set provide general transformation geolev arcing algorithm geoarc bounds hold summarize preliminary experiments geolev geoarc motivate novel algorithm geolev considering geometry margin space since many empirical analytical results show good margins sample lead small generalization error 2 16 natural seek master hypothesis large margins one heuristic seek margin vector uniformly large margins ie vector parallel 1 indicates master hypothesis correct equally confident every instance sample geolev algorithm exploits heuristic attempting find hypotheses whose margin vectors close possible 1 direction focus single iteration leveraging process dropping time subscripts margin vectors printed bold face often normalized euclidean length one thus h margin vector master hypothesis h whose th component let goal vector p 1 normalized length one recall sample size margin vectors lie normalized margin vectors lie dimensional unit sphere note easy rescale confidences multiplying predictions hypothesis h constant change direction hs margin vector therefore assume appropriate normalization without loss generality first decision taken leverager distribution place sample since distribution components also viewed nonnegative vector situation marginspace start iteration shown figure 1 order decrease angle h g must move head h towards g vectors angle goal vector g lie cone normalizations lie rim shown figure h weak hypothesiss margin vector need unit length parallel h tangent rim addition h h decrease angle g hand line h cuts cone angle goal vector g reduced adding multiple h h time angle g cannot decreased h vector lies plane p tangent cone contains vector h shown figure 2 theta fig 1 situation margin space start iteration weak learner learns hypothesis h better random guessing learners edge e id hx positive means delta h positive distribution viewed margin vector perpendicular plane p h lies p therefore leverager able use h reduce angle h g suggested figures appropriate direction general neither jjdjj components positive normalized yield distribution sample weak learner however possible components negative case things complicated 1 component negative flip sign component sign corresponding label sample creates new direction 0 normalized distribution 0 new sample 0 x possibly new labels 0 modified sample 0 distribution 0 used generate new weak hypothesis h let h 0 margins h modified sample 0 h 0 fact complication differentiates geolev arcing algorithms arcing algorithms permitted change sample way second transformation avoiding label flipping discussed section 5 fig 2 direction distribution used geolev sign flips cancel second decision taken algorithm incorporate weak hypothesis h master hypothesis h weak hypothesis edge distribution described used decrease goal find coefficient ff h jjhffhjj2 decreases angle much possible taking derivatives shows minimized discussion see geolev performs kind gradient descent consider angle g current h potential margin space direction steepest descent moving direction approximates gradient takes us towards goal vector since little control hypotheses returned weak learner approximation direction best step size chosen adaptively make much use weak hypothesis possible geolev algorithm summarized figure 3 3 relation previous work breiman 5 4 defines arcing algorithms using potential functions expressed componentwise functions margins form 2 breiman allows componentwise potential f depend sum ff arcing algorithms input sample weak learning algorithm initialize master hypothesis h predict 0 everywhere repeat add else add call weak learner distribution 0 obtaining hypothesis h fig 3 geolev algorithm breiman shows certain conditions f arcing algorithms converge good hypotheses limit furthermore shows adaboost arcing algorithm arcing algorithm polynomial fx completeness describe adaboost algorithm show notation performing feasible direction gradient descent potential function adaboost fits template outlined introduction choosing distribution z z normalizing factor sums 1 master hypothesis updated adding multiple new weak hypothesis coefficient ff chosen minimize exp next iterations z value unlike geoboost margin vectors adaboosts hypotheses normalized show adaboost viewed minimizing potential approximate gradient descent direction steepest descent wrt components margin vector proportional 5 distribution adaboost gives weak learner continuing analogy coefficient ff given new hypothesis minimize potential x updated master hypothesis identical 6 thus adaboosts behavior approximate gradient descent function defined 7 direction descent weak learners hypothesis furthermore bounds adaboosts performance proven schapire singer implicitly performing amortized analysis potential function 8 arcx4 also fits template outlined introduction keeping unnormalized master hypothesis notation distribution chosen trial proportional algorithm also viewed gradient descent potential function th iteration rather computing coefficient ff function weak hypothesis arcx4 always chooses ff 1 thus h weight 1t master hypothesis many gradient descent methods unfortunately dependence potential function makes difficult use amortized analysis connection gradient descent hinted freund 6 noted breiman others 4 8 13 interpretation generalizes previous work relaxing constraints potential function particular show construct algorithms potential functions direction steepest descent negative components potential function view leveraging algorithms shows relationship feasible descent linear programming relationship provides insight role weak learner feasible direction methods try move direction steepest descent however must remain feasible region described constraints descent direction chosen closest negative gradient gammarf satisfying constraints example simplified zoutendijk method chosen direction satisfies constraints maximizes gammarf delta similarly leveraging algorithms discussed constrained produce master hypotheses lying span weak learners hypothesis class one view role weak learner finding feasible direction close given distribution negative gradient fact weak learning assumption used boosting analysis geolev implies always feasible direction gammarf delta bounded zero gradient descent framework outlined provides method deriving corresponding leveraging algorithm smooth potential functions margin space potential functions used adaboost arcx4 advantage components gradients positive thus easy convert distribution hand methods outlined previous section section 5 used handle gradients negative components approach used ratsch et al 13 similarly interpreted potential function margins recently friedman et al 8 given maximum likelihood motivation adaboost introduced another leveraging algorithm based loglikelihood criteria indicate minimizing square loss potential performed less well experiments monotone potentials conjecture nonmonotonicity penalizing margins greater 1 contributing factor methods described section 5 may provide way ameliorate problem convergence bound section examine number iterations required geolev achieve classification error ffl sample key step shows sine angle goal vector g master hypothesis h reduced iteration upper bounding resulting recurrence gives bound rapidly training error decreases begin considering single boosting iteration margin space quantities previously defined recall g h 2normed h addition let h 0 denote new master hypothesis end iteration 0 angle h 0 g assume throughout sample finite delta h edge weak learners hypothesis h respect distribution given weak learner bound decrease depend h r jjhjj 2 note r chosen maintain consistency work schapire singer 15 start iteration end iteration sin 0 recall h 0 h ffh normalized since h already unit length lemma 1 value cos 2 0 maximized sin 0 minimized g proof lemma follows examination first second derivatives cos 2 0 respect ff 2 using value ff little algebra shows although desire bounds hold h find convenient first minimize 15 respect h delta h remaining dependence h expressed function r jjhjj 2 final bound lemma 2 equation 15 minimized h proof lemma follows examining first second derivatives respect h delta h 2 considerably simplifies 15 yielding recall therefore bound two ways using different bounds jjdjj 1 first bounds derived noting jjdjj 1 jjdjj 2 recall combining 18 bound jjdjj 1 yields sin jjhjj 2 20 repeated application bound yields following theorem theorem 1 r edges weak learners hypotheses first iterations sine angle g margin vector master hypothesis computed iteration bound jjdjj 1 another way obtain bound often better note jjdjj 1 delta substituting 18 continuing yields sin continuing results following theorem theorem 2 let r edges weak learners hypotheses angles g margins master hypotheses start first iterations t1 angle g margins master hypothesis produced iteration relate results sample error use following lemma lemma 3 sin angle g master hypothesis h sample error h less ffl proof assume sin rm 2normed hold h positive components therefore master hypothesis correctly classifies examples sample error rate r gamma 1m 2 combining lemma 3 theorem 2 gives following corollary corollary 1 iteration sample error rate geolevs master hypothesis bounded recurrence theorem 2 somewhat difficult analyze apply following lemma abe et al 1 lemma 4 consider sequence fg g nonnegative numbers satisfying g t1 positive constant f c 2 n given lower bound r r values upper bound h 2 jjh jj 2 apply lemma recurrence 22 setting h 2m shows sin 2 25 previous results lead following theorem theorem 3 weak learner always returns hypotheses edge greater r h 2 upper bound jjh jj 2 geolevs hypothesis ffl training error iterations similar bounds obtained freund schapire 7 adaboost theorem 4 iterations sample error rate adaboosts master hypothesis 2 27 dependence jjhjj 1 implicit bounds removed h comparing corollary 1 theorem 4 leads following observations first bound geolev contain squareroot difference would correspond halving number iterations required reach error rate ffl sample effect approximated factor 2 r 2 terms important difference factors multiplying r 2 terms preceding approximation geolevs bound 2m sin 2 jjh jj 2 aboosts bound 1jjh jj 2 1 larger factor better bound dependence sin 2 means geolevs progress tapers approaches zero sample error weak hypotheses equally confident examples jjh jj 2 2 times larger jjh jj 2 1 difference factors simply 2 sin 2 start boosting process close 2 geolevs factor larger however sin 2 small 1m geolev predicts perfectly sample thus geolev seem gain much later iterations difficulty prevents us showing geolev boosting algorithm hand consider less likely situation weak hypotheses produce confident prediction one sample point abstain rest jjh jj 2 1 geolevs bound extra factor 2m sin 2 geolevs bounds uniformly better 3 adaboosts case 5 conversion arcing algorithm geolev algorithm discussed far fit template arcing algorithms modifies labels sample given weak learner 3 must switch recurrence 20 rather recurrence 22 sin 2 small also breaks boosting paradigm weak learner may required produce good hypothesis data consistent concept underlying concept class section describe generic conversion produces arcing algorithms leveraging algorithms kind without placing additional burden weak learner throughout section assume weak learners hypotheses produce values gamma1 1 conversion introduces wrapper weak learner leveraging algorithm replaces signflip trick section 2 wrapper takes weighting leveraging algorithm creates distribution setting negative components zero renormalizing modified distribution 0 given weak learner returns hypothesis h margin vector h margin vector modified wrapper passed leveraging algorithm dx negative h set gamma1 thus leveraging algorithm sees modified margin vector h 0 uses compute ff margins new master hypothesis intuition leveraging algorithm fooled thinking weak hypothesis wrong parts sample actually correct therefore margins master hypothesis actually better tracked leveraging algorithm furthermore apparent edge weak learner increased wrapping transformation intuition formalized following theorems theorem 5 edge weak learner respect distribution sees r edge modified weak hypothesis respect signed weighting requested leveraging algorithm r 0 r proof ensures 0 otherwise assumption h implies r 1 r 0 minimized theorem 6 component master margin vector used wrapped leveraging algorithm ever greater actual margins master hypothesis proof theorem follows immediately noting component h 0 greater corresponding component h 2 call wrapped version geolev geoarc arcing algorithm instructive examine potential function associated geoarc ip min potential similar form following potential function zero entire positive orthant ip leveraging framework described together transformation enables analysis undifferentiable potential functions full implications remain explored 6 preliminary experiments performed experiments comparing geolev geoarc adaboost set 13 datasetsthe 2 class ones used previous experiments uci repository experiments run along lines reported quinlan 12 ran cross validation datasets two class classification leveraging algorithms ran 25 iterations used single node decision trees implemented mlc 9 weak hypotheses note sigma1 valued hypotheses large 2norms noticed splitting criterion used single node large impact results therefore results reported dataset better mutual information ratio gain ratio report comparison adaboost geolev geoarc performed comparably geolev results illustrated figure 4 figure scatter plot generalization error datasets results appear indicate new algorithms comparable adaboost experiments clearly warranted especially interested situations weak learner produces hypotheses small 2norm 7 conclusions directions study presented geolev geoarc algorithms attempt form master hypotheses correct equally confident sample found convenient view algorithms performing feasible direction gradient descent constrained hypotheses produced weak learner potential function used geolev monotonic gradient negative components therefore direction steepest descent cannot simply normalized create distribution weak learner described two ways solve problem first constructing modified sample flipping labels solution mildly unsatisfying strengthens requirements weak learner weak learner must deal broader class possible targets therefore also presented second transformation increase requirements weak learner fact using second transformation actually improve efficiency geolev adaboost fig 4 generalization error geolev versus adaboost 25 rounds leveraging algorithm one open issue whether improvement exploited improve geoarcs performance bounds second open issue determine effectiveness transformations applied nonmonotonic potential functions considered mason et al 11 upper bounded sample error rate master hypotheses produced geolev geoarc algorithms bounds incomparable analogous bounds adaboost bounds indicate ge olevgeoarc may perform slightly better start leveraging process weak hypotheses contain many lowconfidence predictions hand bounds indicate geolevgeoarc may exploit later iterations well may less effective weak learner produces valued hypotheses disadvantages make unlikely geoarc algorithm boosting property one possible explanation geolevgeoarc aim cone inscribed positive orthant margin space sample size grows dimension space increases volume cone becomes diminishing fraction positive orthant adaboosts potential function appears better navigating corners positive orthant however preliminary tests indicate 25 iterations generalization errors geoarcgeolev similar adaboosts 13 classification datasets uci repository comparisons used 1node decision tree classifiers weak learning method would interesting compare relative performances using weak learner produces hypotheses many lowconfidence predictions acknowledgments would like thank manfred warmuthrobert schapire yoav freund arun jagota claudio gentile eurocolt program committee useful comments preliminary version paper r polynomial learnability probabilistic concepts respect kullbackleibler divergence training algorithm optimal margin classifiers bagging predictors arcing edge boosting weak learning algorithm majority decisiontheoretic generalization online learning application boosting additive logistic gression statistical view boosting data mining using mlc improved generalization explicit optimization margins bagging boosting c4 margins adaboost boosting margin new explanation effectiveness voting methods improved boosting algorithms using confidencerated predictions estimation dependences based empirical data tr theory learnable size net gives valid generalization polynomial learnability probabilistic concepts respect kullbackleibler divergence equivalence models polynomial learnability training algorithm optimal margin classifiers design analysis efficient learning algorithms learning boolean formulas introduction computational learning theory boosting weak learning algorithm majority bagging predictors exponentiated gradient versus gradient descent linear predictors decisiontheoretic generalization online learning application boosting general convergence results linear discriminant updates adaptive version boost majority algorithm drifting games additive models boosting inference generalized divergences boosting entropy projection prediction games arcing algorithms improved boosting algorithms using confidencerated predictions empirical comparison voting classification algorithms margin distribution bounds generalization