persistent rescheduledpage cache low overhead object code compatibility vliw architectures objectcode compatibility processor generations open issue vliw architectures potential solution technique termed dynamic rescheduling performs runtime software rescheduling firsttime page faults time required rescheduling pages constitutes large portion overhead method disk caching scheme uses persistent rescheduledpage cache prc presented scheme reduces overhead associated dynamic rescheduling saving rescheduled pages disk across program executions operating system support required dynamic rescheduling management prc implementation details prc discussed results simulations used gauge effectiveness prc indicate prc effective reducing overhead dynamic rescheduling due different overhead requirements programs split prc organization performs better unified prc unified prc studied two different page replacement policies lru overheadbased replacement found lru replacement programs consistently perform better increasing prc sizes highoverhead programs take consistent performance hit compared lowoverhead programs overheadbased replacement performance highoverhead programs improves substantially lowoverhead programs perform slightly worse case lru replacement b introduction unlike contemporary superscalar processors 1 2 3 employ dynamic scheduling vliw processors de published proc 29th annual intl symp microarchitecture paris 1996 pend schedule code generated compiler compiler full knowledge machine model described terms hardware resources available latencies related execution resource correct execution program scheduled one machine model assumptions guaranteed processors exactly machine model supersets assumptions strictly held thus program scheduled particular generation vliw family cannot guaranteed binary compatible generations known objectcode compatibility problem vliw architectures 4 lack objectcode compatibility commonly cited reason vliws may become generalpurpose computing paradigm 5 solutions problem suggested classified hardware software approaches hardware techniques typically employ scheduling hardware 6 7 4 8 9 could substantially increase hardware complexity machine common software approach offline recompilation source programs yields excellent performance compiler access necessary information expose ilp program drawback technique cumbersome use access source code may always pos sible variant offline objectcode translation practical source code unavailable large set programs older architecture exists 10 alternatively interpreter could used translate architectures runtime 11 approach usually suffers poor performance another approach dynamic rescheduling 12 technique program binary scheduled target machine model vliw generation allowed directly execute generation vliw execution proceeds pagefault generated program instruction space results special action page fetched rescheduled machine model target generation perpage rescheduling performed first instance pagefault occurs ie firsttime page faults page code considered atomic unit rescheduling implementation technique requires support operating system specifically page fault service routine extra time incurred rescheduling constitutes overhead technique paper presents scheme reduces overhead associated dynamic rescheduling caching ondisk rescheduled code perpage basis employs structure called persistent rescheduledpage cache prc holds rescheduled pages across multiple executions pro gram organization paper follows section 2 reviews previous work objectcode compatibility explains dynamic rescheduling presents performance measurements code subjected dynamic rescheduling section 3 introduces prc part operating systemmanaged disk caching scheme reduces overhead dynamic rescheduling architecture management prc detailed experimental results measure performance presented section 4 presents conclusions study avenues future work compatibility dynamic rescheduling several hardware approaches reported previously address vliw compatibility problem rau presented technique called splitissue perform superscalar style dynamic scheduling code hardware 4 fill unit originally proposed melvin shebanow patt 6 later extended franklin smotherman 8 adapted achieve limited level compatibility vliws approaches counter principle hardware simplicity one tenets vliw philosophy obvious alternative offline recompilation rescheduling technique advantage terms performance extremely cumbersome use owing offline na ture similar offline binary translation used migrate vax software alpha platform 10 runtime software emulation also used implemented insignia solutions softwindows product 13 approach employing emulation translation techniques utilized fx32 product dec 14 may investigated using interpreter execute ibm system370 binaries ibm rt pc 11 solutions attempt achieve compatibility widely varying architectures although large differences probably exist generations vliw family techniques used still interest interpreters use caching translated sections code across executions means reducing amount translation needed disk storage used cache translated code caching intuitive effective technique reducing runtime translation overhead however majority literature translation contain details management cached code segments reschedule firsttime page fault context switch overhead24 figure 1 sequence events dynamic rescheduling events 13 detection page fault generation mismatch context switch process retrieval page disk respectively events standard overhead dynamic rescheduling event 4 page dynamically rescheduled another technique compatibility dynamic rescheduling dr 12 firsttime pagefaults os invokes module called dynamic rescheduler reschedule page accessed host machine model sequence events dynamic rescheduling illustrated figure 1 detection firsttime page fault shown event 1 events 1 2 3 always take place page fault case dynamic rescheduling however treated special page fault indicated event 4 program binary contains complete description machine originally com piled page fault handler detects generation mismatch host machine program rescheduler module invoked page rescheduled program execution resumes rescheduled page selected replacement physical memory program ter minates written text swap space 15 16 eliminates need reschedule page reaccessed net overhead dynamic rescheduling technique quantitatively expressed terms following three factors 1 time spent rescheduling pages runtime 2 time write rescheduled pages text swap pages replaced 3 amount disk space used save rescheduled pages time disk io page replacement negligibleas writing swap performed asynchronously 15 true overhead reduced time rescheduling disk space required save rescheduled pages dynamic rescheduling performed code motion would give rise code size changes due compensation code may insert andor delete since straightforward handle changes code size dynamically dr framework avoids size changes altogether via special binary encoding eliminates explicit use nops code discussion issues involved therein found 12 beyond scope paper playdoh 17 vliw architecture hewlettpackard laboratories used testbed paper also assumed modifications dr framework made instruction space programs selfmodifying code 21 performance dynamic rescheduling effectiveness dynamic rescheduling measured using programs specint92 suite 1 several unix utility programs 12 expanded order construct interactive load benchmarks divided two categories tools cccp compress gcc grep tbl applications espresso eqntott li lex sc yacc assumed tools would invoked twice often applications would pattern workload two sets inputs used benchmark alternated invocations benchmark using assumptions several workloads created used measure performance benchmarks without dynamic rescheduling three machine models used evaluation performance benchmarks situations required dynamic rescheduling generations 1 2 3 organizations shown figure 2 types functional units shown horizontally execution latency assumptions shown vertically unit pred used perform predicate computation integer compare oper ations predicate computation fp compare operations done fpadd unit three part method used evaluate dynamic rescheduling technique first part intermediate code benchmark scheduled given machine model using vliw scheduler hyperblock scheduling used initial compilation 18 intermediate code profiled order find worstcase estimate execution time terms number cycles number times page code accessed also recorded also indicates unique performance characteristics none fp programs presence prc presented rescheduling software pipelined loops yet implemented current dynamic rescheduling framework code page accessed called native mode experiment second part code scheduled native mode execution rescheduled machine models execution time estimates rescheduled code also generated described time estimate indicates performance rescheduled code without taking account rescheduling overhead incurred rescheduler hence part termed overhead experiment aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa latency fu generation2 aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaaaaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaaaaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaaaaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa generation3 aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa ialu fpadd fpmul pred load store branch figure 2 simulated machine models third part rescheduler compiled scheduled machine model used first part used benchmark input rescheduler benchmark pages taken benchmarks performance rescheduler benchmark used find average time reschedule single 4k page three machine models found 54272 cycles generation1 51200 cycles generation2 48108 cycles generation3 combined number unique page accesses first part experiment estimate total number execution cycles rescheduling overhead rescheduling overhead added execution times nooverhead experiment derive execution times woverhead experiment finally compare performance achieved three parts speedup respect singleunit singleissue processor model base model calculated speedup number cycles execution estimated experimentnumber cycles execution estimated base model three parts assumed page size 4k bytes used many contemporary operating systems 19 20 processors 21 3 results three parts shown figure 3 harmonic gn ov g3 speedup native code genn gn gn ov nooverhead speedup genn code translated genm withoverhead speedup genn code translated genm ov ov ov ov g12 ov g12 ov g13 ov g13 ov figure 3 speedups programs dynamic rescheduling framework bar harmonic mean speedups benchmarks specific generationtogeneration rescheduling set bars identified xaxis gnm label indicates code originally scheduled generationn rescheduled generationm first bar set native performance generationm remaining bars show rescheduled performance without overhead due dynamic rescheduling observed nooverhead speedup rescheduled benchmark code marked ov comparable performance native compiled code nat gn prominent performance degradation rescheduling overhead taken account ov dominant time page fault handling time spent reading page disk executable stored local machine page accessed lan common clientserver environment network latency dominant factor least order magnitude local disk access time spent rescheduling page adds page access latency get estimate much overhead dr would add page fault handling local paging environment experiment conducted measure pagefault service time paging set contiguous blocks local disk sar performance analysis tool 22 used measure average page fault service time two hardware platforms found 2500 2 averaged total 50 pagefaults based average execution times 100 mhz machine dynamic rescheduling increases page fault service time 20 significant increase rescheduling 4 kilobytepage with64bit operations generation2 code generation1 code example would take 54272 cycles times 10 ns approximately 543s 20 average pagefault service time measured apparent case paging lan relative overhead dr lower probably small enough completely neglect two approaches used reduce overhead dr 1 improve performance dr algorithm 2 reduce number pages require rescheduling multiple invocations program second component focus investigation paper os saves rescheduled pages text swap program swapped execution rescheduled pages effectively cached disk programs current execution extending concept os aid caching rescheduled pages single program execution across multiple program executions well following section introduces ossupported caching scheme achieves caching persistent rescheduledpage cache significant overhead introduced dynamic rescheduling largely alleviated use caching scheme employs persistent rescheduled page cache prc prc osmanaged disk cache holds binary images rescheduled pages rescheduled pages cached need rescheduled reaccessed across program runs prc defined behavior program execution program termination duringexecution rescheduled pages stored text swap space program terminates rescheduled pages written prc page placement replacement policies implemented within os concept behind persistent rescheduledpage cache originated softwaremanaged disk caching proven method reducing io traffic 15 16 idea cache recently accessed disk blocks memory hope accessed near future typical unix systems implement file system buffer cache run 133 mhz pentiumbased data general computer running dgux 99 mhz hewlettpackard 9000715 computer running hpux hold recently accessed disk blocks memory using lru algorithm replacement 15 16 buffer cache effectively operates fragmented page cache also reduces amount io page faults 3 variants used distributed file systems sprite 23 andrew 24 purposes remote file caching disk caching also used reduce translation overhead architectural interpreters 11 used effectively dynamic rescheduling 31 persistent rescheduledpage caches prc organized systemwide cache portion file system holds rescheduled pages managed os pages different programs displace case primary configuration parameters prc size placement replacement policies discussed initial program execution nonnative host text pages rescheduled firsttime page faults described section 2 rescheduled page displaced physical memory program execution written text swap space disk prevents rescheduling page multiple times program execution end initial execution rescheduled pages written prc page placement policy explained shortly subsequent executions page fault occurs text page generation mismatch prc probed check presence rescheduled version page rescheduled version present retrieved loaded physical memory overhead rescheduling incurred rescheduled version available page retrieved binary image program rescheduled loaded physical memory page replaced program execution written text swap space program termination rescheduled pages program written prc rescheduled page placed entry cache implies cache effectively fully associative replacement policy lru outline algorithm used prc management shown figure 4 probing prc disk presence page expensive operation eliminated modifying programs disk block pointers execution program exit page rescheduled subsequently written prc program termination separate set disk block pointers prc pointers program set point rescheduled versions pages cache prc pointer annotated indicate original page rescheduled page replaces scheme implements prc probe examination programs disk block pointers 3 term fragmented used disk blocks comprise page might buffer cache time load program rescheduled pages exist set page table entries disk addresses prc run program program termination write rescheduled pages prc needed displace lru pages prc update program prc disk block pointers point prc update prc pointers point program file data structure figure 4 persistent rescheduledpage cache management algorithm one central location rather multiple locations prc disk disk block pointers cached os reduce number disk probes rescheduled version page accessed without probing prcon disk perhaps withoutany disk accesses programs disk block pointers inmemory cache rescheduled pages stored swap program execution managed os using known methods managing text swap space 15 probing multiple prc pointers page faults also eliminated program load time disk block prc pointers program examined determine rescheduled versions pages exist prc page rescheduled version loader modifies page table entry pte page point rescheduled version page fault occurs page rescheduled version rescheduled version accessed directly using updated pte disk accesses inmemory searches required implement prc probe pte page points prc rescheduled version exists remapping pte entries done program load time 15 16 rescheduled pages displaced use prc due replacement prc pointers displaced pages must nullified accomplished os support table maintained prc manager lists locations prc pointers associated page cache page replaced prc pointer set null 32 prc performance figure 5 presents performance prcs different sizes rescheduling across four generationtogeneration prc infinite prc0 prc32 prc64 prc128 prc256 prc512 prc1024 figure 5 speedups benchmarks prc performance unified prc lru replacement bar harmonic mean speedups benchmarks particular generationtogenerationrescheduling prc size set bars identified xaxis gnm label indicates code originally scheduled generationn rescheduled generationm first bar set bars nooverhead performance second bar worstcase overhead prc subsequent bars performance indicated prc size maximum number pages cache holds combinations metric used speedup single universalunit singleissue processor set bars shows harmonic mean speedups rescheduling combination various prc sizes indicated prcn means prc size n pages prcinfinite indicates performance rescheduling unique page accesses performed initial invocations essentially prc without upper bound size prcinfinite speedups nooverhead case speedups mentioned section 21 prc0 indicates performance prc used pages uniquely accessed program rescheduled invocation provides measure worstcase overhead rescheduling page displace page prc based lru replacement policy organization called unified prc performance rescheduling combinations benefits use prc trend larger prc12345 008espresso 022li 023eqntott 026compress 072sc 085gcc cccp tbl grep lex yacc prc infinte prc0 prc32 prc64 prc128 prc256 prc512 prc1024 figure 6 generation1 generation3 rescheduling unified prc lru replacement bar speedup generation1 generation3 rescheduling specified prc size set bars corresponds individual benchmark indicated labels xaxis first bar set bars nooverhead performance second bar worstcase overhead prc subsequent bars performance indicated prc size prc size maximum number pages cache holds provides greater speedup note prc1024 perfect speedup equal prcinfinite case achieved happens total number pages workload less size prc programs completely reside prc size without requirement reschedule prc populated benchmarks benefit equally presence prc illustrate figure 6 presents speedups individual benchmarks generation1 generation3 rescheduling benchmarks 008espresso 023eqntott 026compress show small improvement even large prc others cccp tbl grep show substantial improvement increasing prc size moderate improvement shown 022li 072sc 085gcc lex yacc reason behind behavior explained using overhead ratio metric program overhead ratio defined table 1 unique page counts benchmarks 026compress 8 cccp 34 tbl 50 lex table 2 overhead ratio generation1 generation3 rescheduling overhead ratio overhead 008espresso 435 low 022li 1052 moderate 023eqntott 064 low 026compress 125 low 072sc 629 moderate 085gcc 1750 moderate cccp 5009 high tbl 6415 high grep 2910 high lex 1651 moderate e execution time program r total rescheduling overhead unique page count program avg time required reschedule page unique page count program defined number firsttime page faults occur execution program values unique page counts shown table 1 table 2 shows percentage values overhead ratio generation1 generation3 rescheduling high overhead ratio 20 case indicates amount time taken reschedule relatively high programs benefit use prc cccp tbl grep shown termed highoverhead pro grams programs showed least performance improvement overhead ratio relatively small less 5 programs termed lowoverhead programs 008espresso 023eqntott 026compress others 022li 072sc 085gcc lex yacc value 5 20 referred moderateoverhead programs unified prc low moderateoverhead programs evict highoverhead programs completely unique page count large thus example even unique page count highoverhead program cccp small 35 case moderateoverhead program 085gcc replace pages allocated cccp unique page count relatively high 323 case two programs run alternate fashion one would expect one c preprocessor c compiler number pages cccp rescheduled could sizable thus increasing overhead especially smaller prc sizes better organization prc reduces effect presented next 33 split prc program behavior presence prc varies directly programs overhead ratio one approach partition cache hold pages different classes programs based program overhead ra tio prevent cache pollution benchmark benefits little prc displacing pages program whose performance substantially enhanced prc osgathered statistics dynamically used compute overhead ratios programs determine prc partition program use os needs record unique page counts program execution time along average time take reschedule page purpose prc two partitions labeled 2way split prc one partition hold low moderateoverhead programs exclusive use high overhead programs figure 7 presents results prc across various generationtogeneration reschedul ings prc sizes partitions size though requirement partition sizes varied depending program unique page counts performance practically generationto generation rescheduling combinations improved using 2way split prc unified prc particular generation1 generation3 rescheduling using prc 512 speedup 91 speedup using infinite prc infinite prc corresponds nooverhead experiment described section 21 generation 1 generation2 rescheduling also showed improvement split cache particularly prc256 prc512 general trend larger prcs performed better expected speedups individual benchmarks 2way split prc generation1 generation3 translation shown figure 8 highoverhead benchmarks fared well compared performance unified prc figure 6 low performance smaller prc sizes compete others prc infinite prc0 prc32 prc64 prc128 prc256 prc512 prc1024 figure 7 speedups prc performance 2 way split prc lru replacement partition also observed cache pollutioneffect still persists substantially reduced example observe improvement performance cccp tbl compared unified prc prc sizes based experiments nway split cache pages per partition would better unified cache entries actual implementation prc partitioned much granularity os allows 34 unified prc overheadbased replace ment another technique reduce cache pollution effect observed unified prc use overheadbased prc page replacement policy instead lru works follows page prc associated overhead ratio program page belongs page allowed replace another page prc unless overhead ratio higher overhead ratio page replacement policy ensures highoverhead programs stay prcresident placed prc priority use prc governed overhead ratio higher overhead ratio higher priority consequently low moderate overhead programs incur higher rescheduling overhead since relatively less number pages get cached vocations intuitively scheme similar multiple dynamically resizable partitions within prc135 008espresso 022li 023eqntott 026compress 072sc 085gcc cccp tbl grep lex yacc prc infinte prc0 prc32 prc64 prc128 prc256 prc512 prc1024 figure 8 generation1 generation3 rescheduling 2way split prc lru replacement figure 9 shows performance individual benchmarks generation1 generation3 translation scheme observed top three highoverhead programs cccp tbl grep perform much better replacement policy lru figure 6 show perfect speedup prc sizes 128 low speedups small prc sizes due large number capacity misses hand rest programs show decrease albeit small performance smaller prc sizes compared lru larger prcs performance however improved 4 based fact smaller prc sizes encounter large number capacity misses effectively displacing prc incurring larger rescheduling overhead case 008espresso 022li 023eqntott 026com press 072sc lex however little performance gain prc sizes explained considering case lex lex relatively small unique page count 45 refer table 1 compared 323 085gcc overhead ratio 1651 however slightly lower 085gcc 1750 refer table 2 lex gets displaced prc every invocation 4 mentioned earlier programshave perfect speedup prc 1024 large enough hold entire workload throughout experiment 008espresso 022li 023eqntott 026compress 072sc 085gcc cccp tbl grep lex yacc prc infinte prc0 prc32 prc64 prc128 prc256 prc512 prc1024 figure 9 generation1 generation3 rescheduling unified prc overheadbased replacement 085gcc incurs overhead rescheduling pages subsequent invocation speculated impact phenomenon may reduced 085gcc allowed displace programs invoked relatively remote past combination lru overheadbased schemes may prove effective cases topic future research nonetheless programs 008espresso 022li 023eqntott 026compress 072sc lex low moderateoverhead overall performance across programs better previous two schemes apparent figure 10 compared performance unified prc lru replacement figure 5 2way split prc lru replacement figure 7 overheadbased replacement performs better across prc sizes general performance trend observed overheadbased replacement policy confirms intuition priority use prc dictated overhead ratio objectcode compatibility problem vliws approached either software hardware methods a152535 prc infinite prc0 prc32 prc64 prc128 prc256 prc512 prc1024 figure 10 speedups prc performance unified prc overheadbased replacement technique called dynamic rescheduling uses operating system support reschedule program pagebypage basis firsttime page faults reviewed overhead technique time required reschedule space required save rescheduled pages number times page rescheduled reduced saving rescheduled version disk across program executions persistent rescheduledpage cache introduced part scheme saves rescheduled pages using disk caching prc reduces number times page rescheduled across multiple program executions architecture management prc described specifically use implementation systemwide shared prc investigated unified caches simulated measure performance conclusion drawn prcs general effective reducing overhead associated dynamic rescheduling effectiveness varies across programs dependent overhead ratio associated program identified highoverhead programs effectively displaced low overhead program prc dependent invocation pattern workload partitioned cache organization introduced classified programs based overhead ratios bipartitioned scheme called split prc simulated found improved performance unified prc third scheme overheadbased page replacement policy implemented unified prcand found performance highoverhead programs improved substantially compared two previous schemes lowoverhead programs fared slightly worse previous schemes future research dealing prcs investigate use cache organizations example determination number partitions ideal split prc investigation perprogram prc structure underway also study combination algorithm page replace ment based lru overheadbased schemes conducted acknowledgments discussions mary ann hirsch kishore menezes rest tinker group proved useful development ideas paper mary ann hirsch also helped collect data data general machines research supported national science foundation grants mip9696010and mip9625007 addition research funding intel corporation hewlettpackard international business machines corporation especially thank university illinois impact group use impact compiler comments anonymous referees also appreciated r tuning pentium pro microarchitecture architecture pentium micro processor power powerpc dynamically scheduled vliw processors hardware support large atomic units dynamically scheduled machines architectural framework supporting heteregeneous instructionset architectures fillunit approach multiple instruction issue architectural framework migration cisc higher performance platforms binary translation mimic fast system370 simulator dynamic rescheduling technique object code compatibility vliw archi tectures softwindows unix alpha runs x86 code fx32 design unix operating sustem hpl playdoh architecture specification version 10 effective compiler support predicated execution using hyperblock concepts system administrator r9 data general englewood cliffs data general caching sprite network file system andrew distributed personal computing environment tr andrew distributed personal computing environment design unix operating system mimic fast system370 simulator caching sprite network file system hardware support large atomic units dynamically scheduled machines mips risc architectures architectural framework migration cisc higher performance platforms effective compiler support predicated execution using hyperblock binary translation ibm power powerpc fillunit approach multiple instruction issue dynamic rescheduling dynamically scheduled vliw processors architectural framework supporting heterogeneous instructionset architectures architecture pentium microprocessor tuning pentium pro microarchitecture ctr thomas conte sumedh sathaye properties rescheduling size invariance dynamic reschedulingbased vliw crossgeneration compatibility ieee transactions computers v49 n8 p814825 august 2000 bich c le outoforder execution technique runtime binary translators acm sigops operating systems review v32 n5 p151158 dec 1998