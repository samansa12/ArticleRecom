largescale activeset boxconstrained optimization method spectral projected gradients new activeset method smooth boxconstrained minimization introduced algorithm combines unconstrained method including new linesearch aims add many constraints working set single iteration recently introduced technique spectral projected gradient dropping constraints working set global convergence proved computer implementation fully described numerical comparison assesses reliability new algorithm b introduction problem considered paper consists minimization smooth bounds variables feasi department computer science imeusp university sao paulo rua matao 1010 cidade universitaria 05508090 sao paulo sp brazil author supported pronexoptimization 7679100800 fapesp grants 99080299 01045974 cnpq grant 300151004 email egbirginimeuspbr department applied mathematics imeccunicamp university campinas author supported pronex optimization 7679100800 fapesp grant 01045974 cnpq faepunicamp email martinezimeunicampbr ble set dened boxconstrained minimization algorithms used subalgorithms solving subproblems appear many augmented lagrangian penalty methods general constrained optimization see 11 12 16 17 18 19 20 21 26 28 31 promising novel application reformulation equilibrium problems see 1 references therein methods introduced 11 26 trustregion type iterate x k 2 quadratic approximation f minimized trustregion box objective function value trial point suciently smaller fx k trial point accepted otherwise trust region reduced dierence 11 26 11 trial point face dened cauchy point whereas 26 trial point obtained means specic boxconstrained quadratic solver called quacan see 2 15 23 25 31 13 p 459 trustregion methods boxconstrained optimization introduced 3 29 quacan activeset method uses conjugate gradients within faces approximate internalface minimizations projections add constraints active set orthogonaltotheface direction leave current face approximate minimizer face met 17 clever physical interpretation direction given numerical experiments 16 suggested eciency algorithm 26 relies trustregion strategy strategy quacan dealing constraints motivated us adapt strategy quacan general boxconstrained problems adaptation involves two main decisions one hand one needs choose unconstrained minimization algorithm deal objective function within faces hand necessary dene robust ecient strategies leave faces add active constraints attempts rst decision made 6 10 10 secant multipoint minimization algorithm used 6 authors use secondorder minimization algorithm zhang xu 36 paper adopt leavingface criterion 6 employs spectral projected gradients dened 7 8 see also 4 5 32 33 34 internal minimization faces introduce new general algorithm line search combines backtracking extrapolation compromise every linesearch algorithm accuracy localization onedimensional minimizer economy terms functional evaluations backtrackinglike linesearch algorithms cheap sometimes tend generate excessively small steps reason backtracking complemented simple extrapolation procedure direction chosen step arbitrary provided angle condition satised implementation described paper suggest choose direction using truncatednewton approach means search vector approximate minimizer quadratic approximation function current face use conjugate gradients nd direction rst iterate obviously descent direction property easily monitorized successive conjugate gradient steps present research organized follows section 2 describe unconstrained minimization algorithm deals minimization function box algorithm uses new linesearch technique due technique possible prove either method nishes point boundary perhaps many constraints added converges point box gradient vanishes boxconstrained algorithm described section 3 essentially use algorithm section 2 work within current face spectral projected gradients 7 leave constraints spectral projected gradient technique also allows one leave many bounds add many others working set single iteration feature important largescale calculations section prove global convergence boxconstrained algorithm computational description code gencan given section 4 section 5 show numerical experiments using cute collection section 6 report experiments using large problems 10 7 variables finally section 6 make nal comments suggest lines future research section assume f ir ug set b represent closed faces section 3 dimension n section dimension reduced subspace section 3 gradient section composed derivatives respect free variables section 3 hope using notation rf section lead confusion denote objective dene general iterative algorithm starts interior b either converges unconstrained stationary point nishes boundary b decreased functional value algorithm used within faces boxconstrained method algorithm 21 based line searches armijolike conditions extrapolation given current point x k descent direction k nish line search x k k satises sucient descent criterion directional derivative suciently larger hgx k k sucient descent criterion hold backtracking obtained sucient descent increase directional derivative enough try extrapolation let us explain think philosophy adequate largescale boxconstrained optimization 1 pure backtracking enough proving global convergence many optimization algorithms however accept rst trial point satises armijo condition lead small steps critical situations therefore steps larger unity must tried indicator says worthwhile 2 directional derivative suciently larger consider much decrease increasing steplength direction k accept unit steplength provided satises armijo condition reasonable since usually search direction contains amount secondorder information makes unitary steplength desirable point view preserving satisfactory order convergence 3 unitary steplength satisfy armijo condition backtracking case judge worthwhile compute gradients new trial points would discarded point accepted 4 extrapolation especially useful largescale problems important try add many constraints possible working set extrapolate rather greedy way multiplying steplength xed factor function value decreases 5 think algorithm presented simple way extrapolation devices introduced reasonable balance cost eciency important stress line search coupled virtually minimization procedure computes descent directions z 2 ir n euclidean projection z onto convex set denoted p z section denote p symbol k k represents euclidean norm throughout paper algorithm 21 linesearch based algorithm algorithm starts x 0 2 intb nondimensional parameters given also use small tolerances abs rel 0 initially set k 0 step 1 computing search direction step 11 kg k step 12 compute step 2 linesearch decisions step 21 compute set minf go step 22 else go step 23 step 22 point x k take go step 5 else go step 3 extrapolation else go step 4 backtracking step 23 point x k take k max x fx k1 go step 5 practice point obtained performing step 3 algorithm extrapolation else go step 4 backtracking step 3 extrapolation step 31 max n max set trial max else set trial n step 32 max kp take execution algorithm 21 step 33 fp take step 5 else set trial go step 31 step 4 backtracking step 41 compute new step 42 fx k take go step 5 else go step 41 step 5 k max terminate execution algorithm 21 else set step 1 remarks let us explain main steps algorithm 21 motivations algorithm perform linesearches along directions satisfy anglecosine condition 2 general line search used directions possess secondorder information nat ural step must initially tested accepted sucientdescent directionalderivative conditions 3 4 satised rst test step 21 asks whether x k interior box case fx k try obtain smaller functional values multiplying step xed factor projecting onto box procedure called extrapolation x k interior backtracking interior armijo condition 3 hold also backtracking backtracking stops armijo condition 6 fullled 3 holds test directional derivative condition 4 mentioned 4 satised accept x k new point however 3 holds 4 judge likely taking larger steps along direction k produce decrease objective function case also extrapolation extrapolation procedure try successive projections x k k onto box increasing values entry point interior x k nd k make sure point x tested rst extrapolation nishes decrease function obtained anymore distance two consecutive projected trial points negligible iteration algorithm 21 nishes step 5 corresponding iterate x k1 boundary b algorithm stops encountered boundary point functional value decreased respect previous ones x k1 interior b execution continues increasing iteration number uxdiagrams figures 1 2 help understand structure linesearch procedure xd int aaamax line search bbcondition xnewxd end armijo backtracking extrapolation figure 1 line search procedure extrapolation aamax dist ee aaaatrial aatrialnnaa aamax nna aamax fpxaatrial xnewpxaad end figure 2 extrapolation strategy following theorem prove sequence generated algorithm 21 either stops unconstrained stationary point stops boundary b generates limit unconstrained stationary points theorem 21 algorithm 21 well dened generates points strictly decreasing functional values fx k g sequence generated algorithm 21 one following possibilities holds sequence stops x k gx k ii sequence stops x iii sequence innite least one limit point every limit point x satises gx proof let us prove rst algorithm well dened generates sequence strictly decreasing function values see well dened prove loops steps 3 4 necessarily nish nite time fact step 3 multiply nonnull direction k number greater one take maximum allowable feasible step therefore eventually boundary reached increase condition 5 met loop step 4 classical backtracking loop nishes wellknown directional derivative arguments see 14 exit algorithm always requires fx k sequence strictly decreasing remains prove neither ii hold cluster point x generated sequence satises gx innite subset lim suppose rst ks k k bounded away zero k 2 k 1 therefore exists 0 ks k k k 2 k 1 3 6 k 2 k 1 therefore 2 continuity f implies lim k2k 1 suppose ks k k bounded away zero k 2 k 1 exists k 2 innite subset k 1 lim k2k 2 ks set indices k computed step 22 analogously let k 4 k 2 set indices k computed step 3 k 2 k 4 let k 5 k 2 set indices k computed step 4 k 2 k 5 consider three possibilities k 3 innite ii k 4 innite iii k 5 innite consider rst case 4 ks ks k 2 k 3 since k 3 innite taking convergent subsequence taking limits 7 using continuity obtain since 2 0 1 implies hgx di 0 2 continuity consider case iii case k 5 innite k 2 k 5 exists 0 k ks 0 ks k 10 lim ks 0 9 k 2 k 5 meanvalue theorem exists k 2 0 1 k 2 k 5 dividing ks 0 k k taking limits convergent subsequence obtain inequality similar 8 gx follows arguments consider case ii since considering cases innite sequence generated turns 5 p moreover step 31 trial n p therefore k 2 k 4 writing 0 0 therefore meanvalue theorem k 2 k 4 exists k 2 k thus k 2 k 4 since 0 dividing kd k k taking convergent subsequence k kd k k obtain hgx di 0 2 taking limits get hgx di kgx k implies completes proof 2 3 boxconstrained algorithm problem considered section minimize fx subject x given 1 23 let us divide feasible set disjoint open faces follows smallest ane subspace contains f parallel linear subspace v continuous projected gradient xis dened x 2 f dene g p x main algorithm considered paper described algorithm 31 gencan assume x 0is arbitrary initial point 2 0 1 0 min face contains current iterate x k assume g p otherwise algorithm terminates main iteration algorithm perform test kg 13 takes place judge convenient new iterate belongs f closure f compute x k1 one iteration algorithm 21 set variables restricted free variables f set b previous section corresponds f 13 hold decide constraints abandoned new iterate x k1 computed one iteration spg method described algorithm 32 case computation x k1 compute spectral gradient coecient k following way otherwise dene algorithm 32 algorithm used necessary leave current face according test 13 algorithm 32 spg compute next iterate monotone spg iteration 7 8 spectral step k namely dene search direction k compute x way trying rst perhaps reducing coecient means safeguarded quadratic interpolation procedure remark observe x f x k 2 f x k1 computed algorithm 32 case 13 hold kg p components corresponding free variables g means g p components corresponding xed variables therefore f 0 f 0 according spg iteration 0 0 0 implies x f nish section giving theoretical results roughly speaking prove algorithm well dened karushkuhntucker point computed arbitrary precision moreover dual nondegeneracy innite algorithm identies face limit belongs nite number iterations theorem 31 algorithm 31 well dened proof trivial consequence fact algorithm 21 algorithm 32 spg algorithm 7 well dened 2 theorem 32 assume fx k g generated algorithm 31 suppose exists k 2 f0 k k every limit point fx k g rstorder stationary proof case x k1 computed algorithm 21 k k thus theorem 21 gradient respect free variables tends zero straightforward projection argument follows kg since 13 holds implies kg p every limit point rstorder stationary 2 theorem 33 suppose k 2 f0 exists x k exists limit point fx k g rstorder stationary proof see theorem 33 6 2 theorem 34 suppose stationary points 12 nondegen erate f hypothesis theorem 32 hence thesis must hold proof see theorem 34 6 2 theorem 35 suppose fx k g sequence generated algorithm 31 let arbitrary positive number exists k 2 f0 kg p proof result direct consequence theorems 32 33 2 implementation iteration k algorithm 21 current iterate x k looking direction k satisfying condition 2 use truncatednewton approach compute direction solve newtonian system call algorithm 41 described r 2 fx k following algorithm applies problem initial approximation solution 14 algorithm nds point solution satises qs qs 0 perhaps nal point boundary region dened ksk l u algorithm 41 conjugate gradients parameters 1 k max 2 given algorithm starts step 1 test stopping criteria set step 2 compute conjugate gradient direction step 21 else compute step 22 p step 3 compute step step 31 compute ug step 32 compute step 33 step 4 compute new iterate step 41 compute step 42 b k1 kbkks k1 set step 43 set step 5 compute set step 1 algorithm modication one presented 27 p 529 symmetric positive denite matrices without constraints modications following step 22 test p k descent direction k ie hp k rqs k force condition multiply p k 1 necessary matrixvector products computed exactly safeguard necessary however many cases matrixvector product ap k replaced nitedierence approximation reason perform test order guarantee quadratic decreases along direction p k step 33 test p inequality holds step k direction p k computed minimum among conjugategradient step maximum positive step preserving feasibility rst iteration cg set k max way cg stop angle condition step 42 iteration zero cg keep current approximation solution 14 obtained far step 42 test whether angle condition 2 satised new iterate condition fullled stop algorithm previous iterate also stop algorithm boundary feasible set achieved step 43 convergence criterion conjugategradient algorithm dy namic varies linearly logarithm norm continuous projected gradient beginning value nishing f dene log log used stopping criterion kg p xk 1 algorithm 31 parameter k max maximum number cgiterations call conjugategradient algorithm also varies dynamically way iterations allowed end process beginning reason want invest larger eort solving quadratic subproblems close solution far fact log incrementalquotient version gencan r 2 fx k computed matrixvector products r 2 fx k approximated fact components correspondent free variables computed existence xed variables conveniently exploited 15 5 numerical experiments cute collec tion order assess reliability gencan tested method wellknown alternative algorithms using nonquadratic bound constrained problems 50 variables cute 12 col lection algorithms used comparing gencan box quacan 26 see also 28 lancelot 11 12 spectral projected gradient method spg described spg2 7 see also 8 methods used convergence criterion kg p stopping criteria inhibited gencan used algorithm 31 algorithm 41 algorithms used parameters line search algorithm 32 default parameters mentioned 7 used line search algorithm 21 ie lancelot used exact second derivatives use preconditioning conjugategradient method reason gencan conjugate gradient method computing directions also used without preconditioning options lancelot default ones small number modications made boxquacan provide fair comparison modications initial trustregion radius gencan adopted ii maximum number conjugategradient iterations xed accuracy solving quadratic subproblems dynamic box quacan varying 01 10 5 done gencan iv minimum trustregion radius min xed 10 3 equal corresponding parameter gencan codes written fortran 77 tests done using ultrasparc sun 4 processors 167 mhz 1280 mega bytes main memory solaris 251 operating system compiler workshop compilers 42 oct 1996 fortran 77 42 finally used ag o4 optimize code rst four tables report full performance lancelot spg boxquacan gencan true hessian gencan incremental quotients usual denition iteration lancelot involves one function evaluation however order unify comparison call iteration whole process computes new iterate lower functional value starting current one therefore single lancelot iteration involves one gradient evaluation perhaps several functional evaluations iteration several trustregion problems solved approximately uses number cgiterations problems hadamals scon1ls bounds lower limit equal upper limit boxquacan run circumstances performance method situation reported corresponding table tables report method number iterations fe functional evaluations ge gradient evaluations cg conjugate gradient iterations except case spg cg iterations computed time cpu time seconds nal functional value obtained projected gradient nal point next 3 tables repeat information rst ones compact readable form table 5 report nal functional value obtained method cases least one dierence computed four signicant digits table 7 report method numbers fe gecg idea cg iteration sometimes costly gradientevaluation cost certainly use incrementalquotient version gencan roughly speaking gecg represents amount work used solving subproblems fe represents work done true problem trying reduce objective function table 8 reports computer times problems least one methods used 1 second computer time used lancelot must considered warning made 9 page 136 lancelot require interface using cute tools worth noting lancelot exploits much structure problem n fe ge cg time fx kg p xk1 6467d 06 qrtquad 120 144 178 145 570 139 3625d06 3505d 06 chebyqad 50 22 28 23 463 222 5386d 7229d 06 linverse 1999 22 28 23 2049 4722 6810d02 3003d 06 table 1 performance lancelot provided interface tools consequence although gencan used less iterations less functional evaluations less gradient evaluations less conjugategradient iterations lancelot scon1ls computer time greater one spent lancelot problems like qr3dls chebyqad way lancelot takes advantage sif structure also impressive include additional table motivated observation table 7 observed number functional evaluations per iteration larger gencan lancelot box quacan possible reasons three many spgiterations perhaps many functional evaluations per iteration many tniterations backtracking many tniterations extrapolations classify iterations extrapolation successful unsuccessful ones successful extrapolation iteration extrapolation produced functional value smaller one corresponding rst problem n fe ge time fx kg p xk1 7896d 06 qrtquad 120 598 1025 599 020 3624d06 8049d 06 hadamals 1024 chebyqad 50 841 1340 842 3375 5386d 9549d 06 nonscomp 10000 43 44 44 281 3419d 10 7191d 06 table 2 performance spg trial point unsuccessful extrapolation corresponds failure rst attempt double steplength therefore unsuccessful ex trapolation additional unnecessary functional evaluation done next iterate corresponds rst trial point according report table 9 following features gencan incrementalquotient spgit spg iterations used leaving current face spgfe functional evaluations spgiterations tnit tn iterations tnfe functional evaluations tniterations tnstep 1it tniterations unitary step accepted tnstep 1fe functional evaluations tniterations unitary step accepted necessarily equal tnstep 1 tnbacktrackingit tniterations backtracking necessary problem n fe ge cg time fx kg p xk1 5742d 06 expquad 120 28 523 9133d03 6388d 07 qrtquad 120 22 28 23 214 010 3625d06 5706d 07 chebyqad 50 52 66 53 960 4570 5387d 9535d 06 6559d 06 table 3 performance boxquacan tnbacktrackingfe functional evaluations iterations backtracking tnextrapit successful iterations extrapolation tnextrapfe functional evaluations successful iterations extrapolation tnextrap unsuccessful iterations extrapolation tnextrap fe functional evaluations unsuccessful iterations extrapolation number necessarily equal twice corresponding number iterations problem n fe ge cg time fx kg p xk1 expquad 120 3813d 06 nonscomp 10000 17 43 19 9053d 06 table 4 performance gencan truehessian version problem n fe ge cg time fx kg p xk1 explin 120 17 43 19 expquad 120 21 51 23 53 003 3626d06 2236d 06 chebyqad 50 31 43 2929d 06 nonscomp 10000 table 5 performance gencan incrementalquotient version bdexp 1969d 2744d 1967d qrtquad 3625d06 3624d06 3625d06 3625d06 3625d06 chebyqad 5386d 5386d 5387d 5386d 5386d deconvb 6393d 09 4826d 08 5664d 6043d qr3dls 2245d 08 1973d 05 1450d scon1ls 5981d 04 1224d00 1269d 4549d 04 table final functional values problem lancelot spg boxquacan gencanquot fe gecg fe ge fe gecg fe gecg 43 58 43 expquad qrtquad 178 715 1025 599 28 236 75 101 chebyqad 28 486 1340 842 66 1012 43 918 28 2072 1853 1023 19 415 34 87 scon1ls 9340 5750468 7673022 5000002 8565 4995260 table 7 functional equivalentgradient gecg evaluations mccormck 424 227 523 457 356 hadamals 440 163 180 119 chebyqad 222 3375 4570 1386 2226 qr3dls 43931 220397 228609 97613 52350 table 8 computer time type gencan iterations details truncated newton iterations spg iteration tn iterations step1 backtracking extrap extrap problem fe fe fe fe fe fe table 9 gencan features observing table 9 realise 1 number spgiterations surprisingly small therefore iterations mechanism leave face activated iterations number active constraints remains increased clearly spgiterations responsible relatively high number functional evaluations 2 number iterations backtracking necessary also surprisingly small therefore extrapolations responsible functionalevaluations phenomenon since unsuccessful extrapolation uses one additional unnecessary functional evaluations contribution increasing fe also moderate fact unsuccessful extrapolations responsible 116 functional evaluations considering problems means less 8 evaluations per problem turns many functional evaluations used successful extrapolations considering overall performance method seems really good feature extreme case bdexp one tniteration performed giving successful extrapolation used 11 functional evaluations gave solution problem remarks convergence obtained problems methods tested exception spg solve scon1ls thirty hours computer time method cases obtained lowest functional values gencanquot dier ences seem large enough reveal clear tendency already mentioned 7 behavior spg surprisingly good although method fails solve problem reasonable time behavior problems works quite ecient indicates existence families problems spg probably best possible alternative observation already made 8 boxquacan less successful method set ex periments surprising since authors 16 observed method outperformed lancelot quadratic problems good function far quadratic fact observation motivated present work nevertheless still large scope improvements boxquacan far take account improvements solution quadratic subproblems possible sophisticated strategies updating trustregion radius incorporated 6 experiments large problems wish place q circles radius r rectangle 0 1 way qg intersection circle circle j one point therefore given goal determine c solving problem minimize subject r c r c points c centers desired circles objective function value solution minimization problem zero original problem solved problem known cylinder packing problem 22 present generalization directed sociometry applications table describes main features medium large scale problems type problems 915 sets randomly generated schrages random number generator 35 seed 1 cases used 05 observe n number variables equal 2 q tables 11 12 show performances gencan lancelot internal limitations bigproblems installation cute forbid solution larger instances problems using sif show cpu problem n box table 10 medium largescale classical modied cylinder packing problems times gencan using sif siftime fortran subroutines fs computing function gradient used random initial point generated inside box schrages algorithm seed equal 1 methods found global solution cases table 12 also report number free variables solution far found gencan gencan using fortran subroutines using sif lancelot fe ge cg time fe ge cg time fe ge cg time table 11 gencan lancelot cylinder packing problems 9 28 1064979 9914682 table 12 gencan large problems 7 final remarks numerical algorithms must analyzed point view present state also considerations related possibility im provement chances improvement activeset methods like one presented paper come development new unconstrained algorithms adaptation known unconstrained algorithms specic characteristics problem algorithm computation search direction open many possibilities mentioned introduction secant multipoint scheme dierent procedure leaving faces considered 10 negativecurvature newtonian direction small problems used 6 leaving faces also associated spg particularly interesting alternative preconditioned spectral projected gradient method introduced 30 extension technique paper general linearly constrained optimization another interesting subject possible research theoretical point view extension straightforward convergence proofs oer technical diculties real diculty need project onto feasible set extrapolation steps spg iterations theory extrapolation avoided without aecting global convergence projections essential spg iterations sometimes feasible polytope projections easy compute see 8 cases extension gencan would probably quite ecient acknowledgements authors grateful nick gould helped use sif language also indebted two anonymous referees whose comments helped us lot improve nal version paper r resolution generalized nonlinear complementarity problem limited memory algorithm bound constrained minimization restricted opti mization clue fast accurate implementation common ection surface stack method cute constrained unconstrained testing environment global convergence class trust region algorithms optimization simple bounds globally convergent augmented lagrangean algorithm optimization general constraints simple bounds numerical methods unconstrained optimization nonlinear equations comparing numerical performance two trustregion algorithms largescale boundconstrained minimization optimising palletisation cylinders cases matrix computations preconditioned spectral gradient method unconstrained optimization problems barzilai borwein choice steplength gradient method barzilai borwein gradient method large scale unconstrained minimization problem portable fortran random number generator class inde tr global convergence class trust region algorithms optimization simple bounds globally convergent augmented lagrangian algorithm optimization general constraints simple bounds limited memory algorithm bound constrained optimization matrix computations 3rd ed gradient method retards generalizations estimation optical constants thickness thin films using unconstrained optimization portable fortran random number generator trustregion methods validation augmented lagrangian algorithm gaussnewton hessian approximation using set hardspheres problems dualitybased domain decomposition natural coarsespace variational inequalities0 algorithm 813 resolution generalized nonlinear complementarity problem class indefinite dogleg path methods unconstrained minimization barzilai borwein gradient method large scale unconstrained minimization problem nonmonotone spectral projected gradient methods convex sets newtons method large boundconstrained optimization problems constrained quadratic programming proportioning projections augmented lagrangians adaptive precision control quadratic programming equality constraints ctr g birgin j martnez structured minimalmemory inexact quasinewton method secant preconditioners augmented lagrangian optimization computational optimization applications v39 n1 p116 january 2008 g birgin r castillo j martnez numerical comparison augmented lagrangian algorithms nonconvex problems computational optimization applications v31 n1 p3155 may 2005 g birgin j martnez f h nishihara p ronconi orthogonal packing rectangular items within arbitrary convex regions nonlinear optimization computers operations research v33 n12 p35353548 december 2006